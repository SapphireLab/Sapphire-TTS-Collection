# Towards Controllable Speech Synthesis in the Era of Large Language Models: A Survey

<details>
<summary>基本信息</summary>

- 标题: "Towards Controllable Speech Synthesis in the Era of Large Language Models: A Survey"
- 作者:
  - 01 Tianxin Xie,
  - 02 Yan Rong,
  - 03 Pengfei Zhang,
  - 04 Li Liu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.06602)
  - [Publication]()
  - [Github](https://github.com/imxtx/awesome-controllabe-speech-synthesis)
  - [Demo]()
- 文件:
  - [ArXiv](../PDF/2024.12.09__2412.06602v1__Survey__Towards_Controllable_Speech_Synthesis_in_the_Era_of_LLMs.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

Text-to-speech (TTS), also known as speech synthesis, is a prominent research area that aims to generate natural-sounding human speech from text.
Recently, with the increasing industrial demand, TTS technologies have evolved beyond synthesizing human-like speech to enabling controllable speech generation.
This includes fine-grained control over various attributes of synthesized speech such as emotion, prosody, timbre, and duration.
Besides, advancements in deep learning, such as diffusion and large language models, have significantly enhanced controllable TTS over the past several years.
In this paper, we conduct a comprehensive survey of controllable TTS, covering approaches ranging from basic control techniques to methods utilizing natural language prompts, aiming to provide a clear understanding of the current state of research.
We examine the general controllable TTS pipeline, challenges, model architectures, and control strategies, offering a comprehensive and clear taxonomy of existing methods.
Additionally, we provide a detailed summary of datasets and evaluation metrics and shed some light on the applications and future directions of controllable TTS.
To the best of our knowledge, this survey paper provides the first comprehensive review of emerging controllable TTS methods, which can serve as a beneficial resource for both academic researchers and industry practitioners.

</td><td>

文本转语音 (Text-to-Speech, TTS), 也称为语音合成 (Speech Synthesis) 是旨在从文本生成自然听感的人类语音的重要研究领域.

近年来随着工业需求的增加, TTS 技术已经从合成类似人类的语音演进为实现可控语音生成.
这包括对合成的语音的各种属性进行细粒度的控制, 例如情感, 韵律, 音色以及时长.

除此之外, 深度学习的最新进展例如扩散和大语言模型, 已经在过去几年中显著地增强了可控 TTS.

在本文中, 我们对可控 TTS 进行了全面地调查, 涵盖从基础控制技术到利用自然语言模型的方法, 旨在为研究的当前状态提供清晰的理解.

我们研究了一般可控 TTS 的流程, 挑战, 模型架构和控制策略, 提供现有方法的全面且清晰的分类.

据我们所知, 本文是新兴可控 TTS 方法的首个全面综述, 这对于学术研究者和工业实践者都可以提供有益的参考.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

Speech synthesis, also broadly known as text-to-speech (TTS), is a long-time developed technique that aims to synthesize human-like voices from text ([^1] [^2]), and it has extensive applications in our daily lives, such as health care ([^3] [^4]), personal assistants [^5], entertainment ([^6] [^7]), and robotics ([^8] [^9]).
Recently, TTS has gained significant attention with the rise of large language model (LLM)-powered chatbots, such as ChatGPT [^10] and [LLaMA [11]](../../Models/TextLM/2023.02.27_LLaMA.md), due to its naturalness and convenience for human-computer interaction.
Meanwhile, the ability to achieve fine-grained control over synthesized speech attributes, such as emotion, prosody, timbre, and duration, has become a hot research topic in both academia and industry, driven by its vast potential for diverse applications.

[^1]: Website - Speech Synthesis - Wikipedia [1] https://en.wikipedia.org/wiki/Speech_synthesis
[^2]: Book - An Introduction to Text-to-Speech Synthesis (1997)
[^3]: Speech Technology for Healthcare: Opportunities, Challenges, and State of The Art (2020)
[^4]: Speech Synthesis from Neural Decoding of Spoken Sentences (2019)
[^5]: Alexa vs. Siri vs. Cortana vs. Google Assistant: A Comparison of Speech-Based Natural User Interfaces (2018)
[^6]: Speech-Driven Cartoon Animation with Emotions (2001)
[^7]: Comic-Guided Speech Synthesis (2019)
[^8]: Spoken Language Interaction with Robots: Recommendations for Future Research (2022)
[^9]: Towards Expressive Speech Synthesis in English on A Robotic Platform (2006)
[^10]: Website - Introducing ChatGPT - OpenAI https://openai.com/index/chatgpt/

Deep learning [^12] has made great progress in the past decade due to exponentially growing computational resources like GPUs [^13], leading to the explosion of numerous great works on TTS ([NaturalSpeech [14]](../../Models/E2E/2022.05.09_NaturalSpeech.md); [FastSpeech [15]](../../Models/Acoustic/2019.05.22_FastSpeech.md); [Deep Voice [16]](../../Models/System/2017.02.25_DeepVoice.md); [CosyVoice [17]](../../Models/SpeechLM/ST2S/2024.07.07_CosyVoice.md)).
These methods can synthesize human speech with better quality ([NaturalSpeech [14]](../../Models/E2E/2022.05.09_NaturalSpeech.md)) and can achieve fine-grained control of the generated voice ([Zhang et al. [18]](../../Models/Acoustic/2018.12.11_Learning_Latent_Representations_for_Style_Control_and_Transfer_in_End-to-End_Speech_Synthesis.md); [GST [19]](../../Models/Style/2018.03.23_GST.md); [I2I [20]](../../Models/Style/2019.11.05_I2I.md); [Li et al. [21]](../../Models/Style/2020.11.17_Controllable_Emotion_Transfer_for_End-to-End_Speech_Synthesis.md); [iEmoTTS [22]](../../Models/Style/2022.06.29_iEmoTTS.md)).
Besides, some recent works synthesize speech given multi-modal input, such as face images ([FR-PSS [23]](../../Models/_Basis/2022.04.01_FR-PSS.md); [Face2Speech [24]](../../Models/_Basis/2020.10.25_Face2Speech.md)), cartoons [^7], and videos ([DiffV2S [25]](../../Models/CV/2023.08.15_DiffV2S.md)).
Moreover, with the fast development of open-source LLMs ([LLaMA [11]](../../Models/TextLM/2023.02.27_LLaMA.md); [Mistral [26]](../../Models/TextLM/2023.10.10_Mistral-7B.md); [Qwen [27]](../../Models/TextLM/2023.09.28_Qwen.md); [DeepSeek [28]](../../Models/TextLM/DeepSeek.md); [ChatGLM [29]](../../Models/TextLM/ChatGLM.md)), some researchers propose to synthesize fine-grained controllable speech with natural language description ([LLM+VALL-E [30]](../../Models/SpeechLM/2023.12.30_LLM&VALL-E.md); [Emo-DPO [31]](../../Modules/RLHF/2024.09.16_Emo-DPO.md); [T5-TTS [32]](../../Models/SpeechLM/2024.06.25_T5-TTS.md)), coining a new way to generate custom speech voices.
Meanwhile, powering LLMs with speech synthesis has also been a hot topic in the last few years ([LLaMA-Omni [33]](../../Models/SpokenDialogue/2024.09.10_LLaMA-Omni.md); [SpeechGPT [34]](../../Models/SpokenDialogue/2023.05.18_SpeechGPT.md); [IntrinsicVoice [35]](../../Models/SpokenDialogue/2024.10.09_IntrinsicVoice.md)).
In recent years, a wide range of TTS methods has emerged, making it essential for researchers to gain a comprehensive understanding of current research trends, particularly in controllable TTS, to identify promising future directions in this rapidly evolving field.
Consequently, there is a pressing need for an up-to-date survey of TTS techniques.
While several existing surveys address parametric-based approaches ([Survey by Klatt et al. (1987) [36]](../S19870501.md); [Survey by Dutoit et al. (1997) [37]](../S19970000.md); [Survey by Breen et al. (1992) [38]](../S19920000.md); [Survey by Olive (1985) [39]](../S19851105.md); [Survey by King et al. (2014) [40]](../S20140630.md); [Survey by Zen et al. (2009) [41]](../S20090114.md)) and deep learning-based TTS ([Survey by Tan et al. (2021) [42]](../2021.06.29_A_Survey_on_Neural_Speech_Synthesis_63P/Main.md); [Survey by Ning et al. (2019) [43]](../S20190801.md); [Survey by Kaur et al. (2023) [44]](../S20221113.md); [Survey by Mattheyses et al. (2014) [45]](../S20140221.md); [Survey by Triantafyllopoulos et al. (2023) [46]](../S20221006.md); [Survey by Mu et al. (2021) [47]](../2021.04.20_Review_of_End-to-End_Speech_Synthesis_Technology_Based_on_Deep_Learning_40P.md); [Survey by Mehrish et al. (2023) [48]](../2023.04.30_A_Review_of_DL_Techniques_for_Speech_Processing_55P/Main.md)), they largely overlook the controllability of TTS.
Additionally, these surveys do not cover the advancements in recent years, such as natural language description-based TTS methods.

[^12]: Deep Learning (2015)
[^13]: GPU Computing (2008)

This paper provides a comprehensive and in-depth survey of existing and emerging TTS technologies, with a particular focus on controllable TTS methods.
Fig.01 demonstrates the development of controllable TTS methods in recent years, showing their backbones, feature representations, and control abilities.
The remainder of this section begins with a brief comparison between this survey and previous ones, followed by an overview of the history of controllable TTS technologies, ranging from early milestones to state-of-the-art advancements.
Finally, we introduce the taxonomy and organization of this paper.

</td><td>

语音合成 (Speech Synthesis), 也称为文本转语音 (Text-to-Speech, TTS), 是一项长期发展的技术, 旨在从文本合成类似人类的声音 ([^1] [^2]), 并且在日常生活中有广泛的应用, 如医疗保健 ([^3] [^4]), 个性化助手 [^5], 娱乐 ([^6] [^7]), 以及机器人 ([^8] [^9]).
近期, 随着大语言模型驱动的聊天机器人 (如 ChatGPT [^10] 和 [LLaMA [11]](../../Models/TextLM/2023.02.27_LLaMA.md)) 的兴起, TTS 获得了广泛关注, 因为它在人机交互中的自然性和便利性.
同时, 对合成语音的属性进行细粒度的控制, 如情感, 韵律, 音色, 以及时长, 已经成为学术界和工业界热门研究话题, 其潜在的多样化应用吸引了广泛的注意力.

深度学习 [^12] 在过去十年内由于指数增长的计算资源 (如 GPU [^13]) 取得了巨大的进步, 也促使了 TTS 领域的众多优秀工作的出现 ([NaturalSpeech [14]](../../Models/E2E/2022.05.09_NaturalSpeech.md); [FastSpeech [15]](../../Models/Acoustic/2019.05.22_FastSpeech.md); [Deep Voice [16]](../../Models/System/2017.02.25_DeepVoice.md); [CosyVoice [17]](../../Models/SpeechLM/ST2S/2024.07.07_CosyVoice.md)).

这些方法可以合成更高质量的人类语音 ([NaturalSpeech [14]](../../Models/E2E/2022.05.09_NaturalSpeech.md)) 并且可以实现对生成声音的细粒度控制 ([Zhang et al. [18]](../../Models/Acoustic/2018.12.11_Learning_Latent_Representations_for_Style_Control_and_Transfer_in_End-to-End_Speech_Synthesis.md); [GST [19]](../../Models/Style/2018.03.23_GST.md); [I2I [20]](../../Models/Style/2019.11.05_I2I.md); [Li et al. [21]](../../Models/Style/2020.11.17_Controllable_Emotion_Transfer_for_End-to-End_Speech_Synthesis.md); [iEmoTTS [22]](../../Models/Style/2022.06.29_iEmoTTS.md)).

除此之外, 一些近期工作给定多模态输入来合成语音, 例如面部图像 ([FR-PSS [23]](../../Models/_Basis/2022.04.01_FR-PSS.md); [Face2Speech [24]](../../Models/_Basis/2020.10.25_Face2Speech.md)), 卡通[^7], 以及视频 ([DiffV2S [25]](../../Models/CV/2023.08.15_DiffV2S.md)).

此外, 随着开源大语言模型的快速发展 ([LLaMA [11]](../../Models/TextLM/2023.02.27_LLaMA.md); [Mistral [26]](../../Models/TextLM/2023.10.10_Mistral-7B.md); [Qwen [27]](../../Models/TextLM/2023.09.28_Qwen.md); [DeepSeek [28]](../../Models/TextLM/DeepSeek.md); [ChatGLM [29]](../../Models/TextLM/ChatGLM.md)), 一些研究人员提出使用自然语言描述来合成细粒度可控语音, 创造了一种生成自定义语音声音的新方式 ([LLM+VALL-E [30]](../../Models/SpeechLM/2023.12.30_LLM&VALL-E.md); [Emo-DPO [31]](../../Modules/RLHF/2024.09.16_Emo-DPO.md); [T5-TTS [32]](../../Models/SpeechLM/2024.06.25_T5-TTS.md)).

同时, 赋予大语言模型语音合成能力也已经成为过去几年的热门话题 ([LLaMA-Omni [33]](../../Models/SpokenDialogue/2024.09.10_LLaMA-Omni.md); [SpeechGPT [34]](../../Models/SpokenDialogue/2023.05.18_SpeechGPT.md); [IntrinsicVoice [35]](../../Models/SpokenDialogue/2024.10.09_IntrinsicVoice.md)).

近年来, 出现了很多 TTS 方法, 这使得研究人员必须全面理解当前研究趋势, 特别是在可控 TTS 方面, 以确定这一快速发展领域的有前景的未来方向.

因此, 迫切需要一份最新的 TTS 技术综述.

尽管现有的综述关注了
- 基于参数的方法
  - [Survey by Klatt et al. (1987) [36]](../S19870501.md);
  - [Survey by Dutoit et al. (1997) [37]](../S19970000.md);
  - [Survey by Breen et al. (1992) [38]](../S19920000.md);
  - [Survey by Olive (1985) [39]](../S19851105.md);
  - [Survey by King et al. (2014) [40]](../S20140630.md);
  - [Survey by Zen et al. (2009) [41]](../S20090114.md),
- 基于深度学习的 TTS
  - [Survey by Tan et al. (2021) [42]](../2021.06.29_A_Survey_on_Neural_Speech_Synthesis_63P/Main.md);
  - [Survey by Ning et al. (2019) [43]](../S20190801.md);
  - [Survey by Kaur et al. (2023) [44]](../S20221113.md);
  - [Survey by Mattheyses et al. (2014) [45]](../S20140221.md);
  - [Survey by Triantafyllopoulos et al. (2023) [46]](../S20221006.md);
  - [Survey by Mu et al. (2021) [47]](../2021.04.20_Review_of_End-to-End_Speech_Synthesis_Technology_Based_on_Deep_Learning_40P.md);
  - [Survey by Mehrish et al. (2023) [48]](../2023.04.30_A_Review_of_DL_Techniques_for_Speech_Processing_55P/Main.md).

但它们大多忽略了 TTS 的可控性.
此外, 这些综述没有涵盖最近几年的进展, 例如基于自然语言描述的 TTS 方法.

本文提供了对现有和新兴 TTS 技术的全面且深入的调查, 特别关注可控 TTS 方法.
图 01 展示了近年来可控 TTS 方法的发展, 展示了它们的骨干, 特征表示和控制能力.

本节的剩余部分
- 首先简要比较了本文和之前综述,
- 随后介绍可控 TTS 技术的历史, 从早期里程碑到最先进的进展.
- 最后, 我们介绍了本文的分类体系和组织结构.


</td></tr>
<tr><td colspan="2">

![Images/S20241209_Fig.01.png](Images/S20241209_Fig.01.png)

</td></tr></table>

### A·Comparison with Existing Surveys: 与现有综述的比较

<table><tr><td width="50%">

Several survey papers have reviewed TTS technologies, spanning early approaches from previous decades ([Survey by Klatt et al. (1987) [36]](../S19870501.md); [Survey by Dutoit et al. (1997) [37]](../S19970000.md); [Survey by King et al. (2014) [40]](../S20140630.md); [Survey by Tabet et al. (2011) [49]](../S20110509.md)) to more recent advancements ([Survey by Ning et al. (2019) [43]](../S20190801.md); [Survey by Tan et al. (2021) [42]](../2021.06.29_A_Survey_on_Neural_Speech_Synthesis_63P/Main.md); [Survey by Zhang et al. (2023) [50]](../2023.03.23_A_Survey_on_Audio_Diffusion_Models__TTS_Synthesis_&_Enhancement_in_Generative_AI/Main.md)).
However, to the best of our knowledge, this paper is the first to focus specifically on controllable TTS.
The key differences between this survey and prior work are summarized as follows:

</td><td>

多篇综述论文已经回顾了 TTS 技术的发展历程, 涵盖了从几十年前的早期方法到近年来的最新进展.
- 早期方法:
  - [Survey by Klatt et al. (1987) [36]](../S19870501.md);
  - [Survey by Dutoit et al. (1997) [37]](../S19970000.md);
  - [Survey by King et al. (2014) [40]](../S20140630.md);
  - [Survey by Tabet et al. (2011) [49]](../S20110509.md)
- 近期进展:
  - [Survey by Ning et al. (2019) [43]](../S20190801.md);
  - [Survey by Tan et al. (2021) [42]](../2021.06.29_A_Survey_on_Neural_Speech_Synthesis_63P/Main.md);
  - [Survey by Zhang et al. (2023) [50]](../2023.03.23_A_Survey_on_Audio_Diffusion_Models__TTS_Synthesis_&_Enhancement_in_Generative_AI/Main.md)

然而据我们所知, 本文是首篇专门聚焦于可控 TTS 的论文.

本综述与之前工作的主要区别总结如下:

</td></tr></table>

#### Different Scope: 不同范围

<table><tr><td width="50%">

[Survey by Klatt et al. (1987) [36]](../S19870501.md) provided the first comprehensive survey on formant, concatenative, and articulatory TTS methods, with a strong emphasis on text analysis.
In the early 2010s, [Survey by Tabet et al. (2011) [49]](../S20110509.md) and [Survey by King et al. (2014) [40]](../S20140630.md) explored rule-based, concatenative, and HMM-based techniques.
Later, the advent of deep learning catalyzed the emergence of numerous neural-based TTS methods.
Therefore, [Survey by Ning et al. (2019) [43]](../S20190801.md) and [Survey by Tan et al. (2021) [42]](../2021.06.29_A_Survey_on_Neural_Speech_Synthesis_63P/Main.md) have conducted extensive surveys on neural-based acoustic models and vocoders, while [Survey by Zhang et al. (2023) [50]](../2023.03.23_A_Survey_on_Audio_Diffusion_Models__TTS_Synthesis_&_Enhancement_in_Generative_AI/Main.md) presented the first review of diffusion model-based TTS techniques.
However, these studies offer limited discussion on the controllability of TTS systems.
To address this gap, we present the first comprehensive survey of TTS methods through the lens of controllability, providing an in-depth analysis of model architectures and strategies for controlling synthesized speech.

</td><td>

- [Klatt 等人 (1987) [36]](../S19870501.md) 的综述首次全面探讨了共振峰, 拼接式, 发音式 TTS 方法, 并特别强调了文本分析的重要性.
- 到了 2010 年代初期, [Tabet 等人 (2011) [49]](../S20110509.md) 和 [King 等人 (2014) [40]](../S20140630.md) 的综述进一步研究了基于规则, 拼接式以及隐马尔可夫模型 (HMM) 的 TTS 技术.
- 随后, 深度学习的兴起催生了众多基于神经网络的 TTS 方法.
- 因此 [Ning 等人 (2019) [43]](../S20190801.md) 和 [Tan 等人 (2021) [42]](../2021.06.29_A_Survey_on_Neural_Speech_Synthesis_63P/Main.md) 的综述对基于神经网络的声学模型和声码器进行了广泛探讨, 而 [Zhang 等人 (2023) [50]](../2023.03.23_A_Survey_on_Audio_Diffusion_Models__TTS_Synthesis_&_Enhancement_in_Generative_AI/Main.md) 的综述则首次对基于扩散模型的 TTS 技术进行了回顾.

然而, 这些研究对 TTS 系统可控性的讨论较为有限.

为了填补这一空白, 本文首次从可控性的角度对 TTS 方法进行了全面综述, 深入分析了模型架构以及控制合成语音的策略.

</td></tr></table>

#### Close to Current Demand: 接近当前需求

<table><tr><td width="50%">

With the rapid development of hardware (i.e., GPUs) and artificial intelligence techniques (i.e., transformers, LLMs, diffusion models) in the last few years, the demand for controllable TTS is becoming increasingly urgent due to its broad applications in industries such as filmmaking, gaming, robots, and personal assistants.
Despite this growing need, existing surveys pay little attention to control methods in TTS technologies.
To bridge this gap, we propose a systematic analysis of current controllable TTS methods and the associated challenges, offering a comprehensive understanding of the research state in this field.

</td><td>

近年来随着硬件 (如 GPU) 和人工智能技术 (如 Transformer, 大语言模型, 扩散模型) 的快速发展, 可控 TTS 的需求日益迫切, 因为其在影视制作, 游戏, 机器人以及个人助手等领域的应用越来越广泛.
尽管这种需求不断增长, 但现有的综述对 TTS 技术中的控制方法关注甚少.
为了填补这一空白, 本文提出对当前可控 TTS 方法及其相关挑战进行系统性分析, 旨在为该领域的研究现状提供全面深入的理解.

</td></tr></table>

#### New Insights & Directions: 新见解与方向

<table><tr><td width="50%">

This survey offers new insights through a comprehensive analysis of model architectures and control methods in controllable TTS systems.
Additionally, it provides an in-depth discussion of the challenges associated with various controllable TTS tasks.
Furthermore, we address the question: "Where are we on the path to fully controllable TTS technologies?", by examining the relationship and gap between current TTS methods and industrial requirements.
Based on these analyses, we identify promising directions for future research on TTS technologies.

Table.01 summarizes representative surveys and this paper in terms of main focus and publication year.

</td><td>

本综述
- 通过对可控 TTS 系统中模型架构和控制方法的全面分析, 提供了新的见解.
- 此外, 本文深入探讨了与各种可控 TTS 任务相关的挑战.
- 更进一步, 我们通过审视当前 TTS 方法与工业需求之间的关系与差距, 回答了这样一个问题：**"我们在实现完全可控TTS技术的道路上处于什么位置?"**
- 基于这些分析，我们为TTS技术的未来研究指明了有前景的方向.

</td></tr>
<tr><td colspan="2">

表 01 总结了代表性综述及本文的主要关注点和发表年份。

| 综述 | 主要关注点 | 发表年份 |
|---|---|:-:|
|[Klatt et al. (1987) [36]](../S19870501.md)| 基于规则和拼接的 TTS | 1987 |
|[Tabet et al. (2011) [49]](../S20110509.md)| 基于规则, 拼接, 参数的 TTS | 2011 |
|[King et al. (2014) [40]](../S20140630.md)|参数 TTS 与性能度量 | 2014 |
|[Tan et al. (2021) [42]](../2021.06.29_A_Survey_on_Neural_Speech_Synthesis_63P/Main.md)|神经网络, 高效, 表达性 TTS | 2021 |
|[Zhang et al. (2023) [50]](../2023.03.23_A_Survey_on_Audio_Diffusion_Models__TTS_Synthesis_&_Enhancement_in_Generative_AI/Main.md)|扩散模型 TTS, 语音增强|2023|
|本文|可控 TTS, 评估 | 2024 |

</td></tr></table>

### B·The History of Controllable TTS: 可控 TTS 的历史

<table><tr><td width="50%">

Controllable TTS aims to control various aspects of synthesized speech, such as pitch, energy, speed/duration, prosody, timbre, emotion, gender, or high-level styles.
This subsection briefly reviews the history of controllable TTS ranging from early approaches to the state-of-arts in recent years.

</td><td>

可控性 TTS 的目标是控制合成语音的各个方面, 如音高, 能量, 速度/时长, 韵律, 音色, 情感, 性别, 或高级风格.
本小节简要回顾了可控 TTS 的历史, 从早期方法到近年来工业界的最新技术.

</td></tr></table>

#### Early Approaches: 早期方法

<table><tr><td width="50%">

Before the prevalence of deep neural networks (DNNs), controllable TTS technologies were built primarily on rule-based, concatenative, and statistical methods.
These approaches enable some degree of customization and control, though they were constrained by the limitations of the underlying models and available computational resources.
1) Rule-based TTS systems ([Rbiner et al. [51]](../../Models/_Early/1967.11.01_Digital‐Formant_Synthesizer_for_Speech‐Synthesis_Studies.md); [Allen et al. [52]](../../Models/_Early/1987.04.01_From_Text_to_Speech__The_MITalk_System.md); [Purcell et al. [53]](../../Models/_Early/2006.02.10_Adaptive_Control_of_Vowel_Formant_Frequency__Evidence_from_Real-Time_Formant_Manipulation.md); [Klatt et al. [54]](../../Models/_Early/1980.03.01_Software_for_a_Cascade_or_Parallel_Formant_Synthesizer.md)), such as formant synthesis, were among the earliest methods for speech generation.
These systems use manually crafted rules to simulate the speech generation process by controlling acoustic parameters such as pitch, duration, and formant frequencies, allowing explicit manipulation of prosody and phonetic details through rule adjustments.
1) Concatenative TTS [55] [56] [57] [58] ~\cite{wouters2001control,bulut2002expressive,bulyko2001joint,Stylianou200121}, which dominated the field in the late 1990s and early 2000s, synthesize speech by concatenating pre-recorded speech segments, such as phonemes or diphones, stored in a large database~\cite{Hunt1996373} [59].
These methods can modify the prosody by manipulating the pitch, duration, and amplitude of speech segments during concatenation.
They also allow limited voice customization by selecting speech units from different speakers.
1) Parametric methods, particularly HMM-based TTS [60] [61] [62] [63]~\cite{nose2007style,ling2009integrating,nose2009hmm,yoshimura1998duration,[Yoshimura et al.(1999) [64]](../../Models/_Early/Simultaneous_Modeling_of_Spectrum_Pitch_&_Duration_in_HMM-Based_Speech_Synthesis.md); [Tokuda et al. (2000) [65]](../../Models/_Early/Speech_Parameter_Generation_Algorithms_for_HMM-Based_Speech_Synthesis.md)}, gained prominence in the late 2000s.
These systems model the relationships between linguistic features and acoustic parameters, providing more flexibility in controlling prosody, pitch, speaking rate, and timbre by adjusting statistical parameters.
Some HMM-based systems also supported speaker adaptation [66] [67]~\cite{yamagishi2009robust,yamagishi2009analysis} and voice conversion [68] [69]~\cite{masuko1997voice,wu2006voice}, enabling voice cloning to some extent.
Besides, emotion can also be limitedly controlled by some of these methods~\cite{yamagishi2003modeling,[Yamagishi et al. (2005) [71]](../../Models/_Early/Acoustic_Modeling_of_Speaking_Styles_and_Emotional_Expressions_in_HMM-Based_Speech_Synthesis.md),nose2007style, [Lorenzo et al. (2015) [72]](../../Models/_Early/Emotion_Transplantation_through_Adaptation_in_HMM-Based_Speech_Synthesis.md)} [70].
In addition, they required less storage compared to concatenative TTS and allowed smoother transitions between speech units.

</td><td>

在深度神经网络盛行之前, 可控 TTS 技术主要建立在基于规则, 拼接, 统计方法之上.
这些方法实现了一定程度上的定制和控制, 但它们受到底层模型和可用计算资源的局限性约束.
1. 基于规则的 TTS 系统 (例如共振峰合成, 几乎是最早的语音生成方法)
  - 这些系统使用手工制作的规则来模拟语音生成过程, 通过控制声学参数 (如音高, 时长, 共振峰频率), 允许通过规则调整来实现对韵律和音素细节的显式操作.
2. 拼接式 TTS (1990 年代末到 2000 年代初占据主导地位, 通过预录制的语音片段进行拼接, 如音素或双音节, 存储在大型数据库中)
   - 这些方法可以在拼接时通过操作语音片段的音高, 时长, 幅度来修改韵律.
   - 它们还允许通过选择不同发音人的语音单元来实现有限的语音自定义.
3. 参数方法, 特别是基于 HMM 的 TTS (2000 年代末突出)
   - 这些系统建模语言特征和声学参数之间的关系, 通过调整统计参数提供了在控制韵律, 音高, 说话速率, 音色方面更大的灵活性.
   - 一些基于 HMM 的系统也支持说话人适应和声音转换, 在一定程度上实现了语音克隆.
   - 除此之外, 其中一些方法还可以有限地控制情感.
   - 另外, 它们比起拼接式 TTS 所需存储更少, 且能在语音单元之间实现更平滑的过渡.

</td></tr></table>

#### Neural-Based Synthesis: 基于神经网络合成

<table><tr><td width="50%">

Neural-based TTS technologies emerged with the advent of deep learning, significantly advancing the field by enabling more flexible, natural, and expressive speech synthesis.
Unlike traditional methods, neural-based TTS leverages DNNs to model complex relationships between input text and speech, facilitating nuanced control over various speech characteristics.
Early neural TTS systems, such as [WaveNet [73]](../../Models/Vocoder/2016.09.12_WaveNet.md) and [Tacotron [74]](../../Models/Acoustic/2017.03.29_Tacotron.md) laid the groundwork for controllability.
1) Controlling prosody features like rhythm and intonation is vital for generating expressive and contextually appropriate speech.
Neural-based TTS models achieve prosody control through explicit conditioning or learned latent representations~\cite{shen2018tacotron2,[FastSpeech [15]](../../Models/Acoustic/2019.05.22_FastSpeech.md),ren2020fastspeech2,lancucki2021fastpitch,[MaskGCT [78]](../../Models/SpeechLM/ST2S/2024.09.01_MaskGCT.md)} [75] [76] [77].
1) Speaker control has also gained significant improvement in neural-based TTS through speaker embeddings or adaptation techniques [79] [80] [81] [82]~\cite{fan2015multi,huang2022meta,chen2020multispeech,casanova2022yourtts}.
2) Besides, emotionally controllable TTS [83]~\cite{lei2022msemotts,[Emo-DPO [31]](../../Modules/RLHF/2024.09.16_Emo-DPO.md); [I2I [20]](../../Models/Style/2019.11.05_I2I.md); [iEmoTTS [22]](../../Models/Style/2022.06.29_iEmoTTS.md); [T5-TTS [32]](../../Models/SpeechLM/2024.06.25_T5-TTS.md)} has become a hot topic due to the strong modeling capability of DNNs, enabling the synthesis of speech with specific emotional tones such as happiness, sadness, anger, or neutrality.
These systems go beyond producing intelligible and natural-sounding speech, focusing on generating expressive output that aligns with the intended emotional context.
1) Neural-based TTS can also manipulate timbre (vocal quality)~\cite{[MaskGCT [78]](../../Models/SpeechLM/ST2S/2024.09.01_MaskGCT.md); elias2021paralleltacotron,wang2023neural,[NaturalSpeech [14]](../../Models/E2E/2022.05.09_NaturalSpeech.md),shen2023naturalspeech2,[NaturalSpeech3 [87]](../../Models/Diffusion/2024.03.05_NaturalSpeech3.md)} [84] [85] [86] and style (speech mannerisms)~\cite{li2022styletts,li2024styletts2,huang2022generspeech} [88] [89] [90], allowing for creative and personalized applications.
These techniques lead to one of the most popular research topics, i.e., zero-shot TTS (particularly voice cloning)~\cite{casanova2022yourtts,[MaskGCT [78]](../../Models/SpeechLM/ST2S/2024.09.01_MaskGCT.md),jiang2023megavoic,cooper2020zero} [91] [92].
1) Fine-grained content and linguistic control also become more powerful~\cite{peng2024voicecraft,tan2021editspeech,tae2021editts,seshadri2021emphasis} [93] [94] [95] [96].
These methods can emphasize or de-emphasize specific words or adjust the pronunciation of phonemes through speech editing or generation techniques.

Neural-based TTS technologies represent a significant leap in the flexibility and quality of speech synthesis.
From prosody and emotion to speaker identity and style, these systems empower diverse applications in fields such as entertainment, accessibility, and human-computer interaction.

</td><td>

</td></tr></table>

#### LLM-Based Synthesis: 基于大语言模型合成

<table><tr><td width="50%">

Here we pay special attention to LLM-based synthesis methods due to their superior context modeling capabilities compared to other neural-based TTS methods.
LLMs, such as GPT ([GPT-3 [97]](../../Models/TextLM/2020.05.28_GPT-3.md); )~\cite{achiam2023gpt4}, T5~\cite{raffel2020t5} [98] [99], and PaLM [100]~\cite{chowdhery2023palm}, have revolutionized various natural language processing (NLP) tasks with their ability to generate coherent, context-aware text.
Recently, their utility has expanded into controllable TTS technologies ~\cite{[PromptTTS [101]](../../Models/Acoustic/2022.11.22_PromptTTS.md),leng2023prompttts2,[VoxInstruct [103]](../../Models/SpeechLM/2024.08.28_VoxInstruct.md);shimizu2024promptttspp,[CosyVoice [17]](../../Models/SpeechLM/ST2S/2024.07.07_CosyVoice.md)} [102] [104].
For example, users can synthesize the target speech by describing its characteristics, such as: "A young girl says `I really like it, thank you!' with a happy voice", making speech generation significantly more intuitive and user-friendly.
Specifically, an LLM can detect emotional intent in sentences (e.g., "I’m thrilled" → happiness, "This is unfortunate" → sadness).
The detected emotion is encoded as an auxiliary input to the TTS model, enabling modulation of acoustic features like prosody, pitch, and energy to align with the expressed sentiment.
By leveraging LLMs' capabilities in understanding and generating rich contextual information, these systems can achieve enhanced and fine-grained control over various speech attributes such as prosody, emotion, style, and speaker characteristics ([InstructTTS [105]](../../Models/Acoustic/2023.01.31_InstructTTS.md),[Emo-DPO [31]](../../Modules/RLHF/2024.09.16_Emo-DPO.md); [ControlSpeech [106]](../../Models/SpeechLM/2024.06.03_ControlSpeech.md)).
Integrating LLMs into TTS systems represents a significant step forward, enabling more dynamic and expressive speech synthesis.

</td><td>

</td></tr></table>

### C·Organization of This Survey: 本文结构

<table><tr><td width="50%">

This paper first presents a comprehensive and systematic review of controllable TTS technologies, with a particular focus on model architectures, control methodologies, and feature representations.
To establish a foundational understanding, this survey begins with an introduction to the TTS pipeline in [Section 2](Sec.02_Pipeline.md).
While our focus remains on controllable TTS, [Section 3](Sec.03_UnControllableTTS.md) examines seminal works in uncontrollable TTS that have significantly influenced the field's development.
[Section 4](Sec.04_ControllableTTS.md) provides a thorough investigation into controllable TTS methods, analyzing both their model architectures and control strategies.
[Section 5](Sec.05_Datasets&Evaluation.md) presents a comprehensive review of datasets and evaluation metrics.
[Section 6](Sec.06_Challenges.md) provides an in-depth analysis of the challenges encountered in achieving controllable TTS systems and discusses future directions.
[Section 7](#Sec.07) explores the broader impacts of controllable TTS technologies and identifies promising future research directions, followed by the conclusion in [Section 8](#Sec.08).

</td><td>

本文首次展示了一份全面且系统化的可控 TTS 技术综述, 着重分析模型架构, 控制策略, 和特征表示.
- [第二节](Sec.02_Pipeline.md) 介绍 TTS 流程, 以建立起基础的理解.
- [第三节](Sec.03_UnControllableTTS.md) 仔细研究了不可控 TTS, 其对整个研究领域的发展有极大影响.
- [第四节](Sec.04_ControllableTTS.md) 详细调查了可控 TTS 方法, 分析了其模型架构和控制策略.
- [第五节](Sec.05_Datasets&Evaluation.md) 展示了数据集和评价指标的全面综述.
- [第六节](Sec.06_Challenges.md) 深入分析了实现可控 TTS 系统的挑战, 并提出了未来研究方向.
- [第七节](#Sec.07) 探讨了可控 TTS 技术的广泛影响, 并提出了有前途的研究方向.
- [第八节](#Sec.08) 总结了本文.

</td></tr></table>

# 2·TTS Pipeline: 文本转语音流程

<table><tr><td width="50%">

In this section, we elaborate on the general pipeline that supports controllable TTS technologies, including acoustic models, speech vocoders, and feature representations.
Fig.02 depicts the general pipeline of controllable TTS, containing various model architectures and feature representations, but the control strategies will be discussed in [Section 4](Sec.04_ControllableTTS.md).
Readers can jump to [Section 3](Sec.03_UnControllableTTS.md) if familiar with TTS pipelines.

</td><td>

在本节中, 我们详细介绍支持可控 TTS 技术的一般流程, 包括声学模型, 语音声码器和特征表示.

图 02 展示了可控 TTS 的一般流程, 包含各种模型架构和特征表示, 但控制策略将在 [第 4 节](Sec.04_ControllableTTS.md) 中讨论.

如果读者熟悉 TTS 流程, 可以直接跳转到 [第 3 节](Sec.03_UnControllableTTS.md).

</td></tr>
<tr><td colspan="2">

![../Images/Fig.02.png](../Images/S20241209_Fig.02.png)

</td></tr></table>

## A·Overview: 总览

<table><tr><td width="50%">

A TTS pipeline generally contains three key components, i.e., linguistic analyzer, acoustic model, speech vocoder, and with a conditional input, e.g., prompts, for controllable speech synthesis.
Besides, some end-to-end methods use a single model to encode the input and decode the speech waveforms without generating intermediate features like mel-spectrograms ([Spectrogram [110] [URL]](https://en.wikipedia.org/wiki/Spectrogram)).
- **Linguistic analyzer** aims to extract linguistic features, e.g., phoneme duration and position, syllable stress, and utterance level, from the input text, which is a necessary step in HMM-based methods ([Yoshimura et al.(1999) [64]](../../Models/_Early/Simultaneous_Modeling_of_Spectrum_Pitch_&_Duration_in_HMM-Based_Speech_Synthesis.md); [Tokuda et al. (2000) [65]](../../Models/_Early/Speech_Parameter_Generation_Algorithms_for_HMM-Based_Speech_Synthesis.md)) and a few neural-based methods ([Statistical Parametric Speech Synthesis Using DNNs [111]](../../Models/_Early/2013.05.26_Statistical_Parametric_Speech_Synthesis_Using_DNNs.md); [DBLSTM-RNN [112]](../../Models/_Early/DBLSTM-RNN.md)), but is time-consuming and error-prone.
- **Acoustic model** is a parametric or neural model that predicts the acoustic features from the input texts.
Modern neural-based acoustic models like [Tacotron [74]](../../Models/Acoustic/2017.03.29_Tacotron.md) and later works ([FastSpeech [15]](../../Models/Acoustic/2019.05.22_FastSpeech.md); [FastSpeech2 [76]](../../Models/Acoustic/2020.06.08_FastSpeech2.md); [Diff-TTS [113]](../../Models/Acoustic/2021.04.03_Diff-TTS.md)) directly take character ([CWE[114]](../../Models/SpeechRepresentation/CWE.md)) or word embeddings ([Survey by Almeida et al. (2019) [115]](../S20190125.md)) as the input, which is much more efficient than previous methods.
- **Speech vocoder** is the last component that converts the intermediate acoustic features into a waveform that can be played back.
This step bridges the gap between the acoustic features and the actual sounds produced, helping to generate high-quality, natural-sounding speech ([WaveNet [73]](../../Models/Vocoder/2016.09.12_WaveNet.md); [HiFi-GAN [116]](../../Models/Vocoder/2020.10.12_HiFi-GAN.md)).
[Survey by Tan et al. (2021) [42]](../S20210629.md) have presented a comprehensive and detailed review of acoustic models and vocoders.
Therefore, the following subsections will briefly introduce some representative acoustic models and speech vocoders, followed by a discussion of acoustic feature representations.

</td><td>

文本转语音流程通常包含三个关键组件, 即语言分析器, 声学模型, 语音声码器, 以及条件化输入 (如用于可控语音合成的提示).
除此之外, 一些端到端的模型使用单个模型来编码输入并解码出语音波形, 而无需生成中间特征 (如梅尔频谱图 [Spectrogram [110] [URL]](https://en.wikipedia.org/wiki/Spectrogram)).
- **语言分析器 (Linguistic Analyzer)** 的目的是从输入文本中提取语言学特征 (如音素时长和位置, 音节重音, 句子级别等), 这是基于 HMM 的方法 ([Yoshimura et al.(1999) [64]](../../Models/_Early/Simultaneous_Modeling_of_Spectrum_Pitch_&_Duration_in_HMM-Based_Speech_Synthesis.md); [Tokuda et al. (2000) [65]](../../Models/_Early/Speech_Parameter_Generation_Algorithms_for_HMM-Based_Speech_Synthesis.md)) 和一些神经网络方法 ([Statistical Parametric Speech Synthesis Using DNNs [111]](../../Models/_Early/2013.05.26_Statistical_Parametric_Speech_Synthesis_Using_DNNs.md); [DBLSTM-RNN [112]](../../Models/_Early/DBLSTM-RNN.md)) 的必要步骤, 但耗时且容易出错.
- **声学模型 (Acoustic Model)** 是参数化或神经模型, 从输入文本预测声学特征.
现代基于神经网络的声学模型如 [Tacotron [74]](../../Models/Acoustic/2017.03.29_Tacotron.md) 及后续工作 ([FastSpeech [15]](../../Models/Acoustic/2019.05.22_FastSpeech.md); [FastSpeech2 [76]](../../Models/Acoustic/2020.06.08_FastSpeech2.md); [Diff-TTS [113]](../../Models/Acoustic/2021.04.03_Diff-TTS.md)) 直接采用字符 ([CWE[114]](../../Models/SpeechRepresentation/CWE.md)) 或词嵌入 ([Survey by Almeida et al. (2019) [115]](../S20190125.md)) 作为输入, 这比以前的方法要更高效.
- **语音声码器 (Speech Vocoder)** 是将中间声学特征转换为可以播放的波形的最后一个组件.
这一步弥合声学特征和实际发出的声音之间的差距, 以生成高质量, 听感自然的语音 ([WaveNet [73]](../../Models/Vocoder/2016.09.12_WaveNet.md); [HiFi-GAN [116]](../../Models/Vocoder/2020.10.12_HiFi-GAN.md)).

[Survey by Tan et al. (2021) [42]](../S20210629.md) 展示了关于声学模型和声码器的全面且详细的综述.
因此, 下面的章节将简要介绍一些代表性的声学模型和语音声码器, 并对声学特征表示进行讨论.

</td></tr></table>

## B·Acoustic Models: 声学模型

<table><tr><td width="50%">

Acoustic modeling is a crucial step in TTS because it ensures the generated acoustic features capture the subtleties of human speech.
By accurately modeling acoustic features, modern TTS systems can help generate high-quality and expressive audio that sounds close to human speech.

</td><td>

声学建模是 TTS 中的关键一步, 因为它确保生成的声学特征能够捕获人类语音的微妙之处.
通过精确建模声学特征, 现代 TTS 系统能够生成高质量且富有表现力的音频, 听起来和人类语音接近.

</td></tr></table>

### Parametric Models: 参数模型

<table><tr><td width="50%">

Early acoustic models rely on parametric approaches, where predefined rules and mathematical functions are utilized to model speech generation.
These models often utilize HMMs to capture acoustic features from linguistic input and generate acoustic features by parameterizing the vocal tract and its physiological properties such as pitch and prosody ([Tokuda et al. (2000) [65] [117]](../../Models/_Early/Speech_Parameter_Generation_Algorithms_for_HMM-Based_Speech_Synthesis.md); [Yamagishi et al. (2005) [71]](../../Models/_Early/Acoustic_Modeling_of_Speaking_Styles_and_Emotional_Expressions_in_HMM-Based_Speech_Synthesis.md); [Lorenzo et al. (2015) [72]](../../Models/_Early/Emotion_Transplantation_through_Adaptation_in_HMM-Based_Speech_Synthesis.md); [Zen et al. (2007) [118]](../../Models/_Early/The_HMM-based_Speech_Synthesis_System_(HTS)_Version_2.0.md); [Nose et al. (2012) [119]](../../Models/_Early/An_Intuitive_Style_Control_Technique_in_HMM-Based_Expressive_Speech_Synthesis_Using_Subjective_Style_Intensity_and_Multiple-Regression_Global_Variance_Model.md); [Nishigaki et al. (2015) [120]](../../Models/_Early/Prosody-Controllable_HMM-Based_Speech_Synthesis_Using_Speech_Input.md)).
These methods have relatively low computational costs and can produce a range of voices by adjusting model parameters.
However, the speech quality of these methods is robotic and lacks natural intonation, and the expressiveness is also limited ([Lorenzo et al. (2015) [72]](../../Models/_Early/Emotion_Transplantation_through_Adaptation_in_HMM-Based_Speech_Synthesis.md); [Nishigaki et al. (2015) [120]](../../Models/_Early/Prosody-Controllable_HMM-Based_Speech_Synthesis_Using_Speech_Input.md)).

</td><td>

早期声学模型依赖参数化方法, 使用预定义规则和数学函数来建模语音生成.
这些模型通常使用隐马尔可夫模型来从语言输入中捕获声学特征, 并通过参数化声带及其生理特性 (如音高和语调) 来生成声学特征.
- [Tokuda et al. (2000) [65] [117]](../../Models/_Early/Speech_Parameter_Generation_Algorithms_for_HMM-Based_Speech_Synthesis.md);
- [Yamagishi et al. (2005) [71]](../../Models/_Early/Acoustic_Modeling_of_Speaking_Styles_and_Emotional_Expressions_in_HMM-Based_Speech_Synthesis.md);
- [Lorenzo et al. (2015) [72]](../../Models/_Early/Emotion_Transplantation_through_Adaptation_in_HMM-Based_Speech_Synthesis.md);
- [Zen et al. (2007) [118]](../../Models/_Early/The_HMM-based_Speech_Synthesis_System_(HTS)_Version_2.0.md);
- [Nose et al. (2012) [119]](../../Models/_Early/An_Intuitive_Style_Control_Technique_in_HMM-Based_Expressive_Speech_Synthesis_Using_Subjective_Style_Intensity_and_Multiple-Regression_Global_Variance_Model.md);
- [Nishigaki et al. (2015) [120]](../../Models/_Early/Prosody-Controllable_HMM-Based_Speech_Synthesis_Using_Speech_Input.md).

这些方法拥有相当低的计算成本, 别难过且可以通过调整模型参数来产生广泛的声音.
然而, 这些方法的语音质量是机械的, 缺乏自然的语调, 表达力也有限.

- [Lorenzo et al. (2015) [72]](../../Models/_Early/Emotion_Transplantation_through_Adaptation_in_HMM-Based_Speech_Synthesis.md);
- [Nishigaki et al. (2015) [120]](../../Models/_Early/Prosody-Controllable_HMM-Based_Speech_Synthesis_Using_Speech_Input.md).

</td></tr></table>

### RNN-Based Models: 基于 RNN 的模型

<table><tr><td width="50%">

Recurrent Neural Networks (RNNs) proved particularly effective in early neural-based TTS due to their ability to model sequential data and long-range dependencies, which helps in capturing the sequential nature of speech, such as the duration and natural flow of phonemes.
Typically, these models have an encoder-decoder architecture, where an encoder encodes input linguistic features, such as phonemes or text, into a fixed-dimensional representation, and the decoder sequentially decodes this representation into acoustic features (e.g., mel-spectrogram frames) that capture the frequency and amplitude of sound over time.
[Tacotron2 [75]](../../Models/Acoustic/2017.12.16_Tacotron2.md) is one of the pioneering TTS models that uses RNNs with an attention mechanism, which helps align the text sequence with the generated acoustic features.
It takes raw characters as input and produces mel-spectrogram frames, which are subsequently converted to waveforms.
Another example is [MelNet [121]](../../Models/Acoustic/2019.06.04_MelNet.md), which leverages autoregressive modeling to generate high-quality mel-spectrograms, demonstrating versatility in generating both speech and music, achieving high fidelity and coherence across temporal scales.

</td><td>

循环神经网络在早期基于神经网络的 TTS 中特别有效, 这是因为它们建模序列数据和长期依赖的能力, 有助于捕获语音的序列性质, 例如时长和音素的自然流动.
通常这些模型具有编码器-解码器架构, 编码器将输入语言特征 (如音素或文本) 编码为固定维度的表示, 解码器顺序地将这一表示解码为声学特征 (如梅尔频谱帧), 这些特征捕获声音在时间上的频率和幅度.

- [Tacotron2 [75]](../../Models/Acoustic/2017.12.16_Tacotron2.md) 是一项开创性的 TTS 模型, 它使用带有注意力机制的 RNNs, 这有助于对齐文本序列和生成的声学特征.
它将原始字符作为输入并生成梅尔频谱帧, 随后将其转换为波形.
- [MelNet [121]](../../Models/Acoustic/2019.06.04_MelNet.md) 利用自回归建模来生成高质量梅尔频谱, 展示了生成语音和音乐的多样性, 达到高保真度和时间尺度上的连贯性.

</td></tr></table>

### CNN-Based Models: 基于 CNN 的模型

<table><tr><td width="50%">

Unlike RNNs, which process sequential data frame by frame, CNNs process the entire sequence at once by applying filters across the input texts.
This parallel approach enables faster training and inference, making CNN-based TTS particularly appealing for real-time and low-latency applications.
Furthermore, by stacking multiple convolutional layers with varying kernel sizes or dilation rates, CNNs can capture both short-range and long-range dependencies, which are essential for natural-sounding speech synthesis.
[Deep Voice [16] [122]](../../Models/System/2017.02.25_DeepVoice.md) is one of the first prominent CNN-based TTS models by Baidu, designed to generate mel-spectrograms directly from phoneme or character input.
[ParaNet [123]](../../Models/Acoustic/2019.05.21_ParaNet.md) also utilizes a RNN model to achieve sequence-to-sequence mel-spectrogram generation.
It uses a non-autoregressive architecture, which enables significantly faster inference by predicting multiple time steps simultaneously.

</td><td>

和 RNN 逐帧处理序列数据不同, CNN 通过对输入文本应用滤波器来一次性处理整个序列.
这种并行方法的训练和推理速度更快, 使得基于 CNN 的 TTS 特别适合实时和低延迟应用.
此外, 通过堆叠多个具有不同卷积核大小或膨胀率的卷积层, CNN 可以捕获短期和长期依赖, 对于自然听感语音合成至关重要.
- [Deep Voice [16] [122]](../../Models/System/2017.02.25_DeepVoice.md) 是百度提出的首个卓越的基于 CNN 的 TTS 模型, 它直接从音素或字符输入生成梅尔频谱图.
- [ParaNet [123]](../../Models/Acoustic/2019.05.21_ParaNet.md) 也使用 CNN 模型来实现序列到序列的梅尔频谱图生成.
它使用非自回归架构, 这使得多步预测可以显著加快推理速度.

</td></tr></table>

### Transformer-Based Models: 基于 Transformer 的模型

<table><tr><td width="50%">

[Transformer model [124]](../../Models/_Transformer/2017.06.12_Transformer.md) uses self-attention layers to capture relationships within the input sequence, making them well-suited for tasks requiring an understanding of global contexts, such as prosody and rhythm in TTS.
Transformer-based TTS models often employ an encoder-decoder architecture, where the encoder processes linguistic information (e.g., phonemes or text) and captures contextual relationships, and the decoder generates acoustic features (like mel-spectrograms) from these encoded representations, later converted to waveforms by a vocoder.
[TransformerTTS [125]](../../Models/Acoustic/2018.09.19_TransformerTTS.md) is one of the first TTS models that apply transformers to synthesize speech from text.
It utilizes a standard encoder-decoder transformer architecture and relies on multi-head self-attention mechanisms to model long-term dependencies, which helps maintain consistency and natural flow in speech over long utterances.
[FastSpeech [15]](../../Models/Acoustic/2019.05.22_FastSpeech.md) is a non-autoregressive model designed to overcome the limitations of autoregressive transformers in TTS, achieving faster synthesis than previous methods.
It introduces a length regulator to align text with output frames, enabling the control of phoneme duration.
[FastSpeech2 [76]](../../Models/Acoustic/2020.06.08_FastSpeech2.md) extends FastSpeech by adding pitch, duration, and energy predictors, resulting in more expressive and natural-sounding speech.

</td><td>

[Transformer [124]](../../Models/_Transformer/2017.06.12_Transformer.md) 模型使用自注意力层来捕获输入序列内的联系, 适合需要理解全局上下文的任务, 例如 TTS 中的韵律和节奏.
基于 Transformer 的 TTS 模型通常采用编码器-解码器架构, 编码器处理语言信息 (如音素或文本) 并捕获上下文关系, 解码器生成声学特征 (如梅尔频谱图), 随后由声码器转换为波形.
- [TransformerTTS [125]](../../Models/Acoustic/2018.09.19_TransformerTTS.md) 是首先使用 Transformer 从文本合成语音的 TTS 模型之一, 它采用标准的编码器-解码器 Transformer 架构, 并依赖多头自注意力机制来建模长期依赖, 这有助于保持语音的一致性和自然流动.
- [FastSpeech [15]](../../Models/Acoustic/2019.05.22_FastSpeech.md) 是一种非自回归模型, 旨在克服自回归 Transformer 在 TTS 中的限制, 实现更快的语音合成.
它引入长度调节器来对齐文本和输出帧, 使得音素时长的控制成为可能.
- [FastSpeech2 [76]](../../Models/Acoustic/2020.06.08_FastSpeech2.md) 扩展了 FastSpeech, 增加了音高, 时长, 能量预测器, 产生更富有表现力和听感自然的语音.

</td></tr></table>

### LLM-Based Models: 基于大语言模型的模型

<table><tr><td width="50%">

LLMs ([BERT [126]](../../Models/TextLM/2018.10.11_BERT.md); [GPT-3 [97]](../../Models/TextLM/2020.05.28_GPT-3.md); [LLaMA [11]](../../Models/TextLM/2023.02.27_LLaMA.md); [Mistral [26]](../../Models/TextLM/2023.10.10_Mistral-7B.md)), known for their large-scale pre-training on text data, have shown remarkable capabilities in natural language understanding and generation.
LLM-based TTS models generally use a text description to guide the mel-spectrogram generation, where the acoustic model processes the input text to generate acoustic tokens that capture linguistic and contextual information, such as tone, sentiment, and prosody.
For example, [PromptTTS [101]](../../Models/Acoustic/2022.11.22_PromptTTS.md) uses a textual prompt encoded by [BERT [126]](../../Models/TextLM/2018.10.11_BERT.md) to guide the acoustic model on the timbre, tone, emotion, and prosody desired in the speech output.
PromptTTS first generates mel-spectrograms with token embeddings and then converts them to audio using a vocoder.
[InstructTTS [105]](../../Models/Acoustic/2023.01.31_InstructTTS.md) generates expressive and controllable speech using natural language style prompts.
It leverages discrete latent representations of speech and integrates natural language descriptions to guide the synthesis process, which bridges the gap between TTS systems and natural language interfaces, enabling fine-grained style control through intuitive prompts.

</td><td>

大语言模型 ([BERT [126]](../../Models/TextLM/2018.10.11_BERT.md); [GPT-3 [97]](../../Models/TextLM/2020.05.28_GPT-3.md); [LLaMA [11]](../../Models/TextLM/2023.02.27_LLaMA.md); [Mistral [26]](../../Models/TextLM/2023.10.10_Mistral-7B.md)) 以它们在文本数据上的大规模预训练而著称, 在自然语言理解和生成方面展现出了惊人的能力.
基于大语言模型的 TTS 模型通常使用文本描述来引导梅尔频谱生成, 其中声学模型处理输入文本来生成声学 Token 以捕获语言和上下文信息, 例如声调, 感情, 以及韵律.
- [PromptTTS [101]](../../Models/Acoustic/2022.11.22_PromptTTS.md) 使用 [BERT [126]](../../Models/TextLM/2018.10.11_BERT.md) 编码的文本提示来引导声学模型生成具有特定音色, 声调, 情感, 以及韵律的语音输出.
PromptTTS 首先使用 Token 嵌入生成梅尔频谱, 随后使用声码器将其转换为音频.
- [InstructTTS [105]](../../Models/Acoustic/2023.01.31_InstructTTS.md) 使用自然语言风格提示来生成具有丰富表现力和可控性的语音.
它利用语音的离散潜在表示, 并集成自然语言描述以引导合成过程, 这将 TTS 系统与自然语言接口之间的差距缩小, 使得通过直观的提示实现细粒度的风格控制成为可能.

</td></tr></table>

### Other Acoustic Models: 其他声学模型

<table><tr><td width="50%">

In TTS, GANs ([Multi-SpectroGAN [127]](../../Models/Acoustic/2020.12.14_Multi-SpectroGAN.md); [Ma et al. (2018) [128]](../../Models/Acoustic/TTS-GAN.md); [Guo et al. (2019) [129]](../../Models/Acoustic/2019.04.09_A_New_GAN-Based_End-to-End_TTS_Training_Algorithm.md)), VAEs ([Zhang et al. [18]](../../Models/Acoustic/2018.12.11_Learning_Latent_Representations_for_Style_Control_and_Transfer_in_End-to-End_Speech_Synthesis.md); [GMVAE-Tacotron [130]](../../Models/Acoustic/2018.10.16_GMVAE-Tacotron.md)), and diffusion models ([Diff-TTS [113]](../../Models/Acoustic/2021.04.03_Diff-TTS.md); [Grad-TTS [131]](../../Models/Acoustic/2021.05.13_Grad-TTS.md)) can also be used as acoustic models.
Flow-based methods ([Flow-TTS [132]](../../Models/Acoustic/2020.04.09_Flow-TTS.md); [Glow-TTS [133]](../../Models/Acoustic/2020.05.22_Glow-TTS.md)) are also popular in waveform generation.
Refer to the survey paper from [Survey by Tan et al. (2021) [42]](../S20210629.md) for more details.

The choice of an acoustic model depends on the specific requirements and is a trade-off between synthesis quality, computational efficiency, and flexibility.
For real-time applications, CNN-based or lightweight transformer-based models are preferable, while for high-fidelity, expressive speech synthesis, transformer-based and LLM-based models are better suited.

</td><td>

在 TTS 中, 还有一些其他声学模型:
- 基于 GAN:
  - [Multi-SpectroGAN [127]](../../Models/Acoustic/2020.12.14_Multi-SpectroGAN.md);
  - [Ma et al. (2018) [128]](../../Models/Acoustic/TTS-GAN.md);
  - [Guo et al. (2019) [129]](../../Models/Acoustic/2019.04.09_A_New_GAN-Based_End-to-End_TTS_Training_Algorithm.md);
- 基于 VAE:
  - [Zhang et al. [18]](../../Models/Acoustic/2018.12.11_Learning_Latent_Representations_for_Style_Control_and_Transfer_in_End-to-End_Speech_Synthesis.md);
  - [GMVAE-Tacotron [130]](../../Models/Acoustic/2018.10.16_GMVAE-Tacotron.md);
- 基于扩散:
  - [Diff-TTS [113]](../../Models/Acoustic/2021.04.03_Diff-TTS.md);
  - [Grad-TTS [131]](../../Models/Acoustic/2021.05.13_Grad-TTS.md);
- 基于流:
  - [Flow-TTS [132]](../../Models/Acoustic/2020.04.09_Flow-TTS.md);
  - [Glow-TTS [133]](../../Models/Acoustic/2020.05.22_Glow-TTS.md);

可以从 [Survey by Tan et al. (2021) [42]](../S20210629.md) 获得更多细节.

声学模型的选择取决于具体需求, 是合成质量, 计算效率和灵活性之间的权衡.
- 对于实时应用, 基于 CNN 或轻量 Transformer 的模型更适合;
- 对于高保真, 表达性语音合成, 基于 Transformer 和 LLM 的模型更适合.

</td></tr></table>

## C·Speech Vocoders: 语音声码器

<table><tr><td width="50%">

Vocoders are essential for converting acoustic features such as mel-spectrograms into intelligible audio waveforms and are vital in determining the naturalness and quality of synthesized speech.
We broadly categorize existing vocoders according to their model architectures, i.e., RNN-, CNN-, GAN-, and diffusion-based vocoders.

</td><td>

声码器是将声学特征 (如梅尔频谱图) 转换为可理解的音频波形的必要组件, 它们在确定合成语音的自然度和质量方面扮演着重要角色.
我们将现有的声码器根据其模型架构大致分为以下几类: 基于 RNN, CNN, GAN, 和扩散模型的声码器.

</td></tr></table>

### RNN-Based Vocoders: 基于 RNN 的声码器

<table><tr><td width="50%">

Unlike traditional vocoders ([STRAIGHT [134]](../../Models/Vocoder/2006_STRAIGHT.md); [WORLD [135]](../../Models/Vocoder/2015.11.11_WORLD.md)) that depend on manually designed signal processing pipelines, RNN-based vocoders ([SampleRNN [136]](../../Models/Vocoder/2016.12.22_SampleRNN.md); [WaveRNN [137]](../../Models/Vocoder/2018.02.23_WaveRNN.md); [LPCNet [138]](../../Models/Vocoder/2018.10.28_LPCNet.md); [Multi-Band WaveRNN [139]](../../Models/Vocoder/2019.09.04_Multi-Band_WaveRNN.md)) leverage the temporal modeling capabilities of RNNs to directly learn the complex patterns in speech signals, enabling the synthesis of natural-sounding waveforms with improved prosody and temporal coherence.
For instance, [WaveRNN [137]](../../Models/Vocoder/2018.02.23_WaveRNN.md) generates speech waveforms sample-by-sample using a single-layer recurrent neural network, typically with Gated Recurrent Units (GRU).
It improves upon earlier neural vocoders like [WaveNet [73]](../../Models/Vocoder/2016.09.12_WaveNet.md) by significantly reducing the computational requirements without sacrificing audio quality.
[MB-WaveRNN [139]](../../Models/Vocoder/2019.09.04_Multi-Band_WaveRNN.md) extends WaveRNN by incorporating a multi-band decomposition strategy, where the speech waveform is divided into multiple sub-bands, with each sub-band synthesized at a lower sampling rate.
These sub-bands are then combined to reconstruct the full-band waveform, thereby accelerating the synthesis process while preserving audio quality.

</td><td>

和传统声码器 ([STRAIGHT [134]](../../Models/Vocoder/2006_STRAIGHT.md); [WORLD [135]](../../Models/Vocoder/2015.11.11_WORLD.md)) 依赖于手动设计的信号处理流程不同, 基于 RNN 的声码器 ([SampleRNN [136]](../../Models/Vocoder/2016.12.22_SampleRNN.md); [WaveRNN [137]](../../Models/Vocoder/2018.02.23_WaveRNN.md); [LPCNet [138]](../../Models/Vocoder/2018.10.28_LPCNet.md); [Multi-Band WaveRNN [139]](../../Models/Vocoder/2019.09.04_Multi-Band_WaveRNN.md)) 利用 RNN 的时序建模能力来直接学习语音信号中的复杂模式, 能够合成具有改善韵律和时序连贯性的自然听感波形.
- [WaveRNN [137]](../../Models/Vocoder/2018.02.23_WaveRNN.md) 使用单层循环神经网络, 通常带有门控循环单元 (GRU) 来逐个样本地生成语音波形.
它在更早的神经声码器如 [WaveNet [73]](../../Models/Vocoder/2016.09.12_WaveNet.md) 之上, 显著降低了计算需求, 但不牺牲了音频质量.
- [MB-WaveRNN [139]](../../Models/Vocoder/2019.09.04_Multi-Band_WaveRNN.md) 扩展了 WaveRNN, 采用多带分解策略, 将语音波形分解为多个子带, 其中每个子带以较低的采样率合成.
这些子带随后被合并以重构完整带宽波形, 加快合成过程, 同时保持音频质量.

</td></tr></table>

### CNN-Based Vocoders: 基于 CNN 的声码器

<table><tr><td width="50%">

By leveraging the parallel nature of convolutional operations, CNN-based vocoders ([WaveNet [73]](../../Models/Vocoder/2016.09.12_WaveNet.md); [Parallel WaveNet [140]](../../Models/Vocoder/2017.11.28_Parallel_WaveNet.md); [FFTNet [141]](../../Models/Vocoder/2018.04.15_FFTNet.md)) can generate high-quality speech more efficiently, making them ideal for real-time applications.
A key strength of CNN-based vocoders is their ability to balance synthesis quality and efficiency.
However, they often require extensive training data and careful hyperparameter tuning to achieve optimal performance.
[WaveNet [73]](../../Models/Vocoder/2016.09.12_WaveNet.md) is a probabilistic autoregressive model that generates waveforms sample by sample conditioned on all preceding samples and auxiliary inputs, such as linguistic features and mel-spectrograms.
It employs stacks of dilated causal convolutions, enabling long-range dependence modeling in speech signals without relying on recurrent connections.
[Parallel WaveNet [140]](../../Models/Vocoder/2017.11.28_Parallel_WaveNet.md) addresses WaveNet's inference speed limitations while maintaining comparable synthesis quality.
It introduces a non-autoregressive mechanism based on a teacher-student framework, where the original WaveNet (teacher) distills knowledge into a student model.
The student generates samples in parallel, enabling real-time synthesis without waveform quality degradation.

</td><td>

通过利用卷积操作的并行性, 基于 CNN 的声码器 ([WaveNet [73]](../../Models/Vocoder/2016.09.12_WaveNet.md); [Parallel WaveNet [140]](../../Models/Vocoder/2017.11.28_Parallel_WaveNet.md); [FFTNet [141]](../../Models/Vocoder/2018.04.15_FFTNet.md)) 能够更高效地生成高质量语音, 适用于实时应用.
基于 CNN 的声码器的关键长处是它们能够平衡合成质量和效率.
然而, 它们往往需要大量训练数据和仔细的超参数调整才能达到最佳性能.
- [WaveNet [73]](../../Models/Vocoder/2016.09.12_WaveNet.md) 是一种概率自回归模型, 基于所有先前样本和辅助输入作为条件逐个样本点生成波形, 如语言特征和梅尔频谱图.
它采用了膨胀因果卷积堆叠, 能够不依赖于循环连接进行语音信号的长范围依赖性的建模.
- [Parallel WaveNet [140]](../../Models/Vocoder/2017.11.28_Parallel_WaveNet.md) 解决了 WaveNet 的推理速度限制且保持了相当的合成质量.
它基于教师-学生框架引入了非自回归机制, 其中原始 WaveNet 作为教师模型蒸馏知识到学生模型中.
学生模型并行地生成样本点, 实现实时合成而无波形质量退化.

</td></tr></table>

### GAN-Based Vocoders: 基于 GAN 的声码器

<table><tr><td width="50%">

GANs have been widely adopted in vocoders for high-quality speech generation ([WaveGAN [142]](../../Models/Vocoder/2018.02.12_WaveGAN.md); [GAN-TTS [143]](../../Models/Vocoder/2019.09.25_GAN-TTS.md); [HiFi-GAN [116]](../../Models/Vocoder/2020.10.12_HiFi-GAN.md); [Parallel WaveGAN [144]](../../Models/Vocoder/2019.10.25_Parallel_WaveGAN.md); [MelGAN [145]](../../Models/Vocoder/2019.10.08_MelGAN.md)), leveraging adversarial losses to improve realism.
GAN-based vocoders typically consist of a generator that produces waveforms conditioned on acoustic features, such as mel-spectrograms, and a discriminator that distinguishes between real and synthesized waveforms.
Models like [Parallel WaveGAN [144]](../../Models/Vocoder/2019.10.25_Parallel_WaveGAN.md) and [HiFi-GAN [116]](../../Models/Vocoder/2020.10.12_HiFi-GAN.md) have demonstrated the effectiveness of GANs in vocoding by introducing tailored loss functions, such as multi-scale and multi-resolution spectrogram losses, to ensure naturalness in both time and frequency domains.
These models can efficiently handle the complex, non-linear relationships inherent in speech signals, resulting in high-quality synthesis.
A key advantage of GAN-based vocoders is their parallel inference capability, enabling real-time synthesis with lower computational costs compared to autoregressive models.
However, training GANs can be challenging due to instability and mode collapse.
Despite these challenges, GAN-based vocoders continue to advance the state-of-the-art in neural vocoding, offering a compelling combination of speed and audio quality.

</td><td>

GAN 被广泛应用于声码器进行高质量语音生成 ([WaveGAN [142]](../../Models/Vocoder/2018.02.12_WaveGAN.md); [GAN-TTS [143]](../../Models/Vocoder/2019.09.25_GAN-TTS.md); [HiFi-GAN [116]](../../Models/Vocoder/2020.10.12_HiFi-GAN.md); [Parallel WaveGAN [144]](../../Models/Vocoder/2019.10.25_Parallel_WaveGAN.md); [MelGAN [145]](../../Models/Vocoder/2019.10.08_MelGAN.md))，利用对抗损失提高真实感.
基于 GAN 的声码器通常由一个以声学特征 (如梅尔频谱图) 为条件来生成波形的生成器, 和一个区分真实波形和合成波形的判别器组成.
模型如 [Parallel WaveGAN [144]](../../Models/Vocoder/2019.10.25_Parallel_WaveGAN.md) 和 [HiFi-GAN [116]](../../Models/Vocoder/2020.10.12_HiFi-GAN.md) 已经证明了 GAN 在声码中的有效性, 通过引入特别设计的损失函数 (如多尺度和多分辨率频谱图损失) 来确保时域和频域的自然性.
这些模型能够有效地处理语音信号内在的复杂非线性关联, 以实现高质量合成.
基于 GAN 的声码器的关键优势是它们的并行推理能力, 能够比自回归模型更低的计算成本来实现实时合成.
然而, 训练 GAN 仍然面临着不稳定和模式崩溃的挑战.
尽管如此, 基于 GAN 的声码器仍然在神经声码领域取得了领先地位, 提供了速度和音频质量之间的强有力组合.

</td></tr></table>

### Diffusion-Based Vocoders: 基于扩散的声码器

<table><tr><td width="50%">

Inspired by [diffusion probabilistic models [146]](../../Models/Diffusion/2020.06.19_DDPM.md) that have shown success in visual generation tasks, diffusion-based vocoders ([FastDiff [147]](../../Models/Vocoder/2022.04.21_FastDiff.md); [DiffWave [148]](../../Models/Vocoder/2020.09.21_DiffWave.md); [WaveGrad [149]](../../Models/Vocoder/2020.09.02_WaveGrad.md); [PriorGrad [150]](../../Models/Vocoder/2021.06.11_PriorGrad.md)) present a novel approach to natural-sounding speech synthesis.
The core mechanism of diffusion-based vocoders involves two stages: a forward process and a reverse process.
In the forward process, clean speech waveforms are progressively corrupted by adding noise in a controlled manner, creating a sequence of intermediate noisy representations.
During training, the model learns to reverse this process, progressively denoising the corrupted signal to reconstruct the original waveform.
Diffusion-based vocoders, such as [WaveGrad [149]](../../Models/Vocoder/2020.09.02_WaveGrad.md) and [DiffWave [148]](../../Models/Vocoder/2020.09.21_DiffWave.md), have demonstrated remarkable performance in generating high-fidelity waveforms while maintaining temporal coherence and natural prosody.
They offer advantages over previous vocoders, including robustness to over-smoothing ([Revisiting Over-Smoothness in Text to Speech [151]](../../Models/_Full/2022.02.26_Revisiting_Over-Smoothness_in_Text_to_Speech.md)) and the ability to model complex data distributions.
However, their iterative sampling process can be computationally intensive, posing challenges for real-time applications.

</td><td>

受到[扩散概率模型 [146]](../../Models/Diffusion/2020.06.19_DDPM.md) 在视觉生成任务中的成功的启发, 基于扩散的声码器 ([FastDiff [147]](../../Models/Vocoder/2022.04.21_FastDiff.md); [DiffWave [148]](../../Models/Vocoder/2020.09.21_DiffWave.md); [WaveGrad [149]](../../Models/Vocoder/2020.09.02_WaveGrad.md); [PriorGrad [150]](../../Models/Vocoder/2021.06.11_PriorGrad.md)) 展示了一种全新的自然语音合成方法.
基于扩散的声码器的核心机制由两阶段组成: 前向过程和逆向过程.
在前向过程中, 干净的语音波形被有控制地添加噪声进行逐步损坏, 形成一系列地中间噪声表示.
在训练时, 模型学习逆向这一过程, 逐步去噪破坏信号来重构原始波形.

基于扩散的声码器, 例如 [WaveGrad [149]](../../Models/Vocoder/2020.09.02_WaveGrad.md) 和 [DiffWave [148]](../../Models/Vocoder/2020.09.21_DiffWave.md), 在生成高保真波形的同时保持时域一致性和自然语调, 展示了卓越的性能.
与之前的声码器相比, 它们提供了一些优势, 包括对过度平滑 ([Revisiting Over-Smoothness in Text to Speech [151]](../../Models/_Full/2022.02.26_Revisiting_Over-Smoothness_in_Text_to_Speech.md)) 的鲁棒性和建模复杂数据分布的能力.
然而, 它们迭代式的采样过程可能计算代价高昂, 对于实时应用来说是个挑战.

</td></tr></table>

### Other Vocoders: 其他声码器

<table><tr><td width="50%">

There are also many other types of vocoders such as flow-based ([P-Flow [152]](../../Models/Flow/P-Flow.md); [VoiceFlow [153]](../../Models/Diffusion/2023.09.10_VoiceFlow.md); [PeriodWave [154]](../../Models/Vocoder/2024.08.14_PeriodWave.md); [WaveFlow [155]](../../Models/Vocoder/2019.12.03_WaveFlow.md); [FloWaveNet [156]](../../Models/Vocoder/2018.11.06_FloWaveNet.md)) and VAE-based vocoders ([ParaNet [157]](../../Models/Acoustic/2019.05.21_ParaNet.md); [MSMC-TTS [158]](../../Models/_tmp/2023.05.02_MSMC-TTS.md); [VITS [159]](../../Models/E2E/2021.06.11_VITS.md)).
These methods provide unique strengths for speech synthesis such as efficiency and greater flexibility in modeling complex speech variations.
Readers can refer to the survey paper from [Tan et al. (2021) [42]](../S20210629.md) for more details.

The choice of vocoder depends on various factors.
While high-quality models like GAN-based and diffusion-based vocoders excel in naturalness, they may not be suitable for real-time scenarios.
On the other hand, models like [Parallel WaveNet [140]](../../Models/Vocoder/2017.11.28_Parallel_WaveNet.md) balance quality and efficiency for practical use cases.
The best choice will ultimately depend on the specific use case, available resources, and the importance of factors such as model size, training data, and inference speed.

</td><td>

还有许多其他类型的声码器, 如
- 基于流:
  - [P-Flow [152]](../../Models/Flow/P-Flow.md);
  - [VoiceFlow [153]](../../Models/Diffusion/2023.09.10_VoiceFlow.md); ?
  - [PeriodWave [154]](../../Models/Vocoder/2024.08.14_PeriodWave.md);
  - [WaveFlow [155]](../../Models/Vocoder/2019.12.03_WaveFlow.md);
  - [FloWaveNet [156]](../../Models/Vocoder/2018.11.06_FloWaveNet.md)
- 基于 VAE:
  - [ParaNet [157]](../../Models/Acoustic/2019.05.21_ParaNet.md);
  - [MSMC-TTS [158]](../../Models/_tmp/2023.05.02_MSMC-TTS.md);
  - [VITS [159]](../../Models/E2E/2021.06.11_VITS.md) ?

这些方法为语音合成提供了独特的优势, 如效率和更强的建模复杂语音变化的灵活性.

读者可以参考 [Tan et al. (2021) [42]](../S20210629.md) 的综述论文获取更多细节.

声码器的选择依赖于各种因素.
虽然基于 GAN 和基于扩散的声码器在自然度方面都取得了卓越的成果, 但它们可能不适用于实时场景.
另一方面, 像 [Parallel WaveNet [140]](../../Models/Vocoder/2017.11.28_Parallel_WaveNet.md) 这样的模型在实际用例中平衡质量和效率.
最好的选择将取决于具体的用例, 可用资源, 以及模型大小, 训练数据, 和推理速度等因素的重要性.

</td></tr></table>

## D·Fully End-to-end TTS models: 完全端到端 TTS 模型

<table><tr><td width="50%">

Fully end-to-end TTS methods ([FastSpeech2s [76]](../../Models/Acoustic/2020.06.08_FastSpeech2.md); [VITS [159]](../../Models/E2E/2021.06.11_VITS.md); [Char2Wav [160]](../../Models/E2E/2017.02.18_Char2Wav.md); [ClariNet [161]](../../Models/E2E/2018.07.19_ClariNet.md); [EATS [162]](../../Models/E2E/2020.06.05_EATS.md)) directly generate speech waveforms from textual input, simplifying the "acoustic model → vocoder" pipeline and achieving efficient speech generation.
[Char2Wav [160]](../../Models/E2E/2017.02.18_Char2Wav.md) is an early neural text-to-speech (TTS) system that directly synthesizes speech waveforms from character-level text input.
It integrates two components and jointly trains them: a recurrent sequence-to-sequence model with attention, which predicts acoustic features (e.g., mel-spectrograms) from text, and a [SampleRNN-based neural vocoder [136]](../../Models/Vocoder/2016.12.22_SampleRNN.md) that generates waveforms from these features.
Similarly, [FastSpeech2s [76]](../../Models/Acoustic/2020.06.08_FastSpeech2.md) directly synthesizes speech waveforms from texts by extending [FastSpeech2 [76]](../../Models/Acoustic/2020.06.08_FastSpeech2.md) with a waveform decoder, achieving high-quality and low-latency synthesis.
[VITS [159]](../../Models/E2E/2021.06.11_VITS.md) is another fully end-to-end TTS framework.
It integrates a [variational autoencoder (VAE) with normalizing flows [163]](../../Models/_Full/2015.05.21_Variational_Inference_with_Normalizing_Flows.md) and adversarial training, enabling the model to learn latent representations that capture the intricate variations in speech, such as prosody and style.
VITS combines non-autoregressive synthesis with stochastic latent variable modeling, achieving real-time waveform generation without compromising naturalness.
There are more end-to-end TTS models such as [Tacotron [74]](../../Models/Acoustic/2017.03.29_Tacotron.md), [ClariNet [161]](../../Models/E2E/2018.07.19_ClariNet.md), and [EATS [162]](../../Models/E2E/2020.06.05_EATS.md), refer to another survey ([Survey by Tan et al. (2021) [42]](../S20210629.md)) for more details.
End-to-end controllable methods that emerged in recent years will be discussed in [Section 4](Sec.04_ControllableTTS.md).

</td><td>

完全端到端的 TTS 方法 ([FastSpeech2s [76]](../../Models/Acoustic/2020.06.08_FastSpeech2.md); [VITS [159]](../../Models/E2E/2021.06.11_VITS.md); [Char2Wav [160]](../../Models/E2E/2017.02.18_Char2Wav.md); [ClariNet [161]](../../Models/E2E/2018.07.19_ClariNet.md); [EATS [162]](../../Models/E2E/2020.06.05_EATS.md)) 从文本输入直接生成语音波形, 简化了 "声学模型 → 声码器" 流程, 并实现了高效的语音生成.
- [Char2Wav [160]](../../Models/E2E/2017.02.18_Char2Wav.md) 是早期的神经文本转语音系统, 直接从字符级文本输入合成语音波形.
它集成了两个组件并进行联合训练: 带注意力的循环序列到序列模型, 从文本预测声学特征 (如梅尔频谱图), 以及基于 [SampleRNN [136]](../../Models/Vocoder/2016.12.22_SampleRNN.md) 的神经声码器, 从这些特征生成波形.
- [FastSpeech2s [76]](../../Models/Acoustic/2020.06.08_FastSpeech2.md) 类似地直接从文本生成语音波形, 通过向 [FastSpeech2 [76]](../../Models/Acoustic/2020.06.08_FastSpeech2.md) 添加一个波形解码器来实现, 达到高质量和低延迟的语音合成.
- [VITS [159]](../../Models/E2E/2021.06.11_VITS.md) 是另一种完全端到端 TTS 框架.
它集成了[带标准化流的变分自编码器 [163]](../../Models/_Full/2015.05.21_Variational_Inference_with_Normalizing_Flows.md) 和对抗训练, 使得模型学习到能够捕获语音中复杂变化 (如声调和风格) 的潜在表示.
VITS 将非自回归合成和随机潜在变量建模相结合, 实现了实时波形生成而不损失自然度.

还有其他一些端到端模型例如 [Tacotron [74]](../../Models/Acoustic/2017.03.29_Tacotron.md), [ClariNet [161]](../../Models/E2E/2018.07.19_ClariNet.md), [EATS [162]](../../Models/E2E/2020.06.05_EATS.md).

参考综述论文 ([Survey by Tan et al. (2021) [42]](../S20210629.md)) 获取更多细节.

近年来出现的端到端的可控方法将在[第 4 节](Sec.04_ControllableTTS.md)中讨论.

</td></tr></table>

## E·Acoustic Feature Representations: 声学特征表示

<table><tr><td width="50%">

In TTS, the choice of acoustic feature representations impacts the model's flexibility, quality, expressiveness, and controllability.
This subsection investigates continuous representations and discrete tokens as shown in Fig.02, along with their pros and cons for TTS applications.

</td><td>

在 TTS 中, 声学特征表示的选择影响着模型的灵活性, 质量, 表现力, 以及可控性.
本小节研究了图 02 中所示的连续表示和离散 Token, 以及它们在 TTS 应用中的优缺点.

</td></tr></table>

### Continuous Representations: 连续表示

<table><tr><td width="50%">

Continuous representations (e.g., mel-spectrograms) of intermediate acoustic features use a continuous feature space to represent speech signals.
These representations often involve acoustic features that capture frequency, pitch, and other characteristics without discretizing the signal.
The advantages of continuous features are:
1) Continuous representations retain fine-grained detail, enabling more expressive and natural-sounding speech synthesis.
2) Since continuous features inherently capture variations in tone, pitch, and emphasis, they are well-suited for prosody control and emotional TTS.
3) Continuous representations are more robust to information loss and can avoid quantization artifacts, allowing smoother, less distorted audio.

GAN-based ([HiFi-GAN [116]](../../Models/Vocoder/2020.10.12_HiFi-GAN.md); [Parallel WaveGAN [144]](../../Models/Vocoder/2019.10.25_Parallel_WaveGAN.md); [MelGAN [145]](../../Models/Vocoder/2019.10.08_MelGAN.md)) and diffusion-based methods ([FastDiff [147]](../../Models/Vocoder/2022.04.21_FastDiff.md); [DiffWave [148]](../../Models/Vocoder/2020.09.21_DiffWave.md)) often utilize continuous feature representations, i.e., mel-spectrograms.
However, continuous representations are typically more computationally demanding and require larger models and memory, especially in high-resolution audio synthesis.

</td><td>

中间声学特征的连续表示 (如梅尔频谱图) 使用连续特征空间来表示语音信号.
这些表示往往涉及到捕捉频率, 音高和其他特性的声学特征而不离散化信号.

连续特征的优点有:
1) 连续表示保留细粒度细节, 允许更富有表现力和听感自然的语音合成.
2) 因为连续表示固有地捕捉音调音高和重音的变化, 它们很适合用于韵律控制和情感 TTS.
3) 连续表示对信息损失更鲁棒, 可以避免量化失真, 能获得更光滑, 更少失真的音频.

基于 GAN ([HiFi-GAN [116]](../../Models/Vocoder/2020.10.12_HiFi-GAN.md); [Parallel WaveGAN [144]](../../Models/Vocoder/2019.10.25_Parallel_WaveGAN.md); [MelGAN [145]](../../Models/Vocoder/2019.10.08_MelGAN.md)) 和基于扩散 ([FastDiff [147]](../../Models/Vocoder/2022.04.21_FastDiff.md); [DiffWave [148]](../../Models/Vocoder/2020.09.21_DiffWave.md)) 的模型通常使用连续特征表示, 即梅尔频谱图.
然而, 连续表示通常需要更多的计算需求和更大的模型和内存, 尤其是在高分辨率语音合成中.

</td></tr></table>

### Discrete Tokens: 离散 Tokens

<table><tr><td width="50%">

In discrete token-based TTS, the intermediate acoustic features (e.g., quantized units or phoneme-like tokens) are discrete values, similar to words or phonemes in languages.
These are often produced using quantization techniques or learned embeddings, such as [HuBERT [166]](../../Models/SpeechRepresentation/2021.06.14_HuBERT.md) and [SoundStream [168]](../../Models/SpeechCodec/2021.07.07_SoundStream.md).
The advantages of discrete tokens are:
1) Discrete tokens can encode phonemes or sub-word units, making them concise and less computationally demanding to handle.
2) Discrete tokens often allow TTS systems to require fewer samples to learn and generalize, as the representations are compact and simplified.
3) Using discrete tokens simplifies cross-modal TTS applications like voice cloning or translation-based TTS, as they map well to text-like representations such as LLM tokens.

LLM-based ([MaskGCT [78]](../../Models/SpeechLM/ST2S/2024.09.01_MaskGCT.md); [VoxInstruct [103]](../../Models/SpeechLM/2024.08.28_VoxInstruct.md); [InstructTTS [105]](../../Models/Acoustic/2023.01.31_InstructTTS.md); [ControlSpeech [106]](../../Models/SpeechLM/2024.06.03_ControlSpeech.md)) and zero-shot TTS methods ([CosyVoice [17]](../../Models/SpeechLM/ST2S/2024.07.07_CosyVoice.md); [MaskGCT [78]](../../Models/SpeechLM/ST2S/2024.09.01_MaskGCT.md); [NaturalSpeech3 [87]](../../Models/Diffusion/2024.03.05_NaturalSpeech3.md)) often adopt discrete tokens as their acoustic features.
However, discrete representation learning may result in information loss or lack the nuanced details that can be captured in continuous representations.

Table.04 and Table.03 summarize the types of acoustic features of representative methods.
Table.02 summarizes popular open-source speech quantization methods.

</td><td>

在基于离散 Token 的 TTS 中, 中间声学特征 (如量化单元或类似音素的 Token) 是离散值, 类似于语言中的词或音素.
它们通常使用量化技术或学习到的嵌入来产生, 如 [HuBERT [166]](../../Models/SpeechRepresentation/2021.06.14_HuBERT.md) 和 [SoundStream [168]](../../Models/SpeechCodec/2021.07.07_SoundStream.md).
离散 Token 的优点有:
1) 离散 Token 可以编码音素或子词单元, 使得它们更简洁, 并降低处理的计算需求.
2) 离散 Token 往往允许 TTS 系统学习和泛化所需的样本更少, 因为表示是紧凑和简化的.
3) 使用离散 Token 简化了跨模态 TTS 应用, 如声音克隆或基于翻译的 TTS, 因为它们可以很好地映射到类似于文本的表示 (如 LLM Token).

基于 LLM 的方法 ([MaskGCT [78]](../../Models/SpeechLM/ST2S/2024.09.01_MaskGCT.md); [VoxInstruct [103]](../../Models/SpeechLM/2024.08.28_VoxInstruct.md); [InstructTTS [105]](../../Models/Acoustic/2023.01.31_InstructTTS.md); [ControlSpeech [106]](../../Models/SpeechLM/2024.06.03_ControlSpeech.md)) 和零样本 TTS 方法 ([CosyVoice [17]](../../Models/SpeechLM/ST2S/2024.07.07_CosyVoice.md); [MaskGCT [78]](../../Models/SpeechLM/ST2S/2024.09.01_MaskGCT.md); [NaturalSpeech3 [87]](../../Models/Diffusion/2024.03.05_NaturalSpeech3.md)) 都使用离散 Token 来作为声学特征.

然而, 离散表示学习可能会导致信息损失或缺乏连续表示中可以捕获的细节.

表 4 和表 3 总结了代表性方法的声学特征类型.

表 2 总结了流行的开源语音量化方法. #TODO CSV

</td></tr></table>

# 3·Uncontrollable TTS: 非可控文本转语音

#仅一些简单的总结

<table><tr><td width="50%">

The development of Uncontrollable Text-To-Speech (UC-TTS) systems represents a significant shift from traditional, linguistics-based synthesis to modern, data-driven deep learning techniques.
This shift highlights the integration of both local and global information to produce speech with human-like quality and naturalness.
This survey explores UC-TTS evolution, emphasizing the role of local and global information in enhancing speech fidelity and expressiveness.

In the context of UC-TTS, "uncontrollable" refers to the absence of explicit control mechanisms for speech features such as emotion, timbre, and speaker style.
Despite this lack of explicit control, the goal is to achieve natural, fluid speech while minimizing issues like mispronunciations and omissions.

</td><td>

**非可控文本转语音 (Uncontrollable Text-To-Speech, UC-TTS)** 系统的发展代表着从传统基于语言学的合成到现代数据驱动深度学习技术的重要转变.
这一转变强调了集成局部和全局信息以产生类似人类质量和自然度的语音.

本综述探讨了 UC-TTS 的演进, 强调了局部信息和全局信息在增强语音保真度和表现力的作用.

在 UC-TTS 的语境中, "非可控"一词指的是缺乏对语音特征 (如情感, 音色, 说话人风格) 显式的控制机制.
尽管缺乏显式的控制, 但目标是实现自然流畅的语音, 同时避免如发音错误和漏读等问题.

</td></tr></table>

## A·Early Approaches: Statistical Models: 早期方法: 统计模型

<table><tr><td width="50%">

Early Text-To-Speech (TTS) systems relied on statistical models such as Hidden Markov Models (HMMs) ([Yoshimura et al. (1999) [64]](../../Models/_Early/Simultaneous_Modeling_of_Spectrum_Pitch_&_Duration_in_HMM-Based_Speech_Synthesis.md); [Tokuda et al. (2000) [65]](../../Models/_Early/Speech_Parameter_Generation_Algorithms_for_HMM-Based_Speech_Synthesis.md)) and early neural network-based parametric methods ([Statistical Parametric Speech Synthesis Using DNNs [111]](../../Models/_Early/2013.05.26_Statistical_Parametric_Speech_Synthesis_Using_DNNs.md); [DBLSTM-RNN [112]](../../Models/_Early/DBLSTM-RNN.md)).
These models operated at the frame level, using acoustic models and vocoders for text-to-speech conversion.
Notable contributions from [Tokuda et al. [173]](../../Models/_Early/2013.04.09_Speech_Synthesis_Based_on_Hidden_Markov_Models.md) employed HMMs for statistical parametric synthesis, focusing on local features like phonemes, accents, and prosody to improve speech naturalness.

While robust, these statistical methods were limited by their reliance on pre-segmented data, leading to oversimplified assumptions about speech dynamics.
Local linguistic features were well-modeled, but the global phonetic context was often overlooked, resulting in speech that sounded monotone and lacked emotional depth, as noted by [Zen et al. [174]](../../Models/_Early/2009.01.14_Statistical_Parametric_Speech_Synthesis.md).

</td><td>

早期文本转语音系统依赖于
- 统计模型, 如隐马尔科夫模型 (Hidden Markov Models, HMMs)
  - [Yoshimura et al. (1999) [64]](../../Models/_Early/Simultaneous_Modeling_of_Spectrum_Pitch_&_Duration_in_HMM-Based_Speech_Synthesis.md);
  - [Tokuda et al. (2000) [65]](../../Models/_Early/Speech_Parameter_Generation_Algorithms_for_HMM-Based_Speech_Synthesis.md);
- 早期基于神经网络的参数方法
  - [Statistical Parametric Speech Synthesis Using DNNs [111]](../../Models/_Early/2013.05.26_Statistical_Parametric_Speech_Synthesis_Using_DNNs.md);
  - [DBLSTM-RNN [112]](../../Models/_Early/DBLSTM-RNN.md).

这些模型运行在帧级上, 使用声学模型和声码器进行文本到语音的转换.

值得提及的是, [Tokuda et al. [173]](../../Models/_Early/2013.04.09_Speech_Synthesis_Based_on_Hidden_Markov_Models.md) 使用 HMMs 进行统计参数化语音合成, 着重于局部特征, 如音素, 语调, 和语调, 以提高语音自然度.

尽管这类统计方法很健壮, 但受限于预分割的数据, 导致了对语音动态的过度简化假设.
局部语言特征被很好地建模, 但全局音素上下文往往被忽视, 导致产生单调且缺乏情感深度的语音, 如 [Zen et al. [174]](../../Models/_Early/2009.01.14_Statistical_Parametric_Speech_Synthesis.md) 所述.

</td></tr></table>

## B·Sequence-to-Sequence Models: 序列到序列模型

<table><tr><td width="50%">

The emergence of sequence-to-sequence models represents a significant breakthrough by removing the need for explicit linguistic features, thereby enabling the capture of the nuances and idiosyncrasies of human speech.
Models such as [Tacotron [74]](../../Models/Acoustic/2017.03.29_Tacotron.md) and [Tacotron 2 [175]](../../Models/Acoustic/2017.12.16_Tacotron2.md) utilize recurrent neural networks (RNNs) with attention mechanisms to effectively model the complex, nonlinear nature of speech sequences.
These innovations allow for precise tuning of speech parameters, enhancing prosody and rhythm by modeling entire utterances rather than isolated phonetic units.

Building on these advancements, [DeepVoice3 [176]](../../Models/Acoustic/2017.10.20_DeepVoice3.md) introduces a fully convolutional sequence-to-sequence architecture that significantly accelerates training speed compared to RNN-based models.
This approach achieves training times an order of magnitude faster, enabling scalability to handle large datasets.
Additionally, the use of a position-augmented attention mechanism in Deep Voice 3 enhances the naturalness of synthesized speech, achieving competitive mean opinion scores, especially when paired with advanced neural vocoders like WaveNet.
This development not only improves training efficiency but also enhances the scalability and naturalness of text-to-speech systems.

</td><td>

序列到序列模型的出现代表着一种显著突破, 移除了对显式语言特征的需要, 使得能够捕获人类语音的细微差别和癖好.
诸如 [Tacotron [74]](../../Models/Acoustic/2017.03.29_Tacotron.md) 和 [Tacotron 2 [175]](../../Models/Acoustic/2017.12.16_Tacotron2.md) 之类的模型利用带有注意机制的循环神经网络 (RNNs) 模型来有效地建模语音序列的复杂的非线性特性.
这些创新能够对语音参数进行精确调整, 通过建模整个句子而不是孤立的音素单元来增强语调和韵律.

基于这些进步, [DeepVoice3 [176]](../../Models/Acoustic/2017.10.20_DeepVoice3.md) 提出了一个完全卷积的序列到序列架构, 与基于 RNN 的模型相比显著加快了训练速度.
这种方法能够以 10 倍以上的速度训练, 这使得它能够处理大型数据集.
此外, 在 DeepVoice3 中引入的位置增强注意力机制进一步增强了合成语音的自然度, 可以获得竞争力的平均意见得分, 尤其是与 WaveNet 等先进的神经声码器配合时.
这一发展不仅提高了训练效率, 而且也增强了文本到语音系统的可扩展性和自然度.

</td></tr></table>

## C·Transformer-Based Models: 基于 Transformer 的模型

<table><tr><td width="50%">

Transformer-based architectures advanced the field by enabling computational parallelization and effectively capturing long-range dependencies.
Models like [Transformer TTS [177]](../../Models/Acoustic/2018.09.19_TransformerTTS.md) overcame RNN challenges, such as gradient vanishing, by using efficient training paradigms.
Self-attention mechanisms allowed simultaneous modeling of local phonetic details and global prosodic contexts, resulting in more sophisticated and human-like speech synthesis.

Although transformers improved contextual information incorporation, challenges remained in preserving local phonetic precision.
To address these, techniques such as relative position encodings and localized attention were integrated ([Transformer [124]](../../Models/_Transformer/2017.06.12_Transformer.md)).

</td><td>

基于 Transformer 的架构通过计算并行化和有效捕获长距离依赖, 推动了该领域的发展.
诸如 [Transformer TTS [177]](../../Models/Acoustic/2018.09.19_TransformerTTS.md) 之类的模型采用高效的训练范式来克服 RNN 的挑战, 如梯度消失.
自注意力机制允许同时建模局部音素细节和全局韵律上下文, 产生更复杂更类似人类的语音合成.

尽管 Transformer 提升了上下文信息的整合, 但仍然存在局部音素精度的问题.
为了解决这些问题, 诸如相对位置编码和局部注意力等技术被集成到了模型中 ([Transformer [124]](../../Models/_Transformer/2017.06.12_Transformer.md)).

</td></tr></table>

## D·Advanced Architectures: Integrating Flow and Diffusion Models: 集成 Flow 和 Diffusion 模型

<table><tr><td width="50%">

Recent advancements have shifted towards integrating global information within end-to-end architectures to enhance speech naturalness and coherence.
Flow-based models like [Glow-TTS [133]](../../Models/Acoustic/2020.05.22_Glow-TTS.md) and [Flow-TTS [132]](../../Models/Acoustic/2020.04.09_Flow-TTS.md) exemplify this by employing invertible transformations that maintain the balance between local precision and global coherence.
These architectures enable the synthesis of high-fidelity speech by modeling complex dependencies across the entire utterance, thus improving the overall fluidity and naturalness of the generated speech.

Moreover, the introduction of diffusion models in TTS, such as [WaveGrad 2 [178]](../../Models/Vocoder/2021.06.17_WaveGrad2.md), highlights the shift towards models that can iteratively refine speech output.
These models use score matching and diffusion processes to generate speech directly from phoneme sequences, effectively capturing both local nuances and overarching global patterns.
The iterative nature of these models allows for adjustments that enhance the quality of the synthesized audio, accommodating variations in speech without explicit control over specific attributes.

The integration of adversarial training and variational autoencoders (VAEs) further exemplifies the evolution towards incorporating global information.
Systems like [VITS [159]](../../Models/E2E/2021.06.11_VITS.md) leverage these techniques to enhance expressiveness and naturalness by learning complex mappings between text and speech.
This approach allows the model to manage variations in prosody and rhythm inherently derived from the textual input, aligning with the objectives of UC-TTS to produce diverse and natural speech outputs.

The evolution from HMMs to advanced architectures in UC-TTS exemplifies progress toward synthesizing speech that is both expressive and precise.
The interplay of local and global information is crucial for enhancing speech quality and customizability.
Future UC-TTS research aims to produce high-fidelity, customizable speech by harmonizing deep contextual insights with precise local adjustments, meeting diverse user needs and communication contexts.

</td><td>

近期的进展已经转向在端到端架构中集成全局信息以增强语音自然度和连贯性.

基于流的模型, 如 [Glow-TTS [133]](../../Models/Acoustic/2020.05.22_Glow-TTS.md) 和 [Flow-TTS [132]](../../Models/Acoustic/2020.04.09_Flow-TTS.md), 展示了这一点, 通过使用可逆变换来保持局部精度和全局连贯性的平衡.
这些架构通过对整个句子建模复杂依赖进行高保真语音合成, 这有助于提高合成语音的流畅性和自然度.

此外, 扩散模型的引入, 如 [WaveGrad 2 [178]](../../Models/Vocoder/2021.06.17_WaveGrad2.md), 也突出了转向迭代式精细语音输出的模型的趋势.
这些模型使用得分匹配和扩散过程来直接从音素序列生成语音, 有效捕获局部细微差别和全局模式.
这些模型的迭代性质使得一些调整能够增强合成音频的质量, 适应语音的变化而无需对特定属性进行显式控制.

对抗训练和变分自编码器的集成进一步证明了集成全局信息的进步.
如 [VITS [159]](../../Models/E2E/2021.06.11_VITS.md) 等系统利用这些技术通过学习复杂的文本-语音映射来增强表达性和自然度.
这一方法允许模型管理文本输入中隐含的韵律和节奏, 与 UC-TTS 的目标一致, 生成多样化和自然的语音输出.

从 HMM 到 UC-TTS 中的先进架构的演进, 证明了向生成具有丰富表达性和精确度的语音的进展.
局部和全局信息之间的交互对于增强语音质量和可定制性至关重要.
未来 UC-TTS 研究的目标是通过结合深度上下文见解和精细的局部调整, 实现高保真度、可定制的语音输出, 同时满足广泛的用户需求和通信环境.

</td></tr></table>

# 4·Controllable TTS: 可控文本转语音

<table><tr><td width="50%">

In this section, we first review recent TTS work from the perspective of model architecture, followed by a detailed discussion of control modes in controllable TTS.

</td><td>

在本节中, 我们首先从模型架构的角度回顾近期的 TTS 工作, 然后详细讨论可控 TTS 中的控制模式.

</td></tr></table>

## A·Model Architecture: 模型架构

<table><tr><td width="50%">

Current model architectures can be broadly classified into two main categories: the first is the non-autoregressive (NAR) generative models, which are based on HMMs, neural networks, VAEs, diffusion models, flow matching, and other NAR techniques.
The second category relies on autoregressive (AR) codec language models, which typically quantize speech into discrete tokens and use decoder-only models to autoregressively generate these tokens.
We summarize the NAR-based and AR-based controllable TTS methods in Table.03 and Table.04, respectively.

</td><td>

现有模型架构可以大致分类为两个主要类别:
- 非自回归生成式模型: 基于 HMM, 神经网络, 变分自编码器, 扩散模型, 流匹配, 和其他.
- 自回归编解码器语言模型: 将语音量化为离散 Token, 并使用仅解码器的模型来自回归生成这些 Token.

我们在表格 03 和表格 04 中分别总结了非自回归和自回归可控 TTS 方法.

</td></tr></table>

### Fully Non-Autoregressive (NAR) Architectures: 完全非自回归架构

| NAR Method | Zero-Shot TTS | Pitch | Energy | Speed | Prosody | Timbre | Emotion | Environment | Description | Acoustic Model | Vocoder | Acoustic Feature | Release Time |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|[FastSpeech](../../Models/Acoustic/2019.05.22_FastSpeech.md)|  |  |  | √ | √ |  |  |  |  | Transformer | WaveGlow | MelS | 2019.05 |
|DWAPI~\cite{vekkot2020emotional} |  | √ |  |  | √ |  | √ |  |  | DNN | Straight | MelS + F0 + Intensity | 2020.04 |
|[FastSpeech2](../../Models/Acoustic/2020.06.08_FastSpeech2.md)|  | √ | √ | √ | √ |  |  |  |  | Transformer | Parallel WaveGAN | MelS | 2020.06 |
|[FastPitch](../../Models/Acoustic/2020.06.11_FastPitch.md)|  | √ |  |  | √ |  |  |  |  | Transformer | WaveGlow | MelS | 2020.06 |
|Parallel Tacotron\cite{elias2021parallel} |  |  |  |  | √ |  |  |  |  | Transformer + CNN | WaveRNN | MelS | 2020.10 |
|StyleTagging-TTS~\cite{kim2021expressive} | √ |  |  |  |  | √ | √ |  |  | Transformer + CNN | HiFi-GAN | MelS | 2021.04 |
|SC-GlowTTS~\cite{casanova2021sc} | √ |  |  |  |  | √ |  |  |  | Transformer + Conv | HiFi-GAN | MelS | 2021.06 |
|Meta-StyleSpeech~\cite{min2021meta} | √ |  |  |  |  | √ |  |  |  | Transformer | MelGAN | MelS | 2021.06 |
|DelightfulTTS\cite{liu2021delightfultts} |  | √ |  | √ | √ |  |  |  |  | Transformer + CNN | HiFiNet | MelS | 2021.11 |
|[YourTTS](../../Models/E2E/2021.12.04_YourTTS.md)| √ |  |  |  |  | √ |  |  |  | Transformer | HiFi-GAN | LinS | 2021.12 |
|DiffGAN-TTS\cite{liu2022diffgan} |  | √ |  | √ | √ |  |  |  |  | Diffusion + GAN | HiFi-GAN | MelS | 2022.01 |
|StyleTTS~\cite{li2022styletts} | √ |  |  |  |  | √ |  |  |  | CNN + RNN + GAN | HiFi-GAN | MelS | 2022.05 |
|GenerSpeech~\cite{huang2022generspeech} | √ |  |  |  |  | √ |  |  |  | Transformer + Flow-based | HiFi-GAN | MelS | 2022.05 |
|NaturalSpeech 2~\cite{shen2023naturalspeech2} | √ |  |  |  |  | √ |  |  |  | Diffusion | Codec Decoder | Token | 2022.05 |
|Cauliflow\cite{abbas2022expressive} |  |  |  | √ | √ |  |  |  |  | BERT + Flow | UP WaveNet | MelS | 2022.06 |
|CLONE\cite{liu2022controllable} |  | √ |  | √ | √ |  |  |  |  | Transformer + CNN | WaveNet | MelS + LinS | 2022.07 |
|PromptTTS~\cite{guo2023prompttts} |  | √ | √ | √ |  | √ | √ |  | √ | Transformer | HiFi-GAN | MelS | 2022.11 |
|Grad-StyleSpeech~\cite{kang2023grad} | √ |  |  |  |  | √ |  |  |  | Score-based Diffusion | HiFi-GAN | MelS | 2022.11 |
|PromptStyle~\cite{liu2023promptstyle} | √ | √ |  |  | √ | √ | √ |  | √ | VITS | HiFi-GAN | MelS | 2023.05 |
|StyleTTS 2~\cite{li2024styletts2} | √ |  |  |  | √ | √ | √ |  |  | Diffusion + GAN | HifiGAN / iSTFTNet | MelS | 2023.06 |
|VoiceBox~\cite{le2024voicebox} | √ |  |  |  |  | √ |  |  |  | Flow Matching Diffusion | HiFi-GAN | MelS | 2023.06 |
|MegaTTS 2~\cite{jiang2024mega} | √ |  |  |  | √ | √ |  |  |  | Diffusion + GAN | HiFi-GAN | MelS | 2023.07 |
|PromptTTS 2~\cite{leng2023prompttts2} |  | √ | √ | √ |  | √ |  |  | √ | Diffusion | Codec Decoder | Token | 2023.09 |
|VoiceLDM~\cite{lee2024voiceldm} |  | √ |  |  | √ | √ | √ | √ | √ | Diffusion | HiFi-GAN | MelS | 2023.09 |
|DuIAN-E\cite{gu2023durian} |  | √ |  | √ | √ |  |  |  |  | CNN + RNN | HiFi-GAN | MelS | 2023.09 |
|PromptTTS++~\cite{shimizu2024prompttts++} |  | √ |  | √ | √ | √ | √ |  | √ | Transformer + Diffusion | BigVGAN | MelS | 2023.09 |
|SpeechFlow~\cite{liu2023generative} | √ |  |  |  |  | √ |  |  |  | Flow Matching Diffusion | HiFi-GAN | MelS | 2023.10 |
|P-Flow~\cite{kim2024p} | √ |  |  |  |  | √ |  |  |  | Flow Matching | HiFi-GAN | MelS | 2023.10 |
|E3 TTS~\cite{gao2023e3} | √ |  |  |  |  | √ |  |  |  | Diffusion | / | Waveform→Unet | 2023.11 |
|HierSpeech++~\cite{lee2023hierspeech++} | √ |  |  |  |  | √ |  |  |  | Hierarchical Conditional VAE | BigVGAN | MelS | 2023.11 |
|Audiobox~\cite{vyas2023audiobox} | √ | √ |  | √ | √ | √ |  | √ | √ | Flow Matching | EnCodec | MelS | 2023.12 |
|FlashSpeech~\cite{ye2024flashspeech} | √ |  |  |  |  | √ |  |  |  | Latent Consistency Model | EnCodec | Token | 2024.04 |
|NaturalSpeech 3~\cite{ju2024naturalspeech3} | √ |  |  | √ | √ | √ |  |  |  | Diffusion | EnCodec | Token | 2024.04 |
|InstructTTS~\cite{yang2024instructtts} |  | √ |  | √ | √ | √ | √ |  | √ | Transformer + Diffusion | HiFi-GAN | Token | 2024.05 |
|ControlSpeech~\cite{ji2024controlspeech} | √ | √ | √ | √ | √ | √ | √ | | √ | Transformer + Diffusion | FACodec Decoder | Token | 2024.06 |
|AST-LDM~\cite{kim2024speak} |  |  |  |  |  | √ |  | √ | √ | Diffusion | HiFi-GAN | MelS | 2024.06 |
|SimpleSpeech~\cite{yang2024simplespeech} | √ |  |  |  |  | √ |  |  |  | Transformer Diffusion | SQ Decoder | Token | 2024.06 |
|DiTTo-TTS~\cite{lee2024ditto} | √ |  |  | √ |  | √ |  |  |  | DiT | BigVGAN | Token | 2024.06 |
|E2 TTS~\cite{eskimez2024e2} | √ |  |  |  |  | √ |  |  |  | Flow Matching Transformer | BigVGAN | MelS | 2024.06 |
|MobileSpeech~\cite{ji2024mobilespeech} | √ |  |  |  |  | √ |  |  |  | ConFormer Decoder | Vocos | Token | 2024.06 |
|DEX-TTS~\cite{park2024dex} | √ |  |  |  |  | √ |  |  |  | Diffusion | HiFi-GAN | MelS | 2024.06 |
|ArtSpeech~\cite{wang2024artspeech} | √ |  |  |  |  | √ |  |  |  | RNN + CNN | HiFI-GAN | MelS+Energy+F0+TV | 2024.07 |
|CCSP~\cite{xiao2024contrastive} | √ |  |  |  |  | √ |  |  |  | Diffusion | Codec Decoder | Token | 2024.07 |
|SimpleSpeech 2~\cite{yang2024simplespeech2} | √ |  |  | √ |  | √ |  |  |  | Flow-based Transformer Diffusion | SQ Decoder | Token | 2024.08 |
|E1 TTS~\cite{liu2024e1} | √ |  |  |  |  | √ |  |  |  | DiT | BigVGAN | Continuous Token | 2024.09 |
|VoiceGuider~\cite{yeom2024voiceguider} | √ |  |  |  |  | √ |  |  |  | Diffusion | BigVGAN | MelS | 2024.09 |
|StyleTTS-ZS~\cite{li2024stylettszs} | √ |  |  |  |  | √ |  |  |  | Diffusion + GAN | HifiGAN / iSTFTNet | Token | 2024.09 |
|NansyTTS~\cite{yamamoto2024description} | √ | √ |  | √ | √ | √ |  |  | √ | Transformer | NANSY++ | MelS | 2024.09 |
|NanoVoice~\cite{park2024nanovoice} | √ |  |  |  |  | √ |  |  |  | Diffusion | BigVGAN | MelS | 2024.09 |
|MS$^{2}$KU-VTTS~\cite{he2024multi} |  |  |  |  |  |  |  | √ | √ | Diffusion | BigvGAN | MelS | 2024.10 |
|MaskGCT~\cite{wang2024maskgct} | √ |  |  | √ |  | √ |  |  |  | Masked Generative Transformers | DAC + Vocos | Token | 2024.10 |

#### HMM-Based Approaches: 基于 HMM 的方法

<table><tr><td width="50%">

In the realm of Controllable Text-To-Speech (CTTS), advancements in Hidden Markov Model (HMM) architectures have significantly enhanced the manipulation of speech elements such as emotion and prosody.
Yamagishi et al.~\cite{yamagishi2003modeling} pioneered this field by introducing style-dependent and style-mixed modeling, which allowed precise emulation of human-like emotional nuances and versatile synthesis across various styles by incorporating style as a contextual variable.
Building on this foundation, Qin et al.\cite{qin2006hmm} developed the "average emotion model," which utilized MLLR-based adaptation to modulate emotions like happiness and sadness even with limited data, thus advancing the emotional intelligence of synthetic speech systems.

Furthering expressive variability, [Nose et al. (2012) [119]](../../Models/_Early/An_Intuitive_Style_Control_Technique_in_HMM-Based_Expressive_Speech_Synthesis_Using_Subjective_Style_Intensity_and_Multiple-Regression_Global_Variance_Model.md) integrated subjective style intensities and a multiple-regression global variance model into HMM frameworks, addressing over-smoothing and enabling nuanced emotional expressions.
[Lorenzo-Trueba et al. (2015) [72]](../../Models/_Early/Emotion_Transplantation_through_Adaptation_in_HMM-Based_Speech_Synthesis.md) expanded on these capabilities with CSMAPLR adaptation, introducing "emotion transplantation" to transfer emotional states between speakers while preserving voice distinctiveness, enhancing personalized human-computer interaction.
These innovations in HMM architectures have broadened the expressiveness and individuality in synthetic speech, augmenting technological interfaces and paving the way for future developments in adaptive, lifelike speech solutions.

</td></tr></table>

#### Transformer-Based Approaches: 基于 Transformer 的方法

<table><tr><td width="50%">

Advancements in Controllable Text-to-Speech (TTS) technology highlight the integration of deep learning with audio processing, driven by Transformer-based architectures.
Ren et al. introduced [FastSpeech [15]](../../Models/Acoustic/2019.05.22_FastSpeech.md), a feed-forward non-autoregressive Transformer model that significantly enhances TTS efficiency by reducing inference time and improving the stability issues found in autoregressive models like Tacotron 2.
This model provides precise control over prosodic features through duration prediction, effectively tackling the one-to-many mapping challenge.
[FastSpeech2](../../Models/Acoustic/2020.06.08_FastSpeech2.md) builds on this by integrating pitch and energy control, eliminating the need for the complex teacher-student distillation process, thus enhancing training efficiency and improving voice quality.
[Parallel Tacotron]\cite{elias2021paralleltacotron} further advances TTS by employing a variational autoencoder-based residual encoder, capturing intricate prosodic nuances.
This approach, combined with iterative spectrogram loss, significantly enhances the naturalness and quality of synthesized speech.
Additionally, [FastPitch](../../Models/Acoustic/2020.06.11_FastPitch.md) incorporates direct pitch prediction into its architecture, enabling fully parallelized synthesis and precise pitch manipulation.
This capability enhances expressiveness and retains the efficiency benefits established by FastSpeech.
These innovations significantly contribute to the development of more interactive and natural AI-driven communication systems, underscoring the potential of integrating AI with human-centric disciplines to craft a future where technology and humanity coexist harmoniously.

</td></tr></table>

#### VAE-Based Approaches: 基于 VAE 的方法

<table><tr><td width="50%">

Recent advancements in Controllable Text-To-Speech (TTS) systems are largely driven by the integration of Variational Autoencoder (VAE) architectures, which enhance the flexibility and precision of speech modulation.
[Zhang et al. [18]](../../Models/Acoustic/2018.12.11_Learning_Latent_Representations_for_Style_Control_and_Transfer_in_End-to-End_Speech_Synthesis.md) pioneered the use of VAEs in end-to-end speech synthesis, creating disentangled latent representations that allow effective style control and transfer, especially in prosody and emotion management, outperforming the Global Style Token model in style transfer tasks.
Building on this, Hsu et al.\cite{hsu2018hierarchical} developed a hierarchical generative model with a conditional VAE framework and a Gaussian mixture model, enabling precise control over complex speech attributes such as environment and style, thus improving expressive speech synthesis through refined noise and speaker characteristic management.
Liu et al.\cite{liu2022controllable} further advanced the field with the CLONE model, a single-stage TTS system that resolves the one-to-many mapping issue and enhances high-frequency information reconstruction.
By employing a conditional VAE with normalizing flows and a dual path adversarial training mechanism with multi-band discriminators, CLONE achieves nuanced control over prosody and energy, demonstrating superior performance in both speech quality and prosody control compared to state-of-the-art models.
These collective innovations highlight the adaptability of VAEs in managing complex speech generation tasks, marking significant progress toward more dynamic and versatile TTS technologies, with ongoing research promising even greater advancements.

</td></tr></table>

#### Diffusion-Based Approaches: 基于扩散模型的方法

<table><tr><td width="50%">

The core concept of diffusion-based models is to generate target data by progressively removing noise.
During the forward diffusion phase, noise is incrementally added to the original data to form a noise distribution.
In the generation phase, a reverse denoising process is employed to gradually recover high-quality speech from the noise.
Grad-StyleSpeech~\cite{kang2023grad} introduces a hierarchical transformer encoder to create a representative noise prior distribution for speaker-adaptive settings using score-based diffusion models.
NaturalSpeech 2~\cite{shen2023naturalspeech2} uses a neural audio codec with residual vector quantizers to obtain quantized latent vectors, which are then generated using a diffusion model conditioned on text input.
[NaturalSpeech3 [87]](../../Models/Diffusion/2024.03.05_NaturalSpeech3.md) decomposes speech into distinct subspaces that represent different attributes and generates each subspace independently.
DEX-TTS~\cite{park2024dex} improves DiT-based diffusion networks by applying overlapping patchify and convolution-frequency patch embedding strategies.
E3 TTS~\cite{gao2023e3} models the temporal structure of the waveform through the diffusion process, eliminating the need for any intermediate representations such as spectrogram features or alignment information.

Applying diffusion models to TTS requires a complex pipeline due to the need for precise temporal alignment between text and speech and the high fidelity required for audio data.
This includes domain-specific modeling, such as phoneme and duration~\cite{shen2023naturalspeech2}.
To address the issue of reduced naturalness caused by the addition of duration models, DiTTo-TTS~\cite{lee2024ditto} leverages the off-the-shelf pre-trained text and speech encoders without relying on speech domain-specific modeling by incorporating cross-attention mechanisms with the prediction of the total length of speech representations.
Similarly, SimpleSpeech~\cite{yang2024simplespeech} proposes a speech codec model (SQ-Codec) based on scalar quantization and uses the sentence duration to control the generated speech length.

</td></tr></table>

#### Flow-Based Approaches: 基于流模型的方法

<table><tr><td width="50%">

Flow-based methods leverage invertible flow transformations to learn mappings from target speech features to simple distributions, typically standard Gaussian distributions.
Due to their invertibility, this mechanism can directly sample from the simple distribution and generate high-fidelity speech in the reverse direction.
Audiobox~\cite{vyas2023audiobox} and P-flow~\cite{kim2024p} employ non-autoregressive flow-matching models for efficient and stable speech synthesis.
VoiceBox~\cite{le2024voicebox} also employs a flow-matching to generate speech, effectively casting the TTS task into a speech infilling task.
SpeechFlow~\cite{liu2023generative} is trained on 60k hours of untranscribed speech with flow matching and mask conditions and can be fine-tuned with task-specific data to match or surpass existing expert models.
This highlights the potential of generative models as foundation models for speech applications.
HierSpeech++~\cite{lee2023hierspeech++} proposes a hierarchical variational inference method.
FlashSpeech~\cite{ye2024flashspeech} is built on a latent consistency model and applies a novel adversarial consistency training approach that can train from scratch without the need for a pre-trained diffusion model as the teacher, achieving speech generation in one or two steps.

Recently, E2 TTS~\cite{eskimez2024e2} converts text input into a character sequence with filler tokens and trains a mel spectrogram generator based on audio infilling task, achieving human-level naturalness.
Inspired by E2 TTS, [F5-TTS [194]](../../Models/Diffusion/2024.10.09_F5-TTS.md) refines the text representation with ConvNext v2~\cite{woo2023convnext}, facilitating easier alignment with speech.
E1 TTS~\cite{liu2024e1} further distills a diffusion-based TTS model into a one-step generator with distribution matching distillation~\cite{luo2024diff,yin2024improved}, reducing the number of network evaluations in sampling from diffusion models.
SimpleSpeech 2~\cite{yang2024simplespeech2} introduces a flow-based scalar transformer diffusion model.
The work also provides a theoretical analysis, showing that the inclusion of a small number of noisy labels in a large-scale dataset is equivalent to introducing classifier-free guidance during model optimization.

</td></tr></table>

#### Other NAR Approaches: 其他方法

<table><tr><td width="50%">

Other works leverage GAN-based or Masked Generative-based methods for TTS generation.
StyleTTS 2~\cite{li2024styletts2} employs large pre-trained speech language models (SLMs) such as Wav2Vec 2.0~\cite{baevski2020wav2vec}, [HuBERT [166]](../../Models/SpeechRepresentation/2021.06.14_HuBERT.md), and WavLM~\cite{chen2022wavlm} as discriminators, in combination with a novel differentiable duration modeling approach.
This setup uses SLM representations to enhance the naturalness of the synthesized speech.
[MaskGCT [78]](../../Models/SpeechLM/ST2S/2024.09.01_MaskGCT.md) proposes masked generative transformers without requiring text-speech alignment supervision and phone-level duration.
The model employs a two-stage system, both trained using a mask-and-predict learning paradigm.

</td></tr></table>

### Autoregressive (AR) Architectures: 自回归架构

| AR Method | Zero-Shot TTS | Pitch | Energy | Speed | Prosody | Timbre | Emotion | Environment | Description | Acoustic Model | Vocoder | Acoustic Feature | Release Time |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|Prosody-Tacotron\cite{skerry2018towards} |  | √ |  |  | √ |  |  |  |  | RNN | WaveNet | MelS | 2018.03 |
|GST-Tacotron\cite{stanton2018predicting} |  | √ |  |  | √ |  |  |  |  | CNN + RNN | Griffin-Lim | LinS | 2018.03 |
|GMVAE-Tacotron\cite{hsu2018hierarchical} |  | √ |  | √ | √ |  |  | √ |  | CNN + RNN | WaveRNN | MelS | 2018.12 |
|VAE-Tacotron\cite{zhang2019learning} |  | √ |  | √ | √ |  |  |  |  | CNN + RNN | WaveNet | MelS | 2019.02 |
|DurIAN\cite{yu2019durian} |  | √ |  | √ | √ |  |  |  |  | CNN + RNN | MB-WaveRNN | MelS | 2019.09 |
|Flowtron\cite{valle2020flowtron} |  | √ |  | √ | √ |  |  |  |  | CNN + RNN | WaveGlow | MelS | 2020.07 |
|MsEmoTTS\cite{lei2022msemotts} |  | √ |  |  | √ |  | √ |  |  | CNN + RNN | WaveRNN | MelS | 2022.01 |
|VALL-E~\cite{wang2023neural} | √ |  |  |  |  | √ |  |  |  | LLM | EnCodec | Token | 2023.01 |
|SpearTTS~\cite{kharitonov2023speak} | √ |  |  |  |  | √ |  |  |  | LLM | SoundStream | Token | 2023.02 |
|VALL-E X~\cite{zhang2023speak} | √ |  |  |  |  | √ |  |  |  | LLM | EnCodec | Token | 2023.03 |
|Make-a-voice~\cite{huang2023make} | √ |  |  |  |  | √ |  |  |  | LLM | BigVGAN | Token | 2023.05 |
|TorToise~\cite{betker2023better} |  |  |  |  |  | √ |  |  |  | Transformer + DDPM | Univnet | MelS | 2023.05 |
|MegaTTS~\cite{jiang2023megavoic} | √ |  |  |  |  | √ |  |  |  | LLM + GAN | HiFi-GAN | MelS | 2023.06 |
|SC VALL-E~\cite{kim2023sc} | √ | √ |  | √ | √ | √ | √ |  |  | LLM | EnCodec | Token | 2023.07 |
|Salle~\cite{ji2024textrolspeech} |  | √ | √ | √ | √ | √ | √ |  | √ | LLM | Codec Decoder | Token | 2023.08 |
|UniAudio~\cite{yang2023uniaudio} | √ | √ |  | √ | √ | √ |  |  | √ | LLM | EnCodec | Token | 2023.10 |
|ELLA-V~\cite{song2024ella} | √ |  |  |  |  | √ |  |  |  | LLM | EnCodec | Token | 2024.01 |
|BaseTTS~\cite{lajszczak2024base} | √ |  |  |  |  | √ |  |  |  | LLM | UnivNet | Token | 2024.02 |
|ClaM-TTS~\cite{kim2024clam} | √ |  |  |  |  | √ |  |  |  | LLM | BigVGAN | MelS+Token | 2024.04 |
|RALL-E~\cite{xin2024rall} | √ |  |  |  |  | √ |  |  |  | LLM | SoundStream | Token | 2024.05 |
|ARDiT~\cite{liu2024autoregressive} | √ |  |  | √ |  | √ |  |  |  | Decoder-only Diffusion Transformer | BigVGAN | MelS | 2024.06 |
|VALL-E R~\cite{han2024vall} | √ |  |  |  |  | √ |  |  |  | LLM | Vocos | Token | 2024.06 |
|VALL-E 2~\cite{chen2024vall} | √ |  |  |  |  | √ |  |  |  | LLM | Vocos | Token | 2024.06 |
|Seed-TTS~\cite{anastassiou2024seed} | √ |  |  |  |  | √ | √ |  |  | LLM + Diffusion Transformer | / | Token | 2024.06 |
|VoiceCraft~\cite{peng2024voicecraft} | √ |  |  |  |  | √ |  |  |  | LLM | HiFi-GAN | Token | 2024.06 |
|XTTS~\cite{casanova2024xtts} | √ |  |  |  |  | √ |  |  |  | LLM + GAN | HiFi-GAN | MelS+Token | 2024.06 |
|CosyVoice~\cite{du2024cosyvoice} | √ | √ |  | √ | √ | √ | √ |  | √ | LLM + Conditional Flow Matching | HiFi-GAN | Token | 2024.07 |
|MELLE~\cite{meng2024autoregressive} | √ |  |  |  |  | √ |  |  |  | LLM | HiFi-GAN | MelS | 2024.07 |
|Bailing TTS~\cite{di2024bailing} | √ |  |  |  |  | √ |  |  |  | LLM + Diffusion Transformer | / | Token | 2024.08 |
|VoxInstruct~\cite{zhou2024voxinstruct} | √ | √ | √ | √ | √ | √ | √ |  | √ | LLM | Vocos | Token | 2024.08 |
|Emo-DPO~\cite{gao2024emo} |  |  |  |  |  |  | √ |  | √ | LLM | HiFi-GAN | Token | 2024.09 |
|FireRedTTS~\cite{guo2024fireredtts} | √ |  |  |  | √ | √ |  |  |  | LLM + Conditional Flow Matching | BigVGAN-v2 | Token | 2024.09 |
|CoFi-Speech~\cite{guo2024speaking} | √ |  |  |  |  | √ |  |  |  | LLM | BigVGAN | Token | 2024.09 |
|Takin~\cite{chen2024takin} | √ | √ |  | √ | √ | √ | √ |  | √ | LLM | HiFi-Codec | Token | 2024.09 |
|HALL-E~\cite{nishimura2024hall} | √ |  |  |  |  | √ |  |  |  | LLM | EnCodec | Token | 2024.10 |
|
#### RNN-Based Approaches: 基于 RNN 的方法

<table><tr><td width="50%">

Controllable Text-To-Speech (TTS) technology has seen significant advancements through innovations in neural network architectures, facilitating speech that is both natural-sounding and adaptable in terms of emotion, prosody, and pitch.
A key breakthrough was the introduction of [Tacotron [74]](../../Models/Acoustic/2017.03.29_Tacotron.md), a sequence-to-sequence architecture that effectively integrates prosodic variations, laying the groundwork for precise control over speech attributes.
[Tacotron 2 [175]](../../Models/Acoustic/2017.12.16_Tacotron2.md) further enhanced this capability by better managing prosodic variability, though it averaged these variations, indicating a need for more sophisticated control methods.
To address these constraints, Wang et al. introduced [Global Style Tokens (GSTs) [19]](../../Models/Style/2018.03.23_GST.md), using an unsupervised approach to encapsulate diverse speech styles into fixed tokens, thus enabling versatile style transfer within the Tacotron framework.
Skerry-Ryan et al.\cite{skerry2018towards} further advanced this by incorporating prosodic embeddings, providing detailed control over timing and intonation, and significantly improving the replication of emotional expressions in synthetic speech.

Building on these innovations, emotion-controllable models developed by [Li et al. [21]](../../Models/Style/2020.11.17_Controllable_Emotion_Transfer_for_End-to-End_Speech_Synthesis.md) focus on calibrating emotional nuances using emotion embedding networks and style loss alignment, allowing detailed modulation of emotional strength.
Hierarchical models like MsEmoTTS\cite{lei2022msemotts} refine this approach by segmenting synthesis into global, utterance-level, and local emotional strengths, offering enhanced emotional expressiveness and intuitive control.
These advancements have expanded the scope to produce nuanced TTS outputs, enabling precise control over emotion, prosody, and pitch, with applications ranging from virtual assistants to interactive narratives.
As researchers continue to explore the potential of neural networks in TTS, the technology promises even richer, more engaging digital experiences, moving towards speech synthesis that is indistinguishable from natural human interaction.

</td></tr></table>

#### LLM-Based Approaches: 基于大语言模型的方法

<table><tr><td width="50%">

Inspired by the success of large language models (LLMs) in natural language processing (NLP), recent studies have explored leveraging in-context learning for zero-shot TTS generation.

VALL-E~\cite{wang2023neural} is a pioneering work in this area, formulating TTS as a conditional language modeling problem.
It utilizes EnCodec~\cite{defossez2022high} to discretize waveforms into tokens as intermediate representations and employs a two-stage modeling pipeline: an autoregressive model first generates coarse audio tokens, followed by a non-autoregressive model that iteratively predicts additional codebook codes for refinement.
This hierarchical modeling of semantic and acoustic tokens has set the foundation for many subsequent LLM-based TTS approaches~\cite{kharitonov2023speak,huang2023make,kim2023sc,chen2024takin}.

Building on VALL-E, various improvements have been proposed.
VALL-E X~\cite{zhang2023speak} extends VALL-E to multilingual scenarios, supporting zero-shot cross-lingual speech synthesis and speech-to-speech translation.
ELLA-V~\cite{song2024ella} introduces a sequence order rearrangement step, enhancing local alignment between phoneme and acoustic modalities.
RALL-E~\cite{xin2024rall} incorporates prosody tokens as chain-of-thought prompting~\cite{wei2022chain} to stabilize the generation of speech tokens.
VALL-E R~\cite{han2024vall} improves phoneme-to-acoustic alignment and adopts codec-merging to boost decoding efficiency and reduce computational overhead.
VALL-E 2~\cite{chen2024vall} introduces repetition aware sampling and grouped code modeling for greater stability and faster inference.
HALL-E~\cite{nishimura2024hall} adopts a hierarchical post-training framework, effectively managing the trade-off between reducing frame rate and producing high-quality speech.

Beyond the foundational improvements introduced by VALL-E and its immediate extensions, further advancements have focused on enhancing speech alignment, quality, and robustness.
SpearTTS~\cite{kharitonov2023speak} and Make-a-voice~\cite{huang2023make} use semantic tokens to bridge the gap between text and acoustic features.
FireRedTTS~\cite{guo2024fireredtts} further optimizes the tokenizer architecture to enhance speech quality.
CoFi-Speech~\cite{guo2024speaking} generates speech in a coarse-to-fine manner via a multi-scale speech coding and generation approach, producing natural and intelligible speech.
[CosyVoice [17]](../../Models/SpeechLM/ST2S/2024.07.07_CosyVoice.md) employs supervised semantic tokens to enhance content consistency and speaker similarity in zero-shot voice cloning.
Similarly, BASE TTS~\cite{lajszczak2024base} introduces discrete speech representations based on the WavLM~\cite{chen2022wavlm} self-supervised model, focusing on phonemic and prosodic information.
SeedTTS~\cite{anastassiou2024seed} also proposes a self-distillation method for speech decomposition and a reinforcement learning approach to enhance the robustness, speaker similarity and controllability of generated speech.
Based on this framework, Bailing-TTS~\cite{di2024bailing} enhances the alignment of text and speech tokens using a continual semi-supervised learning strategy, enabling high-quality synthesis of Chinese dialect speech.

Although models using discrete tokens as intermediate representations have achieved notable success in zero-shot TTS, they still face fidelity issues compared to the continuous representation like Mel spectrograms~\cite{meng2024autoregressive,liu2024autoregressive}.
MELLE~\cite{meng2024autoregressive} optimizes the training objectives and sampling strategy, marking the first exploration of using continuous-valued tokens instead of discrete-valued tokens within the paradigm of autoregressive speech synthesis models.
Similar to MELLE, ARDiT~\cite{liu2024autoregressive} encodes audio as vector sequence in continuous space and autoregressively generates these sequences by a decoder-only transformer.

</td></tr></table>

## B·Control Strategies: 控制策略

<table><tr><td width="50%">

The control strategies in existing controllable TTS can be broadly classified into four categories: style tagging using discrete labels, speech reference prompt for customizing a new speaker's voice with just a few seconds of voice input, controlling speech style using natural language descriptions, and the instruction-guided mode.
We illustrate taxonomies of controllable TTS from the perspective of control strategies in Fig.03.

</td></tr></table>

### Style Tagging: 风格标记

<table><tr><td width="50%">

This paradigm typically employs target control attributes, primarily emotion-related controls, as categorical label inputs to enable controllable speech synthesis.
StyleTagging-TTS~\cite{kim2021expressive} utilizes a short phrase or word to represent the style of an utterance and learns the relationship between linguistic embedding and style embedding space by a pre-trained language model.

However, these methods are limited in expressive diversity, as they can only model a small set of pre-defined styles.

</td></tr></table>

### Reference Speech Prompt: 参考语音提示

<table><tr><td width="50%">

This paradigm aims to customize a new speaker’s voice with just a few seconds of voice prompt.
The architecture can be abstracted into two main components: a speaker encoder that processes the reference speech and outputs a speaker embedding, and a conditional TTS decoder that takes both text and speaker embedding as input to generate speech that matches the style of the reference prompt.
MetaStyleSpeech~\cite{min2021meta} and StyleTTS~\cite{li2022styletts} use adaptive normalization as a style conditioning method, enabling robust zero-shot performance.
GenerSpeech~\cite{huang2022generspeech} introduces a multi-level style adapter to improve zero-shot style transfer for out-of-domain custom voices.
SC VALL-E~\cite{kim2023sc} facilitates control over synthesized speech’s emotions, speaking styles, and various acoustic features by incorporating style tokens and scale factors.
ArtSpeech~\cite{wang2024artspeech} revisits the sound production system by integrating articulatory representations into the TTS framework, improving the physical interpretability of articulation movements.

To enhance the learning of contextual information and address the challenge of limited voice data from the target speaker, CCSP~\cite{xiao2024contrastive} proposes a contrastive context-speech pretraining framework that learns cross-modal representations combining both contextual text and speech expressions.
DEX-TTS~\cite{park2024dex} separates styles into time-invariant and time-variant components, enabling the extraction of diverse styles from expressive reference speech.
StyleTTS-ZS~\cite{li2024stylettszs} leverages distilled time-varying style diffusion to capture diverse speaker identities and prosodies.

Some works also decouple timbre and style information from the reference speech, allowing more flexible control over the speaking style~\cite{[NaturalSpeech3 [87]](../../Models/Diffusion/2024.03.05_NaturalSpeech3.md),jiang2024mega,[ControlSpeech [106]](../../Models/SpeechLM/2024.06.03_ControlSpeech.md)}.
MegaTTS 2~\cite{jiang2024mega} introduces an acoustic autoencoder that separately encodes prosody and timbre into the latent space, enabling the transfer of various speaking styles to the desired timbre.
[ControlSpeech [106]](../../Models/SpeechLM/2024.06.03_ControlSpeech.md) uses bidirectional attention and mask-based parallel decoding to capture codec representations in a discrete decoupling codec space, allowing independent control of timbre, style, and content in a zero-shot manner.

</td></tr></table>

### Natural Language Descriptions: 自然语言描述

<table><tr><td width="50%">

Recent studies explore controlling speech style using natural language descriptions that include attributes such as pitch, gender, and emotion, making the process more user-friendly and interpretable.
In this paradigm, several speech datasets with natural language descriptions~\cite{[PromptTTS [101]](../../Models/Acoustic/2022.11.22_PromptTTS.md),ji2024textrolspeech,[ControlSpeech [106]](../../Models/SpeechLM/2024.06.03_ControlSpeech.md)} and associated prompt generation pipelines~\cite{ji2024textrolspeech,leng2023prompttts2,lyth2024natural} have been proposed.
Detailed information about these datasets will be discussed in [Section 5](Sec.05_Datasets&Evaluation.md).
[PromptTTS [101]](../../Models/Acoustic/2022.11.22_PromptTTS.md) uses manually annotated text prompts to describe five speech attributes, including gender, pitch, speaking speed, volume, and emotion.
[InstructTTS [105]](../../Models/Acoustic/2023.01.31_InstructTTS.md) introduces a three-stage training procedure to capture semantic information from natural language style prompts and adds further annotation to the NLSpeech dataset’s speech styles.
PromptStyle~\cite{liu2023promptstyle} constructs a shared space for stylistic and semantic representations through a two-stage training process.
TextrolSpeech~\cite{ji2024textrolspeech} proposes an efficient prompt programming methodology and a multi-stage discrete style token-guided control framework, demonstrating strong in-context capabilities.
NansyTTS~\cite{yamamoto2024description} combines a TTS trained on the target language with a description control model trained on another language, which shares the same timbre and style representations to enable cross-lingual controllability.

Considering not all details about voice variability can be described in the text prompt, PromptTTS++~\cite{shimizu2024prompttts++} and PromptSpeaker~\cite{zhang2023promptspeaker} tries to construct text prompts with more details.
PromptTTS 2~\cite{leng2023prompttts2} designs a variation network to capture voice variability not conveyed by text prompts.
[ControlSpeech [106]](../../Models/SpeechLM/2024.06.03_ControlSpeech.md) proposes the Style Mixture Semantic Density (SMSD) module, incorporating a noise perturbation mechanism to tackle the many-to-many problem in style control and enhance style diversity.

Other works also focus on improving controllability in additional aspects, such as the surrounding environment.
Audiobox~\cite{vyas2023audiobox} introduces both description-based and example-based prompting, integrating speech and sound generation paradigms to independently control transcript, vocal, and other audio styles during speech generation.
VoiceLDM~\cite{lee2024voiceldm} and AST-LDM~\cite{kim2024speak} extend AudioLDM~\cite{liu2023audioldm} to incorporate environmental context in TTS generation by adding a content prompt as a conditional input.
Building on VoiceLDM, MS$^{2}$KU-VTTS~\cite{he2024multi} further expands the dimensions of environmental perception, enhancing the generation of immersive spatial speech.

</td></tr></table>

### Instruction-Guided Control: 指令引导控制

<table><tr><td width="50%">

The natural language description-based TTS methods discussed above require splitting inputs into content and description prompts, which limits fine-grained control over speech and does not align with other AIGC models.
[VoxInstruct [103]](../../Models/SpeechLM/2024.08.28_VoxInstruct.md) proposes a new paradigm that extends traditional text-to-speech tasks into a general human instruction-to-speech task.
Here, human instructions are freely written in natural language, encompassing both the spoken content and descriptive information about the speech.
To enable automatic extraction of the synthesized speech content from raw text instructions, VoxInstruct uses speech semantic tokens as an intermediate representation, bridging the gap in current research by allowing the simultaneous use of both text description prompts and speech prompts for speech generation.

</td></tr></table>

# 5·Datasets and Evaluation: 数据集与评估

## A·Datasets: 数据集

<table><tr><td width="50%">

Achieving fully controllable TTS requires large-scale datasets with rich diversity and fine-grained annotations.
In this subsection, we categorize speech datasets into four types according to the labels they provide, i.e., attribute tags such as age and gender, description, environment, and dialogues.

</td></tr></table>

### Tag-Based Speech Datasets: 基于标记的语音数据集

<table><tr><td width="50%">

Tag-based datasets ([Libri-Light [255]](../../Datasets/2019.12.17_Libri-Light.md); [AISHELL-3 [256]](../../Datasets/2020.10.22_AISHELL-3.md); [ESD [257]](../../Datasets/2020.10.28_ESD.md); [GigaSpeech](../../Datasets/2021.06.13_GigaSpeech.md); [WenetSpeech [259]](../../Datasets/2021.10.07_WenetSpeech.md); [DailyTalk [260]](../../Datasets/DailyTalk.md); [MSceneSpeech [261]](../../Datasets/2024.07.19_MSceneSpeech.md)) are specialized collections of speech data that include metadata tags, such as pitch, emotion, and energy, age, gender, and so on, to enhance the expressiveness and control of TTS systems.
These datasets not only provide audio-text pairs but also offer additional labels that guide the model in generating diverse and context-aware speech outputs.

</td></tr></table>

### Description-Based Speech Datasets: 基于描述的语音数据集

<table><tr><td width="50%">

Description-based TTS datasets ([PromptTTS [101]](../../Models/Acoustic/2022.11.22_PromptTTS.md); [TextrolSpeech](../../Datasets/2023.08.28_TextrolSpeech.md); [VoiceLDM [231]](../../Datasets/VoiceLDM_Data.md); [ControlSpeech [106]](../../Models/SpeechLM/2024.06.03_ControlSpeech.md); [SpeechCraft](../../Datasets/2024.08.24_SpeechCraft.md)) contain speech data paired with detailed textual descriptions of the speech attributes or characteristics, such as emotion, speed, intonation, and style.
These datasets enable training TTS models that interpret descriptive prompts to generate expressive and context-aware speech.

</td></tr></table>

### Speech Environment Datasets: 语音环境数据集

<table><tr><td width="50%">

Speech environment datasets ([VoiceLDM [231]](../../Datasets/VoiceLDM_Data.md); [Fifth CHiME [268]](../../Datasets/2018.03.28_Fifth_CHiME.md)) are collections of speech data annotated with environmental labels, such as park, stadium, office, or street, capturing the acoustic characteristics of various real-world settings.
These datasets are crucial for training models to handle diverse acoustic scenarios, improving their robustness and adaptability.

</td></tr></table>

### Dialogue Speech Datasets: 对话语音数据集

<table><tr><td width="50%">

Dialogue speech datasets ([DailyTalk [260]](../../Datasets/DailyTalk.md)；[Taskmaster-1 [254]](../../Datasets/2019.09.01_Taskmaster-1.md)) capture conversational speech between two or more people, focusing on the natural flow, turn-taking, and context of human dialogue.
These datasets are crucial for training systems in applications like chatbots, virtual assistants, and interactive storytelling.

We summarize open-source speech datasets for controllable TTS in Table.05.
Labels: Pitch, Energy, Speed, Age, Gender, Emotion, Emphasis, Accent, Top., Description, Environment, Dialogue.

</td></tr></table>

|Dataset|Hours|#Speakers|Pitch|Energy|Speed|Age|Gender|Emotion|Emphasis|Accent|Top.|Description|Environment|Dialogue|Language|Release Time|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|[Taskmaster-1 [254]](../../Datasets/2019.09.01_Taskmaster-1.md)| / | /|  |  |  |  |  |  |  |  |  |  |  | √ | en|2019.09 |
|[Libri-Light [255]](../../Datasets/2019.12.17_Libri-Light.md)| 60,000 | 9,722 | | | | | | | | | √ | | | | en | 2019.12 |
|[AISHELL-3 [256]](../../Datasets/2020.10.22_AISHELL-3.md)| 85 | 218 | | | | √ | √ | | | √ | | | | | zh | 2020.10 |
|[ESD [257]](../../Datasets/2020.10.28_ESD.md)|29|10|  |  |  | |  | √ |  |  |  |  |  |  |en,zh |2021.05 |
|[GigaSpeech [258]](../../Datasets/2021.06.13_GigaSpeech.md)| 10,000 | / | |  |  |  |  |  |  |  | √ |  |  |  | en | 2021.06 |
|[WenetSpeech [259]](../../Datasets/2021.10.07_WenetSpeech.md)| 10,000 | / |  |  |  |  |  |  |  | |√ | | | | zh | 2021.07 |
|[PromptSpeech [101]](../../Datasets/PromptSpeech.md)| / | / | √ | √ | √ | |  | √ |  |  |  | √ |  |  | en | 2022.11 |
|[DailyTalk [260]](../../Datasets/DailyTalk.md)| 20 | 2|  |  |  |  |  | √ |  |  | √ |  |  | √ | en|2023.05 |
|[TextrolSpeech [247]](../../Datasets/2023.08.28_TextrolSpeech.md)| 330 | 1,324 | √ | √ | √ |  | √ | √ |  |  |  | √ |  |  | en  | 2023.08|
|[VoiceLDM [231]](../../Datasets/VoiceLDM_Data.md)| /| /| √ |  | |  | √ | √ |  |  |  | √ | √ |  |en | 2023.09|
|[VccmDataset [106]](../../Datasets/2024.06.03_VccmDataset.md)| 330| 1,324| √ | √ | √ |  | √ | √ |  |  |  | √ |  |  | en|2024.06|
|[MSceneSpeech [261]](../../Datasets/2024.07.19_MSceneSpeech.md)| 13 | 13 |  |  |  |  | |  |  |  | √ |  |  | | zh | 2024.07 |
|[SpeechCraft [262]](../../Datasets/2024.08.24_SpeechCraft.md)| 2,391 | 3,200 | √ | √ | √ | √ | √ | √ | √ | | √ | √ | | | en,zh | 2024.08 |

## B·Evaluation: 评估

<table><tr><td width="50%">

The performance of controllable TTS often requires objective and subjective evaluation.
We introduce common evaluation metrics in this subsection.

</td></tr></table>

### Objective Evaluation Metrics: 客观评估指标

<table><tr><td width="50%">

Objective metrics offer automated and reproducible evaluations.
[Mel Cepstral Distortion (MCD) [263]](../../Evaluations/MCD.md) measures the spectral distance between synthesized and reference speech, reflecting how closely the generated audio matches the target in terms of acoustic features.
For intelligibility, the [Word Error Rate (WER) [264]](../../Evaluations/WER.md) is used, comparing transcriptions of synthesized speech to the input text via automated speech recognition.
[Perceptual Evaluation of Speech Quality (PESQ) [265]](../../Evaluations/PESQ.md) is another objective metric designed to evaluate speech quality by comparing degraded audio with a clean reference.
It is widely used in telecommunications and speech synthesis, PESQ models human auditory perception, producing a score (typically 1–4.5) that reflects intelligibility and distortion under various conditions, including noise or compression.

</td></tr></table>

### Subjective Evaluation Metrics: 主观评估指标

<table><tr><td width="50%">

The [Mean Opinion Score (MOS) [266]](../../Evaluations/MOS.md) is the most commonly used subjective metric.
In MOS evaluations, listeners rate the naturalness of synthesized speech on a scale (e.g., 1 to 5), where higher scores indicate better quality.
MOS captures human perception effectively but requires substantial resources for large-scale evaluations.
[Comparison Mean Opinion Score (CMOS) [267]](../../Evaluations/CMOS.md) further evaluates relative quality differences between two TTS audio samples.
Participants listen to paired samples and rate their preference on a scale (e.g., -3 to +3, where negative values favor the first sample).
CMOS is used to measure subtle improvements in TTS systems, complementing absolute MOS ratings.

</td><td>

</td></tr>
<tr><td colspan="2">

Table.06 summarizes common evaluation metrics for TTS.

|Metric | Type | Eval Target | GT Required |
|---|----|----|----|
|[MCD [263]](../../Evaluations/MCD.md)| Objective | Acoustic similarity | √ |
|[PESQ [264]](../../Evaluations/PESQ.md)| Objective | Perceptual quality | √ |
|[WER [265]](../../Evaluations/WER.md)| Objective | Intelligibility | √ |
|[MOS [266]](../../Evaluations/MOS.md)| Subjective | Preference | |
|[CMOS [267]](../../Evaluations/CMOS.md)| Subjective | Preference |

</td></tr></table>

## 6·[Challenges & Future Directions: 挑战与未来方向](Sec.06_Challenges.md)

## 7·Discussion: 讨论

<a id="Sec.07"></a>

### A·Impacts of Controllable TTS: 可控 TTS 的影响

#### Applications: 应用

<table><tr><td width="50%">

Controllable TTS systems allow fine-grained manipulation of speech attributes such as pitch, emotion, speaking rate, and style, enabling a wide range of applications across industries.
One major application is in virtual assistants and customer support systems, where controllable TTS ensures tailored and context-aware responses.
For instance, a virtual assistant can speak in a calm tone for technical support but switch to an enthusiastic tone when presenting promotional offers, enhancing user experience.

In the entertainment industry, controllable TTS is invaluable for creating dynamic voiceovers, audiobooks, and gaming characters.
It enables precise adjustments in tone and delivery, allowing audiobooks to reflect character emotions and gaming characters to exhibit personality traits that align with their roles.
Similarly, in education, TTS systems can adapt speaking styles to suit different learners, such as adopting a slow, clear tone for language learning or an engaging, storytelling style for children’s content.

Controllable TTS is also transformative in assistive technologies, where it can generate speech that reflects the user’s intended emotion or personality.
This is particularly impactful for individuals with speech impairments, enabling more expressive and natural communication.

In content localization, controllable TTS systems adjust speech characteristics to suit regional and cultural preferences, ensuring a natural fit for global audiences.
Additionally, it finds applications in human-computer interaction research, enabling adaptive dialogue systems that modify speech dynamically based on user mood or environment.
By offering this flexibility, controllable TTS systems enhance accessibility, personalization, and engagement across various domains.

</td><td>

</td></tr></table>

#### Deepfakes: 深度伪造

<table><tr><td width="50%">

A deepfake is a type of synthetic media in which a person in an existing image, video, or audio recording is replaced with someone else’s likeness or voice.
This technology uses deep learning, particularly GANs~\cite{zhang2022deepfake}, to create highly realistic, but fabricated, content.
While deepfakes are most commonly associated with video manipulation, such as face swapping~\cite{nirkin2019fsgan}, they can also apply to audio, enabling the creation of synthetic speech that mimics a specific person’s voice, which is well known as voice cloning.

Voice cloning, especially few-shot~\cite{arik2018neural} and zero-shot TTS~\cite{wang2023neural,[MaskGCT [78]](../../Models/SpeechLM/ST2S/2024.09.01_MaskGCT.md)}, poses a significant threat to systems that rely on voice authentication, such as banking, customer service, and other identity verification processes.
With a convincing clone of someone’s voice, attackers could potentially impersonate individuals to gain unauthorized access to sensitive information or accounts.

To address these concerns, it’s essential to establish robust security protocols, consent-based regulations, and public awareness around voice cloning.
Furthermore, advancements in detecting voice clones are equally important to help distinguish genuine voices from synthesized ones, protecting both individuals and organizations from potential misuse.

</td><td>

</td></tr></table>

### B·Limitation of this Survey: 本综述的局限性

<table><tr><td width="50%">

Although we conduct a comprehensive survey of controllable TTS in this paper, there are some limitations we want to address in the future:
1) A unified benchmark method will be provided to evaluate controllable TTS methods.
2) Detailed analysis and control strategies of each specific speech attribute will be provided in an updated version of this paper.
3) The methodologies for feature disentanglement are crucial for controllable TTS but are not adequately discussed.

</td><td>

</td></tr></table>

## 8·Conclusions: 结论

<a id="Sec.08"></a>

<table><tr><td width="50%">

In this survey paper, we first elaborate on the general pipeline for controllable TTS, followed by a glimpse of uncontrollable TTS methods.
Then we comprehensively review existing controllable methods from the perspectives of model architectures and control strategies.
Popular datasets and commonly used evaluation metrics for controllable TTS are also summarized in this paper.
Besides, the current challenges are deeply analyzed and the promising future directions are also pointed out.
To the best of our knowledge, this is the first comprehensive survey for controllable TTS.

Writing a comprehensive survey is a challenging task and an iterative process.
Hence, we will regularly update this survey to offer researchers and practitioners a continuously evolving resource for understanding controllable speech synthesis.

</td><td>

在本综述中, 我们
- 首次阐述了可控 TTS 的一般流程, 并简要介绍了不可控 TTS 方法.
- 从模型架构和控制策略的角度全面回顾了现有的可控方法.
- 总结了可控 TTS 常用的数据集和评价指标.
- 深度分析了现有挑战, 并指出了未来发展方向.

据我们所知, 这是关于可控 TTS 的首篇全面综述.

撰写一篇全面综述是一个具有挑战性的任务, 也是一项迭代式的过程.
因此, 我们将不断更新本综述, 为研究者和从业者提供可持续发展的可控语音合成资源.

</td></tr></table>
