## 2.方法

The last few years have seen rapid growth in expressive and emotional speech synthesis approaches, resulting in a large number of papers and publications in this area. Here, we present the outcomes of a systematic literature review of the last 5 years’ publications within this active research area. This section describes the methodology used to conduct the review, illustrated by Fig.03, which consists of three main stages: paper selection, paper exclusion, and paper classification.

### 2.1.论文选择

For our review, we used the Scopus database to retrieve papers as it encompasses most of the significant journals and conferences pertaining to the speech synthesis field. Our query criteria to find relevant papers on Scopus were twofold: (1) the paper title must include at least one of four words (emotion* OR expressive OR prosod* OR style) that denote expressive speech, and (2) the paper title, abstract, or keywords must comprise the terms “speech” AND “synthesis,” in addition to at least one of the above-mentioned words for expressive speech.
We considered all papers written in English and published in journals or conferences since 2018. The search query was conducted in January 2023, and it yielded 356papers. Scopus provides an Excel file containing all the primary information of the retrieved papers, which we used in the second stage of our review.

### 2.2.论文排除

The exclusion of papers occurred in two phases. In the first phase, we screened the abstract text, while in the second phase, we screened the full text of the paper. Five main constraints were used to exclude papers, including (1) papers that were not related to the TTS field, (2)papers that were not DL-based models, (3) papers that did not focus on expressive or emotional TTS models, (4)papers that were too specific to non-English languages,and (5) papers that lacked details about the applied method. After screening the paper abstracts, we excluded180 papers, mostly based on the first exclusion criterion.
During the second exclusion phase, in which we read the full text of each paper, we identified another 65 papers that met at least one of the five exclusion criteria. Consequently, 111 papers were included in the third stage of our review. Additionally, a group of recently published papers in this area ([17,18,19,20] [InstructTTS (2023)](../../Models/_tmp/2023.01.31_InstructTTS.md), [22,23, [DiffProsody (2023)](../../Models/E2E/2023.07.31_DiffProsody.md), [VoiceBox (2023)](../../Models/Speech_LLM/2023.06.23_VoiceBox.md)) was hand-picked and added to the final set of selected papers. While most of the reviewed papers trained their models on English data, a few other papers used data in other languages as listed in Table 1.

### 2.3.论文分类

After summarizing the approach proposed for generating expressive speech in each selected paper, we categorized the papers based on the learning approach applied in each one. Accordingly, papers are divided into two main categories, including supervised and unsupervised approaches. Under the supervised category, where labeled data is utilized, we identified three subcategories based on how models are employed expressive speech synthesis. The three proposed subcategories are (1)labels as input features, (2) labels as separate layers or models, and (3) labels for emotion predictors/classifiers.

Papers in the unsupervised approaches category are grouped into four different subcategories based on the main structure or method used for modeling expressivity in these papers. From our observation, most of the proposed methods in the last 5 years are based on three main early works in this field, namely, reference encoder [74], global style tokens[75], and latent features via variational autoencoders (VAE)[76] [77]. Specifically, proposed models in most of the papers under this category can be considered as an extension or enhancement of one of the three previously mentioned methods. Besides, we identify a fourth subcategory that includes the recent TTS models representing the new trend in the TTS area, which utilizes in-context learning. There is one factor common to all these four unsupervised models,which is that they are all based on using an audio reference/prompt. Additionally, we added a fifth subcategory (named other approaches) in which we include approaches outside the previous four main unsupervised approaches. Fig.04 illustrates the proposed classification schema for the DL-based expressive speech synthesis models.