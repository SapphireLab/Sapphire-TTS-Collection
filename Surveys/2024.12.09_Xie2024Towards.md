# Towards Controllable Speech Synthesis in the Era of Large Language Models: A Systematic Survey

<details>
<summary>基本信息</summary>

- 标题: "Towards Controllable Speech Synthesis in the Era of Large Language Models: A Systematic Survey."
- 作者:
  - 01 Tianxin Xie
  - 02 Yan Rong
  - 03 Pengfei Zhang
  - 04 Wenwu Wang
  - 05 Li Liu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.06602v3)
  - [Publication]()
  - [Github](https://github.com/imxtx/awesome-controllabe-speech-synthesis)
  - [Demo]()
- 文件:
  - [ArXiv:2412.06602v1](PDF/2024.12.09_2412.06602v1__Survey__Towards_Controllable_Speech_Synthesis_in_the_Era_of_Large_Language_Models__A_Survey.pdf)
  - [ArXiv:2412.06602v2](PDF/2025.03.27_2412.06602v2__Survey__Towards_Controllable_Speech_Synthesis_in_the_Era_of_Large_Language_Models__A_Survey.pdf)
  - [ArXiv:2412.06602v3](PDF/2025.08.25_2412.06602v3__Survey__Towards_Controllable_Speech_Synthesis_in_the_Era_of_Large_Language_Models__A_Systematic_Survey.pdf)
  - [Publication] #TODO

</details>

## Abstract

<table><tr><td width="50%">

Text-to-speech (TTS) has advanced from generating natural-sounding speech to enabling fine-grained control over attributes like emotion, timbre, and style.
Driven by rising industrial demand and breakthroughs in deep learning, e.g., diffusion and large language models (LLMs), controllable TTS has become a rapidly growing research area.
This survey provides **the first** comprehensive review of controllable TTS methods, from traditional control techniques to emerging approaches using natural language prompts.
We categorize model architectures, control strategies, and feature representations, while also summarizing challenges, datasets, and evaluations in controllable TTS.
This survey aims to guide researchers and practitioners by offering a clear taxonomy and highlighting future directions in this fast-evolving field.
One can visit [Github](https://github.com/imxtx/awesome-controllabe-speech-synthesis) for a comprehensive paper list and updates.

</td><td>

文本转语音技术已从生成自然流畅的语音, 发展到能够对情感, 音色和风格等属性进行细粒度控制.
在日益增长的工业需求以及深度学习领域的突破 (如扩散模型和大语言模型) 推动下, 可控 TTS 已成为一个快速发展的研究领域.
本综述提供了**首个**关于可控 TTS 方法的全面回顾, 涵盖了从传统控制技术到利用自然语言提示的新兴方法.
我们对模型架构, 控制策略, 特征表示进行了分类, 同时总结了可控 TTS 中的挑战, 数据集, 评估方法.
本综述旨在通过提供清晰的分类体系, 并指出这一快速演进领域的未来发展方向, 为研究人员和从业者提供指导.
读者可访问 [Github](https://github.com/imxtx/awesome-controllabe-speech-synthesis) 获取全面的论文列表及最新动态.

</td></tr></table>

## 1·Introduction

<table><tr><td width="50%">

Speech synthesis, also known as text-to-speech (TTS), aims to generate human-like speech from text [^Dutoit1997introduction], and has found broad applications in personal assistants [^Lopez2018alexa], entertainment [^Wang2019comic], and robotics [^Marge2022spoken].
Recently, the success of large language models such as ChatGPT [^Openai2022chatgpt] has renewed interest in TTS for natural and intuitive human-computer interaction.
Meanwhile, fine-grained control over speech attributes, such as emotion, timbre, and style, has become a key focus in both academia and industry, unlocking more expressive and personalized voice generation.

</td><td>

</td></tr>
<tr><td>

In the past decade, deep learning has driven remarkable advances in TTS, enabling high-quality synthesis (**NaturalSpeech**[^Tan2022NaturalSpeech], **FastSpeech**[^Ren2019FastSpeech], **CosyVoice**[^Du2024CosyVoice]) and stronger control over speech attributes ([^Wang2018style], [^Li2021controllable], **VoxInstruct**[^Zhou2024VoxInstruct]).
Recent methods have expanded TTS to multi-modal inputs, including images ([^Rong2025seeing]) and videos ([^Choi2023diffv2s]).
Meanwhile, the rise of LLMs ([^Zhao2023survey]) has enabled controllable TTS guided by language prompts ([^Guo2023prompttts], [^Huang2024instructspeech]), opening new possibilities for customized voice synthesis.
Integrating TTS into LLMs has also gained extensive attention ([^Peng2024survey]).
This rapid progress underscores the need for a comprehensive and timely survey to clarify current trends and guide future directions in controllable TTS.

</td><td>

</td></tr>
<tr><td>

While several surveys have examined parametric [^Zen2009statistical] and deep learning–based TTS [^Triantafyllopoulos2023overview_survey], they overlook TTS controllability and recent advances such as description–based methods ([^Guo2023prompttts], [^Yamamoto2024description]).
The key differences between our survey and earlier work are:
**1) Different Scope**: [^Klatt1987review] provided the first review of formant-based, concatenative, and articulatory TTS, with a strong focus on text analysis.
Later, [^Tabet2011speech], [^King2014measuring] explored statistics-based techniques.
With the advent of neural networks, [^Ning2019review], **Survey20210629**[^Tan2021Survey], **Survey20230323**[^Zhang2023Survey] surveyed neural model–based TTS, focusing on acoustics and vocoders.
However, they rarely discuss the controllability.
**2) Closer to Current Demands**: The need for controllable TTS is rapidly growing in industries like filmmaking, gaming, robotics, and virtual assistants.
Yet, existing surveys rarely explore the gaps between current control techniques and real-world demands.

</td><td>

</td></tr>
<tr><td>

To fill this gap, we present the first comprehensive survey of emerging controllable TTS methods.
We first define the core tasks (Sec.~[sec:tasks](#sec:tasks)) and, as shown in Fig.~[Fig.01](#fig:summary), trace the evolution of methods across model architectures (Sec.~[sec:archs](#sec:archs)), control strategies (Sec.~[sec:strategies](#sec:strategies)), and feature representations (Sec.~[sec:features](#sec:features)).
We further summarize relevant datasets and evaluation metrics (Sec.~[sec:datasets_eval](#sec:datasets_eval)), and discuss current challenges and future research directions (Sec.~[sec:directions](#sec:directions)).
For a history of controllable TTS and an overview of the TTS pipeline, see Appendices [sec:appd_history](#sec:appd_history) and [sec:appd_pipeline](#sec:appd_pipeline).

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="fig:summary">![](images/summary.pdf)</a>

</td></tr>
<tr><td>

Fig.01: Recent trends in controllable TTS regarding architectures, feature representations, and control abilities.

</td><td>

</td></tr></table>

## 2·Main Tasks in Controllable TTS

<table><tr><td width="50%">

**Prosody Control** is the most basic task in controllable TTS, aiming to manipulate low-level acoustic features such as pitch (**FastPitch**[^Lancucki2020FastPitch]), duration (**Spark-TTS**[^Wang2025Spark-TTS]), and energy (**DrawSpeech**[^Chen2025DrawSpeech]).
Prosody control ensures naturalness and expressiveness in TTS and is essential for rendering emphasis, rhythm, and nuance in speech.

</td><td>

**韵律控制**是可控 TTS 中最基础的任务, 旨在调节音高 (**FastPitch**[^Lancucki2020FastPitch]), 时长 (**Spark-TTS**[^Wang2025Spark-TTS]) 和能量 (**DrawSpeech**[^Chen2025DrawSpeech]) 等低层次声学特征.
韵律控制确保了 TTS 语音的自然度和表现力, 对于实现语音中的重音, 节奏和细微情感表达至关重要.

</td></tr>
<tr><td>

**Timbre Control** aims to manipulate the acoustic characteristics that define voice quality (e.g., gender, age, nasality), enabling control over how a voice sounds beyond content and prosody.
It supports personalized TTS (**CosyVoice**[^Du2024CosyVoice]), voice conversion (**Vevo**[^Zhang2025Vevo]), and speaker identity editing ([^Huang2024instructspeech]).

</td><td>

</td></tr>
<tr><td>

**Emotion Control** aims to enable the synthesis of emotional speech by manipulating the affective state of the generated voice ([^Kim2021expressive]).
This improves human-computer interaction, storytelling ([^Rong2025dopamine]), and supports emotionally adaptive systems such as virtual assistants.

</td><td>

</td></tr>
<tr><td>

**Style Control** aims to control higher-level attributes of speech such as tone, formality, and discourse mode (e.g., newscast) ([^Zhou2024VoxInstruct], [^Yang2024instructtts]).
This is critical for adapting the speaking behavior of TTS systems to different contexts, audiences, and communication goals.

</td><td>

</td></tr>
<tr><td>

**Language Control** aims to enable TTS systems to synthesize speech in multiple languages (**VALL-E X**[^Zhang2023VALL-EX]), dialects ([^Di2024bailing]), or code-switched contexts (**F5-TTS**[^Chen2024F5-TTS]).
It facilitates cross-lingual communication, multilingual agents, and regionally tailored speech applications.

</td><td>

</td></tr>
<tr><td>

**Environment Control** aims to simulate the acoustic characteristics of a specific setting, such as a park, office, or seaside, by conditioning synthesis on background noise and spatial cues ([^Lee2023VoiceLDM], [^Kim2024speak]).
Speech environment control is useful in filmmaking and audiobooks.

</td><td>

</td></tr></table>

## 3·Methods in Controllable TTS

<table><tr><td width="50%">

This section reviews controllable TTS from three perspectives: model architectures, feature representations, and control strategies, as shown in Fig.~[Fig.01](#fig:summary).

</td><td>

</td></tr></table>

### 3.1·Model Architectures

<table><tr><td width="50%">

The architectures of controllable TTS are primarily divided into two types, i.e., non-autoregressive (NAR) and autoregressive (AR) (See Table [tab:methods_all](#tab:methods_all)).

</td><td>

</td></tr>
<tr><td colspan="2">

| NAR Method | Zero-Shot TTS | Pitch | Energy | Speed | Prosody | Timbre | Emotion | Environment | Description | Acoustic Model | Vocoder | Acoustic Feature | Release Time |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|**FastSpeech**[^Ren2019FastSpeech] | | | | √ | √ | | | | | Transformer | **WaveGlow**[^Prenger2018WaveGlow] | MelS | 2019.05 |
|**FastSpeech2**[^Ren2020FastSpeech2] | | √ | √ | √ | √ | | | | | Transformer | Parallel WaveGAN [^Yamamoto2020parallelwavegan] | MelS | 2020.06 |
|FastPitch [^Lancucki2020FastPitch] | | √ | | | √ | | | | | Transformer | WaveGlow | MelS | 2020.06 |
|Parallel Tacotron [^Elias2021parallel] | | | | | √ | | | | | Transformer + VAE + CNN | WaveRNN [^Kalchbrenner2018wavernn] | MelS | 2020.10 |
|StyleTagging-TTS [^Kim2021expressive] | √ | | | | | √ | √ | | | Transformer + CNN | **HiFi-GAN**[^Kong2020HiFi-GAN] | MelS | 2021.04 |
|SC-GlowTTS [^Casanova2021sc] | √ | | | | | √ | | | | Transformer + Flow | HiFi-GAN | MelS | 2021.06 |
|Meta-StyleSpeech [^Min2021meta] | √ | | | | | √ | | | | Transformer | **MelGAN**[^Kumar2019MelGAN] | MelS | 2021.06 |
|**DelightfulTTS**[^Liu2021DelightfulTTS] | | √ | | √ | √ | | | | | Transformer + CNN | HiFiNet **DelightfulTTS**[^Liu2021DelightfulTTS] | MelS | 2021.11 |
|**YourTTS**[^Casanova2021YourTTS] | √ | | | | | √ | | | | Transformer + Flow | HiFi-GAN | LinS | 2021.12 |
|StyleTTS [^Li2025styletts] | √ | | | | | √ | | | | CNN + RNN | HiFi-GAN | MelS | 2022.05 |
|GenerSpeech [^Huang2022generspeech] | √ | | | | | √ | | | | Transformer + Flow | HiFi-GAN | MelS | 2022.05 |
|Cauliflow [^Abbas2022expressive] | | | | √ | √ | | | | | BERT + Flow | UP WaveNet [^Jiao2021upwavenet] | MelS | 2022.06 |
|CLONE [^Liu2022controllable] | | √ | | √ | √ | | | | | Transformer + CNN | **WaveNet**[^Oord2016WaveNet] | MelS + LinS | 2022.07 |
|PromptTTS [^Guo2023prompttts] | | √ | √ | √ | √ | √ | √ | | √ | BERT + Transformer | HiFi-GAN | MelS | 2022.11 |
|Grad-StyleSpeech [^Kang2023grad] | √ | | | | | √ | | | | Score-based Diffusion | HiFi-GAN | MelS | 2022.11 |
|**NaturalSpeech2**[^Shen2023NaturalSpeech2] | √ | | | | | √ | | | | Diffusion | RVQ-based **NaturalSpeech2**[^Shen2023NaturalSpeech2] | Latent Feature | 2023.04 |
|PromptStyle [^Liu2023promptstyle] | √ | √ | | | √ | √ | √ | | √ | VITS + Flow | HiFi-GAN | MelS | 2023.05 |
|StyleTTS 2[^Li2023styletts2] | √ | | | | √ | √ | √ | | | Flow-based Diffusion + GAN | HifiGAN / iSTFTNet [^Kaneko2022istftnet] | MelS | 2023.06 |
|**VoiceBox**[^Le2023VoiceBox] | √ | | | | | √ | | | | Transformer + Flow | HiFi-GAN | MelS | 2023.06 |
|MegaTTS 2[^Jiang2024mega] | √ | | | | √ | √ | √ | | | Decoder-only Transformer + GAN | HiFi-GAN | MelS | 2023.07 |
|PromptTTS 2[^Leng2023prompttts2] | | √ | √ | √ | √ | √ | | | √ | Diffusion | RVQ-based [^Leng2023prompttts2] | Latent Feature | 2023.09 |
|**VoiceLDM**[^Lee2023VoiceLDM] | | √ | | | √ | √ | √ | √ | √ | Diffusion | HiFi-GAN | MelS | 2023.09 |
|DurIAN-E [^Gu2023durian] | | √ | | √ | √ | | | | | CNN + RNN | HiFi-GAN | MelS | 2023.09 |
|PromptTTS++[^Shimizu2024prompttts++] | | √ | | √ | √ | √ | √ | | √ | Transformer + Diffusion | **BigVGAN**[^Lee2022BigVGAN] | MelS | 2023.09 |
|SpeechFlow [^Liu2023generative] | √ | | | | | √ | | | | Transformer + Flow | HiFi-GAN | MelS | 2023.10 |
|P-Flow [^Kim2024p] | √ | | | | | √ | | | | Transformer + Flow | HiFi-GAN | MelS | 2023.10 |
|E3 TTS [^Gao2023e3] | √ | | | | | √ | | | | Diffusion | \color{gray}{Not required} | Waveform | 2023.11 |
| **HierSpeech++**[^Lee2023HierSpeech++] | √ | | | | | √ | | | | Transformer + VAE + Flow | BigVGAN | MelS | 2023.11 |
|Audiobox [^Vyas2023audiobox] | √ | √ | | √ | √ | √ | | √ | √ | Transformer + Flow | HiFi-GAN | MelS | 2023.12 |
|FlashSpeech [^Ye2024flashspeech] | √ | | | | | √ | | | | Latent Consistency Model | EnCodec | Token | 2024.04 |
|**NaturalSpeech3**[^Ju2024NaturalSpeech3] | √ | | | √ | √ | √ | | | | Transformer + Diffusion | FACodec **NaturalSpeech3**[^Ju2024NaturalSpeech3] | Token | 2024.04 |
|InstructTTS [^Yang2024instructtts] | | √ | | √ | √ | √ | √ | | √ | Transformer + Diffusion | HiFi-GAN | Token | 2024.05 |
|ControlSpeech [^Ji2024controlspeech] | √ | √ | √ | √ | √ | √ | √ | | √ | Transformer + Diffusion | FACodec | Token | 2024.06 |
|AST-LDM [^Kim2024speak] | | | | | | √ | | √ | √ | Diffusion + VAE | HiFi-GAN | MelS | 2024.06 |
|SimpleSpeech [^Yang2024simplespeech] | √ | | | | | √ | | | | Transformer + Diffusion | SQ Codec [^Yang2024simplespeech] | Token | 2024.06 |
|**DiTTo-TTS**[^Lee2024DiTTo-TTS] | √ | | | √ | | √ | | | | DiT + VAE | BigVGAN | MelS | 2024.06 |
|E2 TTS **E2-TTS**[^Eskimez2024E2-TTS] | √ | | | | | √ | | | | Transformer + Flow | BigVGAN | MelS | 2024.06 |
|**MobileSpeech**[^Ji2024MobileSpeech] | √ | | | | | √ | | | | Transformer | Vocos [^Siuzdak2024vocos] | Token | 2024.06 |
|DEX-TTS [^Park2024dex] | √ | | | | | √ | | | | Diffusion | HiFi-GAN | MelS | 2024.06 |
|ArtSpeech [^Wang2024artspeech] | √ | | | | | √ | | | | RNN + CNN | HiFI-GAN | MelS + Energy + F0 | 2024.07 |
|CCSP [^Xiao2024contrastive] | √ | | | | | √ | | | | Diffusion | RVQ-based [^Xiao2024contrastive] | Token | 2024.07 |
|SimpleSpeech 2[^Yang2024simplespeech2] | √ | | | √ | | √ | | | | Flow-based DiT | SQ Codec | Token | 2024.08 |
|E1 TTS [^Liu2024e1] | √ | | | | | √ | | | | DiT + Flow | BigVGAN | Token + MelS | 2024.09 |
|StyleTTS-ZS [^Li2024stylettszs] | √ | | | | | √ | | | | Flow-based Diffusion + GAN | Mel-based Decoder [^Li2024stylettszs] | MelS | 2024.09 |
|NansyTTS [^Yamamoto2024description] | √ | √ | | √ | √ | √ | | | √ | Transformer | NANSY++[^Yamamoto2024description] | MelS | 2024.09 |
|NanoVoice [^Park2024nanovoice] | √ | | | | | √ | | | | Diffusion | BigVGAN | MelS | 2024.09 |
|MS$^{2}$KU-VTTS [^He2024multi] | | | | | | | | √ | √ | Transformer | BigVGAN | MelS | 2024.10 |
|**MaskGCT**[^Wang2024MaskGCT] | √ | | | √ | | √ | | | | Transformer + Flow | Vocos | Token | 2024.10 |
|EmoSphere++[^Cho2024emosphere++] | √ | | | | √ | √ | √ | | | Transformer + Flow | BigVGAN | MelS | 2024.11 |
|EmoDubber [^Cong2024emodubber] | √ | | | | √ | √ | √ | | | Transformer + Flow | Flow-based [^Cong2024emodubber] | MelS | 2024.12 |
|HED [^Inoue2024HED] | √ | | | | | | √ | | | Flow-based Diffusion | Vocos | MelS | 2024.12 |
|DiffStyleTTS [^Liu2025diffstyletts] | | √ | √ | √ | √ | √ | | | | Transformer + Diffusion | HiFi-GAN | MelS | 2025.01 |
|**DrawSpeech**[^Chen2025DrawSpeech] | | | √ | | √ | | | | | Diffusion | HiFi-GAN | MelS | 2025.01 |
|ProEmo [^Zhang2025proemo] | | √ | √ | | | | √ | | √ | Transformer | HiFi-GAN | MelS | 2025.01 |

| AR Method | Zero-Shot TTS | Pitch | Energy | Speed | Prosody | Timbre | Emotion | Environment | Description | Acoustic Model | Vocoder | Acoustic Feature | Release Time |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|Prosody-Tacotron [^Skerry2018towards] | | √ | | | √ | | | | | RNN | WaveNet | MelS | 2018.03 |
|GST-Tacotron [^Stanton2018predicting] | | √ | | | √ | | | | | CNN + RNN | Griffin-Lim | LinS | 2018.03 |
|**GMVAE-Tacotron**[^Hsu2018GMVAE-Tacotron] | | √ | | √ | √ | | | √ | | VAE | WaveRNN | MelS | 2018.12 |
|VAE-Tacotron [^Zhang2019learning] | | √ | | √ | √ | | | | | VAE + CNN + RNN | WaveNet | MelS | 2019.02 |
|DurIAN [^Yu2020durian] | | √ | | √ | √ | | | | | CNN + RNN | Multi-band WaveRNN [^Yu2020durian] | MelS | 2019.09 |
|Flowtron [^Valle2020flowtron] | | √ | | √ | √ | | | | | CNN + RNN | WaveGlow | MelS | 2020.07 |
|MsEmoTTS [^Lei2022msemotts] | | √ | | | √ | | √ | | | CNN + RNN | WaveRNN | MelS | 2022.01 |
|**VALL-E**[^Wang2023VALL-E] | √ | | | | | √ | | | | Decoder-only Transformer | EnCodec | Token | 2023.01 |
|SpearTTS **SPEAR-TTS**[^Kharitonov2023SPEAR-TTS] | √ | | | | | √ | | | | Decoder-only Transformer | **SoundStream**[^Zeghidour2021SoundStream] | Token | 2023.02 |
|**VALL-E X**[^Zhang2023VALL-EX] | √ | | | | | √ | | | | Decoder-only Transformer | EnCodec | Token | 2023.03 |
|Make-A-Voice [^Huang2023makeavoice] | √ | | | | | √ | | | | Encoder-decoder Transformer | Unit-based [^Huang2023makeavoice] | Token | 2023.05 |
|TorToise **TorToiSe-TTS**[^Betker2023TorToiSe-TTS] | | | | | | √ | | | | Decoder-only Transformer + Diffusion | UnivNet [^JangLYKK21univnet] | MelS | 2023.05 |
|MegaTTS [^Jiang2023megavoic] | √ | | | | | √ | | | | Decoder-only Transformer + GAN | HiFi-GAN | MelS | 2023.06 |
|**SC VALL-E**[^Kim2023SCVALL-E] | √ | √ | | √ | √ | √ | √ | | | Decoder-only Transformer | EnCodec | Token | 2023.07 |
|Salle **TextrolSpeech**[^Ji2023TextrolSpeech] | | √ | √ | √ | √ | √ | √ | | √ | Decoder-only Transformer | EnCodec | Token | 2023.08 |
|**UniAudio**[^Yang2023UniAudio] | √ | √ | | √ | √ | √ | | | √ | Decoder-only Transformer | EnCodec | Token | 2023.10 |
|**ELLA-V**[^Song2024ELLA-V] | √ | | | | | √ | | | | Decoder-only Transformer | EnCodec | Token | 2024.01 |
|Base TTS **BASE TTS**[^Lajszczak2024BASE-TTS] | √ | | | | | √ | | | | Decoder-only Transformer | HiFi-GAN + BigVGAN | Token | 2024.02 |
|CLaM-TTS [^Kim2024CLaM-TTS] | √ | | | | | √ | | | | Encoder-decoder Transformer | BigVGAN | Token + MelS | 2024.04 |
|**RALL-E**[^Xin2024RALL-E] | √ | | | | | √ | | | | Decoder-only Transformer | SoundStream | Token | 2024.05 |
|**ARDiT**[^Liu2024ARDiT] | √ | | | √ | | √ | | | | Decoder-only DiT | BigVGAN | MelS | 2024.06 |
|**VALL-E R**[^Han2024VALL-ER] | √ | | | | | √ | | | | Decoder-only Transformer | Vocos | Token | 2024.06 |
| **VALL-E 2**[^Chen2024VALL-E2] | √ | | | | | √ | | | | Decoder-only Transformer | Vocos | Token | 2024.06 |
|**Seed-TTS**[^Anastassiou2024Seed-TTS] | √ | | | | | √ | √ | | | Decoder-only Transformer + DiT | \color{gray}{Unknown} | Latent Feature | 2024.06 |
|**VoiceCraft**[^Peng2024VoiceCraft] | √ | | | | | √ | | | | Decoder-only Transformer | HiFi-GAN | Token | 2024.06 |
|**XTTS**[^Casanova2024XTTS] | √ | | | | | √ | | | | Decoder-only Transformer | HiFi-GAN-based **XTTS**[^Casanova2024XTTS] | Token + MelS | 2024.06 |
|**CosyVoice**[^Du2024CosyVoice] | √ | √ | | √ | √ | √ | √ | | √ | Decoder-only Transformer + Flow | HiFi-GAN | Token | 2024.07 |
|**MELLE**[^Meng2024MELLE] | √ | | | | | √ | | | | Decoder-only Transformer | HiFi-GAN | MelS | 2024.07 |
|**VoxInstruct**[^Zhou2024VoxInstruct] | √ | √ | √ | √ | √ | √ | √ | | √ | Decoder-only Transformer | Vocos | Token | 2024.08 |
|Emo-DPO [^Gao2024emo] | | | | | | | √ | | | Decoder-only Transformer | HiFi-GAN | Token + MelS | 2024.09 |
|**FireRedTTS**[^Guo2024FireRedTTS] | √ | | | | √ | √ | | | | Decoder-only Transformer + Flow | BigVGAN | Token + MelS | 2024.09 |
|CoFi-Speech [^Guo2024CoFi-Speech] | √ | | | | | √ | | | | Decoder-only Transformer | BigVGAN | Token + MelS | 2024.09 |
|Takin [^Chen2024takin] | √ | √ | | √ | √ | √ | √ | | √ | Decoder-only Transformer + Flow | HiFi-GAN | Token + MelS | 2024.09 |
|**HALL-E**[^Nishimura2024HALL-E] | √ | | | | | √ | | | | Decoder-only Transformer | EnCodec | Token | 2024.10 |
|FishSpeech **Fish-Speech**[^Liao2024Fish-Speech] | √ | | | | | √ | | | | Decoder-only Transformer | Firefly-GAN **Fish-Speech**[^Liao2024Fish-Speech] | Token | 2024.11 |
|**SLAM-Omni**[^Chen2024SLAM-Omni] | √ | | | | √ | √ | | | | Decoder-only Transformer | HiFi-GAN | Token + MelS | 2024.12 |
|**IST-LM**[^Yang2024IST-LM] | √ | | | | √ | √ | | | | Decoder-only Transformer | HiFi-GAN | Token + MelS | 2024.12 |
|**KALL-E**[^Xia2024KALL-E] | √ | | | | √ | √ | √ | | | Decoder-only Transformer | WaveVAE **KALL-E**[^Xia2024KALL-E] | Latent Feature | 2024.12 |
|IDEA-TTS [^Lu2025idea-TTS] | √ | | | | | √ | | √ | | Transformer | Flow-based [^Lu2025idea-TTS] | LinS + MelS | 2024.12 |
|**FleSpeech**[^Li2025FleSpeech] | √ | √ | √ | √ | √ | √ | √ | | √ | Flow-based DiT | WaveGAN [^Donahue2018wavegan] | Latent Feature | 2025.01 |
|Step-Audio [^Huang2025stepaudio] | √ | | | | √ | √ | √ | | √ | Decoder-only Transformer | Flow-based [^Huang2025stepaudio] | Token | 2025.02 |
|**Vevo**[^Zhang2025Vevo] | √ | | | | √ | √ | √ | | | Decoder-only Transformer | BigVGAN | Token + MelS | 2025.02 |
|**Spark-TTS**[^Wang2025Spark-TTS] | √ | √ | | √ | √ | √ | | | | Decoder-only Transformer | BiCodec **Spark-TTS**[^Wang2025Spark-TTS] | Token | 2025.03 |
|EmoVoice [^Yang2025emovoice] | | | | | | | √ | | √ | Decoder-only Transformer | HiFi-GAN | Token | 2025.04 |

<a id="tab:methods_all"></a>

Tab.01: A summary of existing controllable neural-based methods.tab:methods_all

</td><td>

</td></tr></table>

#### 3.1.1·Non-Autoregressive Approaches

<table><tr><td width="50%">

Non-autoregressive TTS models generate the entire output speech sequence $\mathbf{y} = (y_1, y_2, \dots, y_T)$ in parallel given the input $\mathbf{x} = (x_1, x_2, \dots, x_T)$:

$$
\underset{\theta}{\arg\max} = P(\mathbf{y} | \mathbf{x}; \theta),
$$

where $\theta$ denotes model parameters.
In this part, we investigate the transformer, variational autoencoder (VAE), diffusion, and flow-based methods.

</td><td>

</td></tr>
<tr><td>

**Transformer-based Methods.**

Transformers enable efficient context modeling and parallel TTS.
**FastSpeech**[^Ren2019FastSpeech] introduced a non-autoregressive transformer that improves inference speed and stability via duration prediction.
**FastSpeech2**[^Ren2020FastSpeech2] adds pitch and energy control, removing the need for distillation and boosting voice quality.
FastPitch [^Lancucki2020FastPitch] further incorporates direct pitch prediction into its architecture, enabling pitch manipulation.

</td><td>

</td></tr>
<tr><td>

**VAE-based Methods.**

VAEs enable structured, continuous latent representations by optimizing a variational lower bound.
VAEs have been leveraged to enhance prosody, emotion, and style control.
[^Hsu2018GMVAE-Tacotron] proposed a hierarchical VAE to control noise and speaking rate.
[^Zhang2019learning] introduced disentangled VAE representations for effective prosody and emotion transfer.
Parallel Tacotron [^Elias2021paralleltacotron] uses a VAE-based residual encoder with iterative spectrogram loss to improve speech naturalness.
CLONE [^Liu2022controllable] further improves prosody and energy modeling using conditional VAEs with normalizing flows [^Kobyzev2020normalizing] and adversarial training, achieving state-of-the-art quality and control.
These advances underscore VAEs' versatility in expressive and controllable speech synthesis.

</td><td>

</td></tr>
<tr><td>

**Diffusion-based Methods.**

Diffusion models (**DDPM**[^Ho2020DDPM]) generate speech by reversing a noise injection process: noise is gradually added during the forward phase and removed in the reverse phase to synthesize high-quality audio.
**NaturalSpeech2**[^Shen2023NaturalSpeech2] uses a latent diffusion-based codec with quantized latent vectors, while **NaturalSpeech3**[^Ju2024NaturalSpeech3] decomposes speech into independent attribute subspaces with factorized diffusion-based codecs.
DEX-TTS [^Park2024dex] improves diffusion transformer (DiT)-based networks via overlapping patches and frequency-aware embeddings.
E3 TTS [^Gao2023e3] eliminates intermediate features by directly modeling waveforms through diffusion.
Text-to-audio models such as **AudioLDM**[^Liu2023AudioLDM] and **Make-An-Audio**[^Huang2023Make-An-Audio] can also generate speech using latent diffusion models.

</td><td>

</td></tr>
<tr><td>

**Flow-based Methods.**

Flow models use invertible flows [^Rezende2015variational], **FM**[^Lipman2022FM] to map speech features to simple distributions, typically Gaussians **WaveGlow**[^Prenger2018WaveGlow], enabling direct, high-fidelity generation via inversion.
Recent models adopt flow-matching **FM**[^Lipman2022FM] for efficient, non-autoregressive synthesis: Audiobox [^Vyas2023audiobox], P-Flow [^Kim2024p], and **VoiceBox**[^Le2023VoiceBox] consider TTS as speech infilling tasks, predicting masked mel-spectrograms.
FlashSpeech [^Ye2024flashspeech] trains a latent consistency model using adversarial training, achieving one- or two-step synthesis.
Inspired by audio infilling, **E2-TTS**[^Eskimez2024E2-TTS] uses filler-augmented text sequences to generate mel-spectrograms with human-level quality.
**F5-TTS**[^Chen2024F5-TTS] builds on this with ConvNeXt v2[^Woo2023convnext] to enhance text-speech alignment by directly learning flows conditioned on text and reference speech.
E1 TTS [^Liu2024e1] distills rectified flow-based diffusion models **RectifiedFlow**[^Liu2022RectifiedFlow] into one-step generators via distribution matching, reducing sampling cost.

</td><td>

</td></tr></table>

#### 3.1.2·Autoregressive Approaches

<table><tr><td width="50%">

Autoregressive TTS models predict the speech sequence $\mathbf{y} = (y_1, \dots, y_T)$ given input $\mathbf{x}$ as:

$$
\underset{\theta}{\arg\max} = \prod_{t=1}^{T} P(y_t | y_{<t}, \mathbf{x}; \theta),
$$

where each frame $y_t$ depends on all previous outputs $y_{<t}$ and the transcript $\mathbf{x}$.
While this enables effective modeling of implicit duration and long-range context, autoregressive TTS models suffer from slower inference, making them more suitable for applications where flexibility is prioritized over speed.
This part investigates recurrent neural networks (RNN) and LLM-based methods.

</td><td>

</td></tr>
<tr><td>

**RNN-based Methods.**

RNNs enable natural-sounding speech synthesis with adjustable prosody, pitch, and emotion.
Prosody-Tacotron [^Skerry2018towards] extends **Tacotron**[^Wang2017Tacotron] by introducing explicit prosodic controls.
[^Wang2018style] proposed Global Style Tokens (GST), enabling unsupervised style transfer.
Emotion-controllable models such as [^Li2021controllable] introduced emotion embeddings and style alignment to modulate emotional intensity.
MsEmoTTS [^Lei2022msemotts] refined this with a hierarchical structure capturing emotion at global, utterance, and local levels, enabling more nuanced synthesis.
These developments bring synthetic speech significantly closer to human expressiveness.

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="fig:llm-based">![](images/figures-llm-based.pdf)</a>

</td></tr>
<tr><td>

Fig.02: The typical architecture of LLM-based TTS.

</td><td>

</td></tr>
<tr><td>

**LLM-based Methods.**

LLM-based TTS is inspired by the success of in-context learning in LLMs.
As illustrated in Fig.~[Fig.02](#fig:llm-based), these approaches typically input target text or instructions with an optional reference speech, using autoregressive decoder-only transformers to generate speech tokens or features, which are then decoded into waveforms.
**VALL-E**[^Wang2023VALL-E] pioneered LLM-based zero-shot TTS by framing it as a conditional language modeling task.
It uses **EnCodec**[^Defossez2022EnCodec] to discretize waveforms into tokens and adopts a two-stage pipeline: an autoregressive model generates coarse audio tokens, followed by a non-autoregressive model for iterative refinement.
This hierarchical modeling of semantic and acoustic tokens has laid the groundwork for many subsequent methods, such as **VALL-E X**[^Zhang2023VALL-EX], **ELLA-V**[^Song2024ELLA-V], **RALL-E**[^Xin2024RALL-E], **VALL-E R**[^Han2024VALL-ER], **MELLE**[^Meng2024MELLE], and **HALL-E**[^Nishimura2024HALL-E].
Beyond the VALL-E series, recent work has further improved text-speech alignment, quality, and robustness.
SpearTTS **SPEAR-TTS**[^Kharitonov2023SPEAR-TTS] and Make-a-Voice [^Huang2023makeavoice] leverage semantic tokens to better bridge text and acoustic features.
**FireRedTTS**[^Guo2024FireRedTTS] refines the tokenizer architecture for improved reconstruction quality.
CoFi-Speech [^Guo2024CoFi-Speech] adopts a coarse-to-fine, multi-scale generation strategy to produce natural, intelligible speech.

</td><td>

</td></tr></table>

#### 3.1.3·Research Trend

<table><tr><td width="50%">

**Traditional CNN/RNN TTS models** face inherent constraints.
RNNs (e.g., Tacotron) are slow due to autoregressive inference and struggle with long-term dependencies.
CNNs (e.g., Deep Voice) lack global prosody modeling.
Both require explicit feature engineering for attributes like emotion and often trade synthesis quality for efficiency (e.g., WaveNet’s high fidelity but high latency).
**Flow-based models** (e.g., Matcha-TTS, F5-TTS) enable non-autoregressive, parallel synthesis with probabilistic control over acoustic features, improving speed and flexibility but increasing training complexity and dataset requirements.
LLM-based models (e.g., VALL-E, InstructTTS) offer natural language-driven control and zero-shot voice cloning, supporting context-aware synthesis, but suffer from high computational cost and potential acoustic artifacts from discrete tokenization.
**Hybrid architectures** (e.g., CosyVoice) integrate LLM-guided semantic conditioning into flow-based generators, combining high-fidelity synthesis with intuitive, instruction-based control.
Users can specify attributes like emotion or speaking style in natural language without compromising audio quality.
Future controllable TTS should balance efficiency, fidelity, and expressiveness, generalizing across voices, styles, and languages.
Bridging instruction-based control and acoustic precision remains a key challenge, motivating advances in modular architectures, instruction grounding, and speech-text-instruction alignment.
Fig.~[Fig.03](#fig:trend_arch) summarizes the evolution and future direction of TTS model architectures.

</td><td>

</td></tr></table>

### 3.2·Control Strategies

<table><tr><td width="50%">

As illustrated in Fig.~[fig:control_strategies](#fig:control_strategies), control strategies in TTS can be broadly categorized into four types: style tagging, reference speech prompt, natural language descriptions, and instruction-guided control.

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="fig:trend_arch">![](images/figures-trend_arch.pdf)</a>

</td></tr>
<tr><td>

Fig.03: The evolution of TTS model architectures

</td><td>

</td></tr></table>

#### 3.2.1·Style Tagging

<table><tr><td width="50%">

This paradigm enables the adjustment of key attributes such as pitch, energy, speech rate, and emotion, which can be controlled using either categorical labels or continuous values.
"Tagging'' refers to using a control signal to control a specific speech attribute.
1) Some approaches use **discrete labels** to control speech attributes.
For example, StyleTagging-TTS [^Kim2021expressive] denotes speech styles with short phrases or words (e.g., angry, happy), learning the relationship between linguistic and style embeddings.
Emo-DPO [^Gao2024emo] enables emotion control through Direct Preference Optimization (DPO)[^Rafailov2023dpo] with LLMs.
**Spark-TTS**[^Wang2025Spark-TTS] provides coarse and fine-grained control, allowing pitch and speaking rate modifications via specially designed tokens and reference speech.
2) Other methods adjust **continuous input signals**.
DiffStyleTTS [^Liu2025diffstyletts] models prosody hierarchically, enabling control over pitch, energy, duration, and style via guiding scale factors.
**DrawSpeech**[^Chen2025DrawSpeech] lets users sketch prosody contours, which are refined and converted into detailed speech by a diffusion model, offering control over intonation.
3) Speech attributes can be controlled by **modifying latent features**.
Cauliflow [^Abbas2022expressive] adjusts speech rate and pausing through a flow-based model conditioned on user-defined parameters.
**DiTTo-TTS**[^Lee2024DiTTo-TTS] uses a DiT to control speech rate by modifying latent length predictions.
These methods show great potential in controlling speech attributes by adjusting input signals or latent variables.
However, these methods are limited in expressive diversity, as they can only model a small set of pre-defined attributes.

</td><td>

</td></tr></table>

#### 3.2.2·Reference Speech Prompt

<table><tr><td width="50%">

This paradigm aims to customize the synthesized voice using only a few seconds of reference speech.
Similar to LLM-based methods, it takes both text and reference speech as input to a conditional TTS model, which generates speech based on both semantic and acoustic features.
MetaStyleSpeech [^Min2021meta] employs adaptive normalization for style conditioning, enabling robust zero-shot performance.
GenerSpeech [^Huang2022generspeech] introduces a multilevel style adapter for improved zero-shot style transfer to out-of-domain custom voices.
**SC VALL-E**[^Kim2023SCVALL-E] integrates style tokens and scale factors for controlling emotion, speaking style, and other acoustic features in the generated speech.
DEX-TTS [^Park2024dex] separates time-invariant and time-variant style components, allowing the extraction of diverse styles.
StyleTTS-ZS [^Li2024stylettszs] uses distilled time-varying style diffusion to capture varied speaker identities and prosodies.
MegaTTS 2[^Jiang2024mega] introduces an acoustic autoencoder to separate prosody and timbre in the latent space, enabling style transfer to any timbre.
ControlSpeech [^Ji2024controlspeech] employs bidirectional attention and parallel decoding to control timbre, style, and content in a zero-shot manner.

</td><td>

</td></tr></table>

#### 3.2.3·Natural Language Descriptions

<table><tr><td width="50%">

Recent studies have explored controlling speech attributes using natural language descriptions, offering better user-friendliness.
PromptTTS [^Guo2023prompttts] uses manually annotated prompts to describe five key speech attributes.
InstructTTS [^Yang2024instructtts] introduces a three-stage training procedure to extract semantics from natural language prompts.
NansyTTS [^Yamamoto2024description] enables cross-lingual control by pairing a TTS model with a description controller trained on a different language using shared timbre and style representations.
To address the limitations of textual prompts in capturing speaker characteristics, PromptTTS++[^Shimizu2024prompttts++] enhances prompt richness by using additional speaker description prompts.
PromptTTS 2[^Leng2023prompttts2] introduces a variation network to model residual variability beyond the prompt.
Further efforts extend controllability to the environmental context.
**VoiceLDM**[^Lee2023VoiceLDM] and AST-LDM [^Kim2024speak] extend **AudioLDM**[^Liu2023AudioLDM] by incorporating content prompts to enable environmental conditioning.
MS$^{2}$KU-VTTS [^He2024multi] further enhances environmental perception by mixing environmental images into the prompt, enabling more immersive speech generation.

</td><td>

</td></tr></table>

#### 3.2.4·Instruction-Guided Synthesis

<table><tr><td width="50%">

Description-based TTS methods separate inputs into content and description prompts, diverging from the unified instruction formats used in chatbots.
To address this, **VoxInstruct**[^Zhou2024VoxInstruct] reframes TTS as a general instruction-to-speech task, where a single natural language prompt conveys both content and style descriptions.
**CosyVoice**[^Du2024CosyVoice] enhances this paradigm using supervised semantic tokens derived from ASR models.
It combines LLM-driven token generation with flow-matching synthesis, enabling precise control over speaker identity, emotion, pitch, speed, and paralinguistic cues through natural language instructions.
**AudioGPT**[^Huang2023AudioGPT] is a multimodal LLM-based agent, incorporating multiple modules for speech understanding, synthesis, and style conversion.
StepAudio [^Huang2025stepaudio] introduces a speech-text model with an instruction-driven TTS module, enabling dynamic control over dialects, emotions, singing, rapping, and speaking styles.
These advancements push toward more intuitive, instruction-driven speech generation.

</td><td>

</td></tr></table>

#### 3.2.5·Instruction-Guided Editing

<table><tr><td width="50%">

Some methods also support speech editing via user instructions.
**VoiceCraft**[^Peng2024VoiceCraft] uses a decoder-only transformer with causal masking and delayed stacking for bidirectional, context-aware instruction-guided editing, such as insertion, deletion, and substitution, while maintaining high naturalness.
InstructSpeech [^Huang2024instructspeech] trains a multi-task LLM on <instruction, input, output> triplets with task embeddings and hierarchical adapters, allowing content and acoustic attributes control.
It supports flexible, free-form speech editing and task adaptation by multi-step reasoning.

</td><td>

</td></tr></table>

#### 3.2.6·Research Trend

<table><tr><td width="50%">

The evolution of TTS control has moved from basic attribute manipulation to sophisticated, instruction-guided synthesis, reflecting AI’s trend toward intuitive, fine-grained control.
Early methods like **style tagging** controlled predefined attributes (e.g., pitch, emotion) but offered limited expressive diversity.
**Reference speech prompts** enabled zero-shot TTS and voice cloning, separating timbre from style for greater personalization.
To improve user-friendliness, **natural language descriptions** (e.g., PromptTTS) allowed users to specify vocal characteristics through text.
The latest advance, **instruction-guided control**, leverages LLMs to interpret free-form instructions combining content and style.
Systems like VoxInstruct and CosyVoice generate nuanced speech, including paralinguistic sounds, enabling highly precise, user-centric synthesis.
Overall, the progression from tags to natural instructions shows a clear trajectory toward more **expressive, personalized, and intuitive TTS**, driven by LLM integration.
Table~[Tab.01](#tab:appd_control_strategies) in the Appendix provides a summary of the strengths and weaknesses of each control strategy.

</td><td>

</td></tr></table>

### 3.3·Feature Representations

<table><tr><td width="50%">

\label{sec:features} The learning and choice of feature representations critically affect flexibility, naturalness, and controllability.
This subsection discusses speech attribute disentanglement and compares continuous and discrete representations, highlighting their trade-offs.

</td><td>

</td></tr>
<tr><td>

**Speech Attribute Disentanglement.**

Attribute disentanglement aims to isolate distinct speech factors, such as speaker identity, emotion, prosody, and content, into separate latent representations.
The two main approaches are: 1) **Adversarial training**[^Goodfellow2020gan] uses auxiliary classifiers to penalize the presence of unwanted attributes in a latent space.
An encoder learns to "fool'' these classifiers, resulting in representations that are invariant to specific attributes like speaker [^Yang2021ganspeech], [^Hsu2019Disentangling], [^Lee2021multi], emotion [^Li2022cross], and style [^Li2023styletts2].
2) **Information bottleneck** uses small-capacity or independent encoder branches to isolate attributes.
Each branch encodes one factor (e.g., content, prosody)[^Ju2024NaturalSpeech3], often with adversarial or reconstruction losses to discourage leakage of other information.
These methods are often combined.
Regularization via KL divergence [^Lu2023speechtriplenet] or quantization **Vevo**[^Zhang2025Vevo] also plays a key role in enforcing disentanglement.

</td><td>

</td></tr>
<tr><td>

**Continuous Representations.**

Continuous representations model speech in a continuous feature space, preserving acoustic details.
The key advantages are: 1) Fine-grained detail retention for natural and expressive synthesis; 2) Inherent encoding of prosody, pitch, and emotion, aiding controllable and emotional TTS; 3) Enable smooth audio reconstruction without quantization artifacts.
GAN-based **HiFi-GAN**[^Kong2020HiFi-GAN], [^Yamamoto2020parallelwavegan], VAE-based **HierSpeech++**[^Lee2023HierSpeech++], **DiTTo-TTS**[^Lee2024DiTTo-TTS], flow-based [^Kim2024pflow], **YourTTS**[^Casanova2021YourTTS], and diffusion-based methods **DiffWave**[^Kong2020DiffWave], [^Huang2022fastdiff] often utilize continuous feature representations.
However, they are computationally intensive and demand large models and datasets for high-fidelity audio generation.
**Discrete Tokens.** Discrete token-based TTS uses quantized units or phoneme-like tokens as acoustic features, which are often derived from quantization **SoundStream**[^Zeghidour2021SoundStream] or learned embeddings **HuBERT**[^Hsu2021HuBERT].
The advantages of discrete tokens are: 1) Discrete tokens can encode phonemes or sub-word units, making them concise and computationally efficient.
2) Discrete tokens often allow TTS systems to require fewer samples to learn and generalize, compared with continuous features.
3) Using discrete tokens simplifies cross-modal TTS applications like description-based TTS, as they are suitable for LLM training.
LLM-based methods **VoxInstruct**[^Zhou2024VoxInstruct], [^Yang2024instructtts], **CosyVoice**[^Du2024CosyVoice] often adopt discrete tokens as acoustic features.
However, discrete feature learning may cause information loss or lack the nuanced details in continuous features.

</td><td>

</td></tr>
<tr><td>

Table~[tab:methods_all](#tab:methods_all) summarizes the features used in existing methods.
We also compare speech quantization and tokenization in Appendix~[sec:appd_feature](#sec:appd_feature) and summarize open-source methods in Table~[tab:speech_feature](#tab:speech_feature) in the Appendix.

</td><td>

</td></tr></table>

## 4·Datasets and Evaluation Methods

### 4.1·Datasets

<table><tr><td width="50%">

Fully controllable TTS systems require large, diverse, and finely annotated datasets to generate expressive, attribute-controllable speech.
There are mainly three types of datasets for controllable TTS:

</td><td>

</td></tr>
<tr><td>

**Tag-based Datasets.**

Tag-based datasets contain speech recordings annotated with predefined discrete attribute labels that describe various aspects of the speech audio [^Zhou2022emotional], [^Busso2008iemocap], [^Ringeval2013RECOLA], [^Bagher2018cmu-Mosei].
Common attributes include pitch, energy, speaking rate, age, gender, emotion, emphasis, accent, and topic.
By leveraging attribute labels, models can dynamically adjust specific speech characteristics, enabling more expressive synthesis.

</td><td>

</td></tr>
<tr><td>

**Description-based Datasets.**

Description-based datasets pair speech samples with rich, free-form textual descriptions that capture nuanced attributes such as intonation, prosody, speaking style, and emotional tone [^Guo2023prompttts], **TextrolSpeech**[^Ji2023TextrolSpeech], **SpeechCraft**[^Jin2024SpeechCraft], [^Lyth2024natural].
Unlike tag-based datasets with predefined categorical labels, these datasets allow models to interpret and generate speech from natural language prompts, enabling context-aware and highly expressive synthesis.

</td><td>

</td></tr>
<tr><td>

**Dialogue Datasets.**

Dialogue datasets [^Byrne2019taskmaster], [^Lee2023dailytalk], [^Yang2022magicdata] contain multi-turn conversational speech involving two or more speakers, emphasizing natural interaction features such as turn-taking, contextual dependencies, speaker intent, pauses, and prosodic variation.
These datasets are essential for generating dynamic and contextually appropriate speech for interactive systems.
By leveraging these datasets, researchers can develop more expressive, context-aware, and highly controllable TTS models.
Table~[tab:datasets](#tab:datasets) in the Appendix summarizes publicly available datasets.

</td><td>

</td></tr></table>

### 4.2·Evaluation Methods

#### 4.2.1·Objective and Subjective Metrics

<table><tr><td width="50%">

**Objective Metrics.**

Objective metrics enable automated and reproducible evaluation.
**Mel Cepstral Distortion** (MCD)[^Kominek2008synthesizer] quantifies spectral distance between synthesized and reference speech.
MCD below 4 suggests good synthesis, while values above 6 imply distortion.
**Fréchet DeepSpeech Distance** (FDSD)[^Binkowski2019GAN-TTS] evaluates speech quality by measuring the distributional distance between synthesized and real speech in the embedding space of a pretrained speech recognition model, such as DeepSpeech [^Hannun2014deepspeech].
It compares the mean and covariance of extracted features; thus, a lower FDSD indicates higher perceptual similarity.
**Word Error Rate** (WER)[^Enwiki2024wer] quantifies speech intelligibility by comparing recognized and reference transcripts.
**Cosine Similarity** assesses speaker similarity by comparing speaker embeddings (extracted using models like ECAPA-TDNN [^Brecht20ECAPA-TDNN] or x-vectors [^Snyder2018xvectors]) of synthesized and reference speech.
Higher values indicate better voice cloning.
**Perceptual Evaluation of Speech Quality** (PESQ)[^Rix2001PESQ] evaluates the intelligibility and distortion of synthesized speech by modeling human auditory perception.

</td><td>

</td></tr>
<tr><td>

**Subjective Metrics.**

Subjective metrics assess the perceptual quality of synthesized speech based on human judgments, capturing aspects like expressiveness and style similarity.
**Mean Opinion Score** (MOS) rates synthesized speech (e.g., naturalness) on a 1–5 scale.
Though effective in capturing human perception, MOS is costly for large-scale use.
**Comparison MOS** (CMOS)[^Loizou2011speech] assesses relative quality by asking participants compare paired samples.
Both are averaged across listeners.
**AB/ABX Tests** present listeners with two samples (AB) by different methods or two plus a reference (ABX) to judge preference or closeness to the reference.
They are very common in evaluating fine-grained or zero-shot TTS.
Appendix~[sec:appd_metrics](#sec:appd_metrics) details the metric computations, while Table~[tab:eval_metrics](#tab:eval_metrics) summarizes the most commonly used ones.

</td><td>

</td></tr></table>

#### 4.2.2·Model-Based Evaluation

<table><tr><td width="50%">

Model-based evaluation is also an emerging technique, e.g., automatic MOS evaluation [^Lian2025apg] and GPT-based evaluation [^Rong2025dopamine].
To evaluate the controllability of existing TTS models, we designed a pipeline using Google Gemini to assess synthesized speech along three dimensions, i.e., instruction following, naturalness, and expressiveness, which are not well captured by traditional metrics.
Details of this evaluation are provided in Appendix~[sec:evaluation](#sec:evaluation).

</td><td>

</td></tr></table>

## 5·Challenges and Future Directions

### 5.1·Challenges

<table><tr><td width="50%">

**Fine-Grained Attribute Control.**

Emotion and other vocal traits are often intertwined and span multiple granularities, making fine-grained control especially difficult.
This requires high-resolution annotations and advanced models capable of capturing subtle attribute variations.
While description-based methods like **VoxInstruct**[^Zhou2024VoxInstruct] allow control via attribute descriptions, precisely targeting a specific granularity or enabling multiscale, fine-grained control remains a big challenge.

</td><td>

</td></tr>
<tr><td>

**Feature Disentanglement.**

Fully controllable TTS requires effective feature disentanglement, yet extracting meaningful and independent speech attributes is challenging due to their interdependence and context sensitivity.
For instance, modifying pitch can also affect emotion and naturalness.
To address this, prior work [^An2022disentangling], [^Wang2023generalizable] leverages pre-trained models on tasks like emotion classification and adversarial training to guide feature separation.
However, designing disentanglement methods for more subtle prosodic attributes, such as sarcasm, remains an open challenge and merits further research.

</td><td>

</td></tr>
<tr><td>

**Scarcity of Datasets.**

Effective control requires training data that spans a wide range of styles, emotions, accents, and prosodic patterns.
Large-scale datasets like **GigaSpeech**[^Chen2021GigaSpeech] and **TextrolSpeech**[^Ji2023TextrolSpeech] exist, but lack the content and scenario diversity, e.g., comedies, thrillers, and cartoons.
Fine-grained, attribute-specific annotations are another bottleneck.
Manual labeling is expensive, laborious, requires expertise, and is often inconsistent, especially for subjective traits like emotion.
Most datasets offer only coarse labels (e.g., gender, age, emotions).
While datasets like **SpeechCraft**[^Jin2024SpeechCraft] and Parler-TTS [^Lyth2024natural] include textual descriptions, none provide annotations across varying conditions within the same speaker.

</td><td>

</td></tr></table>

### 5.2·Future Directions

<table><tr><td width="50%">

**Instruction-Guided Fine-Grained Speech Synthesis and Editing.**

Natural language-driven control of fine-grained speech attributes remains underexplored.
Most existing methods support only a limited set of controllable attributes.
While models like **VoxInstruct**[^Zhou2024VoxInstruct] and **CosyVoice**[^Du2024CosyVoice] show promise in controlling emotion, timbre, and style, they often produce speech that deviates from user intent, requiring multiple synthesis attempts.
Similarly, speech editing methods [^Tae2022editts], [^Tan2021editspeech] typically rely on conditional models with fixed inputs, offering limited flexibility for fine-grained, instruction-guided modifications.
Thus, developing disentangled representations that support precise control through user instructions is promising.

</td><td>

</td></tr>
<tr><td>

**Expressive Multimodal Speech Synthesis.**

Synthesizing speech from multimodal inputs such as texts, images, and videos has broad industrial applications in storytelling, film, and gaming.
While prior work [^Goto2020face2speech], [^Lu2022visualtts], [^Rong2025seeing] explores this direction, current methods struggle to effectively extract and utilize rich multimodal information.
Generating expressive, engaging speech for complex visual content remains a promising area for future research.

</td><td>

</td></tr>
<tr><td>

**Zero-shot Long Speech and Conversational Synthesis with Emotion Consistency.**

Zero-shot TTS enables voice cloning and style transfer without fine-tuning **MaskGCT**[^Wang2024MaskGCT], **F5-TTS**[^Chen2024F5-TTS], **CosyVoice**[^Du2024CosyVoice], yet struggles with generating long, content-emotion consistent speech due to limited reference input.
Overcoming this challenge is key to advancing long speech synthesis.
Besides, conversational TTS, often cascaded and context-unaware, produces robotic and unexpressive speech.
Recent advances leverage LLMs and discrete speech tokens **LLaMA-Omni**[^Fang2024LLaMA-Omni], **SpeechGPT**[^Zhang2023SpeechGPT], but context-aware, emotionally rich conversational TTS remains underexplored.

</td><td>

</td></tr>
<tr><td>

**Large-Scale Dataset Generation.**

Dataset construction is critical for both fine-grained control and editing tasks.
Researchers can leverage pre-trained speech analysis models to annotate attributes like pitch, energy, emotion, gender, and age.
From these annotations, tools like ChatGPT can generate diverse natural language descriptions of speech characteristics.
For speech editing, pre-trained models can assist with tasks such as word substitution, pitch adjustment, and emotion conversion, while ChatGPT can provide varied and semantically rich editing instructions for training and evaluation.
In addition, multi-agent systems can also be utilized to generate long-form and diverse speech content.

</td><td>

</td></tr></table>

## 6·Conclusion

<table><tr><td width="50%">

This survey provides a comprehensive review of controllable TTS methods, covering model architectures, control strategies, and feature representations.
We also summarize commonly used datasets and evaluation metrics, discuss major challenges, and highlight promising future directions.
To the best of our knowledge, this is the first comprehensive survey dedicated to controllable TTS.

</td><td>

</td></tr></table>

## 7·Limitations

<table><tr><td width="50%">

We acknowledge several limitations that may affect the completeness of our survey.
First, this survey does not explore the interactions between controllable attributes.
Most existing studies focus on modeling each factor, such as emotion, speaker identity, or prosody, as an independent variable.
However, understanding how these attributes influence one another during synthesis could lead to more effective and flexible control strategies.
Second, we do not address the efficiency of current controllable TTS systems.
It is important to note that approaches guided by descriptions or instructions often involve considerable computational cost, largely due to their reliance on large language model-based codecs and complex cross-modal architectures.
Third, we have not discussed the broader societal implications of controllable TTS, such as the risks associated with deepfake generation or adversarial attacks.
Finally, this survey does not cover related research areas, including speech enhancement, speech separation, speech pretraining, and speech-to-speech translation, which may offer valuable insights or complementary techniques.
Overcoming these limitations presents important opportunities for future research to deepen our understanding and improve the design of controllable TTS systems.

</td><td>

</td></tr></table>

## 8·Ethics Statements

<table><tr><td width="50%">

The literature search and review were conducted using sources such as Google Scholar, arXiv, DBLP, Scopus, and ChatGPT.
All referenced papers included in this survey were thoroughly read, analyzed, and categorized by the authors.
Portions of the original content in this survey were paraphrased and refined with the assistance of ChatGPT.

</td><td>

</td></tr></table>

## 9·Acknowledgement

<table><tr><td width="50%">

This work was supported by the National Natural Science Foundation of China (No.
62471420), GuangDong Basic and Applied Basic Research Foundation (2025A1515012296), and CCF-Tencent Rhino-Bird Open Research Fund.

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="tab:appd_control_strategies">![]()</a>

</td></tr>
<tr><td>

Tab.01: Summary of the pros and cons of each control strategy.

</td><td>

</td></tr>
<tr><td>

\centering \resizebox{1.0\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{cccccccccccccccc} \toprule \multicolumn{1}{c}{\multirow{2}{*}[-3pt]{Dataset}} | \multicolumn{1}{c}{\multirow{2}{*}[-3pt]{\makecell{Hours\\(at least)}}} | \multicolumn{1}{c}{\multirow{2}{*}[-3pt]{\makecell{\#Speakers\\(at least)}}} | \multicolumn{11}{c}{Labels} | \multicolumn{1}{c}{\multirow{2}{*}[-3pt]{\makecell{Lang}}} | \multicolumn{1}{c}{\multirow{2}{*}[-3pt]{\makecell{Release\\Time}}} |
|\cmidrule(rl){4-14} | | | Pit.
| Ene.
| Spe.
| Age | Gen.
| Emo.
| Emp.
| Acc.
| Top.
| Des.
| Dia.
| | |
|\midrule IEMOCAP [^Busso2008iemocap] | 12 | 10 | √ | √ | √ | | √ | √ | | | | | | en | 2008 |
|RECOLA [^Ringeval2013RECOLA] |3.8 |46 | ||||| √ | ||||| fr | 2013 |
|RAVDESS [^Livingstone2018ryerson] | / | 24 | | | | √ | | √ | | | | | | en | 2018 |
|CMU-MOSEI [^Bagher2018cmu-Mosei] | 65 | 1,000 | |||| | √ | ||||| en | 2018 |
|Taskmaster-1[^Byrne2019taskmaster]| / | / | | | | | | | | | | | √ | en|2019 |
| **AISHELL-3**[^Shi2020AISHELL-3] | 85 | 218 | | | | √ | √ | | | √ | | | | zh | 2020 |
|Common Voice [^Ardila2020commonvoice] | 2,500 | 50,000 | | | | √ | √ | | | √ | | | | multi | 2020 |
|ESD [^Zhou2022emotional] |29|10| | | | | | √ | | | | | | en,zh |2021 |
|**GigaSpeech**[^Chen2021GigaSpeech] | 10,000 | / | | | | | | | | | √ | | | en | 2021 |
|**WenetSpeech**[^Zhang2021WenetSpeech]| 10,000 | / | | | | | | | | |√ | | | zh | 2021 |
|PromptSpeech [^Guo2023prompttts]| / | / | √ | √ | √ | | √| | | | | √ | | en | 2022 |
|MagicData-RAMC [^Yang2022magicdata] | 180 | 663 | | | | | | | | | √ | | √ | zh | 2022 |
|DailyTalk [^Lee2023dailytalk]| 20 | 2| | | | | | √ | | | √ | | √ | en|2023 |
|**TextrolSpeech**[^Ji2023TextrolSpeech] | 330 | 1,324 | √ | √ | √ | | √ | √ | | | | √ | | en | 2023|
|CLESC [^Toloka2024CLESC] | $<$1 | / | √ | √ | √ | | | √ | | | | | | en | 2024 |
|VccmDataset [^Ji2024controlspeech]| 330| 1,324| √ | √ | √ | | √ | √ | | | | √ | | en|2024|
|MSceneSpeech [^Yang2024mscenespeech] | 13 | 13 | | | | | | | | | √ | | | zh | 2024 |
|Parler-TTS [^Lyth2024natural] | 50,000 | / | √ | | √ | | √ | √ | | √ | | √ | | en | 2024|
|**SpeechCraft**[^Jin2024SpeechCraft] | 2,391 | 3,200 | √ | √ | √ | √ | √ | √ | √ | | √ | √ | | en,zh | 2024 |
|\bottomrule \end{tabular}
Abbreviations: Pit(ch), Ene(rgy)=volume, Spe(ed), Gen(der), Emo(tion), Emp(hasis), Acc(ent), Top(ic), Des(cription), Dia(logue).
\end{threeparttable}
} \caption{A summary of publicly available speech datasets for controllable TTS.} \label{tab:datasets} \setlength{\tabcolsep}{1pt} \centering \resizebox{1.0\columnwidth}{!}{
\begin{threeparttable}
\begin{tabular}{cccc} \toprule Metric | Type | Eval Target | GT Required |
|\midrule MCD [^Kominek2008synthesizer]$\downarrow$ | Objective | Acoustic similarity | √ |
|FDSD **GAN-TTS**[^Binkowski2019GAN-TTS]$\downarrow$ | Objective | Acoustic similarity | √ |
|WER [^Enwiki2024wer]$\downarrow$ | Objective | Intelligibility | √ |
|Cosine [^Brecht20ECAPA-TDNN]$\downarrow$ | Objective | Speaker similarity | √ |
|**PESQ**[^Rix2001PESQ]$\uparrow$ | Objective | Perceptual quality | √ |
|\midrule MOS [^Enwiki2024mos]$\uparrow$ | Subjective | Preference | |
|CMOS [^Loizou2011speech]$\uparrow$ | Subjective | Preference | |
|AB Test | Subjective | Preference | |
|ABX Test | Subjective | Perceptual similarity | √ |
|\bottomrule \end{tabular}
GT: Ground truth, $\downarrow$: Lower is better, $\uparrow$: Higher is better.
\end{threeparttable}
} \caption{Widely used evaluation metrics.} \label{tab:eval_metrics}

</td><td>

</td></tr></table>

## 10·Appendix

<table><tr><td width="50%">

\label{sec:appendix}

</td><td>

</td></tr></table>

### 10.1·The History of Controllable TTS

<table><tr><td width="50%">

\label{sec:appd_history} Controllable TTS aims to steer various aspects of synthesized speech, including pitch, energy, speed, prosody, timbre, emotion, gender, and speaking style.
This subsection briefly reviews its development, from early methods to recent advancements.
**Early Methods.** Early controllable TTS systems were primarily based on rule-based, concatenative, and statistical approaches.
Rule-based systems, such as formant synthesis [^Rabiner1968digital], [^Allen1987mitalk], [^Purcell2006adaptive], used handcrafted rules to adjust acoustic parameters like pitch and duration, enabling basic prosody control.
Concatenative systems [^Hunt1996373], [^Wouters2001control], [^Bulut2002expressive] generated speech by stitching pre-recorded speech units together, allowing prosody modifications through pitch and timing adjustments.
Later, Hidden Markov model (HMM)-based statistical methods [^Nose2007style], [^Ling2009integrating], [^Tokuda2000speech] modeled the relationship between linguistic features and acoustic outputs, offering greater flexibility in controlling prosody and speaking rate.
These systems also introduced speaker adaptation [^Yamagishi2009robust] and limited emotional control [^Yamagishi2003modeling], and require less storage and provide smoother transitions than concatenative methods.
**Neural Synthesis.** The emergence of deep learning revolutionized TTS, leading to the development of neural model-based systems capable of producing more natural, expressive, and controllable speech.
Early models like **WaveNet**[^Oord2016WaveNet] and **Tacotron**[^Wang2017Tacotron] demonstrated the potential for prosody control through explicit conditioning **Tacotron2**[^Shen2017Tacotron2], **FastSpeech2**[^Ren2020FastSpeech2].
Neural TTS further enhanced speaker control through speaker embeddings and adaptation techniques [^Fan2015multi], **YourTTS**[^Casanova2021YourTTS], while advances in emotional modeling [^Lei2022msemotts], [^Um2020emotional] enabled the synthesis of speech with specific affective tones.
Recent models have also achieved manipulation of timbre **MaskGCT**[^Wang2024MaskGCT], **NaturalSpeech2**[^Shen2023NaturalSpeech2] and style [^Li2025styletts], [^Huang2022generspeech], fostering the research in zero-shot TTS and voice cloning [^Cooper2020zero].
In addition, methods for fine-grained content control **VoiceCraft**[^Peng2024VoiceCraft], [^Tan2021editspeech] have made it possible to emphasize or edit specific words in synthesized speech.
**LLM-based Synthesis.** More recently, LLM-based approaches have further advanced controllable TTS.
Leveraging models like **BERT**[^Devlin2018BERT], GPT **GPT-3**[^Brown2020GPT-3],  **T5**[^Raffel2019T5], and **PaLM**[^Chowdhery2022PaLM], LLMs bring superior context modeling and intuitive control to speech synthesis [^Guo2023prompttts], **VoxInstruct**[^Zhou2024VoxInstruct].
By interpreting natural language prompts, such as describing a speaker's emotion, age, or style, LLMs can infer nuanced attributes and steer the generation process accordingly.
This enables dynamic, fine-grained control over prosody, emotion, style, and speaker identity [^Yang2024instructtts], [^Gao2024emo], marking a big step toward more flexible and intuitive TTS systems.

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="fig:pipeline">![](images/pipeline.pdf)</a>

</td></tr>
<tr><td>

Fig.04: General pipeline of controllable TTS from the perspective of network structure.
Linguistic analysis is necessary for parametric and a few neural methods but is no longer needed for most modern neural methods.
In this paper, we only review neural model-based controllable TTS methods and do not investigate acoustic features (e.g., MFCC [^Fukada1992adaptive], LSP [^Itakura1975line], F0[^Kawahara1999restructuring]) used in early TTS methods.

</td><td>

</td></tr>
<tr><td>

\centering \resizebox{1.0\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{cccc} \toprule Method | Modeling | Code | Year |
|\midrule **VQ-Wav2Vec**[^Baevski2019VQ-Wav2Vec] | SSCP | \url{https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec\#vq-wav2vec} | 2019 |
|Wav2Vec 2.0[^Baevski2020Wav2Vec2.0] | SSCP | \url{https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec} | 2019 |
|**HuBERT**[^Hsu2021HuBERT] | SSCP | \url{https://github.com/facebookresearch/fairseq/tree/main/examples/hubert} | 2021 |
|Whisper Encoder **Whisper**[^Radford2022Whisper] | SSCP | \url{https://github.com/openai/whisper} | 2022 |
|Data2vec [^Baevski2022data2vec] | SSCP | \url{https://github.com/facebookresearch/fairseq/tree/main/examples/data2vec} | 2022 |
|W2v-BERT 2.0[^Barrault2023Seamless] | SSCP | \url{https://huggingface.co/facebook/w2v-bert-2.0} | 2023 |
|\midrule **SoundStream**[^Zeghidour2021SoundStream] | RVQ-GAN | \url{https://github.com/wesbz/SoundStream} | 2021 |
|Encodec **EnCodec**[^Defossez2022EnCodec] | RVQ-GAN | \url{https://github.com/facebookresearch/encodec} | 2022 |
|**HiFi-Codec**[^Yang2023HiFi-Codec] | RVQ-GAN | \url{https://github.com/yangdongchao/AcademiCodec} | 2023 |
|**SpeechTokenizer**[^Zhang2023SpeechTokenizer] | RVQ-GAN | \url{https://github.com/ZhangXInFD/SpeechTokenizer} | 2023 |
|Descript Audio Codec **DAC**[^Kumar2023DAC] | RVQ-GAN | \url{https://github.com/descriptinc/descript-audio-codec} | 2023 |
|Mimi Codec **Moshi**[^Defossez2024Moshi] | RVQ-GAN | \url{https://github.com/kyutai-labs/moshi} | 2024 |
|**WavTokenizer**[^Ji2024WavTokenizer] | VQ-GAN | \url{https://github.com/jishengpeng/WavTokenizer} | 2024 |
|\bottomrule \end{tabular}
SSCP: Self-supervised context (token) prediction, RVQ: Residual vector quantization **SoundStream**[^Zeghidour2021SoundStream].
\end{threeparttable}
} \caption{Popular open-source speech quantization and tokenization methods.} \label{tab:speech_feature}

</td><td>

</td></tr></table>

### 10.2·Overview of the TTS Pipeline

<table><tr><td width="50%">

In this section, we provide an overview of the general pipeline that supports controllable TTS technologies.
Fig.~[Fig.04](#fig:pipeline) depicts the general pipeline of controllable TTS, containing various model architectures and feature representations.
A TTS pipeline generally contains three key components, i.e., linguistic analyzer, acoustic model, and speech vocoder, where a conditional input, e.g., prompts, can be processed for controllable speech synthesis.
**Linguistic analyzer** aims to extract linguistic features, e.g., phoneme duration and position, syllable stress, and utterance level, from the input text, which is a necessary step in HMM-based methods [^Yoshimura1999simultaneous], [^Tokuda2000speech] and a few neural model-based methods [^Zen2013statistical], [^Fan2014tts], but is time-consuming and error-prone.
**Acoustic model** is a parametric or neural model that predicts the acoustic features from the input texts.
Modern neural acoustic models like **Tacotron**[^Wang2017Tacotron] and later works **FastSpeech**[^Ren2019FastSpeech], **FastSpeech2**[^Ren2020FastSpeech2], **Diff-TTS**[^Jeong2021Diff-TTS] directly take character [^Chen2015joint] or word embeddings [^Almeida2019word] as input, which is much more efficient than previous methods.
**Speech vocoder** is the last component that converts the intermediate acoustic features into a waveform that can be played back.
This step bridges the gap between the acoustic features and the actual sounds produced, helping to generate high-quality, natural-sounding speech **WaveNet**[^Oord2016WaveNet], **HiFi-GAN**[^Kong2020HiFi-GAN].
Besides, some end-to-end methods use a single model to encode the input and decode the speech waveforms without generating intermediate features like mel-spectrograms.
One can refer to **Survey20210629**[^Tan2021Survey] for a more comprehensive and detailed review of acoustic models and vocoders.

</td><td>

</td></tr></table>

### 10.3·Speech Quantization vs Tokenization

<table><tr><td width="50%">

It is worth noting that quantization and tokenization serve distinct purposes in speech processing.
Quantization is primarily used for high-fidelity compression, reducing the precision of numerical representations (e.g., from 32-bit floating point to 8-bit integers) while preserving model performance.
In speech synthesis, quantization is often used in waveform generation (e.g., codec-based approaches like **EnCodec**[^Defossez2022EnCodec]) and neural vocoders to compress audio signals without significant loss of perceptual quality.
Tokenization, on the other hand, is a discretization process that segments continuous data into meaningful units.
In speech tasks, tokenization extracts semantically relevant representations such as phonemes, characters, or learned speech units (e.g., **HuBERT**[^Hsu2021HuBERT] and **Wav2Vec2.0**[^Baevski2020Wav2Vec2.0]).
This makes tokenization particularly suitable for speech-to-text (ASR), TTS, and multimodal NLP tasks, where aligning speech with textual information is crucial.
Tokenization also facilitates training language models on speech data by enabling linguistic or learned unit-based processing rather than raw audio waveform modeling.
Table [tab:speech_feature](#tab:speech_feature) in Appendix~[sec:appd_feature](#sec:appd_feature) summarizes popular open-source speech quantization and tokenization methods.
Table~[tab:methods_all](#tab:methods_all) summarizes the acoustic features of representative methods.

</td><td>

</td></tr></table>

### 10.4·Evaluation Metric Computations

<table><tr><td width="50%">

The performance of controllable TTS often requires objective and subjective evaluation.
We introduce common evaluation metrics in this subsection.
**Objective Evaluation Metrics.** Objective metrics offer automated and reproducible evaluations.
Mel Cepstral Distortion (MCD)[^Kominek2008synthesizer] measures the spectral distance between synthesized and reference speech, reflecting how closely the generated audio matches the target in terms of acoustic features.
A lower MCD value indicates a higher similarity between synthesized and reference speech, meaning better speech synthesis quality.
Typically, an MCD value below 4 suggests good quality, while values above 6 may indicate significant distortion.
The MCD is computed as follows:

$$
MCD = \frac{10}{\ln 10} \cdot \sqrt{2 \sum_{d=1}^{D} (c_d^{(syn)} - c_d^{(ref)})^2},
$$

where $c_d^{(syn)}$ represents the d-th Mel Cepstral Coefficient (MCC) of the synthesized speech, $c_d^{(ref)}$ represents the d-th MCC of the reference speech, $D$ is the number of MCC, and $\frac{10}{\ln 10} \approx 4.342$ is a constant factor that converts the logarithm to a decibel scale.
Fréchet DeepSpeech Distance (FDSD) **GAN-TTS**[^Binkowski2019GAN-TTS] is another metric designed to evaluate the quality and naturalness of synthesized speech.
It is inspired by the Fréchet Inception Distance (FID) [^Heusel2017fid] used in image generation but adapted to speech by leveraging a deep speech recognition model.
FDSD measures the statistical distance between the distributions of real (reference) and synthesized speech in the feature space of a pretrained speech recognition model, such as Deep Speech [^Hannun2014deepspeech].
By comparing the mean and covariance of the extracted feature representations, FDSD provides a perceptually relevant assessment of speech synthesis quality.
A lower FDSD means the synthesized speech is more similar to real speech.
FDSD can be computed as:

$$
FDSD = ||\mu_s - \mu_r||^2 + \text{Tr}(\Sigma_s + \Sigma_r - 2(\Sigma_s \Sigma_r)^{1/2}),
$$

where $\mu_s$ and $\Sigma_s$ are the mean and covariance of the embeddings from the synthesized speech, $\mu_r$ and $\Sigma_r$ are the mean and covariance of the embeddings from the real (reference) speech, $||\mu_s - \mu_r||^2$ represents the squared Euclidean distance between the means, $\text{Tr}(\cdot)$ denotes the trace of a matrix, and $(\Sigma_s \Sigma_r)^{1/2}$ is the geometric mean of the covariance matrices.
For intelligibility, the Word Error Rate (WER) [^Enwiki2024wer] is used.
It measures the difference between the recognized transcript and the reference transcript by computing the number of errors made in the transcription process.
WER is computed as:

$$
WER = \frac{S + D + I}{N},
$$

where $S$ is the number of substitutions (wrong word in place of the correct word), $D$ is the number of deletions (missed words), $I$ is the number of insertions (extra words added), and $N$ is the total number of words in the reference transcript.
Cosine similarity (on speaker embeddings) measures similarity between the speaker embeddings of synthesized and reference speech.
It can be used to evaluate zero-shot TTS (voice cloning) methods, where higher values indicate better speaker similarity.
Given two speaker embeddings, $\mathbf{e_1}$ and $\mathbf{e_2}$, their cosine similarity is defined as:

$$
CosSim(\mathbf{e_1}, \mathbf{e_2}) = \frac{\mathbf{e_1} \cdot \mathbf{e_2}}{\|\mathbf{e_1}\| \|\mathbf{e_2}\|},
$$

where speaker embeddings can be extracted from a pre-trained speaker embedding model (e.g., ECAPA-TDNN [^Brecht20ECAPA-TDNN] and x-vectors [^Snyder2018xvectors]).
Perceptual Evaluation of Speech Quality (PESQ) **PESQ**[^Rix2001PESQ] is another objective metric designed to evaluate speech quality by comparing degraded audio with a clean reference.
It is widely used in telecommunications and speech synthesis.
PESQ models human auditory perception, producing a score in the range $[-0.5,-4.5]$ that reflects intelligibility and distortion under various conditions, including noise or compression.
PESQ involves complex perceptual modeling, its core components can be summarized as:

$$
PESQ = a_0 + a_1 \cdot D_{frame} + a_2 \cdot D_{time},
$$

where $D_{frame}$ is the frame-by-frame perceptual distortion, $D_{time}$ is the time-domain distortion, and $a_0, a_1, a_2$ are regression coefficients.
One can refer to **PESQ**[^Rix2001PESQ] for details.
Signal-to-Noise Ratio (SNR) measures the ratio of signal power to noise power.
A higher SNR indicates a cleaner signal with less noise, while a lower SNR suggests that noise is dominating the signal.
However, in TTS, noise can come from different sources, such as artifacts from vocoders, neural network distortions, or background noise in dataset recordings.
A direct computation of SNR in TTS requires a reference clean speech signal ($x [n]$), a synthesized (or noisy) speech signal ($y [n]$), and extracting the noise component ($e [n] = y [n] - x [n]$) from the synthesized signal.
The SNR for TTS systems can be computed as:

$$
SNR = 10 \log_{10} \left( \frac{P_{\text{signal}}}{P_{\text{noise}}} \right),
$$

where $P_{\text{signal}} = \frac{1}{N} \sum_{n=1}^{N} x [n]^2$ and $P_{\text{noise}} = \frac{1}{N} \sum_{n=1}^{N} e [n]^2$.

**Subjective Evaluation Metrics.**

The Mean Opinion Score (MOS) [^Enwiki2024mos] is the most commonly used subjective metric.
In MOS evaluations, listeners rate various aspects, such as naturalness, expressiveness, quality, intelligibility, et al., of synthesized speech on a scale from 1 to 5, where higher scores indicate better quality.
MOS captures human perception effectively, but is expensive for large-scale evaluations.
Comparison Mean Opinion Score (CMOS) [^Loizou2011speech] further evaluates relative quality differences between two TTS audio samples.
Participants listen to paired samples and rate their preference on a scale (e.g., -3 to +3, where negative values favor the first sample).
CMOS is used to measure subtle improvements in TTS systems, complementing absolute MOS ratings.
MOS and CMOS scores are computed as the average scores across all listeners:

$$
MOS/CMOS = \frac{1}{N} \sum_{i=1}^{N} s_i,
$$

where $s_i$ is the score given by the $i$-th listener, and $N$ is the number of listeners.
AB and ABX tests are also popular in evaluating TTS methods.
An AB test involves presenting two versions of a synthesized speech (from different TTS models) to human listeners and asking them to choose which they prefer.
The goal is to assess which model produces better-sounding speech based on certain criteria, such as naturalness, intelligibility, or clarity.
In an ABX test, listeners compare two synthesized speech samples to a reference speech sample and determine which one is closer in terms of timbre, prosody, emotion, and other relevant features.
ABX tests are widely used in evaluating zero-shot TTS methods.
The AB/ABX test score for a model $m$ is:

$$
Score_{AB}/Score_{ABX} = \frac{N_m}{N},
$$

where $N_m$ represents the number of listeners who prefer the speech synthesized by model $m$, and $N$ denotes the total number of listeners.
Table~[tab:eval_metrics](#tab:eval_metrics) summarizes widely used metrics for TTS.

</td><td>

</td></tr></table>

### 10.5·A Google Gemini-Based Experimental Evaluation of TTS Controllability

<table><tr><td width="50%">

We designed an evaluation pipeline using Gemini to assess synthesized speech in terms of **instruction following**, **naturalness**, and **expressiveness**, because these dimensions are not well captured by traditional metrics.
Conventional scores like MCD, WER, PESQ, speaker similarity, and MOS/CMOS are excluded, as our goal is to explore the feasibility of using multimodal large language models (MLLMs) as subjective evaluators.

</td><td>

</td></tr></table>

#### 10.5.1·Implementation Details

<table><tr><td width="50%">

**Models.**

Due to time constraints, we only evaluate a total of 10 models: 8 open-source systems (F5-TTS, CosyVoice, CosyVoice2, Vevo, SparkTTS, MaskGCT, PromptTTS, and VoxInstruct) and 2 commercial TTS systems (ElevenLabs and MiniMax TTS).

**Tasks.**

Zero-shot TTS and description-based synthesis.
For each model, we synthesize 20 speech samples (10 in English and 10 in Chinese) for each task.

**Dataset.**

For zero-shot TTS, we sampled 10 English utterances from the MSP-Podcast dataset and 10 Chinese utterances from Emo-Emilia to serve as reference speech prompts.
For description-based synthesis, we used ChatGPT to generate diverse textual descriptions as shown in Fig.~[Fig.05](#fig:descriptions).

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="fig:descriptions">![](images/prompts.pdf)</a>

</td></tr>
<tr><td>

Fig.05: Textual descriptions generated by ChatGPT

</td><td>

</td></tr>
<tr><td>

**Metrics Clarification**:

-  Instruction Following
-  Purpose: To assess how accurately the synthesized audio follows the given instruction regarding speech characteristics such as timing, emphasis, and pacing.
-  Focus: Measures the controllability and fidelity of the model in executing user-specified directives.
-  Naturalness
-  Purpose: To evaluate how natural the audio sounds—whether it resembles human speech or exhibits synthetic, robotic qualities.
-  Focus: Measures the perceptual audio quality and realism of the synthesized speech.
-  Expressiveness
-  Purpose: To judge the emotional richness and prosodic variation in the audio, such as tone, intensity, and nuance.
-  Focus: Measures the model’s ability to convey expressive and emotionally engaging speech.

**Gemini Prompts.**

The prompt we use for the evaluation is as follows: *SYSTEM\_PROMPT*: You are a strict quality evaluator for synthesized speech.
Given an audio file of a speech sample, its transcript, and an instruction describing the intended speech characteristics, please rate the audio based on the following three aspects, using the defined criteria.
Output ONLY a JSON dictionary with the keys instruction\_following, naturalness, and expressiveness, each assigned an integer value from 1 to 5.

The evaluation rubrics are as follows:

- **Instruction Following (1--5):**
  - 1 point: The audio completely ignores the instruction; it does not follow the intended timing, emphasis, or pacing.
  - 2 points: It loosely follows the instructions but misses key elements or timing in parts.
  - 3 points: Generally follows the instruction with minor lapses in emphasis or pacing.
  - 4 points: Clearly follows the instruction with only slight deviations.
  - 5 points: Perfectly follows every aspect of the instruction with clear emphasis and precise pacing.

- **Naturalness (1--5):**
  - 1 point: The audio sounds fully synthetic or robotic; extremely unnatural.
  - 2 points: Noticeably synthetic; some unnatural artifacts remain.
  - 3 points: Moderately natural with occasional synthetic artifacts.
  - 4 points: Largely natural sounding with minor imperfections.
  - 5 points: Completely natural; indistinguishable from a human recording.

- **Expressiveness (1--5):**
  - 1 point: The audio is flat and monotone; no emotional variation.
  - 2 points: Minimal expressiveness; emotions are weak or inconsistent.
  - 3 points: Reasonably expressive with some highlights, but could be stronger.
  - 4 points: Clearly expressive with only slight under- or over-emphasis.
  - 5 points: Exceptionally expressive; full emotional richness and nuance.

*USER\_PROMPT*: The synthesized speech is \{audio\}.
The transcript of the audio is: "\{transcript\}".
The instruction for the audio is: "\{instruction\}".

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="tab:eval_results">![]()</a>

</td></tr>
<tr><td>

Tab.02: The evaluation of the controllability of open-source and commercial TTS systems.

</td><td>

</td></tr>
<tr><td>

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="tab:result_alignment">![]()</a>

</td></tr>
<tr><td>

Tab.03: The alignment between model-based evaluation and human preference.

</td><td>

</td></tr>
<tr><td>

</td><td>

</td></tr></table>

#### 10.5.2·Results: Model-Level Performance Comparison

<table><tr><td width="50%">

As shown in Table~[Tab.02](#tab:eval_results), in the zero-shot setting, among the six models, Vevo performs best in both naturalness (4.43±0.55) and expressiveness (4.32±0.75), indicating strong general quality without explicit guidance.
CosyVoice, CosyVoice 2, and F5-TTS follow closely with similar scores (~4.2), while SparkTTS and MaskGCT lag behind, especially in naturalness.
In the instruction-based setting, all models show a clear improvement across all metrics.
CosyVoice achieves the highest overall scores, with instruction following at 4.81±0.28, naturalness at 4.92±0.24, and expressiveness at 4.78±0.29.
Other strong performers include MiniMax TTS and EmoVoice, both exceeding 4.6 in most dimensions.
Even the lowest-scoring instruction-based method (VoxInstruct) outperforms the best zero-shot model in every aspect.

</td><td>

</td></tr></table>

#### 10.5.3·Results: The Reliability of Multimodal LLM-based Evaluation

<table><tr><td width="50%">

We also compare the proposed metrics with existing automated evaluation methods, namely NISQA [^Mittag21_interspeech] and **UTMOS**[^Saeki2022UTMOS].
Specifically, we use 96 synthesized samples to compute the Pearson correlation coefficients between the predicted scores from each method and human ratings, aiming to assess how well each method aligns with human perception.
As shown in Table~[Tab.03](#tab:result_alignment), although the absolute Pearson correlation coefficients of our method are relatively modest, our approach consistently outperforms both NISQA and UTMOS across all three evaluation dimensions: instruction following, naturalness, and expressiveness.
These results suggest that existing automated metrics like NISQA and UTMOS, which are primarily designed for general speech quality assessment, may not capture nuanced attributes such as speaker intent or expressive delivery in controllable TTS tasks.
In contrast, our metric, tailored for instruction-based synthesis evaluation, better reflects human judgments, particularly in aspects beyond raw audio quality.
This supports the need for task-specific evaluation frameworks when benchmarking modern controllable TTS systems.

</td><td>

</td></tr></table>

#### 10.5.4·Conclusion

<table><tr><td width="50%">

To some extent, the proposed MLLM-based evaluation pipeline is able to predict human-aligned scores for instruction following, naturalness, and expressiveness.
We also find that it offers promising potential for automated evaluation of controllable TTS.
In future work, we plan to enhance our survey by designing a more robust and reliable MLLM-based evaluation framework and conducting a comprehensive benchmark of existing controllable TTS methods.

</td><td>

</td></tr></table>

## References

[^Dutoit1997introduction]: Thierry Dutoit. 1997. {An Introduction to Text-to-Speech Synthesis}, volume~3. Springer Science \| Business Media.
[^Lopez2018alexa]: Gustavo L{\'o}pez, Luis Quesada, and Luis~A. Guerrero. 2018. {Alexa vs. Siri vs. Cortana vs. Google} assistant: A comparison of speech-based natural user interfaces. In Advances in Human Factors and Systems Interaction, pages 241--250.
[^Wang2019comic]: Yujia Wang, Wenguan Wang, Wei Liang, and Lap-Fai Yu. 2019. Comic-guided speech synthesis. ACM Transactions on Graphics, 38(6):1--14.
[^Marge2022spoken]: Matthew Marge, Carol Espy-Wilson, Nigel~G Ward, Abeer Alwan, Yoav Artzi, Mohit Bansal, Gil Blankenship, Joyce Chai, Hal Daum{\'e}~III, Debadeepta Dey, and 1 others. 2022. Spoken language interaction with robots: Recommendations for future research. Computer Speech \| Language, 71:101255.
[^Openai2022chatgpt]: {OpenAI}. 2022. Introducing {ChatGPT}. https://openai.com/index/chatgpt/. Accessed: 2024-10-22.
[^Tan2022NaturalSpeech]: [**NaturalSpeech**: End-to-End Text to Speech Synthesis with Human-Level Quality](../Models/E2E/2022.05.09_NaturalSpeech.md). ArXiv:2205.04421v2/TPAMI2024.
[^Ren2019FastSpeech]: [**FastSpeech**: Fast Robust and Controllable Text to Speech](../Models/Acoustic/2019.05.22_FastSpeech.md). ArXiv:1905.09263/NeurIPS2019.
[^Du2024CosyVoice]: [**CosyVoice**: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens](../Models/SpeechLM_TTS/2024.07.07_CosyVoice.md). ArXiv:2407.05407v2.
[^Wang2018style]: Yuxuan Wang, Daisy Stanton, Yu~Zhang, RJ-Skerry Ryan, Eric Battenberg, Joel Shor, Ying Xiao, Ye~Jia, Fei Ren, and Rif~A Saurous. 2018. {Style Tokens}: Unsupervised style modeling, control and transfer in end-to-end speech synthesis. In International Conference on Machine Learning, pages 5167--5176.
[^Li2021controllable]: Tao Li, Shan Yang, Liumeng Xue, and Lei Xie. 2021. Controllable emotion transfer for end-to-end speech synthesis. In 12th International Symposium on Chinese Spoken Language Processing, pages 1--5.
[^Zhou2024VoxInstruct]: [**VoxInstruct**: Expressive Human Instruction-to-Speech Generation with Unified Multilingual Codec Language Modelling](../Models/CodecLM/2024.08.28_VoxInstruct.md). ArXiv:2408.15676v1/ACM Multimedia2024.
[^Rong2025seeing]: Yan Rong and Li~Liu. 2025. Seeing your speech style: A novel zero-shot identity-disentanglement face-based voice conversion. In Proceedings of the AAAI Conference on Artificial Intelligence, volume~39, pages 25092--25100.
[^Choi2023diffv2s]: Jeongsoo Choi, Joanna Hong, and Yong~Man Ro. 2023. {DiffV2S}: Diffusion-based video-to-speech synthesis with vision-guided speaker embedding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7812--7821.
[^Zhao2023survey]: Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, and 1 others. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223, 1(2).
[^Guo2023prompttts]: Zhifang Guo, Yichong Leng, Yihan Wu, Sheng Zhao, and Xu~Tan. 2023. {PromptTTS}: Controllable text-to-speech with text descriptions. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 1--5.
[^Huang2024instructspeech]: Rongjie Huang, Ruofan Hu, Yongqi Wang, Zehan Wang, Xize Cheng, Ziyue Jiang, Zhenhui Ye, Dongchao Yang, Luping Liu, Peng Gao, and Zhou Zhao. 2024{\natexlab{a}}. {InstructSpeech}: Following speech editing instructions via large language models. In Forty-first International Conference on Machine Learning.
[^Peng2024survey]: Jing Peng, Yucheng Wang, Yu~Xi, Xu~Li, Xizhuo Zhang, and Kai Yu. 2024{\natexlab{a}}. A survey on speech large language models. arXiv preprint arXiv:2410.18908.
[^Zen2009statistical]: Heiga Zen, Keiichi Tokuda, and Alan~W Black. 2009. Statistical parametric speech synthesis. Speech Communication, 51(11):1039--1064.
[^Triantafyllopoulos2023overview_survey]: Andreas Triantafyllopoulos, Bj{\"o}rn~W Schuller, G{\"o}k{\c{c}}e {\.I}ymen, Metin Sezgin, Xiangheng He, Zijiang Yang, Panagiotis Tzirakis, Shuo Liu, Silvan Mertes, Elisabeth Andr{\'e}, and 1 others. 2023. An overview of affective speech synthesis and conversion in the deep learning era. Proceedings of the IEEE, 111(10):1355--1381.
[^Yamamoto2024description]: Ryuichi Yamamoto, Yuma Shirahata, Masaya Kawamura, and Kentaro Tachibana. 2024. Description-based controllable text-to-speech with cross-lingual voice control. arXiv preprint arXiv:2409.17452.
[^Klatt1987review]: Dennis~H Klatt. 1987. Review of text-to-speech conversion for english. The Journal of the Acoustical Society of America, 82(3):737--793.
[^Tabet2011speech]: Youcef Tabet and Mohamed Boughazi. 2011. Speech synthesis techniques. a survey. In International Workshop on Systems, Signal Processing and their Applications, pages 67--70.
[^King2014measuring]: Simon King. 2014. Measuring a decade of progress in text-to-speech. Loquens, 1(1):e006--e006.
[^Ning2019review]: Yishuang Ning, Sheng He, Zhiyong Wu, Chunxiao Xing, and Liang-Jie Zhang. 2019. A review of deep learning based speech synthesis. Applied Sciences, 9(19):4050.
[^Tan2021Survey]: [**Survey20210629**: A Survey on Neural Speech Synthesis](2021.06.29_Tan2021Survey.md). ArXiv:2106.15561v3.
[^Zhang2023Survey]: [**Survey20230323**: A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI](2023.03.23_Zhang2023Survey.md). ArXiv:2303.13336v2.
[^Lancucki2020FastPitch]: [**FastPitch**: Parallel Text-to-speech with Pitch Prediction](../Models/Acoustic/2020.06.11_FastPitch.md). ArXiv:2006.06873v2/ICASSP2021.
[^Wang2025Spark-TTS]: [**Spark-TTS**: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens](../Models/SpeechLM_TTS/2025.03.03_Spark-TTS.md). ArXiv:2503.01710v1.
[^Chen2025DrawSpeech]: [**DrawSpeech**: Expressive Speech Synthesis Using Prosodic Sketches as Control Conditions](../Models/Diffusion/2025.01.08_DrawSpeech.md). ArXiv:2501.04256v1/ICASSP2025.
[^Zhang2025Vevo]: [**Vevo**: Controllable Zero-Shot Voice Imitation with Self-Supervised Disentanglement](../Models/SpeechLM_TTS/2025.02.11_Vevo.md). ArXiv:2502.07243v1/ICLR2025.
[^Kim2021expressive]: Minchan Kim, Sung~Jun Cheon, Byoung~Jin Choi, Jong~Jin Kim, and Nam~Soo Kim. 2021. Expressive text-to-speech using style tag. Annual Conference of the International Speech Communication Association, pages 4663--4667.
[^Rong2025dopamine]: Yan Rong, Shan Yang, Guangzhi Lei, and Li~Liu. 2025. Dopamine audiobook: A training-free {MLLM} agent for emotional and human-like audiobook generation. arXiv preprint arXiv:2504.11002.
[^Yang2024instructtts]: Dongchao Yang, Songxiang Liu, Rongjie Huang, Chao Weng, and Helen Meng. 2024{\natexlab{b}}. {InstructTTS}: Modelling expressive {TTS} in discrete latent space with natural language style prompt. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 32:2913--2925.
[^Zhang2023VALL-EX]: [**VALL-E X**: Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling](../Models/CodecLM/2023.03.07_VALL-E_X.md). ArXiv:2303.03926.
[^Di2024bailing]: Xinhan Di, Zihao Chen, Yunming Liang, Junjie Zheng, Yihua Wang, and Chaofan Ding. 2024. {Bailing-TTS}: Chinese dialectal speech synthesis towards human-like spontaneous representation. arXiv preprint arXiv:2408.00284.
[^Chen2024F5-TTS]: [**F5-TTS**: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching](../Models/FlowMatching/2024.10.09_F5-TTS.md). ArXiv:2410.06885v3.
[^Lee2023VoiceLDM]: [**VoiceLDM**: Text-to-Speech with Environmental Context](../Models/Diffusion/2023.09.24_VoiceLDM.md). ArXiv:2309.13664v1.
[^Kim2024speak]: Miseul Kim, Soo-Whan Chung, Youna Ji, Hong-Goo Kang, and Min-Seok Choi. 2024{\natexlab{b}}. Speak in the {Scene}: Diffusion-based acoustic scene transfer toward immersive speech generation. In Annual Conference of the International Speech Communication Association, pages 4883--4887.
[^Prenger2018WaveGlow]: [**WaveGlow**: A Flow-based Generative Network for Speech Synthesis](../Models/Vocoder/2018.10.31_WaveGlow.md). ArXiv:1811.00002v1/ICASSP2019.
[^Ren2020FastSpeech2]: [**FastSpeech2**: Fast and High-Quality End-to-End Text-to-Speech](../Models/Acoustic/2020.06.08_FastSpeech2.md). ArXiv:2006.04558/ICLR2021.
[^Yamamoto2020parallelwavegan]: Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim. 2020. Parallel {WaveGAN}: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6199--6203.
[^Elias2021parallel]: Isaac Elias, Heiga Zen, Jonathan Shen, Yu~Zhang, Ye~Jia, Ron~J Weiss, and Yonghui Wu. 2021{\natexlab{a}}. Parallel {Tacotron}: Non-autoregressive and controllable {TTS}. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 5709--5713.
[^Kalchbrenner2018wavernn]: Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aaron Oord, Sander Dieleman, and Koray Kavukcuoglu. 2018. Efficient neural audio synthesis. In International Conference on Machine Learning, pages 2410--2419.
[^Kong2020HiFi-GAN]: [**HiFi-GAN**: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis](../Models/Vocoder/2020.10.12_HiFi-GAN.md). ArXiv:2010.05646/NeurIPS2020.
[^Casanova2021sc]: Edresson Casanova, Christopher Shulby, Eren G{\"o}lge, Nicolas~Michael M{\"u}ller, Frederico~Santos De~Oliveira, Arnaldo~Candido Junior, Anderson da~Silva Soares, Sandra~Maria Aluisio, and Moacir~Antonelli Ponti. 2021. {SC-GlowTTS}: An efficient zero-shot multi-speaker text-to-speech model. arXiv preprint arXiv:2104.05557.
[^Min2021meta]: Dongchan Min, Dong~Bok Lee, Eunho Yang, and Sung~Ju Hwang. 2021. {Meta-StyleSpeech}: Multi-speaker adaptive text-to-speech generation. In International Conference on Machine Learning, pages 7748--7759. PMLR.
[^Kumar2019MelGAN]: [**MelGAN**: Generative Adversarial Networks for Conditional Waveform Synthesis](../Models/Vocoder/2019.10.08_MelGAN.md). ArXiv:1910.06711/NeurIPS2019.
[^Liu2021DelightfulTTS]: [**DelightfulTTS**: The Microsoft Speech Synthesis System for Blizzard Challenge 2021](../Models/E2E/2021.10.25_DelightfulTTS.md). ArXiv:2110.12612v2.
[^Casanova2021YourTTS]: [**YourTTS**: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for Everyone](../Models/E2E/2021.12.04_YourTTS.md). ArXiv:2112.02418v4/ICML2022.
[^Li2025styletts]: Yinghao~Aaron Li, Cong Han, and Nima Mesgarani. 2025{\natexlab{b}}. {StyleTTS}: A style-based generative model for natural and diverse text-to-speech synthesis. IEEE Journal of Selected Topics in Signal Processing, 19(1):283--296.
[^Huang2022generspeech]: Rongjie Huang, Yi~Ren, Jinglin Liu, Chenye Cui, and Zhou Zhao. 2022{\natexlab{b}}. {GenerSpeech}: Towards style transfer for generalizable out-of-domain text-to-speech. In Advances in Neural Information Processing Systems, pages 1--14.
[^Abbas2022expressive]: Ammar Abbas, Thomas Merritt, Alexis Moinet, Sri Karlapati, Ewa Muszynska, Simon Slangen, Elia Gatti, and Thomas Drugman. 2022. Expressive, variable, and controllable duration modelling in {TTS}. arXiv preprint arXiv:2206.14165.
[^Jiao2021upwavenet]: Yunlong Jiao, Adam Gabry{\'s}, Georgi Tinchev, Bartosz Putrycz, Daniel Korzekwa, and Viacheslav Klimkov. 2021. Universal neural vocoding with parallel {WaveNet}. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6044--6048.
[^Liu2022controllable]: Zhengxi Liu, Qiao Tian, Chenxu Hu, Xudong Liu, Menglin Wu, Yuping Wang, Hang Zhao, and Yuxuan Wang. 2022. Controllable and lossless non-autoregressive end-to-end text-to-speech. arXiv preprint arXiv:2207.06088.
[^Oord2016WaveNet]: [**WaveNet**: A Generative Model for Raw Audio](../Models/Vocoder/2016.09.12_WaveNet.md). ArXiv:1609.03499v2.
[^Kang2023grad]: Minki Kang, Dongchan Min, and Sung~Ju Hwang. 2023. {Grad-StyleSpeech}: Any-speaker adaptive text-to-speech synthesis with diffusion models. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 1--5.
[^Shen2023NaturalSpeech2]: [**NaturalSpeech2**: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers](../Models/Diffusion/2023.04.18_NaturalSpeech2.md). ArXiv:2304.09116v3/ICLR2024Spotlight.
[^Liu2023promptstyle]: Guanghou Liu, Yongmao Zhang, Yi~Lei, Yunlin Chen, Rui Wang, Zhifei Li, and Lei Xie. 2023{\natexlab{a}}. {PromptStyle}: Controllable style transfer for text-to-speech with natural language descriptions. arXiv preprint arXiv:2305.19522.
[^Li2023styletts2]: Yinghao~Aaron Li, Cong Han, Vinay~S Raghavan, Gavin Mischler, and Nima Mesgarani. 2023. Style{TTS} 2: Towards human-level text-to-speech through style diffusion and adversarial training with large speech language models. In 37th Conference on Neural Information Processing Systems, pages 1--28.
[^Kaneko2022istftnet]: Takuhiro Kaneko, Kou Tanaka, Hirokazu Kameoka, and Shogo Seki. 2022. istftnet: Fast and lightweight mel-spectrogram vocoder incorporating inverse short-time {Fourier} transform. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6207--6211.
[^Le2023VoiceBox]: [**VoiceBox**: Text-Guided Multilingual Universal Speech Generation at Scale](../Models/FlowMatching/2023.06.23_Voicebox.md). ArXiv:2306.15687/NeurIPS2023Poster.
[^Jiang2024mega]: Ziyue Jiang, Jinglin Liu, Yi~Ren, Jinzheng He, Zhenhui Ye, Shengpeng Ji, Qian Yang, Chen Zhang, Pengfei Wei, Chunfeng Wang, and 1 others. 2024. Mega-{TTS} 2: Boosting prompting mechanisms for zero-shot speech synthesis. In The Twelfth International Conference on Learning Representations.
[^Leng2023prompttts2]: Yichong Leng, Zhifang Guo, Kai Shen, Xu~Tan, Zeqian Ju, Yanqing Liu, Yufei Liu, Dongchao Yang, Leying Zhang, Kaitao Song, and 1 others. 2023. {PromptTTS} 2: Describing and generating voices with text prompt. In The Twelfth International Conference on Learning Representations.
[^Gu2023durian]: Yu~Gu, Yianrao Bian, Guangzhi Lei, Chao Weng, and Dan Su. 2023. {DurIAN-E}: Duration informed attention network for expressive text-to-speech synthesis. arXiv preprint arXiv:2309.12792.
[^Shimizu2024prompttts++]: Reo Shimizu, Ryuichi Yamamoto, Masaya Kawamura, Yuma Shirahata, Hironori Doi, Tatsuya Komatsu, and Kentaro Tachibana. 2024. {PromptTTS++}: Controlling speaker identity in prompt-based text-to-speech using natural language descriptions. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 12672--12676.
[^Lee2022BigVGAN]: [**BigVGAN**: Large-Scale Generative Adversarial Networks for High-Fidelity Audio Synthesis](../Models/Vocoder/2022.06.09_BigVGAN.md). ArXiv:2206.04658/ICLR2023Poster.
[^Liu2023generative]: Alexander~H Liu, Matthew Le, Apoorv Vyas, Bowen Shi, Andros Tjandra, and Wei-Ning Hsu. 2024{\natexlab{a}}. Generative pre-training for speech with flow matching. In The Twelfth International Conference on Learning Representations.
[^Kim2024p]: Sungwon Kim, Kevin Shih, Joao~Felipe Santos, Evelina Bakhturina, Mikyas Desta, Rafael Valle, Sungroh Yoon, Bryan Catanzaro, and 1 others. 2024{\natexlab{c}}. {P-Flow}: a fast and data-efficient zero-shot {TTS} through speech prompting. Advances in Neural Information Processing Systems, 36.
[^Gao2023e3]: Yuan Gao, Nobuyuki Morioka, Yu~Zhang, and Nanxin Chen. 2023. E3 {TTS}: Easy end-to-end diffusion-based text to speech. In IEEE Automatic Speech Recognition and Understanding Workshop, pages 1--8.
[^Lee2023HierSpeech++]: [**HierSpeech++**: Bridging the Gap between Semantic and Acoustic Representation of Speech by Hierarchical Variational Inference for Zero-shot Speech Synthesis](../Models/E2E/2023.11.21_HierSpeech++.md). ArXiv:2311.12454v2.
[^Vyas2023audiobox]: Apoorv Vyas, Bowen Shi, Matthew Le, Andros Tjandra, Yi-Chiao Wu, Baishan Guo, Jiemin Zhang, Xinyue Zhang, Robert Adkins, William Ngan, and 1 others. 2023. Audiobox: Unified audio generation with natural language prompts. arXiv preprint arXiv:2312.15821.
[^Ye2024flashspeech]: Zhen Ye, Zeqian Ju, Haohe Liu, Xu~Tan, Jianyi Chen, Yiwen Lu, Peiwen Sun, Jiahao Pan, Weizhen Bian, Shulin He, and 1 others. 2024. {FlashSpeech}: Efficient zero-shot speech synthesis. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 6998--7007.
[^Ju2024NaturalSpeech3]: [**NaturalSpeech3/FACodec**: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models](../Models/Diffusion/2024.03.05_NaturalSpeech3.md). ArXiv:2403.03100v3/ICML2024Oral.
[^Ji2024controlspeech]: Shengpeng Ji, Jialong Zuo, Wen Wang, Minghui Fang, Siqi Zheng, Qian Chen, Ziyue Jiang, Hai Huang, Zehan Wang, Xize Cheng, and 1 others. 2024{\natexlab{c}}. {ControlSpeech}: Towards simultaneous zero-shot speaker cloning and zero-shot language style control with decoupled codec. arXiv preprint arXiv:2406.01205.
[^Yang2024simplespeech]: Dongchao Yang, Dingdong Wang, Haohan Guo, Xueyuan Chen, Xixin Wu, and Helen Meng. 2024{\natexlab{c}}. {SimpleSpeech}: Towards simple and efficient text-to-speech with scalar latent transformer diffusion models. In Proceedings of the Annual Conference of the International Speech Communication Association, pages 4398--4402.
[^Lee2024DiTTo-TTS]: [**DiTTo-TTS**: Diffusion Transformers for Scalable Text-to-Speech without Domain-Specific Factors](../Models/FlowMatching/2024.06.17_DiTTo-TTS.md). ArXiv:2406.11427v2/ICLR2025Poster.
[^Eskimez2024E2-TTS]: [**E2 TTS**: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS](../Models/FlowMatching/2024.06.26_E2_TTS.md). ArXiv:2406.18009v2/SLT2024.
[^Ji2024MobileSpeech]: [**MobileSpeech**: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech](../Models/SpeechLM/2024.02.14_MobileSpeech.md). ArXiv:2402.09378v2/ACL2024.
[^Siuzdak2024vocos]: Hubert Siuzdak. 2024. Vocos: Closing the gap between time-domain and {Fourier}-based neural vocoders for high-quality audio synthesis. In The Twelfth International Conference on Learning Representations.
[^Park2024dex]: Hyun~Joon Park, Jin~Sob Kim, Wooseok Shin, and Sung~Won Han. 2024{\natexlab{a}}. {DEX-TTS}: Diffusion-based expressive text-to-speech with style modeling on time variability. arXiv preprint arXiv:2406.19135.
[^Wang2024artspeech]: Zhongxu Wang, Yujia Wang, Mingzhu Li, and Hua Huang. 2024. {ArtSpeech}: Adaptive text-to-speech synthesis with articulatory representations. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 535--544.
[^Xiao2024contrastive]: Yujia Xiao, Xi~Wang, Xu~Tan, Lei He, Xinfa Zhu, Sheng Zhao, and Tan Lee. 2024. Contrastive context-speech pretraining for expressive text-to-speech synthesis. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 2099--2107.
[^Yang2024simplespeech2]: Dongchao Yang, Rongjie Huang, Yuanyuan Wang, Haohan Guo, Dading Chong, Songxiang Liu, Xixin Wu, and Helen Meng. 2024{\natexlab{a}}. {SimpleSpeech} 2: Towards simple and efficient text-to-speech with flow-based scalar latent transformer diffusion models. arXiv preprint arXiv:2408.13893.
[^Liu2024e1]: Zhijun Liu, Shuai Wang, Pengcheng Zhu, Mengxiao Bi, and Haizhou Li. 2025{\natexlab{b}}. {E1 TTS}: Simple and fast non-autoregressive {TTS}. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 1--5.
[^Li2024stylettszs]: Yinghao~Aaron Li, Xilin Jiang, Cong Han, and Nima Mesgarani. 2024. {StyleTTS-ZS}: Efficient high-quality zero-shot text-to-speech synthesis with distilled time-varying style diffusion. arXiv preprint arXiv:2409.10058.
[^Park2024nanovoice]: Nohil Park, Heeseung Kim, Che~Hyun Lee, Jooyoung Choi, Jiheum Yeom, and Sungroh Yoon. 2024{\natexlab{b}}. {NanoVoice}: Efficient speaker-adaptive text-to-speech for multiple speakers. arXiv preprint arXiv:2409.15760.
[^He2024multi]: Shuwei He, Rui Liu, and Haizhou Li. 2024. Multi-source spatial knowledge understanding for immersive visual text-to-speech. arXiv preprint arXiv:2410.14101.
[^Wang2024MaskGCT]: [**MaskGCT**: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer](../Models/SpeechLM_TTS/2024.09.01_MaskGCT.md). ArXiv:2409.00750v3.
[^Cho2024emosphere++]: Deok-Hyeon Cho, Hyung-Seok Oh, Seung-Bin Kim, and Seong-Whan Lee. 2024. {EmoSphere}++: Emotion-controllable zero-shot text-to-speech via emotion-adaptive spherical vector. arXiv preprint arXiv:2411.02625.
[^Cong2024emodubber]: Gaoxiang Cong, Jiadong Pan, Liang Li, Yuankai Qi, Yuxin Peng, Anton van~den Hengel, Jian Yang, and Qingming Huang. 2024. {EmoDubber}: Towards high quality and emotion controllable movie dubbing. arXiv preprint arXiv:2412.08988.
[^Inoue2024HED]: Sho Inoue, Kun Zhou, Shuai Wang, and Haizhou Li. 2024. Hierarchical control of emotion rendering in speech synthesis. arXiv preprint arXiv:2412.12498.
[^Liu2025diffstyletts]: Jiaxuan Liu, Zhaoci Liu, Yajun Hu, Yingying Gao, Shilei Zhang, and Zhenhua Ling. 2025{\natexlab{a}}. {DiffStyleTTS}: Diffusion-based hierarchical prosody modeling for text-to-speech with diverse and controllable styles. In Proceedings of the 31st International Conference on Computational Linguistics, pages 5265--5272.
[^Zhang2025proemo]: Shaozuo Zhang, Ambuj Mehrish, Yingting Li, and Soujanya Poria. 2025{\natexlab{a}}. {PROEMO}: Prompt-driven text-to-speech synthesis based on emotion and intensity control. arXiv preprint arXiv:2501.06276.
[^Skerry2018towards]: RJ~Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron Weiss, Rob Clark, and Rif~A Saurous. 2018. Towards end-to-end prosody transfer for expressive speech synthesis with tacotron. In International Conference on Machine Learning, pages 4693--4702.
[^Stanton2018predicting]: Daisy Stanton, Yuxuan Wang, and RJ~Skerry-Ryan. 2018. Predicting expressive speaking style from text in end-to-end speech synthesis. In IEEE Spoken Language Technology Workshop, pages 595--602. IEEE.
[^Hsu2018GMVAE-Tacotron]: [**GMVAE-Tacotron**: Hierarchical Generative Modeling for Controllable Speech Synthesis](../Models/Acoustic/2018.10.16_GMVAE-Tacotron.md). ArXiv:1810.07217v2/ICLR2019.
[^Zhang2019learning]: Ya-Jie Zhang, Shifeng Pan, Lei He, and Zhen-Hua Ling. 2019. Learning latent representations for style control and transfer in end-to-end speech synthesis. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6945--6949.
[^Yu2020durian]: Chengzhu Yu, Heng Lu, Na~Hu, Meng Yu, Chao Weng, Kun Xu, Peng Liu, Deyi Tuo, Shiyin Kang, Guangzhi Lei, and 1 others. 2020. {DurIAN}: Duration informed attention network for speech synthesis. In Proceedings of the Annual Conference of the International Speech Communication Association, pages 2027--2031.
[^Valle2020flowtron]: Rafael Valle, Kevin Shih, Ryan Prenger, and Bryan Catanzaro. 2020. Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis. arXiv preprint arXiv:2005.05957.
[^Lei2022msemotts]: Yi~Lei, Shan Yang, Xinsheng Wang, and Lei Xie. 2022. {MsEmoTTS}: Multi-scale emotion transfer, prediction, and control for emotional speech synthesis. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:853--864.
[^Wang2023VALL-E]: [**VALL-E**: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers](../Models/CodecLM/2023.01.05_VALL-E.md). ArXiv:2301.02111/TASLP2025.
[^Kharitonov2023SPEAR-TTS]: [**SPEAR-TTS**: Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision](../Models/SpeechLM_TTS/2023.02.07_SPEAR-TTS.md). ArXiv:2302.03540v1/TACL2023.
[^Zeghidour2021SoundStream]: [**SoundStream**: An End-to-End Neural Audio Codec](../Models/Codec/2021.07.07_SoundStream.md). ArXiv:2107.03312v1/TASLP2021.
[^Huang2023makeavoice]: Rongjie Huang, Chunlei Zhang, Yongqi Wang, Dongchao Yang, Luping Liu, Zhenhui Ye, Ziyue Jiang, Chao Weng, Zhou Zhao, and Dong Yu. 2023{\natexlab{b}}. {Make-A-Voice}: Unified voice synthesis with discrete representation. arXiv preprint arXiv:2305.19269.
[^Betker2023TorToiSe-TTS]: [**TorToiSe-TTS**: Better Speech Synthesis Through Scaling](../Models/Diffusion/2023.05.12_TorToise-TTS.md). ArXiv:2305.07243v2.
[^JangLYKK21univnet]: Won Jang, Dan Lim, Jaesam Yoon, Bongwan Kim, and Juntae Kim. 2021. {UnivNet}: A neural vocoder with multi-resolution spectrogram discriminators for high-fidelity waveform generation. In Annual Conference of the International Speech Communication Association, pages 2207--2211.
[^Jiang2023megavoic]: Ziyue Jiang, Yi~Ren, Zhenhui Ye, Jinglin Liu, Chen Zhang, Qian Yang, Shengpeng Ji, Rongjie Huang, Chunfeng Wang, Xiang Yin, and 1 others. 2023. Mega-{TTS}: Zero-shot text-to-speech at scale with intrinsic inductive bias. arXiv preprint arXiv:2306.03509.
[^Kim2023SCVALL-E]: [**SC VALL-E**: Style-Controllable Zero-Shot Text to Speech Synthesizer](../Models/CodecLM/2023.07.20_SC_VALL-E.md). ArXiv:2307.10550v1.
[^Ji2023TextrolSpeech]: [**TextrolSpeech**: A Text Style Control Speech Corpus With Codec Language Text-to-Speech Models](../Datasets/2023.08.28_TextrolSpeech.md). ArXiv:2308.14430v1/ICASSP2024.
[^Yang2023UniAudio]: [**UniAudio**: An Audio Foundation Model Toward Universal Audio Generation](../Models/SpeechLM/ST2S/2023.10.01_UniAudio.md). ArXiv:2310.00704.
[^Song2024ELLA-V]: [**ELLA-V**: Stable Neural Codec Language Modeling with Alignment-Guided Sequence Reordering](../Models/CodecLM/2024.01.14_ELLA-V.md). ArXiv:2401.07333v1.
[^Lajszczak2024BASE-TTS]: [**BASE TTS**: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data](../Models/SpeechLM/ST2S/2024.02.12_BASE-TTS.md). ArXiv:2402.08093v2.
[^Kim2024CLaM-TTS]: [**CLaM-TTS**: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech](../Models/CodecLM/2024.04.03_CLaM-TTS.md). ArXiv:2404.02781v1/ICLR2024.
[^Xin2024RALL-E]: [**RALL-E**: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis](../Models/CodecLM/2024.04.04_RALL-E.md). ArXiv:2404.03204v3.
[^Liu2024ARDiT]: [**ARDiT**: Autoregressive Diffusion Transformer for Text-to-Speech Synthesis](../Models/Diffusion/2024.06.08_ARDiT.md). ArXiv:2406.05551v1.
[^Han2024VALL-ER]: [**VALL-E R**: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment](../Models/CodecLM/2024.06.12_VALL-E_R.md). ArXiv:2406.07855.
[^Chen2024VALL-E2]: [**VALL-E 2**: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers](../Models/CodecLM/2024.06.08_VALL-E_2.md). ArXiv:2406.05370v2.
[^Anastassiou2024Seed-TTS]: [**Seed-TTS**: A Family of High-Quality Versatile Speech Generation Models](../Models/SpeechLM_TTS/2024.06.04_Seed-TTS.md). ArXiv:2406.02430v1.
[^Peng2024VoiceCraft]: [**VoiceCraft**: Zero-Shot Speech Editing and Text-to-Speech in the Wild](../Models/SpeechLM_TTS/2024.03.25_VoiceCraft.md). ArXiv:2403.16973v3/ACL2024.
[^Casanova2024XTTS]: [**XTTS**: A Massively Multilingual Zero-Shot Text-to-Speech Model](../Models/SpeechLM_TTS/2024.06.07_XTTS.md). ArXiv:2406.04904v1.
[^Meng2024MELLE]: [**MELLE**: Autoregressive Speech Synthesis without Vector Quantization](../Models/SpeechLM_TTS/2024.07.11_MELLE.md). ArXiv:2407.08551v2.
[^Gao2024emo]: Xiaoxue Gao, Chen Zhang, Yiming Chen, Huayun Zhang, and Nancy~F Chen. 2024. {Emo-DPO}: Controllable emotional speech synthesis through direct preference optimization. arXiv preprint arXiv:2409.10157.
[^Guo2024FireRedTTS]: [**FireRedTTS**: A Foundation Text-To-Speech Framework for Industry-Level Generative Speech Applications](../Models/SpeechLM_TTS/2024.09.05_FireRedTTS.md). ArXiv:2409.03283v2.
[^Guo2024CoFi-Speech]: [**CoFi-Speech**: Speaking from Coarse to Fine: Improving Neural Codec Language Model via Multi-Scale Speech Coding and Generation](../Models/CodecLM/2024.09.18_CoFi-Speech.md). ArXiv:2409.11630v1.
[^Chen2024takin]: Sijing Chen, Yuan Feng, Laipeng He, Tianwei He, Wendi He, Yanni Hu, Bin Lin, Yiting Lin, Yu~Pan, Pengfei Tan, and 1 others. 2024{\natexlab{b}}. Takin: A cohort of superior quality zero-shot speech generation models. arXiv preprint arXiv:2409.12139.
[^Nishimura2024HALL-E]: [**HALL-E**: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis](../Models/CodecLM/2024.10.06_HALL-E.md). ArXiv:2410.04380v1.
[^Liao2024Fish-Speech]: [**Fish-Speech**: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis](../Models/SpeechLM/2024.11.02_Fish-Speech.md). ArXiv:2411.01156v2.
[^Chen2024SLAM-Omni]: [**SLAM-Omni**: Timbre-Controllable Voice Interaction System with Single-Stage Training](../Models/SpeechLM/Interaction/2024.12.20_SLAM-Omni.md). ArXiv:2412.15649v1.
[^Yang2024IST-LM]: [**IST-LM**: Interleaved Speech-Text Language Models for Simple Streaming Text-to-Speech Synthesis](../Models/CodecLM/2024.12.20_IST-LM.md). ArXiv:2412.16102v3.
[^Xia2024KALL-E]: [**KALL-E**:Autoregressive Speech Synthesis with Next-Distribution Prediction](../Models/SpeechLM_TTS/2024.12.22_KALL-E.md). ArXiv:2412.16846v2.
[^Lu2025idea-TTS]: Ye-Xin Lu, Hui-Peng Du, Zheng-Yan Sheng, Yang Ai, and Zhen-Hua Ling. 2025. Incremental disentanglement for environment-aware zero-shot text-to-speech synthesis. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 1--5.
[^Li2025FleSpeech]: [**FleSpeech**: Flexibly Controllable Speech Generation with Various Prompts](../Models/Acoustic/2025.01.08_FleSpeech.md). ArXiv:2501.04644v2.
[^Donahue2018wavegan]: Chris Donahue, Julian McAuley, and Miller Puckette. 2018. Adversarial audio synthesis. In International Conference on Learning Representations.
[^Huang2025stepaudio]: Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, and 1 others. 2025. {Step-Audio}: Unified understanding and generation in intelligent speech interaction. arXiv preprint arXiv:2502.11946.
[^Yang2025emovoice]: Guanrou Yang, Chen Yang, Qian Chen, Ziyang Ma, Wenxi Chen, Wen Wang, Tianrui Wang, Yifan Yang, Zhikang Niu, Wenrui Liu, and 1 others. 2025. {EmoVoice}: {LLM}-based emotional text-to-speech model with freestyle text prompting. arXiv preprint arXiv:2504.12867.
[^Elias2021paralleltacotron]: Isaac Elias, Heiga Zen, Jonathan Shen, Yu~Zhang, Ye~Jia, Ron~J Weiss, and Yonghui Wu. 2021{\natexlab{b}}. Parallel {Tacotron}: Non-autoregressive and controllable {TTS}. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 5709--5713.
[^Kobyzev2020normalizing]: Ivan Kobyzev, Simon~JD Prince, and Marcus~A Brubaker. 2020. Normalizing flows: An introduction and review of current methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(11):3964--3979.
[^Ho2020DDPM]: [**DDPM**: Denoising Diffusion Probabilistic Models](../Models/Diffusion/_2020.06.19_DDPM.md). ArXiv:2006.11239v2.
[^Liu2023AudioLDM]: [**AudioLDM**: Text-to-Audio Generation with Latent Diffusion Models](../Models/Diffusion/2023.01.29_AudioLDM.md). ArXiv:2301.12503v3/ICML2023.
[^Huang2023Make-An-Audio]: [**Make-An-Audio**: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models](../Models/Diffusion/2023.01.30_Make-An-Audio.md). ArXiv:2301.12661v1/ICML2023.
[^Rezende2015variational]: Danilo Rezende and Shakir Mohamed. 2015. Variational inference with normalizing flows. In International Conference on Machine Learning, pages 1530--1538.
[^Lipman2022FM]: [**FM**: Flow Matching for Generative Modeling](../Models/FlowMatching/_2022.10.06_Flow_Matching.md). ArXiv:2210.02747v2.
[^Woo2023convnext]: Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In~So Kweon, and Saining Xie. 2023. {ConvNeXt V2}: Co-designing and scaling convnets with masked autoencoders. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16133--16142.
[^Liu2022RectifiedFlow]: [**RectifiedFlow**: Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow](../Models/FlowMatching/_2022.09.07_RectifiedFlow.md). ArXiv:2209.03003v1.
[^Wang2017Tacotron]: [**Tacotron**: Towards End-to-End Speech Synthesis](../Models/Acoustic/2017.03.29_Tacotron.md). ArXiv:1703.10135v2/InterSpeech2017.
[^Defossez2022EnCodec]: [**EnCodec**: High Fidelity Neural Audio Compression](../Models/Codec/2022.10.24_EnCodec.md). ArXiv:2210.13438/TMLR2023.
[^Rafailov2023dpo]: Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher~D Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36:53728--53741.
[^Huang2023AudioGPT]: [**AudioGPT**: Understanding and Generating Speech, Music, Sound, and Talking Head](../Models/SpeechLM/Interaction/2023.04.25_AudioGPT.md). ArXiv:2304.12995v1.
[^Goodfellow2020gan]: Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarial networks. Communications of the ACM, 63(11):139--144.
[^Yang2021ganspeech]: Jinhyeok Yang, Jae{-}Sung Bae, Taejun Bak, Young{-}Ik Kim, and Hoon{-}Young Cho. 2021. {GANSpeech}: Adversarial training for high-fidelity multi-speaker speech synthesis. In 22nd Annual Conference of the International Speech Communication Association, pages 2202--2206.
[^Hsu2019Disentangling]: Wei-Ning Hsu, Yu~Zhang, Ron~J. Weiss, Yu-An Chung, Yuxuan Wang, Yonghui Wu, and James Glass. 2019. \href {https://doi.org/10.1109/ICASSP.2019.8683561} {Disentangling correlated speaker and noise for speech synthesis via data augmentation and adversarial factorization}. In ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5901--5905.
[^Lee2021multi]: Sang-Hoon Lee, Hyun-Wook Yoon, Hyeong-Rae Noh, Ji-Hoon Kim, and Seong-Whan Lee. 2021. Multi-spectrogan: High-diversity and high-fidelity spectrogram generation with adversarial style combination for speech synthesis. In Proceedings of the AAAI Conference on Artificial Intelligence, volume~35, pages 13198--13206.
[^Li2022cross]: Tao Li, Xinsheng Wang, Qicong Xie, Zhichao Wang, and Lei Xie. 2022. Cross-speaker emotion disentangling and transfer for end-to-end speech synthesis. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:1448--1460.
[^Lu2023speechtriplenet]: Hui Lu, Xixin Wu, Zhiyong Wu, and Helen Meng. 2023. {SpeechTripleNet}: End-to-end disentangled speech representation learning for content, timbre and prosody. In Proceedings of the 31st ACM International Conference on Multimedia, pages 2829--2837.
[^Kim2024pflow]: Sungwon Kim, Kevin Shih, Joao~Felipe Santos, Evelina Bakhturina, Mikyas Desta, Rafael Valle, Sungroh Yoon, Bryan Catanzaro, and 1 others. 2024{\natexlab{d}}. {P-Flow}: a fast and data-efficient zero-shot {TTS} through speech prompting. Advances in Neural Information Processing Systems, 36:74213--74228.
[^Kong2020DiffWave]: [**DiffWave**: A Versatile Diffusion Model for Audio Synthesis](../Models/Vocoder/2020.09.21_DiffWave.md). ArXiv:2009.09761v3/ICLR2021Oral.
[^Huang2022fastdiff]: Rongjie Huang, Max W.~Y. Lam, Jun Wang, Dan Su, Dong Yu, Yi~Ren, and Zhou Zhao. 2022{\natexlab{a}}. {FastDiff}: A fast conditional diffusion model for high-quality speech synthesis. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, pages 4157--4163.
[^Hsu2021HuBERT]: [**HuBERT**: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](../Models/Tokenizers/2021.06.14_HuBERT.md). ArXiv:2106.07447/TASLP2021.
[^Zhou2022emotional]: Kun Zhou, Berrak Sisman, Rui Liu, and Haizhou Li. 2022. Emotional voice conversion: Theory, databases and {ESD}. Speech Communication, 137:1--18.
[^Busso2008iemocap]: Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette~N Chang, Sungbok Lee, and Shrikanth~S Narayanan. 2008. {IEMOCAP}: Interactive emotional dyadic motion capture database. Language Resources and Evaluation, 42:335--359.
[^Ringeval2013RECOLA]: Fabien Ringeval, Andreas Sonderegger, Juergen Sauer, and Denis Lalanne. 2013. Introducing the {RECOLA} multimodal corpus of remote collaborative and affective interactions. In 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, pages 1--8.
[^Bagher2018cmu-Mosei]: AmirAli Bagher~Zadeh, Paul~Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2018. Multimodal language analysis in the wild: {CMU}-{MOSEI} dataset and interpretable dynamic fusion graph. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2236--2246.
[^Jin2024SpeechCraft]: [**SpeechCraft**: A Fine-grained Expressive Speech Dataset with Natural Language Description](../Datasets/2024.08.24_SpeechCraft.md). ArXiv:2408.13608v1/ACM Multimedia 2024.
[^Lyth2024natural]: Dan Lyth and Simon King. 2024. Natural language guidance of high-fidelity text-to-speech with synthetic annotations. arXiv preprint arXiv:2402.01912.
[^Byrne2019taskmaster]: Bill Byrne, Karthik Krishnamoorthi, Chinnadhurai Sankar, Arvind Neelakantan, Daniel Duckworth, Semih Yavuz, Ben Goodrich, Amit Dubey, Andy Cedilnik, and Kyu-Young Kim. 2019. Taskmaster-1: Toward a realistic and diverse dialog dataset. arXiv preprint arXiv:1909.05358.
[^Lee2023dailytalk]: Keon Lee, Kyumin Park, and Daeyoung Kim. 2023{\natexlab{a}}. {DailyTalk}: Spoken dialogue dataset for conversational text-to-speech. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 1--5.
[^Yang2022magicdata]: Zehui Yang, Yifan Chen, Lei Luo, Runyan Yang, Lingxuan Ye, Gaofeng Cheng, Ji~Xu, Yaohui Jin, Qingqing Zhang, Pengyuan Zhang, and 1 others. 2022. Open {Source} {MagicData-RAMC}: A rich annotated mandarin conversational ({RAMC}) speech dataset. arXiv preprint arXiv:2203.16844.
[^Kominek2008synthesizer]: John Kominek, Tanja Schultz, and Alan~W Black. 2008. Synthesizer voice quality of new languages calibrated with mean mel cepstral distortion. In Spoken Language Technologies for Under-Resourced Languages, pages 63--68.
[^Binkowski2019GAN-TTS]: [**GAN-TTS**: High Fidelity Speech Synthesis with Adversarial Networks](../Models/Vocoder/2019.09.25_GAN-TTS.md). ArXiv:1909.11646v2.
[^Hannun2014deepspeech]: Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, and 1 others. 2014. {Deep Speech}: Scaling up end-to-end speech recognition. arXiv preprint arXiv:1412.5567.
[^Enwiki2024wer]: {Wikipedia}. 2024. Word error rate. https://en.wikipedia.org/wiki/Word_error_rate. Accessed: 2024-12-07.
[^Brecht20ECAPA-TDNN]: Brecht Desplanques, Jenthe Thienpondt, and Kris Demuynck. 2020. {ECAPA-TDNN:} emphasized channel attention, propagation and aggregation in {TDNN} based speaker verification. In 21st Annual Conference of the International Speech Communication Association, pages 3830--3834.
[^Snyder2018xvectors]: David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev Khudanpur. 2018. {X-Vectors}: Robust dnn embeddings for speaker recognition. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 5329--5333.
[^Rix2001PESQ]: [**PESQ**: Perceptual Evaluation of Speech Quality - A New Method for Speech Quality Assessment of Telephone Networks and Codecs](../Evaluations/2001.05.07_PESQ.md). ICASSP2001.
[^Loizou2011speech]: Philipos~C Loizou. 2011. Speech quality assessment. In Multimedia Analysis, Processing and Communications, pages 623--654. Springer.
[^Lian2025apg]: Zhicheng Lian, Lizhi Wang, and Hua Huang. 2025. {APG-MOS}: Auditory perception guided-{MOS} predictor for synthetic speech. arXiv preprint arXiv:2504.20447.
[^An2022disentangling]: Xiaochun An, Frank~K Soong, and Lei Xie. 2022. Disentangling style and speaker attributes for {TTS} style transfer. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:646--658.
[^Wang2023generalizable]: Wenbin Wang, Yang Song, and Sanjay Jha. 2023{\natexlab{b}}. Generalizable zero-shot speaker adaptive speech synthesis with disentangled representations. In Annual Conference of the International Speech Communication Association 2023, pages 4454--4458.
[^Chen2021GigaSpeech]: [**GigaSpeech**: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio](../Datasets/2021.06.13_GigaSpeech.md). ArXiv:2106.06909v1.
[^Tae2022editts]: Jaesung Tae, Hyeongju Kim, and Taesu Kim. 2022. {EdiTTS}: Score-based editing for controllable text-to-speech. In Annual Conference of the International Speech Communication Association, pages 421--425.
[^Tan2021editspeech]: Daxin Tan, Liqun Deng, Yu~Ting Yeung, Xin Jiang, Xiao Chen, and Tan Lee. 2021{\natexlab{a}}. {EditSpeech}: A text based speech editing system using partial inference and bidirectional fusion. In IEEE Automatic Speech Recognition and Understanding Workshop, pages 626--633.
[^Goto2020face2speech]: Shunsuke Goto, Kotaro Onishi, Yuki Saito, Kentaro Tachibana, and Koichiro Mori. 2020. {Face2Speech}: Towards multi-speaker text-to-speech synthesis using an embedding vector predicted from a face image. In Proceedings of the Annual Conference of the International Speech Communication Association, pages 1321--1325.
[^Lu2022visualtts]: Junchen Lu, Berrak Sisman, Rui Liu, Mingyang Zhang, and Haizhou Li. 2022. {VisualTTS}: {TTS} with accurate lip-speech synchronization for automatic voice over. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 8032--8036.
[^Fang2024LLaMA-Omni]: [**LLaMA-Omni**: Seamless Speech Interaction with Large Language Models](../Models/SpeechLM/Interaction/2024.09.10_LLaMA-Omni.md). ArXiv:2409.06666v2/ICLR2025.
[^Zhang2023SpeechGPT]: [**SpeechGPT**: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities](../Models/SpeechLM/Interaction/2023.05.18_SpeechGPT.md). ArXiv:2305.11000v2/EMNLP2023.
[^Livingstone2018ryerson]: Steven~R Livingstone and Frank~A Russo. 2018. The ryerson audio-visual database of emotional speech and song ({RAVDESS}): A dynamic, multimodal set of facial and vocal expressions in {North} {American} {English}. PloS One, 13(5):e0196391.
[^Shi2020AISHELL-3]: [**AISHELL-3**: A Multi-speaker Mandarin TTS Corpus and the Baselines](../Datasets/2020.10.22_AISHELL-3.md). ArXiv:2010.11567v2.
[^Ardila2020commonvoice]: Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. 2020. {Common Voice}: A massively-multilingual speech corpus. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4218--4222.
[^Zhang2021WenetSpeech]: [**WenetSpeech**: A 10000+ Hours Multi-domain Mandarin Corpus for Speech Recognition](../Datasets/2021.10.07_WenetSpeech.md). ArXiv:2110.03370v5.
[^Toloka2024CLESC]: {Toloka}. 2024. Crowd labeled emotions and speech characteristics. https://huggingface.co/datasets/toloka/CLESC. Accessed: 2024-03-23.
[^Yang2024mscenespeech]: Qian Yang, Jialong Zuo, Zhe Su, Ziyue Jiang, Mingze Li, Zhou Zhao, Feiyang Chen, Zhefeng Wang, and Baoxing Huai. 2024{\natexlab{d}}. {MSceneSpeech}: A multi-scene speech dataset for expressive speech synthesis. In Annual Conference of the International Speech Communication Association 2024, pages 1845--1849.
[^Enwiki2024mos]: {Wikipedia}. 2025. Mean opinion score. https://en.wikipedia.org/wiki/Mean_opinion_score. Accessed: 2024-12-07.
[^Rabiner1968digital]: Lawrence~R Rabiner. 1968. Digital-formant synthesizer for speech-synthesis studies. The Journal of the Acoustical Society of America, 43(4):822--828.
[^Allen1987mitalk]: Jonathan Allen, M~Sharon Hunnicutt, Dennis~H Klatt, Robert~C Armstrong, and David~B Pisoni. 1987. From Text to Speech: The MITalk System. Cambridge University Press.
[^Purcell2006adaptive]: David~W Purcell and Kevin~G Munhall. 2006. Adaptive control of vowel formant frequency: Evidence from real-time formant manipulation. The Journal of the Acoustical Society of America, 120(2):966--977.
[^Hunt1996373]: Andrew~J. Hunt and Alan~W. Black. 1996. Unit selection in a concatenative speech synthesis system using a large speech database. In IEEE International Conference on Acoustics, Speech, and Signal Processing, volume~1, pages 373--376.
[^Wouters2001control]: Johan Wouters and Michael~W Macon. 2001. Control of spectral dynamics in concatenative speech synthesis. IEEE Transactions on Speech and Audio Processing, 9(1):30--38.
[^Bulut2002expressive]: Murtaza Bulut, Shrikanth~S Narayanan, and Ann~K Syrdal. 2002. Expressive speech synthesis using a concatenative synthesizer. In Proceedings of the Annual Conference of the International Speech Communication Association, pages 1265--1268.
[^Nose2007style]: Takashi Nose, Junichi Yamagishi, Takashi Masuko, and Takao Kobayashi. 2007. A style control technique for {HMM}-based expressive speech synthesis. IEICE Transactions on Information and Systems, 90(9):1406--1413.
[^Ling2009integrating]: Zhen-Hua Ling, Korin Richmond, Junichi Yamagishi, and Ren-Hua Wang. 2009. Integrating articulatory features into {HMM}-based parametric speech synthesis. IEEE Transactions on Audio, Speech, and Language Processing, 17(6):1171--1185.
[^Tokuda2000speech]: K.~Tokuda, T.~Yoshimura, T.~Masuko, T.~Kobayashi, and T.~Kitamura. 2000. Speech parameter generation algorithms for {HMM}-based speech synthesis. In IEEE International Conference on Acoustics, Speech, and Signal Processing, volume~3, pages 1315--1318.
[^Yamagishi2009robust]: Junichi Yamagishi, Takashi Nose, Heiga Zen, Zhen-Hua Ling, Tomoki Toda, Keiichi Tokuda, Simon King, and Steve Renals. 2009. Robust speaker-adaptive {HMM}-based text-to-speech synthesis. IEEE Transactions on Audio, Speech, and Language Processing, 17(6):1208--1230.
[^Yamagishi2003modeling]: Junichi Yamagishi, Koji Onishi, Takashi Masuko, and Takao Kobayashi. 2003. Modeling of various speaking styles and emotions for {HMM}-based speech synthesis. In Proceedings of the Annual Conference of the International Speech Communication Association, pages 2461--2464.
[^Shen2017Tacotron2]: [**Tacotron2**: Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions](../Models/Acoustic/2017.12.16_Tacotron2.md). ArXiv:1712.05884/ICASSP2018.
[^Ren2020FastSpeech2]: [**FastSpeech2**: Fast and High-Quality End-to-End Text-to-Speech](../Models/Acoustic/2020.06.08_FastSpeech2.md). ArXiv:2006.04558/ICLR2021.
[^Fan2015multi]: Yuchen Fan, Yao Qian, Frank~K Soong, and Lei He. 2015. Multi-speaker modeling and speaker adaptation for {DNN}-based {TTS} synthesis. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 4475--4479.
[^Um2020emotional]: Se-Yun Um, Sangshin Oh, Kyungguen Byun, Inseon Jang, ChungHyun Ahn, and Hong-Goo Kang. 2020. Emotional speech synthesis with rich and granularized control. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 7254--7258.
[^Cooper2020zero]: Erica Cooper, Cheng-I Lai, Yusuke Yasuda, Fuming Fang, Xin Wang, Nanxin Chen, and Junichi Yamagishi. 2020. Zero-shot multi-speaker text-to-speech with state-of-the-art neural speaker embeddings. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6184--6188.
[^Devlin2018BERT]: [**BERT**: Pre-training of Deep Bidirectional Transformers for Language Understanding](../Models/TextLM/2018.10.11_BERT.md). ArXiv:1810.04805v2/NAACL2019.
[^Brown2020GPT-3]: [**GPT-3**: Language Models are Few-Shot Learners](../Models/TextLM/2020.05.28_GPT-3.md). ArXiv:2005.14165v4/NeurIPS2020.
[^Raffel2019T5]: [**T5**: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](../Models/TextLM/2019.10.23_T5.md). ArXiv:1910.10683/JMLR2020.
[^Chowdhery2022PaLM]: [**PaLM**: Scaling Language Modeling with Pathways](../Models/TextLM/2022.04.05_PaLM.md). ArXiv:2204.02311v5.
[^Fukada1992adaptive]: T~Fukada, K~Tokuda, T~Kobayashi, and S~Imai. 1992. An adaptive algorithm for mel-cepstral analysis of speech. In IEEE International Conference on Acoustics, Speech, and Signal Processing, volume~1, pages 137--140.
[^Itakura1975line]: Fumitada Itakura. 1975. Line spectrum representation of linear predictor coefficients of speech signals. The Journal of the Acoustical Society of America, 57(S1):S35--S35.
[^Kawahara1999restructuring]: Hideki Kawahara, Ikuyo Masuda-Katsuse, and Alain De~Cheveigne. 1999. Restructuring speech representations using a pitch-adaptive time--frequency smoothing and an instantaneous-frequency-based {F0} extraction: Possible role of a repetitive structure in sounds. Speech Communication, 27(3-4):187--207.
[^Baevski2019VQ-Wav2Vec]: [**VQ-Wav2Vec**: Self-Supervised Learning of Discrete Speech Representations](../Models/Tokenizers/2019.10.12_VQ-Wav2Vec.md). ArXiv:1910.05453v3/ICLR2020Poster.
[^Baevski2020Wav2Vec2.0]: [**Wav2Vec 2.0**: A Framework for Self-Supervised Learning of Speech Representations](../Models/Tokenizers/2020.06.20_Wav2Vec2.0.md). ArXiv:2006.11477/NeurIPS2020.
[^Radford2022Whisper]: [**Whisper**: Robust Speech Recognition via Large-Scale Weak Supervision](../Models/-ASR/2022.12.06_Whisper.md). ArXiv:2212.04356/ICML2023.
[^Baevski2022data2vec]: Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. 2022. Data2vec: A general framework for self-supervised learning in speech, vision and language. In International Conference on Machine Learning, pages 1298--1312.
[^Barrault2023Seamless]: [**Seamless**: Multilingual Expressive and Streaming Speech Translation](../Models/SpeechLM/2023.12.08_Seamless.md). ArXiv:2312.05187.
[^Defossez2022EnCodec]: [**EnCodec**: High Fidelity Neural Audio Compression](../Models/Codec/2022.10.24_EnCodec.md). ArXiv:2210.13438/TMLR2023.
[^Yang2023HiFi-Codec]: [**HiFi-Codec**: Group-Residual Vector Quantization for High Fidelity Audio Codec](../Models/Tokenizers/2023.05.04_HiFi-Codec.md). ArXiv:2305.02765.
[^Zhang2023SpeechTokenizer]: [**Speechtokenizer**: Unified Speech Tokenizer for Speech Large Language Models](../Models/Tokenizers/2023.08.31_SpeechTokenizer.md). ArXiv:2308.16692/ICLR2024.
[^Kumar2023DAC]: [**DAC**: High-Fidelity Audio Compression with Improved RVQGAN](../Models/Tokenizers/2023.06.11_Descript-Audio-Codec.md). ArXiv:2306.06546v2/NeurIPS2023Spotlight.
[^Defossez2024Moshi]: [**Moshi**: a speech-text foundation model for real-time dialogue](../Models/SpeechLM/Interaction/2024.09.17_Moshi.md). ArXiv:2410.00037v2.
[^Ji2024WavTokenizer]: [**WavTokenizer**: an Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling](../Models/Tokenizers/2024.08.29_WavTokenizer.md). ArXiv:2408.16532v3/ICLR2025.
[^Yoshimura1999simultaneous]: Takayoshi Yoshimura, Keiichi Tokuda, Takashi Masuko, Takao Kobayashi, and Tadashi Kitamura. 1999. Simultaneous modeling of spectrum, pitch and duration in {HMM}-based speech synthesis. In 6th European Conference on Speech Communication and Technology, pages 2347--2350.
[^Zen2013statistical]: Heiga Zen, Andrew Senior, and Mike Schuster. 2013. Statistical parametric speech synthesis using deep neural networks. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 7962--7966.
[^Fan2014tts]: Yuchen Fan, Yao Qian, Feng-Long Xie, and Frank~K Soong. 2014. {TTS} synthesis with bidirectional {LSTM} based recurrent neural networks. In Proceedings of the Annual Conference of the International Speech Communication Association, pages 1964--1968.
[^Jeong2021Diff-TTS]: [**Diff-TTS**: A Denoising Diffusion Model for Text-to-Speech](../Models/Acoustic/2021.04.03_Diff-TTS.md). ArXiv:2104.01409v1/InterSpeech2021.
[^Chen2015joint]: Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun, and Huan-Bo Luan. 2015. Joint learning of character and word embeddings. In Twenty-fourth International Joint Conference on Artificial Intelligence, pages 1236--1242.
[^Almeida2019word]: Felipe Almeida and Geraldo Xex{\'e}o. 2019. Word embeddings: A survey. arXiv preprint arXiv:1901.09069.
[^Heusel2017fid]: Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. {GANs} trained by a two time-scale update rule converge to a local {Nash} equilibrium. Advances in Neural Information Processing Systems, 30.
[^Mittag21_interspeech]: Gabriel Mittag, Babak Naderi, Assmaa Chehadi, and Sebastian Möller. 2021. \href {https://doi.org/10.21437/Interspeech.2021-299} {Nisqa: A deep cnn-self-attention model for multidimensional speech quality prediction with crowdsourced datasets}. In Interspeech 2021, pages 2127--2131.
[^Saeki2022UTMOS]: [**UTMOS**: UTokyo-SaruLab System for VoiceMOS Challenge 2022](../Evaluations/2022.04.05_UTMOS.md). ArXiv:2204.02152/InterSpeech2022.