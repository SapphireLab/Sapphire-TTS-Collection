# WavChat: A Survey of Spoken Dialogue Models

<details>
<summary>基本信息</summary>

- 标题: "WavChat: A Survey of Spoken Dialogue Models"
- 作者:
  - 01 Shengpeng Ji (浙江大学, shengpengji@zju.edu.cn)
  - 02 Yifu Chen (浙江大学)
  - 03 Minghui Fang (浙江大学)
  - 04 Jialong Zuo (浙江大学)
  - 05 Jingyu Lu (浙江大学)
  - 06 Hanting Wang (浙江大学)
  - 07 Ziyue Jiang (浙江大学)
  - 08 Long Zhou (微软)
  - 09 Shujie Liu (微软)
  - 10 Xize Cheng (浙江大学)
  - 11 Xiaoda Yang (浙江大学)
  - 12 Zehan Wang (浙江大学)
  - 13 Qian Yang (浙江大学)
  - 14 Jian Li (腾讯优图实验室)
  - 15 Yidi Jiang (阿里巴巴)
  - 16 Jingzhen He (阿里巴巴)
  - 17 Yunfei Chu (阿里巴巴)
  - 18 Jin Xu (阿里巴巴)
  - 19 Zhou Zhao (浙江大学, zhaozhou@zju.edu.cn)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2411.13577)
  - [Publication]
  - [Github](https://github.com/jishengpeng/WavChat)
  - [Demo]
  - [WeChat](https://mp.weixin.qq.com/s/9DisPbvZBr8NGY2SPytT9Q)
- 文件:
  - [ArXiv]()
  - [Publication] #TODO

</details>

## 摘要


口语对话模型的近期进展, 如 GPT-4o 等系统为代表, 在语音领域获得了广泛关注.
在多模态模型的更广泛背景下, 语音模态为人机交互提供了直接的接口, 使得人工智能和用户之间可以直接交流.
和传统的由自动语音识别 (ASR) + 大语言模型 (LLM) + 文本转语音 (TTS) 组成的三层级联口语对话模型相比, 现代的口语对话模型展现出更高的智能.
这些先进的口语对话模型不仅能够理解音频, 音乐, 以及其他与语音相关的特征, 还能够捕获语音的风格和音色特征.
此外, 它们能够以低延迟生成高质量, 多轮次的语音响应.

尽管口语对话系统取得了不少进步, 但缺乏全面的综述, 来系统地组织和分析这些系统和底层技术.

- 为了解决这一问题, **我们首先将现有的口语对话系统按时间顺序排列, 并将它们分类为级联范式和端到端范式**.
- 然后我们对口语对话模型的核心技术进行了深入的概述, 涵盖了**语音表示**, **训练范式**, **流式/双工/交互能力**等方面.
每一节讨论这些技术的局限性, 并概述了未来研究的考虑因素.
- 此外, 我们从训练和评估口语对话模型的角度, 全面回顾了相关的**数据集**, **评估指标**, 和**基准**.

我们希望这项工作能够促进口语对话系统的学术研究和工业应用的发展.
相关材料可在 [Github](https://github.com/jishengpeng/WavChat) 获得.

## 1.引言

口语对话模型 (**Moshi**[^Moshi], **SpeechGPT**[^SpeechGPT], **Mini-Omni2**[^MiniOmni2]) 代表了人机交互中最直接的方法之一, 从传统的声音助手 ([Alexa [URL]](https://www.alexa.com/); [Siri [URL]](https://www.apple.com/siri/); [Google Assistant [URL]](https://assistant.google.com/)) 进化到最新的智能对话系统 (如 [GPT-4o [URL]](https://openai.com/index/chatgpt-can-now-see-hear-and-speak/)).

口语对话模型的基本定义是指一个能够根据输入语音生成智能口语回应的对话系统.
一方面, **语音模态**在口语对话模型中既是人机交互的输入接口, 也是输出接口.
另一方面, **对话系统** (**LLaMA3.1**[^LLaMA3]) 要求模型具备一定程度的文本智能, 包括理解人类社会知识并生成专业和智能的回应.

近期, 以 GPT-4o 和 **Moshi**[^Moshi] 为代表的智能口语对话系统, 因其超越了传统基于文本的对话模型 (**AudioGPT**[^AudioGPT]) 的语音智能能力而受到广泛关注.

这些对话模型不仅能够生成自然, 类似人类的语音回应 (**Moshi**[^Moshi]; **FunAudioLLM**[^FunAudioLLM]) 还展示了超越文本的高级声学特征 (如音色, 情感和风格) 的理解和生成能力 (**Spoken-LLM**[^SpokenLLM]; **ParalinGPT**[^ParalinGPT]; **E-chat**[^E-chat]).
此外, 它们在处理其他语音相关表示方面表现出色, 包括音乐和音频事件 (**Qwen2-Audio**[^Qwen2Audio]; **Qwen-Audio**[^QwenAudio]; **LTU-AS**[^LTU-AS]; **SALMONN**[^SALMONN]).
它们在现实对话互动 (**VITA**[^VITA]; **MiniOmni2**[^MiniOmni2]) 和低延迟对话体验 (**Moshi**[^Moshi]) 方面的表现进一步使它们在传统对话模型的竞争中脱颖而出.

口语对话模型的历史可以回溯到早期系统, 如 **dGSLM**[^dGSLM], **AudioGPT**[^AudioGPT], 直至最近的进展, 如 GPT-4o 和 **Moshi**[^Moshi].
在此过程中, 许多值得注意的口语对话模型相继出现.
如图 01 所示, 我们按时间顺序组织了这些模型.

![](Images/S20241115_Fig.01.png)

大致上, 它们可以分为两种类型:
- 级联口语对话模型: (**Qwen2-Audio**[^Qwen2Audio], **Qwen-Audio**[^QwenAudio])
- 端到端口语对话模型: ([^FSQ], [^MiniOmni], [^OmniFlatten], [^IntrinsicVoice])

鉴于现有大多数口语对话模型依赖和文本模态的对齐, 级联模型和端到端模型之间的区别十分关键.

如图 02 所示, 我们根据**核心语言模型是否能够直接理解和生成语音表示**将所有口语对话模型划分为级联模型和端到端模型.

![](Images/S20241115_Fig.02.png)

传统的级联口语对话系统, 如 **AudioGPT**[^AudioGPT], 采用以文本为中心媒介的结构, 通常由三个级联模块组成:
- 输入音频通过自动语音识别 ASR 模块 (**Whisper**[^Whisper]) 转写为文本.
- 转写后的文本输入到大语言模型如 ChatGPT 中生成文本回应.
- 最后, 文本回应通过文本转语音模块 ([^VITS2], [^FastSpeech2]) 转换回音频.

尽管这种级联架构利用了大语言模型强大的上下文能力, 但它也引入了一些挑战: 高延迟, 受限互动性, 无法处理非文本信息等.

为了处理这些问题, 近期研究采取了两个主要方向:
- 一些方法 ([^QwenAudio], [^SALMONN]) 专注于优化级联系统中的理解和生成组件以缓解上述局限.
- 另一些方法 ([^MiniOmni], **Mini-Omni2**[^MiniOmni2], [^SpeechGPT-Gen], [^IntrinsicVoice]) 试图采用端到端架构来直接解决这些问题.
  尽管端到端口语对话模型在表示和模型架构方面存在各种差异, 但它们有一个共同特征: 它们不依赖于文本作为中介.
  相反, 这些模型试图直接理解和生成语音表示.
  我们将此类系统定义为端到端口语对话模型.

在构建口语对话系统时, 我们根据涉及的不同级别的智能, 确定了和口语对话模型相关的四种核心技术.
- 第一: 语音表示的设计 (即 Tokenizer 和 Detokenizer)
- 第二: 训练, 推理, 生成范式, 特别是如何在保留或增强现有文本对话模型的智能的同时对齐语音模态和文本模态. 这一部分设计到选择不同的模型架构, 生成策略, 多阶段训练策略.
- 第三: 交互, 双工, 流式的设计.
- 第四: 数据特定相关的挑战, 如何构造口语对话系统的训练数据集和评估其性能.

鉴于这些考虑, 在本文的后续章节中, 我们将按照顺序详细阐述这四种核心技术:
- 第二节: 提供口语对话模型的概述, 包括电影的口语对话场景 (即如何定义一个口语对话模型) 以及级联和端到端口语对话模型的近期进展.
- 第三节: 口语对话系统使用的语音表示.
- 第四节: 系统地讨论训练范式, 特别强调了如何将语音模态与文本模态对齐, 以及多阶段训练策略, 模型架构, 和生成策略.
- 第五节: 强调了口语对话系统的独特特性, 特别是双工, 流式特性, 这些特性使得它们与文本对话系统有所不同.
- 第六节: 考察了口语对话模型的训练数据集构建和特定的评估方法.

在每一节末尾, 我们进行总结和讨论, 以反思关键见解.
最后, 我们在第七节, 总结了本文的主要发现和未来研究的开放问题.

鉴于技术点的复杂性, 我们在图 03 中提供了本文的结构概览.

![](Images/S20241115_Fig.03.png)

## Overall

In this section, we will provide an overall overview of spoken dialogue models.
we begin by defining what constitutes an intelligent spoken dialogue model by examining various dialogue scenarios.
We then provide a comprehensive overview of spoken dialogue models, distinguishing between cascaded spoken dialogue models and end-to-end spoken dialogue models.

### Functions of Spoken Dialogue Systems

Based on the demos and inference interfaces of representative models such as GPT-4o, **Moshi**[^Moshi], Qwen2-Audio[^Qwen2Audio], and **VITA**[^VITA], we categorize the usage scenarios of modern intelligent spoken dialogue models into the following nine representative categories: 1) Text Intelligence, 2) Speech Intelligence, 3) Audio and Music Generation, 4) Audio and Music Understanding, 5) Multilingual Capability, 6) Context Learning, 7) Interaction Capability, 8) Streaming Latency, and 9) Multimodal Capability.
For the nine distinct use cases in spoken dialogue models, we provide corresponding examples for each scenario in Figure \ref{fig:main}.
It is clear from these usage scenarios that a spoken dialogue model is not simply an extension of a text-based dialogue model to the speech modality (i.e., where the speech modality serves merely as an interface for converting speech into text).
Rather, an intelligent spoken dialogue system must be capable of comprehending and generating acoustic information embedded in speech (such as timbre, style, and emotion) and of understanding and producing a wider range of audio representations, including information related to audio events and music.
Additionally, unlike non-streaming text-based systems, spoken dialogue models need to support real-time, interactive streaming capabilities.
These usage scenarios not only highlight the intelligence inherent in spoken dialogue systems but also present significant challenges for building end-to-end spoken dialogue models.
Below, we provide a detailed examination of each of the nine usage scenarios.

#### Text Intelligence

As illustrated in Figure \ref{fig:main} (a), a spoken dialogue system must retain the fundamental capabilities of the original text-based dialogue models, such as ChatGPT.
We define this usage scenario as textual intelligence.
In this context, the spoken dialogue model can intelligently respond to user requests, generating appropriate responses such as travel itineraries, work plans, and scheduling.
However, due to the limitations of voice-based interaction, the textual intelligence of current spoken dialogue systems is more focused on the daily scenarios.
In certain contexts, such as complex mathematical theorem reasoning, the performance requirements for spoken dialogue models differ from those of text-based dialogue models[^024].
These advanced aspects of textual intelligence warrant further exploration in unified multimodal dialogue models.

#### Speech Intelligence

A distinguishing feature of spoken dialogue models, compared to text-based dialogue models[^024], is their ability to understand and generate acoustic information beyond mere textual content.
In the speech modality, not only is the textual content present, but also additional acoustic information, such as timbre (speaker identity) and style (emotion, prosody, etc.).
As illustrated in Figure \ref{fig:main} (b), an intelligent spoken dialogue system should be capable of **understanding** the timbre and style of conversational speech and, ideally, **generating** responses with specified timbre and style in a **zero-shot** manner.

> Figure.03: A general overview about the structure of WavChat

This capability about speech intelligence involves several use cases.
First, on the comprehension side, the spoken dialogue system should generate responses based on the speaker's vocal style.
For example, in the E-chat[^E-chat], a classic example might be: if a user asks, "My phone won't turn on, what should I do?" in a cheerful tone, the system might respond, "It looks like you're excited about getting a new phone.
What type of phone are you interested in?" Conversely, if the user asks the same question in a sad tone, the system might reply, "It's unfortunate your phone isn't working.
If you're familiar with the repair policy, let's proceed with the next steps." This situation indicates that the spoken dialogue system may generate responses with different **content** based on varying acoustic information.
Furthermore, the system should comprehend various acoustic cues, such as accents or emotional states, and adjust its responses of different **acoustic** information accordingly.
For instance, if the speaker is an American, the system might reply with a native English accent, whereas if the speaker is a Shanghainese user, the system could respond using the corresponding dialect.
Similarly, if the user speaks with a sad tone, the dialogue system should be able to generate a more encouraging and empathetic response.

On the generation side, speech intelligence is more prominently reflected in its controllability, such as voice cloning and style control.
For example, the system could be instructed to mimic a specific voice or respond in a designated style (e.g., mimicking a grandmother's soft and gentle voice for a comforting interaction).
Additionally, the system could use a voice prompt provided during the conversation to fully clone the timbre from the prompt and generate speech in that same voice.
In summary, the ability to comprehend and generate acoustic information is one of the key characteristics of an intelligent spoken dialogue model.

#### Audio and Music Generation

In the spoken dialogue models, beyond basic spoken dialogue capabilities, an intelligent spoken dialogue system may be required to generate music and audio.
For example, a user might instruct the system to generate a one-minute piano piece or a ten-second recording of a dog barking.
Additionally, users might provide lyrics and a musical melody, asking the spoken dialogue model to create a pop song.
The system should thus inherit the generative capabilities of large-scale music[^025], [^026], [^027], [^028] and audio[^029], [^030], [^031] models on the output side.

#### Audio and Music Understanding

Complementing its music and audio generation capabilities, a spoken dialogue model should also be able to understand music and audio on the input side[^Qwen2Audio], [^SALMONN].
For instance, when given an audio clip, the intelligent system should identify both its content and acoustic characteristics, such as recognizing whether the sound is a bird chirping or a cat meowing, or whether the music is calm or energetic.
Moreover, the system could extend its understanding by creating literary works—like poetry or songs—based on the given music or audio.

> Figure.04: An overall demonstration of the functions of the spoken dialogue systems.
We describe the ideal capabilities of such systems from nine different perspectives: Text Intelligence, Speech Intelligence, Audio and Music Generation, Audio and Music Understanding, Multilingual Capability, Context Learning, Interaction Capability, Streaming Latency, and Multimodal Capability.
Each function is illustrated with corresponding dialogue examples.

#### Multilingual Capability

Similar to text-based dialogue models, spoken dialogue systems are expected to possess multilingual capabilities.
Specifically, these models should be able to perform multilingual content translation, such as translating a spoken segment in Japanese into French speech clips, effectively inheriting the capabilities of simultaneous interpretation.
In addition to multilingual content translation, the system should also handle multilingual acoustic information.
This means that the intelligent spoken dialogue model should be able to generate responses in various languages and accents, replying in the corresponding accent of the target language based on the different input speech.

#### Context Learning

In the spoken dialogue models, the ability to handle long-form and multi-turn conversations is a key benchmark for evaluating performance (**Moshi**[^Moshi]).
This requires that spoken dialogue models not only support long-duration audio inputs but also generate extended audio outputs.
Moreover, they must be capable of engaging in multi-turn conversations based on historical context.
An important aspect of multi-turn dialogue is the ability to revise previous responses based on new user instructions.
As shown in Figure \ref{fig:main} (f), an intelligent spoken dialogue model should be able to continuously modify its previous replies according to the user’s evolving requests.

#### Interaction Capability

A distinguishing feature of spoken dialogue systems compared to the text-based dialogue models is their duplex and interactive nature (**Moshi**[^Moshi]).
In text-based dialogue, interactions typically follow a half-duplex structure, where the response can only be provided after the question has been completed, and the user is unable to interrupt the reply in real-time.
However, in the spoken dialogue systems, full-duplex interaction is common.
This means that a conversation does not need to be fully completed before a response can be generated.
Both the system and the user can interrupt and interact in real time.
For example, if the user is unsatisfied with the system's response, they can immediately interrupt, causing the system to halt its current generation and respond to the new input.
Additionally, to emulate more natural conversational settings, the system can also interrupt the user when appropriate, such as when clarifying the user’s intent.
Beyond the ability to interrupt, interactive dialogue often includes the use of conversational fillers, such as "okay," "haha," or "oh," which signal acknowledgment or agreement.
Including these within spoken dialogue models enhances the realism and natural flow of conversations.
The underlying requirement for interaction capabilities is that the system should be able to listen and speak simultaneously, responding dynamically to the flow of the interaction.

#### Streaming Latency

Streaming comprehension and generation are also fundamental functionalities of spoken dialogue models (**Mini-Omni2**[^MiniOmni2], [^IntrinsicVoice], [^032].
In the real-world scenarios, a model cannot wait until an entire minute-long audio segment has been processed before generating a response.
Instead, the model must operate on a chunk-based mechanism, dynamically processing and generating audio in real time, one chunk at a time.
Additionally, the streaming requirement means that the entire system must operate in a causal manner—understanding and generating audio based solely on past information, without relying on future information.
Streaming function is often closely tied to the need for low latency.
In practical conversational experiences, the latency of the first token generated by the spoken dialogue model (i.e., the wait time for the user) and the average latency of the generation process are critical factors that influence the overall responsiveness and usability of the spoken dialogue system.

#### Multimodal Capability

Multimodal dialogue capability[^033], **VITA**[^VITA] represents an advanced feature of spoken dialogue models.
In existing systems, this typically refers to the ability to process inputs from multiple modalities, such as video, images, and text, while generating intelligent speech responses.
A spoken dialogue model equipped with this capability achieves the ability to “hear, see, and speak” simultaneously.
Multimodal inputs significantly enhance the potential of these systems; for instance, users can employ various gestures to improve the quality of the model’s generated responses, and the system can develop a deeper understanding of the physical world.
Beyond multimodal inputs, the future of dialogue systems lies in large multimodal models that unify the comprehension and generation capabilities across all modalities, with spoken dialogue serving as the foundational modality.

### Cascaded Spoken Dialogue Systems

The earliest prototype of cascaded spoken dialogue systems can be traced back to AudioGPT[^AudioGPT].
To achieve speech-to-speech dialogue functionality, the system first employed an Automatic Speech Recognition (ASR) model to convert speech into text, followed by ChatGPT for text-based dialogue, and finally, a Text-to-Speech (TTS) model to convert the generated text back into speech.
In this primitive version, speech was used solely as an input-output interface, retaining only the most basic textual intelligence.
For example, in the Huggingface’s open-source Speech-To-Speech framework\footnote{[URL](https://github.com/huggingface/speech-to-speech)}, an additional Voice Activity Detection (VAD) module\footnote{[URL](https://github.com/snakers/silero-vad)} was further layered onto the traditional cascaded modules to distinguish between speech and silent segments, as well as between different speakers.

After the basic textual intelligence had been established in the cascaded spoken dialogue models, researchers began incorporating paralinguistic features, such as emotion and style, to enhance the speech intelligence in the cascaded spoken dialogue models.
For instance, ParalinGPT[^ParalinGPT] and E-chat[^E-chat] integrate conversational context, speech embeddings, and paralinguistic attributes into an autoregressive model via a sliding ndow, allowing the model to generate more accurate text responses by combining historical text and emotional representations.
Similarly, Spoken-LLM[^SpokenLLM] introduces an Emotion2Vec[^034] module to provide style vectors to the Llama2-Chat model.
Through LoRA[^035] fine-tuning, Llama2-Chat is trained not only to generate content-based text responses but also to produce text responses with specific stylistic attributes (e.g., <cheerful, fast, normal>), which can guide downstream TTS systems in generating expressive speech.

In addition to understanding acoustic information within cascaded spoken dialogue models, there have been efforts to directly input speech representations while retaining text as the output modality[^036], [^QwenAudio], [^037].
This forces cascaded spoken dialogue systems to process input speech directly.
A common approach involves integrating frozen speech encoders (such as Whisper[^Whisper]) with trainable encoder adapters, allowing the speech input to be interpreted as a specialized form of text by the large language model.
By extending the vocabulary of the text-based dialogue model, the large language model can process speech as if it were a unique form of text, enabling the generation of appropriate text responses in the cascaded spoken dialogue models.

Notably, these cascaded spoken dialogue models have further advanced beyond the comprehension of human speech alone and can now understand a variety of audio modalities, including music and audio[^LTU-AS], [^SALMONN].
For example, SALMONN[^SALMONN] models both speech and audio information by freezing the Whisper[^Whisper] and BEATs[^038] encoder and bridging them to a large language model via a Window-Level Q-Former[^039].
As a result, these cascaded spoken dialogue systems are capable of further performing a wide range of tasks on the comprehension side.
For instance, models like Qwen-audio[^Qwen2Audio], [^QwenAudio] can handle multiple tasks such as Automatic Speech Recognition (ASR), Speech-to-Text Translation (S2TT), Automatic Audio Captioning (AAC), Acoustic Scene Classification (ASC), Speech Emotion Recognition (SER), Audio Question Answering (AQA), Vocal Sound Classification (VSC), and Music Note Analysis (MNA).
Consequently, these cascaded models are often regarded as part of multitask speech-text large language models.

It is worth noting that the aforementioned cascaded spoken dialogue models generate text only and then directly feed it into a pre-trained TTS module.
However, more recent cascaded spoken dialogue models, such as Llama3.1, have begun integrating trainable TTS modules as part of the decoder within the large language model (LLM).
While these models have made progress in incorporating low-latency streaming functionalities, they are still fundamentally based on generating text content first, which is then converted into speech.
They do not directly generate speech-related representations within the LLM itself.
Therefore, we classify these models as cascaded spoken dialogue systems.

In addition, some recent efforts have focused on enhancing models like Qwen2-Audio[^Qwen2Audio] by incorporating multimodal comprehension capabilities, thereby enabling a degree of multimodal dialogue functionality.
For instance, models such as **VITA**[^VITA] and Baichuan-Omni[^040] integrate various encoders or tokenizers for images, audio, and video into the LLM, allowing the model to understand multimodal inputs and generate corresponding text responses.

The above developments concern the comprehension side of cascaded spoken dialogue systems.
On the generation side, two main types of speech synthesis work are relevant to cascaded spoken dialogue systems.
Firstly, there has been a recent surge of advanced speech synthesis systems that can produce highly expressive and natural audio based on textual input, such as VALL-E (X)[^041], [^042], MegaTTS1/2[^043], [^044], CosyVoice[^045], ChatTTS\footnote{[URL](https://github.com/2noise/ChatTTS)}, FishSpeech\footnote{[URL](https://github.com/fishaudio/fish-speech)}, ParlerTTS[^046], MaskGCT[^047] and F5-TTS[^048].
In addition, there has been significant progress in the field of text-style controllable TTS, with systems like TextrolSpeech[^049], PromptTTS[^050], PromptTTS2[^051], InstructTTS[^052], and ControlSpeech[^053].
These TTS systems can generate highly natural audio based both on the content and style of the text output produced by the cascaded spoken dialogue models.

### End-to-End Spoken Dialogue Systems

Ideally, end-to-end spoken dialogue models should enable **only** speech input and output during both training and inference, thereby achieving multiple intelligent dialogue functions.
However, considering that speech modal is a low-density (contains a lot of acoustic information) modality compared to text modal, and that the volume of available text data far exceeds that of available speech data, many end-to-end spoken dialogue models choose to align the speech modality with the text modality to leverage pre-trained language models (LLMs).
Consequently, as showed in the [Figure.02](#Fig.02), as long as the large language models can directly understand and generate speech representations, we classify such systems as end-to-end spoken dialogue models.
In contrast, if the large language models can only generate text, we categorize the system as cascaded spoken dialogue systems.

The earliest end-to-end spoken dialogue system can be traced back to dGSLM[^dGSLM], which was trained on thousands of hours of dual-track data[^054] using self-attention and cross-attention mechanisms to simulate duplex interactions.
Although dGSLM lacks integration with LLMs and even basic textual intelligence, it is notable as the first fully end-to-end spoken dialogue system that does not rely on text while maintaining excellent conversational interactivity.

Following the release of dGSLM[^dGSLM], the progress in the domain of end-to-end spoken dialogue systems stagnated for a few months.
However, with the advent of ChatGPT, this field experienced rapid development.
A representative approach is **SpeechGPT**[^SpeechGPT], which employs autoregressive language modeling by using a sequence of speech tokens, text tokens, text tokens, and speech tokens.
This method enables the direct generation of speech tokens using textual intelligence, inspiring subsequent end-to-end spoken dialogue systems such as Spectron[^055], SpeechGPT-Gen[^SpeechGPT-Gen], GLM-4-Voice\footnote{[URL](https://github.com/THUDM/GLM-4-Voice)}, and EMOVA[^033].
These systems continue to use an autoregressive framework, generating the text tokens followed by the speech tokens.
Although this approach allows LLMs to generate speech tokens directly, it introduces latency issues since speech token generation cannot begin until the generation of text tokens is complete.
This leads to problems in multi-turn dialogue and overall system delay.

Beyond the design of **SpeechGPT**[^SpeechGPT], another intuitive approach is to directly use the hidden states before the LLM’s softmax layer to predict both text tokens and speech tokens through different projection layers.
This allows the network to share weights up to the projection layer, thereby aligning the speech and text modalities.
The PSLM[^056] model is a typical example of this design.
Another method, proposed by Meta, is the interleaving approach, as seen in Spirit-LM[^057], where speech and text sequences are concatenated into a single token stream and trained using a word-level interleaving method with a small, automatically curated speech-text parallel corpus.
However, this approach requires precise alignment between speech and text.

Recently, several new end-to-end spoken dialogue systems have emerged.
For instance, **Moshi**[^Moshi], which is based on a global-local transformer, can simultaneously generate text and speech acoustic tokens from a multi-layer quantizer.
Starting from a text-based language model backbone, Moshi generates speech tokens from the residual quantizer of a neural audio codec while modeling both the user’s speech and the system’s responses in parallel streams.
This design eliminates the need for explicit speaker turns and allows for the modeling of arbitrary conversational dynamics.
Moreover, Moshi extends previous hierarchical semantic-to-acoustic token generation by first predicting time-aligned text tokens as a prefix to audio tokens.
Similarly, Mini-Omni[^MiniOmni] uses a MusicGen-based[^026] method to simultaneously generate text and speech codec tokens.
It introduces two strategies: autoregressive generation without strict temporal alignment by padding text tokens and batch-parallel inference strategies to boost performance.
**Mini-Omni2**[^MiniOmni2] further enhances this by incorporating multimodal understanding and duplex functionality.
At the same time, Llama-Omni[^032], Freeze-Omni[^058] and IntrinsicVoice[^IntrinsicVoice] design an LLM for real-time voice interaction.
Their commonality lies in the fact that, at the generation stage, the hidden states of the LLM are further fed into the corresponding decoder model.
LLaMA-Omni[^032] integrates a pretrained speech encoder, a speech adapter, an LLM, and a streaming speech decoder.
It eliminates the need for speech transcription, and can simultaneously generate text and speech responses directly from speech instructions with low latency.
Freeze-Omni [^058] designed 3-stage training strategies both for the modeling of speech input and output, enabling it to obtain speech-to-speech
dialogue ability noly by using text-speech paired data.
The core idea of Freeze-Omni lies in transferring the functionalities of spoken dialogue models to the encoder (ASR) and decoder (TTS), rather than assigning these tasks to the large language model.
IntrinsicVoice[^IntrinsicVoice] facilitates the transfer of textual capabilities from pre-trained LLMs to the speech modality by reducing the modality gap between text and speech.
By using a GroupFormer to generate HuBERT tokens from the LLM’s hidden states, IntrinsicVoice effectively reduces speech sequences to lengths comparable to text sequences, generating high-quality audio while significantly speeding up inference and mitigating long-text modeling issues.
Additionally, some end-to-end spoken dialogue models align speech and text through multi-stage training, eliminating the need to generate text during inference.
For example, Omni-Flatten[^OmniFlatten] employs modality alignment, half-duplex dialogue learning, and full-duplex dialogue learning, along with a flattening-style standardization of text and speech tokens, to achieve duplex, text-free speech dialogue during inference.
Similar approaches include SyncLLM[^059].

In this section, we have provided a general overview of current end-to-end spoken dialogue systems.
However, these systems differ significantly in their speech representations, training paradigm, model architectures and generation strategy.
In Section \ref{section3} and \ref{section4}, we will present a detailed classificationfollowed by our discussions at the end of each section.

## Representations of Spoken Dialogue Models

Representations play a critical role in spoken dialogue systems as they determine how the spoken dialogue system comprehends, processes, and generates speech signals.
Additionally, they serve as a bridge between speech and other modalities, thereby directly influencing the system’s performance, functionality, and range of applications.
Compared to text and visual representations, speech representations possess a unique complexity.
Text representations primarily rely on a well-defined symbolic system, conveying meaning through structured elements like vocabulary and syntax.
Visual representations, on the other hand, focus on capturing spatial relationships and visual features in images.
In contrast, speech signals contain both dynamic acoustic features (such as timbre, prosody and emotion) and rich semantic content, requiring representations that not only capture temporal variations but also preserve an understanding of the underlying meaning.

The unique nature of speech has led to the development of two types of representation models.
The representations obtained by these two modeling approaches are often classified as semantic tokens and acoustic tokens.
**One category (semantic) is prediction-based modeling**, these models are trained for representation learning by predicting future frames in an autoregressive manner [^060], [^061] or by using surrounding frames to predict masked frames [^062], [^063], [^064].
This approach tends to prioritize capturing linguistic information within speech, making it particularly useful for recognition and understanding tasks.
**The other category (acoustic) focuses on speech compression and reconstruction** [^065], [^066], [^067], [^068].
These models quantify speech features (which are downsampled from raw wavforms by one encoder) into a series of discrete tokens, then use one decoder to upsample these discrete tokens into the speech, calculating the reconstruction loss against the original signal.
By this approach, we can get discrete acoustic tokens with impressive compression rates and high-fidelity acoustic information, making it more suitable for tasks such as speech synthesis and emotion analysis.

In the spoken dialogue systems, as illustrated in [Figure.02](#Fig.02), different spoken dialogue models employ various approaches for representation selection.
In the following part, we will enumerate the commonly used speech representations in spoken dialogue models from both the input and output perspectives.
At the end of this section, we will thoroughly discuss the advantages and limitations of these representations, as well as the future trends in the development of representations used in spoken dialogue models.

### Speech Representations at the Inputs

**Semantic.** To enhance language models' ability to understand speech representations and align multimodal data at input, using pretrained models such as Wav2Vec [^069], HuBERT [^063], Whisper [^Whisper], and WavLM [^070] to extract high-level semantic features from speech has become a core strategy for many spoken dialogue systems.

- **Wav2Vec.** Wav2Vec [^069] is a foundational work in the field of speech representation learning, pioneering the extraction of self-supervised speech representations from unlabeled speech data.
This approach has driven technological advancements in tasks such as speech recognition, speaker identification, and other speech processing applications.
Wav2Vec employs a multi-layer, one-dimensional convolutional neural network directly on raw speech waveforms to progressively extract temporal speech features.
Training is accomplished through contrastive learning: the model selects a "correct" target (from the current speech frame) alongside several "incorrect" targets (negative samples).
By learning to distinguish positive samples from negatives, the model effectively learns to represent speech features in latent space.
As an improved version of Wav2Vec, Wav2Vec 2.0 [^071] introduces the Transformer architecture and masked modeling.
Wav2Vec 2.0 quantizes the latent speech representations extracted by the CNN and then uses a Transformer to model semantic information, similar to BERT [^072].
It also employs a contrastive learning objective, requiring the model to distinguish the correct quantized representations from multiple candidate representations.
ParalinGPT [^ParalinGPT] aims to incorporate emotional expression in conversational interactions, choosing Wav2Vec 2.0 for its proven capability to encode rich prosodic information, beneficial for speech emotion recognition [^073].
Specifically, ParalinGPT uses Wav2Vec 2.0’s intermediate layer (the 12th layer) for frame-by-frame feature extraction, as this layer has shown optimal results in linear probing tasks for emotion analysis.
Additionally, ParalinGPT applies mean pooling and a linear feature projector to extract utterance embeddings.

- **XLS-R.** XLS-R [^074] is a multilingual self-supervised speech representation model based on the Wav2Vec 2.0 architecture.
It extends and optimizes Wav2Vec 2.0 to support a broader range of languages, particularly low-resource languages.
During cross-lingual training, XLS-R employs multilingual data augmentation and denoising techniques, enhancing the model's adaptability when processing speech in various languages.
USDM [^075] uses XLS-R to obtain continuous intermediate representations at 50Hz, followed by a quantizer [^076] with $K$=10000 to generate speech tokens.

- **HuBERT.** HuBERT [^063] is a commonly used unsupervised learning model that performs K-Means clustering on the MFCC [^077] features of speech to assign pseudo-labels to each frame.
It uses a convolutional encoder to generate a sequence of features at a 20ms frame rate from 16kHz sampled speech.
Finally, it randomly masks a portion of features from consecutive frames as input to the Transformer [^078].
HuBERT generates masked content based on surrounding context, enabling it to capture temporal and semantic information within speech and gain a deeper understanding of contextual details.
Spoken dialogue systems, such as E-Chat [^E-chat], **SpeechGPT**[^SpeechGPT], PSLM [^056], IntrinsicVoice [^IntrinsicVoice], widely use HuBERT as their speech encoder.
E-Chat extracts the weighted sum of the 24 layers from the HuBERT to serve as speech embeddings, and incorporates an additional set of weighted parameters to extract emotion embeddings, thereby enabling emotion-aware capabilities.
SpeechGPT applies K-Means clustering to quantize the continuous features extracted from HuBERT, converting them into discrete unit sequences.
These discrete units are then integrated into the vocabulary of the large language model, enabling direct alignment between the text and speech modalities.
To more effectively integrate the language model with speech streams, PSLM adds an additional embedding layer after extracting features with HuBERT.
IntrinsicVoice uses HuBERT as the speech tokenizer, grouping speech tokens to reduce sequence length.
An embedding layer then converts these tokens into dense embeddings, which are subsequently mapped into the language model's embedding space using a trainable speech adapter.
Spirit-LM [^057] extracts semantic features using HuBERT, employing a K-Means model with 500 units as the basic unit.
It trains a feedforward quantizer with data augmentation techniques [^079] to produce discrete speech tokens.
In the Align-SLM [^080], HuBERT is used and the cluster number K is set to 500.
Notably, when continuous representations are clustered into discrete units, they primarily capture content information, which can be leveraged for modeling and understanding.
This process first extracts 25Hz frame-level continuous representations from the 11-th layer of the HuBERT model, assigns each frame to its closest cluster index, and then de-duplicates consecutive identical indices to shorten the sequence.

- **Whisper.** Whisper [^Whisper], based on the classic encoder-decoder architecture, has gained widespread attention in the field of speech recognition.
The encoder transforms input speech into high-level feature representations, while the decoder generates the corresponding text output from these representations.
Pretrained on large-scale data across various speech environments with text as the target, Whisper demonstrates strong capabilities in extracting semantic information from speech.
Qwen-Audio [^QwenAudio], Qwen-Audio 2 [^Qwen2Audio] use Whisper’s encoder to convert speech into continuous representations, which are then combined with text representations and fed into the large language model.
Mini-Omni [^MiniOmni], **Mini-Omni2**[^MiniOmni2], and LLama-Omni [^032] follow a similar approach, connecting a speech adapter after the Whisper encoder.
Their shared objective is to map speech representations into the text embedding space of the large language model, enhancing the model's ability to understand speech by forcibly aligning them through vocabulary expansion.

- **WavLM.** WavLM [^070] is a pretrained model designed for comprehensive speech processing tasks, playing a critical role in advancing speech technology.
Specifically, WavLM employs a masked speech denoising and prediction framework, where some inputs consist of simulated noise or overlapping speech with masked sections.
The goal is to predict pseudo-labels of the original speech in the masked areas.
This approach enables the model to learn ASR-related information through masked speech prediction, while also gaining knowledge relevant to non-ASR tasks through speech denoising modeling.
The masking and prediction pipeline for speech frames in WavLM is similar to that of HuBERT.
However, WavLM introduces an additional gated relative position bias to enhance the model's sensitivity to temporal information in speech.
SpeechVerse [^036] leverages the pretrained WavLM Large as its backbone speech encoder, encoding all intermediate layer features from WavLM to capture various forms of semantics and achieve better generalization performance.
To address the significant length disparity between speech features and text tokens, SpeechVerse applies a learnable convolutional module for downsampling the speech features.

- **$S^3$ Tokenizer.** CosyVoice [^045] proposes using a supervised automatic speech recognition module to generate a supervised semantic speech($S^3$) tokenizer.
Unlike a standard ASR model, the $S^3$ tokenizer splits the encoder into two parts and introduces a vector quantization layer in between.
The first encoder converts the mel spectrogram into context-aware representations, while the second encoder transforms discrete speech units into continuous hidden states.
Finally, a Transformer-based ASR decoder predicts the posterior probabilities of text labels.
Through supervision in multilingual ASR tasks, the $S^3$ tokenizer can convert speech into semantically consistent tokens that facilitate both speech understanding and generation.
OmniFlatten [^OmniFlatten] uses the $S^3$ tokenizer to extract discrete speech tokens, which are then directly fed into a text-speech pre-trained Transformer.

- **SPIRAL.** SPIRAL [^081] aims to learn representations from speech data that are robust to noise and perturbations.
It uses a teacher-student network, where various perturbations—such as noise addition, gain adjustment, and time-frequency warping—are applied to the speech input of the student model.
The teacher model then guides the student model to produce consistent representations despite these perturbations.
EMOVA [^033] utilizes the SPIRAL’s architecture as a speech encoder to process speech, and employs the finite scalar quantization [^FSQ] to discretize these features.
This process aligns speech with the text vocabulary, allowing for a more natural integration into the LLM.

- **Others.** Some spoken dialogue systems do not use pre-trained representation models; instead, they process input features by stacking fundamental modules.
**VITA**[^VITA] initially decomposes the speech signal using mel filter banks, mimicking the nonlinear perception of sound in humans.
It then processes the input features with a 4-layer CNN downsampling module followed by a 24-layer Transformer.
To align with the subsequent language model, **VITA** employs a simple 2-layer MLP as an adapter.
Freeze-Omni [^082] utilizes a chunk-wise streaming speech encoder to transform input speech features into high-dimensional representations.
An adapter module then maps these high-dimensional representations into the embedding space of the main LLM, ensuring a quick, low-latency response to the input speech.
The speech encoder module consists of several downsampling convolutional layers and Transformer blocks, while the adapter includes only a few downsampling convolutional layers.
Downsampling layers are used to reduce the frame rate of speech features, increase the LLM's processing speed during the prefill phase, and minimize latency.

**Acoustic.** Considering that semantic features are insufficient to capture the emotion, timbre, and style of speech, some representation models, such as Emotion2Vec [^034], attempt to extract acoustic information through self-supervised training.
Others focus on reconstruction objectives to ensure high-fidelity speech, including models like Encodec [^066], SpeechTokenizer[^083], **Mimi**[^Moshi].

- **Encodec.** EnCodec [^066] is a straightforward, streaming, convolution-based encoder-decoder architecture.
Raw speech is downsampled through a series of convolutional layers, mapping it to latent feature representations.
Residual vector quantization [^068] then discretizes the encoder’s continuous latent features.
The quantization objective is to map continuous features to a predefined set of discrete tokens (known as a "codebook") for subsequent compression and transmission.
The decoder restores the discrete features to a waveform close to the original speech through a series of de-convolution layers.
LauraGPT [^084] employs an enhanced version of EnCodec as its speech encoder with specific modifications: (1) adding a reconstruction loss in the magnitude spectral domain to improve mid-to-high frequency signal quality; (2) stacking five strided convolutional blocks with strides of (8, 5, 4, 2, 2) to address the challenges of long sequence lengths, resulting in a token rate of 25Hz per token group; and (3) using 32 quantizers with structured dropout in the Residual Vector Quantization (RVQ) module, each with a vocabulary size of 1024.
This revision increases speech quality by incorporating more quantizers while preserving most information in the shallow quantizers.
LauraGPT ultimately selects the output from the first quantizer layer as the speech token, balancing performance with sequence length efficiency.
The remaining quantizers are used only during the training of the encoder-decoder model.

- **SpeechTokenizer.**
SpeechTokenizer [^083] unifies semantic and acoustic tokens, hierarchically decomposing different aspects of speech information across various RVQ layers.
It is built on the framework of RVQ-GANs, following the same pattern as SoundStream [^068] and EnCodec [^066].
Notably, SpeechTokenizer has substituted the two-layer LSTM, originally following the convolution blocks in the EnCodec encoder, with a two-layer BiLSTM to augment the semantic modeling ability.
SpeechTokenizer uses HuBERT as a semantic teacher, given HuBERT’s proven capacity to encode substantial content information [^085].
During training, it introduces two types of distillation: continuous representation distillation and pseudo-label prediction.
For continuous representation distillation, SpeechTokenizer employs the 9th layer HuBERT representation or the average representation across all HuBERT layers as semantic teachers.
The training objective is to maximize the cosine similarity at the dimension level across all timesteps between the outputs of RVQ first layer and semantic teacher representations.
For pseudo-label prediction, SpeechTokenizer adopts HuBERT units as the target label.
In dialogue systems, SpeechGPT-Gen uses SpeechTokenizer RVQ-1 to process raw speech, primarily enhancing the large language model's ability to model the semantics of speech.

- **Mimi.** Taking inspiration from previous work on SpeechTokenizer, **Mimi**[^Moshi] uses distillation to transfer non-causal, high-level semantic information into the tokens produced by a causal model, allowing for streaming encoding and decoding of semantic-acoustic tokens.
To improve the ability of Mimi to encode speech into compact representations while reconstructing high-quality speech, Transformer modules are added in the encoder and decoder.
Mimi uses WavLM to distill RVQ-1, enriching it with semantic information.
Notably, performing distillation significantly enhances the speech discrimination capability of the first quantizer; however, it can also negatively impact speech quality.
Mimi hypothesizes that this is due to distilling semantic information into the first level of a single RVQ: As higher-order quantizers operate on the residual of the first one, the latter needs to trade speech quality for phonetic discriminability.
Mimi addresses this issue by introducing a split-RVQ approach.
Instead of using a single 8-level RVQ, it extracts semantic information into a simple VQ and applies a parallel 7-level RVQ, combining their outputs at the end.
This removes the constraint that acoustic information must be preserved in the residuals of the semantic quantizer.
After careful design, Mimi serves as the speech encoder in **Moshi**[^Moshi], this approach enhances the model's ability to capture both semantic and acoustic details.

- **Emotion2Vec.** Emotion2Vec [^034] is a versatile speech emotion representation model designed to extract emotional features from speech.
During the pre-training phase, Emotion2Vec conducts online distillation with a teacher network and a student network.
When a specific downstream task is performed, Emotion2Vec is frozen and a lightweight downstream model is trained.
Emotion2Vec introduces an utterance-level loss to control global emotion and employs a frame-level loss to build a frame-wise pretext task, enabling it to learn contextual emotions.
Spoken-LLM[^SpokenLLM] uses features extracted by Emotion2Vec as input for the large language model, aiming to enable the model to understand and respond to emotions.

### Speech Representations at the Outputs

**Semantic.** At the output stage, Most spoken dialogue systems choose to autoregressively model semantic tokens, such as $S^3$ tokens [^045] and HuBERT [^063] units.
It is worth noting that these semantic tokens lack acoustic conditioning and therefore require a vocoder [^086], [^087] or decoder, which futher takes semantic discrete units as input to synthesize speech consistent with the speakers encountered during training.

- **$S^3$ Tokenizer.** OmniFlatten [^OmniFlatten] uses the LLM to autoregressively predict $S^3$ tokens at the speech output stage.
When converting discrete tokens back into speech, it adopts the same optimal transport conditional flow matching model (OT-CFM) as used in CosyVoice [^045].
OT-CFM transforms the speech token sequence into Mel spectrogram, which is then used to generate the final speech with the HiFi-GAN vocoder [^086].

- **Hubert.** Speech tokens extracted by the pre-trained HuBERT [^063] are widely used as generation targets for large language models in the spoken dialogue systems.
**SpeechGPT**[^SpeechGPT] and Spirit-LM [^057] use LLaMA [^024] to autoregressively predict a sequence of units and are trained with a HuBERT unit-based HiFi-GAN [^086] to decode the speech signal from discrete representations.
PSLM [^056] introduces an additional speech projection layer after the Transformer layers to process the hidden states, obtaining semantic tokens via the softmax layler.
The speech decoder in LLama-Omni [^032] operates in a non-autoregressive manner, taking the output hidden states of the large language model as input to generate a discrete HuBERT unit sequence corresponding to the speech response.
The discrete units can be converted into waveform with an additional unit-based vocoder [^087].
IntrinsicVoice [^IntrinsicVoice] introduces Group-Former to enhance the large language model’s capability in sequence modeling.
When the large language model predicts the $<speech>$ token, the global embedding is passed through a projection layer and delivered, along with a set of learnable queries, to the group model, which then predicts units.
IntrinsicVoice uses HiFi-GAN [^086], a non-autoregressive neural vocoder that efficiently generates high-fidelity waveforms, for speech detokenization to reduce overall latency.
Align-SLM [^080] also uses a HiFiGAN-based [^086] model to convert discrete units back into waveforms, utilizing model checkpoints from the textlesslib [^088] library.

- **Others.** USDM [^075] does not generate speech directly from input speech; instead, it first transcribes the speech, generates the response text, and then produces corresponding speech token in an end-to-end pipeline.
By inserting text-related tasks between speech input and output, the model benefits from both pre-trained LLMs and chain-of-thought [^089] reasoning in the intermediate modality.
Since each stage in the pipeline processes all input and output tokens generated by the previous stage.
USDM is more robust to transcription errors and better able to produce contextually relevant spoken responses compared to a cascaded approach with separate modules.
USDM uses the Voicebox [^090] architecture to train a unit-to-speech model for reconstructing speech from units.
EMOVA [^033] generates a response in the form of speech units when given an image or speech input, which is then converted into an output waveform using the U2S detokenizer.
The U2S detokenizer follows the VAE architecture: it uses a speech unit encoder to convert the predicted speech units into continuous embeddings, combines these with style embeddings predicted by the large language model to determine duration, and finally reconstructs the speech waveform through the decoder.

**Acoustic.** Many spoken dialogue systems choose to directly generate tokens from acoustic representation models, such as EnCodec [^066], SpeechTokenizer [^083], and **Mimi**[^Moshi].
These acoustic tokens are then upsampled into the raw waveform through the frozen codec decoder directly.

- **Encodec.** LauraGPT [^084] uses Qwen-1.8B [^091] to predict speech tokens.
When synthesizing speech, it conditions the predictor not only on the speech tokens predicted by the LLM but also on text and speech inputs.
Such text and speech conditionings allow the model to generate high-quality speech signals by leveraging the diverse information in prompt and noisy speeches, which is lacked in the discrete tokens (output from the first quantizer of the Encodec).
The predicted speech tokens and conditioning inputs are delivered together to the codec vocoder.
An encoder-only Transformer models these inputs into dense embeddings, which are then reconstructed into speech by the codec decoder.

- **SNAC.** SNAC [^092] encodes speech into hierarchical tokens, similar to EnCodec [^066] and DAC [^067], by introducing quantization at different time resolutions to form a multi-scale discrete representation of speech.
In this approach, shallow RVQ layers have a lower sampling frequency, covering a broader time span, while deeper RVQ layers sample at higher frequencies.
SNAC introduces modest enhancements over RVQ-GAN by incorporating residual noise blocks, deep convolutions, and local window attention.
The Mini-Omni [^MiniOmni], **Mini-Omni2**[^MiniOmni2] series continues the parallel generation method introduced by MusicGen[^026], utilizing SNAC [^092] as the speech encoder, which comprises seven complementary token layers.
In a single step, it generates eight tokens, including text, while maintaining a one-step delay between layers.
Furthermore, Mini-Omni and Mini-Omni 2 incorporates a batch approach that involves two samples: one requiring both text and speech responses and the other necessitating a text-only response.
By discarding the text token from the first sample and embedding the output from the second sample into the first, it effectively transfer the model’s text-based capabilities to speech tasks, significantly enhancing reasoning abilities with minimal resource overhead.

- **SpeechTokenizer.** On the output side, SpeechGPT-Gen synthesizes speech tokens using flow matching[^093].
Flow matching effectively models the transformation from a simple prior distribution to complex data distributions, yielding promising results in speech generation.
SpeechGPT-Gen [^SpeechGPT-Gen] applies flow matching for perceptual modeling, generating speech tokens that align with those of SpeechTokenizer [^083].
Specifically, given speech $S$, semantic representation $V_1$, perceptual representation $V_{2:8}$ and the complete information representation $V_{1:8} = V_1 + V_{2:8}$ extracted by SpeechTokenizer, perceptual modeling refers to predicting the complete representation $V_{1:8}$ given the prompt speech a and the semantic representation $V_1$.
SpeechGPT-Gen synthesizes response speech by concatenating the output of **SpeechGPT**[^SpeechGPT] with the prompt speech and using a flow matching model.

- **Mimi.** **Mimi**[^Moshi] has eight codebooks at a frame rate of 12.5Hz, which requires 100 autoregressive steps to generate one second speech.
This results in high computational costs and incompatibility with streaming inference.
To address these issues, **Moshi**[^Moshi] proposes the RQ-Transformer, comprising a temporal Transformer and a deep Transformer.
The RQ-Transformer breaks down a flattened sequence of length $K \cdot S$ into $S$ timesteps for a large temporal Transformer which produces a context embedding used to condition a smaller depth Transformer over $K$ steps.
This allows scaling to longer sequences by increasing $S$ or to a higher depth by increasing $K$ than modeling the flattened sequence with a single model.

- **TiCodec.** Ti-Codec[^094] is a decoupled codec model which can separate the time-varying and time-invariant information in speech and quantize them separately.
Inspired by VALL-E [^041], Freeze-Omni [^082] uses a token-based speech decoder which contains NAR prefill and AR generate stage to achieve speech output capabilities.
The speech decoder mainly consists of the NAR decoder, the AR decoder, and the frozen decoder of a codec model [^094].
Both the NAR decoder and AR decoder are built upon transformer blocks.
The NAR decoder is used to model the semantic features from the output of LLM, and then the AR decoder generates speech tokens based on the output of the NAR decoder.
Finally, the decoder of the codec model converts the speech tokens into a speech stream.

### Discussions about Representation used in Spoken Dialogue Systems

#### Semantic Representation vs.
Acoustic Representation

Current dialogue systems typically choose different approaches for the understanding (input) and generation (output) sides based on task requirements.
For example, Spirit-LM [^057] uses semantic representations (HuBERT [^063]) consistently on both ends, while Mini-Omni [^MiniOmni] uses semantic representations (Whisper [^Whisper]) on the input side and acoustic representations (SNAC [^092]) on the output side.
Each combination offers unique advantages and trade-offs, and a consensus on a unified speech representation approach has yet to be reached in practical applications.

We revisited the differences between semantic and acoustic representations, as shown in Table~\ref{comparison_of_rep}.
Benefiting from specific task objectives, models such as Wav2Vec [^069], HuBERT [^063], WavLM [^070], and Whisper [^Whisper] focus on extracting semantic information embedded within the spoken content.
This inherent advantage allows speech to be directly mapped into the embedding space of large language models (LLMs), facilitating alignment with other modalities and fully leveraging the LLM’s strengths.
In contrast, acoustic representations extracted by models like EnCodec [^066] and DAC [^067] are less conducive to LLM understanding, which is why SpeechTokenizer [^083] and **Mimi**[^Moshi] opt for semantic distillation.
In addition, semantic representations offer higher compression rates.
By configuring various downsampling parameters in convolutional layers, models like HuBERT and Whisper easily achieve frame rates of 25Hz to 50Hz.
Spirit-LM [^057], for instance, uses 25Hz HuBERT units, meaning that only 25 tokens are needed to represent one second of speech.
In contrast, acoustic features are designed with compression and reconstruction in mind, where the constraints of signal transmission make extreme compression and high-quality reconstruction challenging to achieve simultaneously.
Although **Mimi**[^Moshi] has achieved a frame rate of 12.5Hz, its use of 8 codebooks means that autoregressively predicting one second of speech requires 100 steps.
Finally, in certain scenarios, semantic representations hold distinct advantages.

However, we must acknowledge that purely semantic representations fall short in naturalness and expressiveness, especially in tasks involving emotional expression or complex speech dynamics, where acoustic representations provide more nuanced information.
For instance, HuBERT [^063] cannot extract prosodic and stylistic features as effectively as EnCodec [^066] or Emotion2Vec [^034].
Notably, using acoustic representations allows for flexible handling of various data types—speech, audio, music, and sound—making dialogue systems more unified and versatile.
Moreover, when acoustic representations are used as the output of a language model, they can seamlessly connect to the codec decoder for speech synthesis.
In contrast, dialogue systems using semantic features often require separately trained vocoders [^057], [^075] or rely on additional text-to-speech toolkits [^032].
This gap is crucial for dialogue systems, as the resulting latency directly impacts the user experience.

Given the unique advantages of semantic and acoustic features across different tasks, future research may shift toward integrating these features.
A valuable perspective is that models like SpeechTokenizer [^083] and **Mimi**[^Moshi] have already attempted to distill semantic representations from HuBERT [^063] or WavLM [^070] into RVQ-1, ensuring a balanced representation of both semantic and acoustic information in the system.
With technological advancements, we look forward to more unified and refined modeling approaches.
A promising direction would be to design new training objectives for speech tokenizers, exploring both data-driven and objective-driven methods, thus avoiding the need for additional pre-trained models.
As spoken dialogue Systems are still evolving, exploring more robust hybrid representations is indeed valuable.

#### Continuous Representation vs.
Discrete Representation

There is still no consensus on whether to use continuous or discrete representations in the spoken dialogue systems.
Considerations on the input side mainly depend on the type of representation model chosen by the system.
Some systems [^MiniOmni], **Mini-Omni2**[^MiniOmni2], [^032] use models like HuBERT [^063] or Whisper [^Whisper] to extract continuous speech representations, which requires adding a speech adapter and an additional training phase focused on modality alignment.
Another systems **SpeechGPT**[^SpeechGPT], [^033], **Moshi**[^Moshi] use models like EnCodec [^066] or **Mimi**[^Moshi] to extract discrete speech representations, adding speech tokens directly to the LLM’s vocabulary, thereby shifting the training burden onto the LLM itself.
Despite the different approaches, the key is to enable large language models to effectively understand speech features.
For autoregressive models, using discrete inputs may appear more manageable; however, whether this truly outperforms continuous inputs in terms of performance remains to be explored.

Language models trained with next-token prediction objectives tend to favor discrete modalities.
Using discrete features on the output side naturally supports simple codec decoders [^MiniOmni], **Mini-Omni2**[^MiniOmni2], **Mimi**[^Moshi], [^082] for reconstructing high-fidelity speech, enhancing speech quality and acoustic control while enabling an end-to-end system.
In contrast, continuous features may require additional text-to-speech toolkits (**VITA**[^VITA]) or vocoders [^032], resulting in a cascaded pipeline and making it difficult to preserve detailed acoustic information.
Another notable advantage of using discrete representations as output is the ability to quickly feed them into the input of the next dialogue round, as demonstrated in OmniFlatten [^OmniFlatten].
In the field of computer vision, a range of work [^095], [^096] has emerged that combines discrete and continuous representations, aiming to fully integrate these modes without information loss, and has already achieved success in certain areas.
These approaches may provide valuable insights for the next generation of spoken dialogue systems.

#### Single-Layer Quantizer vs.
Multi-Layer Quantizer

As previously mentioned regarding compression rates, the number of quantizers must be carefully considered when using the speech codec.
Currently, dialogue systems commonly use multi-layer quantizers, such as those in EnCodec [^066], SpeechTokenizer [^083], SNAC [^092] and **Mimi**[^Moshi].
This inevitably introduces generation latency, as residual vector quantization requires each quantizer’s input to depend on the output of the previous quantizer.
Mini-Omni [^MiniOmni] and **Mini-Omni2**[^MiniOmni2] adopt an approach similar to MusicGen [^026], introducing delayed steps to enable parallel generation across multiple quantizers.
**Moshi**[^Moshi] proposes splitting the RVQ, allowing the eight VQs to generate independently in parallel.
These strategies help mitigate latency issues to some extent but still fall short of the efficiency achieved with semantic representations.

Recently, research on single-layer quantizers has shown promising breakthroughs.
Models like WavTokenizer [^065], Single-Codec [^097], and BigCodec [^098] advocate using a single VQ to discretize speech, achieving competitive results in both reconstruction and generation tasks.
Notably, WavTokenizer [^065] has already achieved an impressive compression rate of 40Hz.
Integrating a single-layer quantizer with dialogue systems is promising, as it allows for rapid extraction of speech features on the input side and significantly reduces the burden of autoregressive modeling.

#### With Text Guidance vs.
Without Text Guidance

In practice, researchers have found direct speech-to-speech generation challenging [^MiniOmni], **Mini-Omni2**[^MiniOmni2], [^032] due to complex mapping relationships, so intermediate texts are often generated to achieve higher generation quality.
Current end-to-end dialogue systems commonly adopt one of two strategies: one [^032], [^IntrinsicVoice] generates the hidden states corresponding to the text response first, which are then post-processed to obtain speech tokens; the other [^MiniOmni], **Mini-Omni2**[^MiniOmni2], **Moshi**[^Moshi] generates text and speech tokens in parallel.
These approaches leverage the text modeling capabilities of large language models, essentially guiding the synthesis of semantically consistent speech by first generating text.
However, this comes at the expense of response speed.

Although directly performing speech-to-speech generation presents challenges such as increased model complexity and inference difficulty, we believe it remains a promising direction for future research.
One approach is to retrain large spoken language models to adapt to specific speech representations.
However, this faces challenges related to data resources, as large-scale and high-quality conversational datasets remain scarce.
Additionally, this method cannot completely eliminate text prompts and requires multi-stage training, starting with text-speech pairs to allow the model to progressively acquire conversational capabilities.
Another approach could begin with speech codecs, as demonstrated by SpeechTokenizer and Mimi’s extensive work in semantic distillation.
We envision a novel speech codec that aligns text and speech during the encoding phase, thereby reducing the generation burden on large language models.
By aligning speech representations with the text representation space earlier in the process, the autoregressive modeling would no longer require text guidance, giving rise to an entirely new paradigm for conversational systems.

## Training Paradigm of Spoken Dialogue Model

Existing text-based large language models have demonstrated strong contextual understanding and reasoning abilities in the field of natural language processing, such as GPT-4 [^099], **LLaMA3.1** [^LLaMA3], and Qwen-2 [^100].
Due to their training on large-scale corpora, these models achieve exceptional accuracy when handling complex contexts.
To further expand the capabilities of large language models, some research [^033], [^Qwen2Audio], **VITA**[^VITA], **Mini-Omni2**[^MiniOmni2] has explored enabling them to understand other modalities, thereby building multimodal interaction abilities.
The spoken dialogue model, also known as the speech-text dialogue model, allows users to interact with LLMs naturally and straightforwardly through speech.
However, the transition from text intelligence to speech intelligence involves two inherent hurdles: one core issue is the insufficient amount of speech data compared to the massive datasets used for pre-training text-based large language models.
For instance, **LLaMA3.1**[^LLaMA3]  uses 800 billion training tokens, and Qwen-2 [^100] is trained on over 7 trillion tokens, whereas pure speech pre-training data often amounts to hundreds of thousands or millions of hours.
For example, **Moshi**'s[^Moshi] pre-training speech data comprises 7 million hours, and the amount of labeled speech data is even smaller, making it difficult to support LLMs in achieving powerful speech intelligence comparable to text.
Another challenge is that speech information density is not as compact as text.
Text commonly uses byte-pair encoding (BPE) [^101], [^102] encoding to compress it into a tight token space, whereas the speech modality includes not only semantic information but also acoustical information, which is less dense.
This undoubtedly increases the difficulty for LLMs to learn.
Understanding and generating the inherent knowledge of the speech modality more effectively is a significant challenge.

Consequently, existing spoken dialogue models aim to build upon text-based LLMs by incorporating the speech modality into these large language models.
**SpeechGPT**[^SpeechGPT], [^033], [^MiniOmni], **Moshi**[^Moshi] support speech-in and speech-out capabilities for LLMs, forming the foundation of basic speech dialogue capabilities.
Some of the latest advanced approaches **Moshi**[^Moshi], [^OmniFlatten], [^059] attempt to transition from traditional turn-based spoken dialogue systems to full-duplex systems, aiming to simulate the natural spontaneity of human conversation.
While these advancements are promising, achieving low latency and natural interaction in full-duplex systems remains a significant challenge.
Moreover, enhancing LLMs to effectively handle the speech modality—mastering both speech comprehension and generation—while maintaining robust natural language text processing capabilities, is hindered by the limited size of labeled speech datasets.
These datasets are far smaller compared to the vast amounts of pure text data available, which risks diminishing the models' original text processing capabilities.
Thus, building a truly end-to-end conversational model that meets real-world requirements necessitates careful consideration of model architecture, training paradigms, and training data.
Overall, we believe that several key aspects are crucial in the training paradigm of spoken dialogue models: aligning speech-text modalities to ensure consistent understanding, designing multi-stage training strategies for gradual adaptation, and optimizing training structures and inference paradigms for efficient performance.

### Architecture Paradigm about Modal Alignment of Speech and Text

To enable large language models (LLMs) to handle both speech input and output, a significant amount of prior work [^103], **LLaMA3.1**[^LLaMA3], [^032], [^MiniOmni], **Moshi**[^Moshi] has focused on adapting text-based foundation models into robust spoken dialogue models.
Based on different architectural paradigms, these approaches can be broadly categorized into five types, as shown in Figure ~\ref{fig:archi_img1}.

> Figure.05: Categorization Diagram of Spoken Dialogue Model Architectural Paradigms.

**Text-Output Only Method.** These systems [^Qwen2Audio], [^QwenAudio], [^LTU-AS], [^E-chat], [^SALMONN], [^104], [^036], **VITA**[^VITA] maintain the text-based LLM’s foundational structure unchanged, **using an audio encoder and adaptor to map speech input into the LLM's pre-trained text latent space directly.** This method of direct embedding alignment, combined with a multi-task training strategy, equips the LLM with the ability to 'listen,' thus enabling it to understand and process speech modality inputs effectively and perform exceptionally well in various audio understanding tasks.
Nevertheless, the output remains text-based, which necessitates the use of an external text-to-speech (TTS) system [^105], [^045] to generate speech output.
LTU-AS [^LTU-AS] uses Whisper [^Whisper] and the Time and Layer-Wise Transformer (TLTR) as its audio encoder, allowing it to recognize both speech and audio events.
Qwen-Audio 1 [^QwenAudio] scales up audio-language pre-training to cover over 30 tasks and various audio types, facilitating universal audio understanding abilities.
It employs a unified encoder for all audio inputs, bridging the gap between audio and textual modalities, and uses the large language model Qwen-7B [^091] as its foundational component.
Qwen-Audio 2 [^Qwen2Audio] simplifies the pre-training process by utilizing natural language prompts for different data and tasks, with DPO [^106] optimizing the model’s performance in terms of factuality and adherence to desired behavior.
SALMMON [^SALMONN] employs dual auditory encoders: a speech encoder from the Whisper model and a non-speech BEATs [^038] audio encoder.
The auditory features from these two encoders are complementary, making them suitable for general audio inputs that contain both speech and non-speech information.
These inputs are then connected to a well-trained LLM using Q-former style attention to generate responses.
**VITA**[^VITA] implements a duplex solution through two independent modules: one generates text responses to user queries, while the other continuously monitors environmental input to selectively provide updated interaction content, although it still requires an external TTS system.
All the aforementioned methods frequently overlook paralinguistic information, including emotion, prosody, and non-verbal elements, rendering them insufficient for scenarios that involve emotional speech dialogue.
ParalinGPT [^ParalinGPT] utilizes an ASR model to obtain text and a speech encoder to extract emotion embeddings, thereby more accurately simulating both the linguistic content and paralinguistic attributes of spoken responses.
E-chat [^E-chat] employs a Hubert speech encoder [^063] to extract speech and emotion features, using a connection module to map these features to the textual space within the LLM decoder.
Although these approaches have explored emotional responses within spoken dialogue systems, they require additional systems to synthesize speech from text and suffer from high latency, making real-time dialogue challenging to achieve.

**Chain-of-Modality (CoM) Method.** This method tokenizes speech into discrete tokens and extends the LLM’s vocabulary to handle both speech input and output.
To address alignment issues between speech and text modalities, Recent works **SpeechGPT**[^SpeechGPT], [^SpeechGPT-Gen], [^107], [^033] utilize a prompting approach called Chain-of-Modality (CoM), which first generates response text autoregressively before producing the corresponding speech.
This technique allows the text LLM's output to guide speech generation, thereby enhancing the quality of the response content.
However, it is not suitable for live interactions, as the model must complete the entire text response before beginning speech generation, leading to increased response latency.
**SpeechGPT**[^SpeechGPT] and SpeechGPT-gen [^SpeechGPT-Gen] employ the SpeechTokenizer [^083] model as a speech token extractor, breaking down speech generation into the prediction of semantic tokens followed by acoustic tokens.
Spectron [^107] performs speech continuation by predicting spectrograms frame-by-frame, optimizing the LLM with a combination of cross-entropy loss for text and reconstruction loss for speech frames.
EMOVA [^033], on the other hand, utilizes the FSPIRAL [^081] architecture for its speech encoder to capture phonetic and tonal information, which is then discretized using finite scalar quantization (FSQ) [^FSQ].
Its speech response procedure is divided into three primary steps: 1) transcribing user instructions into text, 2) generating textual responses based on these instructions, and 3) producing style labels and response speech units from the textual responses.
This process enables EMOVA to facilitate emotional speech dialogue.

**Interleaving Text and Speech Tokens.** Some earlier models [^103], [^108] employed supervised training methods, using specific input and output sequences, and trained on mixed speech-text tasks, including text-to-speech (TTS), automatic speech recognition (ASR), and speech-to-speech translation.
Spirit-LM [^057] leverages the temporal alignment between speech and its transcription, continuing training on a pre-trained text-based LLM using alternating text and speech tokens.
This significantly improves the model’s performance in both speech understanding and generation.
However, it employs discrete Hubert units [^063] as speech representations, which results in some loss of paralinguistic information.
USDM [^075] continues pretraining Mistral-7B [^109] with interleaved speech-text data to capture multimodal semantics.
For dialogue finetuning, it constructs templates using both speech and transcripts of user input as instruction data.

**Parallel Generation of Text and Speech.** PSLM [^056] proposes generating speech and text tokens in parallel to reduce latency; however, this approach may compromise response quality.
Additionally, this method still relies on speech recognition for input [^Whisper], which introduces further delay.
Llama-Omni [^032] introduces a novel streaming speech decoder that can simultaneously generate text responses and discrete speech unit sequences, significantly reducing latency and meeting real-time interaction needs.
**Moshi**[^Moshi] and Mini-Omni [^MiniOmni] adopt similar approaches, introducing dual streams that generate both speech tokens and corresponding text tokens simultaneously on the assistant side, facilitating the transfer of the pre-trained LLM’s textual capabilities to the speech modality, enabling the model to directly engage in reasoning through speech.
The key difference lies in how speech-text alignment is handled: **Moshi**[^Moshi] uses explicit alignment information to supervise the model’s learning, while Mini-Omni [^MiniOmni] allows the LLM to learn implicit alignment information.
On the input side, Mini-Omni feeds continuous speech embeddings from the Whisper encoder [^Whisper] into the LLM, enhancing the model's ability to understand spoken instructions without requiring text input.
However, inconsistencies between speech input and output introduce additional computational overhead, increasing latency in multi-turn dialogue scenarios.
In contrast, Moshi allows users to input speech without relying on text, and generates both text and speech tokens in parallel on the assistant side.
Moshi further extends its architecture to model several speech streams in parallel, allowing for conceptually and practically simple handling of full-duplex dialogues with arbitrary dynamics.

**Speech-to-Speech Generation.** This approach aims to remove the dependency on intermediate text, thereby reducing latency and making the system closer to real-time interaction.
SyncLLM [^059] achieves real-time full-duplex interaction through time chunking methods, integrating time information into LLMs to enable synchronous operation with the real-world clock.
IntrinsicVoice [^IntrinsicVoice] utilizes a specific model to generate multiple speech tokens in a single step, effectively reducing speech token sequences to lengths comparable to text sequences while producing high-quality audio.
Align-SLM [^080] utilizes a pre-trained self-supervised Hubert model [^063] with K-means clustering [^110] to convert continuous speech representations into discrete units.
It employs LoRA adapter [^035] fine-tuning on a pre-trained Twist [^110] to produce multiple speech continuations from a given prompt and uses semantic metrics to generate preference data for Direct Preference Optimization (DPO) [^106].
Experimental results indicate that integrating the preference optimization method significantly improves the semantic comprehension of the Spoken LLM.

### Multi-stage Training strategy

This section primarily discusses the training process of the Spoken Dialogue Model, building upon previous work on spoken dialogue systems.
Generally, this process consists of four stages: text LLM pre-training, modality adaptation and alignment post-training, followed by supervised fine-tuning, and optionally, preference optimization.
The primary goal in training most spoken dialogue systems is to preserve the model's original capabilities while integrating the speech modality for voice interaction into the LLM.
The diagram of multi-stage training can be referred to in Figure ~\ref{fig:archi_img2}.

> Figure.06: Diagram of Multi-stage Training Steps.

#### Text LLM Pre-Training

The goal is to develop a text-intelligent LLM model capable of handling complex contexts and possessing knowledge reasoning abilities, thus preparing it for integration with speech-intelligent LLMs.
Most spoken dialogue systems utilize pre-trained large language models as foundational models rather than pre-training with separate text data themselves.
A series of approaches **SpeechGPT**[^SpeechGPT], [^SpeechGPT-Gen], [^057], [^033], [^032], [^059] use the LLaMA model and its variants as their foundational language model.
On the other hand, [^084], [^MiniOmni], **Mini-Omni2**[^MiniOmni2], [^OmniFlatten] employ the Qwen [^091], [^100] family of large language models as their backbone.
Meanwhile, **Moshi**[^Moshi] employs an RQ-Transformer for hierarchical autoregressive modeling of speech, utilizing a unique structure that involves pre-training a text-only language model with datasets from the internet (e.g., Wikipedia \footnote{[URL](https://dumps.wikimedia.org/)} and StackExchange \footnote{[URL](https://archive.org/details/stackexchange/)}).
The collected data was filtered using a comprehensive preprocessing pipeline to ensure quality and relevance, which included deduplication to remove redundant entries, language identification to retain text in the desired language, and quality filtering to exclude low-quality or irrelevant content based on criteria such as coherence and completeness.
**VITA**[^VITA] utilizes Mixtral 8x7B1 [^111], a representative LLM with a sparse mixture of experts (SMoE) architecture, and performs pure-text instruction tuning for its extended Chinese vocabulary.

#### Modality Adaptation and Alignment Post-training

> Figure.07:Alignment Post-training Methods.

This phase explores strategies to adapt text-based large language models (LLMs) for speech modality input, focusing on aligning text and audio modalities effectively.
The primary goal is to enhance the models' ability to understand and generate speech by bridging the gap between these two modalities.
Common approaches include multimodal training techniques, leveraging unlabeled speech corpora, and employing multi-task learning frameworks.
These methods typically involve fine-tuning existing LLMs with speech-related tasks and integrating speech-specific modules, such as speech adaptors and decoders, to facilitate seamless interaction between text and speech modalities.
Different training tasks for modality adaptation and alignment are shown in Figure ~\ref{fig:archi_img3}.
Spirit-LM [^057] continuously pretrains on text LLM checkpoints using interleaved text and speech tokens to improve the model's performance in speech understanding and generation.
LLaMA-Omni [^032] adopts a two-stage training strategy: the first stage jointly trains a speech adaptor and LLM with speech input and text responses, while the second stage uses the same dataset to train a streaming speech decoder independently.
Consequently, this LLM primarily possesses the capability for speech input understanding, with speech generation handled by a separate decoder module.
**SpeechGPT**[^SpeechGPT], **Moshi**[^Moshi], and **VITA**[^VITA] utilize unlabeled speech corpora to train models in a next-token prediction task.
In the first phase, **VITA** focuses on training the audio encoder and connector, while in the second phase, it optimizes both the connector and the LLM model through multimodal training.
Although capable of processing speech input, it outputs only text.
Spectron [^107] addresses the alignment issue between text and speech representations by jointly supervising multiple objectives.
IntrinsicVoice [^IntrinsicVoice] employs a two-stage training approach, constructing multiple cross-modal tasks from a single dataset to enable the model to better learn the semantic consistency between speech and text.
Mini-Omni [^MiniOmni], EMOVA [^033], and OmniFlatten [^OmniFlatten] adopt similar methodologies, commencing with supervised multi-task fine-tuning of the text LLM backbone to achieve speech-text modality alignment and develop a multimodal LLM[^112], [^113] using Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) tasks.
Notably, Mini-Omni divides the training of various modules into three phases: the first phase utilizes data from speech recognition and synthesis to enhance the model’s abilities in these aspects, training only the ASR and TTS adapters.
The second phase focuses exclusively on enhancing the model’s text capabilities when given speech inputs, updating only the LLM parameters while freezing other modules.
Through these two training phases, the original language LLM’s capabilities are maximally preserved, while adapting to speech modality input and output, thereby addressing the primary modality alignment tasks.

#### Supervised Fine-tuning or Dialogue Dataset Fine-tuning

During this stage, most models use instruction-following datasets or dialogue data for supervised fine-tuning of the LLM, enhancing natural conversational abilities.
**SpeechGPT**[^SpeechGPT], [^SpeechGPT-Gen] propose a two-stage instruction-tuning process that includes cross-modal instruction fine-tuning and chain-of-modality instruction fine-tuning.
Ultimately, the model follows the A-T-T-A method to achieve end-to-end speech input and output.
EMOVA [^033] employs a similar chain-of-modality concept to construct instruction-tuning datasets, empowering it to respond accurately to speech instructions.
**Moshi**[^Moshi], Mini-Omni [^MiniOmni], OmniFlatten [^OmniFlatten], and SyncLLM [^059] utilize spoken dialogue datasets for fine-tuning, endowing the models with conversational interaction capabilities.
Remarkably, Moshi constructs a more natural and realistic dialogue dataset that incorporates elements such as noise and overlap, enabling the model to learn authentic multi-stream interactions.
OmniFlatten fine-tunes the speech-text LLM using interleaved and serialized dialogues across three stages to progressively train the model in acquiring half-duplex and full-duplex communication capabilities.
Similarly, SyncLLM employs a three-stage training procedure that predominantly uses synthetic spoken dialogue data along with a relatively small amount of real-world spoken dialogue data to develop a full-duplex voice agent.

#### Preference Optimization and Reinforcement Learning

The research on leveraging preference optimization to align a spoken dialogue model with human preferences is virtually absent.
Recently, [^114], [^115], [^116] adopted preference optimization for Text-to-Speech (TTS) models to align speech synthesis quality with human preferences but not for spoken dialogue models.
Align-SLM [^080] pioneers the integration of Direct Preference Optimization (DPO) [^106] in textless Spoken Language Models (SLMs) to enhance semantic understanding.
It transforms continuous speech into discrete units using a pre-trained Hubert model and K-means clustering.
LoRA fine-tuning on a Spoken LLM generates multiple speech continuations from prompts.
Semantic metrics create preference data offline, making DPO training efficient and stable, eliminating the need for an external reward model.
Coupled with curriculum learning [^117], Align-SLM progressively refines preference data selection, optimizing semantic feedback, and improving SLM performance.


### Training Frameworks and Generation Strategies

Recent advanced methods in spoken dialogue models employ a variety of innovative techniques to achieve more natural speech output and lower latency.
In this part, we explore various approaches that exemplify these advancements:

- **LLama-Omni.** LLama-Omni [^032] adds a streaming speech decoder that operates after the LLM.
This decoder runs in a non-autoregressive manner, taking the output hidden states from the LLM as input and generating the discrete unit sequence corresponding to the speech response.
To model the variable-length mapping between input and output, LLama-Omni employs an upsample factor, denoted as $\lambda$, along with Connectionist Temporal Classification (CTC) loss [^118].
This ensures that the model can generate speech responses simultaneously with text responses.
Additionally, a predefined chunk size is set to further enable vocoder streaming synthesis of speech waveforms, facilitating real-time interaction and reducing latency.

- **Mini-Omni.** Mini-Omni [^MiniOmni] selects SNAC [^092], a music-grade encoder, to discretize one second of audio into hundreds of tokens, which significantly increases the burden on the LLM for modeling speech tokens.
Delay Pattern language model decoding strategies are often applied in modeling multiple parallel streams of acoustic tokens in speech tasks like MusicGen [^026], VoiceCraft [^119], and Parler-TTS [^046].
Compared with traditional sequential step decoding, this strategy can effectively reduce the time steps required for LLM decoding and generating speech tokens.
Inspired by this, Mini-Omni innovatively applies text-instructed delayed parallel generation to address the issue of long SNAC codebook sequences, simultaneously producing audio and text tokens.
This effectively leverages and preserves the original capabilities of the language model.
Moreover, Mini-Omni proposes a Batch Parallel Decoding method.
Specifically, it generates two samples in parallel for a single input: the first predicts text tokens, and the second predicts both text and speech tokens simultaneously.
The text output from the first sample is embedded into the corresponding positions of the second sample, while the second sample's text output is discarded.
This further enhances the model’s reasoning capabilities during dialogue, maximizing the transfer of its text-based abilities.

- **IntrinsicVoice.** IntrinsicVoice [^IntrinsicVoice] introduces a speech encoder and a streaming vocoder for the tokenization and detokenization of speech, and a GroupFormer for modeling speech and text sequences.
This architecture integrates a large language model (LLM) with a GroupModel.
Specifically, it uses a pre-trained HuBERT encoder [^063] and its corresponding KMeans quantizer [^110] to process speech inputs into discrete units.
These units are organized into a grouped token sequence through a group partition operation.
The grouped tokens are then passed through an embedding layer and adaptor module to map these embeddings into the LLM's embedding space.
The context embeddings output by the LLM are processed through a linear layer and concatenated with a specified number of learnable queries.
This input is fed into a smaller non-autoregressive transformer encoder model, dubbed the "GroupModel," to predict a group of speech tokens in one step.
The introduction of GroupFormer effectively improves the model's ability to handle sequences within a group, mitigates the modality gap between speech and text, accelerates inference speed, and alleviates issues associated with long-sequence modeling.

- **Moshi.** **Moshi**[^Moshi] introduces a mini codec model with 8 codebooks at a frame rate of 12.5 Hz for speech representation, where one second corresponds to 100 speech tokens.
It adopts an RQ-Transformer consisting of a Temporal Transformer and a smaller Depth Transformer as the backbone network for the LLM, hierarchically modeling multi-codebook audio tokens.
Similar architectures have appeared in prior research, such as UniAudio [^120] and Megabyte [^121].
The Depth Transformer models sub-sequence tokens conditioned on temporal context predicted by the Temporal Transformer.
Given the smaller size of the Depth Transformer, sub-sequence generation can almost be viewed as parallel generation.
This allows the model to scale to longer sequences by extending the temporal modeling capacity of the Temporal Transformer or to achieve greater depth by enhancing the hierarchical modeling capabilities of the Depth Transformer, rather than modeling the flattened sequence with a single model.

- **SyncLLM.** SyncLLM [^059] employs an auto-regressive transformer decoder for full-duplex dialogue, integrating time synchronization to align speech units with the real-world clock.
It predicts interleaved speech tokens for both dialogue partners, maintaining timing with speaker tags.
The model is trained on deduplicated HuBERT token sequences to enhance semantic fidelity while managing latency by anticipating user responses.
Interpolation reconstructs token sequences to fit expected structures, facilitating seamless speech synthesis.

**Text-guided generation.** Some end-to-end methods like **SpeechGPT**[^SpeechGPT], [^SpeechGPT-Gen], [^107], [^033] use chain-of-thought reasoning, which allows guiding speech generation with the output of an underlying text LLM.
However, this is fundamentally incompatible with live interactions, as the model needs to produce an entire answer as text before it starts speaking.
Later methods [^032], [^MiniOmni], **Moshi**[^Moshi] can accept user speech input and simultaneously output speech and text, ensuring high-quality responses while significantly reducing latency.
Lama-Omni [^032] utilizes a streaming decoder to generate text and speech tokens in parallel.
Mini-Omni [^MiniOmni] is restructured to transfer language reasoning abilities to streaming audio output through a text-audio parallel decoding approach.
**Moshi**[^Moshi] details a novel feature, the Inner Monologue, which consists of joint modeling of the textual and speech modalities on the system side to improve the quality of interactions.

**W/o text-guided generation.** Other methods achieve speech-to-speech generation without relying on text stream generation.
IntrinsicVoice [^IntrinsicVoice] introduces a novel GroupModel that predicts a group of speech tokens in one step based on global context embeddings.
SyncLLM [^059] predicts interleaved chunks of token sequences at each time step, allowing the model to handle all conversational cues such as backchannels, overlaps, interruptions, etc.

### Discussions about Training Paradigm in Spoken Dialogue Models

#### Text and Speech Modality Alignment

In spoken dialogue systems, the alignment between speech and text modalities is a crucial stage.
To preserve the textual intelligence of large language models (LLMs) as much as possible, nearly all current methodologies **SpeechGPT**[^SpeechGPT], [^056], [^032], [^MiniOmni], **Mini-Omni2**[^MiniOmni2], **Moshi**[^Moshi], [^OmniFlatten] incorporate a post-training phase utilizing speech-text paired data when developing spoken dialogue models.
This may involve either expanding the vocabulary to treat speech tokens as an extension of the original vocabulary or using speech adaptors to map speech embeddings to the original text latent space of the LLM, and designing multi-task training objectives to achieve alignment between text and speech modalities.
For example, data from speech recognition and speech synthesis can be used to train the model's speech recognition and synthesis capabilities.
Although this is an effective strategy, its implementation can still lead to a certain degree of catastrophic forgetting in LLMs due to the large volume of pre-trained text corpora and the imbalance with paired speech-text data, which can harm the model's text-based capabilities.
Therefore, precise parameter design and customized optimization strategies are needed to mitigate this issue as much as possible, as demonstrated by approaches like **Moshi**[^Moshi].

This raises a consideration: during the training phase of spoken dialogue models, is it feasible to directly utilize speech data for adaptation to text-based LLMs, thereby eliminating the necessity for speech-text paired data? This is because unlabeled speech data is abundant and easily accessible, making it convenient and beneficial for training the speech intelligence of LLMs.
This approach would require us to obtain a pre-aligned speech representation with the text modality.
Perhaps we can consider further exploration and experimentation in the speech tokenizer component, such as directly mapping the semantic discrete units of speech onto the text token space to achieve enforced alignment.

#### Different Temporal Alignment Methods in Spoken Dialogue Models

In speech and text modalities, there is often a significant mismatch in sequence lengths.
Even when some speech tokenizers [^065], [^097] employ extreme sequence compression methods, a length gap remains between the two.
Temporal alignment information between speech and text has been explored in tasks like Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) as demonstrated by models such as Whisper [^Whisper], FastSpeech [^FastSpeech2], and VITS [^122].
Recently, some spoken dialogue systems have utilized temporal alignment information to enhance model performance, yielding promising results.
For instance, Spirit-LM [^057] uses interleaving text and speech tokens for continual pre-training on the LLaMA base model, significantly boosting the model’s performance in speech understanding and generation.
Experimental visualizations demonstrate that the similarity between text and speech features is notably higher in models trained with interleaved token sequences compared to those trained without this approach.
This indicates that providing the model with explicit fine-grained temporal alignment information can effectively enhance modality alignment and improve the performance of LLMs.

Mini-Omni [^MiniOmni] achieves parallel generation of text and speech by padding text tokens to match the length of speech tokens, allowing the LLM to implicitly learn the alignment information between speech and text tokens.
This can be viewed as a form of sentence-level temporal alignment information, a method also utilized in recent speech synthesis work [^048].
**Moshi**[^Moshi], on the other hand, uses word-level speech-text temporal alignment information and special marker tokens to achieve similar parallel generation capabilities.
The difference lies in that Mini-Omni fully allows the LLM to implicitly learn the alignment, whereas Moshi provides word-level alignment priors first, and then lets the model learn finer-grained alignments.

Exploring the impact of introducing different levels of temporal alignment priors on the training effectiveness of spoken dialogue models, such as sentence-level, word-level, or phoneme-level, is an intriguing area of research.
Understanding how these various alignment strategies affect model performance can guide the development of more efficient and accurate systems.
For instance, sentence-level alignment might offer a broader contextual understanding, while word-level or phoneme-level alignments could provide more detailed synchronization between speech and text, potentially leading to improvements in nuanced tasks like speech synthesis and understanding.

#### Reinforcement Learning (RL) in Spoken Dialogue Models

Reinforcement Learning (RL) has proven to be an effective learning paradigm in text and image processing [^123], [^124], [^125].
Recent research has shown that Direct Preference Optimization (DPO) [^106] can be extended to music and speech generation [^126], [^115].
MusicRL [^126] uses Reinforcement Learning from Human Feedback (RLHF) to improve music generation by fine-tuning a pretrained model for better text adherence and audio quality.
By collecting extensive human feedback, MusicRL creates a more refined and subjective music generation system.
Seed-TTS [^114] explores RL methods, comparing external reward models like REINFORCE with simpler methods like DPO.
The study highlights using REINFORCE to enhance speaker similarity and emotion controllability in the Seed-TTS system.
Qwen2-Audio [^Qwen2Audio] uses DPO to align with human preferences by optimizing responses based on human-annotated data.
This enhances its ability to follow audio instructions accurately and intelligently respond to complex audio inputs, improving its performance in audio-centric tasks.
However, in the dialogue system field, reinforcement learning techniques based on human feedback [^127] are rarely applied.
Considering the diversity of inputs and outputs in large language models, exploring the incorporation of reinforcement learning strategies such as Proximal Policy Optimization (PPO) [^123] can be beneficial.
Additionally, considering the performance metrics for evaluating spoken dialogue systems, designing targeted reinforcement learning strategies and feedback functions to enhance different objectives is also a direction worth exploring.

## Streaming, Duplex, and Interaction

Streaming, full-duplex technology, and interactions, are crucial elements for enhancing the interactive capabilities of spoken dialogue models because they directly impact the system's responsiveness, the fluidity of natural interaction, and its ability to handle complex interactions.Unlike text language models, spoken dialogue models require real-time processing of user input.
**Streaming** allows the system to instantly acquire and process speech data; **full-duplex technology** enables both the system and user to speak simultaneously, enhancing the naturalness of interaction; and **handling of interactions** provides the model with the ability to recognize and adapt to various conversational contexts, making the dialogue more intelligent and realistic.
Building on early explorations, GPT-4o's advanced spoken dialogue capabilities have ignited a surge of research interest.
With real-time voice processing and natural conversational interaction, these models offer users a seamless and efficient communication experience.
However, achieving these capabilities requires deep research into model architecture, data collection, system design, and training methods.
The model needs to be carefully designed and optimized in terms of real-time performance, stability, and response speed.
At the same time, duplex technology is an indispensable key implementation, which ensures that the voice model has both "ears" and "mouths".
Next, we will first discuss the streaming processing method in Section 5.1, then introduce the key technologies of duplex communication and explains how to handle interactation to improve user experience in Section 5.2.

### Streaming Spoken Dialogue Models

The core of streaming speech models lies in their "real-time" and "continuous" capabilities, meaning they can process input and generate output simultaneously without waiting for complete input.
This includes two main aspects:

- **Streaming Understanding.** The model can process audio input as the user speaks, without needing to wait for the user to finish entirely, allowing it to align more naturally with the flow of conversation.

- **Streaming Generation.** This concept refers to the model's ability to generate output without waiting for all intermediate hidden states.
Instead, it can produce output progressively as processing occurs, which improves responsiveness and allows for smoother, more efficient interactions.

These streaming capabilities allow the model to perform more fluidly in real-time interactions, providing a seamless communication experience for users.
We will explore streaming techniques in both end-to-end and cascaded spoken dialogue models, discussing the implementation methods of streaming in each system and highlighting their similarities and differences.

#### Streaming End-to-End Spoken Dialogue Models

End-to-end streaming spoken dialogue models often leverage the knowledge of pre-trained text language models alongside an audio tokenizer, employing an tokenizer-detokenizer architecture to process and output audio signals.
Based on the concepts of streaming input and output discussed above, end-to-end models also require specific design considerations to enable streaming capabilities.
These designs center around the model’s input and output handling and can be distilled into three core techniques: causal convolution, causal attention mechanisms, and queue management.

**Causal Convolution.** Causal Convolution[^128] is a specialized form of convolution widely used in time-series processing, especially suitable for streaming speech models.
The key feature of causal convolution is that the current output depends only on the current and past inputs, without being influenced by future inputs, thereby strictly respecting temporal order.
Unlike regular convolution, causal convolution achieves this by "shifting" the convolution kernel to avoid accessing future information.
In a one-dimensional time series, if the convolution kernel size is \(k\), a standard convolution would use data from \((t - k/2)\) to \((t + k/2)\) at the current time step \(t\).
Causal convolution, however, pads the input on the left with \(k-1\) zeros so that the kernel only uses data from \(t - k + 1\) to \(t\), aligning the kernel to only consider current and past inputs.
This padding ensures that each layer's output depends solely on current and prior information, maintaining causality.
To further expand the model’s receptive field while preserving causality, **dilated causal convolution** can be used.
This technique introduces gaps within the kernel by inserting zeros between weights, effectively expanding the convolution’s range.
This allows the model to capture longer dependencies in the data without increasing latency, which is particularly useful for streaming applications.
In streaming spoken dialogue models, causal convolution plays a critical role in:

- **Ensuring real-time processing.** Causal convolution allows the model to compute outputs without accessing future frames, enabling real-time processing by generating outputs as input is received, which is essential for streaming.

- **Reducing latency.** By not requiring future input data, causal convolution significantly lowers the latency in speech models, making it more suitable for real-time interaction applications, such as voice assistants and live translation.

**Causal Attention.** Causal Attention is a specialized form of the attention mechanism designed to ensure that each position in a sequence can only attend to previous positions, thus preserving the temporal order crucial for streaming models.
This approach ensures that the model’s current output depends only on past and present information, preventing any “leakage” of future information, which is essential for real-time processing tasks.
In causal attention, the attention mask is typically used to achieve causality.
By applying a mask that blocks connections to future time steps, the model restricts each token’s receptive field to only the tokens before it.
Specifically, a lower triangular mask is applied to the attention matrix, setting values to negative infinity for positions corresponding to future tokens.
This masking technique ensures that the model’s predictions for each time step only consider current and past inputs, thereby adhering to a strict causal structure.
In streaming speech models, causal attention plays a significant role in enabling real-time interaction.
Unlike standard attention, which requires access to the entire sequence, causal attention can operate incrementally.
As new inputs are processed, the model can generate outputs without waiting for future context.

**Queue Management**[^129] Audio streams are typically split into frames, then processed in sequence via a queue management system that ensures real-time, orderly processing.

Some end-to-end models, such as Llama-Omni[^032], Mini-Omni[^MiniOmni] and **Mini-Omni2**[^MiniOmni2], employ non-streaming ASR model Whisper as an audio encoder components.
These models have made improvements on the output side to reduce latency.

- **Mini-Omni.** Mini-Omni use a generation strategy delayed parallel decoding is a that layer-by-layer delays during audio token generation.
This allows the model to generate text and multiple audio tokens simultaneously at each step, accelerating streaming audio generation and ensuring low-latency real-time output.

- **Llama-Omni.** Llama-Omni incorporates a non-autoregressive streaming speech decoder that leverages connectionist temporal classification (CTC) to directly generate a sequence of discrete audio tokens as the response.

- **Intrinsicvoice**.[^IntrinsicVoice] Intrinsicvoice introduced GroupFormer module  to group speech tokens, reducing the length of speech sequences to match that of text sequences.
This approach accelerates inference, alleviates the challenges of long-sequence modeling, and effectively narrows the gap between speech and text modalities.We think they cannot be considered fully streaming because they are not designed to be streaming on the input side.

- **Moshi.**[^Moshi] In contrast, Moshi references the architecture of SpeechTokenizer to train a streaming codec from scratch, serving as the audio tokenizer-detokenizer.
The entire model, including the codec, transformer, and attention mechanism, is built on a causal structure.

- **OmniFlatten.**[^OmniFlatten] OmniFlatten proposes chunk-based processing of text and speech along with gradual learning techniques and data handling to reduce turn-taking delays, such as response delays when users finish speaking or interrupt the system.
These models have achieved true streaming capabilities and established a foundation for diverse, bidirectional interactions.

#### Streaming Cascaded Spoken Dialogue Models

Consistent with the above, ensuring streaming capability in a model relies on designing both input and output for streaming.
Due to its cascaded nature, a cascaded model typically relies on external streaming ASR and TTS components, placing the streaming responsibility on these ASR and TTS modules.

In[^130], comparative studies were conducted on the streaming ASR model **U2++ Conformer**[^131], streaming TTS model **XTTS-v2**[^105], non-streaming ASR **Whisper**, and non-streaming TTS **VITS**[^VITS2].
The combination of streaming components achieved the lowest latency and significantly contributed to interactive interruption capabilities.

### Duplex Technology and Interaction

#### Duplex Technology

The term Duplex originates from the field of communications, used to describe interaction modes between two parties in data transmission.
Depending on the type of communication, duplex is divided into half-duplex and full-duplex.

With the development of audio processing and generation technology , the concept of duplex has been introduced to speech systems, especially within the context of speech language models.
Here, duplex doesn’t just refer to signal transmission but emphasizes the synchronization and natural interaction in human-computer dialogue.
Specifically, within model architecture, it means that the model must retain its ability to perceive external input even while generating a response---essentially, the ability to listen while speaking.

**Simplex.** In simplex communication, data flows in only one direction.
The speaker can send data, while the listener can only receive it.
As shown in Figure \ref{fig:simplex}, the robot continuously transmits audio, while the user has no ability to respond.
This fixed-direction, one-way communication has the limitation of lacking interactivity.

**Half-Duplex.** In half-duplex communication, data flows in both directions but not simultaneously.
The two parties must take turns speaking and listening.
As illustrated in Figure \ref{fig:half-duplex}, the user speaks first, followed by a response delay during which the robot "thinks" before replying.
The robot’s response occurs only after the user has finished speaking, and vice versa.
This turn-taking method is similar to using a walkie-talkie, where each party can only transmit after the other has finished, limiting efficiency.Half-duplex is a common mode in early voice interaction systems.
In a typical half-duplex interaction, there are noticeable pauses in the conversation; the user and the system cannot “speak”  simultaneously, making the conversation feel less smooth, much like communication through a walkie-talkie.
For example, voice assistants like Siri use wake words or button presses to trigger the dialogue and require the speaker to finish a complete sentence before responding.
These systems typically adopt an ASR-LM-TTS cascaded structure and are often constrained by cascade delays and the turn-based nature of text language models.
Although this interaction method is simple and easy to implement, it can feel rigid and disjointed in natural conversational settings, with notable latency.
It is designed more for command execution rather than interactive communication.

**Full-Duplex.** Full-duplex communication allows both parties to send and receive data simultaneously[^132].
Figure \ref{fig:full-duplex} shows the user and robot engaging in overlapping, real-time interaction, where backchannels and interruptions are possible.
This mode enables a natural, two-way conversation, where both the user and robot can speak, respond, and even interrupt each other as needed, much like a phone call.In dialogue systems, full-duplex means that the system and user can speak simultaneously and interrupt each other, making it closer to natural conversation in real life.
Full-duplex large voice models allow the system not only to listen and understand the user while they speak but also to interrupt at appropriate moments or respond with backchannel cues.
Moreover, the system can detect the user’s intent to interrupt and pause itself accordingly, maintaining a smooth flow in the interaction.


The ultimate goal of a spoken dialogue moded is to make the user feel as though they are conversing with a real human friend.
Clearly, full-duplex technology is essential for achieving natural voice dialogue systems, enabling the system to send and receive audio signals simultaneously, thus facilitating real-time interaction.
Unlike text-based models, it doesn’t “cover its ears” while speaking.
Users and intelligent agents can interrupt each other while listening or express their attitude through non-verbal signals, such as interjections or laughter.
The challenges in realizing this lie in ensuring conversational fluidity, seamless turn-taking, and precise timing of interactions.
Developing a full-duplex system that can both generate and receive voice signals in complex interactive scenarios remains a key focus in academic and industrial research.

#### Interaction

Now that we understand duplex technology, we can further explore duplex spoken dialogue model.

We start with some concept.Turn-taking is the core concept in duplex dialogue.
It refers to the process in which speakers take turns speaking in an orderly manner during a conversation, forming a pattern of turn-taking.
Over the past few decades and has been extensively studied across fields such as linguistics, phonetics, and sociology.
Some research [^133], [^134]uses a non-deterministic finite-state machine with six states to describe the turn-taking behavior between the system and the user in a spoken dialogue system (SDS).
It outlines all possible states of turn-taking within an SDS, defining the objective of turn-taking as minimizing mutual silence or overlap between interlocutors, thereby improving communication efficiency.
Turn-taking encompasses three fundamental concepts:

- **Turn-taking cues**[^135], [^136].
These include voice, rhythm, breathing, gaze, or gestures.
Agents can use these cues to determine whether to take a turn from the user or to relinquish the turn.

- **Turn-end detection or prediction.** The distinction between detection[^137], [^138] and prediction[^139], [^140] lies in that detection determines whether the agent should take a turn at the current moment, whereas prediction decides when the turn-taking should occur in the future.

- **Overlap.** This mainly involves two situations.
When the user and agent’s voices overlap, if the user intends to take the turn from the agent, this behavior is defined as an \textit{interruption}[^141], [^055].
If the user has no intention of taking the turn, this behavior is considered \textit{backchannel}[^142] or a listener response, such as "uh-huh," "right."

Through these concepts, we can better understand turn-taking behavior in duplex dialogues.
In summary, our interactions with voice dialogue systems can be categorized as \textit{interruptions}, \textit{backchannels}, and \textit{normal turn exchanges}.

The earliest full-duplex systems used a simple Voice Activity Detection (VAD) component to model whether the user intended to interrupt.
However, this approach is inadequate for handling backchannel interaction forms, leading to frequent interruptions and introducing considerable delays.

We can briefly categorize the exploration of interactions into cascaded systems and end-to-end systems based on duplex technology.
Regardless of the system type, the critical core idea is that the system must continuously track external information in real-time, analyze it, and determine the model’s operational state accordingly.
An interactive voice system must meet two requirements: 1) The ability to accept external information in real-time at any moment.
2) The ability to respond to this information accurately.
This includes:

- **Detecting User Interactions.** When the user tries to interject or provide new information, the system can recognize this intent and immediately stop its output to allow the user to speak.

- **Backchanneling During User Speech.** While the user is speaking, the system can provide brief acknowledgments like "uh-huh" or "I see" to indicate active listening, which encourages the user to continue.

- **Quickly Responding After User Completion.** When the user finishes speaking, the system can promptly recognize this cue and respond without unnecessary delays, maintaining a smooth conversational flow.

- **Handling Pauses in User Speech.** When the user briefly pauses, the system can interpret this as a moment of thought rather than an invitation to respond, thus avoiding premature interruptions and preserving the natural flow.

- **Interrupting the User When Necessary.** In situations where the system detects critical information, it can choose to interrupt the user to provide immediate feedback.
For example, if the user is speaking but the system needs to alert them to an error, it can intervene in real-time to ensure effective communication.

**Cascaded Systems.**
To enable interactive functionality, cascaded spoken dialogue models typically require explicit modeling of dialogue turns.
As the core, the large language model needs effective context and turn management.
Next, we introduce several representative works on interaction in cascaded systems.

- **Duplex Conversation.** In [^143], three core modules are proposed to achieve smooth full-duplex dialogue: user state detection, response signal selection, and interruption detection.
The user state detection module not only focuses on traditional turn-end detection but also identifies whether the user intends to switch turns, continue speaking, or hesitates during their speech.
To achieve this, the system uses a multimodal model, taking audio and text as inputs, and incorporates features such as speech rhythm, pitch, and pauses for more accurate assessment of the user’s state, determining whether to respond immediately or wait longer.
The response signal selection module inserts small backchannel cues (such as "uh-huh" or "right") at appropriate times to simulate natural human conversation.
By analyzing a large volume of real dialogues, this module extracts and trains suitable response signals for various conversation scenarios.
Using multi-label classification, the system selects the optimal response for each dialogue context, significantly reducing user waiting time and enhancing conversation flow.
The interruption detection module flexibly responds to user interruptions.
Unlike traditional rule-based detection methods, this system builds an end-to-end detection model with multimodal input (audio and text) that not only identifies genuine user interruptions but also avoids misinterpreting background noise or unintended voice signals as interruptions.

- **Outbound Agent System.** [^144] proposed a full-duplex dialogue scheme for outbound systems, focusing on the issues of conversational fluidity and timing of interaction in speech dialogue.
This scheme uses semantic analysis to determine whether the user truly intends to interrupt the system and can handle disjointed expressions when users mention named entities.
The core of this system is a full-duplex interaction finite-state machine (FSM), which retrieves text snippets from ASR results every 300 milliseconds to decide whether to interrupt.
Through continuous semantic analysis of user speech, the interruption model identifies meaningful user interruptions and avoids frequent interruptions caused by brief, meaningless responses (like "uh-huh").
The model employs a pre-trained BERT-based text classifier and utilizes streaming input, ensuring that the system can process and analyze user speech in real-time as it is received.
Additionally, the system includes a Discontinuous Expression module to handle user pauses when mentioning named entities.
Specifically, when users hesitate over entities (such as numbers, locations, or company names), VAD may erroneously detect turn-end.

The advent of Large Language Models  has significantly advanced generative AI development.
Models like ChatGPT demonstrate strong capabilities in semantic understanding and logical reasoning, offering a simplified method to integrate various dialogue components into a unified framework, which may simplify SDS construction.
GPT-4o represents a milestone for dialogue systems, showcasing a nearly human-like conversational voice model.
Its flexible interaction style and interruption mechanisms make human-computer interaction more natural and fluid.
However, as a commercial model, its training data and implementation details remain proprietary, making replication challenging.

- **Full-duplex LLM.** [^130] proposed a full-duplex spoken dialogue models based on LLMs, enabling simultaneous reception and transmission of voice signals through a perception module, an action module, and a neural finite-state machine (FSM).
The perception module uses a streaming ASR model, capturing and processing user speech in real-time with 640-millisecond intervals per time step, converting it into token inputs for the LLM.
The action module, utilizing a streaming TTS model, instantly converts the LLM-generated text into audio output and can pause or resume playback as needed, ensuring the system can generate audio while receiving user input.
At the core is the neural FSM, allowing the LLM to switch between "speaking" and "listening" states.
Controlled by FSM signals, the system can dynamically decide to continue speaking, listen, or interrupt based on the dialogue context.
Experimental results show that Wang et al.'s full-duplex streaming system reduces response latency by threefold, achieves a response time within 500 milliseconds in over 50\
- **VITA.** VITA is an open-source multimodal large language model which aimed at enhancing multimodal interaction experiences.
VITA can process multiple modalities, such as video, image, text, and audio, and achieves fluid human-computer interaction through a new duplex architecture involving two simultaneously operating models: one for generating responses to user queries, and another for continuously monitoring environmental inputs.
When a new user query is detected, the generation model pauses, and the monitoring model processes the new query and generates an updated response.
This setup enables VITA to support audio interruption, allowing users to ask new questions during system generation, with the system immediately pausing the current response to handle new input.
VITA’s perception abilities are achieved through multimodal alignment and instruction fine-tuning, enabling it to switch automatically between different inputs.
Additionally, VITA employs state tokens to distinguish user input types, such as query audio, background noise, and text input, facilitating wake-free interaction.
VITA's enhanced listening module prevents unnecessary user feedback from interrupting system responses, improving robustness.

- **CleanS2S.**[^145]
This model employs a structured pipeline to enable responsive and flexible interactions in a spoken dialogue setting.
Designed to facilitate seamless turn-taking and interruption handling, the model consists of several interconnected modules working in a coordinated sequence to optimize user experience.
Starting with user input, the system uses a Voice Activity Detection (VAD) module to continuously monitor for incoming audio signals.
As soon as a user starts speaking, VAD captures the input and immediately initiates processing by sending the audio data to the Automatic Speech Recognition (ASR) module.
This quick detection and response setup allows the system to react to user input without delay.
Once ASR transcribes the audio into text, the transcription is passed to the Large Language Model (LLM), which generates a relevant response based on the user’s query.
Meanwhile, the model is designed to be interruption-aware.
During response generation, if VAD detects a new user input (indicating an interruption or a follow-up query), the system can promptly adjust its processing flow.
In this case, the LLM temporarily pauses its current task, allowing ASR to transcribe the new input, which the LLM then uses to generate an updated response.
This interruption capability is achieved through the model’s layered processing design, allowing for adaptive turn-taking that feels natural and responsive.
The Text-to-Speech (TTS) module then converts the generated text response into audio, which is transmitted to the user via WebSocket.
To further support interruption handling, TTS breaks down lengthy responses into smaller audio segments that are sent progressively.
This segmentation allows the system to stop audio output instantly if an interruption occurs, switching to the new input without delay.
Each segment is prepared and sent only after a brief VAD check, ensuring that the system is ready to pause and handle new input at any time.
This interconnected processing chain—VAD detecting input, ASR transcribing, LLM generating responses, and TTS outputting segmented audio—creates a duplex interaction framework that balances response generation and user-driven interruptions.
By seamlessly coordinating these components, the model provides a fluid, real-time dialogue experience that adapts to user interactions dynamically.

**End-to-End Systems.**
In contrast, end-to-end spoken dialogue models do not require explicit modeling of dialogue turns; instead, they learn interaction modeling directly from training data.
Next, we introduce several representative works on interaction in end-to-end systems.

- **dGSLM.** In end-to-end systems, the introduction of the dGSLM model marks a significant milestone in full-duplex technology development.
Within the dGSLM framework, duplex technology is effectively implemented.
This model demonstrates how to capture complex interactions within dialogues directly from raw audio data through generative spoken dialogue modeling, without relying on text.
The core innovation of dGSLM is the dual-tower Transformer architecture, called the Dialogue Transformer Language Model (DLM), which uses a cross-attention mechanism to enable the system to process two parallel audio channels simultaneously.
Through this architecture, the model not only independently generates speech for each channel but also shares information between channels using cross-attention, effectively modeling silences and interaction events.
It leverages the HuBERT encoder and HiFi-GAN decoder, combined with the dual-tower DLM, and is trained on 2,000 hours of dual-channel telephone conversation audio (Fisher dataset), where each speaker in a conversation is allocated an independent audio track.
The dGSLM model transforms the audio on both channels into discrete tokens using HuBERT, and the DLM model autoregressively predicts the next audio token and its duration.
Finally, the HiFi-GAN[^086] decoder reconstructs the audio for both channels.
This approach differs significantly from traditional text-dependent spoken dialogue models, with a particular emphasis on modeling turn-taking and backchanneling capabilities.
This capability gives dGSLM a notable advantage in duplex voice interaction, better mimicking the natural dynamics of human conversation.
Through its duplex model design, dGSLM represents an essential step forward in interactive capabilities and provides a foundation for further advancements.

- **Moshi.** As a novel full-duplex architecture, Moshi incorporates a rich array of design concepts.
Unlike dGSLM, Moshi does not abandon the language model’s ability in text dialogue.
Moshi’s architecture is based on the Helium language model and Mimi neural audio codec, both trained from scratch.
Helium, as a large pre-trained text language model, provides strong reasoning capabilities, while Mimi handles audio signal encoding and decoding.
To achieve real-time interaction, Moshi is designed as a multi-stream architecture, simultaneously processing "user" and "moshi" audio streams without explicitly modeling speaker turns.
Moshi also introduces the "Inner Monologue" method within the "moshi" audio stream, a process that jointly models text and audio tokens during training and inference.
This approach allows the model to fully utilize textual knowledge while maintaining speech-to-speech system characteristics, significantly enhancing generation quality.
Mimi, a neural audio codec integrating semantic and acoustic information through residual vector quantization and knowledge distillation, captures high-quality user input audio and Moshi’s output voice efficiently.
To jointly model Moshi and user audio streams alongside Moshi’s text tokens, Depth Transformer with streaming inference capabilities is employed.
The Mimi encoder and decoder combine convolutional and Transformer layers, with causal convolutions, allowing for streaming operation.
Moshi is pre-trained on unsupervised audio data to handle speech scenarios and then fine-tuned on the Fisher dataset to address overlapping speech and interruptions.
Finally, the system is further optimized on a custom instruction-tuning dataset, ensuring robust performance across various interactive scenarios.
Experimental results show that Moshi excels in speech modeling and spoken QA tasks, especially in latency, achieving a theoretical latency of 160 milliseconds and 200 milliseconds in practice, significantly lower than the typical 230 milliseconds in natural conversation, enhancing real-time interaction and conversation flow.

- **Parrot.** Parrot[^146] model incorporates multiple features specifically designed to enhance interaction in spoken dialogue.
It uses a dual-channel audio setup, where each channel represents a different speaker.
This configuration allows Parrot to manage both sides of a conversation independently, facilitating real-time turn-taking.
By distinguishing between the user’s input and the system’s response on separate channels, the model can listen and respond in parallel, creating a more natural conversational flow.
To handle simultaneous speaker inputs effectively, Parrot employs a "next-token-pair prediction" mechanism, allowing it to predict tokens for both channels in a coordinated sequence.
This approach helps the model manage conversational dynamics such as overlapping speech and smooth transitions between turns, adjusting response timing based on the user’s input.
During inference, Parrot supports streaming input, enabling continuous processing of user audio on one channel while generating responses on the other.
This streaming capability allows the model to respond to live spoken input in real-time, handling turn-taking, pauses, and interruptions dynamically.
Unlike cascaded systems that rely on intermediate text conversions, Parrot processes audio directly, reducing latency and allowing immediate responses to spoken input.
These interaction-focused design choices make Parrot highly responsive, enabling it to manage turn-taking naturally, respond to interruptions, and handle overlapping speech,

- **Mini-Omni2.** Mini-Omni2 is an open-source multimodal large language model aimed at simulating the multimodal capabilities of GPT-4o in vision, hearing, and text, supporting real-time full-duplex interaction.
Mini-Omni2 combines visual and audio encoders with a language model to enable simultaneous input and output of images, audio, and text.
The model incorporates an interrupt mechanism based on instruction design for more flexible user interactions.
This system uses a delayed parallel generation algorithm, allowing the model to generate text and audio responses simultaneously, greatly improving conversational real-time capabilities and response speed.
To achieve full-duplex interaction, Mini-Omni2 introduces an interrupt mechanism based on a limited instruction approach, trained on a specially constructed dataset with specific irq (interrupt) and n-irq (non-interrupt) state markers for model optimization.
For training Mini-Omni2’s interruption functionality, the researchers used noisy speech data synthesized with specific command phrases (such as "Stop Omni") in various voices and tones to simulate scenarios where users might issue interrupt commands.
The dataset also includes background noises, such as environmental sounds, music, and other dialogues, enhancing the model’s robustness in complex environments.
During training, Mini-Omni2 controls output flow through irq and n-irq state markers, generating these markers in real-time to determine whether to continue output.
In this way, the model can immediately halt generation based on user instructions and switch to "listening" mode in real-time dialogue.
The training data consists of long audio streams from which the model extracts and encodes user commands like "Stop Omni." Researchers inserted interrupt commands at various time points, marking data after the insertion point as irq (interrupt) and data before as n-irq (non-interrupt).
This labeling method ensures that the model learns to accurately identify interrupt commands in complex audio inputs and respond appropriately.

- **SyncLLM.** SyncLLM achieves full-duplex dialogue and interruption capabilities through multi-stream interleaving and chunk processing.
SyncLLM divides the conversation's audio stream into fixed-sized chunks, each corresponding to a specific time interval.
The model alternates between generating user and system speech segments within each time step (chunk), ensuring real-time system responses while processing user speech input.
To maintain temporal synchronization with the user, SyncLLM predicts the user’s speech at each time step before generating each system chunk, using it as context to infer the system’s next response.
This mechanism enables the system to keep pace with the conversation even with network latency.
The chunk method allows SyncLLM to handle both user and system audio streams simultaneously, supporting complex dialogue features like speech overlap, interruption, and real-time feedback.
Additionally, by using de-duplicated speech token sequences and periodic synchronization markers, the model efficiently performs chunk-level real-time inference, making conversation more fluid and natural.

- **OmniFlatten.** Similar to SyncLLM, the OmniFlatten model achieves full-duplex and interruption functionality primarily through multi-stream data processing and progressive training.
To enable full-duplex dialogue, the model adopts a multi-stream architecture that interleaves the user’s speech stream with the assistant’s speech and text streams into a single sequence for training, simplifying multimodal modeling and enhancing real-time capability.
The model first aligns the text language model with modality through multitask supervised fine-tuning, enabling it to understand and generate both speech and text, ensuring basic capability for handling speech and text simultaneously.
Through a progressive training process, OmniFlatten attains full-duplex capability in three stages: initial training for half-duplex dialogue, then removing the user’s text stream to support real-time prediction with multi-stream data, and finally removing the assistant’s text stream to enable pure speech stream generation.
These steps reduce reliance on text and decrease latency, allowing the system to generate voice responses while receiving user speech input.
By using a block-by-block generation strategy, OmniFlatten divides the input and output speech sequences into fixed-size blocks, processing each segment in turn.
This effectively implements streaming processing, ensuring low latency and high responsiveness in full-duplex dialogue, thereby providing a more natural response to user interruptions.

- **Freeze-Omni.** To support duplex dialogue, Freeze-Omni[^082] uses a chunk-level state prediction mechanism for natural turn-taking.
When the user begins speaking, a voice activity detection module identifies the audio input, prompting the model to process the audio chunk by chunk.
After processing each chunk, the model's classification layer predicts the conversation state to determine the next action.
There are three possible states: State 0, where the model continues listening for more input, assuming the user hasn’t completed their turn; State 1, where the model interrupts to provide an immediate response if a quick acknowledgment or feedback is needed; and State 2, where the model has completed processing the current user input and is ready to generate and output a response, thus transitioning smoothly into the response phase without further listening.
This chunk-wise state prediction enables the model to decide effectively when to respond and when to continue listening, enhancing its ability to handle natural conversational cues and support interactive dialogue.

#### Discussions about streaming and interaction

Significant progress has been made in dialogues models, particularly in real-time interaction and semantic understanding, with notable achievements in streaming processing and full-duplex interaction.
Current systems exhibit strong technical capabilities in reducing response latency, enhancing interruption handling, and improving the naturalness of conversation.
However, existing spoken dialogues models still lack a unified system that can handle all forms of interaction seamlessly.
Future research could explore new frameworks to better manage both user interruptions and the system’s ability to interrupt users, making interactions more natural.
Additionally, standardized benchmarks for evaluating interaction capabilities remain underdeveloped.
A unified evaluation benchmark would provide a consistent method for assessing and comparing the performance of different models, thereby advancing the development of more intelligent and responsive interaction systems.

## Training Resources and Evaluation

### Training resources

Training a spoken dialogue system is a complex, multi-stage process, with each stage relying on specific datasets to achieve distinct training objectives and enhance system performance.
This section provides an in-depth analysis of the training resources about the spoken dialogue models, showcasing the data collection and processing methods at each stage and illustrating how these elements contribute to the system's intelligence.
It further reveals how key steps, from foundational architecture to fine-tuning, shape the intelligent development of dialogue systems.

To address the limitations of existing training spoken dialogue data and leverage the knowledge and reasoning capabilities of mature text-based models, many approaches involve \textit{Continue Training} on pre-trained text language models.
This training paradigm encompasses nearly all data types required to build a spoken dialogue system.
The following sections focus on analyzing data acquisition and processing methods under this training flow, covering the following core stages: \textit{Text Language Model Pre-training}, \textit{Post-Train for Audio Modal Adaption}, \textit{Post-Train for Dual-Stream Audio Processing}, \textit{Enhancing Conversational Abilities and Instruction Tuning}.
We have listed commonly used datasets for training in Table \ref{traindataset}.
However, current spoken dialogue models lack exploration in music and sound.
To support future development in spoken dialogue systems, we provide a list of common music and sound datasets in the appendix \ref{music datasets} as a reference.

#### Training resources about Text LLM Pre-training

Text Language Model pre-training serves as the foundational stage for spoken dialogue models.
Through unsupervised learning on large-scale text data, the model acquires knowledge of vocabulary, grammar, and contextual relationships, gaining essential knowledge and reasoning capabilities.
Most spoken dialogue systems are built upon pre-existing open-source text language models (such as Llama[^024], Palm[^147], etc).
Although we does not delve into this stage in detail, it provides a solid foundation for the model’s natural language understanding and generation capabilities.

Modal Alignment
& Mandarin ASR    & AISHELL-1[^148]      & 170 hrs   & \url{https://www.openslr.org/33/}       & Text, Speech \\
& Mandarin ASR    & AISHELL-2[^149]      & 1k hrs   & \url{https://github.com/kaldi-asr/kaldi/tree/master/egs/aishell2}       & Text, Speech \\
& Mandarin TTS    & AISHELL-3[^150]      & 85 hrs, 88,035 utt., 218 spk.   & \url{https://www.aishelltech.com/aishell_3}       & Text, Speech \\
& TTS             & LibriTTS[^151]       & 585 hrs                         & \url{https://www.openslr.org/60/}                 & Text, Speech \\
& ASR             & TED-LIUM[^152]       & 452 hrs                         & \url{https://lium.univ-lemans.fr/ted-lium3/}      & Text, Speech \\
& ASR             & VoxPopuli[^153]      & 1.8k hrs                        & \url{https://github.com/facebookresearch/voxpopuli} & Text, Speech \\
& ASR             & Librispeech[^154]    & 1,000 hrs                       & \url{https://www.openslr.org/12}                  & Text, Speech \\
& ASR             & MLS[^155]            & 44.5k hrs                       & \url{https://www.openslr.org/}                    & Text, Speech \\
& TTS             & Wenetspeech[^156]    & 22.4k hrs                       & \url{https://wenet.org.cn/WenetSpeech/}           & Text, Speech \\
& ASR             & Gigaspeech[^157]     & 40k hrs                         & \url{https://github.com/SpeechColab/GigaSpeech}   & Text, Speech \\
& ASR             & VCTK[^158]           & 300 hrs                         & \url{https://paperswithcode.com/dataset/voice-bank-demand} & Text, Speech \\
& TTS             & LJSpeech[^159]       & 24 hrs                          & \url{https://keithito.com/LJ-Speech-Dataset/}     & Text, Speech \\
& ASR             & Common Voice[^160]   & 2,500 hrs                       & \url{https://commonvoice.mozilla.org/zh-CN}       & Text, Speech \\
& Audio Caption   & Wavcaps[^161]        & 400k clips                      & \url{https://github.com/XinhaoMei/WavCaps}        & Text, Speech \\
& ASR             & LibriLight[^162]     & 60k hrs                         & \url{https://github.com/facebookresearch/libri-light} & Text, Speech \\
& ASR             & PeopleSpeech[^163]     & 30k hrs                         & \url{https://huggingface.co/datasets/MLCommons/peoples_speech} & Text, Speech \\
& Mandarin ASR             & KeSpeech[^164]     & 1,542 hrs                         & \url{https://github.com/KeSpeech/KeSpeech} & Text, Speech \\
& TTS            & Emilia[^165]     & 	101k hrs                        & \url{https://huggingface.co/datasets/amphion/Emilia-Dataset} & Text, Speech \\

Dual-Stream Processing
& Instruction     & Alpaca[^166]         & 52,000 items                    & \url{https://huggingface.co/datasets/tatsu-lab/alpaca} & Text + TTS \\
& Instruction     & Moss           & -                               & \url{https://huggingface.co/fnlp/moss-moon-MiniOmni2-sft} & Text + TTS \\
& Instruction     & BelleCN        & -                               & \url{https://github.com/LianjiaTech/BELLE/tree/main} & Text + TTS \\
& Dialogue        & UltraChat[^167]      & 1.5 million                     & \url{https://github.com/thunlp/UltraChat}          & Text + TTS \\
& Instruction     & Open-Orca[^168]      & -                               & \url{https://huggingface.co/datasets/Open-Orca/OpenOrca} & Text + TTS \\
& Noise          & DNS[^169] & 2425 hrs                       & \url{https://github.com/microsoft/DNS-Challenge} & Noise  data \\
& Noise           & MUSAN [^170]         & -                               & \url{https://www.openslr.org/17/}                  & Noise data\\ \hline

Conversation Fine-Tune
& Dialogue        & Fisher         & 964 hrs                         & \url{https://catalog.ldc.upenn.edu/LDC2LLaMA3T19}     & Text, Speech \\
& Dialogue        & GPT-Talker[^171]     & -                               & \url{https://github.com/AI-S2-Lab/GPT-Talker}      & Text, Speech \\
& Instruction     & INSTRUCTS2S-200K & 200k items                    & \url{https://github.com/ictnlp/LLaMA-Omni}         & Text + TTS \\
& Instruction     & Open Hermes    & 900k items                      & \url{https://ollama.com/library/openhermes}        & Text + TTS \\ \hline

#### Training resources about Post-Train for Audio Modal Alignment

After establishing a text-based foundational model, the system possesses essential knowledge and reasoning abilities.
In this stage, we introduce the audio modality, enabling the text language model to understand and generate speech while minimizing any potential loss of textual knowledge.
This process is known as \textit{modal adaption} or \textit{modal alignment}.
This multimodal structure incorporates an audio encoder with a codebook, helping the model recognize linguistic, emotional, and tonal information in speech.
The audio decoder supports the generation of natural and fluent speech output, while audio signal embeddings and special token types (e.g., speaker-distinguishing tokens for Synchronous LLM, task-distinguishing tokens for OmniFlatten, and state tokens for VITA) are added to the vocabulary of the text language model.

The primary goal at this stage is to align information from different modalities into a unified space or representation, allowing the model to correlate and comprehend such information.
Consequently, the model is often trained on cross-modal tasks such as TTS , ASR , and audio captioning.
The datasets used include numerous paired audio and text samples to ensure effective conversion between modalities.
Commonly used TTS and ASR datasets include Aishell-3[^150], LibriTTS[^151], TED-LIUM[^152], VoxPopuli[^153], Librispeech [^154], MLS[^155], Wenetspeech[^156], Gigaspeech[^157], VCTK[^158], LJSpeech[^159], Common Voice[^160], and others.
For audio captioning, Wavcaps[^161] are frequently used.
Some speech datasets require ASR model transcription to generate corresponding text.

In this phase, the emphasis is placed on capturing and generating audio features and aligning them with text in vector space, rather than focusing on dialogue functionality.Therefore, the data typically consists of single-channel audio, which can be used after resampling.
Notably, in some works, it is essential to ensure word-level alignment between text tokens and audio tokens (e.g., Spirit-LM, Moshi, and OmniFlatten), achievable through tools like the Whisper-timestamped package or other alignment tool.
In Moshi, to prevent catastrophic forgetting, half of the training time is allocated to text data, highlighting the importance of balancing text and audio data during training.

\caption{Datasets used in the various training stages}

**Stage** & **Task** & **Dataset** & **Size** & **URL** & **Modality**

Modal Alignment
& Mandarin ASR    & AISHELL-1[^148]      & 170 hrs   & [URL](https://www.openslr.org/33/)       & Text, Speech \\
& Mandarin ASR    & AISHELL-2[^149]      & 1k hrs   & [URL](https://github.com/kaldi-asr/kaldi/tree/master/egs/aishell2)       & Text, Speech \\
& Mandarin TTS    & AISHELL-3[^150]      & 85 hrs, 88,035 utt., 218 spk.   & [URL](https://www.aishelltech.com/aishell_3)       & Text, Speech \\
& TTS             & LibriTTS[^151]       & 585 hrs                         & [URL](https://www.openslr.org/60/)                 & Text, Speech \\
& ASR             & TED-LIUM[^152]       & 452 hrs                         & [URL](https://lium.univ-lemans.fr/ted-lium3/)      & Text, Speech \\
& ASR             & VoxPopuli[^153]      & 1.8k hrs                        & [URL](https://github.com/facebookresearch/voxpopuli) & Text, Speech \\
& ASR             & Librispeech[^154]    & 1,000 hrs                       & [URL](https://www.openslr.org/12)                  & Text, Speech \\
& ASR             & MLS[^155]            & 44.5k hrs                       & [URL](https://www.openslr.org/)                    & Text, Speech \\
& TTS             & Wenetspeech[^156]    & 22.4k hrs                       & [URL](https://wenet.org.cn/WenetSpeech/)           & Text, Speech \\
& ASR             & Gigaspeech[^157]     & 40k hrs                         & [URL](https://github.com/SpeechColab/GigaSpeech)   & Text, Speech \\
& ASR             & VCTK[^158]           & 300 hrs                         & [URL](https://paperswithcode.com/dataset/voice-bank-demand) & Text, Speech \\
& TTS             & LJSpeech[^159]       & 24 hrs                          & [URL](https://keithito.com/LJ-Speech-Dataset/)     & Text, Speech \\
& ASR             & Common Voice[^160]   & 2,500 hrs                       & [URL](https://commonvoice.mozilla.org/zh-CN)       & Text, Speech \\
& Audio Caption   & Wavcaps[^161]        & 400k clips                      & [URL](https://github.com/XinhaoMei/WavCaps)        & Text, Speech \\
& ASR             & LibriLight[^162]     & 60k hrs                         & [URL](https://github.com/facebookresearch/libri-light) & Text, Speech \\
& ASR             & PeopleSpeech[^163]     & 30k hrs                         & [URL](https://huggingface.co/datasets/MLCommons/peoples_speech) & Text, Speech \\
& Mandarin ASR             & KeSpeech[^164]     & 1,542 hrs                         & [URL](https://github.com/KeSpeech/KeSpeech) & Text, Speech \\
& TTS            & Emilia[^165]     & 	101k hrs                        & [URL](https://huggingface.co/datasets/amphion/Emilia-Dataset) & Text, Speech \\
\hline

Dual-Stream Processing
& Instruction     & Alpaca[^166]         & 52,000 items                    & [URL](https://huggingface.co/datasets/tatsu-lab/alpaca) & Text + TTS \\
& Instruction     & Moss           & -                               & [URL](https://huggingface.co/fnlp/moss-moon-MiniOmni2-sft) & Text + TTS \\
& Instruction     & BelleCN        & -                               & [URL](https://github.com/LianjiaTech/BELLE/tree/main) & Text + TTS \\
& Dialogue        & UltraChat[^167]      & 1.5 million                     & [URL](https://github.com/thunlp/UltraChat)          & Text + TTS \\
& Instruction     & Open-Orca[^168]      & -                               & [URL](https://huggingface.co/datasets/Open-Orca/OpenOrca) & Text + TTS \\
& Noise          & DNS[^169] & 2425 hrs                       & [URL](https://github.com/microsoft/DNS-Challenge) & Noise  data \\
& Noise           & MUSAN [^170]         & -                               & [URL](https://www.openslr.org/17/)                  & Noise data\\ \hline

Conversation Fine-Tune
& Dialogue        & Fisher         & 964 hrs                         & [URL](https://catalog.ldc.upenn.edu/LDC2LLaMA3T19)     & Text, Speech \\
& Dialogue        & GPT-Talker[^171]     & -                               & [URL](https://github.com/AI-S2-Lab/GPT-Talker)      & Text, Speech \\
& Instruction     & INSTRUCTS2S-200K & 200k items                    & [URL](https://github.com/ictnlp/LLaMA-Omni)         & Text + TTS \\
& Instruction     & Open Hermes    & 900k items                      & [URL](https://ollama.com/library/openhermes)        & Text + TTS \\ \hline

#### Training resources about Post-Train for Dual-Stream Dialogue Processing

To ensure that the model possesses the ability to “listen while speaking”.
Most research such as **Moshi**[^Moshi] and OmniFlatten[^OmniFlatten] has implemented a dual audio-stream model: one audio stream generates model output, while the other captures user audio.
The objective of this training phase is to enable the model’s dual-stream processing without requiring complex human-computer interaction modeling.
Consequently, text dialogue data can be converted to speech and processed into dual-track audio format.
However, text dialogue data typically contains content unsuitable for TTS conversion to speech (such as code, formulas, URLs) or long, formal dialogue passages that do not align with spoken language, as real dialogue is often more concise.
Therefore, when synthesizing from text dialogue data, it is necessary to preprocess the text data.
High-quality, open-source text dialogue data is first collected, including datasets like Alpaca[^166], Moss, BelleCN, ultraChat[^167], and Open-Orca[^168].
To ensure suitability for speech synthesis (TTS), heuristic rules are applied to filter out samples with high proportions of non-text elements (such as code, mathematical expressions), samples exceeding 200 words, and samples containing rare symbols.

After filtering the text, TTS models[^045] are used to synthesize speech for each turn in the dialogues.
For consistent voice effects, the model audio stream maintains a uniform voice, while the user audio stream is sampled with varied voices to enhance the model's robustness.
The synthesized dialogue audio is arranged using simulation strategies to achieve natural timing, such as turn-taking, well-timed interruptions, and pauses to maintain fluency and naturalness.
The final dialogue audio is organized in dual-channel format: the conversation begins with a user utterance, followed by alternating user and assistant turns.
After each user turn, the assistant responds immediately; upon completion of the assistant’s turn, a sampled pause length is introduced to simulate the natural rhythm of alternating dialogue.
To better simulate real scenarios, further data augmentation can be applied.
For example, random gain adjustments can be applied to the user audio stream, and background noise randomly selected from datasets like MUSAN[^170] and DNS[^169] can be added to the user audio channel (OmniFlatten).
To simulate echo effects from a user’s microphone, portions of the audio stream can be scaled down and added to the user’s audio stream with random delays between 100 to 500 milliseconds, along with reverberation-like enhancements, helping the model adapt to real-world environments.

#### Training resources about Enhancing Conversational Abilities and Instruction Tuning

While the foundational model has been established, there remains a gap between this and a complete dialogue system.
The above model utilizes non-overlapping dialogue audio, where one party remains silent while the other speaks, failing to fully simulate real conversational dynamics.
Some speech datasets, such as \textit{Generative Expressive Conversational Speech Synthesis}[^171] and \textit{Fisher}, contain dialogues from real-world settings, providing a basis for modeling interruptions and backchannels scenarios in voice dialogue systems.

Currently, there is no suitable dataset for real-world speech instructions.
Most approaches use synthetic methods based on text instruction data to perform \textit{instruction tuning} in this stage.
Common text instruction datasets include \textit{Open Hermes} and \textit{moss-SpeechGPT-sft-data}, though they face similar challenges as text dialogue data, such as unsuitability for TTS conversion and inconsistency with spoken language conventions.
Following the synthetic processes provided by Moshi and Llama-Omni, this aims to generate instruction data in the format of (SpeechInstruction, TextInstruction, TextResponse, SpeechResponse).

The first method is synthetic generation from scratch.
Contexts and summaries are first generated by sourcing high-quality text data from sources like Wikipedia and StackExchange, producing thematic paragraphs as the dialogue foundation, referred to as “context.” Based on these contexts, dialogue summaries are generated.
Next, a specific prompt template guides the generation of complete dialogues, including context and requesting dialogues around the theme with roles as user and system.
The model is prompted to exhibit knowledge on the topic and include interruptions (backchannels) and brief turn-taking, simulating the natural flow of conversation.
To enhance dialogue diversity, additional instructions involving speech emotion and role-playing can be generated, requesting dialogues in specific tones or styles.
Furthermore, dialogues containing spelling errors or misinformation are synthesized to train the system in handling scenarios where user clarification or repetition is required.
Single-turn interactions on basic mathematics, grammar, and factual questions are also generated to ensure the system can handle simple factual tasks.
Finally, scenarios involving ethical or NSFW requests are created to train the system in declining to answer under such conditions.

The second method involves filtering and refining existing text instruction datasets.
Initially, open-source text language models paraphrase text instructions to match spoken language traits, adding fillers like “uh” and “um” to mimic natural speech tone, while converting numbers and symbols into spoken language to ensure the instructions are concise and conversational.
Generated text responses are also optimized to meet TTS output requirements, removing lengthy expressions and complex grammatical structures to make content clear and concise for TTS output.
After adjusting the instruction and response text, a TTS system converts the text to audio.

### Evaluation

Fair and comprehensive evaluation of spoken dialogue models presents a multifaceted challenge.
On the one hand, the field of spoken dialogue still lacks publicly available test sets, comprehensive evaluation metrics, and established benchmarks.
On the other hand, assessing the performance of spoken dialogue systems requires consideration from multiple perspectives.
Basic aspects include the quality of generated speech, robustness, dialogue naturalness and accuracy, as well as response speed and generation time.
Beyond these, more advanced evaluations are needed to assess multi-turn dialogue capabilities (such as long-form speech editing), interaction abilities, and the system's proficiency in audio and music understanding and generation.
Given these requirements, and in line with the comprehensive expectations for spoken dialogue systems outlined in Section \ref{section21}, we will evaluate these systems from two angles: common evaluations and advanced evaluations.
Specifically, we will assess eleven key factors: speech generation quality, text intelligence, speech intelligence, audio and music generation, audio and music understanding, multilingual capability, context learning, interaction capability, streaming latency, multimodal capability, and the safety of dialogue systems.
Finally, we will list the current benchmarks and summarize the common conclusions derived from them.

#### Common Evaluation

**Text Intelligence.** As shown in Figure \ref{fig:main} (a), text intelligence refers to the fundamental understanding and generation capabilities of the spoken dialogue model.
When evaluating text intelligence, the focus is solely on the semantic content generated by the model, without considering other aspects such as timbre, emotion, or style.
In practical evaluations of this kind, some spoken dialogue models output only text[^172], [^SALMONN], [^QwenAudio], [^Qwen2Audio], [^E-chat], while others generate both text and speech **Moshi**[^Moshi], [^MiniOmni],**Mini-Omni2**[^MiniOmni2], or only speech[^OmniFlatten].
Regardless of the output format, we are concerned only with the generated text or the transcribed text from the speech when evaluation the text intelligence in the spoken dialogue models.
There are typically two categories of metrics and benchmarks used to assess text intelligence, MT-Metrics and Acc-Metrics.
The details are outlined as follows:

- **ACC-Metrics.** A common approach to evaluating text intelligence is to use benchmarks typically[^173], [^174], [^175], [^176], [^177], [^178], [^179], [^180], [^181], [^182] employed for large language models, such as the classic MMLU[^183] and GSM-8K [^184].
These benchmarks often include complex multiple-choice questions, which assess the model's reasoning abilities through Acc-Metrics.
Acc-Metrics refers to metrics that measure recognition accuracy, such as accuracy, F-score, and Mean Average Precision (mAP).
It is noteworthy that these benchmarks often evaluate the text-based intelligence of spoken dialogue models from various perspectives.
For example, MMLU [^183] and GSM-8K [^184] are more focused on LLM's core knowledge, Flan [^185], [^186] and Self-instruct [^187] are more focused on LLM's instruction following capability, CoQA [^188] and OpenAssistant [^189] are more focused on LLM's conversational capability.
These benchmarks often contain questions and corresponding answers.
Most of these questions are close-ended questions with short answers, so that they can have good generalization ability, any model that can generate text answers can be evaluated with these benchmarks and accuracy and F-Score can be easily adopted as the evaluation metrics.

- **MT-Metrics.**  With the development of the LLMs, LLMs can follow instructions to accomplish many complex problems, so the scope of the evaluation was further expanded to include open-ended questions.
These open-ended questions often lack standard answers, therefore it's difficult to measure them by common ACC-Metrics.
A common approach is to measure the grammatical similarity between generated and reference utterances using the metrics used to measure grammatical similarity in mechanical translation (e.g.
BLEU [^190], METEOR [^191], ROUGE [^192]).
We collectively refer to these evaluation metrics as **MT-Metrics**.
However, these metrics have certain limitations since one meaning has many different ways to convey.
So there are some metrics like BertScore [^193] focus on evaluating the semantic similarity between two sentences.
And there are also been some methods utilizing LLM to judge the effectiveness of the responses which focusing on human preference [^194], [^195].
The results of these large model-based especially GPT4o-based ratings of evaluation metrics demonstrated a high degree of correlation with human.

**Speech Quality.** Speech quality is one of the fundamental aspects for evaluating the performance of spoken dialogue systems, as it is closely tied to the experience of users.
There are two common dimensions for assessing speech quality: the clarity and naturalness (expressiveness and prosody) of the generated audio, and the robustness of the generated speech, such as the presence of missing or extra words.
The former is typically evaluated by using subjective MOS (Mean Opinion Score) ratings, while the latter is commonly assessed by using WER (Word Error Rate) or CER (Character Error Rate) metrics.

**Streaming Latency.**
In addition to evaluating the quality of text understanding and generated speech, the speed at which a spoken dialogue system generates speech responses is also crucial.
This necessitates the ability to stream both the comprehension and generation of speech in real time, achieving an effect of generating speech while speaking[^IntrinsicVoice], **Moshi**[^Moshi], [^032].
To assess the streaming performance of a model, one typically measures the time taken to generate the first token of speech (i.e., the waiting time after the user finishes speaking) and calculates the overall Real-Time Factor (RTF) of the spoken dialogue model's response.
The RTF value is obtained by dividing the total duration of the speech segment generated by the model by the time taken by the model to generate that response.

#### Advanced Evaluation

**Speech Intelligence.**
Evaluating the speech intelligence of spoken dialogue systems is one of the key aspects.
The definition of speech intelligence in spoken dialogue systems is discussed in detail in Section \ref{section212}.
Given that speech intelligence encompasses a wide range of application scenarios, we address the evaluation separately for the understanding and generation components during the assessment.

- **Understanding.** Ordinary cascaded spoken dialog models based on ASR getting text input will loss many paralinguistic information like speaking style, accent, emotion, etc.
Thus many spoken dialogue models [^E-chat], [^ParalinGPT], [^SpokenLLM] devoted into helping dialog models understand the paralinguistic information.
Evaluating this capability can start from two aspects: a) the accuracy of the paralinguistic information's understanding, b) the ability of **automatically** generating appropriate and coherent content responses and acoustic information based on the varying acoustic input.
**For the former**, since the classes of the paralinguistic information are always limited, for example, sentiments are generally categorized as neural, negative, positive.
So the researchers always use Accuracy or F-Score to evaluate the models' paralinguistic information understanding capability.
Recently, there are many studies [^196], [^197], [^198], [^E-chat], [^SpokenLLM], [^199], [^200] available for researchers to use in identifying speech emotions in the dialogue scenes.
In addition to recognizing speech emotions, recent benchmarks [^201], [^202] has also begun to investigate the influence of speaker age, accent, and other factors on the evaluation of spoken dialogue models.
**For the latter**, recent work[^E-chat] has increasingly focused on the possibility of generating appropriate content responses based on acoustic information from the input.
The current evaluation methods usually transcript the output audio into text through Automatic Speech Recognition and then evaluate the relevance between generated content and the reference content in the internal dataset.
Evaluations are usually conducted in text, so commonly used evaluation metrics are as the same as in the section \ref{eval_text}, like BLEU and METEOR, which are used to measure the similarity between two sentences.
Currently, there is limited research exploring whether spoken dialogue models can autonomously generate appropriate acoustic responses based on varying acoustic information, making it a promising area for future investigation.

- **Generation.** In the generation component, evaluating the speech intelligence of spoken dialogue systems primarily focuses on controllability, i.e., the ability of the dialogue model to respond in a user-specified style and timbre in the zero-shot scenarios.
There are various dimensions to assess style, such as pitch, speech rate, energy, emotion, and accent, among others.
ACC-metrics can be used to evaluate whether the spoken dialogue model can generate speech in the desired style.
Additionally, the evaluation of voice cloning capabilities within the model can borrow metrics from the zero-shot TTS domain[^041], [^203], [^204], [^205], using speaker similarity indices[^070].
Currently, there are few models that explore the generation of speech intelligence in spoken dialogue systems, and this area warrants further refinement and exploration in future work.

**Audio Understanding and Generation.**
In real-world scenarios, the broader definition of speech modality encompasses not only clear human speech but also a wide range of natural sounds such as dog barking and bird chirping, all of which can be considered forms of audio.
Evaluating the ability of spoken dialogue models to understand and generate such audio is a critical aspect of assessing the model’s performance.

- **Audio Understanding.** On the audio comprehension side, various sub-tasks are commonly employed to measure a system's capacity to understand audio, including tasks such as Audio Captioning (AudioCap) [^206], Sound Event Detection (SED) [^207], audio classification, and audio-motivated creative writing[^QwenAudio], among others.
The core of these tasks lies in evaluating the model’s ability to process and interpret the complex acoustic information embedded within the audio.
For tasks like audio classification and SED, which involve fixed outputs, evaluation is relatively straightforward, typically using objective metrics such as accuracy or Mean Average Precision (mAP).
However, for the AudioCap task, the problem is generally open-ended, meaning there are no fixed answers.
As a result, existing evaluation methods are primarily based on measuring the similarity between the generated text and the reference text, using traditional metrics such as BLEU [^190] and METEOR [^191], or newer evaluation approaches involving large language models such as GPT-4o [^194].
In the case of audio-motivated creative writing, where the objective is to generate inventive descriptions from a given audio input, evaluation typically relies on subjective measures, given the divergent nature of the creative process involved.

- **Audio Generation.** Additionally, on the audio generation side, producing high-quality audio should be considered an advanced capability for a conversational spoken dialogue model.
However, as most current spoken dialogue systems lack the ability to generate audio, this remains an area for further exploration in the future end-to-end spoken dialogue systems.
The evaluation of generated audio can draw from methods used in the text-to-audio domain[^208], [^029].
Typically, such evaluations focus on the quality of the generated audio itself, using metrics such as Mean Opinion Score (MOS) and the similarity between generated and target audio.
Objective evaluation metrics for audio similarity often include Fréchet Distance (FD), Inception Score (IS), Kullback-Leibler (KL) divergence, Fréchet Audio Distance (FAD), and CLAP score.
Specifically, Fréchet Audio Distance (FAD) [^209] is adapted from the Fréchet Inception Distance (FID) to the audio domain and serves as a reference-free perceptual metric that quantifies the distance between the generated and ground-truth audio distributions.
The Inception Score (IS) is an effective metric that evaluates both the quality and diversity of generated audio.
KL divergence is computed at the paired sample level between generated and ground-truth audio, based on the label distribution and averaged to produce a final result.
Fréchet Distance (FD) evaluates the similarity between the generated and ground-truth audio distributions.
FD, KL, and IS are built upon the PANNs model [^210], which takes mel-spectrograms as input.
In contrast, FAD uses VGGish [^211] as an audio classifier, processing raw audio waveforms as input.
The CLAP score, adapted from the CLIP score [^212], is a reference-free metric used to assess audio-text alignment and strongly correlates with human perception.

**Music Understanding and Generation.** In advanced spoken dialogue models, the evaluation of music modality understanding and generation follows a methodology similar to that used for audio modality.
Unlike Audio Understanding, which only requires a general description of the events that occur in the audio, Music Understanding requires appreciating the style and genre of music, understanding its keys, themes, and other rich information.
For classification, emotion recognition tasks in music, common metrics such as accuracy can be used.
For music captioning task, MusicCaps [^025] offers a general dataset for evaluating a model's music understanding capability.
For music analysis, Nsynth [^213] provides rich note data information.
In terms of evaluation for music generation, subjective Mean Opinion Score (MOS) assessments or measures of similarity between generated music and target music are commonly used.

**Multilingual Capability.** The ability to speak multiple languages is also required for a spoken dialogue model, but most current models [^214], [^SpokenLLM], [^ParalinGPT], [^172], [^215], [^E-chat] only focus on English and Chinese.
A naive idea is to directly evaluate spoken dialogue models' capability in speech-to-speech or speech-to-text translation tasks [^216], [^217].
These evaluations can be done with common machine learning metrics like BLEU [^190] or BertScore [^193].
However, evaluating the capability of translation is insufficient to measure the model's multilingual conversational ability, and further exploration is still needed in this area of evaluation.
Explicitly requiring a spoken dialogue model to perform speech translation is not a typical use case in conversational scenarios.
In most cases, when a user asks a question in a different language or with a distinct accent, the model is expected to automatically respond in the same language that the user is using.
In this context, it seems more reasonable to evaluate the accuracy of the model’s generated speech in terms of language identification, combined with subjective human assessments, as a more intuitive and appropriate evaluation method.

**Context Learning.** The context learning capability is crucial for maintaining the coherence of an entire conversation.
Similar to a memory function, the challenge lies in how to preserve this capability when relying solely on speech.
Typically, the evaluation of a spoken dialogue model's context learning ability depends on specific long-duration dialogue test sets, after which standard MT-Metrics or Acc-Metrics used in text intelligence evaluations can be applied.
For instance, a model's context learning capability can be assessed by evaluating its QA performance based on the given context [^218].
However, it is important to note the relevance of editing scenarios in long-duration spoken dialogues.
In real spoken dialogue scenarios, the users will modify some certain key information, the model needs to promptly understand and respond accordingly, e.g., the users offer wrong information for solving a problem and modify the condition in the next dialog.
So how to evaluate the model's online understanding ability is still needed further study.

**Interaction Capability.**
Interactive ability is also an essential metric for assessing the advanced capabilities of spoken dialogue systems.
As illustrated in Figure \ref{fig:main} (b), basic interactive ability refers to the system's capacity to allow users to interrupt the conversation at any time.
In this context, it is crucial to evaluate whether the spoken dialogue model can promptly comprehend the user's new input and halt its current response.
This is commonly measured using accuracy.
Furthermore, it is important to assess whether the model can generate a coherent and appropriate response based on the new input, which ties back to previous evaluation standards related to text and speech intelligence.

In addition, in real-world scenarios, beyond basic interruptions, various discourse markers such as "okay", "haha" are often used to indicate interaction.
Current spoken dialogue systems[^dGSLM] typically track the frequency of these markers as a standard evaluation metric.
Looking ahead, it may be valuable to assess whether future spoken dialogue models can effectively and appropriately interrupt human speakers, which could also represent a key dimension for evaluation the interaction capability.

**Multimodal Capability.** Spoken dialogue models primarily focus on the audio modality for both input and output.
However, considering the close coupling between video and audio modalities in practical applications of dialogue systems, recent advancements in spoken dialogue models have incorporated the understanding of video and images in the input stage (**VITA**[^VITA], [^040], [^219]), indicate that future spoken dialogue models need to simultaneously understand visual information and audio information to achieve real-time Audio-Visual Understandings.
The evaluation of such models generally still focuses on the evaluation of dialogue quality, that is, whether the generated dialogue and the reference dialogue are similar.
Therefore, this aspect can still be evaluated using metrics such as BLEU [^190] and METEOR [^191] to assess sentence semantic similarity.
However, research in this area also focuses on the understanding of visual information, and how to evaluate the model's correct understanding of real-time visual information in dialogue is also a difficulty, still can be a future benchmark direction.

**Security.** Security is also an integral part of the evaluation, how to ensure that the output of the model complies with ethical and social norms is a critical aspect.
Spoken dialogue models may encounter security issues such as harmful content generation, privacy pitfalls, bias, and adversarial attacks.
There has been considerable research progress in evaluating text modalities [^220].
The commonly used metric is to evaluate the attack success rate of injection attacks and so on.
However, there are relatively few evaluation methods in the field of speech modality.
How to construct a dataset for attacking spoken dialogue models, avoid poisoning of speech data, and evaluate the model's speech defense capabilities as benchmarks are required further research in the field of spoken dialogue model evaluation in the future.

### Benchmark

We list the common benchmarks for evaluating voice dialogue systems in the table\ref{table:benchmark}, and briefly introduce each benchmark in this section.

- **VoiceBench.**
VoiceBench's[^221] Key evaluation dimensions include general knowledge, instruction-following ability, and safety compliance.
The benchmark incorporates both synthetic and real spoken instructions to simulate diverse speaker styles, environmental conditions, and content variations.
It challenges systems with tasks involving accent adaptability, handling noisy environments, and robustness against content irregularities such as grammatical errors, disfluencies, and mispronunciations.
Additionally, it explores the systems' resilience under varying speaker characteristics (age, pitch, and speaking speed) and environmental challenges like reverberation, background noise, and far-field effects.

- **SUPERB.** [^222]
The benchmark evaluates speech processing models across multiple dimensions, including content recognition, speaker modeling, semantic understanding, and paralinguistic analysis.
Tasks in content recognition cover phoneme recognition, automatic speech recognition, keyword spotting, and query-by-example spoken term detection, focusing on transcription and content detection accuracy.
Speaker modeling involves tasks like speaker identification, automatic speaker verification, and speaker diarization to assess speaker-related features.
Semantic understanding includes intent classification and slot filling, testing models' ability to infer high-level meaning directly from raw audio.
Paralinguistic analysis focuses on emotion recognition, capturing models' ability to interpret affective cues from speech.
The evaluation framework uses publicly available datasets and conventional metrics to provide a standardized testbed for assessing generalizability and task-specific performance.

- **AudioBench.**
AudioBench[^223] evaluates spoken dialogue models across three primary dimensions: speech understanding, audio scene understanding, and voice (paralinguistic) understanding.
It encompasses eight distinct tasks and leverages 26 datasets, including seven newly developed datasets.
The evaluation emphasizes models' ability to handle instruction-following tasks conditioned on audio signals, addressing aspects such as speech recognition accuracy, environmental sound interpretation, and paralinguistic feature extraction (e.g., emotion, gender, accent).


- **AirBench.**
AIR-Bench[^202] assesses the capabilities of Spoken dialogue models to understand and interact based on various audio types, including human speech, natural sounds, and music.
It consists of two primary components: a foundation benchmark with 19 specific audio tasks and over 19,000 single-choice questions, and a chat benchmark featuring more than 2,000 open-ended audio-prompted questions.
The foundation benchmark evaluates fundamental skills such as speech recognition, acoustic scene classification, and music genre identification, focusing on specific subtasks to diagnose model weaknesses.
The chat benchmark tests the models' ability to handle complex, real-world audio-based queries, including mixed audio with varying loudness and temporal offsets.
AIR-Bench introduces a novel audio mixing strategy to simulate complex real-world scenarios and employs GPT-4-based evaluation to judge model-generated hypotheses against reference answers.

- **SpokenWOZ.**
SpokenWOZ[^224] evaluates task-oriented dialogue (TOD) systems in spoken scenarios, addressing challenges unique to spoken conversations, such as incremental processing, disfluencies, incomplete utterances, and Automatic Speech Recognition (ASR) noise.
It introduces novel metrics to assess performance in tasks like cross-turn slot detection and reasoning slot detection, which require integrating information across multiple turns and reasoning from implicit cues.
The benchmark encompasses multi-domain, human-to-human dialogues with diverse speech characteristics, testing systems on both textual and auditory inputs through large-scale annotated datasets with over 200,000 utterances and 249 hours of audio

- **SD-EVAL.**
SD-Eval[^201] evaluates spoken dialogue models across multiple dimensions, focusing on both spoken understanding and response generation beyond textual content.
It assesses models' abilities to process three key types of information embedded in speech: content (e.g., linguistic meaning), paralinguistic cues (e.g., emotion, accent, age), and environmental context (e.g., background sounds).
The benchmark consists of four sub-tasks—emotion, accent, age, and environment—constructed from diverse datasets and totaling 7,303 utterances spanning 8.76 hours.

- **SuperCLUE.**
SuperCLUE evaluates spoken dialogue systems across four main dimensions: voice interaction, general capabilities, scenario applications, and response speed.
Key metrics include interruption recognition, speech tone adjustment, semantic understanding, naturalness of speech, and memory accuracy.
Additionally, it measures real-time data retrieval, reasoning ability, compliance with commands, and multilingual translation accuracy.
Scenario-specific applications like emotional counseling, health consultations, and customer service are assessed for precision and effectiveness.
The final aspect is response timeliness, focusing on latency and delay management.However, this benchmark is not open source and focuse on Mandarine ability


- **MMAU.**
MMAU[^225] evaluates spoken dialogue models across multiple dimensions, encompassing 27 distinct tasks divided into reasoning and information extraction categories.
It assesses models on their ability to comprehend and reason about speech, sound, and music by leveraging advanced cognitive skills and domain-specific knowledge.
Key evaluated areas include temporal event reasoning, speaker role mapping, emotional tone interpretation, eco-acoustic knowledge, phonemic stress pattern analysis, and melodic structure interpretation.
It examines not just basic recognition or transcription capabilities but also models' proficiency in complex reasoning, contextual understanding, and the ability to extract and apply world knowledge.
Additionally, MMAU scrutinizes performance consistency across varying difficulty levels, testing systems' depth of reasoning and robustness in real-world audio scenarios.

## 结论

在这项工作中, 我们系统性地回顾了与口语对话模型相关的研究, 根据两种范式进行分类: 级联口语对话模型和端到端口语对话模型.

此外, 我们提供了口语对话模型背后的核心技术的详细概述, 包括语音表示, 训练范式, 流式双工系统和交互机制.
- 在语音表示模块中, 我们从输入和输出的角度分类和解释语音表示, 着重于语义和声学表示的不同类型.
- 在训练范式模块中, 我们深入讨论了口语对话模型对齐的五种方式, 多阶段训练策略, 模型架构和生成范式.
- 随后, 我们深入分析了口语对话模型流式输入和输出, 以及相关的双工交互技术.
- 最后, 我们整理了与口语对话模型相关的关键训练资源, 评估指标和基准.我们特别关注了在不同场景下对口语对话模型不同层次智能的评估.

需要注意的是, 由于口语对话模型是一项相对较新且新兴的技术, 许多方面如语义和声学表示, 仍然缺乏成熟的范式.
因此, 在每个部分的末尾, 我们专门加入了一个专门的讨论部分, 探讨这些开放性问题, 我们希望这项调查能够为口语对话系统领域的进一步发展做出贡献.

## Appendix

\section{Resources about Music and Sound Datasets}
\label{music datasets}

This section lists commonly used music and sound datasets.
These datasets cover different modalities, including environmental sounds, music, and emotional sounds, and provide some help for the development of future voice dialogue systems.
The table \ref{table:soundmusicdataset} shows the basic information of each dataset, including the dataset name, number of samples, dataset link, and modality type.

\begin{table}[h]
\centering
\caption{Music and Non-Speech Sound Datasets}
\label{table:soundmusicdataset}
% \renewcommand{\arraystretch}{1.3}

% \hspace*{-2.2cm}
\resizebox{1\linewidth}{!}{%
% \begin{Large}
\begin{tabular}{lcccl}
\hline
\textbf{Dataset} & \textbf{Size} & \textbf{URL} & \textbf{Modality} \\ \hline

ESC-50 [^226]             & 2,000 clips (5s each)           & \url{https://github.com/karoldvl/ESC-50}             & Sound \\
UrbanSound8K [^227] & 8,732 clips (<=4s each)         & \url{https://urbansounddataset.weebly.com/urbansound8k.html} & Sound \\
AudioSet [^228]  & 2000k+ clips (10s each)                   & \url{https://research.google.com/audioset/}           & Sound \\
TUT Acoustic Scenes 2017 [^229] & 52,630 segments            & \url{https://zenodo.org/record/4AudioGPT15}                & Sound \\
Warblr      & 10,000 clips                    & \url{https://warblr.net/}                              & Sound \\
FSD50K [^230]          & 51,197 clips (total 108.3 hours) & \url{https://zenodo.org/record/4060432}              & Sound \\
DCASE Challenge [^231]  & varies annually                 & \url{http://dcase.community/}                          & Sound \\
IRMAS [^232]         & 6,705 audio files (3s each)     & \url{https://www.upf.edu/web/mtg/irmas}               & Music \\
FMA  [^233] & 106,574 tracks            & \url{https://github.com/mdeff/fma}                    & Music \\
NSynth [^213]            & 305,979 notes                   & \url{https://magenta.tensorflow.org/datasets/nsynth}  & Music \\
EMOMusic               & 744 songs                       & \url{https://cvml.unige.ch/databases/emoMusic/}        & Music \\
MedleyDB [^234] & 122 multitrack recordings     & \url{https://medleydb.weebly.com/}                     & Music \\
% VIVAE [^235]                   & 1085 clips      & \url{https://zenodo.org/records}                     & emotion vocalizations \\
MagnaTagATune    & 25,863 clips (30s each)         & \url{https://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset} & Music \\
MUSDB[^236] & 150 songs & \url{https://paperswithcode.com/dataset/musdb18} & Music \\
M4Singer[^237] & 700 songs & \url{https://github.com/M4Singer/M4Singer} & Music \\
Jamendo &  600k songs & \url{https://www.jamendo.com/?language=en} & Music \\

\hline

\end{tabular}%
% \end
}

\end{table}


\section{Open-source Spoken Dialogue Models}

In this section, we provide a comprehensive list of publicly available and open-source spoken dialogue models in Table \ref{table:opensource_model}.

\begin{table}[]
\renewcommand{\arraystretch}{1.4}
\centering
\caption{A comprehensive list of publicly available  spoken dialogue models and their URL}
\resizebox{1\linewidth}{!}{%
\begin{tabular}{cc}
\hline
\textbf{Model} & \textbf{URL}                                      \\ \hline
AudioGPT       & \url{https://github.com/AIGC-Audio/AudioGPT}       \\
SpeechGPT      & \url{https://github.com/0nutation/SpeechGPT}              \\
Freeze-Omni & \url{https://github.com/VITA-MLLM/Freeze-Omni}              \\
Baichuan-Omni  & \url{https://github.com/westlake-baichuan-mllm/bc-omni} \\
GLM-4-Voice       & \url{https://github.com/THUDM/GLM-4-Voice }      \\
Mini-Omni     & \url{https://github.com/gpt-omni/mini-omni  }      \\
Mini-Omni2     & \url{https://github.com/gpt-omni/mini-omni2  }      \\
FunAudioLLM    & \url{https://github.com/FunAudioLLM}             \\
Qwen-Audio    & \url{https://github.com/QwenLM/Qwen-Audio }             \\
Qwen2-Audio    & \url{https://github.com/QwenLM/Qwen2-Audio }             \\
LLaMA3.1       & \url{https://www.llama.com}              \\
Audio Flamingo & \url{https://github.com/NVIDIA/audio-flamingo}             \\
Spirit LM      & \url{https://github.com/facebookresearch/spiritlm }             \\
dGSLM          & \url{https://github.com/facebookresearch/fairseq/tree/main/examples/textless_nlp/dgslm}              \\
Spoken-LLM     & \url{https://arxiv.org/abs/2305.11000}              \\
LLaMA-Omni     & \url{https://github.com/ictnlp/LLaMA-Omni }             \\
Moshi          & \url{https://github.com/kyutai-labs/moshi}          \\
SALMONN        & \url{https://github.com/bytedance/SALMONN}              \\
LTU-AS         & \url{https://github.com/YuanGongND/ltu}             \\
VITA           & \url{https://github.com/VITA-MLLM/VITA}             \\
SpeechGPT-Gen  & \url{https://github.com/0nutation/SpeechGPT}              \\
WavLLM &
\url{https://github.com/microsoft/SpeechT5/tree/main/WavLLM}\\
Westlake-Omni  & \url{https://github.com/xinchen-ai/Westlake-Omni}              \\ MooER-Omni  & \url{https://github.com/MooreThreads/MooER}              \\ Hertz-dev  & \url{https://github.com/Standard-Intelligence/hertz-dev}              \\ Fish-Agent  & \url{https://github.com/fishaudio/fish-speech}              \\ SpeechGPT2   & \url{https://0nutation.github.io/SpeechGPT2.github.io/}              \\ \hline
\end{tabular}}
\label{table:opensource_model}
\end{table}

\section{Open-source Codec Models}

In this section, we provide a comprehensive list of publicly available and open-source codec models, as shown in Table ~\ref{tab:all_codec}.


\begin{table}[htbp]
\centering
\caption{A comprehensive list of publicly available codec models and their URL}
\label{tab:all_codec}
\resizebox{1\linewidth}{!}{%
\begin{tabular}{cc}
\toprule
\textbf{Model}                    & \textbf{URL}  \\
\midrule

Encodec [^066]                  & \url{https://github.com/facebookresearch/encodec}          \\
SoundStream [^068]              & \url{https://github.com/wesbz/SoundStream}                 \\
DAC [^067]                     & \url{https://github.com/descriptinc/descript-audio-codec}  \\
WavTokenizer  [^065]           & \url{https://github.com/jishengpeng/WavTokenizer}  \\
SpeechTokenizer [^083] & \url{https://github.com/ZhangXInFD/SpeechTokenizer}        \\
SNAC  [^092]           & \url{https://github.com/hubertsiuzdak/snac}  \\
SemantiCodec [^238]             & \url{https://github.com/haoheliu/SemantiCodec-inference}   \\
**Mimi**[^Moshi] &
\url{https://github.com/kyutai-labs/moshi} \\
HiFi-Codec [^239]              & \url{https://github.com/yangdongchao/AcademiCodec}        \\
FunCodec  [^240]               & \url{https://github.com/modelscope/FunCodec }              \\
APCodec  [^241]                & \url{https://github.com/YangAi520/APCodec/tree/main}       \\
AudioDec [^129]                 & \url{https://github.com/facebookresearch/AudioDec}         \\
FACodec  [^242]                & \url{https://github.com/lifeiteng/naturalspeech3\_facodec} \\
Language-Codec  [^243]         & \url{https://github.com/jishengpeng/Languagecodec}         \\
XCodec  [^244]           & \url{https://github.com/zhenye234/xcodec}  \\
TiCodec  [^094]           & \url{https://github.com/y-ren16/TiCodec}  \\
SoCodec  [^245]           & \url{https://github.com/hhguo/SoCodec}  \\
FUVC  [^246]           & \url{https://github.com/z21110008/FUVC}  \\
HILCodec [^247]           & \url{https://github.com/aask1357/hilcodec}  \\
LaDiffCodec [^248]           & \url{https://github.com/haiciyang/LaDiffCodec}  \\
LLM-Codec [^249]           & \url{https://github.com/yangdongchao/LLM-Codec}  \\
SpatialCodec [^250]           & \url{https://github.com/XZWY/SpatialCodec}  \\
BigCodec [^098] &
\url{https://github.com/Aria-K-Alethia/BigCodec} \\
SuperCodec [^251] &
\url{https://github.com/exercise-book-yq/Supercodec} \\
RepCodec [^252] &
\url{https://github.com/mct10/RepCodec} \\
EnCodecMAE [^253] &
\url{https://github.com/habla-liaa/encodecmae} \\
MuCodec [^254] &
\url{https://github.com/xuyaoxun/MuCodec} \\
SPARC [^255] &
\url{https://github.com/Berkeley-Speech-Group/Speech-Articulatory-Coding} \\
BANC [^256] &
\url{https://github.com/anton-jeran/MULTI-AUDIODEC} \\
SpeechRVQ [^257] &
\url{https://huggingface.co/ibm/DAC.speech.v1.0} \\
QINCo [^258] &
\url{https://github.com/facebookresearch/Qinco} \\
SimVQ [^259] &
\url{https://github.com/youngsheen/SimVQ} \\

\bottomrule
\end{tabular}}
\end{table}

## 参考文献

[^Moshi]: [Moshi/Mimi](../Models/SpeechLM/Interaction/2024.09.17_Moshi.md)
[^SpeechGPT]: [SpeechGPT](../Models/SpeechLM/Interaction/2023.05.18_SpeechGPT.md)
[^MiniOmni2]: [Mini-Omni2](../Models/SpeechLM/Interaction/2024.10.15_Mini-Omni2.md)
[^LLaMA3]: [LLaMA3.1](../Models/TextLM/2024.07.31_LLaMA3.md)
[^AudioGPT]: [AudioGPT]
[^FunAudioLLM]: speechteam2024funaudiollm
[^SpokenLLM]: lin2024advancing
[^ParalinGPT]: lin2024paralinguistics
[^E-chat]: xue2023chat
[^Qwen2Audio]: chu2024qwen2
[^QwenAudio]: chu2023qwen
[^LTU-AS]: gong2023joint
[^SALMONN]: tang2023salmonn
[^VITA]: [VITA](../Models/SpeechLM/Interaction/2024.08.09_VITA.md)
[^dGSLM]: nguyen2023generative
[^FSQ]: mentzer2023finite
[^MiniOmni]: xie2024mini
[^OmniFlatten]: zhang2024omniflatten
[^IntrinsicVoice]: zhang2024intrinsicvoice
[^Whisper]: radford2023robust
[^VITS2]: kong2023vits2
[^FastSpeech2]: ren2020fastspeech
[^SpeechGPT-Gen]: zhang2024speechgpt
[^024]: touvron2023llama
[^025]: agostinelli2023musiclm
[^026]: copet2024simple
[^027]: lam2024efficient
[^028]: ma2024foundation
[^029]: huang2023make
[^030]: liu2023audioldm
[^031]: liu2024audioldm
[^032]: fang2024llama
[^033]: chen2024emova
[^034]: ma2023emotion2vec
[^035]: hu2021lora
[^036]: das2024speechverse
[^037]: kong2024audio
[^038]: chen2022beats
[^039]: li2023blip
[^040]: li2024baichuan
[^041]: wang2023neural
[^042]: zhang2023speak
[^043]: jiang2024mega
[^044]: jiang2023mega
[^045]: du2024cosyvoice
[^046]: lyth2024natural
[^047]: wang2024maskgct
[^048]: chen2024f5
[^049]: ji2024textrolspeech
[^050]: guo2023prompttts
[^051]: leng2023prompttts
[^052]: yang2024instructtts
[^053]: ji2024controlspeech
[^054]: cieri2LLaMA3fisher
[^055]: marge2022spoken
[^056]: mitsui2024pslm
[^057]: nguyen2024spirit
[^058]: wang2024freezeomnismartlowlatency
[^059]: veluri2024beyond
[^060]: chung2020vector
[^061]: shain2020acquiring
[^062]: chi2021audio
[^063]: hsu2021hubert
[^064]: liu2020mockingjay
[^065]: ji2024wavtokenizer
[^066]: defossez2022high
[^067]: kumar2024high
[^068]: zeghidour2021soundstream
[^069]: schneider2019wav2vec
[^070]: chen2022wavlm
[^071]: baevski2020wav2vec
[^072]: devlin2018bert
[^073]: li2023exploration
[^074]: babu2021xls
[^075]: kim2024unified
[^076]: barrault2023seamless
[^077]: zheng2Moshicomparison
[^078]: vaswani2017attention
[^079]: gat2022augmentation
[^080]: lin2024alignslmtextlessspokenlanguage
[^081]: huang2022spiral
[^082]: xiong2024freeze
[^083]: zhang2023speechtokenizer
[^084]: du2023lauragpt
[^085]: mohamed2022self
[^086]: kong2020hifi
[^087]: polyak2021speech
[^088]: kharitonov2022textless
[^089]: wei2022chain
[^090]: le2024voicebox
[^091]: bai2023qwen
[^092]: siuzdak2024snac
[^093]: lipman2022flow
[^094]: ren2024fewer
[^095]: zhou2024transfusion
[^096]: xie2024show
[^097]: li2024single
[^098]: xin2024bigcodec
[^099]: achiam2023gpt
[^100]: yang2024qwen2
[^101]: gage1994new
[^102]: sennrich2015neural
[^103]: rubenstein2023audiopalm
[^104]: hu2024wavllm
[^105]: casanova2024xtts
[^106]: rafailov2024direct
[^107]: nachmani2023spoken
[^108]: maiti2024voxtlm
[^109]: chaplot2023albert
[^110]: hassid2024textually
[^111]: jiang2024mixtral
[^112]: jin2024efficientmllm
[^113]: li2024surveybenchmarksmultimodallarge
[^114]: anastassiou2024seed
[^115]: zhang2024speechalign
[^116]: chen2024enhancing
[^117]: bengio2009curriculum
[^118]: graves2006connectionist
[^119]: peng2024voicecraft
[^120]: yang2023uniaudio
[^121]: yu2023megabyte
[^122]: kim2021conditional
[^123]: schulman2017proximal
[^124]: sutton1999policy
[^125]: wallace2024diffusion
[^126]: cideron2024musicrl
[^127]: huang2023survey
[^128]: bai2018empirical
[^129]: wu2023audiodec
[^130]: wang2024full
[^131]: wu2021u2++
[^132]: ma2024language
[^133]: raux2009finite
[^134]: sacks1974simplest
[^135]: duncan1972some
[^136]: duncan1974signalling
[^137]: hara2019turn
[^138]: lala2017attentive
[^139]: lala2019smooth
[^140]: ekstedt2020turngpt
[^141]: khouzaimi2016reinforcement
[^142]: hara2018prediction
[^143]: lin2022duplex
[^144]: jin2021duplex
[^145]: CleanS2S
[^146]: meng2024sd
[^147]: anil2023palm
[^148]: bu2017aishell
[^149]: du2018aishell
[^150]: shi2020aishell
[^151]: zen2019libritts
[^152]: rousseau2012ted
[^153]: wang2021voxpopuli
[^154]: panayotov2015librispeech
[^155]: pratap2020mls
[^156]: zhang2022wenetspeech
[^157]: chen2021gigaspeech
[^158]: veaux2013voice
[^159]: ljspeech17
[^160]: ardila2019common
[^161]: mei2024wavcaps
[^162]: kahn2020libri
[^163]: galvez2021people
[^164]: tang2021kespeech
[^165]: he2024emilia
[^166]: maeng2017alpaca
[^167]: ding2023enhancing
[^168]: OpenOrca
[^169]: reddy2Moshiinterspeech
[^170]: snyder2015musan
[^171]: liu2024generative
[^172]: shu2023llasm
[^173]: talmor2018commonsenseqa
[^174]: liang2022holistic
[^175]: zellers2019hellaswag
[^176]: clark2018think
[^177]: sakaguchi2021winogrande
[^178]: chen2021evaluating
[^179]: zhong2023agieval
[^180]: mishra2021cross
[^181]: wang2022super
[^182]: feng2022mmdialog
[^183]: hendrycks2020measuring
[^184]: cobbe2021training
[^185]: longpre2023flan
[^186]: wei2021finetuned
[^187]: wang2022self
[^188]: reddy2019coqa
[^189]: kopf2024openassistant
[^190]: papineni2SpeechGPTbleu
[^191]: banerjee2AudioGPTmeteor
[^192]: lin2LLaMA3rouge
[^193]: zhang2019bertscore
[^194]: zheng2023judging
[^195]: liu2023g
[^196]: goel2024audio
[^197]: busso2008iemocap
[^198]: poria2018meld
[^199]: firdaus2020meisd
[^200]: busso2016msp
[^201]: ao2024sd
[^202]: yang2024air
[^203]: shen2023naturalspeech
[^204]: ji2024mobilespeech
[^205]: wang2024ham
[^206]: kim2019audiocaps
[^207]: mesaros2021sound
[^208]: huang2023make2
[^209]: kilgour2018fr
[^210]: kong2020panns
[^211]: hershey2017cnn
[^212]: hessel2021clipscore
[^213]: engel2017neural
[^214]: gong2023listen
[^215]: wang2023blsp
[^216]: jia2022cvss
[^217]: wang2020covost1
[^218]: lipping2022clotho
[^219]: park2024let
[^220]: dong2024attacks
[^221]: chen2024voicebench
[^222]: yang2021superb
[^223]: wang2024audiobench
[^224]: si2024spokenwoz
[^225]: sakshi2024mmau
[^226]: piczak2015esc
[^227]: salamon2014dataset
[^228]: gemmeke2017audio
[^229]: mesaros2016tut
[^230]: fonseca2021fsd50k
[^231]: mesaros2017dcase
[^232]: bosch2012comparison
[^233]: fma_dataset
[^234]: bittner2014medleydb
[^235]: holz2022variably
[^236]: rafii2017musdb18
[^237]: zhang2022m4singer
[^238]: liu2024semanticodec
[^239]: yang2023hifi
[^240]: du2024funcodec
[^241]: ai2024apcodec
[^242]: ju2024naturalspeech
[^243]: ji2024language
[^244]: ye2024codecdoesmatterexploring
[^245]: guo2024socodec
[^246]: zheng2024fuvc
[^247]: ahn2024hilcodec
[^248]: yang2024generative
[^249]: yang2024uniaudio
[^250]: xu2024spatialcodec
[^251]: zheng2024supercodec
[^252]: huang2023repcodec
[^253]: pepino2023encodecmae
[^254]: xu2024mucodec
[^255]: cho2024articulatory
[^256]: ratnarajah2023m3
[^257]: shechtman24_interspeech
[^258]: huijben2024residual
[^259]: zhu2024addressing
