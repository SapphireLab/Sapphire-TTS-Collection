# WavChat: A Survey of Spoken Dialogue Models

<details>
<summary>基本信息</summary>

- 标题: "WavChat: A Survey of Spoken Dialogue Models"
- 作者:
  - 01 Shengpeng Ji (浙江大学, shengpengji@zju.edu.cn)
  - 02 Yifu Chen (浙江大学)
  - 03 Minghui Fang (浙江大学)
  - 04 Jialong Zuo (浙江大学)
  - 05 Jingyu Lu (浙江大学)
  - 06 Hanting Wang (浙江大学)
  - 07 Ziyue Jiang (浙江大学)
  - 08 Long Zhou (微软)
  - 09 Shujie Liu (微软)
  - 10 Xize Cheng (浙江大学)
  - 11 Xiaoda Yang (浙江大学)
  - 12 Zehan Wang (浙江大学)
  - 13 Qian Yang (浙江大学)
  - 14 Jian Li (腾讯优图实验室)
  - 15 Yidi Jiang (阿里巴巴)
  - 16 Jingzhen He (阿里巴巴)
  - 17 Yunfei Chu (阿里巴巴)
  - 18 Jin Xu (阿里巴巴)
  - 19 Zhou Zhao (浙江大学, zhaozhou@zju.edu.cn)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2411.13577)
  - [Publication]
  - [Github](https://github.com/jishengpeng/WavChat)
  - [Demo]
  - [WeChat](https://mp.weixin.qq.com/s/9DisPbvZBr8NGY2SPytT9Q)
- 文件:
  - [ArXiv]()
  - [Publication] #TODO

</details>

## 摘要

口语对话模型的近期进展, 如 GPT-4o 等系统为代表, 在语音领域获得了广泛关注.
在多模态模型的更广泛背景下, 语音模态为人机交互提供了直接的接口, 使得人工智能和用户之间可以直接交流.
和传统的由自动语音识别 (ASR) + 大语言模型 (LLM) + 文本转语音 (TTS) 组成的三层级联口语对话模型相比, 现代的口语对话模型展现出更高的智能.
这些先进的口语对话模型不仅能够理解音频, 音乐, 以及其他与语音相关的特征, 还能够捕获语音的风格和音色特征.
此外, 它们能够以低延迟生成高质量, 多轮次的语音响应.

尽管口语对话系统取得了不少进步, 但缺乏全面的综述, 来系统地组织和分析这些系统和底层技术.

- 为了解决这一问题, **我们首先将现有的口语对话系统按时间顺序排列, 并将它们分类为级联范式和端到端范式**.
- 然后我们对口语对话模型的核心技术进行了深入的概述, 涵盖了**语音表示**, **训练范式**, **流式/双工/交互能力**等方面.
每一节讨论这些技术的局限性, 并概述了未来研究的考虑因素.
- 此外, 我们从训练和评估口语对话模型的角度, 全面回顾了相关的**数据集**, **评估指标**, 和**基准**.

我们希望这项工作能够促进口语对话系统的学术研究和工业应用的发展.
相关材料可在 [Github](https://github.com/jishengpeng/WavChat) 获得.

## 1·引言

口语对话模型 (**Moshi**[^Moshi], **SpeechGPT**[^SpeechGPT], **Mini-Omni2**[^Mini-Omni2]) 代表了人机交互中最直接的方法之一, 从传统的声音助手 ([Alexa [URL]](https://www.alexa.com/); [Siri [URL]](https://www.apple.com/siri/); [Google Assistant [URL]](https://assistant.google.com/)) 进化到最新的智能对话系统 (如 [GPT-4o [URL]](https://openai.com/index/chatgpt-can-now-see-hear-and-speak/)).

口语对话模型的基本定义是指一个能够根据输入语音生成智能口语回应的对话系统.
一方面, **语音模态**在口语对话模型中既是人机交互的输入接口, 也是输出接口.
另一方面, **对话系统** (**LLaMA3.1**[^LLaMA3]) 要求模型具备一定程度的文本智能, 包括理解人类社会知识并生成专业和智能的回应.

近期, 以 GPT-4o 和 **Moshi**[^Moshi] 为代表的智能口语对话系统, 因其超越了传统基于文本的对话模型 (**AudioGPT**[^AudioGPT]) 的语音智能能力而受到广泛关注.

这些对话模型不仅能够生成自然, 类似人类的语音回应 (**Moshi**[^Moshi]; **FunAudioLLM**[^FunAudioLLM]) 还展示了超越文本的高级声学特征 (如音色, 情感和风格) 的理解和生成能力 (**Spoken-LLM**[^Spoken-LLM]; **ParalinGPT**[^ParalinGPT]; **E-chat**[^E-chat]).
此外, 它们在处理其他语音相关表示方面表现出色, 包括音乐和音频事件 (**Qwen2-Audio**[^Qwen2-Audio]; **Qwen-Audio**[^Qwen-Audio]; **LTU-AS**[^LTU-AS]; **SALMONN**[^SALMONN]).
它们在现实对话互动 (**VITA**[^VITA]; **Mini-Omni2**[^Mini-Omni2]) 和低延迟对话体验 (**Moshi**[^Moshi]) 方面的表现进一步使它们在传统对话模型的竞争中脱颖而出.

口语对话模型的历史可以回溯到早期系统, 如 **dGSLM**[^dGSLM], **AudioGPT**[^AudioGPT], 直至最近的进展, 如 GPT-4o 和 **Moshi**[^Moshi].
在此过程中, 许多值得注意的口语对话模型相继出现.
如图 01 所示, 我们按时间顺序组织了这些模型.

![](Images/S20241115_Fig.01.png)

大致上, 它们可以分为两种类型:
- 级联口语对话模型: (**Qwen2-Audio**[^Qwen2-Audio], **Qwen-Audio**[^Qwen-Audio])
- 端到端口语对话模型: (**FSQ**[^FSQ], **Mini-Omni**[^Mini-Omni], **OmniFlatten**[^OmniFlatten], **IntrinsicVoice**[^IntrinsicVoice])

鉴于现有大多数口语对话模型依赖和文本模态的对齐, 级联模型和端到端模型之间的区别十分关键.

如图 02 所示, 我们根据**核心语言模型是否能够直接理解和生成语音表示**将所有口语对话模型划分为级联模型和端到端模型.

![](Images/S20241115_Fig.02.png)

传统的级联口语对话系统, 如 **AudioGPT**[^AudioGPT], 采用以文本为中心媒介的结构, 通常由三个级联模块组成:
- 输入音频通过自动语音识别 ASR 模块 (**Whisper**[^Whisper]) 转写为文本.
- 转写后的文本输入到大语言模型如 ChatGPT 中生成文本回应.
- 最后, 文本回应通过文本转语音模块 (**VITS2**[^VITS2], **FastSpeech2**[^FastSpeech2]) 转换回音频.

尽管这种级联架构利用了大语言模型强大的上下文能力, 但它也引入了一些挑战: 高延迟, 受限互动性, 无法处理非文本信息等.

为了处理这些问题, 近期研究采取了两个主要方向:
- 一些方法 (**Qwen-Audio**[^Qwen-Audio], [^SALMONN]) 专注于优化级联系统中的理解和生成组件以缓解上述局限.
- 另一些方法 (**Mini-Omni**[^Mini-Omni], **Mini-Omni2**[^Mini-Omni2], **SpeechGPT-Gen**[^SpeechGPT-Gen], **IntrinsicVoice**[^IntrinsicVoice]) 试图采用端到端架构来直接解决这些问题.
  尽管端到端口语对话模型在表示和模型架构方面存在各种差异, 但它们有一个共同特征: 它们不依赖于文本作为中介.
  相反, 这些模型试图直接理解和生成语音表示.
  我们将此类系统定义为端到端口语对话模型.

在构建口语对话系统时, 我们根据涉及的不同级别的智能, 确定了和口语对话模型相关的四种核心技术.
- 第一: 语音表示的设计 (即 Tokenizer 和 Detokenizer)
- 第二: 训练, 推理, 生成范式, 特别是如何在保留或增强现有文本对话模型的智能的同时对齐语音模态和文本模态. 这一部分设计到选择不同的模型架构, 生成策略, 多阶段训练策略.
- 第三: 交互, 双工, 流式的设计.
- 第四: 数据特定相关的挑战, 如何构造口语对话系统的训练数据集和评估其性能.

鉴于这些考虑, 在本文的后续章节中, 我们将按照顺序详细阐述这四种核心技术:
- 第二节: 提供口语对话模型的概述, 包括电影的口语对话场景 (即如何定义一个口语对话模型) 以及级联和端到端口语对话模型的近期进展.
- 第三节: 口语对话系统使用的语音表示.
- 第四节: 系统地讨论训练范式, 特别强调了如何将语音模态与文本模态对齐, 以及多阶段训练策略, 模型架构, 和生成策略.
- 第五节: 强调了口语对话系统的独特特性, 特别是双工, 流式特性, 这些特性使得它们与文本对话系统有所不同.
- 第六节: 考察了口语对话模型的训练数据集构建和特定的评估方法.

在每一节末尾, 我们进行总结和讨论, 以反思关键见解.
最后, 我们在第七节, 总结了本文的主要发现和未来研究的开放问题.

鉴于技术点的复杂性, 我们在图 03 中提供了本文的结构概览.

![](Images/S20241115_Fig.03.png)

## 2·整体视角

在本节中, 我们将对口语对话模型进行整体概述.
首先, 我们通过考察各种对话场景来定义什么构成了一个智能口语对话模型.
然后, 我们提供了口语对话模型的全面概述, 区分级联口语对话模型和端到端口语对话模型.

### 2.1·口语对话系统的功能

基于 GPT-4o, **Moshi**[^Moshi], **Qwen2-Audio**[^Qwen2-Audio], **VITA**[^VITA] 等代表性模型的演示和推理接口, 我们将现代智能口语对话模型的使用场景分为以下九个代表性类别:
1) 文本智能;
2) 语音智能;
3) 音频和音乐生成;
4) 音频和音乐理解;
5) 多语言能力;
6) 上下文学习;
7) 交互能力;
8) 流式延迟;
9) 多模态能力.

对于口语对话模型中的九个不同用例, 我们在图 04 中为每个场景提供了相应的示例.
从这些使用场景可以清楚地看出, 口语对话模型不仅仅是基于文本对话模型向语音模态的扩展 (即, 语音模态仅作为将语音转换为文本的接口).
相反, 智能口语对话模型必须能够理解和生成语音中嵌入的声学信息 (如音色, 风格, 以及情感), 并且能够理解和生成更广泛的音频表示, 包括与音频时间和音乐相关的信息.
此外, 与非流式的基于文本的系统不同, 口语对话模型需要支持实时, 交互式的流式传输能力.
这些使用场景不仅突出了口语对话系统中固有的智能, 还展示了构建端到端口语对话模型带来的重大挑战.
下面, 我们将对这九种使用场景进行详细考察.

#### 2.1.1·文本智能

如图 04 (a) 所示, 口语对话系统必须保留原始基于文本的对话模型 (如 ChatGPT) 的基本能力.
我们将这一使用场景定义为文本智能.
在这种情况下, 口语对话模型可以智能地响应用户请求, 生成合适的响应, 如旅行行程, 工作计划和日程安排.

然而, 由于基于声音交互的局限性, 当前的口语对话系统的文本智能更侧重于日常场景.
在某些情况下, 如复杂的数学定理推理, 口语对话模型的性能要求与基于文本的对话模型有所不同 (**LLaMA**[^LLaMA]).
这些文本智能的高级方面在统一的多模态对话模型中值得进一步探索.

#### 2.1.2·语音智能

和基于文本的对话模型 (**LLaMA**[^LLaMA]) 相比, 口语对话模型的显著特征之一是它们能够理解和生成超越文本内容的声学信息的能力.
在语音模态中, 不仅存在文本内容, 还有额外的声学信息 (例如音色/说话人身份和风格/情感/韵律等).
如图 04 (b) 所示, 智能口语对话模型应该能够理解对话语音的音色和风格, 并且理想情况下, 以零样本方式生成具有指定音色和风格的响应.

语音智能的能力涉及到多种用例.

在**理解方面**, 口语对话系统应该基于说话人声学风格生成响应.
例如, 在 **E-chat**[^E-chat] 中, 一个典型的例子可能是:
如果用户以欢快的语气询问:"我的手机无法开机, 我该怎么办?",
系统可能会回答: "看起来你对新手机很兴奋, 你对哪种手机感兴趣?".
如果用户以悲伤的语气询问同样的问题, 系统可能会回答: "很遗憾你的手机无法正常工作.
如果你熟悉维修政策, 继续下一步吧."
这种情况表明, 口语对话系统可能会根据不同的声学信息生成不同内容的响应.

此外, 系统应该理解各种声学线索, 如口音或情绪状态, 并根据相应的不同声学信息调整其响应.
例如, 如果说话人是一位美国人, 系统可能会用本地的英语口音进行回复, 而如果说话人是上海人, 系统可以用相应的方言回复.
类似地, 如果用户以悲伤的语气说话, 对话系统应能够生成更具有鼓励性和同理心的响应.

在**生成方面**, 语音智能更突出地体现在其可控性上, 例如声音克隆和风格控制.
例如, 系统可以被指示模仿特定声音或以指定风格回应 (模仿祖母的轻声细语以进行安慰性的互动)
此外, 系统可以在对话时使用提供的声音提示来完全克隆提示中的音色并生成相同声音的语音.

总而言之, 理解和生成声学信息的能力是智能口语对话模型的关键特征之一.

#### 2.1.3·音频和音乐生成

在口语对话模型中, 除了基本的口语对话能力之外, 智能口语对话系统可能还被要求生成音乐和音频.
例如, 用户可能会指示系统生成一段一分钟的钢琴段落或十秒钟的狗叫录音.
此外, 用户可能提供歌词和音乐旋律, 要求口语对话模型创作一首流行歌曲.
因此, 系统应在输出侧继承大规模的音乐 (**MusicLM**[^MusicLM], **MusicGen**[^MusicGen], **MeLoDy**[^MeLoDy], **Survey20240826**[^S20240826]) 和音频 (**Make-An-Audio**[^Make-An-Audio], **AudioLDM**[^AudioLDM], **AudioLDM2**[^AudioLDM2]) 模型的生成能力.

#### 2.1.4·音频和音乐理解

除了音乐和音频生成能力之外, 口语对话模型应该还能够理解输入侧的音乐和音频 (**Qwen2-Audio**[^Qwen2-Audio], **SALMONN**[^SALMONN]).
例如, 当给定一段音频时, 智能系统应该识别其内容和声学特征, 例如识别声音是鸟鸣还是猫叫, 音乐是平静还是充满活力.

此外, 系统还可以通过基于给定的音乐或音频来创作文学作品 (诗歌/歌曲) 来扩展其理解能力.

#### 2.1.5·多语言能力

和基于文本的对话模型类似, 口语对话系统也应该具备处理多语言的能力.
具体来说, 这些模型应该能够进行多语言的内容翻译, 例如将日语口语片段翻译成法语语音片段, 有效地继承同声传译的能力.

除了多语言内容翻译, 系统还应该能够处理多语言声学信息.
这意味着智能口语对话模型应该能够以各种语言和口音生成响应, 根据不同的输入语音以目标语言的响应口音回应.

#### 2.1.6·上下文学习

在口语对话模型中, 处理长篇和多轮对话的能力是评估性能的关键基准 (**Moshi**[^Moshi]).
这要求口语对话模型不仅要支持长时段音频输入还需要生成扩展的音频输出.
此外, 它们必须能够基于历史上下文进行多轮对话.
多轮对话的重要方面之一是基于新的用户指令对之前的响应进行修正的能力.
如图 04 (f) 所示, 智能口语对话模型应该能够持续地根据用户不断变化的请求修改其之前的回复.

#### 2.1.7·交互能力

与基于文本的对话模型相比, 口语对话系统的一个显著特点是它们的双工和交互性 (**Moshi**[^Moshi]).
在基于文本的对话中, 交互通常遵循半双工结构, 即响应只能在问句完成后提供, 用户无法实时干扰回复.
然而, 在口语对话系统中, 全双工交互是常见的.
这意味着在生成响应之前不需要完全完成对话.
系统和用户都可以实时打断和互动.
例如, 如果用户不满意系统的响应, 他们可以立即打断, 使得系统暂停当前生成并响应新的输入.

此外, 为了模仿更自然的对话场景, 系统还可以在适当的时候打断用户例如在澄清用户意图时.

除了打断能力外, 交互对话通常还包括对话填充词 (如 "okay," "haha," 或 "oh,"), 它们表示确认或同意.
在口语对话模型中包括这些内容可以增强对话的真实性和自然流动性.

交互能力的基本要求是同时听和说, 动态地响应交互流程.

#### 2.1.8·流式延迟

流式理解和生成也是口语对话模型的基本功能 (**Mini-Omni2**[^Mini-Omni2], **IntrinsicVoice**[^IntrinsicVoice], **LLaMA-Omni**[^LLaMA-Omni]).
在真实场景中, 模型不能等到整个分钟长的音频片段处理完成后才生成响应.
相反, 模型必须以分块的机制运行, 实时地动态处理和生成音频, 一次处理一个分块.
此外, 流式要求意味着整个系统必须以因果方式运行, 仅基于过去信息理解和生成音频, 而不依赖未来信息.
流式功能通常和低延迟需求紧密相关.
在实际对话体验中, 口语对话模型生成第一个 Token 的延迟 (即用户等待时间) 和生成过程的平均延迟是影响口语对话系统整体响应及可用性的关键因素.

#### 2.1.9·多模态能力

多模态对话能力 (**EMOVA**[^EMOVA], **VITA**[^VITA]) 是口语对话模型的一个高级特性.
在现有系统中, 这通常指的是处理来自多个模态 (如音频, 图像, 文本) 的输入, 同时生成智能语音响应的能力.
具备这种能力的口语对话模型可以实现 "听, 看, 说" 同时进行.
多模态输入显著提升了这些系统的潜力.

例如, 用户可以通过各种手势来提升模型生成响应的质量, 且系统可以对物理世界进行更深入的理解.

除了多模态输入, 对话系统的未来在于大型多模态模型, 这些模型统一了所有模态的理解和生成能力, 而口语对话扮演着基础模态.

### 2.2·级联口语对话系统

级联口语对话模型的最早原型可以追溯到 **AudioGPT**[^AudioGPT].
为了实现语音到语音对话功能, 该系统首先采用自动语音识别 (ASR) 模型来将语音转换为文本, 后跟 ChatGPT 进行基于文本的对话, 最后使用文本转语音 (TTS) 模型将生成的文本转换为语音.
在这一原始版本中, 语音仅作为输入输出接口, 仅保留了最基础的文本智能.
- 例如, HuggingFace 开源的语音到语音框架 ([Github](https://github.com/huggingface/speech-to-speech)) 中进一步在传统级联模块上叠加一个语音活动检测 (Voice Activity Detection, VAD) 模块来区分语音和静音片段, 以及不同说话人.

级联口语对话模型中建立了基本的文本智能后, 研究人员开始融入副语言特征, 例如情感和风格, 以增强级联口语对话模型中的语音智能.
- 例如, **ParalinGPT**[^ParalinGPT] 和 **E-chat**[^E-chat] 通过滑动窗口机制将对话上下文, 语音嵌入和副语言属性集成到自回归模型中, 使得模型能够结合历史文本和情感表示来生成更准确的文本响应.
- 类似地, Spoken-LLM[^Spoken-LLM] 引入了 **Emotion2Vec**[^Emotion2Vec] 模块来为 LLaMA2-Chat 模型提供风格向量.
通过 **LoRA**[^LoRA] 微调, LLaMA2-Chat 被训练成生成不仅基于内容的文本响应, 还能够生成具有具体的风格属性 (如 `<cheerful, fast, normal>`) 的文本响应, 以指导下游 TTS 系统来生成更具表现力的语音.

除了在级联口语对话模型中理解声学信息, 还有其他一些努力试图直接输入语音表示同时保留文本作为输出模态 (***SpeechVerse**[^SpeechVerse], **Qwen-Audio**[^Qwen-Audio], **Audio Flamingo**[^AudioFlamingo]).
这迫使级联口语对话模型直接处理输入语音.
一个常用方法是整合冻结的语音编码器 (如 **Whisper**[^Whisper]) 与可训练的编码器适配器, 允许语音输入被解释为特定的文本形式, 由大型语言模型进行处理.
通过扩展基于文本的对话模型的词表, 大语言模型能够将语音作为一种独特形式的文本来处理, 从而在级联口语对话模型生成适当的文本响应.

值得注意的是, 这些级联口语对话模型不局限于单独理解人类语音, 还能够理解多种音频模态, 包括音乐和音频 (**LTU-AS**[^LTU-AS], **SALMONN**[^SALMONN]).
- **SALMONN**[^SALMONN] 通过冻结 **Whisper**[^Whisper] 和 **BEATs**[^BEATs] 编码器并将它们通过窗口级别 Q-Former (**BLIP-2**[^BLIP2]) 与大型语言模型进行桥接来建模语音和音频信息.

因此, 这些级联口语对话系统能够在理解方面执行广泛的任务.

- 如 **Qwen2-Audio**[^Qwen2-Audio], **Qwen-Audio**[^Qwen-Audio] 的模型能够处理多种任务, 例如自动语音识别, 语音到文本翻译, 自动音频字幕, 声学场景分类, 语音情感识别, 音频问答, 歌声分类和音乐音符分析.

因此, 这些级联模型通常被视为多任务语音-文本大语言模型的一部分.

值得注意的是, 上述级联口语对话模型仅生成文本, 然后直接将它们输入到预训练的 TTS 模块.
然而, 最近的级联口语对话模型, 例如 LLaMA 3.1, 开始将可训练的文本转语音模块作为大语言模型的编码器的一部分进行集成.
尽管这些模型在集成低延迟流式功能方面取得了进展, 但它们仍然首先生成文本内容然后转化为语音.
它们并不直接将语音相关的表示输入到 LLM 内部.
因此, 我们将这些模型分类为级联口语对话系统.

此外, 一些近期的模型 (如 **Qwen2-Audio**[^Qwen2-Audio]) 尝试通过融入多模态理解能力, 从而实现一定程度的多模态对话功能.
- 模型如 **VITA**[^VITA] 和 **Baichuan-Omni**[^Baichuan-Omni] 将图像, 音频和视频的编码器或 Tokenizer 集成到 LLM 中, 使得模型能够理解多模态输入并生成相应的文本响应.

上述发展涉及到级联口语对话模型的理解方面.

在生成方面, 级联口语对话系统有两种主要的语音合成工作.
- 近年来先进语音合成系统能够基于文本输入生成高度表现力和自然的音频.
   - **VALL-E**[^VALL-E];
   - **VALL-E X**[^VALL-E-X];
   - **MegaTTS** [^MegaTTS];
   - **MegaTTS2** [^MegaTTS2];
   - **CosyVoice**[^CosyVoice];
   - [ChatTTS [Github]](https://github.com/2noise/ChatTTS);
   - [FishSpeech [Github]](https://github.com/fishaudio/fish-speech);
   - **ParlerTTS**[^ParlerTTS];
   - **MaskGCT**[^MaskGCT];
   - **F5-TTS**[^F5-TTS].
- 文本风格可控的 TTS 系统方面也取得了显著进展.
   - **TextrolSpeech**[^TextrolSpeech];
   - **PromptTTS**[^PromptTTS];
   - **PromptTTS2**[^PromptTTS2];
   - **InstructTTS**[^InstructTTS];
   - **ControlSpeech**[^ControlSpeech].

这些文本转语音系统能够基于级联口语对话模型生成的文本内容和风格生成高度自然的音频.

### 2.3·端到端口语对话模型

理想情况下, 端到端口语对话模型应该训练和推理时仅使用语音输入和输出, 从而实现多种智能对话功能.
然而, 考虑到语音模态相对于文本模态是低密度模态 (包含大量声学信息), 且可用的文本数据量远远超过可用的语音数据, 许多端到端的口语对话模型选择将语音模态和文本模态对齐来利用预训练语言模型.
因此, 如[图 02](#Fig.02) 所示, 只要大语言模型能够直接理解和生成语音表示, 我们将此类系统归类为端到端口语对话模型.
相反, 如果大语言模型只能够生成文本, 我们将该系统归类为级联口语对话系统.

最早的端到端口语对话系统可以回溯到 **dGSLM**[^dGSLM], 该系统基于数千小时的二轨数据 (**Fisher Corpus**[^Fisher]) 训练, 使用自注意力和交叉注意力机制来模拟双工交互.
尽管 dGSLM 缺乏与 LLMs 的集成, 甚至缺乏基本的文本智能, 但它作为第一个完全端到端的口语对话系统, 不依赖文本的同时保持了出色的对话交互性, 具有重要意义.

随着 **dGSLM**[^dGSLM] 的发布, 端到端口语对话模型的发展停滞了几个月.
然而, 随着 ChatGPT 的出现, 这一领域迅速发展.
- 一个代表性的方法是 **SpeechGPT**[^SpeechGPT], 通过使用 {输入语音 Token, 输入文本 Token, 响应文本 Token, 响应语音 Token} 的序列进行自回归语言建模.
- 这种方式使得文本智能能够直接生成语音 Token, 启发了后续的端到端口语对话系统, 如 **Spectron**[^Spectron], **SpeechGPT-Gen**[^SpeechGPT-Gen], [**GLM-4-Voice** [Github]](https://github.com/THUDM/GLM-4-Voice) 和 **EMOVA**[^EMOVA].
这些系统仍然使用自回归框架, 先生成文本 Token 再生成语音 Token.

尽管这种方法允许 LLM 直接生成语音 Token, 但它引入了延迟问题, 因为语音 Token 生成只能在文本 Token 生成完成后开始.
这导致多轮对话和整体系统延迟问题.

除了 **SpeechGPT**[^SpeechGPT] 的设计之外, 另一种直观的方法是直接使用 LLM 的 softmax 层之前的隐藏状态通过不同映射层来预测文本 Token 和语音 Token.
这使得网络可以在映射层之前共享权重, 从而对齐语音和文本模态.
- 例如, **PSLM**[^PSLM] 是典型的这种设计.
- Meta 提出的另一种方法是交错方法, 如 **SpiRit-LM**[^SpiRit-LM], 其中语音和文本序列被拼接成单个 Token 流, 并使用小型手工制作的语音-文本并行语料库进行词级交错方法训练.
  然而这种方法要求语音和文本的精确对齐.

近期出现了几个新的端到端口语对话系统.
- **Moshi**[^Moshi] 是基于全局-局部 Transformer 的模型, 可以从多层量化器同时生成文本和语音声学 Token.
  从基于文本的语言模型骨干开始, Moshi 从神经音频编解码器的残差量化器中生成音频 Token 并对用户的语音和系统响应进行并行流建模.
  这一设计消除了显式切换说话人的需要并允许建模任意对话动态.
  此外, Moshi 通过先预测时间对齐的文本 Token 作为音频 Token 的前缀来扩展先前的层次化语义转声学 Token 生成.
- **Mini-Omni**[^Mini-Omni] 类似地使用基于 **MusicGen**[^MusicGen] 的方法来同时生成文本和语音编解码器 Token.
  它引入了两种策略: 通过填充文本 Tokens 进行无严格时间对齐的自回归生成和批量并行推理策略以提升性能.
- **Mini-Omni2**[^Mini-Omni2] 通过整合多模态理解和双工功能进一步增强了这一方法.

**LLaMA-Omni**[^LLaMA-Omni], **Freeze-Omni**[^Freeze-Omni], **IntrinsicVoice**[^IntrinsicVoice] 设计了 LLM 以进行实时声音交互.
它们的共同点在于在生成阶段, LLM 的隐藏状态被进一步输入到对应的解码器模型中.
- **LLaMA-Omni**[^LLaMA-Omni] 继承了预训练语音编码器, 语音适配器, 大语言模型和流式语音解码器.
它消除了语音转写的需要, 并能够直接从语音指令中以低延迟直接生成文本和语音响应.
- **Freeze-Omni**[^Freeze-Omni] 设计了三阶段训练策略用于语音输入和输出的建模, 使其仅通过使用文本-语音配对数据即可获得语音到语音的对话能力.
其核心思想是将口语对话模型的功能转移到编码器 (ASR) 和解码器 (TTS) 中, 而不是将其分配给大型语言模型.
- **IntrinsicVoice**[^IntrinsicVoice] 通过减少文本和语音之间的模态差距, 促进了预训练 LLMs 文本能力迁移到语音模态
  它通过使用 GroupFormer 从 LLM 的隐藏状态生成 HuBERT Token, 有效地将语音序列缩短到与文本序列相当的长度, 生成高质量音频的同时显著加快推理速度, 并缓解了长文本建模问题.

此外, 一些端到端口语对话模型通过多阶段训练对齐语音和文本, 从而在推理过程中无需生成文本.
- **OmniFlatten**[^OmniFlatten] 通过模态对齐, 半双工对话学习和全双工对话学习, 一起文本和语音 Token 的扁平标准化, 实现了推理过程中的双工无文本语音对话.
- 类似的方法包括 **SyncLLM**[^SyncLLM].

### 2.4·小结

在本节中, 我们提供了当前端到端口语对话系统的概述.
然而, 这些系统在语音表示, 训练范式, 模型架构, 生成策略上存在显著差异.
在[第 3 节](#Sec.03) 和[第 4 节](#Sec.04), 我们将进行详细分类, 并在每节的末尾进行讨论.

## 3·口语对话模型中的表示技术

**表示 (Representations)** 在口语对话系统中扮演者至关重要的角色, 因为它们决定了语音对话系统如何理解, 处理和生成语音信号.
此外, 它们作为语音和其他模态之间的桥梁, 因此直接影响系统的性能, 功能和应用范围.
和文本表示与视觉表示相比, 语音表示具有独特的复杂性.
- 文本表示主要依赖于定义明确的符号系统, 通过结构化元素 (如词汇和句法) 传达含义.
- 视觉表示侧重于捕捉图像中的空间关系和视觉特征.
- 语音信号包含动态声学特征 (如音色, 韵律和情感) 和丰富的语义内容, 要求相应的表示不仅捕获时序变化而且还能保留对底层含义的理解.

语音的独特性促使了两种表示模型的发展.
由这两种建模方法获得的表示通常被分类为**语义 (Semantic) Token** 和**声学 (Acoustic) Token**.

1. 语义类是基于预测的建模. 这些模型被训练以进行表示学习, 如以自回归方式预测未来帧 (**VQ-APC**[^VQ-APC]; **DNNSeg**[^DNNSeg]) 或使用周围帧来预测被掩膜的帧 (**Audio ALBERT**[^AudioALBERT]; **HuBERT**[^HuBERT]; **Mockingjay**[^Mockingjay]).
   这种方法倾向于优先捕捉语音中的语言信息, 特别适用于识别和理解任务.
2. 声学类专注于语音压缩和重建 (**WavTokenizer**[^WavTokenizer]; **EncCodec**[^EnCodec], **DAC**[^DAC], **SoundStream**[^SoundStream]).
   这些模型将语音特征量化为一系列离散 Token (通过编码器对原始波形进行下采样), 然后使用解码器来上采样这些离散 Token 为语音, 计算和原始信号之间的重构损失.
   通过这种方法, 我们可以获得具有惊人压缩率和高保真度声学信息的离散声学 Token, 更适合例如语音合成和情感分析等任务.

在口语对话系统中, 如图 2 所示, 不同口语对话模型对于表示选择采用不同的方法.
在接下来的部分, 我们将枚举口语对话模型中常用的语音表示, 既包括输入端, 也包括输出端.
在最后, 我们将详细讨论这些表示的优势和局限性, 以及在口语对话模型中使用的语音表示的未来趋势.

### 3.1·输入端的语音表示

#### 语义类

为了增强语言模型对理解语音表示的能力和在输入时对齐多模态数据, 使用预训练模型, 如 **Wav2Vec**[^Wav2Vec], **HuBERT**[^HuBERT], **Whisper**[^Whisper], **WavLM**[^WavLM] 来从语音中提取高级语义特征已经成为许多口语对话系统的核心策略.

##### Wav2Vec, Wav2Vec2.0, XLS-R

**Wav2Vec**[^Wav2Vec] 是语音表示学习领域的基础工作, 开创了从无标注语音数据中提取自监督语音表示.
这一方法推动了语音识别, 说话人识别和其他语音处理应用等任务的技术进步.
**Wav2Vec** 采用多层一维卷积神经网络直接在原始语音波形上进行逐步提取时序语音特征.
训练通过对比学习完成: 模型选择一个 "正确" 目标 (当前语音帧), 以及多个 "错误" 目标 (负样本).
通过学习区分正样本和负样本, 模型有效地在潜在空间中学习如何表示语音特征.

**Wav2Vec2.0**[^Wav2Vec2] 作为 **Wav2Vec** 的改进版本, 引入了 Transformer 架构和掩模建模.
**Wav2Vec2.0** 量化了由 CNN 提取的潜在语音表示, 然后使用 Transformer 来建模语义信息, 类似于 **BERT**[^BERT].
它还采用对比学习目标, 要求模型从多个候选表示中区分出正确量化表示.

- **ParalinGPT**[^ParalinGPT] 旨在将情感表达纳入对话互动, 选择具有编码丰富韵律信息方面的能力的 **Wav2Vec2.0**, 有益于语音情感识别 (**Survey20221005**[^S20221005]).
具体来说, **ParalinGPT** 使用 **Wav2Vec2.0** 中间层 (第 12 层) 进行逐帧特征提取, 因为这一层在情感分析线性探测任务中表现出了最佳结果.
此外, **ParalinGPT** 应用平均池化和线性特征映射器来提取发言嵌入.

**XLS-R**[^XLS-R] 是一种基于 **Wav2Vec2.0** 架构的多语言自监督语音表示模型.
它扩展并优化了 **Wav2Vec2.0** 以支持更广泛的语言范围, 特别是低资源语言.
在跨语言训练中, **XLS-R** 采用多语言数据增强和降噪技术, 增强了模型在处理多种语言语音时的适应性.
- **USDM**[^USDM] 使用 **XLS-R** 获得 50Hz 的连续中间表示, 然后使用 $K$=10000 的量化器 (**Seamless**[^Seamless]) 生成语音 Token.

##### HuBERT

**HuBERT**[^HuBERT] 是一种常用的无监督学习模型, 对语音的 MFCC[^MFCC] 特征进行 K-Means 聚类, 为每帧分配伪标签.
它使用卷积编码器从 16kHz 采样率的语音以 20ms 的帧率生成特征序列.
最后, 它随机掩膜连续帧的一部分特征作为 **Transformer**[^Transformer] 的输入.
HuBERT 基于周围的上下文生成掩膜内容, 使其能够捕获语音中的时序和语义信息并获得上下文细节的更深刻理解.

口语对话系统, 例如 **E-Chat**[^E-chat], **SpeechGPT**[^SpeechGPT], **PSLM**[^PSLM], **IntrinsicVoice**[^IntrinsicVoice] 广泛使用了 HuBERT 作为它们的语音编码器.
- E-Chat 提取 HuBERT 的 24 层权重之和作为语音嵌入, 并结合一组额外的权重参数来提取情感嵌入, 从而实现情感感知的能力.
- SpeechGPT 对 HuBERT 提取的连续特征进行 K-Means 聚类来量化, 将其转化为离散单元序列.
  这些离散单元随后整合到大语言模型的词表中, 实现文本和语音模态之间的直接对齐.
- 为了更有效地将集成语言模型和语音流, PSLM 在 HuBERT 提取特征后添加了额外的嵌入层.
- IntrinsicVoice 使用 HuBERT 作为语音 Tokenizer , 将语音 Token 进行分组以减少序列长度. 然后嵌入层将这些 Token 转换为密集的嵌入, 并通过可训练的语音适配器映射到语言模型的嵌入空间中.
- **SpiRit-LM**[^SpiRit-LM] 使用 HuBERT 提取语义特征, 使用500 个基本单元的 K-Means 模型作为基础单元.
  它采用数据增强技术 [^Gat2022Augmentation] 训练了一个前馈量化器, 以生成离散语音 Token.
- **Align-SLM** [^Align-SLM] 使用 HuBERT, 聚类数 K=500.

值得注意的是, 当连续表示被聚类到离散单元时, 它们主要捕获内容信息, 这可以用于建模和理解.
这一过程首先从 HuBERT 的第 11 层提取 25Hz 帧级连续表示, 将每帧分配到最近的聚类索引, 然后消除连续相同的索引来缩短序列.

##### Whisper

**Whisper**[^Whisper] 是基于经典的编码器-解码器架构的模型, 在语音识别领域获得了广泛关注.
编码器将输入语音转换为高级特征表示, 而解码器则从这些表示生成相应的文本输出.
在大规模的各种语音环境的数据上以文本为目标进行预训练, Whisper 展示了从语音提取语义信息的强大能力.
- **Qwen-Audio**[^Qwen-Audio], **Qwen2-Audio**[^Qwen2-Audio] 使用 Whisper 的编码器将语音转换为连续表示, 然后与文本表示结合并输入到大型语言模型中.
- **Mini-Omni**[^Mini-Omni], **Mini-Omni2**[^Mini-Omni2], **LLama-Omni**[^LLaMA-Omni] 采用类似的方法, 在 Whisper 编码器之后连接了一个语音适配器.

它们共同的目标是将语音表示映射到大语言模型的文本嵌入空间, 通过词表扩展强制对齐从而增强模型理解语音的能力.

##### WavLM

**WavLM**[^WavLM] 是一种为全面语音处理任务设计的预训练模型, 在推动语音技术发展中发挥了关键作用.
具体来说, WavLM 采用了掩膜语音降噪和预测框架, 其中一些输入包含模拟噪声或带有掩码部分的叠加语音.
目标是预测掩膜区域中的原始语音的伪标签.
这种方法使得模型能够通过掩膜语音预测学习到与 ASR 相关的信息, 同时也通过语音降噪建模获得其他和非 ASR 任务相关的知识.
WavLM 中语音帧的掩码和预测流程和 HuBERT 类似.
然而, WavLM 引入额外的门控相对位置偏置来增强模型对语音中时序信息的敏感性.
- **SpeechVerse**[^SpeechVerse] 利用预训练的 WavLM Large 作为其语音编码器的骨干, 将 WavLM 的所有中间层特征编码到语义表示中, 实现更好的泛化性能.
  为了处理语音特征和文本 Token 之间的显著的长度差异, SpeechVerse 采用了一个可学习的卷积模块来对语音特征进行下采样.

##### $S^3$ Tokenizer

**CosyVoice** [^CosyVoice] 提出使用一个监督的自动语音识别模块生成**监督语义语音 Tokenizer  (Supervised Semantic Speech ($S^3$) Tokenizer)**.
和标准的 ASR 模型不同, $S^3$ Tokenizer 将编码器分为两个部分, 并在这两个部分之间引入向量量化层.
第一个编码器将梅尔频谱转化为上下文感知的表示, 而第二个编码器将离散语音单元转换为连续隐藏状态.
最后, 基于 Transformer 的 ASR 解码器预测文本标签的后验概率.
通过在多语言 ASR 任务上的监督学习, $S^3$ Tokenizer 能够将语音转换为语义上一致的 Token, 这有助于促进语音理解和生成.
- **OmniFlatten**[^OmniFlatten] 使用 $S^3$ Tokenizer 提取离散语音 Token, 并直接将其输入到文本语音预训练的 Transformer 中.

##### SPIRAL

**SPIRAL**[^SPIRAL] 旨在从语音数据中学习对噪声和扰动健壮的表示.
它使用了教师-学生网络, 其中对学生模型的语音输入应用了各种扰动, 例如加噪, 增益调整, 以及时频扭曲.
教师模型随后引导学生模型在这些扰动下生成一致的表示.
- **EMOVA**[^EMOVA] 利用 SPIRAL 的架构作为语音编码器处理语音, 并采用**有限标量量化 (Finite Scalar Quantization, FSQ)**[^FSQ] 来离散这些特征.
这一过程将语音和文本词表对齐, 从而更自然地集成到 LLM 中.

##### 其他

一些口语对话系统并没有使用预训练的表示模型, 它们通过堆叠基础模块来处理输入特征.
- **VITA**[^VITA] 首先使用梅尔滤波器组分解语音信号, 模拟人类对声音的非线性感知.
  然后, 使用四层卷积神经网络下采样模块和 24 层 Transformer 来处理输入特征.
  为了和后续的语言模型对齐, VITA 采用了一个简单的两层 MLP 作为适配器.
- **Freeze-Omni**[^Freeze-Omni] 使用分块流式语音编码器将输入语音特征转换为高维表示.
  然后, 适配器模块将这些高维表示映射到主 LLM 的嵌入空间, 确保对输入语音的快速, 低延迟响应.
  语音编码器模块由数个下采样卷积层和 Transformer 块组成, 而适配器只包含几个下采样卷积层.
  下采样层用于减少语音特征的帧率, 在预填充阶段提高 LLM 的处理速度, 并减少延迟.

#### 声学类

考虑到语义特征不足以捕获语音的情感, 音色和风格,
- 一些表示模型如 **Emotion2Vec**[^Emotion2Vec] 尝试通过自监督训练提取声学信息.
- 其他模型则着重于重建目标以确保高保真语音, 包括 **Encodec**[^EnCodec], **SpeechTokenizer**[^SpeechTokenizer], **Mimi**[^Moshi] 等.

##### EnCodec

**Encodec**[^EnCodec] 是一种直接, 流式, 卷积式的编码器-解码器架构.
原始语音通过一系列卷积层进行下采样, 将其映射到潜在特征表示.
残差向量量化 (**SoundStream**[^SoundStream]) 随后将编码器的连续潜在特征离散化.
量化目标是将来连续特征映射到离散 Token 的预定义集合 (称为 "码本") 进行后续压缩和传输.
解码器通过连续的反卷积层将离散特征恢复为接近原始语音的波形.
- LauraGPT [^LauraGPT] 采用了 EnCodec 的增强版本作为其语音编码器, 进行了特定修改:
  (1) 在幅度谱域中添加重构损失以提高中到高频信号质量;
  (2) 堆叠五个步长为 (8, 5, 4, 2, 2) 的卷积块以处理长序列长度的挑战, 实现每 Token 组的码率为 25Hz;
  (3) 在 Residual Vector Quantization (RVQ) 模块中使用 32 个量化器, 其词汇大小为 1024, 并采用结构性 Dropout.
  这种改进通过引入更多量化器来提高语音质量, 同时保留了浅层量化器中的大部分信息.
  LauraGPT 最终从第一个量化器层的输出中选择语音 Token, 在性能与序列长度效率之间取得平衡.
  剩余的量化器仅在编码器-解码器模型训练时使用.

##### SpeechTokenizer

**SpeechTokenizer**[^SpeechTokenizer] 统一了语义和声学 Token, 在不同的 RVQ 层上分层次地分解语音信息地不同方面.
它建立在 RVQ-GANs 框架之上, 遵循与 **SoundStream**[^SoundStream] 和 **Encodec**[^EnCodec] 相同的模式.
值得注意的是, SpeechTokenizer 替换了 EnCodec 编码器中原本跟随卷积块地两层 LSTM 为两层 BiLSTM, 以增强语义建模能力.
鉴于 HuBERT 在编码大量内容信息方面的能力 (**Survey20220521**[^S20220521]), SpeechTokenizer 使用 HuBERT 作为语义教师.
在训练时, 它引入了两种蒸馏方法: 连续表示蒸馏和伪标签预测.
- 连续表示蒸馏: SpeechTokenizer 采用 HuBERT 第 9 层表示或 HuBERT 所有层的平均表示作为语义教师.
  训练目标是在所有时间步上最大化 RVQ 第一层输出与语义教师表示之间的维度级余弦相似度.
- 伪标签预测: SpeechTokenizer 采用 HuBERT 单元作为目标标签.

- 在对话系统中, SpeechGPT-Gen 使用 SpeechTokenizer RVQ-1 处理原始语音, 主要增强了大型语言模型对语音语义建模能力.

##### Mimi

受到先前 SpeechTokenizer 工作的启发, **Mimi**[^Moshi] 使用蒸馏将非因果的, 高级语义信息迁移到由因果模型产生的 Token 中, 从而实现语义-声学 Token 的流式编码和解码.
为了提升 Mimi 编码语音为紧凑表示并重建高质量语音的能力, 在编码器和解码器中添加了 Transformer 模块.
Mimi 使用 WavLM 蒸馏 RVQ-1, 丰富其语义信息.
值得注意的是, 进行蒸馏显著增强了第一个量化器的语音区分能力, 然而, 也可能导致语音质量下降.
Mimi 假设这是由于将语义信息蒸馏到单个 RVQ 第一级: 由于高阶量化器操作的是第一级的残差, 后者需要权衡语音质量与音素区分能力.
Mimi 解决这一问题的方法是引入分割 RVQ 方法.
与使用单个 8 级 RVQ 不同, Mimi 提取语义信息到简单 VQ 中, 并应用并行 7 级 RVQ, 在最后将它们合并.
这消除了对语义量化器残差中必须保留声学信息的约束.
经过仔细设计, Mimi 作为 **Moshi**[^Moshi] 的语音编码器, 这种方法增强了模型的能力捕捉到语义和声学细节.

##### Emotion2Vec

**Emotion2Vec**[^Emotion2Vec] 是一种多功能的语音情感表示模型, 旨在从语音中提取情感特征.
在预训练阶段, Emotion2Vec 通过教师网络和学生网络进行在线蒸馏.
在执行特定的下游任务时, Emotion2Vec 被冻结, 并训练了一个轻量级的下游模型.
Emotion2Vec 引入了话语级损失, 以控制全局情感, 并采用帧级损失来构建帧间预训练任务, 使其能够学习上下文情绪.
- **Spoken-LLM**[^Spoken-LLM] 使用 Emotion2Vec 提取的特征作为大型语言模型的输入, 旨在使模型理解和响应情感.

### 3.2·输出端的语音表示

#### 语义类

在输出阶段, 大多数口语对话系统选择自回归地建模语义 Token, 如 $S^3$ Tokens [^CosyVoice] 和 HuBERT [^HuBERT] 单元.
值得注意的是这些语义 Token 缺乏声学条件化, 因此需要声码器 (HiFi-GAN [^HiFi-GAN]; [^Polyak2021Speech]) 或解码器, 进一步将语义离散单元作为输入, 以合成与训练期间遇到的发言人一致的语音.

##### $S^3$ Tokenizer

OmniFlatten [^OmniFlatten] 在语音输出阶段使用大语言模型 (LLM) 自回归地预测 $S^3$ Token.
当将离散 Token 转换为语音时, 它采用了 CosyVoice [^CosyVoice] 使用的最优传输条件流匹配 (OT-CFM) 模型.
OT-CFM 将语音 Token 序列转换为梅尔频谱图, 然后使用 HiFi-GAN [^HiFi-GAN] 声码器生成最终的语音.

##### HuBERT

由预训练 HuBERT [^HuBERT] 提取的语音 Token 被广泛作为口语对话系统中大语言模型的生成目标.
- **SpeechGPT**[^SpeechGPT] 和 **SpiRit-LM**[^SpiRit-LM] 使用 LLaMA [^LLaMA] 自回归地预测一系列单元, 并使用基于 HuBERT 单元的 HiFi-GAN [^HiFi-GAN] 来解码语音信号的离散表示.
- PSLM [^PSLM] 在 Transformer 层之后引入额外的语音映射层, 以处理隐藏状态, 并通过 Softmax 层获得语义 Token.
- **LLaMA-Omni**[^LLaMA-Omni] 中的语音解码器以非自回归的方式运行, 接受大语言模型的输出隐藏状态作为输入, 生成与语音响应对应的离散 HuBERT 单元序列. 离散单元可以用额外的基于单元的声码器 [^Polyak2021Speech] 转换为波形.
- **IntrinsicVoice**[^IntrinsicVoice] 引入 Group-Former 以增强大语言模型在序列建模方面的能力.当大语言模型预测 `<speech>` Token 时, 全局嵌入通过一个映射层传递和分发, 并与可学习的查询集合一起传递给组模型, 以预测单元.
  IntrinsicVoice 使用 HiFi-GAN [^HiFi-GAN], 一种非自回归神经声码器, 有效生成高质量波形, 用于语音解码以减少整体延迟.
- Align-SLM [^Align-SLM] 也使用基于 HiFi-GAN [^HiFi-GAN] 的模型将离散单元转换为波形, 使用了 textless-lib 库[^textless-lib]中的模型检查点.

##### 其他

- USDM [^USDM] 不直接根据输入语音来生成语音, 它首先将语音转录为文本, 生成响应文本, 然后在端到端流程中生成相应的语音 Token.
  通过在语音输入和输出之间插入文本相关任务, 模型在中间模态中获得了预训练 LLM 和**思维链 (Chain-of-Thought, CoT)**[^CoT] 推理的好处.
  由于流程中的每个阶段都处理前一阶段生成的所有输入和输出 Token.
  USDM 对转录出现的错误更具健壮性, 并且比使用分离模块的级联方法更能生成上下文相关的口语响应.
  USDM 使用 **VoiceBox**[^VoiceBox] 架构训练了一个单元到语音模型, 以从单元中重构语音.
- **EMOVA**[^EMOVA] 在给定图像或语音输入时生成语音单元形式的响应, 然后使用 U2S 解码器将其转换为输出波形.
  U2S 解码器遵循 VAE 架构: 它使用语音单元编码器将预测的语音单元转换为连续嵌入, 将这些嵌入与大语言模型预测的风格嵌入相结合, 以确定时长, 最后通过解码器重构语音波形.

#### 声学类

许多口语对话系统选择直接从声学表示模型 (如 EnCodec [^EnCodec], SpeechTokenizer [^SpeechTokenizer], **Mimi**[^Moshi] 生成 Token.
这些声学 Token 之后通过冻结的编解码器的解码器部分直接上采样回原始波形.

##### EnCodec

LauraGPT [^LauraGPT] 使用 Qwen-1.8B [^Qwen] 来预测语音 Token.
当合成语音时, 它除了对 LLM 预测的语音 Token 进行条件化外, 还对文本和语音输入进行条件化.
这种文本和语音条件化使模型能够利用提示和带噪语音中的丰富信息生成高质量语音信号, 而离散 Token (Encodec 的第一量化器输出) 则缺乏这种能力.
预测出的语音 Token 和条件化输入一起被送入编解码器的声码器.
仅编码器架构的 Transformer 模型将这些输入编码为稠密嵌入, 然后编解码器的解码器部分将其重建为语音信号.

##### SNAC

SNAC [^SNAC] 将语音编码为分层 Token, 类似于 EnCodec [^EnCodec] 和 DAC [^DAC], 通过引入在不同时间分辨率的量化, 形成语音的多尺度离散表示.
在这种方法中, 浅层 RVQ 层有更低的采样频率, 覆盖更宽的时间范围, 而更深的 RVQ 层在更高频率采样.
SNAC 相对于 RVQ-GAN 引入了适当的增强, 包括残差噪声块, 深度卷积, 局部窗口注意力.
- **Mini-Omni**[^Mini-Omni], **Mini-Omni2**[^Mini-Omni2] 系列是 [^MusicGen] 引入的并行生成方法的延续, 利用 SNAC [^SNAC] 作为语音编码器, 由七个互补的 Token 层组成.
  在单步中, 它生成八个 Token, 包括文本, 而各层之间的延迟保持为一步.
  此外, Mini-Omni 和 Mini-Omni2 还采用批处理方法, 其中包含两个样本: 一个需要文本和语音响应, 另一个仅需要文本响应.
  通过丢弃第一个样本的文本 Token, 将第二个样本的输出嵌入到第一个样本中, 实际上将模型的文本功能转移到语音任务中, 显著增强推理能力, 最小化资源开销.

##### SpeechTokenizer

在输出侧, **SpeechGPT-Gen**[^SpeechGPT-Gen]使用**流匹配 (Flow Matching)**[^FlowMatching] 合成语音 Token.
流匹配有效地建模了从简单先验分布到复杂数据分布的转换, 从而在语音生成中获得良好结果.
**SpeechGPT-Gen**[^SpeechGPT-Gen] 应用流匹配进行感知建模, 生成与 SpeechTokenizer [^SpeechTokenizer] 语音 Token 匹配的语音 Token.
具体来说, 给定语音 $S$, 语义表示 $V_1$, 感知表示 $V_{2:8}$ 和由 SpeechTokenizer 提取的完整信息表示 $V_{1:8} = V_1 + V_{2:8}$, 感知建模是根据提示语音 $a$ 和语义表示 $V_1$ 预测完整表示 $V_{1:8}$.
SpeechGPT-Gen 通过将 **SpeechGPT**[^SpeechGPT] 的输出与提示语音连接起来, 并使用流匹配模型合成响应语音.

##### Mimi

*Mimi**[^Moshi] 有 8 个码本, 帧率为 12.5Hz, 需 100 个自回归步骤生成 1 秒语音.
这导致了高计算成本和与流式推理不兼容.
为了解决这些问题, **Moshi**[^Moshi] 提出了 RQ-Transformer, 由一个时序 Transformer 和一个深度 Transformer 组成.
RQ-Transformer 将长度为 $K \cdot S$ 的扁平序列分解为 $S$ 个时间步, 用于一个大的时序 Transformer, 产生一个上下文嵌入用于对 $K$ 个步骤的小深度 Transformer 进行条件化.
这使得扩展到更长的序列成为可能, 因为可以增加 $S$ 或增加 $K$ 而不用模型扁平序列.

##### TiCodec

**TiCodec**[^TiCodec] 是一种解耦的编解码器模型, 可以分离语音中的时间变化和不变的信息, 并分别量化它们.
- 受 VALL-E [^VALL-E] 的启发, Freeze-Omni [^Freeze-Omni] 使用基于 Token 的语音解码器, 其中包含 NAR 预填充和 AR 生成阶段, 实现语音输出能力.
语音解码器主要由 NAR 解码器, AR 解码器和编解码器模型 (**TiCodec**[^TiCodec]) 的冻结解码器组成 .
NAR 解码器和 AR 解码器都基于 Transformer 块.
NAR 解码器用于从 LLM 的输出中建模语义特征, 然后 AR 解码器基于 NAR 解码器的输出生成语音 Token.
最后, 编解码器模型的解码器将语音 Token 转换为语音流.

### 3.3·相关讨论

#### 语义表示对比声学表示

现有的对话系统通常根据任务需求选择不同的方法进行理解 (输入) 和生成 (输出).
- SpiRit-LM[^SpiRit-LM] 在输入和输出端都使用语义表示 (HuBERT [^HuBERT])
- **Mini-Omni**[^Mini-Omni] 在输入端使用语义表示 (Whisper [^Whisper]) 而在输出端使用声学表示 (SNAC [^SNAC])

每种组合都提供了独特的优势和权衡, 而在实际应用中达成统一的语音表示方法还没有达成共识.

我们回归语义表示和声学表示的区别, 如表格 01 所示

![Images/S20241115_Tab.01.png](Images/S20241115_Tab.01.png)

语义表示的优势:
- 受益于具体的任务目标, 如 Wav2Vec [^Wav2Vec], HuBERT [^HuBERT], WavLM [^WavLM], Whisper [^Whisper] 等模型都专注于从说话内容中提取语义信息.
这种内在优势使得**语音可以直接映射到大语言模型的嵌入空间中, 有助于和其他模态对齐, 并充分利用大语言模型的强项**.
与之相反, 由模型 (例如 EnCodec [^EnCodec], DAC [^DAC]) 提取的声学表示不利于语言模型理解, 这也是为什么 SpeechTokenizer [^SpeechTokenizer] 和 **Mimi**[^Moshi] 选择使用语义蒸馏.
- 此外, **语义表示提供更高的压缩率**.
通过在卷积层中配置不同的下采样参数, 模型 (例如 HuBERT 和 Whisper) 能够轻松实现 25Hz 到 50Hz 的帧率.
例如 SpiRit-LM[^SpiRit-LM] 采用 25Hz HuBERT 单元, 这意味着只需要 25 个标记来表示一秒的语音.
与之相反, 声学特征是为了压缩和重建而设计的, 信号传输的限制使得极限压缩和高质量重建难以同时实现.
尽管 **Mimi**[^Moshi] 已经实现了 12.5Hz 的帧率, 但它使用 8 个码本, 这意味着自回归地预测一秒的语音需要 100 步.
- 最后, **在某些情况下, 语义表示具有独特的优势**.

声学表示的优势:
- 然而, 我们必须承认纯粹的语义表示在自然性和表现力方面存在缺陷, 特别是在涉及到情感表达或复杂语音动态的任务中, 而这些任务中的声学表示能提供更多细致的信息.
  HuBERT [^HuBERT] 不能像 EnCodec [^EnCodec] 或 Emotion2Vec [^Emotion2Vec] 那样有效地提取语调和风格特征.
- 值得注意的是, 使用声学表示可以灵活地处理各种数据类型——语音, 音频, 音乐, 声音——这使得对话系统更加统一和多样化.
- 当声学表示用作语言模型的输出时, 它可以无缝地连接到编解码器的解码器部分以进行语音合成.
  与之相反, 使用语义特征的对话系统通常要求单独训练好的声码器 ([^SpiRit-LM], [^USDM]) 或依赖额外的文本到转语音工具箱 (**LLaMA-Omni**[^LLaMA-Omni])
  这种差距对于对话系统至关重要, 因为其导致的延迟会直接影响用户体验.

鉴于语义特征和声学特征在不同任务中的独特优势, 未来研究可能会转向集成这些特征.
一个有价值的视角是, 模型如 SpeechTokenizer [^SpeechTokenizer] 和 **Mimi**[^Moshi] 已经试图将语义表示从 HuBERT [^HuBERT] 或 WavLM [^WavLM] 中蒸馏到 RVQ-1, 确保系统中语义和声学信息的平衡.

随着技术进步, 我们期待着更加统一和完善的模型方法.
一个有希望的方向是为语音分词器设计新的训练目标, 探索数据驱动和目标驱动方法, 避免使用额外的预训练模型.
由于对话系统仍在不断发展, 探索更加健壮的混合表示是有价值的.

#### 连续表示对比离散表示

现在口语对话系统仍无该选择连续表示还是离散表示的共识.
输入侧的考虑主要依赖于系统选择的表示模型的类型.
- 一些系统 (**Mini-Omni**[^Mini-Omni], **Mini-Omni2**[^Mini-Omni2], **LLaMA-Omni**[^LLaMA-Omni]) 使用模型 (HuBERT [^HuBERT] 或 Whisper [^Whisper]) 来提取连续语音表示, 要求增加一个语音适配器和额外的专注于模态对齐的训练阶段.
- 另一些系统 (**SpeechGPT**[^SpeechGPT], **EMOVA**[^EMOVA], **Moshi**[^Moshi]) 使用模型 (EnCodec [^EnCodec] 或 **Mimi**[^Moshi]) 来提取离散语音表示, 将语音 Token 直接添加到大语言模型的词表中, 从而将训练负担移到大语言模型本身.

尽管使用了不同方法, 但关键是使得大语言模型能有效地理解语音特征.
对于自回归模型, 使用离散输入可能更容易管理, 然而, 是否真的在性能方面优于连续输入仍有待探索.

使用下一个 Token 目标进行训练的语言模型倾向于离散模态.
- 在输出侧使用离散特征自然地支持简单的编解码器的解码器 (**Mini-Omni**[^Mini-Omni], **Mini-Omni2**[^Mini-Omni2], **Mimi**[^Moshi], [^Freeze-Omni]) 以重构高保真度语音, 增强语音质量和声学控制的同时, 实现端到端系统.
相比之下, 连续特征可能需要额外的文本转语音工具箱 ((**VITA**[^VITA]) 或声码器 (**LLaMA-Omni**[^LLaMA-Omni]), 变成级联流程使得难以保留详细的声学信息.
- 使用离散表示的另一个优势是能够快速地将其输入到下一轮对话轮次中, 如 [^OmniFlatten].

在计算机视觉领域, 涌现了一系列工作 ([^Transfusion], [^Show-o]), 试图将离散和连续表示结合, 目的是在无信息损失的情况下完全整合这些模态, 并且已经在某些领取取得了成功.
这些方法可能为下一代口语对话模型提供了有价值的见解.

#### 单层量化器对比多层量化器

如前所述, 关于压缩率, 在使用语音编解码器时量化器的数量必须经过仔细考虑.
目前, 对话系统通常使用多层量化器, 如 EnCodec [^EnCodec], SpeechTokenizer [^SpeechTokenizer], SNAC [^SNAC], **Mimi**[^Moshi].
这不可避免地引入了生成延迟, 因为残差向量量化要求每个量化器的输入依赖于前一个量化器的输出.

**Mini-Omni**[^Mini-Omni] 和 **Mini-Omni2**[^Mini-Omni2] 采取了与 MusicGen [^MusicGen] 类似的策略, 引入延迟步骤以实现多个量化器之间的并行生成.

**Moshi**[^Moshi] 提出将残差向量量化拆分, 使得八个向量量化器能够独立并行生成.
这些策略在一定程度上缓解了延迟问题, 但仍未达到语义表示的效率水平.

最近, 关于单层量化器的研究已经取得了令人鼓舞的成果.
诸如 **WavTokenizer**[^WavTokenizer], Single-Codec [^Single-Codec], and BigCodec [^BigCodec] 等模型主张使用单层 VQ 对语音进行离散化, 取得了在重构和生成任务中的具有竞争力的结果.
值得注意的是, WavTokenizer [^WavTokenizer] 已经实现了 40Hz 的压缩率.
将单层量化器与对话系统集成是有希望的, 因为它允许在输入侧快速提取语音特征, 并且大大减少了自回归建模的负担.

#### 文本引导对比无文本引导

实践中, 研究人员发现直接进行语音到语音生成具有挑战性 (**Mini-Omni**[^Mini-Omni], **Mini-Omni2**[^Mini-Omni2], **LLaMA-Omni**[^LLaMA-Omni]), 这是由于复杂的映射关系, 因此常常先生成中间文本来实现更高的生成质量.
当前的端到端对话系统通常采用以下两种策略之一:
1. 一种策略 (**LLaMA-Omni**[^LLaMA-Omni], **IntrinsicVoice**[^IntrinsicVoice]) 是首先生成和文本响应对应的隐藏状态, 然后再进行后处理以获得语音 Token.
2. 另一种策略 (**Mini-Omni**[^Mini-Omni], **Mini-Omni2**[^Mini-Omni2], **Moshi**[^Moshi]) 是并行生成文本和语音 Token.

这些方法利用了大语言模型的文本建模能力, 本质上是通过先生成文本来引导语义一致的语音合成.
然而, 这带来了响应速度的开销.

尽管直接应用语音到语音的生成面临模型复杂度增加和推理困难等挑战, 但我们仍然相信它是未来研究的有希望的方向.
- 一种方法是重新训练大型口语对话模型以适应具体的语音表示.
然而, 这面临与数据资源相关的挑战, 因为大规模和高质量的对话数据仍然稀缺.
此外, 这种方法不能完全消除文本提示, 要求多阶段训练, 首先从文本-语音对开始, 使得模型逐渐获得对话能力.
- 另一种方法可以从语音编解码器入手, 如 SpeechTokenizer 和 Mimi 在语义蒸馏方面的广泛工作.
  我们设想一种新颖的语音编解码器, 在编码阶段对齐文本和语音, 从而减轻大型语言模型的生成负担.
  通过在过程的早期对齐语音表示与文本表示空间, 自回归建模将不再需要文本引导, 从而产生了一个全新的对话系统范式.

## 4·口语对话模型的训练范式

现有的基于文本的大语言模型已经在自然语言处理领域展现出了强大的上下文理解和推理能力, 如 **GPT-4**[^GPT-4], **LLaMA3.1** [^LLaMA3] 和 **Qwen-2**[^Qwen2].
由于它们在大规模语料库上进行训练, 这些模型在处理复杂上下文时取得了卓越的准确性.
为了进一步扩展大语言模型的能力, 一些研究 (**EMOVA**[^EMOVA], **Qwen2-Audio**[^Qwen2-Audio], **VITA**[^VITA], **Mini-Omni2**[^Mini-Omni2]) 探索了让大语言模型理解其他模态的可能性, 从而构建多模态交互能力.

口语对话模型, 也称为语音-文本对话模型, 允许用户通过语音以自然且直接的方式与 LLMs 进行交互.
然而, 从文本智能到语音只能的转变, 涉及到两个内在的障碍:
一个核心的问题是相比于预训练的基于文本的大语言模型所使用的海量数据集相比, 缺乏足够的语音数据.
- **LLaMA3.1**[^LLaMA3] 使用 8000 亿个训练 Token, **Qwen-2**[^Qwen2] 在超过 7 万亿个 Token 上训练, 而纯语音预训练数据往往只占数十万或数百万小时.
- **Moshi**[^Moshi] 的预训练语音数据包含 7 百万小时, 而标注语音数据却很少, 使得它很难支持 LLMs 在语音智能方面取得与基于文本的模型相当的能力.

另一个挑战是语音信息密度不如文本紧凑.
文本通常使用**字节对编码 (BPE)** [^Gage1994New], [^BPE] 将其压缩到紧密的 Token 空间中, 而语音模态不仅包含语义信息, 还包含声学信息, 其密度较低.
这无疑增加了 LLMs 学习的难度.

更有效地理解和生成语音模态的内在知识是一个重大挑战.

因此, 现有的口语对话模型旨在通过将语音模态引入大语言模型中, 从而在这些基于文本的大语言模型基础上进行构建.
- **SpeechGPT**[^SpeechGPT], **EMOVA**[^EMOVA], **Mini-Omni**[^Mini-Omni], **Moshi**[^Moshi] 支持 LLMs 的语音输入和输出功能, 形成了基本的语音对话能力的基础.
- 一些最新的先进方法 (**Moshi**[^Moshi], **OmniFlatten**[^OmniFlatten], **SyncLLM**[^SyncLLM]) 试图从传统的基于轮次的口语对话系统转变为全双工系统, 旨在模拟人类对话的自然随意性.

尽管这些进展前景广阔, 但在全双工系统中实现低延迟和自然交互的能力仍然是一个重要挑战.
此外, 增强 LLMs 以有效地处理语音模态, 即掌握语音理解和生成, 同时保持稳健的自然语言文本处理能力, 受到标注语音数据集规模有限的阻碍.
与大量可用的纯文本数据相比, 这些数据集的规模要小得多, 这可能会削弱模型原有的文本处理能力.

因此, 构建一个真正满足现实需求的端到端对话模型, 需要在模型架构, 训练范式, 训练数据方面进行仔细的考虑.

总的来说, 我们认为口语对话模型的训练范式中有几个关键方面至关重要:
- 使语音-文本模态一致, 以确保一致理解;
- 设计多阶段训练策略, 逐步适应;
- 优化训练结构和推理范式, 以实现高效性能.

### 4.1·语音文本模态对齐的架构范式

为了使得大语言模型能够处理语音输入和输出, 大量先前工作 (**AudioPaLM**[^AudioPaLM]; **LLaMA3.1**[^LLaMA3], **LLaMA-Omni**[^LLaMA-Omni], **Mini-Omni**[^Mini-Omni], **Moshi**[^Moshi]) 都集中在将基于文本的基础模型转化为健壮的口语对话模型.
基于不同的架构范式, 这些方法可以大致分为五类, 如图 05 所示.

![Images/S20241115_Fig.05.png](Images/S20241115_Fig.05.png)

#### 仅输出文本

这些系统 (Qwen2-Audio[^Qwen2-Audio], Qwen-Audio[^Qwen-Audio], LTU-AS[^LTU-AS], E-Chat[^E-chat], SALMONN[^SALMONN], WavLLM[^WavLLM], SpeechVerse[^SpeechVerse], VITA[^VITA]) 保持了基于文本的大语言模型的基础架构不变, **使用音频编码器和适配器将语音输入直接映射到大语言模型预训练的文本潜在空间**.
这种直接嵌入对齐的方法, 和多任务训练策略相结合, 使得大语言模型具备了听的能力, 从而能够有效地理解和处理语音模态输入, 并在各种音频理解任务中表现出色.
然而, 输出仍然是基于文本的, 这需要使用外部的文本转语音系统 ([^XTTS], [^CosyVoice]) 来生成语音输出.
- LTU-AS [^LTU-AS] 使用 Whisper [^Whisper] 与时间和层级 Transformer (TLTR) 作为音频编码器, 使其能够识别语音和音频事件.
- Qwen-Audio [^Qwen-Audio] 将音频-语言预训练扩展到覆盖超过三十个任务和各种音频类型, 促进了通用音频理解能力. 它对所有的音频输入采用了统一的编码器, 弥合了音频和文本模态之间的差距, 并使用大语言模型 Qwen-7B [^Qwen] 为基础组件.
- Qwen2-Audio [^Qwen2-Audio] 通过为不同数据和任务使用自然语言提示来简化预训练过程, 并使用 DPO [^DPO] 优化模型在真实性和遵循期望行为方面的表现.
- SALMMON [^SALMONN] 采用了双听觉编码器: 来自 **Whisper** 模型的语音编码器和非语音的 BEATs [^BEATs] 音频编码器. 这两个编码器的听觉特征互补, 使其适合用于包含语音和非语音信息的通用音频输入. 这些输入随后通过 Q-Former 风格注意力机制连接到一个经过良好训练的 LLM 以生成响应.
- **VITA**[^VITA] 通过两个独立的模块实现全双工方案: 一个模块生成对用户查询的文本响应, 另一个持续监控环境输入以有选择地提供更新的交互内容, 尽管它仍然需要额外的 TTS 系统.

上述提及的方法经常忽略副语言信息, 包括情感, 韵律, 和非语言元素, 这使得它们在涉及情感语音对话的场景中表现不足.

- ParalinGPT [^ParalinGPT] 使用 ASR 模型获取文本, 并使用语音编码器提取情感嵌入, 从而更准确地模拟语音响应的语言内容和副语言属性.
- E-chat [^E-chat] 采用 [^HuBERT] 语音编码器提取语音和情感特征, 使用连接模块将这些特征映射到 LLM 解码器中的文本空间.

尽管这些方法已经探索了口语对话系统的情感响应, 它们要求额外的系统来从文本合成语音并面临高延迟问题, 使得实时对话难以实现.

#### 模态链

这种方法将语音分词为离散 Token 并扩展大语言模型的词表以处理语音输入和输出.
为了处理语音和文本模态之间的对齐问题, 近期工作 (**SpeechGPT**[^SpeechGPT], [^SpeechGPT-Gen], [^Spectron], **EMOVA**[^EMOVA]) 利用了名为模态链 (Chain-of-Modality, CoM) 的提示方法, 首先自回归地生成响应文本然后生成相应的语音.
这种技术允许文本大语言模型的输出来引导语音生成, 从而增强响应内容的质量.
然而, 这不适合现场交互, 因为模型必须完成整个文本响应才能开始语音生成, 这导致响应延迟增加.
- **SpeechGPT**[^SpeechGPT] 和 **SpeechGPT-Gen**[^SpeechGPT-Gen] 采用 SpeechTokenizer [^SpeechTokenizer] 模型作为语音 Token 提取器, 将语音生成分解为语义 Token 预测和音频 Token 预测.
- **EMOVA**[^EMOVA] 使用 FSPIRAL [^SPIRAL] 架构作为其语音编码器, 捕捉语音的音素和音调信息, 然后使用**标量向量量化 (FSQ)** [^FSQ] 对其离散化.
  其语音响应过程分为三个主要步骤: (1) 将用户指令转化为文本, (2) 根据这些指令生成文本响应, (3) 从文本响应生成样式标签和响应语音单元.
  这种过程使得 EMOVA 能够促进情感语音对话.

#### 交错文本和语音 Token

一些早期的模型 ([^AudioPaLM], [^VoxtLM]) 采用了监督训练方法, 使用具体的输入和输出序列, 并在混合语音-文本任务上进行训练, 包括文本到语音 (TTS), 自动语音识别 (ASR), 以及语音到语音的翻译.
- Spirit-LM[^SpiRit-LM] 利用语音和其转写之间的时序对齐, 使用交替文本和语音 Token 继续训练预训练的基于文本的大语言模型 (LLM).
  这种方法显著提升了模型在语音理解和生成中的性能.
  但是, 它采用离散的 HuBERT[^HuBERT] 单元作为语音表示, 这导致部分丢失了副语音信息.
- USDM [^USDM] 继续使用语音-文本数据对 Mistral-7B [^Mistral7B] 进行预训练, 以捕捉多模态语义.
  为了对话微调, 它使用用户输入的语音和转写作为指令数据构造模板.

#### 并行生成文本和语音

- PSLM [^PSLM] 提出并行地生成语音和文本 Token 来减少延迟, 然而这种方法可能会损害响应的质量.
  此外, 这种方法仍然依赖于对输入进行语音识别 (**Whisper**[^Whisper]), 这引入了进一步的延迟.
- LLaMA-Omni[^LLaMA-Omni] 引入了新颖的流式语音编码器, 以同时生成文本响应的离散语音单元序列, 显著减少延迟并适应实时交互的需求.
- **Moshi**[^Moshi] 和 Mini-Omni [^Mini-Omni] 采用相似方法, 引入双流架构在助手端同时生成语音 Token 和相应的文本 Token, 从而促进了预训练大语言模型的文本能力向语音模态的迁移, 使得模型能够直接通过语音进行推理.

关键区别在于如何处理语音-文本对齐:
- **Moshi**[^Moshi] 使用显式的对齐信息来监督模型的学习;
- Mini-Omni [^Mini-Omni] 允许 LLM 学习隐式的对齐信息.

在输入侧, Mini-Omni 将 **Whisper**[^Whisper] 编码器生成的连续语音嵌入输入到 LLM 中, 增强了模型理解语音指令的能力, 而无需文本输入.
然而, 语音输入和输出之间的不一致性引入了额外的计算开销, 增加了多轮对话场景中的延迟.
相比之下, Moshi 允许用户直接输入语音而不依赖于文本, 并在助手端并行生成文本和语音 Token.
Moshi 进一步扩展了架构以并行建模多个语音流, 从而能够从概念上和实践上简单地处理具有任意动态的全双工对话.

#### 语音转语音生成

这种方法旨在移除对中间文本的依赖, 从而减少延迟并使系统更接近实时互动.
- SyncLLM [^SyncLLM] 通过时间分块方法实现实时全双工交互, 将时间信息集成到 LLMs 中, 以便与真实世界时钟同步运行.
- IntrinsicVoice [^IntrinsicVoice] 使用特定模型在单步生成多个语音 Token, 有效地将语音 Token 序列长度与文本序列长度相当, 同时产生高质量的音频.
- Align-SLM [^Align-SLM] 使用预训练的自监督 HuBERT[^HuBERT] 模型和 K-means 聚类 (**TWIST**[^TWIST]) 将连续语音表示转换为离散单元. 它采用在预训练的 **TWIST**[^TWIST] 上微调的 LoRA[^LoRA] 适配器来从给定的提示生成多个语音延续, 并使用语义度量来生成偏好数据用于**直接偏好优化 (DPO)**[^DPO]. 实验结果表明, 将偏好优化方法集成到 Spoken LLM 中可以显著提高语义理解能力.

### 4.2·多阶段训练策略

This section primarily discusses the training process of the Spoken Dialogue Model, building upon previous work on spoken dialogue systems.
Generally, this process consists of four stages: text LLM pre-training, modality adaptation and alignment post-training, followed by supervised fine-tuning, and optionally, preference optimization.
The primary goal in training most spoken dialogue systems is to preserve the model's original capabilities while integrating the speech modality for voice interaction into the LLM.
The diagram of multi-stage training can be referred to in Figure ~\ref{fig:archi_img2}.

> Figure.06: Diagram of Multi-stage Training Steps.

#### Text LLM Pre-Training

The goal is to develop a text-intelligent LLM model capable of handling complex contexts and possessing knowledge reasoning abilities, thus preparing it for integration with speech-intelligent LLMs.
Most spoken dialogue systems utilize pre-trained large language models as foundational models rather than pre-training with separate text data themselves.
A series of approaches **SpeechGPT**[^SpeechGPT], [^SpeechGPT-Gen], [^SpiRit-LM], **EMOVA**[^EMOVA], [^LLaMA-Omni], [^SyncLLM] use the LLaMA model and its variants as their foundational language model.
On the other hand, [^LauraGPT], [^Mini-Omni], **Mini-Omni2**[^Mini-Omni2], [^OmniFlatten] employ the Qwen [^Qwen], [^Qwen2] family of large language models as their backbone.
Meanwhile, **Moshi**[^Moshi] employs an RQ-Transformer for hierarchical autoregressive modeling of speech, utilizing a unique structure that involves pre-training a text-only language model with datasets from the internet (e.g., Wikipedia \footnote{[URL](https://dumps.wikimedia.org/)} and StackExchange \footnote{[URL](https://archive.org/details/stackexchange/)}).
The collected data was filtered using a comprehensive preprocessing pipeline to ensure quality and relevance, which included deduplication to remove redundant entries, language identification to retain text in the desired language, and quality filtering to exclude low-quality or irrelevant content based on criteria such as coherence and completeness.
**VITA**[^VITA] utilizes Mixtral 8x7B1 [^111], a representative LLM with a sparse mixture of experts (SMoE) architecture, and performs pure-text instruction tuning for its extended Chinese vocabulary.

#### Modality Adaptation and Alignment Post-training

> Figure.07:Alignment Post-training Methods.

This phase explores strategies to adapt text-based large language models (LLMs) for speech modality input, focusing on aligning text and audio modalities effectively.
The primary goal is to enhance the models' ability to understand and generate speech by bridging the gap between these two modalities.
Common approaches include multimodal training techniques, leveraging unlabeled speech corpora, and employing multi-task learning frameworks.
These methods typically involve fine-tuning existing LLMs with speech-related tasks and integrating speech-specific modules, such as speech adaptors and decoders, to facilitate seamless interaction between text and speech modalities.
Different training tasks for modality adaptation and alignment are shown in Figure ~\ref{fig:archi_img3}.
Spirit-LM [^SpiRit-LM] continuously pretrains on text LLM checkpoints using interleaved text and speech tokens to improve the model's performance in speech understanding and generation.
LLaMA-Omni [^LLaMA-Omni] adopts a two-stage training strategy: the first stage jointly trains a speech adaptor and LLM with speech input and text responses, while the second stage uses the same dataset to train a streaming speech decoder independently.
Consequently, this LLM primarily possesses the capability for speech input understanding, with speech generation handled by a separate decoder module.
**SpeechGPT**[^SpeechGPT], **Moshi**[^Moshi], and **VITA**[^VITA] utilize unlabeled speech corpora to train models in a next-token prediction task.
In the first phase, **VITA** focuses on training the audio encoder and connector, while in the second phase, it optimizes both the connector and the LLM model through multimodal training.
Although capable of processing speech input, it outputs only text.
Spectron [^Spectron] addresses the alignment issue between text and speech representations by jointly supervising multiple objectives.
IntrinsicVoice [^IntrinsicVoice] employs a two-stage training approach, constructing multiple cross-modal tasks from a single dataset to enable the model to better learn the semantic consistency between speech and text.
Mini-Omni [^Mini-Omni], **EMOVA**[^EMOVA], and OmniFlatten [^OmniFlatten] adopt similar methodologies, commencing with supervised multi-task fine-tuning of the text LLM backbone to achieve speech-text modality alignment and develop a multimodal LLM[^112], [^113] using Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) tasks.
Notably, Mini-Omni divides the training of various modules into three phases: the first phase utilizes data from speech recognition and synthesis to enhance the model’s abilities in these aspects, training only the ASR and TTS adapters.
The second phase focuses exclusively on enhancing the model’s text capabilities when given speech inputs, updating only the LLM parameters while freezing other modules.
Through these two training phases, the original language LLM’s capabilities are maximally preserved, while adapting to speech modality input and output, thereby addressing the primary modality alignment tasks.

#### Supervised Fine-tuning or Dialogue Dataset Fine-tuning

During this stage, most models use instruction-following datasets or dialogue data for supervised fine-tuning of the LLM, enhancing natural conversational abilities.
**SpeechGPT**[^SpeechGPT], [^SpeechGPT-Gen] propose a two-stage instruction-tuning process that includes cross-modal instruction fine-tuning and chain-of-modality instruction fine-tuning.
Ultimately, the model follows the A-T-T-A method to achieve end-to-end speech input and output.
**EMOVA**[^EMOVA] employs a similar chain-of-modality concept to construct instruction-tuning datasets, empowering it to respond accurately to speech instructions.
**Moshi**[^Moshi], Mini-Omni [^Mini-Omni], OmniFlatten [^OmniFlatten], and SyncLLM [^SyncLLM] utilize spoken dialogue datasets for fine-tuning, endowing the models with conversational interaction capabilities.
Remarkably, Moshi constructs a more natural and realistic dialogue dataset that incorporates elements such as noise and overlap, enabling the model to learn authentic multi-stream interactions.
OmniFlatten fine-tunes the speech-text LLM using interleaved and serialized dialogues across three stages to progressively train the model in acquiring half-duplex and full-duplex communication capabilities.
Similarly, SyncLLM employs a three-stage training procedure that predominantly uses synthetic spoken dialogue data along with a relatively small amount of real-world spoken dialogue data to develop a full-duplex voice agent.

#### Preference Optimization and Reinforcement Learning

The research on leveraging preference optimization to align a spoken dialogue model with human preferences is virtually absent.
Recently, [^114], [^115], [^116] adopted preference optimization for Text-to-Speech (TTS) models to align speech synthesis quality with human preferences but not for spoken dialogue models.
Align-SLM [^Align-SLM] pioneers the integration of Direct Preference Optimization (DPO) [^DPO] in textless Spoken Language Models (SLMs) to enhance semantic understanding.
It transforms continuous speech into discrete units using a pre-trained Hubert model and K-means clustering.
LoRA fine-tuning on a Spoken LLM generates multiple speech continuations from prompts.
Semantic metrics create preference data offline, making DPO training efficient and stable, eliminating the need for an external reward model.
Coupled with curriculum learning [^117], Align-SLM progressively refines preference data selection, optimizing semantic feedback, and improving SLM performance.


### Training Frameworks and Generation Strategies

Recent advanced methods in spoken dialogue models employ a variety of innovative techniques to achieve more natural speech output and lower latency.
In this part, we explore various approaches that exemplify these advancements:

- **LLama-Omni.** LLama-Omni [^LLaMA-Omni] adds a streaming speech decoder that operates after the LLM.
This decoder runs in a non-autoregressive manner, taking the output hidden states from the LLM as input and generating the discrete unit sequence corresponding to the speech response.
To model the variable-length mapping between input and output, LLama-Omni employs an upsample factor, denoted as $\lambda$, along with Connectionist Temporal Classification (CTC) loss [^118].
This ensures that the model can generate speech responses simultaneously with text responses.
Additionally, a predefined chunk size is set to further enable vocoder streaming synthesis of speech waveforms, facilitating real-time interaction and reducing latency.

- **Mini-Omni.** Mini-Omni [^Mini-Omni] selects SNAC [^SNAC], a music-grade encoder, to discretize one second of audio into hundreds of tokens, which significantly increases the burden on the LLM for modeling speech tokens.
Delay Pattern language model decoding strategies are often applied in modeling multiple parallel streams of acoustic tokens in speech tasks like MusicGen [^MusicGen], VoiceCraft [^119], and Parler-TTS [^ParlerTTS].
Compared with traditional sequential step decoding, this strategy can effectively reduce the time steps required for LLM decoding and generating speech tokens.
Inspired by this, Mini-Omni innovatively applies text-instructed delayed parallel generation to address the issue of long SNAC codebook sequences, simultaneously producing audio and text tokens.
This effectively leverages and preserves the original capabilities of the language model.
Moreover, Mini-Omni proposes a Batch Parallel Decoding method.
Specifically, it generates two samples in parallel for a single input: the first predicts text tokens, and the second predicts both text and speech tokens simultaneously.
The text output from the first sample is embedded into the corresponding positions of the second sample, while the second sample's text output is discarded.
This further enhances the model’s reasoning capabilities during dialogue, maximizing the transfer of its text-based abilities.

- **IntrinsicVoice.** IntrinsicVoice [^IntrinsicVoice] introduces a speech encoder and a streaming vocoder for the tokenization and detokenization of speech, and a GroupFormer for modeling speech and text sequences.
This architecture integrates a large language model (LLM) with a GroupModel.
Specifically, it uses a pre-trained HuBERT encoder [^HuBERT] and its corresponding KMeans quantizer [^TWIST] to process speech inputs into discrete units.
These units are organized into a grouped token sequence through a group partition operation.
The grouped tokens are then passed through an embedding layer and adaptor module to map these embeddings into the LLM's embedding space.
The context embeddings output by the LLM are processed through a linear layer and concatenated with a specified number of learnable queries.
This input is fed into a smaller non-autoregressive transformer encoder model, dubbed the "GroupModel," to predict a group of speech tokens in one step.
The introduction of GroupFormer effectively improves the model's ability to handle sequences within a group, mitigates the modality gap between speech and text, accelerates inference speed, and alleviates issues associated with long-sequence modeling.

- **Moshi.** **Moshi**[^Moshi] introduces a mini codec model with 8 codebooks at a frame rate of 12.5 Hz for speech representation, where one second corresponds to 100 speech tokens.
It adopts an RQ-Transformer consisting of a Temporal Transformer and a smaller Depth Transformer as the backbone network for the LLM, hierarchically modeling multi-codebook audio tokens.
Similar architectures have appeared in prior research, such as UniAudio [^120] and Megabyte [^121].
The Depth Transformer models sub-sequence tokens conditioned on temporal context predicted by the Temporal Transformer.
Given the smaller size of the Depth Transformer, sub-sequence generation can almost be viewed as parallel generation.
This allows the model to scale to longer sequences by extending the temporal modeling capacity of the Temporal Transformer or to achieve greater depth by enhancing the hierarchical modeling capabilities of the Depth Transformer, rather than modeling the flattened sequence with a single model.

- **SyncLLM.** SyncLLM [^SyncLLM] employs an auto-regressive transformer decoder for full-duplex dialogue, integrating time synchronization to align speech units with the real-world clock.
It predicts interleaved speech tokens for both dialogue partners, maintaining timing with speaker tags.
The model is trained on deduplicated HuBERT token sequences to enhance semantic fidelity while managing latency by anticipating user responses.
Interpolation reconstructs token sequences to fit expected structures, facilitating seamless speech synthesis.

**Text-guided generation.** Some end-to-end methods like **SpeechGPT**[^SpeechGPT], [^SpeechGPT-Gen], [^Spectron], **EMOVA**[^EMOVA] use chain-of-thought reasoning, which allows guiding speech generation with the output of an underlying text LLM.
However, this is fundamentally incompatible with live interactions, as the model needs to produce an entire answer as text before it starts speaking.
Later methods [^LLaMA-Omni], [^Mini-Omni], **Moshi**[^Moshi] can accept user speech input and simultaneously output speech and text, ensuring high-quality responses while significantly reducing latency.
Lama-Omni [^LLaMA-Omni] utilizes a streaming decoder to generate text and speech tokens in parallel.
Mini-Omni [^Mini-Omni] is restructured to transfer language reasoning abilities to streaming audio output through a text-audio parallel decoding approach.
**Moshi**[^Moshi] details a novel feature, the Inner Monologue, which consists of joint modeling of the textual and speech modalities on the system side to improve the quality of interactions.

**W/o text-guided generation.** Other methods achieve speech-to-speech generation without relying on text stream generation.
IntrinsicVoice [^IntrinsicVoice] introduces a novel GroupModel that predicts a group of speech tokens in one step based on global context embeddings.
SyncLLM [^SyncLLM] predicts interleaved chunks of token sequences at each time step, allowing the model to handle all conversational cues such as backchannels, overlaps, interruptions, etc.

### Discussions about Training Paradigm in Spoken Dialogue Models

#### Text and Speech Modality Alignment

In spoken dialogue systems, the alignment between speech and text modalities is a crucial stage.
To preserve the textual intelligence of large language models (LLMs) as much as possible, nearly all current methodologies **SpeechGPT**[^SpeechGPT], [^PSLM], [^LLaMA-Omni], [^Mini-Omni], **Mini-Omni2**[^Mini-Omni2], **Moshi**[^Moshi], [^OmniFlatten] incorporate a post-training phase utilizing speech-text paired data when developing spoken dialogue models.
This may involve either expanding the vocabulary to treat speech tokens as an extension of the original vocabulary or using speech adaptors to map speech embeddings to the original text latent space of the LLM, and designing multi-task training objectives to achieve alignment between text and speech modalities.
For example, data from speech recognition and speech synthesis can be used to train the model's speech recognition and synthesis capabilities.
Although this is an effective strategy, its implementation can still lead to a certain degree of catastrophic forgetting in LLMs due to the large volume of pre-trained text corpora and the imbalance with paired speech-text data, which can harm the model's text-based capabilities.
Therefore, precise parameter design and customized optimization strategies are needed to mitigate this issue as much as possible, as demonstrated by approaches like **Moshi**[^Moshi].

This raises a consideration: during the training phase of spoken dialogue models, is it feasible to directly utilize speech data for adaptation to text-based LLMs, thereby eliminating the necessity for speech-text paired data? This is because unlabeled speech data is abundant and easily accessible, making it convenient and beneficial for training the speech intelligence of LLMs.
This approach would require us to obtain a pre-aligned speech representation with the text modality.
Perhaps we can consider further exploration and experimentation in the speech tokenizer component, such as directly mapping the semantic discrete units of speech onto the text token space to achieve enforced alignment.

#### Different Temporal Alignment Methods in Spoken Dialogue Models

In speech and text modalities, there is often a significant mismatch in sequence lengths.
Even when some speech tokenizers [^WavTokenizer], [^Single-Codec] employ extreme sequence compression methods, a length gap remains between the two.
Temporal alignment information between speech and text has been explored in tasks like Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) as demonstrated by models such as Whisper [^Whisper], FastSpeech [^FastSpeech2], and VITS [^122].
Recently, some spoken dialogue systems have utilized temporal alignment information to enhance model performance, yielding promising results.
For instance, Spirit-LM [^SpiRit-LM] uses interleaving text and speech tokens for continual pre-training on the LLaMA base model, significantly boosting the model’s performance in speech understanding and generation.
Experimental visualizations demonstrate that the similarity between text and speech features is notably higher in models trained with interleaved token sequences compared to those trained without this approach.
This indicates that providing the model with explicit fine-grained temporal alignment information can effectively enhance modality alignment and improve the performance of LLMs.

Mini-Omni [^Mini-Omni] achieves parallel generation of text and speech by padding text tokens to match the length of speech tokens, allowing the LLM to implicitly learn the alignment information between speech and text tokens.
This can be viewed as a form of sentence-level temporal alignment information, a method also utilized in recent speech synthesis work [^F5-TTS].
**Moshi**[^Moshi], on the other hand, uses word-level speech-text temporal alignment information and special marker tokens to achieve similar parallel generation capabilities.
The difference lies in that Mini-Omni fully allows the LLM to implicitly learn the alignment, whereas Moshi provides word-level alignment priors first, and then lets the model learn finer-grained alignments.

Exploring the impact of introducing different levels of temporal alignment priors on the training effectiveness of spoken dialogue models, such as sentence-level, word-level, or phoneme-level, is an intriguing area of research.
Understanding how these various alignment strategies affect model performance can guide the development of more efficient and accurate systems.
For instance, sentence-level alignment might offer a broader contextual understanding, while word-level or phoneme-level alignments could provide more detailed synchronization between speech and text, potentially leading to improvements in nuanced tasks like speech synthesis and understanding.

#### Reinforcement Learning (RL) in Spoken Dialogue Models

Reinforcement Learning (RL) has proven to be an effective learning paradigm in text and image processing [^123], [^124], [^125].
Recent research has shown that Direct Preference Optimization (DPO) [^DPO] can be extended to music and speech generation [^126], [^115].
MusicRL [^126] uses Reinforcement Learning from Human Feedback (RLHF) to improve music generation by fine-tuning a pretrained model for better text adherence and audio quality.
By collecting extensive human feedback, MusicRL creates a more refined and subjective music generation system.
Seed-TTS [^114] explores RL methods, comparing external reward models like REINFORCE with simpler methods like DPO.
The study highlights using REINFORCE to enhance speaker similarity and emotion controllability in the Seed-TTS system.
Qwen2-Audio [^Qwen2-Audio] uses DPO to align with human preferences by optimizing responses based on human-annotated data.
This enhances its ability to follow audio instructions accurately and intelligently respond to complex audio inputs, improving its performance in audio-centric tasks.
However, in the dialogue system field, reinforcement learning techniques based on human feedback [^127] are rarely applied.
Considering the diversity of inputs and outputs in large language models, exploring the incorporation of reinforcement learning strategies such as Proximal Policy Optimization (PPO) [^123] can be beneficial.
Additionally, considering the performance metrics for evaluating spoken dialogue systems, designing targeted reinforcement learning strategies and feedback functions to enhance different objectives is also a direction worth exploring.

## Streaming, Duplex, and Interaction

Streaming, full-duplex technology, and interactions, are crucial elements for enhancing the interactive capabilities of spoken dialogue models because they directly impact the system's responsiveness, the fluidity of natural interaction, and its ability to handle complex interactions.Unlike text language models, spoken dialogue models require real-time processing of user input.
**Streaming** allows the system to instantly acquire and process speech data; **full-duplex technology** enables both the system and user to speak simultaneously, enhancing the naturalness of interaction; and **handling of interactions** provides the model with the ability to recognize and adapt to various conversational contexts, making the dialogue more intelligent and realistic.
Building on early explorations, GPT-4o's advanced spoken dialogue capabilities have ignited a surge of research interest.
With real-time voice processing and natural conversational interaction, these models offer users a seamless and efficient communication experience.
However, achieving these capabilities requires deep research into model architecture, data collection, system design, and training methods.
The model needs to be carefully designed and optimized in terms of real-time performance, stability, and response speed.
At the same time, duplex technology is an indispensable key implementation, which ensures that the voice model has both "ears" and "mouths".
Next, we will first discuss the streaming processing method in Section 5.1, then introduce the key technologies of duplex communication and explains how to handle interactation to improve user experience in Section 5.2.

### Streaming Spoken Dialogue Models

The core of streaming speech models lies in their "real-time" and "continuous" capabilities, meaning they can process input and generate output simultaneously without waiting for complete input.
This includes two main aspects:

- **Streaming Understanding.** The model can process audio input as the user speaks, without needing to wait for the user to finish entirely, allowing it to align more naturally with the flow of conversation.

- **Streaming Generation.** This concept refers to the model's ability to generate output without waiting for all intermediate hidden states.
Instead, it can produce output progressively as processing occurs, which improves responsiveness and allows for smoother, more efficient interactions.

These streaming capabilities allow the model to perform more fluidly in real-time interactions, providing a seamless communication experience for users.
We will explore streaming techniques in both end-to-end and cascaded spoken dialogue models, discussing the implementation methods of streaming in each system and highlighting their similarities and differences.

#### Streaming End-to-End Spoken Dialogue Models

End-to-end streaming spoken dialogue models often leverage the knowledge of pre-trained text language models alongside an audio tokenizer, employing an tokenizer-detokenizer architecture to process and output audio signals.
Based on the concepts of streaming input and output discussed above, end-to-end models also require specific design considerations to enable streaming capabilities.
These designs center around the model’s input and output handling and can be distilled into three core techniques: causal convolution, causal attention mechanisms, and queue management.

**Causal Convolution.** Causal Convolution[^128] is a specialized form of convolution widely used in time-series processing, especially suitable for streaming speech models.
The key feature of causal convolution is that the current output depends only on the current and past inputs, without being influenced by future inputs, thereby strictly respecting temporal order.
Unlike regular convolution, causal convolution achieves this by "shifting" the convolution kernel to avoid accessing future information.
In a one-dimensional time series, if the convolution kernel size is \(k\), a standard convolution would use data from \((t - k/2)\) to \((t + k/2)\) at the current time step \(t\).
Causal convolution, however, pads the input on the left with \(k-1\) zeros so that the kernel only uses data from \(t - k + 1\) to \(t\), aligning the kernel to only consider current and past inputs.
This padding ensures that each layer's output depends solely on current and prior information, maintaining causality.
To further expand the model’s receptive field while preserving causality, **dilated causal convolution** can be used.
This technique introduces gaps within the kernel by inserting zeros between weights, effectively expanding the convolution’s range.
This allows the model to capture longer dependencies in the data without increasing latency, which is particularly useful for streaming applications.
In streaming spoken dialogue models, causal convolution plays a critical role in:

- **Ensuring real-time processing.** Causal convolution allows the model to compute outputs without accessing future frames, enabling real-time processing by generating outputs as input is received, which is essential for streaming.

- **Reducing latency.** By not requiring future input data, causal convolution significantly lowers the latency in speech models, making it more suitable for real-time interaction applications, such as voice assistants and live translation.

**Causal Attention.** Causal Attention is a specialized form of the attention mechanism designed to ensure that each position in a sequence can only attend to previous positions, thus preserving the temporal order crucial for streaming models.
This approach ensures that the model’s current output depends only on past and present information, preventing any “leakage” of future information, which is essential for real-time processing tasks.
In causal attention, the attention mask is typically used to achieve causality.
By applying a mask that blocks connections to future time steps, the model restricts each token’s receptive field to only the tokens before it.
Specifically, a lower triangular mask is applied to the attention matrix, setting values to negative infinity for positions corresponding to future tokens.
This masking technique ensures that the model’s predictions for each time step only consider current and past inputs, thereby adhering to a strict causal structure.
In streaming speech models, causal attention plays a significant role in enabling real-time interaction.
Unlike standard attention, which requires access to the entire sequence, causal attention can operate incrementally.
As new inputs are processed, the model can generate outputs without waiting for future context.

**Queue Management**[^129] Audio streams are typically split into frames, then processed in sequence via a queue management system that ensures real-time, orderly processing.

Some end-to-end models, such as Llama-Omni[^LLaMA-Omni], Mini-Omni[^Mini-Omni] and **Mini-Omni2**[^Mini-Omni2], employ non-streaming ASR model Whisper as an audio encoder components.
These models have made improvements on the output side to reduce latency.

- **Mini-Omni.** Mini-Omni use a generation strategy delayed parallel decoding is a that layer-by-layer delays during audio token generation.
This allows the model to generate text and multiple audio tokens simultaneously at each step, accelerating streaming audio generation and ensuring low-latency real-time output.

- **Llama-Omni.** Llama-Omni incorporates a non-autoregressive streaming speech decoder that leverages connectionist temporal classification (CTC) to directly generate a sequence of discrete audio tokens as the response.

- **Intrinsicvoice**.[^IntrinsicVoice] Intrinsicvoice introduced GroupFormer module  to group speech tokens, reducing the length of speech sequences to match that of text sequences.
This approach accelerates inference, alleviates the challenges of long-sequence modeling, and effectively narrows the gap between speech and text modalities.We think they cannot be considered fully streaming because they are not designed to be streaming on the input side.

- **Moshi.**[^Moshi] In contrast, Moshi references the architecture of SpeechTokenizer to train a streaming codec from scratch, serving as the audio tokenizer-detokenizer.
The entire model, including the codec, transformer, and attention mechanism, is built on a causal structure.

- **OmniFlatten.**[^OmniFlatten] OmniFlatten proposes chunk-based processing of text and speech along with gradual learning techniques and data handling to reduce turn-taking delays, such as response delays when users finish speaking or interrupt the system.
These models have achieved true streaming capabilities and established a foundation for diverse, bidirectional interactions.

#### Streaming Cascaded Spoken Dialogue Models

Consistent with the above, ensuring streaming capability in a model relies on designing both input and output for streaming.
Due to its cascaded nature, a cascaded model typically relies on external streaming ASR and TTS components, placing the streaming responsibility on these ASR and TTS modules.

In[^130], comparative studies were conducted on the streaming ASR model **U2++ Conformer**[^131], streaming TTS model **XTTS-v2**[^XTTS], non-streaming ASR **Whisper**, and non-streaming TTS **VITS**[^VITS2].
The combination of streaming components achieved the lowest latency and significantly contributed to interactive interruption capabilities.

### Duplex Technology and Interaction

#### Duplex Technology

The term Duplex originates from the field of communications, used to describe interaction modes between two parties in data transmission.
Depending on the type of communication, duplex is divided into half-duplex and full-duplex.

With the development of audio processing and generation technology , the concept of duplex has been introduced to speech systems, especially within the context of speech language models.
Here, duplex doesn’t just refer to signal transmission but emphasizes the synchronization and natural interaction in human-computer dialogue.
Specifically, within model architecture, it means that the model must retain its ability to perceive external input even while generating a response---essentially, the ability to listen while speaking.

**Simplex.** In simplex communication, data flows in only one direction.
The speaker can send data, while the listener can only receive it.
As shown in Figure \ref{fig:simplex}, the robot continuously transmits audio, while the user has no ability to respond.
This fixed-direction, one-way communication has the limitation of lacking interactivity.

**Half-Duplex.** In half-duplex communication, data flows in both directions but not simultaneously.
The two parties must take turns speaking and listening.
As illustrated in Figure \ref{fig:half-duplex}, the user speaks first, followed by a response delay during which the robot "thinks" before replying.
The robot’s response occurs only after the user has finished speaking, and vice versa.
This turn-taking method is similar to using a walkie-talkie, where each party can only transmit after the other has finished, limiting efficiency.Half-duplex is a common mode in early voice interaction systems.
In a typical half-duplex interaction, there are noticeable pauses in the conversation; the user and the system cannot “speak”  simultaneously, making the conversation feel less smooth, much like communication through a walkie-talkie.
For example, voice assistants like Siri use wake words or button presses to trigger the dialogue and require the speaker to finish a complete sentence before responding.
These systems typically adopt an ASR-LM-TTS cascaded structure and are often constrained by cascade delays and the turn-based nature of text language models.
Although this interaction method is simple and easy to implement, it can feel rigid and disjointed in natural conversational settings, with notable latency.
It is designed more for command execution rather than interactive communication.

**Full-Duplex.** Full-duplex communication allows both parties to send and receive data simultaneously[^132].
Figure \ref{fig:full-duplex} shows the user and robot engaging in overlapping, real-time interaction, where backchannels and interruptions are possible.
This mode enables a natural, two-way conversation, where both the user and robot can speak, respond, and even interrupt each other as needed, much like a phone call.In dialogue systems, full-duplex means that the system and user can speak simultaneously and interrupt each other, making it closer to natural conversation in real life.
Full-duplex large voice models allow the system not only to listen and understand the user while they speak but also to interrupt at appropriate moments or respond with backchannel cues.
Moreover, the system can detect the user’s intent to interrupt and pause itself accordingly, maintaining a smooth flow in the interaction.


The ultimate goal of a spoken dialogue moded is to make the user feel as though they are conversing with a real human friend.
Clearly, full-duplex technology is essential for achieving natural voice dialogue systems, enabling the system to send and receive audio signals simultaneously, thus facilitating real-time interaction.
Unlike text-based models, it doesn’t “cover its ears” while speaking.
Users and intelligent agents can interrupt each other while listening or express their attitude through non-verbal signals, such as interjections or laughter.
The challenges in realizing this lie in ensuring conversational fluidity, seamless turn-taking, and precise timing of interactions.
Developing a full-duplex system that can both generate and receive voice signals in complex interactive scenarios remains a key focus in academic and industrial research.

#### Interaction

Now that we understand duplex technology, we can further explore duplex spoken dialogue model.

We start with some concept.Turn-taking is the core concept in duplex dialogue.
It refers to the process in which speakers take turns speaking in an orderly manner during a conversation, forming a pattern of turn-taking.
Over the past few decades and has been extensively studied across fields such as linguistics, phonetics, and sociology.
Some research [^133], [^134]uses a non-deterministic finite-state machine with six states to describe the turn-taking behavior between the system and the user in a spoken dialogue system (SDS).
It outlines all possible states of turn-taking within an SDS, defining the objective of turn-taking as minimizing mutual silence or overlap between interlocutors, thereby improving communication efficiency.
Turn-taking encompasses three fundamental concepts:

- **Turn-taking cues**[^135], [^136].
These include voice, rhythm, breathing, gaze, or gestures.
Agents can use these cues to determine whether to take a turn from the user or to relinquish the turn.

- **Turn-end detection or prediction.** The distinction between detection[^137], [^138] and prediction[^139], [^140] lies in that detection determines whether the agent should take a turn at the current moment, whereas prediction decides when the turn-taking should occur in the future.

- **Overlap.** This mainly involves two situations.
When the user and agent’s voices overlap, if the user intends to take the turn from the agent, this behavior is defined as an \textit{interruption}[^141], [^Spectron].
If the user has no intention of taking the turn, this behavior is considered \textit{backchannel}[^142] or a listener response, such as "uh-huh," "right."

Through these concepts, we can better understand turn-taking behavior in duplex dialogues.
In summary, our interactions with voice dialogue systems can be categorized as \textit{interruptions}, \textit{backchannels}, and \textit{normal turn exchanges}.

The earliest full-duplex systems used a simple Voice Activity Detection (VAD) component to model whether the user intended to interrupt.
However, this approach is inadequate for handling backchannel interaction forms, leading to frequent interruptions and introducing considerable delays.

We can briefly categorize the exploration of interactions into cascaded systems and end-to-end systems based on duplex technology.
Regardless of the system type, the critical core idea is that the system must continuously track external information in real-time, analyze it, and determine the model’s operational state accordingly.
An interactive voice system must meet two requirements: 1) The ability to accept external information in real-time at any moment.
2) The ability to respond to this information accurately.
This includes:

- **Detecting User Interactions.** When the user tries to interject or provide new information, the system can recognize this intent and immediately stop its output to allow the user to speak.

- **Backchanneling During User Speech.** While the user is speaking, the system can provide brief acknowledgments like "uh-huh" or "I see" to indicate active listening, which encourages the user to continue.

- **Quickly Responding After User Completion.** When the user finishes speaking, the system can promptly recognize this cue and respond without unnecessary delays, maintaining a smooth conversational flow.

- **Handling Pauses in User Speech.** When the user briefly pauses, the system can interpret this as a moment of thought rather than an invitation to respond, thus avoiding premature interruptions and preserving the natural flow.

- **Interrupting the User When Necessary.** In situations where the system detects critical information, it can choose to interrupt the user to provide immediate feedback.
For example, if the user is speaking but the system needs to alert them to an error, it can intervene in real-time to ensure effective communication.

**Cascaded Systems.**
To enable interactive functionality, cascaded spoken dialogue models typically require explicit modeling of dialogue turns.
As the core, the large language model needs effective context and turn management.
Next, we introduce several representative works on interaction in cascaded systems.

- **Duplex Conversation.** In [^143], three core modules are proposed to achieve smooth full-duplex dialogue: user state detection, response signal selection, and interruption detection.
The user state detection module not only focuses on traditional turn-end detection but also identifies whether the user intends to switch turns, continue speaking, or hesitates during their speech.
To achieve this, the system uses a multimodal model, taking audio and text as inputs, and incorporates features such as speech rhythm, pitch, and pauses for more accurate assessment of the user’s state, determining whether to respond immediately or wait longer.
The response signal selection module inserts small backchannel cues (such as "uh-huh" or "right") at appropriate times to simulate natural human conversation.
By analyzing a large volume of real dialogues, this module extracts and trains suitable response signals for various conversation scenarios.
Using multi-label classification, the system selects the optimal response for each dialogue context, significantly reducing user waiting time and enhancing conversation flow.
The interruption detection module flexibly responds to user interruptions.
Unlike traditional rule-based detection methods, this system builds an end-to-end detection model with multimodal input (audio and text) that not only identifies genuine user interruptions but also avoids misinterpreting background noise or unintended voice signals as interruptions.

- **Outbound Agent System.** [^144] proposed a full-duplex dialogue scheme for outbound systems, focusing on the issues of conversational fluidity and timing of interaction in speech dialogue.
This scheme uses semantic analysis to determine whether the user truly intends to interrupt the system and can handle disjointed expressions when users mention named entities.
The core of this system is a full-duplex interaction finite-state machine (FSM), which retrieves text snippets from ASR results every 300 milliseconds to decide whether to interrupt.
Through continuous semantic analysis of user speech, the interruption model identifies meaningful user interruptions and avoids frequent interruptions caused by brief, meaningless responses (like "uh-huh").
The model employs a pre-trained BERT-based text classifier and utilizes streaming input, ensuring that the system can process and analyze user speech in real-time as it is received.
Additionally, the system includes a Discontinuous Expression module to handle user pauses when mentioning named entities.
Specifically, when users hesitate over entities (such as numbers, locations, or company names), VAD may erroneously detect turn-end.

The advent of Large Language Models  has significantly advanced generative AI development.
Models like ChatGPT demonstrate strong capabilities in semantic understanding and logical reasoning, offering a simplified method to integrate various dialogue components into a unified framework, which may simplify SDS construction.
GPT-4o represents a milestone for dialogue systems, showcasing a nearly human-like conversational voice model.
Its flexible interaction style and interruption mechanisms make human-computer interaction more natural and fluid.
However, as a commercial model, its training data and implementation details remain proprietary, making replication challenging.

- **Full-duplex LLM.** [^130] proposed a full-duplex spoken dialogue models based on LLMs, enabling simultaneous reception and transmission of voice signals through a perception module, an action module, and a neural finite-state machine (FSM).
The perception module uses a streaming ASR model, capturing and processing user speech in real-time with 640-millisecond intervals per time step, converting it into token inputs for the LLM.
The action module, utilizing a streaming TTS model, instantly converts the LLM-generated text into audio output and can pause or resume playback as needed, ensuring the system can generate audio while receiving user input.
At the core is the neural FSM, allowing the LLM to switch between "speaking" and "listening" states.
Controlled by FSM signals, the system can dynamically decide to continue speaking, listen, or interrupt based on the dialogue context.
Experimental results show that Wang et al.'s full-duplex streaming system reduces response latency by threefold, achieves a response time within 500 milliseconds in over 50\
- **VITA.** VITA is an open-source multimodal large language model which aimed at enhancing multimodal interaction experiences.
VITA can process multiple modalities, such as video, image, text, and audio, and achieves fluid human-computer interaction through a new duplex architecture involving two simultaneously operating models: one for generating responses to user queries, and another for continuously monitoring environmental inputs.
When a new user query is detected, the generation model pauses, and the monitoring model processes the new query and generates an updated response.
This setup enables VITA to support audio interruption, allowing users to ask new questions during system generation, with the system immediately pausing the current response to handle new input.
VITA’s perception abilities are achieved through multimodal alignment and instruction fine-tuning, enabling it to switch automatically between different inputs.
Additionally, VITA employs state tokens to distinguish user input types, such as query audio, background noise, and text input, facilitating wake-free interaction.
VITA's enhanced listening module prevents unnecessary user feedback from interrupting system responses, improving robustness.

- **CleanS2S.**[^145]
This model employs a structured pipeline to enable responsive and flexible interactions in a spoken dialogue setting.
Designed to facilitate seamless turn-taking and interruption handling, the model consists of several interconnected modules working in a coordinated sequence to optimize user experience.
Starting with user input, the system uses a Voice Activity Detection (VAD) module to continuously monitor for incoming audio signals.
As soon as a user starts speaking, VAD captures the input and immediately initiates processing by sending the audio data to the Automatic Speech Recognition (ASR) module.
This quick detection and response setup allows the system to react to user input without delay.
Once ASR transcribes the audio into text, the transcription is passed to the Large Language Model (LLM), which generates a relevant response based on the user’s query.
Meanwhile, the model is designed to be interruption-aware.
During response generation, if VAD detects a new user input (indicating an interruption or a follow-up query), the system can promptly adjust its processing flow.
In this case, the LLM temporarily pauses its current task, allowing ASR to transcribe the new input, which the LLM then uses to generate an updated response.
This interruption capability is achieved through the model’s layered processing design, allowing for adaptive turn-taking that feels natural and responsive.
The Text-to-Speech (TTS) module then converts the generated text response into audio, which is transmitted to the user via WebSocket.
To further support interruption handling, TTS breaks down lengthy responses into smaller audio segments that are sent progressively.
This segmentation allows the system to stop audio output instantly if an interruption occurs, switching to the new input without delay.
Each segment is prepared and sent only after a brief VAD check, ensuring that the system is ready to pause and handle new input at any time.
This interconnected processing chain—VAD detecting input, ASR transcribing, LLM generating responses, and TTS outputting segmented audio—creates a duplex interaction framework that balances response generation and user-driven interruptions.
By seamlessly coordinating these components, the model provides a fluid, real-time dialogue experience that adapts to user interactions dynamically.

**End-to-End Systems.**
In contrast, end-to-end spoken dialogue models do not require explicit modeling of dialogue turns; instead, they learn interaction modeling directly from training data.
Next, we introduce several representative works on interaction in end-to-end systems.

- **dGSLM.** In end-to-end systems, the introduction of the dGSLM model marks a significant milestone in full-duplex technology development.
Within the dGSLM framework, duplex technology is effectively implemented.
This model demonstrates how to capture complex interactions within dialogues directly from raw audio data through generative spoken dialogue modeling, without relying on text.
The core innovation of dGSLM is the dual-tower Transformer architecture, called the Dialogue Transformer Language Model (DLM), which uses a cross-attention mechanism to enable the system to process two parallel audio channels simultaneously.
Through this architecture, the model not only independently generates speech for each channel but also shares information between channels using cross-attention, effectively modeling silences and interaction events.
It leverages the HuBERT encoder and HiFi-GAN decoder, combined with the dual-tower DLM, and is trained on 2,000 hours of dual-channel telephone conversation audio (Fisher dataset), where each speaker in a conversation is allocated an independent audio track.
The dGSLM model transforms the audio on both channels into discrete tokens using HuBERT, and the DLM model autoregressively predicts the next audio token and its duration.
Finally, the HiFi-GAN[^HiFi-GAN] decoder reconstructs the audio for both channels.
This approach differs significantly from traditional text-dependent spoken dialogue models, with a particular emphasis on modeling turn-taking and backchanneling capabilities.
This capability gives dGSLM a notable advantage in duplex voice interaction, better mimicking the natural dynamics of human conversation.
Through its duplex model design, dGSLM represents an essential step forward in interactive capabilities and provides a foundation for further advancements.

- **Moshi.** As a novel full-duplex architecture, Moshi incorporates a rich array of design concepts.
Unlike dGSLM, Moshi does not abandon the language model’s ability in text dialogue.
Moshi’s architecture is based on the Helium language model and Mimi neural audio codec, both trained from scratch.
Helium, as a large pre-trained text language model, provides strong reasoning capabilities, while Mimi handles audio signal encoding and decoding.
To achieve real-time interaction, Moshi is designed as a multi-stream architecture, simultaneously processing "user" and "moshi" audio streams without explicitly modeling speaker turns.
Moshi also introduces the "Inner Monologue" method within the "moshi" audio stream, a process that jointly models text and audio tokens during training and inference.
This approach allows the model to fully utilize textual knowledge while maintaining speech-to-speech system characteristics, significantly enhancing generation quality.
Mimi, a neural audio codec integrating semantic and acoustic information through residual vector quantization and knowledge distillation, captures high-quality user input audio and Moshi’s output voice efficiently.
To jointly model Moshi and user audio streams alongside Moshi’s text tokens, Depth Transformer with streaming inference capabilities is employed.
The Mimi encoder and decoder combine convolutional and Transformer layers, with causal convolutions, allowing for streaming operation.
Moshi is pre-trained on unsupervised audio data to handle speech scenarios and then fine-tuned on the Fisher dataset to address overlapping speech and interruptions.
Finally, the system is further optimized on a custom instruction-tuning dataset, ensuring robust performance across various interactive scenarios.
Experimental results show that Moshi excels in speech modeling and spoken QA tasks, especially in latency, achieving a theoretical latency of 160 milliseconds and 200 milliseconds in practice, significantly lower than the typical 230 milliseconds in natural conversation, enhancing real-time interaction and conversation flow.

- **Parrot.** Parrot[^146] model incorporates multiple features specifically designed to enhance interaction in spoken dialogue.
It uses a dual-channel audio setup, where each channel represents a different speaker.
This configuration allows Parrot to manage both sides of a conversation independently, facilitating real-time turn-taking.
By distinguishing between the user’s input and the system’s response on separate channels, the model can listen and respond in parallel, creating a more natural conversational flow.
To handle simultaneous speaker inputs effectively, Parrot employs a "next-token-pair prediction" mechanism, allowing it to predict tokens for both channels in a coordinated sequence.
This approach helps the model manage conversational dynamics such as overlapping speech and smooth transitions between turns, adjusting response timing based on the user’s input.
During inference, Parrot supports streaming input, enabling continuous processing of user audio on one channel while generating responses on the other.
This streaming capability allows the model to respond to live spoken input in real-time, handling turn-taking, pauses, and interruptions dynamically.
Unlike cascaded systems that rely on intermediate text conversions, Parrot processes audio directly, reducing latency and allowing immediate responses to spoken input.
These interaction-focused design choices make Parrot highly responsive, enabling it to manage turn-taking naturally, respond to interruptions, and handle overlapping speech,

- **Mini-Omni2.** Mini-Omni2 is an open-source multimodal large language model aimed at simulating the multimodal capabilities of GPT-4o in vision, hearing, and text, supporting real-time full-duplex interaction.
Mini-Omni2 combines visual and audio encoders with a language model to enable simultaneous input and output of images, audio, and text.
The model incorporates an interrupt mechanism based on instruction design for more flexible user interactions.
This system uses a delayed parallel generation algorithm, allowing the model to generate text and audio responses simultaneously, greatly improving conversational real-time capabilities and response speed.
To achieve full-duplex interaction, Mini-Omni2 introduces an interrupt mechanism based on a limited instruction approach, trained on a specially constructed dataset with specific irq (interrupt) and n-irq (non-interrupt) state markers for model optimization.
For training Mini-Omni2’s interruption functionality, the researchers used noisy speech data synthesized with specific command phrases (such as "Stop Omni") in various voices and tones to simulate scenarios where users might issue interrupt commands.
The dataset also includes background noises, such as environmental sounds, music, and other dialogues, enhancing the model’s robustness in complex environments.
During training, Mini-Omni2 controls output flow through irq and n-irq state markers, generating these markers in real-time to determine whether to continue output.
In this way, the model can immediately halt generation based on user instructions and switch to "listening" mode in real-time dialogue.
The training data consists of long audio streams from which the model extracts and encodes user commands like "Stop Omni." Researchers inserted interrupt commands at various time points, marking data after the insertion point as irq (interrupt) and data before as n-irq (non-interrupt).
This labeling method ensures that the model learns to accurately identify interrupt commands in complex audio inputs and respond appropriately.

- **SyncLLM.** SyncLLM achieves full-duplex dialogue and interruption capabilities through multi-stream interleaving and chunk processing.
SyncLLM divides the conversation's audio stream into fixed-sized chunks, each corresponding to a specific time interval.
The model alternates between generating user and system speech segments within each time step (chunk), ensuring real-time system responses while processing user speech input.
To maintain temporal synchronization with the user, SyncLLM predicts the user’s speech at each time step before generating each system chunk, using it as context to infer the system’s next response.
This mechanism enables the system to keep pace with the conversation even with network latency.
The chunk method allows SyncLLM to handle both user and system audio streams simultaneously, supporting complex dialogue features like speech overlap, interruption, and real-time feedback.
Additionally, by using de-duplicated speech token sequences and periodic synchronization markers, the model efficiently performs chunk-level real-time inference, making conversation more fluid and natural.

- **OmniFlatten.** Similar to SyncLLM, the OmniFlatten model achieves full-duplex and interruption functionality primarily through multi-stream data processing and progressive training.
To enable full-duplex dialogue, the model adopts a multi-stream architecture that interleaves the user’s speech stream with the assistant’s speech and text streams into a single sequence for training, simplifying multimodal modeling and enhancing real-time capability.
The model first aligns the text language model with modality through multitask supervised fine-tuning, enabling it to understand and generate both speech and text, ensuring basic capability for handling speech and text simultaneously.
Through a progressive training process, OmniFlatten attains full-duplex capability in three stages: initial training for half-duplex dialogue, then removing the user’s text stream to support real-time prediction with multi-stream data, and finally removing the assistant’s text stream to enable pure speech stream generation.
These steps reduce reliance on text and decrease latency, allowing the system to generate voice responses while receiving user speech input.
By using a block-by-block generation strategy, OmniFlatten divides the input and output speech sequences into fixed-size blocks, processing each segment in turn.
This effectively implements streaming processing, ensuring low latency and high responsiveness in full-duplex dialogue, thereby providing a more natural response to user interruptions.

- **Freeze-Omni.** To support duplex dialogue, Freeze-Omni[^Freeze-Omni] uses a chunk-level state prediction mechanism for natural turn-taking.
When the user begins speaking, a voice activity detection module identifies the audio input, prompting the model to process the audio chunk by chunk.
After processing each chunk, the model's classification layer predicts the conversation state to determine the next action.
There are three possible states: State 0, where the model continues listening for more input, assuming the user hasn’t completed their turn; State 1, where the model interrupts to provide an immediate response if a quick acknowledgment or feedback is needed; and State 2, where the model has completed processing the current user input and is ready to generate and output a response, thus transitioning smoothly into the response phase without further listening.
This chunk-wise state prediction enables the model to decide effectively when to respond and when to continue listening, enhancing its ability to handle natural conversational cues and support interactive dialogue.

#### Discussions about streaming and interaction

Significant progress has been made in dialogues models, particularly in real-time interaction and semantic understanding, with notable achievements in streaming processing and full-duplex interaction.
Current systems exhibit strong technical capabilities in reducing response latency, enhancing interruption handling, and improving the naturalness of conversation.
However, existing spoken dialogues models still lack a unified system that can handle all forms of interaction seamlessly.
Future research could explore new frameworks to better manage both user interruptions and the system’s ability to interrupt users, making interactions more natural.
Additionally, standardized benchmarks for evaluating interaction capabilities remain underdeveloped.
A unified evaluation benchmark would provide a consistent method for assessing and comparing the performance of different models, thereby advancing the development of more intelligent and responsive interaction systems.

## Training Resources and Evaluation

### Training resources

Training a spoken dialogue system is a complex, multi-stage process, with each stage relying on specific datasets to achieve distinct training objectives and enhance system performance.
This section provides an in-depth analysis of the training resources about the spoken dialogue models, showcasing the data collection and processing methods at each stage and illustrating how these elements contribute to the system's intelligence.
It further reveals how key steps, from foundational architecture to fine-tuning, shape the intelligent development of dialogue systems.

To address the limitations of existing training spoken dialogue data and leverage the knowledge and reasoning capabilities of mature text-based models, many approaches involve \textit{Continue Training} on pre-trained text language models.
This training paradigm encompasses nearly all data types required to build a spoken dialogue system.
The following sections focus on analyzing data acquisition and processing methods under this training flow, covering the following core stages: \textit{Text Language Model Pre-training}, \textit{Post-Train for Audio Modal Adaption}, \textit{Post-Train for Dual-Stream Audio Processing}, \textit{Enhancing Conversational Abilities and Instruction Tuning}.
We have listed commonly used datasets for training in Table \ref{traindataset}.
However, current spoken dialogue models lack exploration in music and sound.
To support future development in spoken dialogue systems, we provide a list of common music and sound datasets in the appendix \ref{music datasets} as a reference.

#### Training resources about Text LLM Pre-training

Text Language Model pre-training serves as the foundational stage for spoken dialogue models.
Through unsupervised learning on large-scale text data, the model acquires knowledge of vocabulary, grammar, and contextual relationships, gaining essential knowledge and reasoning capabilities.
Most spoken dialogue systems are built upon pre-existing open-source text language models (such as Llama[^LLaMA], Palm[^147], etc).
Although we does not delve into this stage in detail, it provides a solid foundation for the model’s natural language understanding and generation capabilities.

Modal Alignment
& Mandarin ASR    & AISHELL-1[^148]      & 170 hrs   & \url{https://www.openslr.org/33/}       & Text, Speech \\
& Mandarin ASR    & AISHELL-2[^149]      & 1k hrs   & \url{https://github.com/kaldi-asr/kaldi/tree/master/egs/aishell2}       & Text, Speech \\
& Mandarin TTS    & AISHELL-3[^150]      & 85 hrs, 88,035 utt., 218 spk.   & \url{https://www.aishelltech.com/aishell_3}       & Text, Speech \\
& TTS             & LibriTTS[^151]       & 585 hrs                         & \url{https://www.openslr.org/60/}                 & Text, Speech \\
& ASR             & TED-LIUM[^152]       & 452 hrs                         & \url{https://lium.univ-lemans.fr/ted-lium3/}      & Text, Speech \\
& ASR             & VoxPopuli[^153]      & 1.8k hrs                        & \url{https://github.com/facebookresearch/voxpopuli} & Text, Speech \\
& ASR             & Librispeech[^154]    & 1,000 hrs                       & \url{https://www.openslr.org/12}                  & Text, Speech \\
& ASR             & MLS[^155]            & 44.5k hrs                       & \url{https://www.openslr.org/}                    & Text, Speech \\
& TTS             & Wenetspeech[^156]    & 22.4k hrs                       & \url{https://wenet.org.cn/WenetSpeech/}           & Text, Speech \\
& ASR             & Gigaspeech[^157]     & 40k hrs                         & \url{https://github.com/SpeechColab/GigaSpeech}   & Text, Speech \\
& ASR             & VCTK[^158]           & 300 hrs                         & \url{https://paperswithcode.com/dataset/voice-bank-demand} & Text, Speech \\
& TTS             & LJSpeech[^159]       & 24 hrs                          & \url{https://keithito.com/LJ-Speech-Dataset/}     & Text, Speech \\
& ASR             & Common Voice[^160]   & 2,500 hrs                       & \url{https://commonvoice.mozilla.org/zh-CN}       & Text, Speech \\
& Audio Caption   & Wavcaps[^161]        & 400k clips                      & \url{https://github.com/XinhaoMei/WavCaps}        & Text, Speech \\
& ASR             & LibriLight[^162]     & 60k hrs                         & \url{https://github.com/facebookresearch/libri-light} & Text, Speech \\
& ASR             & PeopleSpeech[^163]     & 30k hrs                         & \url{https://huggingface.co/datasets/MLCommons/peoples_speech} & Text, Speech \\
& Mandarin ASR             & KeSpeech[^164]     & 1,542 hrs                         & \url{https://github.com/KeSpeech/KeSpeech} & Text, Speech \\
& TTS            & Emilia[^165]     & 	101k hrs                        & \url{https://huggingface.co/datasets/amphion/Emilia-Dataset} & Text, Speech \\

Dual-Stream Processing
& Instruction     & Alpaca[^166]         & 52,000 items                    & \url{https://huggingface.co/datasets/tatsu-lab/alpaca} & Text + TTS \\
& Instruction     & Moss           & -                               & \url{https://huggingface.co/fnlp/moss-moon-Mini-Omni2-sft} & Text + TTS \\
& Instruction     & BelleCN        & -                               & \url{https://github.com/LianjiaTech/BELLE/tree/main} & Text + TTS \\
& Dialogue        & UltraChat[^167]      & 1.5 million                     & \url{https://github.com/thunlp/UltraChat}          & Text + TTS \\
& Instruction     & Open-Orca[^168]      & -                               & \url{https://huggingface.co/datasets/Open-Orca/OpenOrca} & Text + TTS \\
& Noise          & DNS[^169] & 2425 hrs                       & \url{https://github.com/microsoft/DNS-Challenge} & Noise  data \\
& Noise           & MUSAN [^170]         & -                               & \url{https://www.openslr.org/17/}                  & Noise data\\ \hline

Conversation Fine-Tune
& Dialogue        & Fisher         & 964 hrs                         & \url{https://catalog.ldc.upenn.edu/LDC2LLaMA3T19}     & Text, Speech \\
& Dialogue        & GPT-Talker[^171]     & -                               & \url{https://github.com/AI-S2-Lab/GPT-Talker}      & Text, Speech \\
& Instruction     & INSTRUCTS2S-200K & 200k items                    & \url{https://github.com/ictnlp/LLaMA-Omni}         & Text + TTS \\
& Instruction     & Open Hermes    & 900k items                      & \url{https://ollama.com/library/openhermes}        & Text + TTS \\ \hline

#### Training resources about Post-Train for Audio Modal Alignment

After establishing a text-based foundational model, the system possesses essential knowledge and reasoning abilities.
In this stage, we introduce the audio modality, enabling the text language model to understand and generate speech while minimizing any potential loss of textual knowledge.
This process is known as \textit{modal adaption} or \textit{modal alignment}.
This multimodal structure incorporates an audio encoder with a codebook, helping the model recognize linguistic, emotional, and tonal information in speech.
The audio decoder supports the generation of natural and fluent speech output, while audio signal embeddings and special token types (e.g., speaker-distinguishing tokens for Synchronous LLM, task-distinguishing tokens for OmniFlatten, and state tokens for VITA) are added to the vocabulary of the text language model.

The primary goal at this stage is to align information from different modalities into a unified space or representation, allowing the model to correlate and comprehend such information.
Consequently, the model is often trained on cross-modal tasks such as TTS , ASR , and audio captioning.
The datasets used include numerous paired audio and text samples to ensure effective conversion between modalities.
Commonly used TTS and ASR datasets include Aishell-3[^150], LibriTTS[^151], TED-LIUM[^152], VoxPopuli[^153], Librispeech [^154], MLS[^155], Wenetspeech[^156], Gigaspeech[^157], VCTK[^158], LJSpeech[^159], Common Voice[^160], and others.
For audio captioning, Wavcaps[^161] are frequently used.
Some speech datasets require ASR model transcription to generate corresponding text.

In this phase, the emphasis is placed on capturing and generating audio features and aligning them with text in vector space, rather than focusing on dialogue functionality.Therefore, the data typically consists of single-channel audio, which can be used after resampling.
Notably, in some works, it is essential to ensure word-level alignment between text tokens and audio tokens (e.g., Spirit-LM, Moshi, and OmniFlatten), achievable through tools like the Whisper-timestamped package or other alignment tool.
In Moshi, to prevent catastrophic forgetting, half of the training time is allocated to text data, highlighting the importance of balancing text and audio data during training.

\caption{Datasets used in the various training stages}

**Stage** & **Task** & **Dataset** & **Size** & **URL** & **Modality**

Modal Alignment
& Mandarin ASR    & AISHELL-1[^148]      & 170 hrs   & [URL](https://www.openslr.org/33/)       & Text, Speech \\
& Mandarin ASR    & AISHELL-2[^149]      & 1k hrs   & [URL](https://github.com/kaldi-asr/kaldi/tree/master/egs/aishell2)       & Text, Speech \\
& Mandarin TTS    & AISHELL-3[^150]      & 85 hrs, 88,035 utt., 218 spk.   & [URL](https://www.aishelltech.com/aishell_3)       & Text, Speech \\
& TTS             & LibriTTS[^151]       & 585 hrs                         & [URL](https://www.openslr.org/60/)                 & Text, Speech \\
& ASR             & TED-LIUM[^152]       & 452 hrs                         & [URL](https://lium.univ-lemans.fr/ted-lium3/)      & Text, Speech \\
& ASR             & VoxPopuli[^153]      & 1.8k hrs                        & [URL](https://github.com/facebookresearch/voxpopuli) & Text, Speech \\
& ASR             & Librispeech[^154]    & 1,000 hrs                       & [URL](https://www.openslr.org/12)                  & Text, Speech \\
& ASR             & MLS[^155]            & 44.5k hrs                       & [URL](https://www.openslr.org/)                    & Text, Speech \\
& TTS             & Wenetspeech[^156]    & 22.4k hrs                       & [URL](https://wenet.org.cn/WenetSpeech/)           & Text, Speech \\
& ASR             & Gigaspeech[^157]     & 40k hrs                         & [URL](https://github.com/SpeechColab/GigaSpeech)   & Text, Speech \\
& ASR             & VCTK[^158]           & 300 hrs                         & [URL](https://paperswithcode.com/dataset/voice-bank-demand) & Text, Speech \\
& TTS             & LJSpeech[^159]       & 24 hrs                          & [URL](https://keithito.com/LJ-Speech-Dataset/)     & Text, Speech \\
& ASR             & Common Voice[^160]   & 2,500 hrs                       & [URL](https://commonvoice.mozilla.org/zh-CN)       & Text, Speech \\
& Audio Caption   & Wavcaps[^161]        & 400k clips                      & [URL](https://github.com/XinhaoMei/WavCaps)        & Text, Speech \\
& ASR             & LibriLight[^162]     & 60k hrs                         & [URL](https://github.com/facebookresearch/libri-light) & Text, Speech \\
& ASR             & PeopleSpeech[^163]     & 30k hrs                         & [URL](https://huggingface.co/datasets/MLCommons/peoples_speech) & Text, Speech \\
& Mandarin ASR             & KeSpeech[^164]     & 1,542 hrs                         & [URL](https://github.com/KeSpeech/KeSpeech) & Text, Speech \\
& TTS            & Emilia[^165]     & 	101k hrs                        & [URL](https://huggingface.co/datasets/amphion/Emilia-Dataset) & Text, Speech \\
\hline

Dual-Stream Processing
& Instruction     & Alpaca[^166]         & 52,000 items                    & [URL](https://huggingface.co/datasets/tatsu-lab/alpaca) & Text + TTS \\
& Instruction     & Moss           & -                               & [URL](https://huggingface.co/fnlp/moss-moon-Mini-Omni2-sft) & Text + TTS \\
& Instruction     & BelleCN        & -                               & [URL](https://github.com/LianjiaTech/BELLE/tree/main) & Text + TTS \\
& Dialogue        & UltraChat[^167]      & 1.5 million                     & [URL](https://github.com/thunlp/UltraChat)          & Text + TTS \\
& Instruction     & Open-Orca[^168]      & -                               & [URL](https://huggingface.co/datasets/Open-Orca/OpenOrca) & Text + TTS \\
& Noise          & DNS[^169] & 2425 hrs                       & [URL](https://github.com/microsoft/DNS-Challenge) & Noise  data \\
& Noise           & MUSAN [^170]         & -                               & [URL](https://www.openslr.org/17/)                  & Noise data\\ \hline

Conversation Fine-Tune
& Dialogue        & Fisher         & 964 hrs                         & [URL](https://catalog.ldc.upenn.edu/LDC2LLaMA3T19)     & Text, Speech \\
& Dialogue        & GPT-Talker[^171]     & -                               & [URL](https://github.com/AI-S2-Lab/GPT-Talker)      & Text, Speech \\
& Instruction     & INSTRUCTS2S-200K & 200k items                    & [URL](https://github.com/ictnlp/LLaMA-Omni)         & Text + TTS \\
& Instruction     & Open Hermes    & 900k items                      & [URL](https://ollama.com/library/openhermes)        & Text + TTS \\ \hline

#### Training resources about Post-Train for Dual-Stream Dialogue Processing

To ensure that the model possesses the ability to “listen while speaking”.
Most research such as **Moshi**[^Moshi] and OmniFlatten[^OmniFlatten] has implemented a dual audio-stream model: one audio stream generates model output, while the other captures user audio.
The objective of this training phase is to enable the model’s dual-stream processing without requiring complex human-computer interaction modeling.
Consequently, text dialogue data can be converted to speech and processed into dual-track audio format.
However, text dialogue data typically contains content unsuitable for TTS conversion to speech (such as code, formulas, URLs) or long, formal dialogue passages that do not align with spoken language, as real dialogue is often more concise.
Therefore, when synthesizing from text dialogue data, it is necessary to preprocess the text data.
High-quality, open-source text dialogue data is first collected, including datasets like Alpaca[^166], Moss, BelleCN, ultraChat[^167], and Open-Orca[^168].
To ensure suitability for speech synthesis (TTS), heuristic rules are applied to filter out samples with high proportions of non-text elements (such as code, mathematical expressions), samples exceeding 200 words, and samples containing rare symbols.

After filtering the text, TTS models[^CosyVoice] are used to synthesize speech for each turn in the dialogues.
For consistent voice effects, the model audio stream maintains a uniform voice, while the user audio stream is sampled with varied voices to enhance the model's robustness.
The synthesized dialogue audio is arranged using simulation strategies to achieve natural timing, such as turn-taking, well-timed interruptions, and pauses to maintain fluency and naturalness.
The final dialogue audio is organized in dual-channel format: the conversation begins with a user utterance, followed by alternating user and assistant turns.
After each user turn, the assistant responds immediately; upon completion of the assistant’s turn, a sampled pause length is introduced to simulate the natural rhythm of alternating dialogue.
To better simulate real scenarios, further data augmentation can be applied.
For example, random gain adjustments can be applied to the user audio stream, and background noise randomly selected from datasets like MUSAN[^170] and DNS[^169] can be added to the user audio channel (OmniFlatten).
To simulate echo effects from a user’s microphone, portions of the audio stream can be scaled down and added to the user’s audio stream with random delays between 100 to 500 milliseconds, along with reverberation-like enhancements, helping the model adapt to real-world environments.

#### Training resources about Enhancing Conversational Abilities and Instruction Tuning

While the foundational model has been established, there remains a gap between this and a complete dialogue system.
The above model utilizes non-overlapping dialogue audio, where one party remains silent while the other speaks, failing to fully simulate real conversational dynamics.
Some speech datasets, such as \textit{Generative Expressive Conversational Speech Synthesis}[^171] and \textit{Fisher}, contain dialogues from real-world settings, providing a basis for modeling interruptions and backchannels scenarios in voice dialogue systems.

Currently, there is no suitable dataset for real-world speech instructions.
Most approaches use synthetic methods based on text instruction data to perform \textit{instruction tuning} in this stage.
Common text instruction datasets include \textit{Open Hermes} and \textit{moss-SpeechGPT-sft-data}, though they face similar challenges as text dialogue data, such as unsuitability for TTS conversion and inconsistency with spoken language conventions.
Following the synthetic processes provided by Moshi and Llama-Omni, this aims to generate instruction data in the format of (SpeechInstruction, TextInstruction, TextResponse, SpeechResponse).

The first method is synthetic generation from scratch.
Contexts and summaries are first generated by sourcing high-quality text data from sources like Wikipedia and StackExchange, producing thematic paragraphs as the dialogue foundation, referred to as “context.” Based on these contexts, dialogue summaries are generated.
Next, a specific prompt template guides the generation of complete dialogues, including context and requesting dialogues around the theme with roles as user and system.
The model is prompted to exhibit knowledge on the topic and include interruptions (backchannels) and brief turn-taking, simulating the natural flow of conversation.
To enhance dialogue diversity, additional instructions involving speech emotion and role-playing can be generated, requesting dialogues in specific tones or styles.
Furthermore, dialogues containing spelling errors or misinformation are synthesized to train the system in handling scenarios where user clarification or repetition is required.
Single-turn interactions on basic mathematics, grammar, and factual questions are also generated to ensure the system can handle simple factual tasks.
Finally, scenarios involving ethical or NSFW requests are created to train the system in declining to answer under such conditions.

The second method involves filtering and refining existing text instruction datasets.
Initially, open-source text language models paraphrase text instructions to match spoken language traits, adding fillers like “uh” and “um” to mimic natural speech tone, while converting numbers and symbols into spoken language to ensure the instructions are concise and conversational.
Generated text responses are also optimized to meet TTS output requirements, removing lengthy expressions and complex grammatical structures to make content clear and concise for TTS output.
After adjusting the instruction and response text, a TTS system converts the text to audio.

### Evaluation

Fair and comprehensive evaluation of spoken dialogue models presents a multifaceted challenge.
On the one hand, the field of spoken dialogue still lacks publicly available test sets, comprehensive evaluation metrics, and established benchmarks.
On the other hand, assessing the performance of spoken dialogue systems requires consideration from multiple perspectives.
Basic aspects include the quality of generated speech, robustness, dialogue naturalness and accuracy, as well as response speed and generation time.
Beyond these, more advanced evaluations are needed to assess multi-turn dialogue capabilities (such as long-form speech editing), interaction abilities, and the system's proficiency in audio and music understanding and generation.
Given these requirements, and in line with the comprehensive expectations for spoken dialogue systems outlined in Section \ref{section21}, we will evaluate these systems from two angles: common evaluations and advanced evaluations.
Specifically, we will assess eleven key factors: speech generation quality, text intelligence, speech intelligence, audio and music generation, audio and music understanding, multilingual capability, context learning, interaction capability, streaming latency, multimodal capability, and the safety of dialogue systems.
Finally, we will list the current benchmarks and summarize the common conclusions derived from them.

#### Common Evaluation

**Text Intelligence.** As shown in Figure \ref{fig:main} (a), text intelligence refers to the fundamental understanding and generation capabilities of the spoken dialogue model.
When evaluating text intelligence, the focus is solely on the semantic content generated by the model, without considering other aspects such as timbre, emotion, or style.
In practical evaluations of this kind, some spoken dialogue models output only text[^172], [^SALMONN], [^Qwen-Audio], [^Qwen2-Audio], [^E-chat], while others generate both text and speech **Moshi**[^Moshi], [^Mini-Omni],**Mini-Omni2**[^Mini-Omni2], or only speech[^OmniFlatten].
Regardless of the output format, we are concerned only with the generated text or the transcribed text from the speech when evaluation the text intelligence in the spoken dialogue models.
There are typically two categories of metrics and benchmarks used to assess text intelligence, MT-Metrics and Acc-Metrics.
The details are outlined as follows:

- **ACC-Metrics.** A common approach to evaluating text intelligence is to use benchmarks typically[^173], [^174], [^175], [^176], [^177], [^178], [^179], [^180], [^181], [^182] employed for large language models, such as the classic MMLU[^183] and GSM-8K [^184].
These benchmarks often include complex multiple-choice questions, which assess the model's reasoning abilities through Acc-Metrics.
Acc-Metrics refers to metrics that measure recognition accuracy, such as accuracy, F-score, and Mean Average Precision (mAP).
It is noteworthy that these benchmarks often evaluate the text-based intelligence of spoken dialogue models from various perspectives.
For example, MMLU [^183] and GSM-8K [^184] are more focused on LLM's core knowledge, Flan [^185], [^186] and Self-instruct [^187] are more focused on LLM's instruction following capability, CoQA [^188] and OpenAssistant [^189] are more focused on LLM's conversational capability.
These benchmarks often contain questions and corresponding answers.
Most of these questions are close-ended questions with short answers, so that they can have good generalization ability, any model that can generate text answers can be evaluated with these benchmarks and accuracy and F-Score can be easily adopted as the evaluation metrics.

- **MT-Metrics.**  With the development of the LLMs, LLMs can follow instructions to accomplish many complex problems, so the scope of the evaluation was further expanded to include open-ended questions.
These open-ended questions often lack standard answers, therefore it's difficult to measure them by common ACC-Metrics.
A common approach is to measure the grammatical similarity between generated and reference utterances using the metrics used to measure grammatical similarity in mechanical translation (e.g.
BLEU [^190], METEOR [^191], ROUGE [^192]).
We collectively refer to these evaluation metrics as **MT-Metrics**.
However, these metrics have certain limitations since one meaning has many different ways to convey.
So there are some metrics like BertScore [^193] focus on evaluating the semantic similarity between two sentences.
And there are also been some methods utilizing LLM to judge the effectiveness of the responses which focusing on human preference [^194], [^195].
The results of these large model-based especially GPT4o-based ratings of evaluation metrics demonstrated a high degree of correlation with human.

**Speech Quality.** Speech quality is one of the fundamental aspects for evaluating the performance of spoken dialogue systems, as it is closely tied to the experience of users.
There are two common dimensions for assessing speech quality: the clarity and naturalness (expressiveness and prosody) of the generated audio, and the robustness of the generated speech, such as the presence of missing or extra words.
The former is typically evaluated by using subjective MOS (Mean Opinion Score) ratings, while the latter is commonly assessed by using WER (Word Error Rate) or CER (Character Error Rate) metrics.

**Streaming Latency.**
In addition to evaluating the quality of text understanding and generated speech, the speed at which a spoken dialogue system generates speech responses is also crucial.
This necessitates the ability to stream both the comprehension and generation of speech in real time, achieving an effect of generating speech while speaking[^IntrinsicVoice], **Moshi**[^Moshi], [^LLaMA-Omni].
To assess the streaming performance of a model, one typically measures the time taken to generate the first token of speech (i.e., the waiting time after the user finishes speaking) and calculates the overall Real-Time Factor (RTF) of the spoken dialogue model's response.
The RTF value is obtained by dividing the total duration of the speech segment generated by the model by the time taken by the model to generate that response.

#### Advanced Evaluation

**Speech Intelligence.**
Evaluating the speech intelligence of spoken dialogue systems is one of the key aspects.
The definition of speech intelligence in spoken dialogue systems is discussed in detail in Section \ref{section212}.
Given that speech intelligence encompasses a wide range of application scenarios, we address the evaluation separately for the understanding and generation components during the assessment.

- **Understanding.** Ordinary cascaded spoken dialog models based on ASR getting text input will loss many paralinguistic information like speaking style, accent, emotion, etc.
Thus many spoken dialogue models [^E-chat], [^ParalinGPT], [^Spoken-LLM] devoted into helping dialog models understand the paralinguistic information.
Evaluating this capability can start from two aspects: a) the accuracy of the paralinguistic information's understanding, b) the ability of **automatically** generating appropriate and coherent content responses and acoustic information based on the varying acoustic input.
**For the former**, since the classes of the paralinguistic information are always limited, for example, sentiments are generally categorized as neural, negative, positive.
So the researchers always use Accuracy or F-Score to evaluate the models' paralinguistic information understanding capability.
Recently, there are many studies [^196], [^197], [^198], [^E-chat], [^Spoken-LLM], [^199], [^200] available for researchers to use in identifying speech emotions in the dialogue scenes.
In addition to recognizing speech emotions, recent benchmarks [^201], [^202] has also begun to investigate the influence of speaker age, accent, and other factors on the evaluation of spoken dialogue models.
**For the latter**, recent work[^E-chat] has increasingly focused on the possibility of generating appropriate content responses based on acoustic information from the input.
The current evaluation methods usually transcript the output audio into text through Automatic Speech Recognition and then evaluate the relevance between generated content and the reference content in the internal dataset.
Evaluations are usually conducted in text, so commonly used evaluation metrics are as the same as in the section \ref{eval_text}, like BLEU and METEOR, which are used to measure the similarity between two sentences.
Currently, there is limited research exploring whether spoken dialogue models can autonomously generate appropriate acoustic responses based on varying acoustic information, making it a promising area for future investigation.

- **Generation.** In the generation component, evaluating the speech intelligence of spoken dialogue systems primarily focuses on controllability, i.e., the ability of the dialogue model to respond in a user-specified style and timbre in the zero-shot scenarios.
There are various dimensions to assess style, such as pitch, speech rate, energy, emotion, and accent, among others.
ACC-metrics can be used to evaluate whether the spoken dialogue model can generate speech in the desired style.
Additionally, the evaluation of voice cloning capabilities within the model can borrow metrics from the zero-shot TTS domain[^VALL-E], [^203], [^204], [^205], using speaker similarity indices[^WavLM].
Currently, there are few models that explore the generation of speech intelligence in spoken dialogue systems, and this area warrants further refinement and exploration in future work.

**Audio Understanding and Generation.**
In real-world scenarios, the broader definition of speech modality encompasses not only clear human speech but also a wide range of natural sounds such as dog barking and bird chirping, all of which can be considered forms of audio.
Evaluating the ability of spoken dialogue models to understand and generate such audio is a critical aspect of assessing the model’s performance.

- **Audio Understanding.** On the audio comprehension side, various sub-tasks are commonly employed to measure a system's capacity to understand audio, including tasks such as Audio Captioning (AudioCap) [^206], Sound Event Detection (SED) [^207], audio classification, and audio-motivated creative writing[^Qwen-Audio], among others.
The core of these tasks lies in evaluating the model’s ability to process and interpret the complex acoustic information embedded within the audio.
For tasks like audio classification and SED, which involve fixed outputs, evaluation is relatively straightforward, typically using objective metrics such as accuracy or Mean Average Precision (mAP).
However, for the AudioCap task, the problem is generally open-ended, meaning there are no fixed answers.
As a result, existing evaluation methods are primarily based on measuring the similarity between the generated text and the reference text, using traditional metrics such as BLEU [^190] and METEOR [^191], or newer evaluation approaches involving large language models such as GPT-4o [^194].
In the case of audio-motivated creative writing, where the objective is to generate inventive descriptions from a given audio input, evaluation typically relies on subjective measures, given the divergent nature of the creative process involved.

- **Audio Generation.** Additionally, on the audio generation side, producing high-quality audio should be considered an advanced capability for a conversational spoken dialogue model.
However, as most current spoken dialogue systems lack the ability to generate audio, this remains an area for further exploration in the future end-to-end spoken dialogue systems.
The evaluation of generated audio can draw from methods used in the text-to-audio domain[^208], [^Make-An-Audio].
Typically, such evaluations focus on the quality of the generated audio itself, using metrics such as Mean Opinion Score (MOS) and the similarity between generated and target audio.
Objective evaluation metrics for audio similarity often include Fréchet Distance (FD), Inception Score (IS), Kullback-Leibler (KL) divergence, Fréchet Audio Distance (FAD), and CLAP score.
Specifically, Fréchet Audio Distance (FAD) [^209] is adapted from the Fréchet Inception Distance (FID) to the audio domain and serves as a reference-free perceptual metric that quantifies the distance between the generated and ground-truth audio distributions.
The Inception Score (IS) is an effective metric that evaluates both the quality and diversity of generated audio.
KL divergence is computed at the paired sample level between generated and ground-truth audio, based on the label distribution and averaged to produce a final result.
Fréchet Distance (FD) evaluates the similarity between the generated and ground-truth audio distributions.
FD, KL, and IS are built upon the PANNs model [^210], which takes mel-spectrograms as input.
In contrast, FAD uses VGGish [^211] as an audio classifier, processing raw audio waveforms as input.
The CLAP score, adapted from the CLIP score [^212], is a reference-free metric used to assess audio-text alignment and strongly correlates with human perception.

**Music Understanding and Generation.** In advanced spoken dialogue models, the evaluation of music modality understanding and generation follows a methodology similar to that used for audio modality.
Unlike Audio Understanding, which only requires a general description of the events that occur in the audio, Music Understanding requires appreciating the style and genre of music, understanding its keys, themes, and other rich information.
For classification, emotion recognition tasks in music, common metrics such as accuracy can be used.
For music captioning task, MusicCaps [^MusicLM] offers a general dataset for evaluating a model's music understanding capability.
For music analysis, Nsynth [^213] provides rich note data information.
In terms of evaluation for music generation, subjective Mean Opinion Score (MOS) assessments or measures of similarity between generated music and target music are commonly used.

**Multilingual Capability.** The ability to speak multiple languages is also required for a spoken dialogue model, but most current models [^214], [^Spoken-LLM], [^ParalinGPT], [^172], [^215], [^E-chat] only focus on English and Chinese.
A naive idea is to directly evaluate spoken dialogue models' capability in speech-to-speech or speech-to-text translation tasks [^216], [^217].
These evaluations can be done with common machine learning metrics like BLEU [^190] or BertScore [^193].
However, evaluating the capability of translation is insufficient to measure the model's multilingual conversational ability, and further exploration is still needed in this area of evaluation.
Explicitly requiring a spoken dialogue model to perform speech translation is not a typical use case in conversational scenarios.
In most cases, when a user asks a question in a different language or with a distinct accent, the model is expected to automatically respond in the same language that the user is using.
In this context, it seems more reasonable to evaluate the accuracy of the model’s generated speech in terms of language identification, combined with subjective human assessments, as a more intuitive and appropriate evaluation method.

**Context Learning.** The context learning capability is crucial for maintaining the coherence of an entire conversation.
Similar to a memory function, the challenge lies in how to preserve this capability when relying solely on speech.
Typically, the evaluation of a spoken dialogue model's context learning ability depends on specific long-duration dialogue test sets, after which standard MT-Metrics or Acc-Metrics used in text intelligence evaluations can be applied.
For instance, a model's context learning capability can be assessed by evaluating its QA performance based on the given context [^218].
However, it is important to note the relevance of editing scenarios in long-duration spoken dialogues.
In real spoken dialogue scenarios, the users will modify some certain key information, the model needs to promptly understand and respond accordingly, e.g., the users offer wrong information for solving a problem and modify the condition in the next dialog.
So how to evaluate the model's online understanding ability is still needed further study.

**Interaction Capability.**
Interactive ability is also an essential metric for assessing the advanced capabilities of spoken dialogue systems.
As illustrated in Figure \ref{fig:main} (b), basic interactive ability refers to the system's capacity to allow users to interrupt the conversation at any time.
In this context, it is crucial to evaluate whether the spoken dialogue model can promptly comprehend the user's new input and halt its current response.
This is commonly measured using accuracy.
Furthermore, it is important to assess whether the model can generate a coherent and appropriate response based on the new input, which ties back to previous evaluation standards related to text and speech intelligence.

In addition, in real-world scenarios, beyond basic interruptions, various discourse markers such as "okay", "haha" are often used to indicate interaction.
Current spoken dialogue systems[^dGSLM] typically track the frequency of these markers as a standard evaluation metric.
Looking ahead, it may be valuable to assess whether future spoken dialogue models can effectively and appropriately interrupt human speakers, which could also represent a key dimension for evaluation the interaction capability.

**Multimodal Capability.** Spoken dialogue models primarily focus on the audio modality for both input and output.
However, considering the close coupling between video and audio modalities in practical applications of dialogue systems, recent advancements in spoken dialogue models have incorporated the understanding of video and images in the input stage (**VITA**[^VITA], [^Baichuan-Omni], [^219]), indicate that future spoken dialogue models need to simultaneously understand visual information and audio information to achieve real-time Audio-Visual Understandings.
The evaluation of such models generally still focuses on the evaluation of dialogue quality, that is, whether the generated dialogue and the reference dialogue are similar.
Therefore, this aspect can still be evaluated using metrics such as BLEU [^190] and METEOR [^191] to assess sentence semantic similarity.
However, research in this area also focuses on the understanding of visual information, and how to evaluate the model's correct understanding of real-time visual information in dialogue is also a difficulty, still can be a future benchmark direction.

**Security.** Security is also an integral part of the evaluation, how to ensure that the output of the model complies with ethical and social norms is a critical aspect.
Spoken dialogue models may encounter security issues such as harmful content generation, privacy pitfalls, bias, and adversarial attacks.
There has been considerable research progress in evaluating text modalities [^220].
The commonly used metric is to evaluate the attack success rate of injection attacks and so on.
However, there are relatively few evaluation methods in the field of speech modality.
How to construct a dataset for attacking spoken dialogue models, avoid poisoning of speech data, and evaluate the model's speech defense capabilities as benchmarks are required further research in the field of spoken dialogue model evaluation in the future.

### Benchmark

We list the common benchmarks for evaluating voice dialogue systems in the table\ref{table:benchmark}, and briefly introduce each benchmark in this section.

- **VoiceBench.**
VoiceBench's[^221] Key evaluation dimensions include general knowledge, instruction-following ability, and safety compliance.
The benchmark incorporates both synthetic and real spoken instructions to simulate diverse speaker styles, environmental conditions, and content variations.
It challenges systems with tasks involving accent adaptability, handling noisy environments, and robustness against content irregularities such as grammatical errors, disfluencies, and mispronunciations.
Additionally, it explores the systems' resilience under varying speaker characteristics (age, pitch, and speaking speed) and environmental challenges like reverberation, background noise, and far-field effects.

- **SUPERB.** [^222]
The benchmark evaluates speech processing models across multiple dimensions, including content recognition, speaker modeling, semantic understanding, and paralinguistic analysis.
Tasks in content recognition cover phoneme recognition, automatic speech recognition, keyword spotting, and query-by-example spoken term detection, focusing on transcription and content detection accuracy.
Speaker modeling involves tasks like speaker identification, automatic speaker verification, and speaker diarization to assess speaker-related features.
Semantic understanding includes intent classification and slot filling, testing models' ability to infer high-level meaning directly from raw audio.
Paralinguistic analysis focuses on emotion recognition, capturing models' ability to interpret affective cues from speech.
The evaluation framework uses publicly available datasets and conventional metrics to provide a standardized testbed for assessing generalizability and task-specific performance.

- **AudioBench.**
AudioBench[^223] evaluates spoken dialogue models across three primary dimensions: speech understanding, audio scene understanding, and voice (paralinguistic) understanding.
It encompasses eight distinct tasks and leverages 26 datasets, including seven newly developed datasets.
The evaluation emphasizes models' ability to handle instruction-following tasks conditioned on audio signals, addressing aspects such as speech recognition accuracy, environmental sound interpretation, and paralinguistic feature extraction (e.g., emotion, gender, accent).


- **AirBench.**
AIR-Bench[^202] assesses the capabilities of Spoken dialogue models to understand and interact based on various audio types, including human speech, natural sounds, and music.
It consists of two primary components: a foundation benchmark with 19 specific audio tasks and over 19,000 single-choice questions, and a chat benchmark featuring more than 2,000 open-ended audio-prompted questions.
The foundation benchmark evaluates fundamental skills such as speech recognition, acoustic scene classification, and music genre identification, focusing on specific subtasks to diagnose model weaknesses.
The chat benchmark tests the models' ability to handle complex, real-world audio-based queries, including mixed audio with varying loudness and temporal offsets.
AIR-Bench introduces a novel audio mixing strategy to simulate complex real-world scenarios and employs GPT-4-based evaluation to judge model-generated hypotheses against reference answers.

- **SpokenWOZ.**
SpokenWOZ[^224] evaluates task-oriented dialogue (TOD) systems in spoken scenarios, addressing challenges unique to spoken conversations, such as incremental processing, disfluencies, incomplete utterances, and Automatic Speech Recognition (ASR) noise.
It introduces novel metrics to assess performance in tasks like cross-turn slot detection and reasoning slot detection, which require integrating information across multiple turns and reasoning from implicit cues.
The benchmark encompasses multi-domain, human-to-human dialogues with diverse speech characteristics, testing systems on both textual and auditory inputs through large-scale annotated datasets with over 200,000 utterances and 249 hours of audio

- **SD-EVAL.**
SD-Eval[^201] evaluates spoken dialogue models across multiple dimensions, focusing on both spoken understanding and response generation beyond textual content.
It assesses models' abilities to process three key types of information embedded in speech: content (e.g., linguistic meaning), paralinguistic cues (e.g., emotion, accent, age), and environmental context (e.g., background sounds).
The benchmark consists of four sub-tasks—emotion, accent, age, and environment—constructed from diverse datasets and totaling 7,303 utterances spanning 8.76 hours.

- **SuperCLUE.**
SuperCLUE evaluates spoken dialogue systems across four main dimensions: voice interaction, general capabilities, scenario applications, and response speed.
Key metrics include interruption recognition, speech tone adjustment, semantic understanding, naturalness of speech, and memory accuracy.
Additionally, it measures real-time data retrieval, reasoning ability, compliance with commands, and multilingual translation accuracy.
Scenario-specific applications like emotional counseling, health consultations, and customer service are assessed for precision and effectiveness.
The final aspect is response timeliness, focusing on latency and delay management.However, this benchmark is not open source and focuse on Mandarine ability


- **MMAU.**
MMAU[^225] evaluates spoken dialogue models across multiple dimensions, encompassing 27 distinct tasks divided into reasoning and information extraction categories.
It assesses models on their ability to comprehend and reason about speech, sound, and music by leveraging advanced cognitive skills and domain-specific knowledge.
Key evaluated areas include temporal event reasoning, speaker role mapping, emotional tone interpretation, eco-acoustic knowledge, phonemic stress pattern analysis, and melodic structure interpretation.
It examines not just basic recognition or transcription capabilities but also models' proficiency in complex reasoning, contextual understanding, and the ability to extract and apply world knowledge.
Additionally, MMAU scrutinizes performance consistency across varying difficulty levels, testing systems' depth of reasoning and robustness in real-world audio scenarios.

## 结论

在这项工作中, 我们系统性地回顾了与口语对话模型相关的研究, 根据两种范式进行分类: 级联口语对话模型和端到端口语对话模型.

此外, 我们提供了口语对话模型背后的核心技术的详细概述, 包括语音表示, 训练范式, 流式双工系统和交互机制.
- 在语音表示模块中, 我们从输入和输出的角度分类和解释语音表示, 着重于语义和声学表示的不同类型.
- 在训练范式模块中, 我们深入讨论了口语对话模型对齐的五种方式, 多阶段训练策略, 模型架构和生成范式.
- 随后, 我们深入分析了口语对话模型流式输入和输出, 以及相关的双工交互技术.
- 最后, 我们整理了与口语对话模型相关的关键训练资源, 评估指标和基准.我们特别关注了在不同场景下对口语对话模型不同层次智能的评估.

需要注意的是, 由于口语对话模型是一项相对较新且新兴的技术, 许多方面如语义和声学表示, 仍然缺乏成熟的范式.
因此, 在每个部分的末尾, 我们专门加入了一个专门的讨论部分, 探讨这些开放性问题, 我们希望这项调查能够为口语对话系统领域的进一步发展做出贡献.

## Appendix

\section{Resources about Music and Sound Datasets}
\label{music datasets}

This section lists commonly used music and sound datasets.
These datasets cover different modalities, including environmental sounds, music, and emotional sounds, and provide some help for the development of future voice dialogue systems.
The table \ref{table:soundmusicdataset} shows the basic information of each dataset, including the dataset name, number of samples, dataset link, and modality type.

\begin{table}[h]
\centering
\caption{Music and Non-Speech Sound Datasets}
\label{table:soundmusicdataset}
% \renewcommand{\arraystretch}{1.3}

% \hspace*{-2.2cm}
\resizebox{1\linewidth}{!}{%
% \begin{Large}
\begin{tabular}{lcccl}
\hline
\textbf{Dataset} & \textbf{Size} & \textbf{URL} & \textbf{Modality} \\ \hline

ESC-50 [^226]             & 2,000 clips (5s each)           & \url{https://github.com/karoldvl/ESC-50}             & Sound \\
UrbanSound8K [^227] & 8,732 clips (<=4s each)         & \url{https://urbansounddataset.weebly.com/urbansound8k.html} & Sound \\
AudioSet [^228]  & 2000k+ clips (10s each)                   & \url{https://research.google.com/audioset/}           & Sound \\
TUT Acoustic Scenes 2017 [^229] & 52,630 segments            & \url{https://zenodo.org/record/4AudioGPT15}                & Sound \\
Warblr      & 10,000 clips                    & \url{https://warblr.net/}                              & Sound \\
FSD50K [^230]          & 51,197 clips (total 108.3 hours) & \url{https://zenodo.org/record/4060432}              & Sound \\
DCASE Challenge [^231]  & varies annually                 & \url{http://dcase.community/}                          & Sound \\
IRMAS [^232]         & 6,705 audio files (3s each)     & \url{https://www.upf.edu/web/mtg/irmas}               & Music \\
FMA  [^233] & 106,574 tracks            & \url{https://github.com/mdeff/fma}                    & Music \\
NSynth [^213]            & 305,979 notes                   & \url{https://magenta.tensorflow.org/datasets/nsynth}  & Music \\
EMOMusic               & 744 songs                       & \url{https://cvml.unige.ch/databases/emoMusic/}        & Music \\
MedleyDB [^234] & 122 multitrack recordings     & \url{https://medleydb.weebly.com/}                     & Music \\
% VIVAE [^235]                   & 1085 clips      & \url{https://zenodo.org/records}                     & emotion vocalizations \\
MagnaTagATune    & 25,863 clips (30s each)         & \url{https://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset} & Music \\
MUSDB[^236] & 150 songs & \url{https://paperswithcode.com/dataset/musdb18} & Music \\
M4Singer[^237] & 700 songs & \url{https://github.com/M4Singer/M4Singer} & Music \\
Jamendo &  600k songs & \url{https://www.jamendo.com/?language=en} & Music \\

\hline

\end{tabular}%
% \end
}

\end{table}


\section{Open-source Spoken Dialogue Models}

In this section, we provide a comprehensive list of publicly available and open-source spoken dialogue models in Table \ref{table:opensource_model}.

\begin{table}[]
\renewcommand{\arraystretch}{1.4}
\centering
\caption{A comprehensive list of publicly available  spoken dialogue models and their URL}
\resizebox{1\linewidth}{!}{%
\begin{tabular}{cc}
\hline
\textbf{Model} & \textbf{URL}                                      \\ \hline
AudioGPT       & \url{https://github.com/AIGC-Audio/AudioGPT}       \\
SpeechGPT      & \url{https://github.com/0nutation/SpeechGPT}              \\
Freeze-Omni & \url{https://github.com/VITA-MLLM/Freeze-Omni}              \\
Baichuan-Omni  & \url{https://github.com/westlake-baichuan-mllm/bc-omni} \\
GLM-4-Voice       & \url{https://github.com/THUDM/GLM-4-Voice }      \\
Mini-Omni     & \url{https://github.com/gpt-omni/mini-omni  }      \\
Mini-Omni2     & \url{https://github.com/gpt-omni/mini-omni2  }      \\
FunAudioLLM    & \url{https://github.com/FunAudioLLM}             \\
Qwen-Audio    & \url{https://github.com/QwenLM/Qwen-Audio }             \\
Qwen2-Audio    & \url{https://github.com/QwenLM/Qwen2-Audio }             \\
LLaMA3.1       & \url{https://www.llama.com}              \\
Audio Flamingo & \url{https://github.com/NVIDIA/audio-flamingo}             \\
Spirit LM      & \url{https://github.com/facebookresearch/spiritlm }             \\
dGSLM          & \url{https://github.com/facebookresearch/fairseq/tree/main/examples/textless_nlp/dgslm}              \\
Spoken-LLM     & \url{https://arxiv.org/abs/2305.11000}              \\
LLaMA-Omni     & \url{https://github.com/ictnlp/LLaMA-Omni }             \\
Moshi          & \url{https://github.com/kyutai-labs/moshi}          \\
SALMONN        & \url{https://github.com/bytedance/SALMONN}              \\
LTU-AS         & \url{https://github.com/YuanGongND/ltu}             \\
VITA           & \url{https://github.com/VITA-MLLM/VITA}             \\
SpeechGPT-Gen  & \url{https://github.com/0nutation/SpeechGPT}              \\
WavLLM &
\url{https://github.com/microsoft/SpeechT5/tree/main/WavLLM}\\
Westlake-Omni  & \url{https://github.com/xinchen-ai/Westlake-Omni}              \\ MooER-Omni  & \url{https://github.com/MooreThreads/MooER}              \\ Hertz-dev  & \url{https://github.com/Standard-Intelligence/hertz-dev}              \\ Fish-Agent  & \url{https://github.com/fishaudio/fish-speech}              \\ SpeechGPT2   & \url{https://0nutation.github.io/SpeechGPT2.github.io/}              \\ \hline
\end{tabular}}
\label{table:opensource_model}
\end{table}

\section{Open-source Codec Models}

In this section, we provide a comprehensive list of publicly available and open-source codec models, as shown in Table ~\ref{tab:all_codec}.


\begin{table}[htbp]
\centering
\caption{A comprehensive list of publicly available codec models and their URL}
\label{tab:all_codec}
\resizebox{1\linewidth}{!}{%
\begin{tabular}{cc}
\toprule
\textbf{Model}                    & \textbf{URL}  \\
\midrule

Encodec [^EnCodec]                  & \url{https://github.com/facebookresearch/encodec}          \\
SoundStream [^SoundStream]              & \url{https://github.com/wesbz/SoundStream}                 \\
DAC [^DAC]                     & \url{https://github.com/descriptinc/descript-audio-codec}  \\
WavTokenizer  [^WavTokenizer]           & \url{https://github.com/jishengpeng/WavTokenizer}  \\
SpeechTokenizer [^SpeechTokenizer] & \url{https://github.com/ZhangXInFD/SpeechTokenizer}        \\
SNAC  [^SNAC]           & \url{https://github.com/hubertsiuzdak/snac}  \\
SemantiCodec [^238]             & \url{https://github.com/haoheliu/SemantiCodec-inference}   \\
**Mimi**[^Moshi] &
\url{https://github.com/kyutai-labs/moshi} \\
HiFi-Codec [^239]              & \url{https://github.com/yangdongchao/AcademiCodec}        \\
FunCodec  [^240]               & \url{https://github.com/modelscope/FunCodec }              \\
APCodec  [^241]                & \url{https://github.com/YangAi520/APCodec/tree/main}       \\
AudioDec [^129]                 & \url{https://github.com/facebookresearch/AudioDec}         \\
FACodec  [^242]                & \url{https://github.com/lifeiteng/naturalspeech3\_facodec} \\
Language-Codec  [^243]         & \url{https://github.com/jishengpeng/Languagecodec}         \\
XCodec  [^244]           & \url{https://github.com/zhenye234/xcodec}  \\
TiCodec  [^TiCodec]           & \url{https://github.com/y-ren16/TiCodec}  \\
SoCodec  [^245]           & \url{https://github.com/hhguo/SoCodec}  \\
FUVC  [^246]           & \url{https://github.com/z21110008/FUVC}  \\
HILCodec [^247]           & \url{https://github.com/aask1357/hilcodec}  \\
LaDiffCodec [^248]           & \url{https://github.com/haiciyang/LaDiffCodec}  \\
LLM-Codec [^249]           & \url{https://github.com/yangdongchao/LLM-Codec}  \\
SpatialCodec [^250]           & \url{https://github.com/XZWY/SpatialCodec}  \\
BigCodec [^BigCodec] &
\url{https://github.com/Aria-K-Alethia/BigCodec} \\
SuperCodec [^251] &
\url{https://github.com/exercise-book-yq/Supercodec} \\
RepCodec [^252] &
\url{https://github.com/mct10/RepCodec} \\
EnCodecMAE [^253] &
\url{https://github.com/habla-liaa/encodecmae} \\
MuCodec [^254] &
\url{https://github.com/xuyaoxun/MuCodec} \\
SPARC [^255] &
\url{https://github.com/Berkeley-Speech-Group/Speech-Articulatory-Coding} \\
BANC [^256] &
\url{https://github.com/anton-jeran/MULTI-AUDIODEC} \\
SpeechRVQ [^257] &
\url{https://huggingface.co/ibm/DAC.speech.v1.0} \\
QINCo [^258] &
\url{https://github.com/facebookresearch/Qinco} \\
SimVQ [^259] &
\url{https://github.com/youngsheen/SimVQ} \\

\bottomrule
\end{tabular}}
\end{table}

## 参考文献

[^Moshi]: [Moshi/Mimi](../Models/SpeechLM/Interaction/2024.09.17_Moshi.md)
[^SpeechGPT]: [SpeechGPT](../Models/SpeechLM/Interaction/2023.05.18_SpeechGPT.md)
[^Mini-Omni2]: [Mini-Omni2](../Models/SpeechLM/Interaction/2024.10.15_Mini-Omni2.md)
[^LLaMA3]: [LLaMA3.1](../Models/TextLM/2024.07.31_LLaMA3.md)
[^AudioGPT]: [AudioGPT](../Models/SpeechLM/Interaction/2023.04.25_AudioGPT.md)
[^FunAudioLLM]: speechteam2024funaudiollm
[^Spoken-LLM]: lin2024advancing
[^ParalinGPT]: lin2024paralinguistics
[^E-chat]: xue2023chat
[^Qwen2-Audio]: [Qwen2-Audio](../Models/SpeechLM/ST2T/2024.07.15_Qwen2-Audio.md)
[^Qwen-Audio]: [Qwen-Audio](../Models/SpeechLM/ST2T/2023.11.14_Qwen-Audio.md)
[^LTU-AS]: gong2023joint
[^SALMONN]: tang2023salmonn
[^VITA]: [VITA](../Models/SpeechLM/Interaction/2024.08.09_VITA.md)
[^dGSLM]: nguyen2023generative
[^FSQ]: mentzer2023finite
[^Mini-Omni]: xie2024mini
[^OmniFlatten]: zhang2024omniflatten
[^IntrinsicVoice]: zhang2024intrinsicvoice
[^Whisper]: radford2023robust
[^VITS2]: kong2023vits2
[^FastSpeech2]: ren2020fastspeech
[^SpeechGPT-Gen]: zhang2024speechgpt
[^LLaMA]: touvron2023llama
[^MusicLM]: agostinelli2023musiclm
[^MusicGen]: copet2024simple
[^MeLoDy]: lam2024efficient
[^S20240826]: Foundation Models for Music: A Survey
[^Make-An-Audio]: huang2023make
[^AudioLDM]: liu2023audioldm
[^AudioLDM2]: liu2024audioldm
[^LLaMA-Omni]: fang2024llama
[^EMOVA]: chen2024emova
[^Emotion2Vec]: ma2023emotion2vec
[^LoRA]: hu2021lora
[^SpeechVerse]: das2024speechverse
[^AudioFlamingo]: kong2024audio
[^BEATs]: chen2022beats
[^BLIP2]: li2023blip
[^Baichuan-Omni]: li2024baichuan
[^VALL-E]: wang2023neural
[^VALL-E-X]: zhang2023speak
[^MegaTTS]: jiang2024mega
[^MegaTTS2]: jiang2023mega
[^CosyVoice]: du2024cosyvoice
[^ParlerTTS]: lyth2024natural
[^MaskGCT]: wang2024maskgct
[^F5-TTS]: chen2024f5
[^TextrolSpeech]: ji2024textrolspeech
[^PromptTTS]: guo2023prompttts
[^PromptTTS2]: leng2023prompttts
[^InstructTTS]: yang2024instructtts
[^ControlSpeech]: ji2024controlspeech
[^FisherCorpus]: cieri2LLaMA3fisher
[^Spectron]: marge2022spoken
[^PSLM]: mitsui2024pslm
[^SpiRit-LM]: nguyen2024spirit
[^Freeze-Omni]: wang2024freezeomnismartlowlatency
[^SyncLLM]: veluri2024beyond
[^VQ-APC]: chung2020vector
[^DNNSeg]: Acquiring language from speech by learning to remember and predict
[^AudioALBERT]: chi2021audio
[^HuBERT]: hsu2021hubert
[^Mockingjay]: liu2020mockingjay
[^WavTokenizer]: ji2024wavtokenizer
[^EnCodec]: defossez2022high
[^DAC]: kumar2024high
[^SoundStream]: zeghidour2021soundstream
[^Wav2Vec]: schneider2019wav2vec
[^WavLM]: chen2022wavlm
[^Wav2Vec2]: baevski2020wav2vec
[^BERT]: devlin2018bert
[^S20221005]: Exploration of A Self-Supervised Speech Model: A Study on Emotional Corpora
[^XLS-R]: babu2021xls
[^USDM]: kim2024unified
[^Seamless]: barrault2023seamless
[^MFCC]: Comparison of different implementations of mfcc
[^Transformer]: vaswani2017attention
[^Gat2022Augmentation]: Augmentation Invariant Discrete Representation for Generative Spoken Language Modeling
[^Align-SLM]: lin2024alignslmtextlessspokenlanguage
[^SPIRAL]: huang2022spiral
[^SpeechTokenizer]: zhang2023speechtokenizer
[^LauraGPT]: du2023lauragpt
[^S20220521]: mohamed2022self
[^HiFi-GAN]: kong2020hifi
[^Polyak2021Speech]: Speech Resynthesis from Discrete Disentangled Self-Supervised Representations.
[^textless-lib]: kharitonov2022textless
[^CoT]: wei2022chain
[^VoiceBox]: le2024voicebox
[^Qwen]: bai2023qwen
[^SNAC]: siuzdak2024snac
[^FlowMatching]: lipman2022flow
[^TiCodec]: ren2024fewer
[^Transfusion]: zhou2024transfusion
[^Show-o]: xie2024show
[^Single-Codec]: li2024single
[^BigCodec]: xin2024bigcodec
[^GPT-4]: achiam2023gpt
[^Qwen2]: yang2024qwen2
[^Gage1994New]: Gage1994New
[^BPE]: sennrich2015neural
[^AudioPaLM]: rubenstein2023audiopalm
[^WavLLM]: hu2024wavllm
[^XTTS]: casanova2024xtts
[^DPO]: rafailov2024direct
[^Spectron]: nachmani2023spoken
[^VoxtLM]: maiti2024voxtlm
[^Mistral7B]: chaplot2023albert
[^TWIST]: hassid2024textually
[^111]: jiang2024mixtral
[^112]: jin2024efficientmllm
[^113]: li2024surveybenchmarksmultimodallarge
[^114]: anastassiou2024seed
[^115]: zhang2024speechalign
[^116]: chen2024enhancing
[^117]: bengio2009curriculum
[^118]: graves2006connectionist
[^119]: peng2024voicecraft
[^120]: yang2023uniaudio
[^121]: yu2023megabyte
[^122]: kim2021conditional
[^123]: schulman2017proximal
[^124]: sutton1999policy
[^125]: wallace2024diffusion
[^126]: cideron2024musicrl
[^127]: huang2023survey
[^128]: bai2018empirical
[^129]: wu2023audiodec
[^130]: wang2024full
[^131]: wu2021u2++
[^132]: ma2024language
[^133]: raux2009finite
[^134]: sacks1974simplest
[^135]: duncan1972some
[^136]: duncan1974signalling
[^137]: hara2019turn
[^138]: lala2017attentive
[^139]: lala2019smooth
[^140]: ekstedt2020turngpt
[^141]: khouzaimi2016reinforcement
[^142]: hara2018prediction
[^143]: lin2022duplex
[^144]: jin2021duplex
[^145]: CleanS2S
[^146]: meng2024sd
[^147]: anil2023palm
[^148]: bu2017aishell
[^149]: du2018aishell
[^150]: shi2020aishell
[^151]: zen2019libritts
[^152]: rousseau2012ted
[^153]: wang2021voxpopuli
[^154]: panayotov2015librispeech
[^155]: pratap2020mls
[^156]: zhang2022wenetspeech
[^157]: chen2021gigaspeech
[^158]: veaux2013voice
[^159]: ljspeech17
[^160]: ardila2019common
[^161]: mei2024wavcaps
[^162]: kahn2020libri
[^163]: galvez2021people
[^164]: tang2021kespeech
[^165]: he2024emilia
[^166]: maeng2017alpaca
[^167]: ding2023enhancing
[^168]: OpenOrca
[^169]: reddy2Moshiinterspeech
[^170]: snyder2015musan
[^171]: liu2024generative
[^172]: shu2023llasm
[^173]: talmor2018commonsenseqa
[^174]: liang2022holistic
[^175]: zellers2019hellaswag
[^176]: clark2018think
[^177]: sakaguchi2021winogrande
[^178]: chen2021evaluating
[^179]: zhong2023agieval
[^180]: mishra2021cross
[^181]: wang2022super
[^182]: feng2022mmdialog
[^183]: hendrycks2020measuring
[^184]: cobbe2021training
[^185]: longpre2023flan
[^186]: wei2021finetuned
[^187]: wang2022self
[^188]: reddy2019coqa
[^189]: kopf2024openassistant
[^190]: papineni2SpeechGPTbleu
[^191]: banerjee2AudioGPTmeteor
[^192]: lin2LLaMA3rouge
[^193]: zhang2019bertscore
[^194]: zheng2023judging
[^195]: liu2023g
[^196]: goel2024audio
[^197]: busso2008iemocap
[^198]: poria2018meld
[^199]: firdaus2020meisd
[^200]: busso2016msp
[^201]: ao2024sd
[^202]: yang2024air
[^203]: shen2023naturalspeech
[^204]: ji2024mobilespeech
[^205]: wang2024ham
[^206]: kim2019audiocaps
[^207]: mesaros2021sound
[^208]: huang2023make2
[^209]: kilgour2018fr
[^210]: kong2020panns
[^211]: hershey2017cnn
[^212]: hessel2021clipscore
[^213]: engel2017neural
[^214]: gong2023listen
[^215]: wang2023blsp
[^216]: jia2022cvss
[^217]: wang2020covost1
[^218]: lipping2022clotho
[^219]: park2024let
[^220]: dong2024attacks
[^221]: chen2024voicebench
[^222]: yang2021superb
[^223]: wang2024audiobench
[^224]: si2024spokenwoz
[^225]: sakshi2024mmau
[^226]: piczak2015esc
[^227]: salamon2014dataset
[^228]: gemmeke2017audio
[^229]: mesaros2016tut
[^230]: fonseca2021fsd50k
[^231]: mesaros2017dcase
[^232]: bosch2012comparison
[^233]: fma_dataset
[^234]: bittner2014medleydb
[^235]: holz2022variably
[^236]: rafii2017musdb18
[^237]: zhang2022m4singer
[^238]: liu2024semanticodec
[^239]: yang2023hifi
[^240]: du2024funcodec
[^241]: ai2024apcodec
[^242]: ju2024naturalspeech
[^243]: ji2024language
[^244]: ye2024codecdoesmatterexploring
[^245]: guo2024socodec
[^246]: zheng2024fuvc
[^247]: ahn2024hilcodec
[^248]: yang2024generative
[^249]: yang2024uniaudio
[^250]: xu2024spatialcodec
[^251]: zheng2024supercodec
[^252]: huang2023repcodec
[^253]: pepino2023encodecmae
[^254]: xu2024mucodec
[^255]: cho2024articulatory
[^256]: ratnarajah2023m3
[^257]: shechtman24_interspeech
[^258]: huijben2024residual
[^259]: zhu2024addressing
