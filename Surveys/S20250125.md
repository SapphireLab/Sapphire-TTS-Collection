# Audio-Language Models for Audio-Centric Tasks: A Survey

<details>
<summary>基本信息</summary>

- 标题: "Audio-Language Models for Audio-Centric Tasks: A Survey"
- 作者:
  - 01 Yi Su (College of Computer Science and Technology, Changsha)
  - 02 Jisheng Bai (School of Marine Science and Technology, Northwestern Polytechnical University)
  - 03 Qisheng Xu (College of Computer Science and Technology, Changsha)
  - 04 Kele Xu (College of Computer Science and Technology, Changsha)
  - 05 Yong Dou (College of Computer Science and Technology, Changsha)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.15177)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv](PDF/S20250125__Audio-Language_Models_for_Audio-Centric_Tasks[2501.15177v1].pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

Audio-Language Models (ALMs), which are trained on audio-text data, focus on the processing, understanding, and reasoning of sounds.
Unlike traditional supervised learning approaches learning from predefined labels, ALMs utilize natural language as a supervision signal, which is more suitable for describing complex real-world audio recordings.
ALMs demonstrate strong zero-shot capabilities and can be flexibly adapted to diverse downstream tasks.
These strengths not only enhance the accuracy and generalization of audio processing tasks but also promote the development of models that more closely resemble human auditory perception and comprehension.
Recent advances in ALMs have positioned them at the forefront of computer audition research, inspiring a surge of efforts to advance ALM technologies.
Despite rapid progress in the field of ALMs, there is still a notable lack of systematic surveys that comprehensively organize and analyze developments.
This deficiency not only limits researchers' comprehensive understanding and evaluation of existing technologies but also hinders the rapid adoption and improvement of new methods.
In this paper, we present a comprehensive review of ALMs with a focus on general audio tasks, aiming to fill this gap by providing a structured and holistic overview of ALMs.
Specifically, we cover:
(1) the background of computer audition and audio-language models;
(2) the foundational aspects of ALMs, including prevalent network architectures, training objectives, and evaluation methods;
(3) foundational pre-training and audio-language pre-training approaches;
(4) task-specific fine-tuning, multi-task tuning and agent systems for downstream applications;
(5) datasets and benchmarks;
(6) current challenges and future directions.

Our review provides a clear technical roadmap for researchers to understand the development and future trends of existing technologies, offering valuable references for implementation in real-world scenarios.

</td><td>

**音频语言模型 (Audio-Language Models, ALMs)**, 在音频-文本数据上训练, 专注于声音的处理, 理解和推理.
和从预定义标签中学习的传统监督学习方法不同, ALMs 利用自然语言作为监督信号, 更适合描述复杂的真实世界音频录音.
ALMs 展示了强大的零样本能力, 并且可以灵活地适应多样化的下游任务.
这些优势不仅增强了音频处理任务的准确性和泛化能力, 而且促进了更接近人类听觉和理解的模型的发展.

ALMs 的近期进展已经将它们置于计算机听觉研究的前沿, 引发了提升 ALM 技术的热潮.
尽管 ALMs 领域迅猛发展, 但仍缺乏全面组织和分析发展的系统性综述.
这种不足不仅限制了研究人员对现有技术的全面理解和评估, 还阻碍了新方法的快速采用和改进.

在本文中, 我们展示了 ALMs 着重于通用音频任务的全面回顾, 提供 ALMs 的结构和历史回顾, 旨在填补这一空白.
具体来说, 我们涵盖了:
1. 计算机听觉和音频语言模型的背景;
2. 音频语言模型的基础方面, 包括流行的网络架构, 训练目标和评估方法;
3. 基础预训练和音频语言预训练方法;
4. 用于下游应用的特定任务微调, 多任务调参和智能体系统.
5. 数据集和基准;
6. 当前挑战和未来方向.

我们的综述为研究人员理解现有技术的发展和未来趋势提供了清晰的技术路线图, 为在实际场景中实现这些技术提供了宝贵的参考.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

Enabling machines to hear like humans and process audio-centric tasks has long been a significant challenge (**Pengi**[^001]).
**Audio-Language Models (ALMs)**, which are trained on audio-text data, focus on the processing, understanding, and reasoning of sounds.
This area is emerging as a prominent research field at the intersection of audio processing and Natural Language Processing.
ALMs are not only applicable to basic audio tasks, such as audio classification (**CLAP**[^002]), but also show great potential for more complicated scenarios.
These include tasks such as audio-text retrieval [^003], audio generation (**AudioLDM**[^004]), automatic audio captioning (**EnCLAP**[^005]), audio source separation [^006], automatic speech translation (**AudioPaLM**[^007]), and audio chatbots (**SpeechGPT**[^008]).

</td><td>

让机器像人类一样听取和处理以音频为中心的任务长期以来一直是一个显著挑战 (**Pengi**[^001]).
**音频语言模型 (Audio-Language Models, ALMs)**, 训练在音频-文本数据上, 专注于声音的处理, 理解和推理.
这一领域正在成为音频处理和自然语言处理相交叉的重要研究领域.

ALM 不仅适用于基础的音频任务, 还在更复杂的场景中展现出巨大潜力.
- 音频分类, (**CLAP**[^002])
- 音频文本检索,
- 音频生成 (**AudioLDM**[^004]),
- 自动音频描述 (**EnCLAP**[^005]),
- 音频声源分离,
- 自动语言翻译 (**AudioPaLM**[^007])
- 音频聊天机器人 (**SpeechGPT**[^008]).

</td></tr>
<tr><td>

In contrast to audio representation learning based on labeled data for specific tasks, ALM can learn from more descriptive textual information, expanding the scope of supervision to include human-annotated captions and readily available titles and descriptions from web sources[^009].
Natural language is well-suited for characterizing real-world audio, which frequently involves multiple overlapping sound events, thereby enabling models to learn their intrinsic relationships [^010].
Furthermore, using natural language as supervision avoids the model's reliance on task-specific predefined labels, enhancing the potential for models to generalize effectively to open-world scenarios.

</td><td>

</td></tr>
<tr><td>

As large language models (LLMs) exhibit remarkable comprehension capabilities, researchers have explored their integration as guiding components within ALMs.
However, pre-trained LLMs still face challenges in generalizing across a broad spectrum of downstream tasks [^011], necessitating additional transfer steps such as post-training and collaboration with other foundational models.
Within this research landscape, language provides a unified mechanism for constructing instances, enabling LLMs to undergo instruction tuning and in-context learning across diverse tasks.
This approach bridges the gap between auditory information and language understanding, facilitating the alignment of multiple components within ALMs.
Furthermore, language serves as a versatile human-machine interface, empowering users to instruct LLM agents to collaborate effectively with audio-language systems.

</td><td>

</td></tr>
<tr><td>

Despite the strong interest shown by the audio community in ALMs, there is still a lack of comprehensive surveys to review the current research status.
Existing relevant reviews include speech-language models (**Survey**[^012] **WavChat**[^013]), codec-based models (**VALL-E**[^014]), ALMs for specific tasks such as audio-text retrieval [^015], automated audio captioning [^016], speech-to-text translation [^017], and audio-language datasets [^018].
Here, we present the first comprehensive survey on ALMs, aiming to achieve an exhaustive coverage of the entire ALM research landscape from the perspective of model training.
Additionally, we adopt a perspective centered on general audio-centric tasks that encompasses a diverse range of audio types to provide a more detailed reflection of the current state and development of computer audition.
This survey method reflects mutual promotion and constraints among different research aspects from model to data, aids in systematically summarizes challenges and future directions, and serves as a guide for researchers and practitioners interested in ALM techniques, thereby facilitating further academic research and industrial applications in the field.

</td><td>

</td></tr>
<tr><td>

We first look at recent advances in ALM research and draw the timeline as shown in Figure.01.
**CLAP**[^002] is considered a significant milestone.
Previous work includes some audio-caption datasets [^019] [^020] [^021], which were initially used for automatic audio caption model training and also served as data foundations for ALMs, inspiring subsequent work.
Since the introduction of pre-training and large-scale datasets [^022], the advantages of ALMs have gradually gained attention.
Recently, numerous new works have emerged, primarily reflecting the intertwined development between pre-training and downstream models.
With increasing model research, recent studies have focused on the lack of unified evaluation standards and proposed various benchmarks.
It shows a high correlation between datasets, pre-training, downstream models, and benchmark research in ALMs.
Additionally, we observe that, driven by commercial applications, research interests have shifted more towards the speech domain.
However, audio typically encompasses a variety of environmental events, including human voices, natural sounds, music rhythms, etc., which presents significant challenges to general audio modeling (**BEATs**[^023]).

</td><td>

</td></tr>
<tr><td>

In the subsequent sections of this paper, we first introduce the background of audio-language pre-training and transfer paradigm ([Section.02](#Section.02)).
We then describe the foundations of ALMs, including model architecture, training objectives, and evaluation methods ([Section.03](#Section.03)).
Following this, we review the topics of representation pre-training ([Section.04](#Section.04)), downstream transfer ([Section.05](#Section.05)), and related data ([Section.06](#Section.06)).
Building on these foundations, we discuss the challenges and future research directions ([Section.07](#Section.07)), before concluding the paper ([Section.08](#Section.08)).

</td><td>

在本文中的后续章节中,
- [第二节](#Section.02)首先介绍了音频语言预训练和迁移范式的背景;
- [第三节](#Section.03)描述了 ALMs 的基础, 包括模型架构, 训练目标和评估方法;
- [第四节](#Section.04)回归了表示预训练话题;
- [第五节](#Section.05)讨论了下游迁移话题;
- [第六节](#Section.06)回顾了相关数据;
- [第七节](#Section.07)讨论了挑战和未来方向;
- [第八节](#Section.08)总结了本文.

</td></tr></table>

## 2·Background: 背景

<a id="Section.02"></a>

<table><tr><td width="50%">

This section begins by discussing the development of computer audition paradigms, with a particular focus on how ALMs are trained and transfer for downstream, as well as the reasons for the shift towards the audio-language paradigm.
We then introduce the training stages and establish a research landscape for ALMs, providing a structured basis for the comprehensive review in the following sections.

</td><td>

</td></tr></table>

### Pre-training and Transfer Paradigm

<table><tr><td width="50%">

The pre-training and transfer paradigm involves initially training on large-scale public datasets to get robust representations, and then applying knowledge gained from one context to another to enhance the performance on downstream tasks.
This approach accelerates supervised learning on downstream tasks.

However, as this paradigm evolves, two challenges emerge.
First, models may overfit by exploiting simple label mappings, achieving high performance on specific tasks without truly understanding the underlying audio content [^024], leading to poor generalization to new data.
Second, the high cost of manual annotation exacerbates the difficulty of obtaining limited labeled datasets for learning audio representation [^025].

To address these challenges, ALMs have been proposed to learn audio concepts through natural language supervision (**CLAP** [^002]).
Firstly, this form of supervision provides more details about the audio, enabling models to understand the meanings and make decisions accordingly like a human.
For example, natural language can describe the temporal order of multiple events using words such as "simultaneous", "before", and "after" (**CompA** [^026]), better reflecting the complex composition of audio compared to predefined labels and helping models learn their intrinsic relationships [^010].
Additionally, audio-text data is easier to obtain than well-defined labeled datasets, effectively expanding the scale of datasets.
For instance, we can use "dog" or "barking" to label a dog barking, but inconsistencies among multiple annotators make it difficult to create a perfectly accurate audio dataset.
While ALMs are able to leverage the natural language processing capabilities of pre-trained models to extract similar semantic features from different forms of descriptions.
Besides human-annotated captions and translations, titles and descriptions related to audio found abundantly on the web can also serve as sources of text annotation.

</td><td>

</td></tr></table>

### Audio-Language Training Stages

<table><tr><td width="50%">

As data and model sizes grow, the training strategies for ALMs become more intricate.
From the viewpoints of representation learning and downstream task application, we first categorize the training stages aimed at enhancing task-independent audio representations as falling within the scope of pre-training, while fine-tuning and cooperating before the model is applied to downstream tasks are defined as part of the transfer process.

ALMs pre-training can be further divided into multiple stages, typically including the pre-training of foundational models, followed by audio-language pre-training on paired data.
Some may also involve further training on a broader range of data and tasks.

Although ALMs have achieved strong zero-shot capabilities in audio retrieval, transfer remains an important stage for applying models to downstream tasks.
Task-specific fine-tuning is one of the most widely used methods.
It involves supervised fine-tuning of pre-trained models on downstream task datasets and may require the addition of some adaptive modules.
Another category of methods includes transferring simultaneously on multiple tasks to make the model more universal or gain from multi-task knowledge sharing.
Unlike task-specific fine-tuning, which focuses directly on task performance, instruction tuning and in-context learning aim to enhance (or unlock) the LLM's ability to follow human instructions.
Essentially, it fine-tunes ALMs with a set of formatted instances in natural language form [^027], thus helping the model generalize to various downstream tasks.
Multi-task transfer can also be achieved by cooperating multiple models to form an agent system.

</td><td>

</td></tr></table>

### Research Landscape

<table><tr><td width="50%">

Based on current research and our definition of audio-language training stages, we construct a research landscape for ALMs, as shown in Figure.\ref{fig2: framework}.
From the training dimension, ALMs are divided into pre-training and transfer.
ALMs achieve multimodal perception by integrating pre-trained audio and language models, then undergo further pre-training on extensive audio-text data.
Transfer is crucial for combining these models with other networks and applying them to various downstream tasks.
Data is an essential element for model training and evaluation.
Different types of datasets can be utilized at various stages of training, and benchmarks provide unified and comprehensive standards for model evaluation, playing an important role in optimizing the models.
Therefore, research on ALMs can be developed in three fields: (a) pre-training for representation learning, (b) downstream transfer, and (c) datasets and benchmarks.

Within the scope of the research landscape, we designed a review outline as shown in Figure.\ref{fig3:outline}.
We first provide an overview of the foundation on ALMs, thereby comprehensively reviewing related work from three research fields.
According the progress across areas, we systematically propose the challenges and future directions for ALMs.

</td><td>

</td></tr></table>

## 3·Foundations: 基础

<a id="Section.03"></a>

<table><tr><td width="50%">

In this section, we will introduce the general foundations of ALMs, including commonly-used architectures, training objectives, and evaluation methods.

</td><td>

</td></tr></table>

### ALM Architectures

<table><tr><td width="50%">

Audio-language models and systems typically comprising audio and language encoders, and may include other multimodal alignment mechanisms and language models.
As shown in Figure. \ref{fig4:arch}, current ALMs can generally be divided into four types: Two Towers, Two Heads, One Head and Cooperated Systems.

</td><td>

</td></tr></table>

#### Two Towers

<table><tr><td width="50%">

The basic form of ALMs, with one encoder and a projector for each modality, embeddings will be aligned in a joint space.
Among them, the most prominent landmark pretraining research is **Contrastive Language-Audio Pretraining (CLAP)**[^002], which incorporates a contrastive learning framework to bring audio and text descriptions into a joint multimodal space, learning the mapping relationship between the two modalities.
Furthermore, based on the concept of modality alignment, mechanisms can be added between two independent encoders to facilitate communication, with the aim of achieving early-stage modality fusion during the representation phase [^028].

</td><td>

</td></tr></table>

#### Two Heads

<table><tr><td width="50%">

A mainstream form that utilizes one encoder and a projector for each modality, with a language model on top.
Here, "Head" refers to a network that unifies a certain modal representation space into a unified space [^029] [^030] [^031].
Language modeling has first been proven to possess strong capabilities in semantic feature extraction within the field of speech [^032], making it a natural design choice to incorporate language models into ALMs.
With the development of large language models, many works have utilized LLMs as the backbone for ALM inference, expanding the perceptual modalities of large language models and leveraging their emergent understanding capabilities.
This has led to classic works such as **SpeechGPT**[^008], **Pengi**[^001], and **Qwen-Audio**[^033], making Two Heads a unified architecture of Large Audio-Language Models.
In this structure, modality fusion can also be promoted through communication mechanisms between encoders [^034].
It is important to note that in some works, text inputs may only undergo tokenization without the need for a dedicated text encoder, and these models can be considered under a special type of Two Heads framework.

</td><td>

</td></tr></table>

#### One Head

<table><tr><td width="50%">

A unified multimodal input form that uses one encoder to handle two different modalities simultaneously, with a language model on top.
In the vision community, a line of work has conducted research on the One Head architecture based on the view that the same multimodal processing module can achieve better alignment.
That is, using a unified space to represent two modalities.
However, there are relatively few related studies in audio-language [^035].

</td><td>

</td></tr></table>

#### Cooperated Systems

<table><tr><td width="50%">

This system employs an LLM as a planning agent and comprises various model types mentioned above.
Its design facilitates the selection and utilization of each model's inherent complementary strengths, tailored to downstream task requirements.
Through the collaboration of these diverse models, the system can tackle a wider array of complex tasks compared to a solitary model alone [^036].

</td><td>

</td></tr></table>

### Training Objectives

<table><tr><td width="50%">

Training objectives are used to guide model learning during pre-training and transfer.
As shown in Figure.\ref{fig5: objectives}(a), pre-training contrastive, generative, or discriminative objectives guide the model to learn pretext tasks on audio, text, or audio-text paired data, aiming to learn audio semantic features and audio-language correlations.
As illustrated in Figure.\ref{fig5: objectives}(b), task-specific fine-tuning as a commonly adopted transfer method, employs either generative or discriminative objectives depending on the context.
Another line of transfer methods with generative language models in Figure.\ref{fig5: objectives}(c) aims to improve unlock pre-training models' generalization ability on downstream tasks through standard language modeling objectives.
Note that the above training objectives can be used in combination.

</td><td>

</td></tr></table>

#### Contrastive Objectives

<table><tr><td width="50%">

It is the most commonly used type of training objective in audio-language pre-training, which aims to train the model to bring positive sample pairs closer together and push negative sample pairs further apart within a shared embedding space for audio and text, thereby learning the audio-language correlations and obtaining distinguishable representations between audio samples.
The most widely implemented approach for this category of objective is using a symmetric audio-text infoNCE [^037] loss function to measure the similarity between audio and text embeddings.
Let the $i-th$ sample pair be ${x_{i}, t_{i}}$.
Given an audio encoder $h_a(\cdot)$ and a text encoder $h_t(\cdot)$, the embedding vectors for the audio sample $x_{i}$ and its corresponding caption $t_{i}$ can be represented as:

$$
z_{i}^{a}=h_a(x_i)
$$

$$
z_{i}^{t}=h_t(t_i)
$$

The similarity between audio and text embeddings is calculated using the dot product.
The infoNCE loss for the audio dimension, $l_{a}$, is defined as the average of a normalized function measuring the similarity of different texts to the same audio query.
Similarly, the contrastive loss for the text dimension, $l_{t}$, measures the similarity of different audios to the same text query.
For a batch with $B$ audio-text pairs, we have:

$$
l_{i}^{a}=-\log \frac{\exp \left(z_{i}^{a} \cdot z_{i}^{t} / \tau\right)}{\sum_{j=1}^{B} \exp \left(z_{i}^{a} \cdot z_{j}^{t} / \tau\right)}
$$

$$
l_{i}^{t}=-\log \frac{\exp \left(z_{i}^{t} \cdot z_{i}^{a} / \tau\right)}{\sum_{j=1}^{B} \exp \left(z_{i}^{t} \cdot z_{j}^{a} / \tau\right)}
$$

where $\tau$ represents a temperature parameter used to scale the range of logits.
When setting the contrastive objective to be completely symmetrical, the total loss for the audio-text pairs in one batch can be defined as:

$$
\mathcal{L}_{con}=\frac{1}{2B} \sum_{i=1}^{B}(l_{i}^{a} + l_{i}^{t})
$$

</td><td>

</td></tr></table>

#### Generative Objectives

<table><tr><td width="50%">

Generative methods have proven to be powerful and effective in audio representation learning.
They lead the network in learning semantic features of audio through pretext tasks such as masked reconstruction [^038].
In audio-language pre-training, similar approaches are introduced, guiding representation learning through audio or audio-related language generation tasks.
These methods are often combined with contrastive learning to bolster the robustness of learned audio embeddings or improve computational efficiency.
During transfer, these generative objectives can help the model adapt to corresponding generative tasks and are widely used in transfer with generative LLMs.

During pre-training, the most common method for audio mask reconstruction is based on the audio spectrogram.
Let $M\left (\cdot \right )$ denote the masking operation, and let $f_{a}\left (\cdot \right )$ and $p_{ae}\left (\cdot \right )$ represent the spectrogram encoder and audio embedding projection layer, respectively.
To achieve masked spectrogram prediction, an additional decoder $f_{a}^{-1} \left (\cdot \right )$ is added to the model.
For an audio sample with the original spectrogram $a$, spectrogram reconstruction can be represented as $\hat{a} = f_{a}^{-1} (p_{ae}(f_{a} (M(a))))$.
Using $\hat{a}_{n}$ and $a_{n}$ to denote the decoder prediction output of the $n-th$ masked spectrogram patch and the original true patch, respectively.
For a spectrogram divided into $N$ patches, the audio reconstruction loss used for self-supervision can be defined as minimizing the $L2$ (mean squared error, MSE) loss:

$$
\mathcal{L}_{ar}=\frac{1}{N}\sum _{n=1}^{N}\left\|\hat{a}_{i}-a_{i}\right\|_{2}
$$

Since ALMs include both audio and language modalities as inputs, some works have similarly designed masked cross-modal reconstruction tasks, which typically involve methods such as cross-attention mechanisms to communicate between the encoders of the two modalities and perform reconstruction on the audio representation.

During audio generation transfer, training objectives essentially enhance the model's performance by minimizing the distance between the predicted embedding $\hat{z}$ and its corresponding ground truth $z$.
This distance metric can be chosen based on the situation, with common options including $L1$ and $L2$ distances.
The training objective can also be set as a weighted sum of multiple distances.
For an audio sample, generative audio modeling objective can be represented as:

$$
\mathcal{L}_{am}=\frac{1}{T} \frac{1}{L} \sum_{t=1}^{T} \sum_{l=1}^{L} \alpha \left\| \hat{z}_{t, l}-{z}_{t, l}\right\|_{1}+\beta \left\| \hat{z}_{t, l}-{z}_{t, l}\right\|_{2}
$$

where $T$ denotes the total number of frames, $L$ denotes embedding dimension, and $\alpha$ and $\beta$ are weight hyperparameters.
In addition to the method that uses embedding differences as a training objective, it is also possible to directly train jointly with the decoder network, designing the training objective directly on the predicted audio amplitude.
For instance, aiming to learn a decoder net $h_{de}\left (\cdot \right )$ that maps known audio $x_{i}$ and query $t_{i}$ to a predicted audio $\hat{a}_i$.
Denote $z_{i}^{t}$ as the embedding of the language query, the training objective could be to minimize the $L1$ (mean absolute error, MAE) loss between the amplitude spectrogram $|a_i|$ of the ground truth target audio source and the predicted $|\hat{a}_i|$:

$$
|\hat{a}_i| = h_{de}\left (z_{i}^{t} \right )
$$

$$
\mathcal{L}^{'}_{am} = \sum_{i=1}^B \left \| |{a}_i| - |\hat{a}_i| \right \|_{1}
$$

Generative language modeling objectives are used to guide ALM in generating audio-related text that is consistent with the ground truth.
On one hand, they can be used to force the model to learn audio-language correlations to promote representation learning, and help improve the model's performance on corresponding downstream tasks (e.g., automatic caption generation).
On the other hand, as a standard loss for generative language modeling, it is also commonly used during ALM transfer with language model [^039].

An additional text decoder (language pre-trained model) is required in language generation.
When using an autoregressive language model to predict tokenized text associated with a given audio sample $x$, the language modeling objective is defined as minimizing the negative log-likelihood of the current ground-truth token (cross-entropy, CE loss), given the previous ground-truth tokens:

$$
\mathcal{L}_{lm}=-\frac{1}{T} \sum_{t=1}^{T} \log P_{\theta}\left(y_{t} \mid y_{1: t-1}, x\right)
$$

Here, $y_{t}$ is the $t-th$ ground-truth token of the given caption $y$, $T$ is the total length of the caption, and $\theta$ represents the model's learnable parameters.
Non-autoregressive language models also adopt a similar negative log likelihood objective without temporal averaging.

</td><td>

</td></tr></table>

#### Discriminative Objectives

<table><tr><td width="50%">

They are used to guide the model in learning to predict the correct label, and can be broadly categorized into classification and retrieval objectives.
Here, we take the cross-entropy function as an example to uniformly calculate the loss between the predicted output and the ground truth.

Audio classification is one of the most extensively studied downstream tasks.
It aims to recognize patterns from specific audio inputs to predict given labels.
For a batch of $B$ audio samples, the objective can be expressed as:

$$
\mathcal{L}_{cls} = -\frac{1}{B} \sum_{i=1}^{B} \sum_{c=1}^{C} y_{i,c} \log(\hat{p}_{i,c})
$$

where $C$ is the number of classes. $y_{i,c}$ is the true label of the $i-th$ sample in class $c$ (0 or 1). $\hat{p}_{i,c}$ is the predicted probability of the $i-th$ sample in class $c$.

Audio-Text Retrieval (ATR) aims to find matching items between audio clips and textual descriptions.
Given a query in one modality (audio or text), the goal is to retrieve the corresponding item from a pool of candidates in the other modality.
Here, we use a scoring function $S\left (\cdot \right )$ to represent the model's prediction output by measuring the correlation between audio and text.

Denote $Y$ as a set of $m$ possible caption texts, the correspondence caption of a given audio $x_i$ is

$$
\hat{y}_i = \arg\max_{y_j \in Y} \frac{\exp(S(z_{i}^{a}, z_{j}^{t}))}{\sum_{k=1}^{m} \exp(S(z_{i}^{a}, z_{k}^{t}))}
$$

Then, retrieval tasks can be considered as instance-level classification, so the objective can be formatted as:

$$
\mathcal{L}_{atr} = - \sum_{i=1}^B \log(\hat{y}_i)
$$

Specially, audio-text matching is pretext task designed to forcing a more fine-grained alignment between audio and text embeddings than contrastive pre-training.
It train the model to predict whether a given text correctly describes a provided audio, can be seen as a binary classification task requiring the model to determine whether an audio-language pair is a match or not.
The matching objective can be defined as:

$$
\mathcal{L}_{mat}=p \log \mathcal{S}\left(z^{a}, z^{t}\right)+(1-p) \log \left(1-\mathcal{S}\left(z^{a}, z^{t}\right)\right)
$$

Here, $p$ is 1 if the audio and text are paired, otherwise it is 0.

</td><td>

</td></tr></table>

### Evaluation Methods

<table><tr><td width="50%">

Model evaluation aims to fairly measure the performance of models under the same experimental setup and tasks.
The evaluation methods for ALMs mainly include zero-shot (ZS), linear probe, supervised fine-tuning, and instruction-following evaluation.
Each of these methods has their own focus, collectively forming the basis for a comprehensive performance evaluation of ALMs.

</td><td>

</td></tr></table>

#### Zero-Shot Evaluation

<table><tr><td width="50%">

It focuses on assessing the ability of contrastive ALMs in open-set retrieval.
This zero-shot prediction is primarily conducted by measuring the similarity between audio and text embeddings.
Notably, aside from direct text-to-audio or audio-to-text retrieval, considering that labels are also a special form of language.
This allows for zero-shot evaluation on classification tasks such as sound event detection and emotion recognition.

</td><td>

</td></tr></table>

#### Linear Probe Evaluation

<table><tr><td width="50%">

It is a common experimental setup for evaluating pre-trained models, and it is used to assess the audio representation of ALMs.
It involves adding a linear header (usually an MLP) to the frozen pre-trained model and training the header on downstream tasks, allowing the model to be adapted for specific tasks and datasets.
Although this simple transfer learning setup may not achieve optimal performance on specific tasks, it minimizes the variables introduced, hence its widespread adoption for conducting fair representational evaluations.
In linear probe evaluation, the selected tasks are usually fundamental linear tasks like classification.

</td><td>

</td></tr></table>

#### Supervised Fine-tune Evaluation

<table><tr><td width="50%">

It further examines the generalization ability of the pre-trained model to downstream tasks and its task-specific performance.
For a given downstream task, the audio encoder is unfrozen and fine-tuned along with an attached head.
The model's performance is then validated on the test set and compared with state-of-the-art (SOTA) models for that task.
This evaluation approach not only covers classification tasks but often combines various types of tasks including detection and generation, thoroughly testing the model's adaptability and flexibility.

</td><td>

</td></tr></table>

#### Instruction-Following Evaluation

<table><tr><td width="50%">

It emphasizes the model's ability to understand complex instructions and respond accurately, serving as an important indicator of the task generalization ability of LALMs.
This evaluation method can be considered a special type of zero-shot evaluation or supervised fine-tuning evaluation, depending on whether instruction tuning is performed.

</td><td>

</td></tr></table>

## 4·Representation Pre-Training: 表示预训练

<a id="Section.04"></a>

<table><tr><td width="50%">

In recent years, the pursuit of powerful audio representations has led to significant growth in both audio dataset sizes and model scales.
Training ALMs becomes more complex, encompassing multiple stages of pre-training aimed at enhancing task-independent audio representations before transfer to downstream tasks.

Audio encoders and text encoders are the most critical components of ALMs.
They provide the initialization of model parameters for post pre-training on audio-text pairs or transfer on downstream task datasets.
Recent studies have shown that the choice of encoders significantly impacts the generation of powerful representations through Audio-language pre-training and enhances the performance of downstream tasks [^040].

</td><td>

</td></tr></table>

### Audio Pre-training

<table><tr><td width="50%">

From the perspective of model architecture, pre-trained models used to initialize encoder in ALM mainly include convolutional neural network (CNN)-based, Transformer-based, and codec-based models.

</td><td>

</td></tr></table>

#### CNN-based Models

<table><tr><td width="50%">

CNNs are widely used in audio pre-training due to their strong feature extraction capabilities, parameter sharing, and sparse connectivity.
In audio applications, CNNs-based models usually use short-time Fourier transform (STFTs) to convert time-domain waveforms into log-mel spectrograms as input.
Models like AlexNet and VGG perform well in audio classification tasks ([^041], [^042]), and their performance is related to the design of network depth and width.
PANNs [^043] based on CNN14 achieve good results in Audioset labeling, highlighting the effectiveness of CNNs in learning audio representations.

</td><td>

</td></tr></table>

#### Transformer-based Models

<table><tr><td width="50%">

These models utilize self-attention mechanisms to capture long-range dependencies in audio signals and enhance the understanding of complex patterns.
Wav2vec 2.0 [^044] and HuBERT [^045] are designed for speech tasks, combining CNN with self-attention to manage local features and long-term temporal dynamics.
Wav2vec 2.0 uses a unique self-supervised method, which converts raw audio into potential representations through CNN, and then generates contextual output with masks and Transformer.
Training involves contrastive tasks to predict masked segments to enhance generalization ability.
HuBERT improves this by introducing noise labels from offline clustering to form a codebook for pre-text prediction, encouraging the model to capture acoustic details and long-range temporal relationships.
Considering supervised models face overfitting risks during fine-tuning [^046], Whisper [^047] addresses this issue with a multi-task training framework, which uses an encoder-decoder Transformer with additional convolutional layers for processing audio spectrograms, promoting next token prediction across tasks and offering wide adaptability.
AST [^048] is the first model without convolutions and based purely on attention for audio spectrograms.
While flexible, it requires more data for training and faces challenges with large GPU memory and long training times [^49].
HTSAT [^050] deals with these issues through a hierarchical model structure, processing audio signals through multiple layers, with each layer's Transformer capturing temporal and structural information, enabling more effective handling of long audio signals.
AudioMAE [^038] explores the audio extension of the Masked Autoencoder (MAE) [^051], adopting a Transformer-based encoder-decoder design, masking log-mel spectrograms at a high rate, with the encoder providing non-masked tokens and the decoder reconstructing masked parts.
After fine-tuning, it achieves advanced performance in classification tasks and is one of the mainstream self-supervised pre-training models in the audio domain.

</td><td>

</td></tr></table>

#### Codec-based Models

<table><tr><td width="50%">

Based on the encoder-decoder structure, they can convert continuous audio into discrete tokens for developing ALMs.
Despite some loss of information, they excel at extracting acoustic features and achieving high-quality audio reconstruction [^052].
SoundStream [^053] is a pioneer, using streamable SEANets [^054] for encoding and decoding, and Residual Vector Quantization (RVQ) bottleneck for parallel token flow, optimized through reconstruction and adversarial loss.
EnCodec [^055] enhances this framework by adding LSTM layers and using a Transformer-based language model to improve sequence modeling performance.

</td><td>

</td></tr></table>

### Language Pre-training

<table><tr><td width="50%">

Language modeling is a fundamental area in artificial intelligence.
Based on parameter scale, language pre-training models can be categorized into small ($\leq 1$  billion), medium ($1-10$ billion), large ($10-100$ billion), and very large ($>100$ billion).
Models with over $1$ billion parameters are referred to as LLMs.
By pre-training on vast amounts of text data, these models learn rich semantic representations and grammatical structures.
With the development of fine-tuning techniques incorporating human feedback, numerous LLMs with tens of billions of parameters have emerged, demonstrating promising capabilities for multimodal tasks.
Renowned language pre-training models used as text encoders in ALMs including: GPT (Generative Pre-trained Transformers) families ([^056], [^057], [^058], [^059], [^060], [^061]), LLaMA (Large Language Model Meta AI) families ([^062], [^063], [^064], [^065]), Qwen ([^066], [^067], [^068], [^069]), and OPT (Open Pre-trained Transformer) families ([^070], [^071]).
Note that language pre-training is not the main focus of this paper, for more details please refer to ([^011], [^072]).

</td><td>

</td></tr></table>

### Audio-Language Pre-training

<table><tr><td width="50%">

From model training perspective, the objectives in audio-language pre-training can generally be categorized into three types: contrastive, generative objectives, and discriminative objectives.
Table.\ref{tab1:ALPs} shows the audio-language pre-trained models and objectives they use.

</td><td>

</td></tr></table>

#### Contrastive Pre-trained Models

<table><tr><td width="50%">

Contrastive learning is a common pre-training method that learns feature representations by distinguishing similar sample pairs (positive pairs) from dissimilar ones (negative pairs).
Inspired by the CLIP [^080] model, which leverages large-scale web visual-text pairs to expand the training scale for visual tasks and achieves excellent performance across various tasks, researchers have begun to apply this contrastive training paradigm to the audio field.
[Microsoft's MS-CLAP](../Models/_Basis/2022.06.09_CLAP_(CNN14+BERT).md)[^002] (prefix added by the editor for distinction) is the first contrastive language-audio pre-training model.
It uses the symmetric audio-text infoNCE loss function \ref{eq:infonce} for pre-training based on audio-text paired datasets and audios from other tasks such as audio classification.

Subsequent studies have focused on dataset scaling.
LAION-CLAP [^022] released a larger audio-caption dataset, LAION-Audio-630K, and trained the first fully open-source CLAP model based on this dataset together with other public datasets.
MS-CLAP V2 [^050] not only leverages a more extensive multi-task trained audio encoder but also further expands the audio-text paired dataset for contrastive language-audio learning.
BLAT \cite{xu2023blat}, from another prospective, proposes using an audio captioning model to generate audio-text pair data for contrastive pretraining.

Another line of research aims to address the inherent shortcomings of the vanilla CLAP.
The experiments of ACBA \cite{wu2023ACBA} show that CLAP has limited understanding of natural language, especially regarding the order or concurrent arrangement of sound events.
It suggests modifying the original pre-training dataset to provide more audio-language pairs about ordering.
**CompA** [^026] addresses the issue that current benchmarks cannot measure the lack of combinatorial reasoning in models and proposes contrastive training with composition-aware hard negatives and a modular contrastive loss to improve combinatorial reasoning capabilities.
MGA-CLAP \cite{li2024MGACLAP} tackles the problem of different granularities between modalities by adopting a shared codebook, designing a locality-aware block and a hard-negative guided loss to achieve fine-grained alignment.

Additionally, MusCALL \cite{manco2022contrastive} conducts pre-training research in professional fields such as music and performs excellently in related tasks.

</td><td>

</td></tr></table>

#### Generative Pre-trained Models

<table><tr><td width="50%">

Generative pre-training aims to learn deeper semantic representations by setting generative audio or text as pretext tasks.
Cross-modal Transformer for Audio-and-Language (CTAL) [^028] is an early exploration of audio-language pretraining through masked language modeling and cross-attention based masked cross-modal acoustic modeling.
Fast Language-Audio Pre-training (FLAP) \cite{yeh2023flap} conduct representation learning through the combination of masked reconstruction and contrastive learning.
It generates multiple augmented views of the audio through masking for inter-modal contrast and learns to reconstruct the masked parts of the audio spectrogram.
This masking reduces the amount of data that needs to be processed, thereby lowering the computational complexity and making it more efficient than contrastive learning with raw spectrograms.
Additionally, by incorporating the masked reconstruction task, the model is encouraged to compress information into each sample embedding, making the audio embedding not only close to their textual counterparts but also producing more informative original inputs.
M2D-CLAP \cite{niizumi2024m2dclap} addresses the issue of MAE using all patches to encode training signals, which may lead to underutilization of inductive biases.
By combining Masked Modeling Duo (M2D) \cite{niizumi2023m2d} to train the audio encoder while contrastive learning further promotes input modeling, thus enhancing the effectiveness of the learned representations.
Practice in the vision-language has shown that integrating auxiliary captioning objectives in contrastive learning can provide stronger supervision \cite{yu2022coca, li2022blip}.
Cacophony \cite{zhu2024cacophony} improves CLAP by incorporating an auxiliary captioning objective, encouraging the audio encoder to capture fine-grained patterns closely matching text descriptions.

</td><td>

</td></tr></table>

#### Discriminative Pre-trained Models

<table><tr><td width="50%">

Discriminative pre-training aims to set up a pretext task of audio-text matching, allowing the model to learn cross-modal alignment features.
MINT [^034] is a framework that enhances audio-language pre-training through multiple objectives.
Specifically, it introduces Bridge-Net as a trainable module, taking the output of the audio encoder and text as inputs to the Bridge-Net.
It uses contrastive objective to align the audio and text representations by maximizing the mutual information between them, combines matching objective for fine-grained audio-text alignment, and employs generative objective to guide the audio-grounded text generation task, forcing the model to extract audio features to capture all necessary information about generating the text.

</td><td>

</td></tr></table>

## 5·Downstream Transfer: 下游迁移

<a id="Section.05"></a>

<table><tr><td width="50%">

Downstream transfer is crucial for enhancing the performance of ALMs, enabling them to adapt efficiently to new tasks.
Transfer focuses on applying the knowledge gained from solving pretext tasks to solve other related tasks (**Pengi**[^001]).
This can be divided into task-specific fine-tuning or multi-task tuning and cooperation.

</td><td>

</td></tr></table>

### Task-Specific Fine-Tuning

<table><tr><td width="50%">

Pre-trained audio and language models can undergo supervised fine-tuning on specific downstream task datasets.
In most cases, this is an essential step to leverage pre-trained models combines with other adaptive modules, such as adding a linear projection layers to map audio representations onto a specified number of categories.
Moreover, this task-specific fine-tuning bridges the gap between the source and target domains, facilitating knowledge transfer across domains.

As a result, we are seeing remarkable improvements in various audio-centric discriminative and generative audio tasks.
In Table.\ref{tab2:task-specific fine-tuning models}, we compare different task-specific fine-tuning models in terms of application domain, architectures or pre-trained models, downstream tasks, input and output modalities.
For the application domain, we use "speech" and "music" to represent domain-specific models, and "audio" to indicate models that can handle more general audio tasks.

Common discriminative tasks include audio classification (AC) and audio-text retrieval (ATR).
Audio classification broadly encompasses more specific scenarios such as Sound Event Classification, Accent Classification, Chord Classification, Spoken Language Identification, speech emotion recognition (SER), and spoken language understanding (SLU).
Generative tasks can be further divided into audio generation and language generation.
Typical audio generative tasks include: speech continuation (SC), text-to-speech (TTS), and speech enhancement (SE) in the speech domain \cite{[^032], zhang2023VALLEX, xue2024mmace,eskimez2024e2tts, anastassiou2024seedtts, chen2024f5tts, Borsos2023audiolm}; piano continuation (PC), stereo generation (SG), and text-to-music (TTM) in the music domain \cite{agostinelli2023musiclm, melechovsky2024mustango, Borsos2023audiolm}; as well as text-guided audio continuation (TAC), text-to-audio (TTA), language-query sound source separation (LASS), and text-query sound event detection (TSED) in other general audio domains \cite{kreuk2023audiogen, yang2023diffsound, **AudioLDM**[^004], [^006], dong2023clipsep, liu2023separate, mahmud2024opensep, yuan2024flowsep, yin2024TQSED}.
Language generation, on the other hand, centers on transcription, translation, and caption.
Typical tasks include automatic speech recognition (ASR) \cite{[^047], li2024style, bai2024seedasr}, speech-to-text translation (S2TT) \cite{zhang2023VALLEX}, speech-to-speech translation (S2ST) [^047], and automatic audio captioning (AAC) \cite{doh2023lp, mei2021audio, **EnCLAP**[^005], Kim2024enclap++, Ghosh2024Recap, li2024drcap}.

</td><td>

</td></tr></table>

#### Models for Discriminative Tasks

<table><tr><td width="50%">

Through contrastive learning with natural language supervision, audio-language pre-trained models can perform zero-shot audio classification and retrieval tasks within a unified framework, as shown in Figure. \ref{fig6:discriminative_models}.
Although pre-trained models already have strong generalization capabilities, supervised fine-tuning on downstream datasets remains an important step to enhance task performance (**CLAP**[^002]; [^022]; [^050]).
Audio classification refers to mapping input audio to a label, and the audio encoder from the pre-trained model can serve as a powerful audio pre-training model, used to train additional classifiers on specific datasets for fine-tuning.
Retrieval can be divided into audio-to-text and text-to-audio, aiming to filter corresponding samples of another modality from a pool based on the input, with classification considered as a special case of retrieval.

ATR research primarily focuses on enhancing system performance by integrating orthogonal SOTA works.
For instance, CNN14-NetRVLAD \cite{lou2022contextatr} explores various audio features and sequence aggregation methods to improve audio-text alignment, utilizing the CNN14-based audio pretraining model PANNs and NetRVLAD pooling techniques to achieve significant improvements in bidirectional audio-text retrieval.
EN-CA-IMC \cite{hu2023enatr} implements an audio enhancement strategy (EN) involving Gaussian noise, pitch adjustment, and time shift to reduce model overflow, combined with a co-attentive mechanism (CA) and intra-modal contrastive learning (IMC) between enhanced and original audio to capture richer audio features.
CED-BERT-LE \cite{yan2024bridging} integrates a language enhancement strategy (LE) for effective cross-lingual retrieval and uses consistent ensemble distillation (CED) to handle variable-length audio segments, demonstrating proficiency across seven additional languages with minimal extra training data.
Recently, the mini-batch Learning-to-match (m-LTM) framework \cite{luong2024revisiting} adapts the Learning-to-match approach using mini-batch subsampling and Mahalanobis-enhanced ground metrics, leveraging soft-matching from entropic optimal plans and Mahalanobis distance to learn a rich, expressive joint embedding space for audio and text modalities.

</td><td>

</td></tr></table>

### Models for Generative Tasks

<table><tr><td width="50%">

Audio generation aims to enable the model to learn to generate audio that better conforms to textual conditions, facilitating the creation of personalized audio for applications such as augmented reality and virtual reality, game development, and video editing (**AudioLDM**[^004]).
A common method is to use a codec structure that converts continuous audio into discrete tokens for language modeling, ensuring high fidelity \cite{wu2024towards}.
AudioGEN \cite{kreuk2023audiogen} is an pioneering audio language codec that uses data augmentation techniques, multi-stream modeling, and classifier-free guidance to improve adherence to text, addressing challenges such as distinguishing multiple sound sources, handling real-world recording conditions, and encoding high-fidelity audio.
**VALL-E** [^032], VALL-E X \cite{zhang2023VALLEX}, and MusicLM \cite{agostinelli2023musiclm} perform well in speech and music respectively.
SpeechX \cite{wang2024speechx} combines neural codec language modeling with task-specific prompts to achieve unified and scalable modeling and provides a consistent method for utilizing text input in speech enhancement and conversion tasks.
Recent approaches further advance the development of audio generation technology by combining advanced generative models such as flow models and Diffusion. flow-based models like Flow-TTS \cite{miao2020flow} improve the overall fluency and naturalness of generated speech by using reversible transformations to balance local accuracy and global coherence.
Diffsound \cite{yang2023diffsound} is an work based on diffusion models, using text as a condition for controlled audio generation by the diffusion model.
Seed-TTS \cite{anastassiou2024seedtts} is a foundational speech generation model that excels in speech in-context learning and achieves good performance in speaker similarity and naturalness.

Audio source separation is a crucial task in audio enhancement applications and can be considered a type of audio generative task according to objectives in fine-tuning.
LASS-Net [^006] is the first proposed end-to-end network for the language-queried audio source separation (LASS) task, capable of separating the target source from an audio mixture based on user's natural language descriptions (e.g., “a woman speaking”).
Transfer ALMs to such tasks typically requires new design in the model architecture.
For example, as shown in Figure. \ref{fig7:audiosep}, AudioSep \cite{liu2023separate} uses the text pretraining that achieves audio-text modality alignment during the ALM pretraining process as the query encoder for text and connects it to the prediction network for downstream tasks (audio separation network), and conduct post-training on specific task datasets.
It not only achieved audio segmentation for arbitrary descriptions but was also extended to speech enhancement tasks, demonstrating the flexibility of combining natural language prompts with task paradigms.
This provides the potential to integrate multiple audio enhancement tasks into a unified framework.
Current research is unfolding in two main aspects: improving separation performance \cite{yuan2024flowsep} and enhancing generalizability \cite{mahmud2024opensep}.

Language generation aims to enable the model to perform audio-centric text prediction tasks, and fine-tuning can be conducted through language generative objectives.
The two sub-tasks, audio understanding and language generation, are accomplished by the audio encoder and the text decoder, respectively.
Recent work has also built upon this basic architecture by adding multiple encoders or decoders [^017].

Automated Audio Captioning (AAC) is an emerging audio perception task that has gained popularity in recent years.
It involves recognizing environments, sound events, and the temporal relationships between these events, and then describing them in fluent sentences.
Currently, the standard approach to address this problem is based on an encoder-decoder deep learning framework, with many studies focusing on innovative network architectures and training schemes for improvements [^016].
The Audio Captioning Transformer (ACT) \cite{mei2021audio} is the first full Transformer architecture designed specifically for AAC, excelling in capturing global information and temporal relationships within audio signals.
**EnCLAP**[^005] integrates two powerful acoustic representation models, EnCodec and CLAP, with the pre-trained language model BART to generate more accurate and contextually relevant captions.
AutoCap \cite{haji2024taming} leverages extra guidance from metadata to enhance feature extraction, capturing nuanced acoustic information.
LP-MusicCaps \cite{doh2023lp} addresses data scarcity in music captioning by using LLMs to generate descriptions from large-scale tag datasets.

</td><td>

</td></tr></table>

### Multi-task Tuning and Cooperation

<table><tr><td width="50%">

In the early stages of audio processing, research efforts were primarily focused on breaking down complex tasks into simpler, independent audio tasks.
Some studies \cite{**Pengi**[^001]; das2024speechverse} suggest that while these task-specific models were effective, they lacked understanding capabilities and failed to capture the connections between tasks, resulting in poor scalability and limited robustness when dealing with the inherent variability and complexity of audio signals.
Some remarkable work has combined LLM in ALM achieving understanding and reasoning across diverse audio tasks.
This convergence of large-scale language modeling with audio processing has opened up new possibilities to create more powerful, context-aware, and human-like audio understanding and reasoning systems.

As listed in Table.\ref{tab3:multi-task models and systems}, the recently popular ALMs can be categorized into two main types: Multi-task Models (including codec-based models and LALMs) and Audio-Language Agent Systems.

</td><td>

</td></tr></table>

#### Multi-task Tuning Models

<table><tr><td width="50%">

Multi-task tuning relies on a unified learning framework as illustrated in Figure. \ref{fig:multi-task tuning}, which can be achieved by multi-task sequence-to-sequence modeling, instruction tuning or in-context learning.

Multi-task sequence-to-sequence modeling is a classic approach involves designing a multi-task sequence format for jointly represented tokens, which are widely adopted by codec-based ALMs.
Codecs convert continuous audio into discrete tokens, which can easily be merged with text, MIDI, and other contextual tokens, and then passed to a language model to generate an output sequence that is further converted into the desired audio, text, or other outputs as needed for the task (**VALL-E**[^014]).
This codec-based pipeline based on discrete token has the advantage of high fidelity and is widely used in generative tasks.
Moreover, its flexible encoding allows for more natural interaction between text and audio, enabling the use of a single model to perform ASR, TTS, S2TT, MT, and S2ST tasks, using context or ID to indicate the task to which a sample belongs \cite{wang2023viola}.
Depending on the type of language model employed, codecs can be categorized into autoregressive (AR) and non-autoregressive (NAR) models.
However, they may suffer from information loss due to the quantization of voice signals into discrete tokens, leading to significant performance degradation in some tasks compared to models that use continuous speech features \cite{wang2024lauragpt}.

Apart from codecs, Large Audio-Language Models (LALMs) aims to leverage the emergent understanding and reasoning capabilities of LLMs to build more generalized audio-centric models.
These models typically employ an audio pre-trained model as the encoder, set up encoders for other modalities as needed, and add adaptive modules before connecting an LLM on top.
However, pre-trained LLMs still lack sufficient cross-task generalization capabilities and require further post-training [^011].
Multi-task tuning for LALMs can be similar to codecs, adopting a unified sequence format, or achieved through instruction tuning or in-context learning.
VioLA \cite{wang2023viola} is a Transformer-based autoregressive decoder network that unifies cross-modal tasks involving speech and text.
The model converts speech into discrete tokens and integrates task IDs and language IDs to enhance the ability to handle different languages and tasks.
Experimental results demonstrate its excellent performance in both single-modal and cross-modal tasks.
LauraGPT \cite{wang2024lauragpt} operates based on a unified task expression (input embeddings, task ID, output tokens) to handle multiple tasks.
Its modular and flexible design allows the model to perform complex tasks by combining sub-tasks such as S2ST.
**AudioPaLM**[^007] is a model for speech understanding and generation.
In combined task learning, the model is instructed to output intermediate steps of complex tasks, which aligns with the spirit of chain-of-thought prompting.
For example, in Speech-to-Speech Translation (S2ST), the model can perform end-to-end conversion from English audio tokens to French audio tokens through a single autoregressive decoding process, guided by task tags like [ASR AST S2ST English French].
This enables the integration of ASR, MT, and TTS within a unified model, eliminating the need for a separated pipeline approach.
UniAudio \cite{yang2024uniaudio} is an advanced codec-based ALM supports 11 diverse audio generation tasks.
It achieves this by tokenizing both target audio and various condition modalities, then concatenating these source-target pairs into unified [conditions, target] sequences.
The system leverages LM for next-token prediction and incorporates a multi-scale Transformer architecture to efficiently handle the extended sequence lengths introduced by neural codec-based tokenization.

**Pengi**[^001] frames all audio tasks as text-generation tasks, allowing for comprehensive audio understanding and generation.
Pengi demonstrates remarkable capabilities across zero-shot tasks in general audio including environmental sound, speech, and music tasks, and achieves good performance in close-ended and open-ended audio tasks.
**Qwen-Audio** [^033] adopts a multi-task training framework, which uses hierarchical tags to condition the decoder, allowing for effective knowledge sharing while avoiding interference issues.
This approach enables the model to handle diverse audio types and tasks, including automatic speech recognition (ASR), audio captioning, and audio question-answering.
Qwen-audio is pre-trained across over 30 tasks in environmental sound, speech, and music, and achieves state-of-the-art results on 9 evaluation benchmarks without requiring task-specific fine-tuning, surpassing baselines on several tasks such as automatic speech recognition and acoustic scene classification.

Instruction tuning is another one of the current mainstream methods, which aims to provide multiple audio tasks with unified instance construction formats using natural language.
This allows a single model to handle multiple tasks without explicit fine-tuning on specific tasks but by enhancing its instruction-following capability \cite{liu2024instructiontuning}.
[SpeechGPT](../Models/SpokenDialogue/2023.05.18_SpeechGPT.md)[^008] incorporates discrete speech representations into a large language model framework, enabling intrinsic cross-modal conversational abilities.
SpeechGPT is built upon SpeechInstruct, a pioneering large-scale cross-modal speech instruction dataset, and employs a sophisticated three-stage training strategy: modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning.
This approach allows SpeechGPT to perceive and generate multi-modal content effectively, demonstrating impressive capabilities in following cross-modal human instructions and showcasing the potential for handling multiple modalities within a single model.
LLark \cite{gardner2023llark} and MusiLingo \cite{deng2024musilingo} are instruction-tuned multimodal model designed specifically for music tasks, from classification and regression to more complex captioning and reasoning challenges and has demonstrated impressive capabilities in zero-shot tasks.
SALMONN \cite{tangsalmonn} conduct a three-stage cross-modal training method of pre-training, instruction tuning and an additional activation tuning stage proposed to resolve the issue of over-fitting to the speech recognition and audio captioning tasks in instruction tuning.
SALMONN demonstrates impressive capabilities across zero-shot and downstream tasks in environmental sound, speech, and music, and introduce two novel tasks: audio-based storytelling and speech audio co-reasoning.
LTU [^024] aims to leverage the comprehension abilities of LLM to enable an audio foundation model to listen, think, and understand.
To train LTU, they created a new OpenAQA-5M dataset consisting of closed-ended and open-ended diverse (audio, question, answer) tuples, using a perception-to-understanding curriculum with an autoregressive training framework.
LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning, as well as emerging audio reasoning and compression abilities that are absent in existing audio models.
GAMA \cite{ghosh2024gama} integrates multiple types of audio representations using a custom Audio Q-Former and a multi-layer aggregator, capturing diverse aspects from semantic generalization to surface-level audio properties.
They introduced CompA-R and CompA-R-test, a human-labeled dataset for open-ended audio question answering requiring complex reasoning.
Fine-tuned on CompA-R, GAMA demonstrates strong performance in complex reasoning and instruction-following tasks.

In-context learning enables LLMs to quickly adapt to specific tasks using a few examples, allowing them to predict unseen input labels without additional parameter updates.
In the transfer of ALMs, the goal is to learn and enhance vocabulary from the context of prompt texts without performing backpropagation \cite{Chen2024SALM}, thereby unlocking the capabilities of frozen LLMs for a wide range of audio understanding and generation tasks \cite{yang2024uniaudio1.5}.
Voicebox \cite{NIPS2023Voicebox} is a non-autoregressive flow-matching model trained to infill speech based on audio context and text, utilizing over 50,000 hours of unfiltered or enhanced speech data for training.
Voicebox can perform various tasks through in-context learning, such as mono or cross-lingual zero-shot TTS, noise removal, content editing, style conversion, and diverse sample generation, achieving high fidelity and efficiency in speech synthesis and editing.
SALM \cite{Chen2024SALM} equipped with multitask and in-context learning capabilities.
SALM achieving comparable performance to task-specific models through instruction tuning for ASR and AST.
Additionally, a speech-supervised in-context training method is proposed to bridge the gap between LLM training and downstream speech tasks, further enhancing the in-context learning ability of speech-to-text models.
Audio Flamingo \cite{kong2024audioflamingo} supports instruction tuning on interleaved audio-text pairs, thereby excelling in robust multi-turn dialogue capabilities.
By incorporating in-context learning, it achieves quick adaptation to unseen tasks.
The model attains state-of-the-art results on several close-ended and open-ended audio understanding tasks without the need for task-specific fine-tuning.

</td><td>

</td></tr></table>

### Audio-Language Agent Systems

<table><tr><td width="50%">

Agent Systems utilize LLMs as agents to transform human instructions, achieving task transfer in downstream tasks by cooperating different foundational pre-trained models, as shown in Figure. \ref{fig:agent system}.
By leveraging the reasoning capabilities of LLMs, these agent systems are designed to invoke and collaborate with a diverse array of audio models and tools to accomplish complex tasks initiated by humans.

SpeechAgents \cite{zhang2024speechagents} is a multi-modal, multi-agent system designed to simulate human communication.
The agents collaborate in a distributed and parallel manner to formulate decisions and execute corresponding actions, significantly enhancing work efficiency and effectiveness.
A Multi-Agent Tuning technique was proposed, allowing the system to handle complex, multi-party communications more effectively.
Notably, the system shows excellent scalability, performing well even with up to 25 agents.
This capability makes SpeechAgents particularly suitable for tasks such as drama creation and audio novel generation.
MusicAgent \cite{yu2023musicagent} is a system designed to integrate and organize various music processing tasks.
The method employed by MusicAgent consists of a comprehensive toolset for collecting music-related tools from diverse sources, and an autonomous workflow powered by LLMs that organizes these tools, decomposes user requests into multiple sub-tasks automatically, and invokes the appropriate music tools to fulfill these requirements.
MusicAgent is capable of handling a wide range of music processing tasks and aims to bridge the gap between tools from different sources by unifying data formats (text, MIDI, ABC notation, audio), thereby enabling seamless collaboration among tools on different platforms.
AudioGPT \cite{huang2024audiogpt} is an innovative multi-modal system that integrates ChatGPT with audio foundation models and input/output interfaces to process complex audio information and conduct spoken conversations.
Its method involves four key stages: modality transformation, task analysis, model assignment, and response generation.
AudioGPT demonstrates proficiency in various audio-related tasks, including speech processing, music analysis and generation, and sound effect understanding and creation.

SpeechAgents \cite{zhang2024speechagents} is a multimodal and multi-agent system that proposes a multi-agent tuning technique.
The agents collaborate in a distributed and parallel manner to make decisions and execute actions, thereby significantly improving work efficiency and effectiveness.
With excellent scalability, SpeechAgents is particularly suitable for tasks such as drama creation and audio novel generation.
MusicAgent \cite{yu2023musicagent} is a system designed to integrate and organize various music processing tasks.
Driven by LLMs, it automatically decomposes user requests into multiple sub-tasks and invokes the appropriate music tools to fulfill these requirements.
MusicAgent can handle a wide range of music processing tasks and aims to bridge the gap between tools from different sources by unifying data formats (text, MIDI, ABC notation, audio), enabling seamless collaboration among tools on different platforms.
AudioGPT \cite{huang2024audiogpt} is an innovative multimodal system that integrates ChatGPT with audio foundation models and input/output interfaces to process complex audio information and conduct spoken conversations.
AudioGPT demonstrates proficiency in various audio-related tasks, including speech processing, music analysis and generation, and sound effect understanding and creation.

</td><td>

</td></tr></table>

## 6·Datasets and Benchmarks: 数据集 & 基准

<a id="Section.06"></a>

<table><tr><td width="50%">

High-quality, large-scale data is crucial for the learning of audio representations.
In this section, we review the audio-language datasets, which primarily include audio-text paired datasets and audio question answering datasets.
Their basic forms are illustrated in Figure. \ref{fig11: datasets}.

</td><td>

</td></tr></table>

### Audio-Text Paired Datasets

<table><tr><td width="50%">

Audio-text paired datasets consist of audio recordings paired with corresponding text, such as captions (description of audios), transcriptions (for speech datasets) or translations (transcriptions in another language of what the speaker says).
For natural language can describe multiple events in the audio more freely and in greater detail, audio-caption datasets providing more information for representation pre-training.
Datasets composed of transcriptions and translations are more often used for training models on specific tasks.
Commonly used audio-text paired datasets for training ALMs are listed in Table.\ref{tab4:paired datasets}.

</td><td>

</td></tr></table>

### Audio Question Answering Datasets

<table><tr><td width="50%">

Audio Question Answering Datasets are designed for training and evaluating models that can answer questions about audio content.
These datasets are structured as triplets of audio, question, and answer.
Initially, such datasets were specifically created for Audio Question Answering (AQA) tasks, constructing diverse questions to train models on how to answer similar queries.
In recent years, AQA tasks have been increasingly utilized as a proxy task for model learning perception and reasoning about audio \cite{fayek2020temporal}.
They can serve as a format that integrates multiple audio tasks [^024] and can also be used to guide models in better following human instructions ([SpeechGPT](../Models/SpokenDialogue/2023.05.18_SpeechGPT.md)[^008]).
We have listed several popular open-source QA datasets in Table.\ref{tab5:QA datasets}.

</td><td>

</td></tr></table>

### Benchmarks

<table><tr><td width="50%">

Evaluating ALMs involves integrating them with specific audio tasks and datasets, leading to variations in standards and makes model comparison unfair.
To solve this, several benchmarks have been created, which not only release test datasets but also establish corresponding evaluation strategies.
These benchmarks ensure strict and consistent evaluation of ALMs, enhancing scientific reliability.

Benchmarks can be categorized into three types: task-specific, cross-task, and audio instruction benchmarks.
Task-specific benchmarks like SoundDescs [^015], **CompA**[^026], and ADU-Bench \cite{gao2024ADUbench} evaluate models on particular tasks such as ATR and audio dialogue.
Cross-task benchmarks such as ARCH \cite{la2024ARCH} and MMAU \cite{sakshi2024mmau} test generalization across multiple domains and tasks, emphasizing diverse training data.
Other speech-related cross-task benchmarks include SUPERB \cite{yang2021superb}, Dynamic-SUPERB \cite{Huang2024Dynamic-Superb}, LeBenchmark \cite{evain2021LeBenchmark}, LAPE \cite{Ghosh2022LAPE}, VoiceBench \cite{chen2024voicebench}, as well as music domain benchmarks like MusicBench \cite{melechovsky2024mustango} and MuChoMusic \cite{weck2024muchomusic}.
Audio instruction benchmarks like AIR-Bench \cite{yang2024air} and AudioBench \cite{wang2024audiobench} assess the abilities of LALMs in understanding audio signals and following instructions, using a unified human-computer conversation format to cover various speech and audio tasks.

These benchmarks aim to improve model performance and ensure fair comparison across different scenarios.
The baseline performances on these benchmarks reflect the progress of model research and help identify current limitations to guide future studies, thereby advancing the field of audio-language learning and developing more powerful models.

</td><td>

</td></tr></table>

## 7·Challenges and Future Directions: 挑战 & 未来方向

<a id="Section.07"></a>

<table><tr><td width="50%">

In this section, we will analyze some challenges currently faced by ALM research and humbly share possible research directions.

From a data perspective, there are three challenges and potential research directions.

1) Audio Sources:
The current audio-language datasets have limited sources of audio samples [^018], mainly including AudioSet \cite{gemmeke2017audioset}, FreeSound \cite{fonseca2017freesound}, and BBC Sound Effects \cite{bbc}.
When using multiple audio-language datasets for model training, it is often limited due to the overlap of data sources, restricting the scaling up of model performance.
In addition, the limited training data presents distribution shift issues \cite{chang2024context}, resulting in ALM lacking robustness and generalization ability when facing a variety of audio types and qualities.
Compared with the data sources of LLM and VLM, the diversity of audio data is still limited \cite{raffel2020exploring}.
We hope to develop methods for constructing online audio datasets to expand the nearly limitless scale of internet resources.

2) Text Sources:
Currently, audio-language datasets focus on generating audio captions from known labeled datasets or manual annotation, hindering the acquisition of larger-scale datasets.
The evolution of LLMs encourages more researchers to create ALM-suitable datasets via model generation methods.
Despite their powerful performance, generated text may still contain inaccuracies like hallucinations \cite{liu2024mminstruct}.
The quality issue of generated datasets significantly restricts ALM development.
Constructing dependable model generation methods is a crucial research direction needing further advancement.
Additionally, the vast but noisy audio-related text on the internet, such as audio introductions and comments, remains overlooked.

3) Evaluation Benchmarks and Analysis:
A unified model evaluation framework is essential for identifying model limitations and advancing the field.
While there has been significant progress in ALM research, empirical analysis across a wide range of audio types, tasks, and models remains scarce.
This gap is partly due to the limited availability of audio resources, as data used in evaluations should not overlap with the training set.
Therefore, we encourage the construction of evaluation benchmarks with broader coverage beyond mainstream audio sources, from other public or private domains, and look forward to extensive empirical analyses based on these benchmarks.

</td><td>

</td></tr>
<tr><td>

From a model perspective, there are four challenges and potential research directions.

1) Unified multimodal encoder:
Current ALMs mainly employ two separate networks for processing audio and language modal inputs.
In the vision community, there is a proposition that unifying learning on the One Head architecture can enhance communication efficiency across data modalities, improve training efficiency, and alignment \cite{tschannen2022image, [^031]}.
However, there are relatively few related studies in the field of audio-language [^035].
Compared to training methods, the model architecture is a more fundamental issue and deserves more effort to explore and improve.

1) Pre-trained Foundation Models:
Literature has experimentally demonstrated that the choice of pre-trained audio and text encoders is crucial for generating robust representations and improving task performance \cite{deshmukh2022wavtext, [^022], [^040]}.
However, with the rapid development of language models, the primary audio encoders are still pre-trained models such as PANNs [^043] and AST [^048].
We acknowledge that the open ALM community lacks a large, well-trained audio encoder to achieve performance improvements in ALM.

1) Continual Learning:
Existing work highlights the use of larger-scale training data to enhance the pretraining performance of ALMs.
However, as streamable data accumulates, pre-training from scratch requires increasingly large storage and computational resources, limiting the continuous growth of models.
Moreover, when old data is unavailable, such pretraining from scratch is not permissible.
We aspire for models to directly continue learning on new datasets without affecting their performance on old tasks and zero-shot predictions.
This method of continual learning for ALMs has yet to be studied.

1) Efficient Learning:
Compared to the natural language and vision communities, sources of audio are relatively limited, and pre-training ALMs on large-scale training data requires intensive computation, making sustainability a significant issue.
We look forward to research on data-efficient ALMs that can train effective models with limited audio-text data.
For example, learning not only from each audio-text pair but also utilizing more useful information from supervised learning between paired data \cite{wu2022data}.
Also employing active learning or curriculum learning methods are possible to make more efficient use of limited data \cite{wan2023activesurvey}.
Moreover, for audio tasks and transfer learning methods are orthogonal, we encourage combining ALMs with more transfer learning methods to explore a better trade-off between parameter-efficient learning and performance.

</td><td>

</td></tr></table>

## 8·Conclusions: 结论

<a id="Section.08"></a>

<table><tr><td width="50%">

ALMs can effectively utilize natural language to learn detailed descriptions of audio, achieving zero-shot predictions without any task-specific fine-tuning and demonstrating high performance on fine-tuned audio-centric tasks.
Models combined with LLMs exhibit listening, thinking, and reasoning capabilities for complex audio and have achieved significant success in following audio instructions.
This survey extensively reviews ALMs for audio tasks from multiple perspectives, including background, foundation, representation pre-training, downstream transfer, training and evaluation data.
It abstracts the common characteristics of related work through illustration and formalization, and compares and summarizes mainstream research through tables.
The survey covers a wide range of tasks in various audio types such as natural sounds, speech, and music, providing a clear panorama of the latest developments in ALMs.
On this basis, we analyze the current challenges in this field and humbly propose potential research directions that we believe could benefit the future development of this emerging but very promising area.

</td><td>

</td></tr></table>

## References: 参考文献

[^001]: [**Pengi**: An Audio Language Model for Audio Tasks.](../Models/SpeechLM/ST2T/2023.05.19_Pengi.md) NeurIPS2023.
[^002]: [**CLAP** Learning Audio Concepts from Natural Language Supervision.](../Models/_Basis/2022.06.09_CLAP_(CNN14+BERT).md) IEEE@ICASSP2023.
[^003]: Bridging Language Gaps in Audio-Text Retrieval. InterSpeech2024.
[^004]: [**AudioLDM**: Text-to-Audio Generation with Latent Diffusion Models.](../Models/Diffusion/2023.01.29_AudioLDM.md) ICML2023.
[^005]: **EnCLAP**: Combining Neural Audio Codec and Audio-Text Joint Embedding for Automated Audio Captioning. ICASSP2024.
[^006]: Separate What You Describe: Language-Queried Audio Source Separation.
[^007]: [**AudioPaLM**: A Large Language Model that Can Speak and Listen.](../Models/SpeechLM/ST2ST/2023.06.22_AudioPaLM.md) ArXiv:2306.12925.
[^008]: [**SpeechGPT**: Empowering Large Language Models with Intrinsic Crossmodal Conversational Abilities.](../Models/SpokenDialogue/2023.05.18_SpeechGPT.md) EMNLP2023.
[^009]: Audio Dataset.[URL](https://github.com/LAION-AI/audio-dataset/blob/main/README.md).
[^010]: Audio Caption: Listen and Tell. ICASSP2019.
[^011]: A Survey of Large Language Models. ArXiv:2303.18223.
[^012]: [**Survey**: Recent Advances in Speech Language Models: A Survey.](2024.10.01_Recent_Advances_in_Speech_Language_Models__A_Survey_20P/Main.md) ArXiv:2407.06947.
[^013]: [**WavChat**: A Survey of Spoken Dialogue Models.](2024.11.15_WavChat_60P/Main.md) ArXiv:2411.13577.
[^014]: [**VALL-E**: Neural Codec Language Models Are Zero-Shot Text to Speech Synthesizers.](../Models/SpeechLM/ST2S/2023.01.05_VALL-E.md) ArXiv:2301.02111.
[^015]: Audio Retrieval with Natural Language Queries: A Benchmark Study.
[^016]: Beyond the Status Quo: A Contemporary Survey of Advances and Challenges in Audio Captioning.
[^017]: Recent Advances in Direct Speech-to-Text Translation.
[^018]: Audio-Language Datasets of Scenes and Events: A Survey.
[^019]: AudioCaps: Generating Captions for Audios in The Wild.
[^020]: Clotho: An Audio Captioning Dataset.
[^021]: Clotho-AQA: A Crowdsourced Dataset for Audio Question Answering.
[^022]: Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation.
[^023]: [BEATs: Audio Pre-Training with Acoustic Tokenizers.](../Models/SpeechRepresentation/2022.12.18_BEATs.md)
[^024]:
[^025]:
[^026]:
[^027]:
[^028]:
[^029]:
[^030]:
[^031]:
[^032]:
[^033]:
[^034]:
[^035]:
[^036]:
[^037]:
[^038]:
[^039]:
[^040]:
[^041]:
[^042]:
[^043]:
[^044]:
[^045]:
[^046]:
[^047]:
[^048]:
[^049]:
[^050]:
