---
PDF: 
标题: Deep Learning-Based Expressive Speech Synthesis - A Systematic Review of Approaches, Challenges, and Resources
作者:
  - Huda Barakat
  - Oytun Turk
  - Cenk Demiroglu
机构:
  - OzyeginU
代码: 
ArXiv: 
提出时间: 
出版社: 
发表期刊: 
发表时间: 
引文数量: 
被引次数: 
tags:
  - 语音合成_TTS
  - "#综述_Survey"
  - "#表现性语音_Expressive_Speech"
---

@import "../style.less"

# Deep Learning-Based Expressive Speech Synthesis: A Systematic Review of Approaches,  Challenges, and Resources<br>基于深度学习的表现性语音合成: 方法, 挑战, 资源的系统性回顾

## A.摘要

> Speech synthesis has made significant strides thanks to the transition from machine learning to deep learning models. 
> Contemporary <term>text-to-speech (TTS)</term> models possess the capability to generate speech of exceptionally high quality, closely mimicking human speech. Nevertheless, given the wide array of applications now employing TTS models, mere high-quality speech generation is no longer sufficient. 
> Present-day TTS models must also excel at producing expressive speech that can convey various speaking styles and emotions, akin to human speech. Consequently,researchers have concentrated their efforts on developing more efficient models for expressive speech synthesis in recent years. This paper presents a systematic review of the literature on expressive speech synthesis models published within the last 5 years, with a particular emphasis on approaches based on deep learning. We offer a comprehensive classification scheme for these models and provide concise descriptions of models falling into each category.
> Additionally, we summarize the principal challenges encountered in this research domain and outline the strategies employed to tackle these challenges as documented in the literature. In the Section 8, we pinpoint some research gaps in this field that necessitate further exploration. Our objective with this work is to give an all-encompassing overview of this hot research area to offer guidance to interested researchers and future endeavors in this field.

## 1.引言

> Since the late 1950s, computer-based text-to-speech systems (TTS) have undergone significant advancements, culminating in the production of models that generate speech almost indistinguishable from that of a human. This progress has followed a path consisting of several stages, beginning with conventional methods named as concatenative synthesis and progressing to more advanced approaches known as **Statistical Parametric Speech Synthesis (SPSS)**. Advanced approaches are mainly based on machine learning algorithms like <algo>hidden Markov models (HMMs)</algo> and <algo>gaussian mixture models (GMMs)</algo>. Despite this progress, speech generated by these methods was still noticeably artificial. However, the emergence of deep learning (DL) as a new branch under machine learning (ML) in 2006 has led to significant improvements. Speech synthesis researchers,like many in other research fields, started incorporating deep neural networks (DNN) in their models. Initially,DNNs replaced HMMs and GMMs in SPSS models while the main structure still follows the primary framework of SPSS models as shown in Fig. 1. As discussed in **"Statistical Parametric Speech Synthesis Using Deep Neural Networks"**, the deep learning-based models have overcome many limitations and problems associated with machine learning-based models.

> Researchers continue to aim for improved speech quality and more human-like speech despite past advancements. Additionally, they seek to simplify the framework of the text-to-speech models due to the intricate nature of the SPSS structure, which limits progress in this field to those with extensive linguistic knowledge and expertise. Deep learning advancements have brought about the simple encoder-decoder structure for TTS models as <algo>sequence-to-sequence (Seq2Seq)</algo> approaches. The pro-posed approaches have simplified the structure of conventional TTS with multiple components into training a single network that converts a set of input text characters/phonemes into a set of acoustic features (mel-spectrograms). A main concern in these advanced TTS models is the mapping process between the input and output sequences, which is a one-to-many problem, as the single input text can have multiple speech variations as output. In fact, there are two groups of recent TTS models, as shown in Fig.02. The first group generates mel-spectrograms in a sequential (autoregressive) manner using soft and automatic attention alignments between input and output sequences, such as the Tacotron model ([Tacotron (2017)](../2017.03.29_Tacotron/2017.03_Tacotron.md) [Tacotron2 (2017)](../../Models/AV1_Acoustic/2017.12_Tacotron2.md)). The second group utilizes hard alignments between the phonemes/characters and mel-spectro-grams, and thus its speech generation process is parallel(non-autoregressive), as in the FastSpeech model ([FastSpeech (2019)](../../Models/AV1_Acoustic/2019.05_FastSpeech.md) [FastSpeech2 (2020)](../../Models/AV1_Acoustic/2020.06_FastSpeech2.md)).This improvement in the structure of the TTS model has encouraged rapid development in the field within the last few years, during which the proposed models produced speech that is nearly indistinguishable from human speech.

> Human speech is highly expressive and reflects various factors, such as the speaker’s identity, emotion, and speaking style. In addition, there are many applications in which speech synthesis can be utilized, especially expressive speech synthesis. For instance, audiobooks and podcast applications that create audio versions of eBooks and podcasts, translation applications which provide real-time translation of foreign language text, dubbing applications that generate an alternative audio track for a video with different content, speaker, or language, and content creation applications which help produce audio versions of textual content, such as blogs and news articles. E-learning applications that allow for adding voice-over audio to e-learning courses, and conversational AI applications enable machines to communicate with users in a human-like manner, such as AI chatbots and virtual assistants.

> As spoken language is a crucial component in such applications, users must feel as if they are communicating with a real human rather than a machine. Therefore, the speech generated by these applications should convey appropriate emotion, intonation, stress, and speaking style to match the ongoing conversation or the content type and context of the text being read.

> As a result, there has been a recent attention towards building efficient expressive speech synthesis models as another step forward in achieving human-like speech. Therefore, many studies have been devoted to expressive speech synthesis (ETTS) as a hot research area, particularly over the last 5 years. In this work, we present the findings of our systematic literature review on ETTS field from the past 5 years. Firstly, we suggest a classification schema of deep learning-based ETTS models that are proposed during this period, based on structures,and learning methods followed in each study. A summary is then provided for each category in the classification schema and main papers related to this category. After that, we outline the main challenges in the ETTS area and solutions that have been proposed to solve them from literature. Finally, we conclude with a discussion of the implications of our work and a highlight of some gaps that require further research in this area.

> During our work on this review of expressive speech synthesis literature, we came across several review papers that focus on different stages of development in the speech synthesis field. The majority of these reviews concentrate on DL-based TTS approaches, while only a few papers cover recent TTS approaches in addition to early conventional ones. However, to the best of our knowledge, there are no review papers that cover the fast growth in the (expressive) speech synthesis area, especially in the last few years. Therefore, our main goal in this review is to provide an overview of research trends, techniques, and challenges in this area during this period. We hope that our work will offer researchers a comprehensive understanding of how and what has been accomplished in this field and the gaps that need to be filled as guidance for their future efforts.

> While we were writing this paper, we came across an interesting recent review paper **"An Overview of Affective Speech Synthesis and Conversion in the Deep Learning Era"** that is similar to our work. However, the review covers emotional speech synthesis (ESS) as a sub-field of voice transformation while our work is more comprehensive as a systematic literature review that discusses approaches,challenges, and resources. Furthermore, the taxonomy we provide for the reviewed approaches differs from the one given in as elaborated in the next section.

> The remaining sections of this paper are structured as follows: Section 2 provides an explanation of the methodology employed for conducting this review. Sections 3and 4 describe the different main and sub-categories of the proposed classification schema for DL-based expressive TTS models. Main challenges facing ETTS models and how they have been tackled in the literature are covered in Section 5. We then give a brief description of ETTS datasets and applied evaluation metrics in Sections 6 and 7, respectively. Finally, Section 8 concludes the paper.

## 2.方法

> The last few years have seen rapid growth in expressive and emotional speech synthesis approaches, resulting in a large number of papers and publications in this area. Here, we present the outcomes of a systematic literature review of the last 5 years’ publications within this active research area. This section describes the methodology used to conduct the review, illustrated by Fig.03, which consists of three main stages: paper selection, paper exclusion, and paper classification.

### 2.1.论文选择

> For our review, we used the Scopus database to retrieve papers as it encompasses most of the significant journals and conferences pertaining to the speech synthesis field. Our query criteria to find relevant papers on Scopus were twofold: (1) the paper title must include at least one of four words (emotion* OR expressive OR prosod* OR style) that denote expressive speech, and (2) the paper title, abstract, or keywords must comprise the terms “speech” AND “synthesis,” in addition to at least one of the above-mentioned words for expressive speech.
> We considered all papers written in English and published in journals or conferences since 2018. The search query was conducted in January 2023, and it yielded 356papers. Scopus provides an Excel file containing all the primary information of the retrieved papers, which we used in the second stage of our review.

### 2.2.论文排除

> The exclusion of papers occurred in two phases. In the first phase, we screened the abstract text, while in the second phase, we screened the full text of the paper. Five main constraints were used to exclude papers, including (1) papers that were not related to the TTS field, (2)papers that were not DL-based models, (3) papers that did not focus on expressive or emotional TTS models, (4)papers that were too specific to non-English languages,and (5) papers that lacked details about the applied method. After screening the paper abstracts, we excluded180 papers, mostly based on the first exclusion criterion.
> During the second exclusion phase, in which we read the full text of each paper, we identified another 65 papers that met at least one of the five exclusion criteria. Consequently, 111 papers were included in the third stage of our review. Additionally, a group of recently published papers in this area ([17,18,19,20] [InstructTTS (2023)](../2023.01_InstructTTS/2023.01_InstructTTS.md), [22,23, [DiffProsody (2023)](../2023.07_DiffProsody/2023.07_DiffProsody.md), [VoiceBox (2023)](../2023.06_VoiceBox/2023.06_VoiceBox.md)) was hand-picked and added to the final set of selected papers. While most of the reviewed papers trained their models on English data, a few other papers used data in other languages as listed in Table 1.

### 2.3.论文分类

> After summarizing the approach proposed for generating expressive speech in each selected paper, we categorized the papers based on the learning approach applied in each one. Accordingly, papers are divided into two main categories, including supervised and unsupervised approaches. Under the supervised category, where labeled data is utilized, we identified three subcategories based on how models are employed expressive speech synthesis. The three proposed subcategories are (1)labels as input features, (2) labels as separate layers or models, and (3) labels for emotion predictors/classifiers.

> Papers in the unsupervised approaches category are grouped into four different subcategories based on the main structure or method used for modeling expressivity in these papers. From our observation, most of the proposed methods in the last 5 years are based on three main early works in this field, namely, reference encoder [74], global style tokens[75], and latent features via variational autoencoders (VAE)[76] [77]. Specifically, proposed models in most of the papers under this category can be considered as an extension or enhancement of one of the three previously mentioned methods. Besides, we identify a fourth subcategory that includes the recent TTS models representing the new trend in the TTS area, which utilizes in-context learning. There is one factor common to all these four unsupervised models,which is that they are all based on using an audio reference/prompt. Additionally, we added a fifth subcategory (named other approaches) in which we include approaches outside the previous four main unsupervised approaches. Fig.04 illustrates the proposed classification schema for the DL-based expressive speech synthesis models.

## 3.监督学习方法


Supervised approaches refer to models that are trained on datasets with emotion labels. Those labels guide model training, enabling it to learn accurate weights.
Early deep learning-based expressive speech synthesis systems were primarily supervised models that utilized labeled speech exhibiting various emotions (such as sadness, happiness, and anger) or speaking styles (such as talk-show, newscaster, and call-center). Note that the term style has also been used to refer to a set of emotions or a mixture of emotions and speaking styles ([ST-TTS](../2021.04_ST-TTS/2021.04_ST-TTS.md), [68] [78] [79]. 
**Generally, the structure of early conventional TTS models was built upon two primary networks: one for predicting duration and the other for predicting acoustic features. These acoustic features were then converted to speech using vocoders. Both networks receive linguistic features extracted from the input text.** 
In supervised ETTS approaches, speech labels (emotions and/or styles) are represented in the TTS model as either input features or as separate layers, models, or sets of neurons for each specific label. The following sections explain these three representations in detail then we provide a general summary of the supervised approaches reviewed in this work in Table 2.

> 监督学习方法是指在带有情感标签的数据集上训练的模型。这些标签指导模型训练以确保能够学习到准确的权重。
> 早期基于深度学习的表达性语音合成系统主要是监督模型，他们使用带有各种情感 （如悲伤、快乐和愤怒）或说话风格（如脱口秀、新闻播音和呼叫中心）标签的语音。注意，“风格”这一术语同样用于指代一组情感或情感和说话风格的混合。
> **通常早期传统 TTS 模型的结构是建立在两个主要网络之上：一个用于预测时长；另一个用于预测声学特征，这些声学特征之后通过声码器转换为语音。这两个网络都接收从输入文本中提取出的的语义特征。**
> 在监督 ETTS 方法中，语音标签（情感与、或风格）在 TTS 模型中被表示为输入特征或为每个特定标签的单独层、模型或神经元集合。以下小节将详细解释这三种表示，然后在表格二中提供本文回顾的监督方法的概述。

|引用序号|算法简称|输入|情感标签表示|是否支持情绪转移|TTS 模型|
|:-:|:-:|---|---|:-:|:-:|
|80||语言特征+情感标签|独热编码||DL-SPSS, HMM|
|65||语言特征+情感标签|独热编码/单独层|√|DL-SPSS|
|66||语言特征+情感标签|感知向量/矩阵||DL-SPSS|
|41||语言特征+情感标签|独热编码||DL-SPSS|
|42||语言特征+情感标签|依赖层||DL-SPSS|
|81||语言特征+情感标签|独热编码/神经元集合|√|DL-SPSS|
|43||语言特征+情感标签|独热编码/依赖层/独立模型||DL-SPSS|
|82||语言特征+情感标签|独热编码|√|DL-SPSS|
|83||音素序列+语言模型特征+情感标签|嵌入向量||Encoder-Dttention-Decoder|
|28, 78||语言特征+情感标签|独热编码/依赖层/独立模型|√|DL-SPSS|
|26||音素序列+梅尔频谱+情感标签|独热编码 GSTs 权重的真实值||Tacotron2|
|27||音素序列+语言特征+情感标签|嵌入向量||Tacotron2|
|84||语言特征+情感标签|嵌入与其他数据标签联合||DL-SPSS|
|85||语言特征+韵律特征+情感标签|分类器的真实值||DL-SPSS|
|86||音素序列+情感标签|嵌入向量||Transformer TTS|
|32, 36||字符序列+梅尔频谱+情感标签|分类器的真实值||Tacotron2|
|69||语言特征+情感标签|独热编码/依赖层|√|DL-SPSS|
|34||音素序列+梅尔频谱+情感标签|分类器的真实值||Tacotron2|
|64||字符序列+语言模型特征+情感标签|分类器的真实值||Tacotron2|
|39, 87||音素序列+梅尔频谱+情感标签|分类器的真实值||Tacotron2|

### 3.1 Labels as Input Features 标签作为输入特征

The most straightforward method for representing emotion labels of annotated datasets as input to the TTS model is by using a one-hot vector. This approach entails using a vector with a size equivalent to the number of available labels. In this vector, a value of (1) is assigned to the index corresponding to the label ID, while all other values are set to (0). Many early ETTS models [43] [56] [65] [69] [78] [80] [82] [84] advocated for this direct representation of emotion labels in order to generate speech encompassing various emotions.
> 用于表示带有注释的数据集的情感标签作为 TTS 模型输入的最直接方法是使用独热编码。这种方法需要使用和可用标签数量长度相同的向量。在这个向量内，将值为 1 分配给标签 ID 对应的索引，其他值为 0. 许多早期 ETTS 模型提倡这种直接表示情感标签的方法以生成包含各种情绪的语音。

The one-hot emotion vector, also referred to as a style/emotion code in some studies [43] [78] [80] [82], is concatenated with the input linguistic features of the model.
> 独热编码在某些文献中也被称为风格/情感编码，和模型的输入语言特征进行拼接。

When dealing with large number of labels, the one-hot representation becomes both high-dimensional and sparse. Moreover, in other scenarios, merging label vectors with input features instead of concatenation can lead to length mismatch issues. 
> 当处理大量标签时，独热编码表示变得高维且稀疏。而且在其他情况下，将标签向量和输入特征合并而不是拼接会导致长度不匹配问题。

In both situations, the embedding layer offers a solution by creating a continuous representation for each label, known as embedding vectors. Unlike the one-hot vector, which is constrained in size based on the number of labels, an emotion embedding can have any dimension, regardless of the number of available labels.
> 在这两种情况下，嵌入层提供了一种解决方案：通过给每个标签创建一个连续的表示，即嵌入向量。和独热编码受到标签数量的限制不同，情感嵌入可以有任意的维度，和可用标签数量无关。

For instance, in [84], each sample in the training dataset has three separated labels including speaker, style(emotion), and cluster. In this context, the cluster value indicates the consistency in speech quality of a given speaker and style pair. If one-hot vector is used to represent each unique combined label of each sample, the resulting label vector will be high dimensional (which in this case is 67). Therefore, the three one-hot vectors representing the given three labels are combined and passed as input to an embedding layer to reduce its dimension (in this case 15). On a different note, [41] utilizes an embedding layer to expand concise binary one-hot label vectors to match with the dimensions of the input features to be added together as input to the TTS model.
> 例如, 在文献 [84] 中, 训练数据集的每个样本有三个独立的标签, 包括说话人, 风格 (情感) 和聚类类别. 在这种情况下, 聚类类别值表明了同时给定说话人和风格在语音质量方面的一致性. 如果独热编码用于表示每一个唯一拼接的标签, 那么标签向量会变得非常高维. 因此这三个独热编码向量分别表示给定的三个标签结合并输入到嵌入层进行降维. 而文献 [41] 使用嵌入层将简洁的二进制独热标签向量扩展到匹配输入特征的维度, 以便相加作为 TTS 模型的输入.

To address the potential disparities between a talker’s intent and a listener’s perception when annotating emotional samples, in [66], a different methodology for representing labels is introduced. In the context of N emotion classes, each sample from the talker may be perceived by the listener as one of the N emotions. In response to this, the paper suggests the adoption of a singular vector termed the ’perception vector,’ with N dimensions. This vector represents how samples from a specific emotion class are distributed among the N emotions, based on the listener’s perception. Furthermore, in the context of multiple listeners, each emotion class can be represented as a confusion matrix that captures the diverse perceptions of samples belonging to that emotion class by multiple listeners.
> 为了解决在注释情感样本时说话人意图和倾听者的感知之间的潜在差异, 文献 [66] 引入了表示标签的不同方法. 在具有 N 个情绪类别的情况下, 来自说话人的每个样本可能被倾听者感知为这 $N$ 个情绪的其中之一. 对此, 文献 [66] 建议采样一个名为"感知向量"的单个 N 维向量. 这一向量表示特定情绪类别的样本如何根据倾听者的感知在 N 个情绪上分布. 此外, 在多倾听者的情况下, 每个情绪类可以表示为一个混淆矩阵, 捕获由多个倾听者提供的属于该情绪类别的样本的多样性感知.

### 3.2 Labels as Separate Layers/Models 标签作为单独层/模型

In this approach, to represent emotion or style labels in TTS models, each label is associated with either a separate instance of the DNN model, an emotion-specific layers, or a set of emotion-specific neurons within a layer. Initially, the model is trained using neutral data, which typically has larger size. Subsequently, in the first approach, multiple copies of the trained model are fine-tuned using emotion-specific data of small size [43] [78]. In the second approach, instead of creating an individual model for each emotion, only specific model layers (usually the uppermost or final layers) from the employed DNN model are assigned to each emotion [43] [65] [69] [78] as shown by Fig. 5. While shared layers are adjusted during training using neutral data, output layers corresponding to each emotion are modified exclusively when the model is trained with data from the respective emotion.
> 在这种方法中, TTS 模型内为了表示情感或风格标签, 每个标签要么和 DNN 模型的单独示例即一个特定情感层, 要么和一层内的特定情感神经元集合相关联. 
> 首先, 模型使用通常尺寸较大的中性数据进行训练. 
> 第一种方法, 对多个已经训练好的模型的副本分别使用小尺寸的特定情感数据进行微调; 
> 第二种方法, 不为每种情感创建单独模型, 而是只将所使用的 DNN 模型中的特定层 (通常是最上层/最终层) 分配给每种情感. 如图五所示. 使用中性数据训练时共享层会进行调整, 对应每种情感的输出层仅在模型使用相应情感的数据进行训练是进行修改.

Alternatively, when dealing with limited data for certain emotions/styles, the model can initially undergo training for emotions with large amount of data. Following this step, the weights of the shared layers within the model are fixed, and only the weights of the top layers are fine-tuned using the limited, emotion-specific data [42]. 
> 当处理某些情感或风格的有限数据时, 模型可以先为具有大量数据的情感进行训练. 完成后将共享层的权重固定, 只有最顶层的权重使用有限的, 特定情感的数据进行微调. 如文献 [42]. 

Another method for representing emotion labels involves allocating specific neurons from a layer within the DNN model for each emotion. In this approach, the hidden layers of the model could be expanded by introducing new neurons. Then, as outlined in [81], particular neurons from this expanded set are assigned to represent each distinct emotion. Importantly, the associated weights of these specific neuron subsets are adjusted solely during the processing of data relevant to the corresponding emotion. Furthermore, by substituting the subset of neurons dedicated to a particular emotional class with a different set, the model becomes capable of generating speech imbued with the desired emotional class.
This capability holds true even for new speakers who only possess neutral data, and in this case, it is known as **expression/emotion transplantation**.
> 其他表示情感标签的方法是将 DNN 模型层中特定的神经元分配给每种情感. 这种方法可以通过引入新的神经元来扩展模型的隐藏层. 如文献 [81] 从扩展的神经元集合中分配特定神经元来表示每种不同的情感. 重要的是只有在处理和相应情感相关的数据时, 这些特定神经元子集的关联权重才会进行调整. 此外, 通过加入专门用于某种特定情感类别的神经元, 模型可以生成具有所需情感类的语音.
> 这种能力对仅有中性数据的新说话人也成立, 这称为表达/情感移植.

### 3.3 Labels for Emotion Predictors/Classifiers 标签用于情感预测器/分类器

Another common approach to utilize emotion labels is to use them directly or via emotion predictor or classifier to support the process of extracting emotion/prosody embedding. 
> 另一种常见的使用情感标签的方法是直接使用它们或者通过情感预测器/分类器以支持提取情感/韵律嵌入的过程.

For example, in **"End-to-End Emotional Speech Synthesis Using Style Tokens  and Semi-Supervised Training"** emotion labels represented as one-hot vectors are used as targets for the weight vectors of GSTs (explained in Section 4.3) where a cross entropy loss between the two vectors is added to the total loss function. Yoon et al. [64] proposes a joint emotion predictor based on the Generative Pre-trained Transformer (GPT)-3 [88]. The proposed predictor produces two outputs including emotion class and emotion strength based on features extracted from input text by (GPT)-3. A joint emotion encoder is then used to encode the predictor outputs into a joint emotion embedding. The joint emotion predictor is trained with the guidance of the emotion labels and emotion strength values obtained via a <algo>ranking support vector machine (RankSVM)</algo> [89].

> 文献 [026] 中情感标签表示成独热向量, 作为 GSTs 权重向量的目标值, 这两个向量之间的交叉熵损失被添加到总损失函数中;
> 文献 [64] 提出了基于 GPT-3 的联合情感预测器, 这个预测器基于 GPT-3 从输入文本中提取的特征产生两个输出包括情感类别和情感强度. 联合情感编码器将预测器的输出编码为一个联合情感嵌入. 联合情感预测器在通过 RankSVM 获得的情感标签和情感强度值得指导下进行训练.

In [32], an emotion classifier is used to produce more discriminative emotion embeddings. Initially, the input Mel-spectrogram features from the reference-style audio and those predicted by the proposed TTS model are passed to two reference encoders (explained in Section 4.1) to generate reference embeddings. Both embeddings are then fed to two emotion classifiers, which consist of intermediate fully connected (FC) layers. The output of the second FC layer from both classifiers is considered as the emotion embedding.
Apart from the loss of the classifiers, an additional loss function is established between the resulting emotion embeddings from the two classifiers. Similarly, an emotion classifier is also employed in [36] to reduce irrelevant information in the generated emotion embedding from an emotion encoder with reference speech (Mel-spectrogram) as input.
> 文献 [32] 使用情感分类器用于产生更具有区分性得情感嵌入. 首先从参考风格音频的输入梅尔频谱特征和 TTS 模型的相应预测传递到两个参考编码器中以生成参考嵌入. 两个嵌入之后都输入到两个情感分类器中, 由中间全连接层组成. 分类器的第二个全连接层的输出被视为情感嵌入. 除了分类器的损失之外, 还建立了两个分类器产生的情感嵌入结果之间的附加损失函数. 
> 文献 [36] 使用了情感分类器用于减少带有参考语音 (梅尔频谱) 的情感编码器生成的情感嵌入的无关信息.

Several other studies [34] [36] [39] that support multiple speakers also suggest utilizing a speaker classifier in addition to the emotion classifier. This approach aims to improved the speaker embedding derived from speaker encoders. Moreover, these studies introduce an adversarial loss between the speaker encoder and the emotion classifier using a gradient reversal layer (GRL) [90].The purpose of this is to minimize the potential transfer of emotion-related information into the speaker embedding. The GRL technique involves updating the weights of the speaker encoder by utilizing the inverse of the gradient values obtained from the emotion classifier during the training process.
> 文献 [34] [36] [39] 支持多说话人的研究也建议使用说话人分类器以及情感分类器. 这一方法旨在改善说话人编码器导出的说话人嵌入. 此外这些研究通过使用一个 GRL 引入了说话人编码器和情感分类器的对抗损失. 目的是最小化情感相关的信息转移到说话人嵌入中. GRL 技术涉及在训练过程中使用从情感分类器获得的梯度值的逆来更新说话人编码器的权重.

## 4.无监督学习方法

> Due to the limited availability and challenges associated with collecting or preparing labeled datasets of expressive speech, as discussed in Section 6, many researchers tend to resort to unsupervised approaches for generating expressive speech. Within these approaches, models are trained to extract speaking styles or emotions from expressive speech data through unsupervised methods.
> Unsupervised models typically utilize reference speech as an input to the TTS model, which extracts a style or prosody embedding which is then used to synthesize speech resembling the input style reference. In the literature, three primary structures emerge as baseline models for unsupervised ETTS models: including reference encoders, global style tokens, and variational autoencoders, which are explained in the following three sections. In addition, we identify the recent TTS models that utilize in-context learning as another group of unsupervised approaches. The last subcategory under the unsupervised approaches involves other individual approaches. We then provide a general summary of all the unsupervised approaches reviewed in this work in Table 3.

由于表达性语音的标注数据集的收集或准备相关的有限可用性和挑战, 许多研究人员倾向于采用无监督方法用于生成表达性语音. 在这些方法中, 模型通过无监督方法被训练用于从表达性语音数据中提取说话风格或情感. 无监督模型通常使用参考语音作为输入传递给 TTS 模型, 该模型提取风格或韵律嵌入, 之后用于合成类似于输入风格参考的语音. 
现有的文献中出现了三个主要结构作为无监督 ETTS 模型的基线模型: Reference Encoders, Global Style Tokens, VAEs.
此外我们认为近期使用上下文学习的 TTS 模型为其他一组无监督学习方法.最后一个子类别还涉及到其他个别方法.
我们在表格三种提供了本文回顾的所有无监督方法的一般性总结.

|序号|组别|TTS 模型|韵律级别|


### 4.1 Direct Reference Encoding 

> The main approach, based on a reference or prosody encoder, can be traced back to an early Google paper[74]. The paper suggests using a reference encoder to produce a low-dimensional embedding for a given style reference audio, which is called a prosody embedding.
> This encoder takes spectrograms as input to represent the reference audio. The generated prosody embedding is then concatenated with the text embedding derived from the text encoder of a Seq2Seq TTS model such as [Tacotron](2017.03_Tacotron.md), [Tacotron2](2017.12_Tacotron2.md). Figure 6 shows reference encoder integrated to the TTS model.

基于参考或韵律编码器的主要方法可以回溯到 Google 的一篇论文 [74] Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron. 该文献建议使用参考编码器以为给定风格参考音频生成低维嵌入, 称为韵律嵌入. 这一编码器将频谱作为输入来表示参考音频. 生成的韵律嵌入会与 Seq2Seq TTS 模型如 Tacotron 的文本编码器导出的文本嵌入相拼接. 图六展示了参考编码器集成到 TTS 模型.

> Various features have been employed in the literature as inputs for the reference encoder. For example, in the work [85], MFCC features extracted using the openSMILE toolkit [139] are fed into one of the encoders within its style extraction model, which is composed of a multi-modal dual recurrent encoder (MDRE). In another study [31], the reference encoder is proposed as a ranking function model, aimed at learning emotion strength at the phoneme level. This model leverages the OpenSMILE toolkit to extract 384-dimensional emotion-related features from segments of reference audio, derived using a forced alignment model for phoneme boundaries. Furthermore, in work [63], a word-level prosody embedding is generated. This is achieved by extracting phoneme-level F0 features from reference speech using the WORLD vocoder [140] and an internal aligner operating with the input text.

文献中已经使用了各种特征作为参考编码器的输入.
- 文献 [85] 使用 OpenSMILE 工具箱提取 MFCC 特征被输入到其风格提取模型的一个编码器中. 该模型有一个多模态对偶循环编码器组成.
- 文献 [31] 参考编码器被作为一个排序函数模型, 旨在音素级别学习情感强度. 这个模型利用 OpenSMILE 工具箱从参考音频片段中提取和情感相关的 384 维特征, 这些片段是对音素边界采用强制对齐模型获得的.
- 文献 [63] 生成了基于单词级别的韵律嵌入. 这是通过使用 WORLD 声码器和与输入问题一起操作的内置对齐器从参考音频中提取音素级别的 F0 特征来实现的.

> A prosody-aware module is proposed in [37] which extracts other prosody-related features. The prosody-aware module consists of an encoder, an extractor, and a predictor. The encoder receives the three phoneme-level features including logarithmic fundamental frequency(LF0), intensity, and duration from the extractor as input and generates the paragraph prosody embedding with the assistance of an attention unit. Simultaneously, the predictor is trained to predict these features at inference time based on the input text embedding only.

文献 [37] 提出了韵律感知模块用于提取其他韵律相关特征. 该模块由一个编码器, 一个提取器和一个预测器组成. 编码器接收来自提取器的三个音素级别特征包括对数基频 LF0, 强度和时长作为输入, 并借助注意力单元生成段落韵律嵌入. 同时预测器被训练用于在推理时仅输入文本嵌入来预测这些特征.

> In Daft-Exprt TTS model [118], the prosody encoder receives pitch, energy and spectrogram as input. The prosody encoder then uses FiLM conditioning layers[141] to carry out affine transformations to the intermediate features of specific layers in the TTS model. A slightly modified version of the [FastSpeech2](../../Models/AV1_Acoustic/2020.06_FastSpeech2.md) model is utilized in this work where the phoneme encoder,prosody predictor and the decoder are the conditioned components. The prosody predictor is similar to the variance adaptor of [FastSpeech2](../../Models/AV1_Acoustic/2020.06_FastSpeech2.md) but without the length regulator, and it estimates pitch, energy and duration at phoneme-level.

在 Draft-Exprt TTS 模型中, 韵律编码器接受音高, 能量和频谱作为输入. 然后韵律编码器使用 FiLM 条件层对 TTS 模型的特定层的中间特征执行仿射变换. 这项工作使用了 FastSpeech2 的稍微修改版本, 其中音素编码器, 韵律预测器和解码器都是条件化组件. 韵律预测器和 FastSpeech2 的方差适配器相似但没有长度调节器, 且其在音素水平估计音高, 能量和时长.

> A pre-trained Wav2Vec model [142] has also been utilized for extracting features from the reference waveform.

文献 [142] 采用预训练 Wav2Vec 模型用于从参考波形中提取特征. 

> These features serve as input to the reference encoders of the proposed [Emo-VITS](../2022.11_Emo-VITS/2022.11_Emo-VITS.md) model, which integrates an emotion network into the VITS model [143] to enhance expressive speech synthesis. In fact, the emotion network in the Emo-VITS model comprises two reference encoders. The resulting emotion embeddings from these encoders are then combined through a feature fusion module that employs an attention mechanism. Wav2vec2.0-derived features from the reference waveform in this work are particularly suitable for attention-based fusion and contribute to reducing the textual content within the resulting embeddings.

Emo-VITS 将这些特征作为 Emo-VITS 的参考编码器的输入, 它将情感网络集成到 VITS 模型中用于增强表达性语音合成. 实际上, 情感网络包含了两个参考编码器. 这些编码器的情感嵌入输出之后通过一个特征融合模块进行结合然后应用注意力机制. 由 Wave2Vec 2.0 从参考音频导出的特征尤其适合基于注意力的融合, 且有助于在结果嵌入中减少文本内容.

> In contrast, [60] proposes a an image style transfer module to generate input for reference encoder. The concept of image style transfer involves altering the artistic style of an image from one domain to another while retaining the image’s original content [144]. In specific research, the style reconstruction module from VGG-19[145], a deep neural network primarily used for image classification, is employed to extract style-related information from the Mel-spectrogram used as input image. Subsequently, the output of this module is fed into the reference encoder to generate the style embedding.

文献 [60] 提出了一种图像风格转换模块用于生成参考编码器的输入. 图像风格迁移的概念涉及到图像的艺术风格从一个领域的转化到另一个领域, 同时保留图像原始内容. 具体而言来自 VGG-19 的风格重构模块, 一个主要用于图像分类的神经网络, 将梅尔频谱视为输入图像从中提取和风格相关的信息. 之后这个模块的输出传递到参考编码器中以生成风格嵌入.

### 4.2 Latent Features via Variational Auto‑Encoders 通过 VAE 获取隐特征

> The goal of TTS models under this is to map input speech from the higher dimensional space to a well-organized and lower-dimensional latent space utilizing variational auto-encoders (VAEs) [146]. VAE is a generative model that is trained to learn the mapping between observed data x and continuous random vectors z in an unsupervised manner. In detail, VAEs learn a Gaussian distribution denoted as the latent space from which the latent vectors representing the given data x can be sampled. A typical variational autoencoder consists of two components. First, the encoder learns the parameters of the z vectors (latent distribution), namely the mean $\mu(x)$ and variance $\sigma^2(x)$, based on the input data x. Second,the decoder regenerates the input data x based on latent vectors z sampled from the distribution learned by the encoder. In addition to the reconstruction loss between the model input and the data, variational autoencoders are also trained to minimize a latent loss, which ensures that the latent space follows a Gaussian distribution.

在这类 TTS 模型中, 目标是利用 VAE 将来自于高维空间的语音映射到组织良好且维度较低的隐空间. VAE 是一种生成模型, 通过无监督学习的方式学习观察数据 $x$ 和连续随机向量 $z$ 之间的映射. 具体来说, VAE 学习一个记为隐空间的高斯分布, 从中可以采样到能表示给定数据 $x$ 的隐向量. 一个典型的变分自编码器由两部分组成: 一是编码器基于输入数据学习 $z$ 向量即隐分布的参数: 均值和方差, 二是解码器基于解码器学习到的分布中采样的隐变量 $z$ 重新生成输入数据. 除了模型输出和数据之间的重构损失, 变分自编码器还需要最小化一个潜在损失, 使得隐空间服从高斯分布.

> Utilizing VAEs in expressive TTS models as shown by Fig. 7, allows for mapping the various speech styles within the given dataset to be encoded as latent vectors,often referred to as prosody vectors, within this latent space. During inference, these latent vectors can be sampled directly or with the guidance of reference audio from the VAE’s latent space. Furthermore, the latent vectors offer the advantage of disentangling prosody features,meaning that some specific dimensions of these vectors independently represent single prosody features such as pitch variation or speaking rate. Disentangled prosody features allow for better prosody control via manipulating the latent vectors with different operations such as interpolation and scaling [77]. 

在表达性 TTS 中使用 VAEs 如图七所示, 允许将给定数据集中各种语音风格编码为隐变量, 通常称为韵律向量. 在推理时这些隐变量可以从隐空间直接采样或在参考音频的指导下采样. 此外, 隐变量还提供了分离韵律特征的优势, 意味着这些向量某些特定维度独立地表示单个韵律特征, 如音高变化或语速. 分离的韵律特征通过某些操作以进行更好的韵律控制, 如插值和缩放.

> The two early papers, [76] [77], can be regarded as the baseline for latent feature-based approaches. The former study [76] introduces VAE within the VoiceLoop model [147], while the latter [77]incorporates VAE into [Tacotron2](2017.12_Tacotron2.md) as an end-to-end TTS model for expressive speech synthesis.

文献 [76] [77] 可以视为基于隐特征方法的基线模型. [76] 将 VAE 引入 VoiceLoop 模型, [77] 将 VAE 集成到 Tacotron2 中作为表达性语音合成的端到端 TTS 模型.

> In the same direction of modeling the variation of the prosodic features in expressive speech, studies [109] [110] propose a hierarchical structure for the baseline variational autoencoder, known as Clockwork Hierarchical Variational AutoEncoder (CHiVE). Both the encoder and decoder in the CHiVE model have several layers to capture prosody at different levels based on the input text’s hierarchical structure. Accordingly, linguistic features are also used alongside acoustic features as input to the model’s encoder. The model’s layers are dynamically clocked at specific rates: sentence, words, syllables, and phones. The encoder hierarchy goes from syllables to the sentence level, while the decoder hierarchy is in the reversed order.

表达性语音中建模韵律特征变化的方面, 文献 [109], [110] 为基线 VAE 提出了一个层次结构, 即 CHiVE. 编码器和解码器都有数层, 基于输入文本的层次结构在不同级别捕获韵律. 因此除了声学特征之外, 语言特征也作为模型编码器的输入. 模型的层以特定的速率动态计时: 句子, 单词, 音节和音素. 编码器的层次结构从音节到句子, 而解码器则相反;

> The CHiVE-BERT model in [110], differs from the main model in [109] as it utilizes BERT [148] features for input text at the word-level. Since the features extracted by the BERT model incorporate both syntactic and semantic information from a large language model, CHiVE-BERT model is expected to have improved the prosody generation.

ChiVE-BERT 模型使用 BERT 特征作为单词级别的输入文本. 由于 BERT 模型提取的特征包含大型语言模型的语法和语义信息, 所以 CHiVE-BERT 模型预计将提高韵律的生成.

> Other studies [DiffProsody](../2023.07_DiffProsody/2023.07_DiffProsody.md) [53] propose Vector-Quantized Variational Auto-Encoder (VQ-VAE) to achieve discretized latent prosody vectors. In vector quantization(VQ) [149], latent representations are mapped from the prosody latent space to a codebook of a limited number of prosody codes. Specifically, during training, the nearest neighbor lookup algorithm is applied to find the nearest codebook vector to the output of the reference encoder and used to condition TTS decoder. 

DiffProsody [53] 提出矢量量化 VAE 用于实现离散的隐韵律向量. 在 VQ 中, 隐表示从韵律潜在空间映射到有限数量的韵律代码的码本. 特别地在训练时, 最近邻查找算法应用于查找参考编码器输出最近的码本向量, 并用于条件 TTS 模型. 

> To further improve the quality of latent prosody vectors and consequently the expressiveness of the generated speech, [DiffProsody](../2023.07_DiffProsody/2023.07_DiffProsody.md) proposes a diffusion-based VQ-VAE model.

为了进一步提高隐韵律向量的质量和后续生成语音的表达性, Diff-Prosody 提出了一种基于扩散的 VQ-VAE 模型.

> In the proposed model a prosody generator that utilizes a denoising diffusion generative adversarial networks (DDGANs) [150] is trained to generate the prosody latent vectors based only on text and speaker information. At inference time, the prosody generator is used to produce prosody vectors based on input text and with no need for an audio reference which improves both quality and speed of speech synthesis.

在提出的模型中, 一个韵律生成器使用去噪扩散对抗生成模型仅使用文本和说话人信息生成韵律隐变量. 在推理时, 韵律生成器用于基于输入文本的韵律向量而无需音频参考, 从而提升语音合成的质量和速度.

> While most of the studies in this category follow the baseline model and use mel-spectrograms to represent the reference audio, other studies extract correlated prosody features as input to the VAE. For instance, frame-level F0, energy, and duration features are extracted from the reference speech as basic input for the hierarchical encoder of the CHiVE model [109]. These same features are also used as input for the VAE encoder in work [35], but at the phoneme level. In work [68], multi-resolution VAEs are employed, each with acoustic and linguistic input vectors. The acoustic feature vectors for each encoder include 70 mel-cepstral coefficients, log F0value, a voiced/unvoiced value, and 35 mel-cepstral analysis aperiodicity measures.

此类的大部分研究都遵循基线模型并使用梅尔频谱表示参考音频, 其他研究提取相关韵律特征作为 VAE 的输入. 例如 帧级别的 F0, 能量, 时长特征从参考音频中提取, 作为 ChiVE 模型层次编码器的输入. 这些相同特征同样在文献 [35] 中使用, 但是是音素级别.

文献 [68] 采用了多分辨率 VAEs, 每个都有声学和语言输入向量. 声学特征包括 70 个梅尔频谱系数, 对数 F0 值, 有声/无声值和 35 个 mel-cepstral 分析非周期度量.

### 4.3 Global Style Tokens 

> The Global Style Tokens (GST) approach for expressive synthesis was first introduced in [75]. The paper proposes a framework to learn various speaking styles (referred to as style tokens) in an unsupervised manner within an end-to-end TTS model. The proposed approach can be seen as a soft clustering method that learns soft style clusters for expressive styles in an unlabeled dataset. In detail, GST, as shown by Fig. 8, extends the approach introduced in [74] by passing the resulting style embedding from the reference encoder to an attention unit,which functions as a similarity measure between the style embedding and a bank of randomly initialized tokens.
> During training, the model learns the style tokens and a set of weights, where each style embedding is generated via a weighted sum of the learned tokens. In fact, the obtained weights represent how each token contributes to the final style embedding. Therefore, each token will represent a single style or a single prosody-related feature, such as pitch, intensity, or speaking rate.
> At inference time, a reference audio can be passed to the model to generate its corresponding style embedding via a weighted sum of the style tokens. Alternatively, each individual style token can be used as a style embedding.
> In addition, GSTs offer an enhanced control over the speaking style through various operations. These include manual weight refinement, token scaling with different values, or the ability to condition different parts of the input text with distinct style tokens.

用于表达性语音合成的全局风格标记方法在文献 [75] 中被首次提出. 该文献提出了一个框架用于端到端 TTS 模型中以无监督学习的方式用于学习各种说话风格 (称为风格标记). 该方法可以视为一种软聚类方法用于无标签数据集中学习软风格簇. 具体地 GST 如图八所示, 将文献 [74] 的方法进行了扩展, 将参考编码器输出的风格嵌入传递给注意力单元, 作为风格嵌入和一组随机初始标记的相似性度量. 在训练时, 模型学习风格标记和一组权重, 其中每个风格嵌入可以通过学习到的标记进行加权和获得. 实际上, 获得的权重表示每个标记对于最终风格嵌入的贡献. 因此每个标记将表示单个风格或单个韵律相关的特征, 例如音高, 强度或语速. 在推理时, 参考音频传递给模型通过风格标记的加权和用于生成对应风格嵌入. 或者每个单独的风格标记作为风格嵌入.
此外 GSTs 提供了通过各种操作增强对说话风格的控制, 包括手动权重细化, 标记按不同值缩放或使用不同风格标记条件话输入文本的不同部分.

> The GST-TTS model can be further enhanced by modeling different levels of prosody to improve both expressiveness and control over the generated speech. For instance, [46] proposes a fine-grained GST-TTS model where word-level GSTs are generated to capture local style variations (WSVs) through a prosody extractor. The WSV extractor consists of a reference encoder and a style token layer, as described in [75], along with an attention unit to produce the word-level style token.

> In [133] a hierarchical structure of multi-layer GSTs with residuals is proposed. The model employs three GST layers, each with 10 tokens, resulting in a better interpretation of the tokens of each level. Upon tokens analysis, it was found that the first-layer tokens learned speaker representations, while the second-layer tokens captured various speaking style features such as pause position, duration, and stress. The third-layer tokens, however, were able to generate higher-quality samples with more distinct and interpretable styles. Similarly, in[50], a multi-scale GST extractor is proposed to extract speaking style at different levels. This extractor extracts style embeddings from the reference mel-spectrogram using three style encoders at global, sentence, and sub word levels, and combines their outputs to form the multi-scale style embedding.

GST-TTS 模型能够通过建模不同级别的韵律来提升生成语音的表达性和控制.

- 文献 [46] 提出了细粒度 GST-TTS 模型, 通过韵律提取器生成了词级别的 GSTs 以捕获局部风格变化 (WSVs). WSV 提取器由参考编码器和风格标记层组成, 如文献 [75] 所述, 以及用于生成单词级别的注意力单元.
- 文献 [133] 具有残差的多层 GSTs 的层次结构, 应用三层 GST 层, 每个有 10 个标记, 从而得到各个级别标记的更佳解释. 标记分析发现第一层标记学习到说话人表示, 第二层捕获了各种说话风格例如停顿位置, 时长和强调. 第三层标题能够生成高质量的样本, 具有更明显和可解释的风格. 
- 类似地 [50], 一个多尺度 GST 提取器用于提取不同级别的说话风格, 该提取器使用三种风格编码器按全局, 句子和单词级别从参考梅尔频谱中提取风格嵌入, 并将它们的输出结合以形成多尺度的风格嵌入.

> With only a small portion of the training dataset labeled with emotions, **"End-to-End Emotional Speech Synthesis Using Style Tokens  and Semi-Supervised Training"** proposes a semi-supervised GST model for generating emotional speech. The model applies a cross-entropy loss between the one-hot vectors representing the emotion labels and the weights of GSTs,in addition to the GST-TTS reconstruction loss. The semi-GST model is trained on a dataset in which only 5%of the samples are labeled with emotion classes, while the rest of the dataset is unlabeled. After training, each style token represents a specific emotion class from the training dataset and can be used to generate speech in the corresponding emotion.

当训练集只有一小部分带有情感标签时, 文献 [026] 提出了一种半监督的 GST 模型用于生成情感语音. 该模型应用表示情感标签的独热编码和 GSTs 的权重之间的交叉熵损失, 以及 GST-TTS 重构损失. 这个 semi-GST 模型在训练集只有 5% 的样本具有情感类别的情况下训练, 训练后每个风格标记表示训练集中一个特定的情感类别并且能用于对应情感生成语音.

> Furthermore, in [92], a speech emotion recognition(SER) model is proposed with the GST-TTS to generate emotional speech while acquiring only a small labeled dataset for training. The paper formulates the training process as reinforcement learning (RL). In this frame-work, the GST-TTS model is treated as the agent, and its parameters serve as the policy. The policy aims to predict the emotional acoustic features at each time step, where these features represent the actions. The pre-trained SER model then provides feedback on the predicted features through emotion recognition accuracy, which represents the reward. The policy gradient strategy is employed to perform backpropagation and optimize the TTS model to achieve the maximum reward.

文献 [92] 一个语音情感识别 SER 模型被提出和 GST-TTS 模型结合用于生成情感语音, 只需要很小部分带标签的数据集用于训练. 该文献将训练过程形式化为强化学习. 在此架构下, GST-TTS 模型视为智能体, 其参数作为策略. 策略旨在预测每个时间步的情感声学特征, 这些特征表示动作. 预训练 SER 模型通过情感识别精度提供反馈, 即奖励. 策略梯度策略用于优化 TTS 模型以达到最大奖励.

> In contrast, the Mellotron model [114] introduces a unique structure for the GSTs, enabling Mellotron to generate speech in various styles, including singing styles, based on pitch and duration information extracted from the reference audio. This is achieved by obtaining a set of explicit and latent variables from the reference audio. Explicit variables (text, speaker, and F0contour) capture explicit audio information, while latent variables (style tokens and attention maps) capture the latent characteristics of speech that are hard to extract explicitly.

文献 [114] Mellotron 引入 GSTs 独特结构, 使得 Mellotron 能够基于从参考音频中提取的音高和时长信息生成各种风格的语音, 包括歌唱风格. 这通过从参考音频中获取显式和隐式变量实现. 显式变量捕获显式音频信息, 隐式变量捕获语音中难以显式提取的隐藏特征.

### 4.4.基于上下文学习的方法

> These is a group of recent TTS models that are trained on a large amounts of data using in-context learning strategy. During in-context learning (also called prompt engineering), the model is trained to predict missing data based its context. In other words, the model is trained with a list of input-output pairs formed in a way that represents the in-context learning task. After training, the model should be able to predict the output based on a given input.

近期有一组 TTS 模型通过上下文学习策略在大量数据上进行训练. 在上下文学习 (或提示工程) 中, 模型被训练用于基于上下文预测缺失数据. 换句话说模型通过表示上下文学习任务的输入输出对列表进行训练, 训练后模型能够预测给定输入的输出.

> For the TTS task, the provided style reference (referred to as prompt) is considered as part of the entire utterance to be synthesized. The TTS model training task is to generate the rest of this utterance following the style of the provided prompt as shown by Fig. 9. By employing this training strategy, recent TTS models such as [VALL-E (2023)](../2023.01_VALL-E/2023.01_VALL-E.md), [NaturalSpeech2 (2022)](../2022.03_NaturalSpeech/2023.04_NaturalSpeech2.md), and [Voicebox (2023)](../2023.06_VoiceBox/2023.06_VoiceBox.md) are capable of producing zero-shot speech synthesis using only a single acoustic prompt. Furthermore, these models demonstrate the ability to replicate speech style/emotion from a provided prompt ([NaturalSpeech2 (2022)](../2022.03_NaturalSpeech/2023.04_NaturalSpeech2.md), [VALL-E (2023)](../2023.01_VALL-E/2023.01_VALL-E.md)) or reference ([Voicebox (2023)](../2023.06_VoiceBox/2023.06_VoiceBox.md)) to the synthesized speech.

对于 TTS 任务, 提供的风格参考 (即提示) 被考虑为要合成的整个发言的一部分. TTS 模型训练任务即遵循提示的风格生成这个发言剩下的部分. 通过应用这种训练策略, 近期 TTS 模型例如 VALL-E, NaturalSpeech2 和 VoiceBox 能够使用单个声学提示进行零次语音合成. 此外, 这些模型说明了从提供的提示或参考复制语音风格/情感到合成语音的能力.

> In [VALL-E (2023)](../2023.01_VALL-E/2023.01_VALL-E.md), a language model is trained on tokens from [Encodec (2022)](../../Models/LLM3_Codec2Wav/2022.10_EnCodec.md), and the input text is used to condi-tion the language model. Specifically, the Encodec model tokenizes audio frames into discrete latent vectors/codes,where each audio frame is encoded with eight codebooks.
> VALL-E employs two main models: the first one is an auto-regressive (AR) model that predicts the first code of each frame, and the second is non-auto-regressive (NAR)model that predicts the other seven codes of the frame.

VALL-E 一个语言模型在 Encodec 的标记上训练, 且输入文本用于条件化语言模型. 特别地, Encodec 模型将音频帧离散化为离散的隐向量/代码, 每个音频帧由八个码本进行编码. VALL-E 应用两个主要模型一个是自回归模型能够预测每帧的第一个编码, 第二个是非自回归模型用于预测其他七个编码.

Instead of discrete tokens used in VALL-E, [NaturalSpeech2 (2022)](../2022.03_NaturalSpeech/2023.04_NaturalSpeech2.md) represents speech as latent vectors from a neural audio codec with residual vector quantizers. The latent vectors are then predicted via a diffusion model,conditioned on input text, pitch from a pitch predictor,and input speech prompt.
> 和 VALL-E 不同, NaturalSpeech 2 使用具有残差向量量化的神经音频编解码器将语音表示为隐向量. 隐向量之后通过扩散模型进行预测, 根据输入问题, 音高预测器的音高和输入语音提示进行条件化.

> Another example of in-context training is [Voicebox (2023)](../2023.06_VoiceBox/2023.06_VoiceBox.md) which is a versatile generative model for speech trained on a large amount of multilingual speech data. The model is trained on a text-guided speech infilling task, which gives it the flexibility to perform various speech tasks such as zero-shot TTS, noise removal, content editing,and diverse speech sampling. Voicebox is modeled as a non-autoregressive (NAR) flow-matching model with the ability to consider future context.

另一个基于上下文训练的例子是 Voicebox, 是一个在大量多语言语音数据上训练的语音的通用生成模型. 该模型在文本引导的语音填充任务上进行训练, 这使其能够执行各种语音任务, 例如零次 TTS, 去噪, 内容编辑和多样化的语音采样. Voicebox 被建模为一个非自回归的流匹配模型, 能够考虑未来上下文.


### 4.5 Other Approaches 其他方法

> This category containes reviewed papers that propose individual techniques or methods which cannot be categorized under any of the previously mentioned unsupervised approaches. 

这个类别包含了一些不能归类为之前提到的任何无监督方法的单独技术和方法的总结.

> For instance, in [121], a neural encoder is introduced to encode the residual error between the predictions of a trained average TTS model and the ground truth speech. The encoded error is then used as a style embedding that conditions the decoder of the TTS model to guide the synthesis process. 

文献 [121] 引入神经编码器来编码一个训练好的平均 TTS 模型预测和真实语音之间的残差误差. 然后编码的误差被用作风格嵌入用于条件化 TTS 模型的解码器以指导合成过程.

> Raitio and Seshadri [128] improves prosody modeling of [FastSpeech2](../../Models/AV1_Acoustic/2020.06_FastSpeech2.md) model with an additional variance adaptor for utterance-wise prosody modeling. 

文献 [128] 通过使用额外的方差适配器用于语调韵律建模, 以提升 FastSpeech2 模型的韵律建模.

> As context information is strongly related to speech expressivity, [45] proposes using multiple self-attention layers in [Tacotron2](../../Models/AV1_Acoustic/2017.12_Tacotron2.md) encoder to better capture the con-text information in the input text. The outputs of these layers in the encoder are combined through either direct aggregation (concatenation) or weighted aggregation using a multi-head attention layer. 

由于上下文信息和语音表达性强相关, 文献 [45] 在 Tacotron 编码器中使用多个自注意力层用于更好地捕获输入文本的内容信息. 这些层的输出通过直接聚合 (拼接) 或加权聚合 (多头注意力层) 进行结合.

> Additionally, there are some papers that propose using only input text to obtain prosody-related representations/embeddings without any style references, and those are further discussed in Section 5.2.4.

此外, 有些文献提出只使用输入文本用于获得韵律相关表示/嵌入, 无需风格参考, 这在后续的 5.2.4 中进行讨论.

## 5.表现性语音合成的主要挑战

> In this section, we list and explain the most important challenges that face expressive TTS models and the main solutions that have been proposed in the literature to overcome these challenges. We then provide a summary of papers addressing each challenge in [Table 5]().

本节列出并展示表现性语音合成模型面临的最重要挑战, 以及在现有文献中提出的克服这些挑战的主要解决方案. 在表格五种提供了解决每个挑战的文献的总览.

|参考文献|信息泄露|缺少参考音频|韵律可控性|未知风格/说话人|
|---|:-:|:-:|:-:|:-:|
||√|√|√|√|
||√|√||√|
|[097](#K.Lee2021)|√||√|√|
||√|||√|
|[102](#K.Zhang2022)|√||||
|||√|√||
|[047](#C.Lu2021) [111](#D.Tan2020)|√|√|||
|[019](#S.Jo2023)|√||√||
||||√||
|||√|||


### 5.1.无关信息泄露 (Irrelevant Information Leakage)

> One main problem in unsupervised approaches that rely on having a style reference or a prompt, is the leakage of irrelevant information, like speaker or text related information, into the generated style or prosody embedding. 
> This irrelevant information within the speech style can lead to degradation in the quality of the synthesized speech. 
> As a result, many studies have investigated this problem, and several solutions have been proposed as outlined below.

在依赖风格参考或提示的无监督学习方法中, 一个主要的问题是无关信息的泄露, 如说话人或文本相关的信息进入到生成的风格或韵律嵌入.
这种语音风格中的无关信息可能导致合成语音的质量下降.
因此许多文献研究了这一问题并提出了以下几种方案.

#### 5.1.1.对抗性训练 (Adversarial Training)

> [Adversarial training](#Y.Ganin2016) is one of the widely used techniques to confront the information leakage problem.
> Typically, a classifier is trained to distinguish the type of unwanted information (such as speaker or content information) that is leaking from the prosody reference audio into the generated prosody embedding. 
> During the training process, the weights of the employed prosody encoder/extractor from the reference audio are modified with gradient inversion of the proposed classifier. 
> In other words, the classifier penalizes the prosody encoder/extractor for any undesired information in its output. 
> A **Gradient Reversal Layer (GRL)** is usually used to achieve the inversion of the classifier gradients.

对抗训练是广泛用于处理信息泄露的一种技术.
通常, 训练一个分类器用于区分从韵律参考音频泄露到生成的韵律嵌入中的不需要的信息类型 (例如说话人或内容信息).
在训练过程中, 参考音频中使用的韵律编码器或提取器的权重被分类器的梯度反转修改.
换句话说, 分类器在它的输出中对任何不需要的信息惩罚韵律编码器或提取器.
通常使用**梯度反转层 (Gradient Reversal Layer, GRL)** 来获得分类器梯度的反转.

> Several studies utilize adversarial training to prevent the flow of either speaker or content-related information from the given reference audio to the resulting prosody embedding. 
> For instance, the [VAE-TTS model](#C.Lu2021) learns phoneme-level 3-dimensional prosody codes. 
> The VAE is conditioned on speaker and emotion embeddings, besides the tone sequence and mel-spectrogram from the reference audio. 
> Adversarial training using a **Gradient Reversal Layer (GRL)** is applied to disentangle speaker and tone from the resulting prosody codes.
> Similarly, adversarial training is introduced to the style encoder of the [cross-speaker emotion transfer model](#S.Jo2023) to learn a speaker-independent style embedding, where the target speaker embedding is provided from a separate speaker encoder.

有几项研究利用对抗训练来防止说话人或内容相关信息从给定参考音频流动到生成韵律嵌入.
例如, VAE-TTS 模型学习音素级别的三维韵律编码.
用说话人和情感嵌入, 参考音频的语调序列和梅尔频谱条件化 VAE.
采用**梯度反转层**的对抗学习用于解耦韵律编码中的说话人和语调.
类似地, 跨说话人情感转移模型中的风格编码器也引入了对抗训练用于学习说话人独立的风格嵌入, 其中目标说话人嵌入由单独的说话人编码器提供.

> The [STYLER model](#K.Lee2021) employs multiple style encoders to decompose the style reference into several components, including duration, pitch, speaker, energy, and noise. 
> Both channel-wise and frame-wise bottleneck layers are added to all the style encoders to eliminate content-related information from the resulting embeddings. 
> Furthermore, as noise is encoded individually by a separate encoder in the model, other encoders are constrained to exclude noise information by employing either domain adversarial training or residual decoding.

在 STYLE 模型中使用了多个风格编码器将风格参考分解为多个成分, 包括时长, 音高, 能量和噪声.
在所有的风格编码器中添加通道级和帧级瓶颈层从导出的嵌入中排除内容相关的信息.
此外, 由于噪声在模型中通过单独的编码器编码, 通过应用领域对抗训练或残差解码来约束其他编码器以排除噪声信息.

> In [111](#D.Tan2020), prosody is modeled at the phone-level and utterance-level by two separate encoders. 
> The first encoder consists of two sub-encoders: a style encoder and a content encoder, besides two supporting classifiers. 
> The first classifier predicts phone identity based on the content embedding, while the other classifier makes the same prediction but based on the style embedding. 
> The content encoder is trained via collaborative training with the guidance of the first classifier, while adversarial training is used to train the style encoder, utilizing the second classifier.

文献 111 中韵律通过两个单独的编码器在音素级别和语调级别进行建模.
第一个编码器由两个子编码器组成: 一个风格编码器和一个内容编码器, 以及两个支持分类器.
首个分类器基于内容嵌入预测音素标识, 其他分类器则基于风格嵌入进行相同的预测.
结合第一个分类器的指导采用协作训练来训练内容编码器, 利用第二个分类器采用对抗训练来训练风格编码器.

> On the other hand, [102](#K.Zhang2022) proposes adversarial training for the style reference by inverting the gradient of an **Automatic Speech Recognition (ASR)** model. 
> The proposed model introduces a shared layer between an ASR and a reference encoder-based model. 
> Specifically, a single BiLSTM layer from the listener module of a pre-trained ASR model serves as the prior layer to the reference encoder. 
> The process starts by passing the reference Mel-spectrogram to the shared layer to produce the shared embedding as input to both the reference encoder and the ASR model. 
> A **Gradient Reversal Layer (GRL)** is employed by the ASR model to reverse its gradient on the shared layer. 
> Accordingly, the reference encoder parameters are modified so that the ASR model fails to recognize the shared embedding, and thus content leakage to the style embedding from the reference encoder is reduced.

另一方面, 文献 102 提出通过反转自动语音识别模型的梯度对风格参考进行对抗性训练.
该模型在 ASR 和基于参考编码器的模型之间引入了一个共享层.
具体地, 预训练 ASR 模型中听众模块中的单个 BiLSTM 层作为参考编码器的前一层.
将参考梅尔频谱传递到共享层用于生成共享嵌入, 作为参考编码器和 ASR 模型的输入.
ASR 模型通过使用梯度反转层来反转共享层的梯度.
因此, 参考编码器的参数被修改使得 ASR 模型无法识别共享嵌入, 从而减少从参考编码器到风格嵌入器的内容泄露.

#### 5.1.2.韵律分类器 (Prosody Classifiers)

> This is a supporting approach used by some studies to produce more discriminative prosody embeddings by passing them to a prosody classifier. 
> This method can be applied when the training data is labeled with emotion or style labels. 
> In the [two consecutive studies](#T.Li2022) [](#T.Li2021) from the same research group, an auxiliary reference encoder is proposed and located after the decoder of the [baseline TTS model](#R.Skerry2018). 
> The two reference encoders in the model are followed by emotion classifiers to further enhance the discriminative nature of their resulting embeddings.
> However, the emotion embedding that is passed to the TTS model is the output of an intermediate hidden layer of the classifiers. 
> In addition to the classification loss, an additional style loss is also applied between the two emotion embeddings from the two employed emotion classifiers.

这是某些研究中采用的支持方法, 通过将它们传递给韵律分类器用于产生更具区分性的韵律嵌入.
当训练数据带有情感或风格标签时可以采用此方法.
在同一研究小组的两项连续研究中, 给基线 TTS 模型的解码器后添加一个辅助的参考编码器.
模型中的两个参考编码器后跟着情感分类器用于进一步增强其生成嵌入的区分性.
然而传递给 TTS 模型的情感嵌入是分类器的中间隐藏层的输出.
除了分类器损失外, 还应用了来源于两个情感分类器的情感嵌入之间的附加风格损失.

> In [36], alongside the text encoder, two encoders are introduced to generate embeddings for speaker and emotion from a reference audio. 
> To further disentangle emotion, speaker, and text information, both speaker and emotion encoders are supported with a classifier to predict speaker and emotion labels, respectively. 
> Similarly, in paper [39], a model with two encoders and two classifiers is proposed to produce disentangled embeddings for speakers and emotions from a reference audio. 
> However, the paper claims that some emotional information is lost during the process of disentangling speaker identity from the emotion embedding. 
> As a result, an ASR model is introduced to compensate for the missing emotional information. 
> The emotion embedding is incorporated within a pre-trained ASR model through a **Global Context (GC)** block. 
> This block extracts global emotional features from the ASR model’s intermediate features (AIF). 
> Subsequently, a prosody compensation encoder is utilized to generate emotion compensation information from the output of the AIF layer, which is then added to the emotion encoder output.

在文献 [36] 中, 除了文本编码器之外, 还引入了两个编码器从参考音频中生成说话人和情感的嵌入.
为了进一步解耦情感, 说话人和文本信息, 说话人和情感编码器都用一个分类器进行支持, 分别用于预测说话人和情感标签.
类似地, 文献 [39] 中, 提出了一个具有两个编码器和两个分类器的模型以从参考音频中产生解耦的说话人和情感嵌入.
然而该文献声称再从情感嵌入中解耦说话人标识时丢失了一些情感信息.
因此引入了一个 ASR 模型用于补偿丢失的情感信息.
情感嵌入通过全局文本块被整合进预训练 ASR 模型中.
这个块从 ASR 模型的中间特征 (AIF) 中提取全局情感特征.
然后, 使用韵律补偿编码器从 AIF 层的输出中生成情感补偿信息, 之后加入到情感编码器的输出中.

#### 5.1.3.信息瓶颈 (Information Bottleneck)

> The information bottleneck is a technique used to control information flow via a single layer/network. 
> It helps prevent information leakage as it projects input into a lower dimension so that there is not enough capacity to model additional information and only important information is passed through it. 
> In other words, the bottleneck can be seen as a down-sampling and up-sampling filter that restricts its output and generates a pure style embedding. 
> Several prosody-reference based approaches, as in [86] [93] [97] [101] [130], have employed this technique to prevent the flow of speaker or content-related information from the reference audio to the prosody embedding.

信息瓶颈是一种通过单层或网络控制信息流的技术。
它有助于防止信息泄露，因为它将输入投射到较低维度从而没有足够的容量来建模额外信息，只有重要信息通过它。
换句话说，信息瓶颈可以视为一个下采样和上采样滤波器，它限制了输出并生成纯风格嵌入。
有几项基于韵律参考的研究已经应用了这一技术用于防止说话人或内容相关的信息从参考音频流入韵律嵌入。

> In [93], a bottleneck layer named sieve layer is introduced to the style encoder in GST-TTS to generate pure style embedding. 
> Similarly, in the multiple style encoders model STYLER [97], each encoder involves a channel-wise bottleneck block of two bidirectional-LSTM layers to eliminate content information from encoders’ output. 
> Another example is the cross-speaker-style transfer Transformer-TTS model proposed in [86] with both speaker and style embeddings as input to the model encoder. 
> The speaker-style-combined output from the encoder is then passed to a prosody bottleneck sub-network, which produces a prosody embedding that involves only prosody-related features. 
> The proposed bottleneck sub-network consists of two CNN layers, a squeeze-and-excitation (SE) block [152], and a linear layer. 
> The encoder output is then concatenated with the resulting prosody embedding and used as input to the decoder.

- [文献 093]() 在 GST-TTS 的风格编码器中引入了一个名为筛层 (Sieve Layer) 的瓶颈层用于生成纯风格嵌入.
- [文献 097]() 提出的多风格编码器模型 STYLER 中，每个编码器都包含两个双向 LSTM 层通道级别瓶颈块用于消除编码器输出中的内容信息.
- [文献 086]() 提出的跨说话人风格迁移 Transformer-TTS 模型中, 说话人和风格嵌入都作为模型编码器的输入. 编码器输出的说话人-风格组合被传递到韵律瓶颈自网络, 导出仅包含韵律相关特征的韵律嵌入. 之后将编码器的输出与产生的韵律嵌入进行拼接作为解码器的输入.

> The Copycat TTS model [130] is a prosody transfer model via VAE. 
> The model applies three techniques to disentangle the source speaker information from the prosody embedding. 
> One of these techniques is to use a temporal bottleneck encoder [153] within the reference encoder of the model. 
> The prosody embedding that is sampled from the latent space is passed to the bottleneck to reduce speaker identity-related information in the prosody embedding before it flows to the model decoder.
> Similarly, the model proposed in [101] produces a style embedding with less irrelevant style information by adding a variational information bottleneck (VIB) [154] layer to the reference encoder. 
> The idea behind this layer is to introduce a complexity constraint on mutual information(MI) between the reference encoder input and output so that it only flows out style-related information.

- [文献 130]() 提出的 Copycat TTS 是通过 VAE 进行韵律迁移的模型. 模型应用三种技术来从韵律嵌入中解耦源说话人信息. 其中之一是在模型的参考编码器中使用**时序瓶颈编码器** [文献 153](), 在传递给模型解码器之前, 从隐空间中采样的韵律嵌入被传递到瓶颈以减少其中和说话人身份相关的信息.
- [文献 101]() 通过给参考编码器添加变分信息瓶颈 (Variational Information Bottleneck, VIB) 层以生成具有更少风格无关信息的风格嵌入. 该层背后的思想是在参考编码器的输入和输出之间的互信息引入一个复杂度约束使其只流出风格相关的信息.

#### 5.1.4.实例归一化 (Instance Normalization)

> Batch normalization (BN), first introduced in [155], is utilized in deep neural networks to accelerate the training process and increase its stability. Essentially, a batch normalization layer is added before each layer in deep neural networks to adjust the means and variances of the layer inputs, as illustrated by Eq.(1):
> $$
>   IN(x) = \gamma \left[\dfrac{x-\mu(x)}{\sigma(x)}\right]+\beta, \tag{1}
> $$
> 
> where $\gamma$, $\beta$ are affine parameters learned from data and $\mu$, $\sigma$ are the mean and standard deviation which are calculated for each feature channel across the batch size.
> Instance normalization (IN) also follows equation (1); however, it calculates means and variances across spatial dimensions independently for each channel and each sample (instance). 
> In the field of computer vision, stylization approach is significantly improved by replacing (BN) layers with (IN) layers [156]. 
> Consequently, researchers in the expressive speech field have started to apply IN to extract better prosody representations. 
> For example, an instance normalization (IN) layer is used at the reference encoder in [130], at the prosody extractor in [93], and at the style encoder in [96] to remove style/prosody irrelevant features (such as speaker identity features) and enhance the learned style/prosody embedding.

**批量归一化 (Batch Normalization, BN)** 由[文献 155]() 首次提出, 用于加速深度神经网络的训练过程并提高其稳定性. 本质上, 在深度神经网络的每一层之前添加批量归一化层用于调整输入的均值和方差, 如[公式一]()所示.
其中 $\gamma$ 和 $\beta$ 是从数据中学习到的仿射参数, $\mu$ 和 $\sigma$ 是在整个批量大小上为每个特征通道计算的均值和方差.

**实例归一化 (Instance Normaliztion, IN)** 同样满足[公式一](), 然而它独立地为每个通道和每个样本 (又称实例) 在空间维度上计算均值和方差. 

在计算机视觉领域, 通过将批量归一化层替换为实例归一化层, 风格化方法得到了显著提升. 因此, 表达性语音领域的研究人员尝试应用实例归一化用于提取更好的韵律表示. 

例如以下研究都使用了实例归一化层以去除风格/韵律无关特征 (如说话人身份特征) 并增强学习到的风格/韵律嵌入.

- [文献 130]() 的参考编码器;
- [文献 093]() 的韵律提取器;
- [文献 096]() 的风格编码器.

#### 5.1.5.互信息最小化 (Mutual Information Minimization)

> For a pair of random variables, mutual information (MI)is defined as the information obtained on one random variable by observing the other. 
> Specifically, if $X$ and $Y$ are two variables, then $MI(X; Y)$ shown by Venn diagram in [Fig.10](#FIG10), can be seen as the KL-divergence between the joint distribution $(P_{XY})$ and the product of the marginals $(P_X, P_Y)$ as in [equation (2)](#EQ02). If the two random variables $X$ and $Y$ represent linguistic and style vectors, applying $MI$ minimization between these two vectors helps to produce style vectors with less information from the content vector.
> $$
>   MI(X;Y)=DL_{KL}(P_{(X,Y)}\| P_X\otimes P_Y),\tag{2}
> $$

> For example, in [137], the Mutual Information Neural Estimation algorithm (MINE) [157] is employed to estimate the mutual information between the content and style vectors. 
> The algorithm uses a neural network that is trained to maximize the lower bound of the mutual information between the style and content vectors. 
> Simultaneously, the TTS model aims to minimize the reconstruction loss, making the overall problem a max-min problem. 
> Alternatively, in [InstructTTS](../2023.01_InstructTTS/2023.01_InstructTTS.md), the CLUB method [158], which computes an upper bound as the MI estimator, is used to prevent the leakage of speaker and content information into the style embedding.

> A new approach is proposed in [117] for MI estimation and minimization to reduce content/speaker information transfer to the style embedding in a VAE based approach. 
> Typically, the model needs to estimate MI between latent style embeddings and speaker/content embeddings. 
> To avoid the exponentially high statistical variance of the finite-sampling MI estimator, the paper suggests using a new algorithm for information divergence named Rényi divergence. 
> Two variations from the Rényi divergence family are proposed, including minimizing the Hellinger distance and minimizing the sum of Rényi divergences.

对于一对随机变量, 互信息 (Mutual Information, MI) 被定义为通过观察另一个随机变量获得的当前随机变量的信息.
具体地, 若 $X$ 和 $Y$ 为两个变量, 那么 $MI(X;Y)$ 如图十显示的韦恩图所示, 可以视为联合概率分布 $P_{XY}$ 和边际概率分布 $(P_X,P_Y)$ 的乘积之间的 KL 散度. 若两个随机变量 $X$ 和 $Y$ 分别表示语言向量和风格向量, 对这两个向量应用互信息最小化将有助于生成具有更少来自内容向量的信息的风格向量.
$$
    MI(X;Y)=DL_{KL}(P_{(X,Y)}\| P_X\otimes P_Y),\tag{2}
$$

- 文献 [137] 将文献 [157] 的互信息网络估计算法 (Mutual Information Neural Estimation, MINE) 应用于估计内容向量和风格向量之间的互信息. 该算法使用一个神经网络, 被训练以最大化风格向量和内容向量之间互信息的下界. 同时, TTS 模型最小化重构损失, 使得整体变成了最大-最小问题.
- 文献 [021] 将文献 [158] 使用 CLUB 方法计算一个上界作为 MI 估计量, 用于防止说话人和内容信息泄露到风格嵌入中.
- 文献 [117] 提出了一种用于互信息估计的新方法并在基于 VAE 方法中最小化用于减少内容/说话人信息转移到风格嵌入中. 通常模型需要估计隐风格嵌入和说话人/内容嵌入之间的互信息. 为了避免有限采样互信息估计其的指数高统计方差, 作者建议使用一种新算法用于信息散度, 名为 Rényi 散度. Rényi 散度导出两种变体, 包括最小化 Hellinger 距离和最小化 Rényi 散度之和.

Fig. 10 Venn diagram of two random variables X and Y where P(X)and P(Y) represent their entropies, P(X|Y) is the conditional entropy of X given Y and P(Y|X) is the conditional entropy of Y given X, H(X,Y)is the joint entropy of X and Y and MI(X,Y) is their mutual information


#### 5.1.6.波形转向量特征 (Wav2Vec Features)

> Wav2Vec [142] model converts speech waveform into context-dependent vectors/features. The model is trained via self-supervised or in-context training algorithms which are explained in [Section 4.4](#Sec4.4.). Features generated by wav2vec and similar models such as HuBERT [159] provide better representations of speech and its lexical and non-lexical information. Therefore, these models are utilized nowadays in different speech processing tasks such as speech recognition, synthesis, and downstream emotion detection.

Wav2Vec 模型将语音波形转化为上下文相关的向量或特征. 该模型通过自监督或上下文训练算法进行训练. 由 Wav2Vec 和类似的模型如 HuBERT 生成的特征提供了更好的语音及其词汇和非词汇信息的表示. 因此这些模型现在被用于不同的语音处理任务例如语音识别, 合成和下游情感检测.

> Some studies such as [Emo-VITS](../2022.11_Emo-VITS/2022.11_Emo-VITS.md) [120] use Wav2vec 2.0 as a feature extractor to provide input to the reference encoder instead of spectrum features or raw audio waveform. Figure 11 illustrates the framework of the wav2vec technique and how it is utilized as a feature extractor with TTS models. The wav2vec model converts the continuous audio features into quantized finite set of discrete representations called tokens. This is done using a quantization module that maps the continuous feature vectors into a discrete set of tokens from a learned codebook. As those tokens are more abstract, they reduce the complexity of the features by retaining important features while filtering out all the irrelevant information. Because of that abstraction, it is harder to reconstruct audio from the wav2vec features, which means leakage of linguistic content into feature vectors is significantly lower compared to other features such as MFCCs.

一些研究使用 Wav2Vec 2.0 作为特征提取器, 为参考编码器提供输入, 而不是原始音频波形的频谱特征. 图十一展示了 Wac2Vec 技术及其作为特征提取器如何与 TTS 模型相结合. Wav2Vec 模型将连续音频特征转化为名为 token 的离散表示的量化有限集. 这通过使用量化模块将连续特征向量映射到取自学习好的码本的离散 token 集合. 当这些 token 越抽象, 它们通过保留重要特征并过滤掉所有不相关的信息来减低特征的复杂度. 由于这种抽象性, 很难从 Wav2Vec 特征中重构音频, 这意味着与其他特征如 MFCC 相比, 语言内容泄漏到特征向量将明显下降.

#### 5.1.7.正交性损失 (Orthogonality Loss)

> Studies [34] [39] propose a model with two separate encoders to encode speaker and emotion information through speaker and emotion classification loss, along with gradient inversion of the emotion classification loss in the speaker encoder. Additionally, to disentangle the source speaker information from the emotion embedding, the emotion embedding is made orthogonal to the speaker embedding with an orthogonality loss shown in equation (3). An ablation study in [34] showed that applying an orthogonality constraint helped the encoders learn both speaker-irrelevant emotion embedding and emotion-irrelevant speaker embedding.
> $$
    \mathcal{L}_{orth} = \sum_{i=1}^n \|S_i-e_i\|_F^2\tag{3}
> $$
> 
> where $\|\cdot\|_F$ is the Frobenius norm, $e_i$ is the emotion embedding and $S_i$ is the speaker embedding.

一些研究提出了具有两个独立编码器的模型, 并且通过说话人和情感分类损失, 以及说话人编码器中的情感分类损失的梯度反转来编码说话人和情感信息. 此外, 为了从情感嵌入中解耦源说话人的信息, 情感嵌入通过公式三所示的正交性损失与说话人嵌入正交. 
$$
    \mathcal{L}_{orth} = \sum_{i=1}^n \|S_i-e_i\|_F^2\tag{3}
$$

其中 $\|\cdot\|_F$ 是 F 范数, $e_i$ 是情感嵌入, $S_i$ 是说话人嵌入.

消融实验表明应用正交性约束有助于编码器学习与说话人无关的情感嵌入和与情感无关的说话人嵌入.

### 5.2.无参考音频推理 (Inference without Reference Audio)

> A main drawback of the unsupervised approaches ([Section 4](#Sec4)) is that they require a reference audio for the desired prosody or style of the generated speech. However, prosody references are not always available for the desired speaker, style, or text. Besides, using prosody reference introduces the leakage problem as discussed in [Section 5.1](). As a result, different techniques have been proposed that enable unsupervised expressive speech synthesis without prosody references. Some techniques utilize the reference audio at training phase while at inference phase speech synthesis can be done with or without a reference audio. Other techniques depend on input text only to generate prosody embedding at both training and inference phases. In the following three sections, we will describe techniques for inference without reference audio applied with each of the three main unsupervised ETTS approaches. In [Section 5.2.4](), we will discuss some ETTS approaches that are based on text only. Then in Table 4, we summarize main approaches that are used to extract text-based features with related papers links.

第四节中总结的无监督方法的一个主要缺点是, 它们需要一个参考音频用于所生成语音的期望韵律或风格. 然而所需说话人/风格/文本的韵律参考并不总是能够获得.
此外使用韵律参考会引入第5.1节讨论的泄露问题. 
因此出现了各种技术使得无监督表达性语音合成能够无韵律参考.
一些技术在训练阶段使用参考音频, 而推理阶段语音合成就可以用或不同参考音频.
其他技术只依赖于输入文本用于在训练和推理阶段生成韵律嵌入.
在以下三小节中将描述无参考音频的推理技术, 应用于三种主要的无监督表达性语音合成方法中.
在第四小节将讨论一些仅依赖于文本的表达性语音合成方法.
表格四总结了相关文献中用于提取基于文本的特征的主要方法.

#### 5.2.1.无参考音频的直接参考编码 Direct Reference Encoding without Reference Audio 

> In several studies, prosody predictors are trained jointly with the proposed reference encoder to bypass the requirement for reference audio at inference time. The prosody predictors are trained to predict either the prosody embeddings generated by reference encoders[50] [96] [111] [116], or the acoustic features used as input to reference encoders [37] [63]. As input to these prosody predictors, most studies utilize the phoneme embeddings[37] [63] [96] [111].

在一些研究中, 韵律预测器与所提出的参考编码器联合训练, 以绕过推理时对参考音频的要求. 韵律预测器被训练用于预测参考编码器生成的韵律嵌入或用作参考编码器输入的声学特征.
作为这些韵律预测器的输入, 大多数研究使用音素嵌入.

> Alternatively, features extracted from input text can also be used as input for prosody predictors. In [50], the prosody predictor has a hierarchical structure that utilizes contextual information at both the sentence and paragraph levels to predict prosody embeddings. The input features for this predictor are in the form of 768-dimensional phrase embeddings extracted by the pre-trained language model XLNet [160]. Sentence embeddings are initially predicted from the input features using an attention network. Then a second attention network is used to predict the paragraph-level prosody embedding.

或者从输入文本提取的特征也可以作为韵律预测器的输入.
文献 [050] 中韵律预测其有一个层次结构, 利用句子和段落级别的上下文信息来预测韵律嵌入. 这一预测器的输入特征是使用预训练语言模型 XLNet 提取的 768 维的短语嵌入形式. 首先通过注意力网络从输入特征中预测句子嵌入, 然后第二个注意力网络用于预测段落级别的韵律嵌入.

> Furthermore, in [33], emotion is modelled at three levels: global, utterance, and syllable (local). The model employs three prosody encoders, each with a predictor trained to predict the corresponding prosody embedding based on input text. The global-level predictor functions as an emotion classifier, where the output of its final soft-max layer serves as the global emotion embedding. The emotion label’s embedding is used as the ground truth for this emotion classifier. Both the utterance and local prosody encoders receive level-aligned mel-spectrograms as input and produce utterance prosody embedding and local prosody strength embedding, respectively. Similarly, two prosody predictors are used to predict utterance and local-level embeddings based on the output from the text encoder of the TTS model.

文献 [033] 中情感被建模在三个级别: 全局, 语调和音节 (局部). 模型应用三个韵律编码器, 每个编码器都有一个预测器训练成基于输入文本预测对应的韵律嵌入. 全局预测器作为情感分类器, 其最终的 softmax 层输出作为全局情感嵌入. 情感标签的嵌入作为这一情感分类器的真实值. 语调和局部韵律编码器都接收级别对齐的梅尔频谱作为输入, 并分别生成语调韵律嵌入和局部韵律强度嵌入. 类似地, 两个韵律预测器基于 TTS 模型的文本编码器的输出预测语调和局部级别嵌入.

> In contrast, the prosody predictor proposed in paper [44] learns multiple mixed Gaussian distributions model (GMM) for prosody representations. Therefore, the final outputs of the prosody predictor involve three parameters: mean, variance, and weight of multiple mixed Gaussian distributions from which prosody representations can be sampled at inference time. As input, the predictor receives two phoneme-level sequences including embeddings from the text encoder and embeddings from a pre-trained language model. Similar work is proposed in [95] where only phoneme embeddings are used as input to the prosody predictor. GMM in both studies is modeled via the mixture density network [161].

文献 [044] 提出的韵律预测器学习多重混合高斯分布模型用于韵律表示. 因此韵律预测器的最终输出包含三个参数: 均值, 方差和多混合高斯分布的权重. 从这些分布中可以在推理时采样韵律表示. 作为输入, 预测器接收两个音素级别的序列包括来自文本编码器的嵌入和来自预训练语言模型的嵌入.

文献 [095] 提出的相似工作, 只使用音素嵌入作为韵律预测器的输入.

两项工作的 GMM 都通过混合密度网络进行建模.

#### 5.2.2.无参考语音的VAE类方法 (VAE‑Based Approaches without Reference Audio)

> Sampling from the latent space without reference audio results in less controllability of style. In addition, it can also introduce naturalness degradation and inappropriate contextual prosody with regard to the input text [68] [129]. 
> Therefore, to avoid sampling the latent space without a reference, authors of [131] proposed utilizing the same prosody embedding of the most similar training sentence to input sentence at inference time. The selection process is based on measuring cosine similarity between sentences’ linguistic features. Three methods are proposed for extracting sentence linguistic information including 
> (1) calculating the syntactic distance between words in the sentence using constituency trees [162], 
> (2) averaging the contextual word embeddings (CWE) for the words in the sentence using BERT, and 
> (3) combining the previous two methods.

在没有参考音频的情况下从隐空间采样会导致风格的可控性降低. 此外它还会引入自然性退化和和不适合输入文本的语境韵律.
因此为了避免在没有参考的情况下从隐空间中采样, 文献 [131] 提出推理时用和输入句子最相似的训练句子的相同韵律嵌入. 选择过程基于句子语言特征之间的余弦相似度. 三种方法用于提取句子语言信息:
1. 使用句法树计算句子中单词的句法距离;
2. 使用 BERT 计算句子中单词的上下文词嵌入 (Contextual Word Embeddings, CWE) 的平均值;
3. 结合前两种方法.

> Other studies approach the problem in alternative ways, seeking to enhance the sampling process either through refining the baseline model structure or by incorporating text-based components into the baseline.
> Regarding the improvement of the baseline structure, study [68] suggests the combination of multiple variational autoencoders to generate latent variables at three distinct levels: utterance-level, phrase-level, and word-level. Furthermore, they apply a conditional prior (CP) to learn the latent space distribution based on the input text embedding. To account for dependencies within the input text, they employ Autoregressive (AR) latent converters to transform latent variables from coarser to finer levels.
> An alternative approach is proposed in [126] by replacing the conventional VAE encoder with a residual encoder that leverages phoneme embedding and a set of learnable free parameters as inputs. With this modified structure, the model learns a latent distribution that represents various prosody styles for a specific sentence (i.e.,the input text), in addition to capturing potential global biases within the applied dataset (represented by the free parameters). At the same time, with this modification, the problem of speaker and content leakage into prosody embedding is addressed.

其他研究以不同的方式解决这个问题, 寻求通过改进基线模型结构或向基线模型中添加基于文本的组件来增强采样过程.
关于基线结构的改进:
- 文献 [068] 建议组合多个变分自编码器用于生成三个不同级别的隐变量: 语调级, 短语级和单词级. 此外应用了条件先验基于输入文本嵌入学习隐空间分布. 为了考虑输入文本内的依赖性, 他们应用自回归隐转化器将隐变量从粗糙级别转化为精细级别.
- 文献 [126] 提出了一种替代方法, 通过将传统 VAE 替换为一个利用音素嵌入和一组可学习自由参数作为输入的残差编码器. 采用这一结构, 模型为一个具体句子即输入文本学习一个隐分布表示各种韵律风格, 同时捕捉应用数据集的潜在全局偏差 (由自由参数表示). 同时说话人和内容泄露到韵律嵌入的问题也得到解决.

> Various studies propose training a predictor for the latent prosody vectors based on features extracted from the input text [35] [47]. The proposed model in [47] generates fine-grained prosody latent codes of three dimensions at phoneme-level. These prosody codes are then used to guide the training process of a prosody predictor that receives phoneme embeddings as input, in addition to emotion and speaker embeddings as sentence-level conditions. In [35], the predicted mean values of the latent space distribution are employed as prosody codes. Similarly, a prosody predictor is trained to predict these prosody codes using two text-based inputs, including sentence-level embeddings from a pre-trained BERT model and contextual information considering BERT embeddings of a few of surrounding k sentences given the current sentence.

多项研究提出基于从输入文本中提取的特征训练一个隐韵律向量的预测器.
- 文献 [047] 提出的模型在音素级别生成细粒度的三维韵律隐代码. 这些韵律代码之后用于指导韵律预测器的训练. 预测器接收音素嵌入作为输入, 此外情感和说话人嵌入作为句子级别条件.
- 文献 [035] 中隐空间分布的预测均值作为韵律编码. 类似地训练一个韵律预测器使用两个基于文本的输入包括来自预训练 BERT 模型的句子级别嵌入和考虑当前句子周围的 k 个句子的 BERT 嵌入的上下文信息用于预测这些韵律编码.

> Alternatively, study [129] proposed training a sampler, i.e., Gaussian parameters, to sample the latent space using features extracted from the input text. Three different structures are investigated for the sampler based on the input features it receives. The applied text-based features include BERT representations of a sentence (semantic information), the parsing tree of the sentence (syntactic information) after it is fed to a graph attention network, and the concatenation of outputs from the previous two samplers.

文献 [129] 提出训练一个采样器, 即高斯参数, 使用输入文本提取的特征对隐空间进行采样. 根据采样器接收的输入特征, 研究了三种不同的结构. 应用基于文本的特征包括句子的 BERT 表示 (语义信息), 图注意力网络输出的句子的解析树 (语法信息) 和前两个采样器的输出的拼接.

#### 5.2.3.无参考音频的 GST 类方法 (GST‑Based Approaches without Reference Audio)

> There are GST-TTS models that utilize text-based features from pre-trained language models such as BERT to guide expressive speech synthesis at inference time without a reference. In [ST-TTS](../2021.04_ST-TTS/2021.04_ST-TTS.md), the training dataset is labeled with short phrases that describe the style of the utterance and are known as style tags. A pre-trained Sentence BERT (SBERT) model is used to produce embeddings for each style tag as input to a style tag encoder. The style embedding from the GST-TTS model is used as ground truth for the style tag encoder. During inference, either a reference audio or a style tag can be used to generate speech.

有些 GST-TTS 模型使用来自预训练语言模型 (如 BERT) 的基于文本的特征用于指导在推理时进行无参考的表达性语音合成.
- ST-TTS 中训练集用描述语调风格的短语进行标注, 即风格标签. 用一个预训练的句子 BERT 模型 (SBERT) 为每个风格标签生成嵌入作为风格标签编码器的输入. 来自 GST-TTS 模型的风格嵌入作为风格标签编码器的真实值. 在推理时, 可以使用参考音频或风格标签用于生成语音.

> Alternatively, pre-trained language models are used to extract features from the input text and train a prosody predictor to predict the style embedding based on these text-based features (Context-Aware Style Predictor, [46] [50] [73] [91] [94]. In [94], the baseline model [75] is extended with a prosody predictor module that extracts time-aggregated features from the output of the baseline text encoder. Two pathways are suggested for the targets of the predictor output: either using the weights of the GSTs or the final style embedding. Similarly, in [73], two prosody predictors are investigated, using different inputs from a pre-trained multi-language BERT model. While the first predictor utilizes BERT embeddings for the sub-word sequence of input text, the other predictor employs only the CLS token from the sentence-level information extracted by the BERT model. Both inputs provide rich information for the predictors to synthesize prosodic speech based solely on input text.

或者使用预训练的语言模型来从输入文本中提取特征并训练一个韵律预测器, 基于这些基于文本的特征来预测风格嵌入.
- 文献 [094] 中将基线模型 [075] 用韵律预测器模块进行扩展, 用于从基线文本编码器的输出中提取时间聚合特征. 对于预测器输出的目标有两个建议: 使用 GSTs 的权重或最终风格嵌入.
- 文献 [073] 研究了两个韵律预测器, 使用来自预训练多语言 BERT 模型的不同输入. 第一个预测器使用输入文本的子词序列的 BERT 嵌入, 第二个预测器仅使用从 BERT 模型提取的句子级别信息中的 CLS token. 这两个输入都为预测器提供了丰富的信息从而仅依靠输入文本合成韵律语音.

> The multi-scale GST-TTS proposed in [50] which employs three style encoders, also introduces three style predictors that employ hierarchical context encoders (HCE). The input to the first predictor is the BERT sub-word-level semantic embedding sequence. The attention units in the HCE, however, are used to aggregate the resulting context embedding sequence from lower-level as input to higher-level predictors. Additionally, the output of the higher-level predictor is used to condition the lower-level predictor. BERT embeddings are also used in [46] but at word level and are passed as input to the proposed prosody predictor. The style embedding which is generated via word-level GSTs is used to guide the prosody predictor during model training.

- 文献 [050] 提出的多尺度 GST-TTS 采用了三个风格编码器, 也引入了三个采用层次上下文编码器 (Hierarchical Context Encoders, HCE) 的风格预测器. 第一个预测器的输入是 BERT 子词级别语义嵌入序列. HCE 中的注意力单元用于聚合由低层次的上下文嵌入序列作为高层次预测器的输入. 此外高层次预测器的输出用于条件化低层次预测器. 
- 文献 [044] 同样使用 BERT 嵌入但是在单词级别, 并且作为韵律预测器的输入. 通过词级 GSTs 生成的风格嵌入在模型训练时用于指导韵律预测器.

> A Context-aware prosody predictor is proposed in **"Context-Aware Coherent Speaking Style Prediction with Hierarchical Transformers for Audiobook Speech Synthesis"** which considers both text-side context information and speech-side style information from preceding speech. This predictor comprises two hierarchical components: a sentence encoder and a fusion context encoder. The context-aware input to the predictor includes word-level embeddings from XLNet [160] for each word in the current sentence, as well as the N preceding and following sentences. The sentence encoder focuses on learning low-level word meanings within each sentence, while the fusion context encoder captures high-level contextual semantics between the sentences. Additionally, style embeddings from previous sentences are integrated into the fusion context encoder input to account for speech-side information.

文献提出的上下文感知的韵律预测器, 考虑来源于之前语音的文本侧的上下文信息和语音侧的风格信息. 这一预测器包含两个层次组件: 一个句子编码器和一个融合内容编码器. 预测器的内容感知输入包括 XLNet 对当前句子每个单词的的词级嵌入和 N 个前后句子. 句子编码器重点学习句子内低层次单词含义, 而融合内容编码器捕获句子间的高级上下文语义. 此外前面句子的风格嵌入整合到融合内容编码器的输入以考虑语音侧的信息.

> In [91] Speech emotion recognition model (SER) is employed as a style descriptor to learn the implicit connection between style features and input text. Deep style features for both synthesized speech and reference speech are obtained from a small intermediate fully connected layer of a pre-trained SER model during training. The extracted style features are compared where an additional loss is introduced to the GST-TTS model loss. At inference time only text is used to synthesize expressive speech.

文献 [091] 语音情感识别 (Speech Emotion Recognition, SER) 模型作为风格描述器, 学习风格特征和输入文本之间的隐式联系. 合成语音和参考语音的深度风格特征在训练时从预训练 SER 模型的一个小的中间全连接层中获得. 当给 GST-TTS 模型损失引入额外的损失后, 对提取的风格特征进行了对比. 在推理时仅输入文本用于合成表达性语音.

#### 5.2.4.仅依赖于文本的表达性语音合成方法 (ETTS Approaches Based only on Text)

> This category involves approaches that depend solely on input text to obtain prosody-related representations/embeddings during TTS model training. Several features related to speech prosody have been proposed by various studies for extraction from input text and subsequent transmission to a DNN-based module to generate prosody representations. For instance, the features extracted by the pre-trained language models can capture both semantic and syntactic relationships with the input text, making them effective representations for prosody. In [83], input text word-level embeddings are extracted by the Embeddings from Language Models (ELMo) model [163] and used to generate context-related embeddings via a context encoder. Similarly, in [29], BERT is employed to extract embeddings for utterance sentences and pass them to a specific context-encoder to aggregate these embeddings and form a final context vector.

这类方法包含在训练阶段仅依赖输入文本以获取韵律相关表示或嵌入的方法. 各种研究从输入文本中提取出和语音韵律相关的若干特征, 然后传输到基于深度神经网络的模块以生成韵律表示. 例如由预训练语言模型提取的特征可以捕获输入文本的语义和语法关系, 使之成为韵律的有效表示.
- 文献 [083] 通过 Embeddings from Language Models, ELMo 提取输入文本词级嵌入并通过内容编码器生成内容相关的嵌入.
- 文献 [029] 使用 BERT 提取语调序列的嵌入并将它们传递给具体文本编码器以聚合这些嵌入并形成最终的上下文向量.

> Other studies, such as [30] [40] [54], utilize graph representations of input text, which can also reflect semantic and syntactic information about the given text. In [30], the graphical representations of prosody boundaries in Chinese text are passed to a graph encoder based on Graph Neural Networks (GNN) to generate prosodic information for the input text. The prosody boundaries of the Chinese language can be manually annotated or predicted using a pre-trained model. In contrast, [54] combines BERT-extracted features for input text with its graph dependency tree to produce word-level prosody representations. Specifically, the input text is passed through both BERT and a dependency parsing model to extract the dependency tree for word-level BERT embedding. A Relational Gated Graph Network (RGGN) is used to convert this dependency tree into word-level semantic representations upon which the decoder of the TTS model is conditioned.

其他研究利用输入文本的图表示, 同样可以反映给定文本的语义和语法信息.
- 文献 [030] 将汉语文本的韵律边界的图形表示转递给基于图神经网络的图编码器以生成输入文本的韵律信息. 中文的韵律边界可以人工标注或使用预训练模型预测. 
- 文献 [054] 将 BERT 从输入文本提取的特征和它的图依赖树结合用于产生词级韵律表示. 具体地, 输入文本传输到 BERT 和一个依赖解析模型用于提取词级 BERT 嵌入的依赖树. 使用相关门控图网络 (Relational Gated Graph Network, RGGN) 将依赖树转化为词级语义表示, 并以此条件化 TTS 模型的解码器.

> Different text-based features have been extracted from input text to obtain prosody (style) embeddings in [40]. The paper utilizes an emotion lexicon to extract word-level emotion features, including VAD (valence, arousal, dominance) and BE5 (joy, anger, sadness, fear, disgust). Additionally, the [CLS] embedding by BERT for each utterance is also extracted. The obtained features are then passed to a style encoder to produce a style embedding.

- 文献 [40] 从输入文本中提取不同的基于文本的特征用于获得韵律/风格嵌入. 使用一个情感词典用于提取词级情感特征, 包括 VAD (效价, 唤醒, 支配) 和 BES (喜悦, 愤怒, 悲伤, 恐惧, 厌恶). 此外由 BERT 对每个语调提取 [CLS] 嵌入. 获得的特征之后传递给风格编码器以产生风格嵌入.

> Other models under this category train a prosody encoder/predictor jointly with an autoregressive TTS model such as Tacotron 2, to encode some prosody-related features utilizing text-based features. The trained encoder is then used at inference time to encode prosody-related features based on input text to the TTS model. The text-based input to these prosody encoders in most of the studies is the text’s character/phoneme embeddings [20] [48] [71] [72] [103], while some studies use features extracted from the input text [64] [125]. For instance, [125] employs four ToBI (Tones and Break Indices) features as word-level prosody tags that are combined with the phoneme embedding as input to the TTS model. A ToBI predictor is jointly trained to predict four ToBI features based on grammatical and semantic information extracted from the input text using a self-supervised language representation model ELECTRA [164].

其他模型训练一个韵律编码器或预测器, 和自回归 TTS 模型 (如 Tacotron 2) 联合训练, 使用基于文本的特征以编码一些韵律相关的特征. 训练的编码器在推理时根据 TTS 模型的输入文本编码韵律相关的特征. 这些用于韵律编码器的基于文本的输入在大多数研究中采用文本的字符/音素嵌入, 其他一些研究使用从输入文本提取的特征.
- 文献 [125] 采用四个 ToBI 特征作为词级韵律变迁, 和音素嵌入结合作为 TTS 模型的输入. ToBI 预测器联合训练基于语法和语义信息来预测这四个 ToBI 特征, 这些信息是使用自监督语言表示模型 ELECTRA 从输入文本中提取出来的.

> In addition to the previously mentioned features, several other prosodic features are also proposed as the output of the prosody predictors in other studies. For example, the prosody predictor in [103] predicts a set of utterance-wise acoustic features, including log-pitch, log-pitch range, log-phone duration, log-energy, and spectral tilt. In [48], the proposed pitch predictor outputs a continuous pitch representation, which is converted into discrete values using Vector Quantization (VQ) [149]. Furthermore, studies [20] [71] propose predicting the three prosody-related features, i.e., F0, energy, and duration, either by a single acoustic features predictor (AFP)[71] or via three separated predictors [20]. 

除了之前提到的特征, 其他研究还提出了其他韵律特征作为韵律预测器的输出. 
- 文献 [103] 的韵律预测器预测一组语调声学特征, 包括对数音高, 对数音高范围, 对数音素时长, 对数能量和频谱倾斜.
- 文献 [048] 的音高预测器输出一个连续的音高表示, 使用矢量量化技术转化为离散值.
- 文献 [020] [071] 通过单个声学特征预测器 (AFP) 或三个单独的预测器预测三个韵律相关的特征, 即 F0, 能量和时长.

> Another type of emotion embedding is sentiment feature embedding, which is utilized to produce expressive speech by extracting sentiment information from the input text. This is demonstrated in work [135], where the Stanford Sentiment Parser is used to generate vector embeddings or sentiment probabilities based on the tree structure of the sentence. To synthesize expressive speech, different combinations of probabilities and vector embeddings (for individual words or word-context) are added to the linguistic features as inputs to the TTS model.

另一种情感嵌入是情感特征嵌入, 它用于通过从输入文本中提取情感信息来产生富有表现力的语音. 这在文献 [135] 中得到了证明, 其中使用了斯坦福情感解析器来根据句子的树结构生成向量嵌入或情感概率. 为了合成富有表现力的语音, 将不同组合的概率和向量嵌入（对于单个单词或单词上下文）添加到作为 TTS 模型输入的语言特征中.

### 5.3 Prosody Controllability 韵律可控性

Text-to-speech is a one-to-many mapping problem, i.e.,for one piece of text there could be many valid prosody patterns because of speaker-specific variations. Accord-ingly, providing a kind of controllability over prosody-related features in synthesized speech is essential for generating expressive speech with different variations.
However, it’s not always easy to mark-up prosody or even to define boundaries between prosody events, i.e., dura-tion boundaries can vary depending on segmentation,pitch contour prediction is error-prone, and prosody fea-tures may not always correlate well with what listeners perceive.
Several studies in literature have addressed the control-lability issue in terms of selecting an emotion/style class or intensity level and adjusting prosody-related features at different speech levels. In this section, we discuss stud-ies considering prosody controllability.

文本到语音转换是一个一一对应的问题，即对于一段文本，由于说话人特定的变化，可能存在许多有效的韵律模式。因此，在合成的语音中提供对韵律相关特征的控制对于生成具有不同变化的表达性语音至关重要。
然而，标记韵律或定义韵律事件之间的界限并不总是容易的，即持续时间界限可能会根据分段而变化，音高轮廓预测容易出错，韵律特征并不总是与听众感知相关。
在文献中，有几项研究从选择情感/风格类别或强度级别以及在不同的语音级别调整韵律相关特征的角度解决了可控性问题。在本节中，我们将讨论考虑韵律可控性的研究。

#### 5.3.1 Modeling‑specific prosody styles 

This group of studies provides individual representa-tions of expressive styles/emotions, enabling the control of prosody in synthesized speech by offering the ability to select from available representations or adjust their values. In some studies [55] [70] [116], style is modeled at a single speech/text level, while in other studies [68] [79] [133] a multi-level or hierarchical model of expressive styles is used to allow for a better capture of prosody var-iation in expressive speech.
In single-level prosody modeling approaches, [55] is one of the early studies that extends a baseline with fine-grained control over the speaking style/prosody of syn-thesized speech. The proposed modification involves adding an embedding network with temporal structure to either the speech-side or text-side of the TTS model.
Accordingly, the resulting prosody embedding is of vari-able length, and it is used to condition input to either encoder or decoder based on the position of the embed-ding network. Speech-side prosody embedding provides adjustment of prosody at frame-level, while text-side prosody embedding enables phoneme-level prosody control.
Single-level prosody embeddings can be converted into discrete embeddings as in [70] [116]. Discrete pros-ody representations are easier to control and analyze and provide a better interpretation of prosodic styles.

这些研究提供了表达风格/情感的个体表示，使得在合成语音中控制语调成为可能，通过提供从可用表示中选择或调整其值的能力。在某些研究中，风格在单个语音/文本级别建模，而在其他研究中，使用多级或分层模型来更好地捕捉表达性语音中的语调变化。

在单级语调建模方法中，[55]是早期研究之一，它扩展了基线，对合成语音的说话风格/语调进行细粒度控制。提出的修改涉及在TTS模型的语音侧或文本侧添加具有时间结构的嵌入网络。

因此，得到的语调嵌入是可变长度的，并且根据嵌入网络的位置用于条件输入到编码器或解码器。语音侧语调嵌入提供帧级语调调整，而文本侧语调嵌入允许音素级语调控制。

单级语调嵌入可以转换为离散嵌入，如[70] [116]。离散语调表示更容易控制和分析，并提供语调风格的更好解释。


In [116], a word-level prosody embedding is proposed based on decision trees and a GMM. A word-level refer-ence encoder is first used to obtain word-level prosody embedding from reference audio. A binary decision tree is employed to cluster embeddings with their identities based on their phonetic information. Prosody embed-dings of words in each leaf node will differ only in their prosodies. Then prosody embeddings of each leaf can be clustered via a GMM model where clusters represent prosody tags. If the applied GMM consists of five com-ponents and a tree of ten leaf nodes, a set of 50 prosody tags is produced. At inference time, prosody tags can be selected manually or via a prosody predictor that is trained to select appropriate prosody tags based on input text.
In [70], an audiobook speech synthesis model is pro-posed. The model uses a character-acting-style extrac-tion module based on ResCNN [165] to extract different character acting styles from the input speech. Discrete character-level styles are obtained via vector quantization(VQ) [149], which maps them to a codebook, limiting the number of styles. At inference, the discrete character-act-ing-styles are predicted via a style predictor. The charac-ter-level style predictor uses both character embeddings from Skip-Gram [166] and text-based features from RoB-ERTa [167] as input.
在[116]中，基于决策树和GMM提出了一个词级语调嵌入。首先使用词级参考编码器从参考音频中获取词级语调嵌入。使用二叉决策树根据其音素信息对嵌入进行聚类。每个叶节点中单词的语调嵌入仅在语调上有所不同。然后，可以通过GMM模型对每个叶的语调嵌入进行聚类，其中聚类表示语调标签。如果应用的GMM包含五个组件和一个十个叶节点的树，则会产生50个语调标签。在推理时，语调标签可以手动选择或通过训练以根据输入文本选择适当语调标签的语调预测器选择。

在[70]中，提出了一种有声读物语音合成模型。该模型使用基于ResCNN [165]的角色扮演风格提取模块从输入语音中提取不同的角色扮演风格。通过矢量量化(VQ) [149]获得离散角色级风格，将其映射到码本，限制风格数量。在推理时，通过风格预测器预测离散角色扮演风格。角色级风格预测器使用来自Skip-Gram [166]的角色嵌入和来自RoBERTa [167]的文本特征作为输入。



Regarding multi-level prosody modeling, some stud-ies propose enhancing prosody control in the baseline models [74] [75] [77] by modifying their single-level pros-ody modeling to multiple levels. For instance, [133] pro-poses a hierarchical structure of [75] with multiple GST layers. Three GST layers are employed in the proposed model, each consisting of 10 tokens, which were found to yield better token interpretation. Tokens of the first and second layers were found to learn different speak-ers and styles, but these representations were not easily interpreted. Interestingly, the tokens in the third layer were able to generate higher quality samples with more distinct and interpretable styles. Specifically, third-layer styles exhibit clear differences in their features, includ-ing pitch, stress, speaking rate, start offset, rhythm, pause position, and duration.
Model in [77] is further extended in [68] with three VAEs to generate three different levels (utterance, phrase,and word) of latent variables with varying time resolu-tions. Acoustic features and linguistic features are passed as input to the three VAEs. Initially, a conditional prior(CP) is applied to learn a distribution for sampling utter-ance-level latent variables based on linguistic features from the input text. The generated latent variables are passed to other levels via auto-regressive (AR) latent converters that convert latent variables from coarser-level to finer-level with input text condition. In fact, the utterance-level latent variables can be used to control the generated speech styles, regardless of latent variables of other levels, as they are predicted based on the utterance-level latent variables.




The Controllable Expressive Speech Synthesis (ConEx)model in [79] proposes modeling prosody at two levels,utterance-level (global) and phone-level (local), using reference encoders [74]. However, the global prosody embedding is used to condition the local prosody embed-ding, resulting in an integrated prosody embedding.
The local embeddings are 3D vectors that are converted into discrete local prosody embeddings (codes) via vec-tor quantization (VQ) [149]. At inference time, the integrated prosody embedding is predicted by an auto-regressive (AR) prior model trained to predict categori-cal distributions for each of the discrete codes utilizing global prosody embedding and the phoneme embed-ding as inputs. While global prosody embedding can be obtained from training samples or from an audio refer-ence, local prosody embeddings for a given global pros-ody embedding are achieved via the AR prior model.
Fine-grained prosody control can be achieved by select-ing a specific phoneme to start adjusting prosody from.
The AR prior model will first generate the top k pros-ody options for this phoneme. Then, the local prosody sequence will be generated autoregressively for each of the first top k options by the AR prior model.

#### 5.3.2 Modeling‑specific prosody features 

This group of studies provides individual representations of prosody-related features. Control over prosody of the synthesized speech is provided via selecting or adjust-ing a specific representation of a specific prosody-related feature. Some studies in this direction model prosody features at the global or utterance-level [97] [128], while other studies propose modeling at fine-grained lev-els [48] [63] [71] [122] [138], such as phoneme, syllable, or word-level.
The STYLER model [97], for example, employs multi-ple style encoders to factor speech style into several com-ponents, including duration, pitch, speaker, energy, and noise. This structure enables STYLER to generate con-trollable expressive speech by adjusting each of the indi-vidually modeled features. Furthermore, with the explicit noise encoding, other encoders can be constrained to exclude noise information as a style factor, and thus the model can generate clean speech even with noisy refer-ences. Adjusting the style factors, various styles of speech can be generated from STYLER.

这些研究提供了与韵律相关的特征的个体表示。通过选择或调整特定韵律相关特征的特定表示来控制合成的语音的韵律。在这个方向上，一些研究在全局或句子级别建模韵律特征[97] [128]，而其他研究则提出在细粒度级别建模[48] [63] [71] [122] [138]，例如音素、音节或单词级别。

例如，STYLER模型[97]使用多个风格编码器将语音风格分解为几个组成部分，包括持续时间、音高、说话者、能量和噪声。这种结构使STYLER能够通过调整每个单独建模的特征来生成可控的表达性语音。此外，通过显式噪声编码，其他编码器可以被约束以排除噪声信息作为风格因素，因此模型可以生成干净的语音，即使使用嘈杂的参考。通过调整风格因素，可以从STYLER生成各种风格的语音。


> Adjusting several features at fine-grained levels can be a difficult task. 
> For example, [FastSpeech2](../../Models/AV1_Acoustic/2020.06_FastSpeech2.md) provides fine-grained control over pitch range, duration,energy, which are modeled at the phone-level (phone-wise), and it is not easy to adjust these features to achieve a specific prosodic output. 
> Raitio and Seshadri [128] improves [FastSpeech2](../../Models/AV1_Acoustic/2020.06_FastSpeech2.md) with an utterance-wise (coarse-grained) prosody model using an additional variance adaptor. That second variance adaptor is the same as the original one, but it models five features at the utterance-level: pitch, pitch range, duration, energy,and spectral tilt. These features are then concatenated with the corresponding output of the first variance adaptor. Such utterance-wise prosody model enables easier control of prosody while still allowing modification at the phone-level. To control high-level prosody,a bias is added to the corresponding utterance-wise prosody predictions. A phone-level prosody control is achieved by directly modifying the phone-wise features.
Fine-grained control over a specific prosody-feature can also be required specially for strong speaking styles.
To that end, in [71], a predictor is proposed to predict F0, energy, and duration features at the phoneme-level.
During inference, the predicted features are generated based on the input text alone; however, they can also be provided externally and modified as desired.
Furthermore, two prosody modeling levels are pro-posed in [63]: the local level (word-level) and global level (utterance-wise). The global prosody embedding is the emotion embedding obtained by a reference-based encoder. The local prosody embedding is obtained from a predictor of the F0 features at the word-level with global prosody embedding and the phoneme embed-ding as inputs. Both embeddings are then passed to a multi-style encoder to form the final multi-style pros-ody embedding. Therefore, modifying the predicted F0values can provide control of prosody at the utterance,word, and phoneme levels.

在细粒度级别调整多个特征可能是一项困难的任务。例如，FastSpeech2 在音素级别（音素级）提供对音高范围、持续时间和能量的细粒度控制，并且不容易调整这些特征以实现特定的韵律输出。Raitio和Seshadri[128]通过使用额外的变异适配器改进FastSpeech2，该变异适配器在句子级别（粗粒度）使用韵律模型。第二个变异适配器与原始变异适配器相同，但在句子级别建模五个特征：音高、音高范围、持续时间、能量和频谱倾斜。然后，将这些特征与第一个变异适配器的相应输出连接起来。这种句子级别的韵律模型允许更容易地控制韵律，同时仍然允许在音素级别进行修改。为了控制高级别韵律，向相应的句子级别韵律预测添加偏差。通过直接修改音素级特征来实现音素级韵律控制。

对于强烈的说话风格，可能需要对特定韵律特征进行细粒度控制。为此，[71]中提出了一种预测器来预测音素级别的F0、能量和持续时间特征。在推理过程中，根据输入文本生成预测特征；但是，它们也可以从外部提供并根据需要进行修改。此外，[63]中提出了两个韵律建模级别：局部级别（单词级别）和全局级别（句子级别）。全局韵律嵌入是从基于参考的编码器获得的情绪嵌入。局部韵律嵌入是从具有全局韵律嵌入和音素嵌入作为输入的预测器获得的音素级别的F0特征。然后，将这两个嵌入传递到一个多风格编码器，以形成最终的多风格韵律嵌入。因此，修改预测的F0值可以提供对句子、单词和音素级别的韵律的控制。

More flexibility in controlling the F0 feature is pro-vided in the controllable deep auto-regressive model(C-DAR) model [138] which allows for F0 contour adjustment by the user. To achieve this goal, three strat-egies are used: 1) context awareness by conditioning the model on the preceding and following speech dur-ing training, 2) conditioning the model on some ran-dom segments of ground truth F0, and 3) predicting F0 values in reverse order. Additionally, several text-based features are used as input to the model, includ-ing word embeddings derived from BERT, V/UV label,one-hot vector for the nearby punctuation, and pho-neme encodings. At inference, F0 values specified by the user are used as alternatives for the ground truth F0 segments, and the model predicts the rest of the utter-ance’s F0 contour through context awareness.
Discrete fine-grained representations for prosody features as in [48] [122] are also useful to limit the number of the obtained representations. Both studies [48] [122]utilize VQ [149] to map each prosody embedding to the closest discrete representation from a predefined code-book. In [48], a pitch predictor is used to predict charac-ter-level continuous pitch representation using character embeddings from the text encoder as input. Zhang et al.[122], however, produces syllable-level prosody embed-dings from a reference encoder that takes F0, intensity,and duration features from reference audio as input. The resulting prosody embeddings are then mapped to a pre-defined codebook to extractb discrete prosody codes.

在可控深度自回归模型（C-DAR）模型[138]中提供了对F0特征的更多控制灵活性，该模型允许用户调整F0轮廓。为了实现这一目标，使用了三种策略：1）通过在训练期间将模型条件化在先前和随后的语音上来实现上下文感知，2）将模型条件化在地面真F0的一些随机片段上，以及3）以相反的顺序预测F0值。此外，将几个基于文本的特征用作模型的输入，包括从BERT派生的词嵌入、V/UV标签、附近标点符号的一热向量和音素编码。在推理时，用户指定的F0值被用作地面真F0片段的替代品，并且模型通过上下文感知预测整个句子的F0轮廓。

与[48] [122]中一样，离散细粒度表示对于韵律特征也很有用，可以限制获得的表示的数量。两项研究[48] [122]都使用VQ[149]将每个韵律嵌入映射到预定义码本中最近的离散表示。在[48]中，使用音高预测器来预测使用文本编码器作为输入的字符嵌入的字符级连续音高表示。然而，张等人[122]从参考编码器产生音节级韵律嵌入，该参考编码器将参考音频的F0、强度和持续时间特征作为输入。然后，将得到的韵律嵌入映射到一个预定义的码本，以提取离散的韵律代码。


Resulting prosody codes in [48] represent the pitch and other suprasegmental information that can be adjusted via a specific bias value to generate speech with differ-ent pitch accents. The codes in [122], can be interpreted as representing some prosody features such as pitch and duration. The prosody variation at the syllable-level can be manually controlled by assigning each syllable the desired prosody code from the codebook.
In [125], ToBI features, which involve a set of con-ventions used for transcribing and annotating speech prosody, are used. The applied ToBI features are four word-level tags: pitch accents, boundary tones, phrase accents, and break indices. The extracted ToBI tags are used as input to TTS model. Simultaneously, a ToBI pre-dictor is trained to predict these prosody tags based on grammatical and semantic information extracted from the input text using a self-supervised language model.
The resulting model had the ability to control the stress,intonation, and pause of the generated speech to sound natural, utilizing only ToBI tags from the text-based predictor.

在[48]中，得到的韵律代码表示音高和其他超音段信息，可以通过特定的偏置值进行调整，以生成具有不同音高重音的语音。在[122]中，代码可以解释为表示一些韵律特征，如音高和持续时间。可以通过将每个音节分配从码本中所需的韵律代码来手动控制音节级别的韵律变化。

在[125]中，使用了ToBI特征，这是一种用于转录和注释语音韵律的约定集。应用的ToBI特征是四个单词级别的标签：音高重音、边界音、短语重音和停顿索引。提取的ToBI标签用作TTS模型的输入。同时，训练了一个ToBI预测器来预测这些韵律标签，基于使用自监督语言模型从输入文本中提取的语法和语义信息。

最终模型能够仅使用文本预测器中的ToBI标签来控制生成的语音的强调、语调和停顿，使其听起来自然。


#### 5.3.3 Modeling prosody strength 

This group of studies focus on regulating the strength of emotion or prosody. For instance, [61] utilizes the distance between emotion embeddings and the neutral emotion embeddings to identify scalar values for emotion intensity. It proposes a phoneme-level emotion embed-ding and a fine-grained emotion intensity. The emo-tion embedding is first obtained via a reference encoder.
The emotion intensity is then generated by an intensity extractor that takes the emotion embedding as input. The intensity extractor produces intensity as a scalar value based on the distance between the emotion embedding and the centroid of a pre-defined cluster for neutral emo-tion embeddings. The resulting emotion intensity values are quantized into pseudo-labels that serve as the index for an intensity embedding table.
Another method for learning emotion strength values in an unsupervised manner is by using ranking functions.
Studies [27] [31] [33] [64] utilize a ranking function-based method named relative attributes [89] for this purpose. In[33], prosody is modeled at three levels: global-level rep-resentation by emotion embedding, utterance-level rep-resented by prosody embedding from a reference-based encoder, and the local-level represented by emotion strength. The study trains an emotion strength extractor at the syllable-level based on input speech utilizing the ranking function. Simultaneously, a predictor of emo-tion strength is trained based on features extracted from input text via BERT model. Besides changing emotion label and emotion reference audio, the model provides manual control of the emotion strength values in the syn-thesized speech.
Alternatively, the reference encoder in [31] functions as a ranking function to learn a phoneme-level emotion strength (descriptor) sequence. The proposed ranking function [89] receives its input from fragments of target reference audio obtained via a forced alignment model to phoneme boundaries. The OpenSMILE [139] tool is then used to extract 384-dimensional emotion-related features from these reference speech fragments as input to the ranking function. Similarly, the proposed ranking function in [27] takes a set of acoustic features extracted from the input speech via OpenSMILE tool but at the utter-ance-level as input. The ranking function leverages the difference between neutral samples and samples associ-ated with each emotion class in the dataset. The training process is formulated as solving a max-margin optimiza-tion problem. The resulting emotion strength scalars can be manually adjusted or predicted based on text or refer-ence speech.
In [64], both emotion class and emotion strength value are obtained via a joint emotion predictor based only on the input text. The input to the predictor is features extracted from input text via the Generative Pre-trained Transformer (GPT)-3 [88]. Emotion class and emotion strength are the two outputs of the predictor where the former is represented as a one-hot encoded vector and the latter is presented as a scalar value. Emotion labels and emotion strength values which are also obtained via[89], are used as ground truth for predictor training.
Another ranking method is proposed in **"Cross-Speaker Emotion Transfer by Manipulating Speech Style Latents"** using the ranking support vector machine. The model generates style embedding and speaker embedding via two separate encoders. Both style and speaker embeddings at infer-ence time are represented by centroids of each single speaker and style embeddings. However, a linear SVM is trained with the model to provide the ability for style embedding adjustment. The proposed SVM model is trained to classify between neutral emotion and a specific emotion embedding, where the learned hyperplane is utilized to move(scale) the style vectors in a direction towards/opposite to the hyperplane.
Another type of control that contributes to generat-ing speech with a better representation of local prosodic variation is introduced in [124]. The proposed model suggests an unsupervised approach to obtain word-level prominence and phrasal boundary strength features. For this purpose, continuous wavelet transform (CWT) [168]is utilized to extract continuous estimates of word promi-nence and boundary information from the audio signal.
First, the three prosodic signals f0, energy, and duration are extracted and combined as input to the CWT. Then,the combined signal is decomposed via CWT into scales that represent prosodic hierarchy. Word and phrase-level prosody are then obtained by following ridges or valleys across certain scales. The continuous word prominence and boundary estimates are achieved via the integration of the resulting lines aligned with the textual informa-tion. With manually identified intervals, the continuous values of prominence and boundary strength are then discretized.

#### 5.3.4 Prosody clustering 

In this section, methods for selecting the appropri-ate prosody embedding for the referenced-based ETTS models are described. To begin with, clustering methods are utilized in [57] [58] to generate representative pros-ody embeddings for each emotion class when the GST-TTS model is trained with a labeled dataset. Initially,the resulting emotion embeddings are clustered in a 2d space. In [57], the centroid of each cluster is used as the weights of the GSTs to generate emotion embedding for each emotion class. In [58], the weight vector that repre-sents each emotion cluster is obtained by considering the inter and intra distances between emotion embedding clusters. Specifically, an algorithm is used for minimizing each embedding distance to the target emotion cluster and maximizing its distance to other emotion clusters.
Similarly, clustering algorithms are applied in [112] [113] to achieve discrete prosody embeddings but for two specific prosody-related features. The two studies employ K-means algorithm to cluster F0 and duration features extracted for each phoneme. The centroids of the clus-ters are then used as discrete F0 and duration values/tokens for each phoneme. work [112] applies a balanced clustering method with duration features to overcome degradation in voice quality that appeared in [113] dur-ing duration control. Moreover, to keep phonetic and prosodic information separate during training, an atten-tion unit is introduced to map prosody tokens to decoder hidden states and generate prosody context vectors. The resulting discrete tokens for F0 and duration features provide a fine-grained level of control over prosody by changing the corresponding prosodic tokens for each phoneme.
In [105], a cross-domain SER model with the GST-TTS model is proposed to obtain emotion embeddings for an unlabeled dataset. The cross-domain SER model is trained using two datasets including: 1) an SER data-set (source) labeled with emotions, and 2) a TTS data-set (target) that is not labeled. Simultaneously, the SER model trains an emotion classifier that generates soft labels for the unlabeled TTS dataset. These soft labels are then used to train an extended version of the baseline in[74] with an emotion predictor. In the training process,the weights of the style tokens layer are passed as input to the predictor, which employs the learned soft labels as ground truth values. At inference time, weights vectors for each emotion class are averaged to obtain the emo-tion class embedding. However, since the predicted labels for the TTS dataset are soft labels, and thus not entirely reliable, only the top K samples with the highest posterior probabilities are selected.

### 5.4 Speech synthesis for unseen speakers and unseen styles

Building a speech synthesis model that supports mul-tiple speakers or styles can be achieved by training TTS model with a multi-speaker multi-style dataset. How-ever, generating speech for an unseen speaker or style is a challenging task for which several solutions have been proposed in the literature. A popular approach is to fine-tune the averaged TTS model with some samples from the unseen target speaker or style. The fine-tuning pro-cess may require a single sample from the unseen speaker or style (referred to as one-shot models) or a few samples(referred to as few-shot models). There are also models that do not require any fine-tuning steps, and these are known as zero-shot TTS models.
For instance, the fine-tuning process proposed in [112]focused on sentences used in the process to ensure pho-netic coverage, meaning that each phoneme should appear at least once in these sentences. The proposed model requires about 5 minutes of recordings from the unseen target speaker to clone the voice and allow for manipulation of some voice features (such as F0 and duration) by the model at the phoneme-level.
Another approach to address the problem of unseen data is to employ specific structures in the TTS model,as proposed in [52] [96] [97] [107]. As an example, in [107],a cycle consistency network is proposed with two Vari-ational Autoencoders (VAEs). The model incorporates two training paths: a paired path and an unpaired path.
The unpaired path refers to training scenarios where the reference audio differs from the output (target) speech in terms of text, style, or speaker. Two separate style encod-ers are utilized in the model, with one dedicated to each path. This structure facilitates style transfer among intra-speaker, inter-speaker, and unseen speaker scenarios.
In [52], the U-net structure proposed for the TTS model supports one-shot speech synthesis for unseen styles and speakers. The U-net structure is used between the style encoder and the mel decoder of the TTS model,with an opposite flow between them. Both the style encoder and decoder consist of multiple modules with the main building unit as ResCnn1D and instance nor-malization (IN) layers. The decoder receives phoneme embedding and produces the Mel-spectrogram as out-put. In parallel, the style encoder receives the reference audio and produces its linguistic content with guidance from the content (text) encoder. The style encoder mod-ules produce latent variables, i.e., mean, and standard deviation, for the hidden inputs in the IN layers. These latent variables are used to bias and scale the normal-ized hiddens of the corresponding module layers in the decoder.
A separate encoder (reference encoder) has been used in [96] to extract speaker-related information besides the prosody encoder (extractor) that encodes prosody fea-tures into the prosody embedding. A prosody predictor is also trained to predict the prosody embedding based on the phoneme-embedding. While the instance nor-malization (IN) layer is utilized by the prosody extractor to remove global (speaker) information and to keep pros-ody-related information, the speaker encoder is designed with a special structure (Conv2D layers, residual blocks(GLU with fully connected layers), and a multi-head self-attention unit) for better extraction of speaker informa-tion. Moreover, instead of concatenation or summation with the decoder input, the speaker embedding is adap-tively affine transformed to the different FFT blocks of the decoder through a Speaker-Adaptive Linear Modu-lation (SALM) network that is inspired by Feature-wise Linear Modulation (FiLM) [141]. The speaker encoder and conditioning of decoder blocks with speaker embed-ding allow the model to generate natural speech for unseen speakers with only a single reference sample(zero-shot).The attention unit used in seq2seq TTS models aims at mapping the different length between text and audio pairs. However, it can get unstable when the input is not seen during training [97]. The STYLER model has addressed this issue by using a linear compression or expansion of the audio to match the text’s length via a method named Mel Calibrator. With this simplification of the alignment process as a scaling method, the unseen data robustness issue is alleviated and all audio-related style factors become dependent only on the audio.
Similarly, in [119], the Householder Normalizing Flow[169] is incorporated into the VAE-based baseline model[77]. The Householder normalizing flow applies a series of easily invertible affine transformations to align the VAE’s latent vectors (style embeddings) with a full covari-ance Gaussian distribution. As a result, the correlation among the latent vectors is improved. Generally, this architecture enhances the disentanglement capability of the baseline model and enables it to generate embedding for unseen style with just a single (one-shot) utterance of around one second length.
The Multi-SpectroGAN TTS model proposed in [98]is a multi-speaker model trained based on adversarial feedback. The model supports the generation of speech for unseen styles/speakers by introducing adversarial style combination (ASC) during the training process.
Style combinations result from mixing/interpolating style embeddings from different source speakers. The model is then trained with adversarial feedback using mixed-style mel-spectrograms. Two mixing methods are employed:binary selection or manifold mix-up via linear combina-tion. This training strategy enables the model to generate more natural speech for unseen speakers.

> Lastly, recent TTS models based on in-context learning ([NaturalSpeech2 (2022)](../2022.03_NaturalSpeech/2023.04_NaturalSpeech2.md), [VALL-E (2023)](../2023.01_VALL-E/2023.01_VALL-E.md), [Voicebox](../2023.06_VoiceBox/2023.06_VoiceBox.md)) all share the capability to perform zero-shot speech synthesis, as explained in Section 4.4. In fact, the in-context training strategy underlies the ability of these models to synthesize speech given only a style prompt with the input text. Specifically, the synthesis process treats the provided prompt/reference as part of the desired output speech. Therefore, the model’s goal is to predict the rest of this speech in the same style as the given part (prompt) and with the input text. In Table 5 we list papers addressing each challenge.

## 6.数据集与开源代码

## 7.评价指标

## 8.讨论

## 9.结论

## R.参考文献

- 019 [<a id="S.Jo2023">Cross-Speaker Emotion Transfer by Manipulating Speech Style Latents]()
- 032 [<a id="T.Li2021">Controllable Emotion Transfer for End-to-End Speech Synthesis</a>]()
- 034 [<a id="T.Li2022">Cross-Speaker Emotion Disentangling and Transfer for End-to-End Speech Synthesis</a>]()
- 047 [<a id="C.Lu2021">Multi-Speaker Emotional Speech Synthesis with Fine-Grained Prosody Modeling</a>]()
- 074 [<a id="R.Skerry2018">Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron</a>]()
- 090 [<a id="Y.Ganin2016">Domain-Adversarial Training of Neural Networks</a>]()
- 097 [<a id="K.Lee2021">STYLER: Style Factor Modeling with Rapidity and Robustness via Speech Decomposition for Expressive and Controllable Neural Text To Speech</a>]()
- 102 [<a id="K.Zhang2022">Joint and Adversarial Training with ASR for Expressive Speech Synthesis</a>]()
- 111 [<a id="D.Tan2020">Fine-Grained Style Modeling, Transfer and Prediction in Text-to-Speech Synthesis via Phone-Level Content-Style Disentanglement</a>]()