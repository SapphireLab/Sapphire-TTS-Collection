# Recent Advances in Discrete Speech Tokens: A Review

<details>
<summary>基本信息</summary>

- 标题: "Recent Advances in Discrete Speech Tokens: A Review"
- 作者:
  - 01 Yiwei Guo
  - 02 Zhihan Li
  - 03 Hankun Wang
  - 04 Bohan Li
  - 05 Chongtian Shao
  - 06 Hanglei Zhang
  - 07 Chenpeng Du
  - 08 Xie Chen
  - 09 Shujie Liu
  - 10 Kai Yu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.06490)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv](2025.02.10__2502.06490v2__Recent_Advances_in_Discrete_Speech_Tokens_A_Review.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation.
These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures.
Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches.
This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types.
Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.

</td><td>

在大语言模型时代, 语音生成技术的迅速发展使得离散语音 Token 成为语音表示的基础范式.
这些 Token 具有离散, 紧凑和简洁的特点, 不仅有利于高效的传输和存储, 而且与语言建模框架本质上兼容, 能够实现语音与文本主导的LLM架构的无缝集成.

目前的研究将离散语音 Token 分为两类：声学 Token 和语义 Token, 每一类都已经发展成了具有独特设计理念和方法论的丰富研究领域.

本综述系统地总结了现有的分类法和离散语音 Token 化的最新创新, 批判性地分析了每种范式的优缺点, 并对不同 Token 类型进行了系统的实验比较.

此外, 我们还识别了该领域持续存在的挑战, 并提出了潜在的研究方向, 旨在为未来离散语音 Token 的开发和应用提供可行的见解, 以激发未来的进展.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

The rapid advancement of large language models (LLMs) in natural language processing has revolutionized speech generation tasks[^cui2024recent], [^ji2024wavchat], with speech being tokenized and modeled using decoder-only Transformers[^transformer].
Efforts starting from GSLM[^lakhotia2021generative] and AudioLM[^borsos2023audiolm] aim to develop text-free spoken LLMs, akin to how current LLM-powered chatbots enable text-based interactions.
Other works, including VALL-E[^valle] and VioLA[^wang2024viola], extend this approach to conditional speech generation tasks, such as zero-shot text-to-speech and speech translation.
However, this paradigm requires data to be tokenized, as LLMs typically process discrete data only.
Textual tokens naturally meet this requirement because they are designed as discrete units separated by clear boundaries, whereas raw speech signals are continuous and boundary-less.
Therefore, a necessary step before applying speech data to LLM is the \textbf{tokenization of speech}, whose goal is:

</td><td>

</td></tr>
<tr><td>

*To convert long speech waveforms into short discrete tokens for downstream tasks. These tokens should be compatible with the underlying textual representations, especially for language modeling approaches targeted at speech.*


</td><td>

</td></tr>
<tr><td>

As a result, significant efforts have been directed towards developing efficient and powerful speech tokenization methods. Generally, these methods are based on two distinct principles, giving rise to two types of speech tokens: acoustic tokens and semantic tokens.
Acoustic tokens are derived from neural codecs designed to encode speech at a low bitrate while preserving as much information as possible. In contrast, semantic tokens originate from speech self-supervised learning (SSL)[^mohamed2022self], which aims to learn a more phonetic or semantic representation space, making it easier for speech recognition.
These two nearly independent lines of research magically intersect in the context of language modeling for speech.
Now, there are also efforts that try to design a speech tokenizer that accomplishes the two objectives simultaneously[^zhang2024speechtokenizer], [^kyutai2024moshi].

Consequently, speech tokenization has become a core problem of speech processing under the new paradigm, with versatile downstream applications, as shown in Figure.01.

</td><td>

</td></tr>
<tr><td>

Despite the rapid development and numerous recent works, a comprehensive taxonomy of methodologies in discrete speech tokens has not been clearly constructed.
Existing reviews[^mohamed2022self], [^anees2024speech], [^wu2024towards], [^cui2024recent], [^kim2024neural], [^ji2024wavchat] in this field overlook the diverse categories and methodologies in both acoustic and semantic tokens.
- For example, [^cui2024recent], [^ji2024wavchat] focus primarily on methods in spoken language modeling, providing only brief descriptions of some speech tokens used in existing models.
- The taxonomy of neural audio codecs has been summarized in [^wu2024towards], [^du2025codecfake], but the realm of semantic tokens is still overlooked.

</td><td>

</td></tr>
<tr><td>

In this review, we provide a comprehensive overview of the concepts, methods, and characteristics of various types of discrete speech tokens, with their applications in spoken language understanding, speech generation, and spoken dialogue models.
We hope that through this review, the community can have a clear understanding of the current development and key technologies of discrete speech tokens, so as to promote further research in the future.

</td><td>

</td></tr>
<tr><td>

Our contributions are summarized as follows:
- This review is the first to focus specifically on discrete speech tokens with sufficient depth in the LLM era.
- We construct a comprehensive taxonomy of current research on discrete speech tokens and meticulously review the motivation, representative approaches, and challenges in each sub-category.
- We provide a unified comparison of different types of discrete speech tokens in terms of reconstruction and voice conversion performance, covering both acoustic and semantic tokens.
- We summarize the current challenges and potential future directions for discrete speech tokens, including decoupled and variable frame rate tokens.

The structure of this review is shown in Figure.02.

</td><td>

</td></tr>
<tr><td>

Following [^borsos2023audiolm], [^kharitonov2023speak], [^yang2024towards], we classify discrete speech tokens into acoustic and semantic tokens based on their principles.
We will characterize the two types of tokens both by conceptual descriptions and unified experimental comparisons.

</td><td>

</td></tr></table>

## 2·Pre-Requisites: Discrete Representation Learning

<table><tr><td width="50%">

Discrete speech tokens are obtained through the quantization of continuous representations, which is usually achieved by offline clustering or online vector quantization algorithms.
This section provides a concise overview of the existing quantization methods commonly used in discrete speech tokens.

Denote $\bm x\in \mathbb R^d$ as a vector in the $d$-dimensional continuous space. A quantization process $q$ transforms $\bm x$ into a discrete \textit{token} in a finite set, i.e. $q(\bm x): \mathbb R^d\to\{1,2,...,V\}$ where $V$ is the \textit{vocabulary size}.
The output tokens are sometimes referred to as \textit{indexes} in the finite $V$-cardinal set.
The function $q$ is usually associated with a \textit{codebook} $\mathcal C=\{\bm c_1,\bm c_2,...,\bm c_V\}$ where every \textit{code-vector} $\bm c_i\in\mathbb R^d$ corresponds to the $i$-th token.
The code-vectors are representations of tokens in the original $d$-dimensional space.
As $V$ elements can be encoded using $\lceil \log_2 V\rceil$ raw bits\footnote{We use $\lceil z\rceil$ to denote the ceiling of a scalar $z$, i.e., the smallest integer greater than or equal to $z$.
Similarly, $\lfloor z\rfloor$ denotes the floor of $z$, i.e., the largest integer less than or equal to $z$.}, quantization often compresses the cost for data storage and transmission to a great extent.

</td><td>

</td></tr></table>

### Offline Clustering

<table><tr><td width="50%">

Clustering is a simple approach for quantization.
Given a dataset $X=\{\bm x_1,\bm x_2,...\bm x_N\}$, a clustering algorithm aims to assign each sample $\bm x_i$ to a group such that some cost is minimized.
The most frequently used clustering method for discrete speech tokens is k-means clustering[^IKOTUN2023178], e.g. in GSLM[^lakhotia2021generative].
K-means is a clustering algorithm based on Euclidean distances.
Its training process iteratively assigns each data sample to the nearest centroid, and moves cluster centroids till convergence, with a pre-defined number of clusters.
After training, the centroids form the codebook, and new data can be quantized to the index of the nearest centroid in this Voronoi partition.
In practice, centroids are usually initialized with the k-means++ algorithm[^kmeans++] for better convergence.

Hierarchical agglomerative clustering has also been used in discrete speech tokens, which iteratively merges the closest clusters.
It is usually applied after k-means to reduce the number of clusters[^cho2024sd], [^baade2024syllablelm].
Other clustering algorithms are less explored in the context of discrete speech tokens.

</td><td>

</td></tr></table>

### Vector Quantization

<table><tr><td width="50%">

Clustering is often an isolate process, thus cannot be optimized together with other neural network modules.
Instead, vector quantization (VQ)[^gray1984vector] enables a learnable network module that allows gradients to pass through when producing discrete representations.
Autoencoders with a VQ module is termed VQ-VAE[^VQVAE].
There are multiple VQ methods:

</td><td>

</td></tr>
<tr><td>

#### K-Means VQ

Like k-means clustering, k-means VQ method finds the code-vector closest to the input, i.e.

$$
    q(\bm x)=\underset{i\in \{1,2,...,V\}}{\arg\min} \|\bm x-\bm c_i\|^2.
$$

Then, code-vector $\bm c_k\triangleq\bm c_{q(\mathbf x)}$ is fed to subsequent networks.
As the $\min$ operation is not differentiable, straight-through estimators (STEs)[^bengio2013estimating] are usually applied to graft gradients, i.e. $\operatorname{STE}(\bm c_k,\bm x)=\bm x+\operatorname{sg}(\bm c_k-\bm x)$ where $\operatorname{sg(\cdot)}$ stops tracking gradients.
In this way, the input value to subsequent networks is still $\bm c_k$, but gradients are grafted to $\bm x$ in back propagation.

Auxiliary loss functions are often used together with k-means VQ[^VQVAE]: commitment loss $\mathcal L_{\text{cmt}}=\|\operatorname{sg}(\bm c_k)-\bm x\|^2$ and codebook loss $\mathcal L_{\text{code}}=\|\operatorname{sg}(\bm x)-\bm c_k\|^2$.
The commitment loss pushes the continuous input $\bm x$ towards the closest codebook entry, while the codebook loss does the opposite and updates the code-vector $\bm c_k$.
The two loss terms are weighted by different factors to put different optimization strengths on $\bm x$ and $\bm c_k$, as pushing $\bm c_k$ towards $\bm x$ is an easier task.
It is also common to replace $\mathcal L_{\text{code}}$ with exponential moving average (EMA) to update the codebook instead[^razavi2019generating], which does not rely on explicit loss functions.

VQ in high-dimensional spaces is known to suffer from codebook collapse,  where the codebook usage is highly imbalanced[^lancucki2020robust], [^dhariwal2020jukebox].
To improve the utilization of codebook, random replacement (as known as \textit{codebook expiration}) can be applied[^dhariwal2020jukebox] on code-vectors that have remained inactive for a long time.
Other solutions include additional auxiliary constraints such as entropy penalty[^chang2022maskgit], [^yu2024language], factorized codebook lookup in low-dimensional space[^yu2022vectorquantized], and adding a linear projection to update all code-vectors together[^zhu2024addressing].

</td><td>

</td></tr>
<tr><td>

#### Gumbel VQ

Instead of quantizing by Euclidean distance, another choice is by probability.
Gumbel VQ[^jang2017categorical] uses Gumbel-Softmax as a proxy distribution for traditional Softmax to allow differentiable sampling.
Given input $\bm x$ and a codebook of size $V$, a transform $h(\cdot)$ is applied on $\bm x$ into $V$ logits: $\bm l=h(\bm x)\in \mathbb R^V$.
In inference, quantization is performed by choosing the index with the largest logit, i.e. $q(\bm x)=\arg\max_i \left\{\bm l^{(i)}\right\}$.
In training, samples are drawn from the categorical distribution implied by $\bm l$ for the subsequent neural networks.
To achieve efficient sampling and let gradients pass through, Gumbel trick is used:

$$
\begin{aligned}
    &\bm u\in \mathbb R^V\sim \operatorname{Uniform}(0, 1),\bm v=-\log(-\log(\bm u)) \\
    &\bm s=\operatorname{Softmax}((\bm l+\bm v)/\tau)
\end{aligned}
$$

where Eq.\eqref{eq:gumbel-noise} samples Gumbel noise $\bm v$ element-wise, and Eq.\eqref{eq:gumbel-softmax} calculates Gumbel-Softmax distribution $\bm s$ with a temperature $\tau$.
The forward pass simply use $j=\arg\max_i \{\bm s^{(i)}\}$ as the sampled index, but the true gradient of Gumbel-Softmax is used in backward pass.
In other words, the gradient on the one-hot distribution corresponding to $j$ is grafted to $\bm s$ as an approximate.
The temperature $\tau$ balances the approximation accuracy and gradient variances.
The transform $h(\cdot)$ is usually parameterized as neural networks, or negatively proportional to Euclidean distances[^jiang2023latent].

After quantization, code-vector $\bm c_k$ with $k=q(\bm x)$ is fed to subsequent networks.
Gumbel VQ does not require additional losses, since code-vectors can be directly learned with gradients and do not need to be pushed towards $\bm x$.

</td><td>

</td></tr>
<tr><td>

#### Finite Scalar Quantization

As mentioned before, VQ methods based on code-vector assignment usually suffer from codebook collapse.
Despite many efforts, this remains a crucial challenge.
Finite scalar quantization (FSQ)[^mentzer2024finite] is an alternative to perform quantization in scalar domain.
FSQ quantizes each dimension of a vector $\bm x$ into $L$ levels.

For the $i$-th dimension $\bm x^{(i)}$, FSQ transforms the values to into limited range and then rounds to integers, i.e.

$$
    q\left(\bm x^{(i)}\right)=\operatorname{round}\left(\lfloor L/2\rfloor\tanh\left(\bm x^{(i)}\right)\right).
$$

The quantized values are thus integers ranging from $-\lfloor L/2 \rfloor$ to $\lfloor L/2 \rfloor$\footnote{Following [^mentzer2024finite], this is the symmetric case for $L$ being odd. When $L$ is even, there is an offset before rounding to obtain asymmetric quantized values.}.
For a $d$-dimensional vector $\bm x$, there are $L^d$ possible quantization outcomes.
Hence, FSQ usually requires a much smaller hidden dimension than VQ.
STE is applied to pass gradients.
As quantization is simply done via rounding to integers, there is no explicit codebooks associated with the FSQ process.
FSQ is reported to have  better codebook usage\footnote{Although there is no longer a codebook associated with code-vectors, codebook usage can still be measured among all possible $V=L^d$ outcomes.} especially for a large $V$ compared to VQ methods, without the need for auxiliary losses.

</td><td>

</td></tr>
<tr><td>

#### Other VQ Tricks

In many cases, a single VQ module suffers from a highly-limited representation space, thus results in poor performance compared to continuous counterparts.
There are some widely-used VQ tricks that introduce multiple quantizers to refine the quantized space, as shown in Figure.03:

- **Grouped VQ (GVQ)**, also known as **product quantization**[^product_quantization]. It groups the input vector $\bm x$ by dimensions and apply VQ on different parts of $\bm x$ independently. They can have different or shared codebooks. The VQ outputs are then concatenated along dimensions to match that of $\bm x$. For instance, GVQ is used in neural word embeddings[^9164982] and speech self-supervised learning models[^vq-wav2vec], [^baevski2020wav2vec] to achieve efficient quantization.
- **Residual VQ (RVQ)**, also known as **multi-stage quantization**[^multiple-stage-vector-quantization]. It adopts a serial approach that iteratively quantizes the residual of the last quantizer.
Similar to GVQ, RVQ also has multiple quantizers.
For the $i$-th quantizer $q_i$ with input $\bm x_i$ and output code-vector $\bm c_{k}$, the residual is defined as $\bm x_{i+1}=\bm x_i-\bm c_{k}$.
The outputs from all $q_i$ are finally summed as the quantized result of $\bm x$.
In this way, information in the codebooks is supposed to follow a coarse-to-fine order, and more details in the original $\bm x$ can be preserved than a plain quantizer.
It is used in various speech codecs[^zeghidour2021soundstream], [^encodec], [^kumar2024high], for instance.

GVQ and RVQ can be flexibly combined to form GRVQ[^yang2023hifi] that applies RVQ on each GVQ branch for better codebook utilization.
A network can also contain multiple VQ modules at different places, like cross-scale VQ (CSVQ)[^jiang22_interspeech] where every decoder layer has a quantizer inside.

Note that RVQ naturally produces an order of importance in residual layers, while all quantizers in GVQ are equally important.
Such order of importance can also be enforced in GVQ by a "nested dropout" trick[^rippel2014learning].

</td><td>

</td></tr></table>

## 3·Speech Tokenization Methods: Acoustic Tokens

<table><tr><td width="50%">

Acoustic Tokens, also known as Speech Codecs, refer to the discrete representations optimized mainly for signal compression and reconstruction.
The audio codec technology arises long ago.
Traditional codecs, including MP3[^rfc5219], Opus[^Valin2012DefinitionOT] and EVS[^dietz2015overview], typically take advantage of signal processing algorithms to improve quality and lower the bitrate.

In the deep learning era, numerous codec models based on neural networks have emerged.
These models typically consist of an encoder that compresses speech signals and a decoder that reconstructs the speech signals, with a quantizer situated between the two.
The quantizer is also parameterized and jointly trained with the whole network in an end-to-end manner.
The codebook indices produced by the quantizer are referred to as acoustic tokens.
To improve the representation ability of discrete VQ spaces and thus obtain better codec performance, RVQ, GVQ, GRVQ and FSQ tricks are commonly applied in the quantization module.

We list the VQ method, number of quantizers $Q$, frame rate $F$, vocabulary size $V$ for each quantizer, and the resulting bitrate of existing neural acoustic speech tokens in Table.\ref{tab:acoustic-metadata}.

</td><td>

</td></tr></table>

### Model Architectures

<table><tr><td width="50%">

Although acoustic codec models differ from one to one regarding their purposes, most of them share a similar encoder-quantizer-decoder framework.
With audio clip $\bm x$ that can either be time-domain sampling points, frequency-domain features or even other machine learning features, an encoder $f_\theta(\cdot)$ transforms it to $f_\theta(\bm x)$ in a continuous latent vector space.
The encoder $f_\theta(\cdot)$ will usually perform downsampling to reduce the temporal length of the input signals, especially for waveform inputs.
A VQ module $q_\phi(\cdot)$ discretizes $f_\theta(\bm x)$ into tokens and corresponding codebook vectors $\bm c$.
A decoder $g_\psi(\cdot)$ then uses $\bm c$ to reconstruct $\hat {\bm x}$, and a certain distance metric of $d(\bm x, \hat{\bm x})$ is usually optimized.
There are two major paradigms for designing the encoder, decoder, and quantizers, which can be summarized as diagrams in Fig.\ref{fig:acoustic-paradigms}.

</td><td>

</td></tr>
<tr><td>

#### VQ-GAN

VQ-GAN[^esser2021taming] is a very commonly adopted framework of acoustic tokens that trains a VQ-VAE with GAN objectives.
Besides the original reconstruction and VQ objectives in a VQ-VAE, VQ-GAN uses discriminators $d_\xi(\bm x, \hat{\bm x})$ to distinguish real and reconstructed data that adversarially train the generator network composed of $f_\theta,q_\phi$, and $g_\psi$. In acoustic tokens, there are usually multiple discriminators, e.g. multi-resolution and multi-scale STFT discriminators from the neural vocoder researches[^kumar2019melgan], [^jang21_interspeech].
The generator architecture of VQ-GAN-based acoustic tokens has multiple choices, with the three most representative ones visualized in Fig.\ref{fig:generator}: CNN-based, Transformer-based, and U-Net-based.

The CNN-based generator is the most widely used architecture so far in acoustic tokens.
SoundStream[^zeghidour2021soundstream] and EnCodec[^encodec] are two famous early neural acoustic tokens that operate in an end-to-end VQ-GAN manner.
They receive time-domain waveforms as inputs and directly reconstruct waveforms.
Their encoder and decoder have a mirrored architecture to perform down and up-samplings.
In SoundStream, the encoder and decoder are purely constructed by convolutional neural networks (CNNs) while EnCodec augments them with an LSTM.
The CNN encoder down-samples the waveform to a high-dimensional embedding sequence, whose frame rate is determined by the sampling rate, CNN kernel sizes and strides at a fixed ratio.
The continuous embeddings are passed to an RVQ quantizer, and the quantized vectors are summed before being transformed to the waveform domain by the CNN decoder.
The training criteria include reconstruction loss (in the time and frequency domain), adversarial loss, feature matching loss, and quantization losses for RVQ layers.
To allow for a flexible choice of bitrates, structured dropout is adopted where the number of codebooks in the RVQ module can be randomly chosen[^zeghidour2021soundstream], such that only a portion of quantizers in front are activated during training.
The acoustic tokens can consequently reside in variable bitrates depending on the chosen number of RVQ quantizers.
The inputs and outputs of the codec model can also be frequency-domain features like magnitude and phase spectra for reducing computation burden[^du2024funcodec].
There, the convolution kernels are typically 2D instead of 1D in the time-domain codecs.

Later, Transformers[^transformer] have been adopted, e.g. in Single-Codec[^singlecodec] and Mimi[^kyutai2024moshi].
They can be directly applied to frequency-domain inputs and outputs.
When operating on waveform-domain inputs or outputs, a CNN[^kyutai2024moshi] or patchifying[^wu2024ts3codectransformerbasedsimplestreaming], [^parker2024scalingtransformerslowbitratehighquality] operation is usually added before or after the Transformer blocks.
In Mimi, a shallow Transformer layer is added after the CNN-based encoder, and vice versa in its decoder.
Recently, some propose to use purely Transformer-based backbone and discard the CNN blocks, e.g. TS3-Codec[^wu2024ts3codectransformerbasedsimplestreaming].
As Transformers demonstrate superior modeling ability and scaling property, these works prove to outperform CNN-based codecs either with less computation[^wu2024ts3codectransformerbasedsimplestreaming] or larger scale[^parker2024scalingtransformerslowbitratehighquality].
However, to ensure stream-ability, an attention mask should be employed[^kyutai2024moshi].
The encoder and decoder can also be designed to be different.
For example, Single-Codec[^singlecodec] uses Conformer[^conformer] encoder and CNN decoder, while LSCodec[^guo2024lscodec] uses the reverse configuration.

Though RVQ or GVQ is usually applied, most acoustic tokens contain only one quantization module as a whole.
However, there are also U-Net-based codecs where multiple quantizers are employed, e.g. CoFi-Codec[^guo2024speaking] and ESC[^gu2024esc].
Each sub-encoder or decoder in the U-Net can be a CNN or Transformer.
This offers a more flexible control of the resolution of each VQ stream (Section \ref{sec:multi-resolution}).

It is also noteworthy that training a separate vocoder on top of existing acoustic tokens may result in improved audio quality than the original decoded outputs, since reconstructing waveform alone may be simpler than optimizing VQ representation and reconstruction at the same time.
This is exemplarily verified in AudioDec[^audiodec], MBD[^san2023discrete] and Vocos[^siuzdak2024vocos].
Therefore, some acoustic tokens directly simplify the VQ-GAN training objective back to the original VQ-VAE, where the discrete tokens are obtained first by a simple reconstruction loss, and a vocoder is trained as an additional stage, like AudioDec[^audiodec] and LSCodec[^guo2024lscodec].
These works are denoted as ``VQ-VAE+GAN'' in Table \ref{tab:acoustic-metadata}.

#### Latent Diffusion

Different from VQ-GAN which uses GAN to generate waveforms or frequency features, some codecs also use latent diffusion[^ho2020denoising], [^song2021scorebased], [^rombach2022high] as an alternative.
These codecs use discretized tokens as a condition to generate some latent acoustic space, e.g. from a pretrained continuous speech autoencoder.
Since diffusion models are strong generative models, acoustic tokens of this type does not need discriminators and adversarial training like VQ-GAN.
For instance, LaDiffCodec[^yang2024generative] uses EnCodec tokens to condition the diffusion process from Gaussian noise to the latent space in a pretrained and frozen waveform autoencoder.
This is to bridge the gap of reconstruction quality between discrete and continuous representations and improve the codec performance compared to the original acoustic tokens.
Inference efficiency is a major concern of these models unless specifically optimized in limited sampling steps.

</td><td>

</td></tr></table>

### General-Purpose Acoustic Tokens

#### Motivation

<table><tr><td width="50%">

In this section, we describe the most common type of neural acoustic tokens (speech codecs) that are designed only with the objective of speech signal reconstruction.
Those acoustic tokens are optimized towards better signal or perceptual quality under bitrates as low as possible.

</td><td>

</td></tr></table>

#### Approaches

<table><tr><td width="50%">

##### Advanced VQ methods and model architectures

Based on SoundStream and EnCodec, more codecs with advanced VQ methods, network structure, or optimization strategies have been researched with depth.
As an example, DAC[^kumar2024high] achieves remarkable reconstruction quality by adding periodic inductive bias, better discriminators, modified loss functions, and a better VQ mechanism from ViT-VQGAN[^yu2022vectorquantized] to improve codebook usage.
Specifically, it performs L2-normed code lookup in a low-dimensional space (e.g. 8 or 32) instead of a high-dimensional space like 1024.
Other architectural improvements include using frequency-domain inputs[^APCodec], [^ai24b_interspeech], [^singlecodec], variance-constrained residual blocks[^ahn2024hilcodec], multi-filter bank discriminator[^ahn2024hilcodec], selective down-sampling back-projection[^zheng2024supercodec], etc.

Several training tricks are explored, such as not applying VQ with a certain probability and pure adversarial training proposed in Moshi[^kyutai2024moshi].
Also, the training of neural speech codecs does not need to be end-to-end, i.e. the learning of VQ representations and signal reconstruction can be separated.
[^audiodec], [^du2024apcodec+] adopt a two-stage training process that introduces adversarial losses and an additional vocoder after training only with metric losses, to achieve improved quality.
Additional training criteria around the VQ module are proposed for better VQ utilization, such as delicate code-vector replacement strategy, codebook balancing loss, and similarity loss between consecutive RVQ layers proposed in ERVQ[^zheng2024ervq].

Other VQ methods besides GVQ or RVQ also exist in speech codecs.
NDVQ[^niu2024ndvq] improves the capacity of RVQ space by changing codebook {vectors} to parameterized Gaussian {distributions}.
FSQ has also been introduced to several speech codecs, like SQ-Codec[^yang24l_interspeech] where scalar rounding is applied to each of its 32-dimensional latent space.
Stable-Codec[^parker2024scalingtransformerslowbitratehighquality] adopts FSQ in a Transformer-based architecture, exhibiting strong scalability to large model sizes up to 950M parameter count.
It also explores a flexible post-training quantization level adjustment technique and residual FSQ strategy.

Note that most acoustic tokens require multiple quantizers, but \textbf{single-codebook} codecs have also been explored.
Single-Codec[^singlecodec] designs an encoder consisting of Conformer and bidirectional LSTM to better compress mel spectrogram inputs.
WavTokenizer[^ji2024wavtokenizer] and BigCodec[^xin2024bigcodec] further explores single-codebook codec modeling with better network designs or larger parameter count.
TS3-Codec[^wu2024ts3codectransformerbasedsimplestreaming] adopts a fully Transformer design that leads to a better single-codebook codec with fewer computation overhead.
LSCodec[^guo2024lscodec] also achieves single-codebook coding with speaker disentanglement (Section \ref{sec:acoustic-disen}).
These single-codebook codecs with remarkably low bitrates offer great benefit to downstream speech generation models on simplicity and efficiency.

</td><td>

</td></tr>
<tr><td>

##### Temporal Redundancy Reduction

Instead of capturing all the information through VQ layers like the previously mentioned codecs, some researchers have attempted to reduce the redundant bitrate of time-varying VQ codes.
One reasonable method is to encode the global information in speech, e.g. speaker timbre and channel effects, by a global encoder instead of the time-varying codes.

Disen-TF-Codec[^jiang2023disentangled] is the first to explore VQ-GAN codec models with an additional global encoder that aids the codec decoder.
In Disen-TF-Codec, the global features are designed to be sequential to adapt to speaker changes during transmission.
In TiCodec[^ticodec], the global tokens are time-invariant and vector-quantized instead.
They are extracted from different segments of an utterance in conjunction with time-varying tokens.

Similar global encoders are also seen in [^guo2024socodec], [^guo2024speaking], [^singlecodec].
FreeCodec[^zheng2024freecodecdisentangledneuralspeech] further incorporates a prosody encoder[^ren2022prosospeech] that compresses the low-frequency range of mel spectrograms into a low frame rate VQ sequence to assist in reconstruction.

Another typical example of temporal redundancy reduction is predictive coding, as seen in TF-Codec[^jiang2023latent].
This approach captures temporal-varying information in the latent space by autoregressive prediction, which significantly reduces redundancy and entropy in the residual part for  quantization.
LMCodec[^LMCodec] employs autoregressive prediction from coarse codes (first RVQ levels) to fine codes (last RVQ levels)[^borsos2023audiolm], enabling the transmission of fewer codes.

</td><td>

</td></tr>
<tr><td>

##### Multi-Resolution and Variable-Bitrate Coding

Rather than relying solely on uni-resolution tokens, where all quantizers share the same temporal frequency, it is reasonable to design multi-resolution codecs, because speech contains both fast and slow information streams.
For instance, many vowels exhibit slowly changing characteristics, while events such as explosive consonants and background noises require fine-grained modeling. Therefore, incorporating multiple temporal resolutions in codecs is likely to reduce the necessary bitrate.
SNAC[^Siuzdak_SNAC_Multi-Scale_Neural_2024] is a notable multi-resolution acoustic token.
It follows the DAC[^kumar2024high] architecture, but in each RVQ layer, residuals are downsampled before codebook look-up and upsampled afterward.
This enables SNAC to have three RVQ streams at a frame rate of 12, 23, 47Hz respectively.
Similarly, CoFi-Codec[^guo2024speaking] achieves multi-resolution coding by GVQ quantizers within its U-Net-based architecture.
LLM-Codec[^yang2024uniaudio15] also adopts this idea to achieve very low frame rates with semantic distillation (Section \ref{sec:acoustic-distillation}).

In addition to multiple temporal resolutions, it is also feasible to consider the varying information intensities across different speech frames.
This observation motivates the design of codecs to allocate different numbers of quantizers for different speech frames.
As an example, VRVQ[^chae2024variable] automatically selects the number of RVQ quantizers per frame by a predictor that is jointly trained with the whole network.

</td><td>

</td></tr></table>

#### Challenges

<table><tr><td width="50%">

Despite the emergence of single-codebook and low-bitrate codecs[^singlecodec], [^ji2024wavtokenizer], [^xin2024bigcodec], [^guo2024lscodec], achieving ideal reconstruction quality with a highly limited VQ space remains a challenging problem.

Additionally, as acoustic tokens aim to encode all necessary information for signal recovery, they may become redundant and overly complex for downstream modeling.
While scaling up the model size or switching to non-causal networks has been shown to improve performance[^singlecodec], [^xin2024bigcodec], [^parker2024scalingtransformerslowbitratehighquality], these approaches may also compromise streamability or efficiency.
Furthermore, simply introducing global encoders like [^jiang2023disentangled], [^ticodec], [^guo2024speaking] does not guarantee disentanglement (Section \ref{sec:acoustic-disen}) and may still result in redundancy within the time-varying codes.

</td><td>

</td></tr></table>

### Acoustic Tokens with Semantic Distillation

#### Motivation

<table><tr><td width="50%">

Acoustic tokens are a convenient choice for spoken language models, as they can be directly converted back to waveforms without the need for extra vocoders.
However, if reconstruction is the sole objective of these tokens, their representation space may become overly complex and overly focused on acoustic details, in contrast to natural language tokens that primarily carry semantic information.
A natural improvement is to incorporate speech semantic features either from speech self-supervised learning (SSL) models, supervised models, or even text transcriptions.
Since speech SSL models aim to capture high-level phonetic or semantic information without external supervision[^mohamed2022self], integrating SSL features does not impose additional data requirements for injecting semantic information into the training process.
Acoustic tokens with criteria beyond reconstruction are sometimes referred to as having a ``mixed objective''[^cui2024recent].
Given that the primary purpose of these models remains acoustic reconstruction in these models, we continue to refer to them as acoustic tokens.
The process of introducing semantic information into acoustic tokens is termed \textbf{semantic distillation}, with approaches summarized in Fig. \ref{fig:acoustic-distill}.

</td><td>

</td></tr></table>

#### Approaches

<table><tr><td width="50%">

##### Semantic feature guidance

The earliest effort in semantic distillation is to guide some RVQ layers in acoustic tokens towards semantic features, which are typically SSL features.
Since information in RVQ naturally follows a coarse-to-fine order, guiding early RVQ layers towards semantic-oriented features helps establish and reinforce a semantic-to-acoustic information hierarchy.
For example, SpeechTokenizer[^zhang2024speechtokenizer] uses a HuBERT[^hsu2021hubert] SSL model to guide the first RVQ layer in EnCodec.
This ensures that the first RVQ layer contains more semantic information, thereby pushing acoustic details to the subsequent RVQ layers.
This distillation is implemented either by regressing the first RVQ output to continuous HuBERT embeddings or by classifying it into discrete HuBERT tokens.
LLM-Codec alternatively uses Whisper[^whisper] and T5[^raffel2020exploring] as semantic teachers.
Mimi[^kyutai2024moshi] uses a WavLM[^chen2022wavlm] teacher and applies distillation to a specialized VQ module rather than the first RVQ layer.
Since SSL feature guidance occurs only during the training stage, it does not incur additional inference costs.
It has been reported that TTS language models trained with such acoustic tokens exhibit better robustness than those with unguided tokens[^zhang2024speechtokenizer].

</td><td>
</td></tr>
<tr><td>

##### Fixed Semantic Codebook

A more direct approach to achieve semantic distillation is to integrate semantic knowledge into the codebook of quantizers.
This forces the quantization space itself to be more semantic-related.
This method is proposed in LLM-Codec[^yang2024uniaudio15] where all three RVQ codebooks are initiated from the token embedding module of LLaMa-2[^touvron2023llama2] and remain frozen during training.
This approach not only reduces the bitrate of the codec but also significantly enhances the semantic representation ability of LLM-Codec.

</td><td>

</td></tr>
<tr><td>

##### Semantic Features as Inputs or Outputs

Semantic features can also be compressed together with the acoustic features.
This requires the encoder and quantizer to construct a shared acoustic and semantic space that balances the two information sources.
The first attempt in this direction is made in [^siahkoohi22_interspeech] where Conformer representations from a pretrained wav2vec 2.0[^baevski2020wav2vec] are combined with CNN encoder outputs for quantization.
SemantiCodec[^liu2024semanticodec] quantizes AudioMAE[^huang2022masked] SSL features without relying on acoustic inputs.
The quantized SSL features then serve as a condition for acoustic reconstruction using latent diffusion, which resembles a vocoder that transforms semantic inputs into acoustic outputs.
Providing aligned phoneme sequences instead of SSL features to the quantizer has also shown benefits on reducing bitrates[^du2024funcodec].

Moreover, semantic features can also serve as outputs, thereby reinforcing the constraint that semantic information be compressed into the discrete latent space.
For instance, [^guo2024socodec], [^ye2024codec] combine hidden HuBERT embeddings with acoustic features before RVQ and jointly optimizes acoustic and semantic reconstruction objectives.
X-Codec 2.0[^ye2025llasa] improves it by using w2v-BERT 2.0[^barrault2023seamless] and FSQ.

</td><td>

</td></tr></table>

#### Challenges


<table><tr><td width="50%">

Guiding part of the RVQ layers towards semantic features does not guarantee that acoustic information is encoded in the remaining layers, as shown by the degraded VC performance in SpeechTokenizer[^zhang2024speechtokenizer].
It may impose a greater challenge for the VQ layer to encode both acoustic and semantic information if semantic features serve as inputs as well.
Additionally, fixing a semantic codebook could negatively impact acoustic reconstruction ability, as the VQ representation space becomes overly restricted.

</td><td>

</td></tr></table>

### Acoustic Tokens with Disentanglement

#### Motivation

<table><tr><td width="50%">

Another line of mixed-objective acoustic tokens is {disentanglement}.
A prominent research direction is the disentanglement of speaker timbre information, as this is a global trait among all the speech information aspects.
Encoding speaker information into every token timestep is redundant; thus, removing the global speaker timbre can make the information in acoustic tokens more compact and reduce the necessary bitrate.
Speaker-decoupled speech tokens can alleviate the modeling burden for downstream tasks. For example, a TTS model using these tokens can achieve independent control over prosody and speaker identity.
The disentanglement of speaker timbre also enables an acoustic token to perform voice conversion (VC), as timbre from the target speaker can be easily combined with the speaker-agnostic content tokens from the source speech.

Note that in Section \ref{sec:acoustic}, it is mentioned that some codecs introduce a global encoder to reduce the necessary bitrate of time-variant tokens[^jiang2023disentangled], [^ticodec], [^singlecodec], [^zheng2024freecodecdisentangledneuralspeech].
They have already demonstrated some ability to decouple global speaker timbre and local contents, albeit in an \textbf{implicit} manner through the natural information bottleneck from VQ.
In this section, we elaborate on \textbf{explicit} methods, which involve specialized training techniques and criteria to achieve disentanglement.

</td><td>

</td></tr></table>

#### Approaches

<table><tr><td width="50%">

##### Gradient reversal layer (GRL)

The GRL technique[^drl] is commonly used for disentanglement. Suppose speaker information needs to be disentangled, and a classifier (or speaker verifier, etc.) $s_\mu(\cdot)$ receives some latent feature $\bm h$ from the acoustic token to perform speaker discriminative tasks.
GRL operates by negating the gradient sign before $s_\mu(\cdot)$, thereby forcing $\bm h$ to fool the speaker classifier while the classifier itself improves, similar to adversarial training.

SSVC[^SSVC] is one of the pioneering efforts in this direction.
SSVC attempts to decouple content and speaker representations from WavLM features.
The content branch is quantized via RVQ, and the speaker branch is trained using a contrastive loss to produce speaker embeddings.
Disentanglement is enforced by a GRL between the speaker embeddings produced from the speaker branch and the content representations.

Similarly, PromptCodec minimizes an SSIM loss[^wang2004image] between content and speaker representations, with the help of a pretrained speaker verification model.

Such GRL technique is not limited to disentangling speaker timbre alone.
FACodec[^facodec] employs supervised decoupling to factorize speech into speaker timbre, content, prosody, and acoustic detail information.
The timbre extractor in FACodec is optimized via a speaker classification loss.

For the other components -- prosody, content, and acoustic detail -- separate RVQ modules are applied prior to the supervised decoupling process.
For each component, some supervision signal with the desired information is applied, and GRL is employed to other non-related information components.

These three quantized features are then combined before applying GRL with the speaker information.
Finally, the decoder integrates all four information branches to reconstruct the speech signal.

</td><td>

</td></tr>
<tr><td>

##### Perturbation

For speaker disentanglement, a more straightforward approach is to apply speaker timbre perturbations to speech signals and leverage the strong information bottleneck created by the discrete VQ module.
When the encoder is unable to learn sufficient timbre information, and the decoder is provided with prominent timbre, the bottleneck in the middle will naturally prevent timbre from being encoded[^qian2019autovc].
This idea is adopted in LSCodec[^guo2024lscodec] to achieve speaker decoupling and ultra-low-bitrate coding.
LSCodec leverages continuous WavLM features to represent speaker timbre.
These features are fed to a Conformer-based decoder by position-agnostic cross attention[^du2024unicats], [^li2024sef].
A stretching-based speaker perturbation algorithm is applied to the input waveform to facilitate speaker disentanglement.
The training process of LSCodec involves multiple stages where a VQ module is injected after constructing a speaker-decoupled continuous space.
Through this approach, LSCodec achieves high-quality speech reconstruction and voice conversion using only a single codebook with very low bitrates.

</td><td>

</td></tr>
<tr><td>

##### Source Separation

Apart from the disentanglement of speaker timbre, source separation has also been explored in the context of acoustic tokens.
SD-Codec[^bie2024learning] proposes to decouple different audio sources in the neural codec, like speech, music, and sound effects, by employing  multiple parallel RVQ modules.
This approach allows for more efficient and targeted processing of each audio component.

</td><td>

</td></tr></table>

#### Challenges

<table><tr><td width="50%">

The GRL technique for disentanglement inherently carries the risk of a more complex optimization trajectory.
Additionally, some disentanglement methods require supervised data[^facodec], which imposes a significant constraint.
Due to the intricate nature of speech informatics, current efforts are still suboptimal compared to semantic tokens, particularly in terms of VC performance[^guo2024lscodec].

</td><td>

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td></tr></table>

## References: 参考文献

[^9164982]:
[^ahn2024hilcodec]:
[^ai24b_interspeech]:
[^anees2024speech]:
[^APCodec]:
[^audiodec]:
[^baade2024syllablelm]:
[^baevski2020wav2vec]:
[^barrault2023seamless]:
[^bengio2013estimating]:
[^bie2024learning]:
[^borsos2023audiolm]:
[^chae2024variable]:
[^chang2022maskgit]:
[^chen2022wavlm]:
[^cho2024sd]:
[^conformer]:
[^cui2024recent]:
[^dhariwal2020jukebox]:
[^dietz2015overview]:
[^drl]:
[^du2024apcodec+]:
[^du2024funcodec]:
[^du2024unicats]:
[^du2025codecfake]:
[^encodec]:
[^esser2021taming]:
[^facodec]:
[^gray1984vector]:
[^gu2024esc]:
[^guo2024lscodec]:
[^guo2024socodec]:
[^guo2024speaking]:
[^ho2020denoising]:
[^hsu2021hubert]:
[^huang2022masked]:
[^IKOTUN2023178]:
[^jang2017categorical]:
[^jang21_interspeech]:
[^ji2024wavchat]:
[^ji2024wavtokenizer]:
[^jiang2023disentangled]:
[^jiang2023latent]:
[^jiang22_interspeech]:
[^kharitonov2023speak]:
[^kim2024neural]:
[^kmeans++]:
[^kumar2019melgan]:
[^kumar2024high]:
[^kyutai2024moshi]:
[^lakhotia2021generative]:
[^lancucki2020robust]:
[^li2024sef]:
[^liu2024semanticodec]:
[^LMCodec]:
[^mentzer2024finite]:
[^mohamed2022self]:
[^multiple-stage-vector-quantization]:
[^niu2024ndvq]:
[^parker2024scalingtransformerslowbitratehighquality]:
[^product_quantization]:
[^qian2019autovc]:
[^raffel2020exploring]:
[^razavi2019generating]:
[^ren2022prosospeech]:
[^rfc5219]:
[^rippel2014learning]:
[^rombach2022high]:
[^san2023discrete]:
[^siahkoohi22_interspeech]:
[^singlecodec]:
[^siuzdak2024vocos]:
[^Siuzdak_SNAC_Multi-Scale_Neural_2024]:
[^song2021scorebased]:
[^SSVC]:
[^ticodec]:
[^touvron2023llama2]:
[^transformer]:
[^Valin2012DefinitionOT]:
[^valle]:
[^vq-wav2vec]:
[^VQVAE]:
[^wang2004image]:
[^wang2024viola]:
[^whisper]:
[^wu2024towards]:
[^wu2024ts3codectransformerbasedsimplestreaming]:
[^xin2024bigcodec]:
[^yang2023hifi]:
[^yang2024generative]:
[^yang2024towards]:
[^yang2024uniaudio15]:
[^yang24l_interspeech]:
[^ye2024codec]:
[^ye2025llasa]:
[^yu2022vectorquantized]:
[^yu2024language]:
[^zeghidour2021soundstream]:
[^zhang2024speechtokenizer]:
[^zheng2024ervq]:
[^zheng2024freecodecdisentangledneuralspeech]:
[^zheng2024supercodec]:
[^zhu2024addressing]:
