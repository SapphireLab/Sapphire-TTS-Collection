# Sec.02·MultiModal Tokenization: 多模态分词

Tokenization is the first and a fundamental step for multimodal sequential modeling under the next token prediction framework.
It decomposes information from various sources, such as images, videos, and audio clips, into a sequence of minimal, manageable units known as tokens for the NTP model to learn.
Table.02 provides an overview of the tokenizers used across various modalities in recent research.

Despite being derived from various modalities, these tokenization methods can all be categorized into two prototypes: **discrete tokenization**  and **continuous tokenization**.
In this section, we will initially introduce the general definition and basics techniques of training multimodal tokenizers ([Section.2.1](#Section.2.1)), then the fundamentals and applications of discrete tokens ([Section.2.2](#Section.2.2), [Section.2.3](#Section.2.3)) and continuous tokens ([Section.2.4](#Section.2.4), [Section.2.5](#Section.2.5)) in NTP framework.

## 2.1·Tokenization of Different Modalities: 不同模态的分词

<a id="Section.2.1"></a>

## 2.2·Discrete Tokenization Basics: 离散分词基础

<a id="Section.2.2"></a>

## 2.3·Discrete Tokenization for Different Modalities: 不同模态的离散分词

<a id="Section.2.3"></a>

## 2.4·Continuous Tokenization Basics: 连续分词基础

<a id="Section.2.4"></a>

## 2.5·Continuous Tokenization for Different Modalities: 不同模态的连续分词

<a id="Section.2.5"></a>
