# 5Â·Datasets & Evaluation

In this section, we delve into several crucial aspects of training and evaluating MMNTP models. The subdivision begins with an exploration of the training datasets (Section~\ref{sec: training_dataset}), categorized into pre-training and fine-tuning datasets. The pre-training datasets are further divided based on modality into text-only, image-based, video-based, and audio-based data, which are essential for modality alignment and the establishment of a unified multimodal representation. Following this, fine-tuning datasets are described, focusing on their specific applications in multimodal understanding and multimodal generation tasks.

Additionally, we discuss the evaluation of MMMNTP models (Section~\ref{sec: evaluation_dataset}), which is pivotal in measuring their effectiveness and capability across various modalities. This aspect is divided into holistic evaluation and emerging evaluation benchmarks. Holistic evaluation benchmarks, such as MME~\citep{fu2023mme} and SEED-Bench~\citep{li2023seedbench}, comprehensively assess the integration and interplay between different modalities like image, text, and video. Emergent benchmarks, including SparklesEval~\citep{huang2023sparkles} and HallusionBench~\citep{guan2024hallusionbench}, push the boundaries further by testing specialized capabilities like conversational competence, mathematical reasoning, and mitigation of hallucinations in model outputs.
