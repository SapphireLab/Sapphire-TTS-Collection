# 6·Challenges

In this section, we propose four currently unsolved challenges, primarily stemming from the MMNTP training paradigm.
We also recommend that the readers refer to surveys that address other open challenges, such as evaluation of multimodal LLMs~\citep{huang2024surveyevaluationmultimodallarge,fu2024mmesurveycomprehensivesurveyevaluation}, efficient LMM architectures~\citep{jin2024efficientmultimodallargelanguage}, generative approaches and auto-regressive models for vision~\citep{jiang2024surveyvisionautoregressivemodel}.

## Scaling Up MMNTP Models with Unlabeled Multimodal Data

The utilization of abundant unlabeled text data is one key to the success of LLMs~\citep{zhao2024surveylargelanguagemodels}. However, the potential of using large amounts of unlabeled multimodal data in training MMNTP models has not been fully investigated. Most large multimodal models currently rely on labeled pair-wise data, such as image-caption or audio-text pairs during training. Recent studies~\citep{chameleonteam2024chameleon,yang2024visionmodelpretraininginterleaved,luo2024deem} have attempted to leverage the enormous amounts of interleaved text-image data available on the internet. However, their performance on downstream tasks does not provide a significant advantage over models trained solely on pair-wise data, such as LLaVA~\citep{liu2023llava} and Qwen2-VL~\citep{Qwen2vl}. Consequently, determining how to better utilize unlabeled multimodal data such as text-image interleaved webpages, rich-text figures, text-free audios, videos, screenshots of graphical user interface (GUI), etc., remains a crucial question in the effort to scale up MMNTP models.

From another perspective, the benefits from scaling up unlabeled text data and model sizes can be curved with Scaling Law~\citep{kaplan2020scalinglaw,hoffmann2022trainingcomputeoptimallargelanguage}, which forms a fundamental basis and faith for the development of LLMs. It elucidates the intricate relationship between model performance, model size, and the amount of training data, while also guiding the optimal allocation of computational resources during LLM training. However, the return of scaling MMNTP models remains largely under-explored. Some studies \citep{aghajanyan2023scalinglawsgenerativemixedmodal,sun2024scalinglawhypothesismultimodal,VAR} have explored or hypothesized the scaling behaviors of MMNTP models in the self-supervised training manner. According to \citet{aghajanyan2023scalinglawsgenerativemixedmodal}, data from distinct modalities exhibit varying scaling behaviors. However, the reasons behind these differences and their impact on the performance of downstream tasks across various modalities remain unclear. Furthermore, it is still uncertain whether MMNTP models can develop similar emergent abilities \citep{wei2022emergent} in downstream tasks as LLMs do when the training is scaled up.

## Overcome Modalities' Interference and Boost Synergy in MMNTP Training

The second challenge comes with the multitask learning nature of MMNTP models, where predicting tokens belonging to different modalities could be viewed as different tasks. A critical challenge within this framework is maintaining the performance of each individual task from interference while investigating whether tasks from different modalities can provide mutual assistance~\citep{crawshaw2020multitasklearningdeepneural}.

A primary concern here is modality interference~\citep{zhang2024pretrainedlanguagemodelshelp}, where the performance of one task might negatively impact another due to conflicting information or noise from different modalities. Recent studies underscore how jointly training multimodal tasks in an NTP fashion can pose optimization challenges, especially when a single transformer decoder model is used to generate both text and image outputs \citep{chameleonteam2024chameleon}. To mitigate issues like gradient norm explosion, techniques such as QK-Norm have been employed \citep{henry2020querykeynormalizationtransformers}. However, the root causes of these optimization difficulties in MMNTP models remain largely unexplored. Further evidence of interference is seen when MMNTP models are built on pretrained large language models, as the language capability often deteriorates when adding more modalities  \citep{zhang2024pretrainedlanguagemodelshelp}. This demonstrates that interference can lead to suboptimal learning outcomes, as the model struggles to balance competing demands from the different modalities.

## Increase Efficiency in the Training and Inference of MMNTP Models

Efficiency remains a long-standing goal in the training and deployment of deep learning models~\citep{Menghani_2023}. Due to the similarities in backbone model structures and NTP training objectives between LLMs and MMNTP models, many advanced methods designed to enhance the efficiency of LLMs~\citep{wan2024efficientlargelanguagemodels} can also be effectively applied to MMNTP models. However, there are also several new challenges arising in the training and inference of MMNTP models due to the involvement of data from different modalities.

**Training System Efficiency**

A big challenge in scaling up MMNTP models lie in the low efficiency in training large-scale MMNTP models on massive GPUs due to the inherent heterogeneity of both models and data across different modalities. Different from the text (1D) data in LLMs, MMNTP training involves high dimensional representations such as image (2D) and video (3D) data, where little has been done to optimize the training of these models from a system perspective. In particular, MMNTP exhibits scaling dependence with modality encoders. Recent studies have found that substantial idle GPU time (GPU Bubble) arises from the complex data dependencies when training MMNTP models that employ both visual encoders and a backbone LLM with pipeline parallelism~\citep{scale1}. To address the issue, several efficient and adaptive training frameworks have been developed to optimize the scheduling of encoder computations and overlap GPU communication with computation. For example, Optimus reduces training time by decomposing image encoder layer computations into smaller kernels and scheduling those kernel executions within LLM bubbles, minimizing the pipeline idle time~\citep{scale2}. DistTrain leverages disaggregated model orchestration and data reordering to improve the training efficiency and scalability of MMNTP, achieving significant improvements in model FLOPS utilization and throughput~\citep{scale3}. Despite these advancements, the unique challenges posed by multimodal architecture, how to develop more advanced system optimizations to train MMNTP on large-scale production clusters with thousands of GPUs remains an open research question.

MMNTP training also exhibit highly variable sequence lengths. As MMNTP models move towards more complex tasks such as multi-image reasoning, multi-modal RAG, video understanding, supporting MMNTP with long and variable sequence length becomes critical. However, existing large model training systems and the underlying parallelism technologies (data, tensor, pipeline) are limited in their ability to support efficient long sequence training. Recent studies have proposed several sequence parallelism techniques, such as DeepSpeed-Ulysses~\citep{scale4}, Ring-Attention~\citep{scale5}, and Unified Sequence Parallelism~\citep{scale6}, to enable long sequence training for LLMs. However, applying these sequence parallelism strategies to MMNTP needs to take careful consideration in handling heterogeneous data from different modalities, each with distinct characteristics and sequence lengths, which motivates advanced system-algorithm co-design to address this challenge.

**Multimodal Tokenization Efficiency in MMNTP Models**

As mentioned in the tokenization section, multimodal input like image, audio and video originally resides in a continuous space, which contains a lot of redundant information. The multimodal tokenization process has large room for efficiency improvement, where the core question is: can we use less tokens to represent the multimodal input while maintaining the performance? In the scope of single-modal image modeling and dual-encoder VLM architectures, various model compression methods—such as pruning, knowledge distillation, and quantization—are applied to accelerate image encoders. Model pruning \citep{zhu2021vision,Lin_2024_CVPR} sparsifies the encoder backbone and removes certain modules. Knowledge distillation \citep{yuan2021tokens,wu2023tinyclip} utilizes soft labels as supervision and train competent smaller dense models. Model quantization \citep{yu2022unified,liu2021post} replaces models and computation to low-precision counterparts. Though proven effective in single-model scenarios, the adaptability of these methods to training MMNTP models remains largely unexplored.

**Modeling Efficiency in Understanding and Generation**

Although image tokens occupy a significant portion of the input sequence of MMNTP models, it is discovered that the the LLM backbone only pays a small portion of attention to the image tokens compared to the language tokens~\citep{chen2024image}. This phenomenon raises the question  whether we can reduce the number of image tokens during training and inference without sacrificing performance as they take up most of the sequence length but gain the least attention from the model. In single-modal image modeling, these tokens can be largely pruned using pre-trained priors \citep{marin2021token,xiong2024pyra} or through entirely training-free methods \citep{bolya2022token}, with minimal impact on performance. For MMNTP models such as Llava~\citep{liu2023llava} and QwenVL~\citep{QwenVL}, \citet{chen2024image} proposed a pruning approach that removes most image tokens without compromising performance. However, the reason behind the redundancy of image tokens and how to leverage this phenomenon remains underexplored for MMNTP models of different modalities.

In the realm of multimodal generation, the challenge of modeling efficiency is equally pronounced. A central issue is how to define an effective generative training objective that suits the Next Token Prediction (NTP) manner for various modalities, given that data from different modalities possess distinct structures. A vanilla NTP training objective may not adequately address these differences, prompting the development of specialized objectives tailored to the characteristics of each modality. For instance, methods like MaskGIT~\citep{MaskGIT}, MAGViT~\citep{magvit}, VAR~\citep{VAR}, and DnD-Transformer~\citep{dnd-transformer} have been proposed to better accommodate the unique aspects of different modalities. Moreover, enhancing generation quality efficiently can be achieved through post-generation refinement techniques such as super-resolution \citep{lu2023unifiedio2}. Super-resolution methods aim to upscale the outputs of language models by either fine-tuning the backbone model \citep{CogView,CogView2} or by incorporating additional modules \citep{text2image2,kondratyuk2023videopoet}. Despite significant advancements in this area, diffusion models remain the de facto model in visual generation applications. However, a detailed comparison of generation quality and efficiency between MMNTP models and diffusion models is still lacking in the literature, leaving room for further exploration and research.

## MMNTP as Universal Interfaces

LLMs have exhibited notable advancements in collaborating with external models in the framework of NTP~\citep{qin2023toolllm,shen2023hugginggpt,gupta2022visprog,hao2022languagemodelsgeneralpurposeinterfaces}. It highlights the growing potential for language models to extend their capabilities beyond mere text generation. In this survey, we have mentioned that the next token prediction paradigm has been unifying vision, audio and different multimodal task. However, an ultimate challenge is beyond current explored modalities, achieving a universal interface connecting tasks from various sources, such as robotics~\citep{brohan2023rt2}, molecular~\citep{Flam_Shepherd_2022} and proteins~\citep{Ruffolo2024DesigningPW}. The key problem is how to formulate a different task as next token prediction, and whether such formulation is efficient and scalable in solving the problem, which is largely underexplored outside the language, vision and audio data.

**Design NTP Training Objectives for Different Modalities**

For non-text modalities, simply linearizing the data into a 1D sequence and conducting NTP (Next Token Prediction) training may not be the most effective approach. This strategy poses two potential issues: it can overlook the inherent structure of multimodal data and lead to excessively long sequence lengths. For instance, the spatial relationships in images and temporal relationships in videos are crucial elements that the traditional NTP training fails to consider. The number of tokens required to represent an image or video increases linearly with the image's resolution and the video's duration. To address these challenges, several approaches have been developed to adapt NTP training objectives to various modalities, such as images and videos. Notable examples include MaskGiT~\citep{MaskGIT}, VAR~\citep{VAR}, DnD-Transformer~\citep{dnd-transformer}, and Next-Block-Prediction~\citep{anonymous2024next}. These efforts aim to better capture the unique structures present in different types of data and can reduce the inference time by generating multiple tokens at one time.

**Comparison to Diffusion**

Diffusion models represent another popular framework for generative modeling and they have been extensively applied to multimodal data beyond the original image modality~\citep{yang2024diffusionmodelscomprehensivesurvey}. These applications extend to areas such as language~\citep{li2022diffusionlmimprovescontrollabletext}, robotics~\citep{chi2024diffusionpolicyvisuomotorpolicy}, and drug discovery~\citep{huang2024proteinligand}. When comparing NTP and diffusion models, both approaches share the fundamental idea of breaking down a complex generation task into multiple, more manageable steps. The major difference between these two approaches lies in how the task is decomposed. NTP breaks down the data according to its dimensional order, whereas diffusion models deconstruct the data globally in a coarse-to-fine manner. There is no consensus on which method is superior, and new approaches are emerging that combine these two modeling techniques. For instance, auto-regressive diffusion models~\citep{MAR,Transfusion} incorporate elements of both strategies. Exploring how different modeling methods perform on various modalities is a fascinating frontier in the field.
