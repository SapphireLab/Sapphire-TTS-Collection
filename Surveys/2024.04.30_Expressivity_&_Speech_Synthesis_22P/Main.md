# Expressivity & Speech Synthesis

<details>
<summary>基本信息</summary>

- 标题: Expressivity and Speech Synthesis
- 作者:
  - [Andreas Triantafyllopoulos](../../Authors/Andreas_Triantafyllopoulos.md)
  - [Björn W. Schuller](../../Authors/Björn_W._Schuller)
- 机构:
  - 机构
- 时间:
  - 预印时间: 20??.??.?? ArXiv v1
  - 更新笔记: 20??.??.??
- 发表:
  - 期刊/会议
- 链接:
  - [ArXiv](https://arxiv.org/abs/2404.19363)
- 标签:
  - ?
- 页数: ?
- 引用: ?
- 被引: ?

</details>

## Abstract

Imbuing machines with the ability to talk has been a longtime pursuit of artificial intelligence (AI) research. From the very beginning, the community has not only aimed to synthesise high-fidelity speech that accurately conveys the semantic meaning of an utterance, but also to colour it with inflections that cover the same range of affective expressions that humans are capable of. After many years of research, it appears that we are on the cusp of achieving this when it comes to single, isolated utterances. This unveils an abundance of potential avenues to explore when it comes to combining these single utterances with the aim of synthesising more complex, longer-term behaviours. In the present chapter, we outline the methodological advances that brought us so far and sketch out the ongoing efforts to reach that coveted next level of artificial expressivity. We also discuss the societal implications coupled with rapidly advancing expressive speech synthesis (ESS) technology and highlight ways to mitigate those risks and ensure the alignment of ESS capabilities with ethical norms.

## 1.Introduction

Talking machines have long captured human imagination. Already in Homer’s Iliad (8th century BC) we read of Hephaestus’ “golden maids” [1, Hom. Il. 18.388] (emphasis ours):

Waiting-women hurried along to help their master [Hephaestus]. They were made of gold, but looked like real girls and could not only speak and use their limbs but were also endowed with intelligence and had learned their skills from the immortal gods.

That they could speak was seen as a prerequisite of intelligence (and being worthy of serving in a god’s retinue). The first documented efforts to endow machines with the capacity to speak date back to late 17th century AC, with C. G. Kratzenstein’s vowel resonators [2]. Arguably though, the quest for human-like speech synthesis began in earnest with the advent of computers in the 1950-1960s, with 1961 seeing the first digital vocoder implemented on an IBM 7090 [3].

The field has come a long way since then. Starting with model- and ‘rule’-based approaches [3, 4], quickly moving to data-driven concatenative synthesis [5, 6, 7, 8], and then later to statistical models [9, 10], text-to-speech synthesis (TTS) has progressed in leaps-and-bounds in recent years with the advent of deep learning (DL) [11]. Expressive speech synthesis (ESS) followed a parallel developmental path, with Cahn’s Affect Editor [12, 13] and Murray’s HAMLET [14, 15] representing the earliest methods relying on rules, while later approaches transitioned to concatenative [16, 17], parametric [18, 19], and, finally, DL-based [20] ESS. With each change in technology came associated gains in fidelity and naturalness [20].

This trend is exemplified by the recent wave of advances in the broader generative artificial intelligence (GenAI) field [21]. Progress in probabilistic generation, currently spearheaded by “diffusion models” [22], have brought GenAI in the epicentre of attention for various stakeholders – societal, commercial, and, increasingly, regulatory (see e. g., the recent EU AI Act [23]). Text generation has been the most prominent example of that new era, with large language models (LLMs) like ChatGPT [24], Llama [25], or Claude spearheading recent innovations. Mirroring that success, the quest for ESS breakthroughs is currently being taken on by an increasing number of research groups and companies, and has become a staple of speech technology conferences (INTERSPEECH, ICASSP, SLT, etc.).

Expectedly, synthesis quality and controllability are improving at an accelerating rate [20]. Moreover, as a result of increased commercial interest, ESS systems of unprecedented capabilities are being constantly released to the public, in off-the-shelf, easy-to-use toolkits that can be co-opted by a wider and wider cohort of layman users for their own purposes. On top of that, foundation models have recently surfaced as a key differentiator in GenAI and beyond [26] and are beginning to impact ESS as well [27]. This means that we will soon – if not already – be living in a “metaverse” [28] populated with expressive artificial intelligence (AI) agents whose voices are indistinguishable to humans, and whose capabilities may vastly exceed (or enhance) the voices of average people. Accordingly, this increases the probability that bad actors, or even well-intentioned users, misuse the technology – a problem encompassed in the broader AI “alignment” conversation [29].

Beyond that, the present situation begs the question: What else remains to be done? Is ESS research finished? Are our goals finally achieved? Should ESS researchers quit and change their field of study? Thankfully not. As we argue, contemporary research is largely geared towards “expressive primitives” – states and traits which are straightforward to depict and can be simulated within a singular utterance – and which we call Stage I ESS research. Typical examples include the synthesis of “emotional voices”: this results in speech which will be perceived as conveying a particular emotion (e. g., happiness). However, a major promise of ESS systems lies in facilitating a conversational interface between humans and AI agents. In fact, given the rise of modern text-based conversational agents (i. e., ‘chatbots’) like ChatGPT [24], we expect ESS systems to become embedded in voice-driven conversation applications, where emulating an emotional state goes beyond portraying that emotion within a particular utterance. In other words, appropriateness becomes an essential aspect – what to say, when, and how. What is more, there are expressive states which cannot be distilled to a single component, such as political stances, moods, or dispositions [30]. Given the anticipated mastering of synthesising unitary utterances, we expect an increased focus on synthesising more nuanced, longer-term states and traits of expressive agents, as well as adapting to the context of (real-time) conversations with different individuals. This we call Stage II research, and it is still in its nascent stages.

This chapter aims to chart this emerging landscape of expressivity in the era of GenAI. With that in mind, our goal is not to give a technical survey of state-of-the-art systems, as there exist a plethora of older and more recent surveys that sketch out the inner workings of TTS and ESS approaches over the years; we recommend [8, 10, 11, 16, 20, 22, 31] as good starting points. Therefore, we intentionally place limited emphasis on the technical implementations of existing ESS systems and only cover them briefly in Section 2 and Section 5.1. Instead, we explore deeper questions that are highly pertinent for the present and future of the field. Among others, we discuss:
- What are the states and traits that we can expect ESS systems to cover (cf. Section 4)?
- How can we move from classic, simple expressive ‘primitives’ (cf. Section 5) to more complex behaviours? How can we jointly synthesise multiple – perhaps contradictory – states (cf. Section 6)?
- How can we move away from a ‘one-size-fits-all’ approach and towards a more personalised approach to synthesis (cf. Section 6.3)?
- What role do foundation models play (cf. Section 7)?
- Crucially, what happens to the world once we achieve our wildest dreams regarding the capabilities of ESS models? Recent progress points to a future where AI agents can communicate with much higher levels of expressive aptitude than average humans. What impact will this have on our trust in politicians, marketeers, or journalists? How will it influence our perception of others? How will it influence our perception of ourselves (cf. Section 8)?

We hope that our discussion of these questions will help shape future research and provide a template and roadmap for the next generation of ESS systems. Importantly, our discussion is grounded in the applications that ESS enables, as these dictate its ecology and thus the affordances that it may develop, and so set, in turn, the framework for current and future research efforts.

The remainder of our chapter is organised as follows. We begin with a brief historical overview of ESS research, and continue with discussing the applications which it facilitates and the taxonomy of states and traits which it aims to simulate. Next, we describe the technical underpinnings of contemporary systems. Following that, we discuss our notion of a Stage II system and review how foundation models are utilised in this field. Our final section outlines the ethical considerations entailed by a rapidly advancing technology.

## 2.A blitz history lesson

Expressivity was already embedded in TTS systems from their first incarnation. The first vocoder by Kelly and Gerstman [3] allowed for the manipulation of prosodic and timbre attributes that are related to affect [30, 32]. However, the first documented attempts to use this functionality came much later, with rule-based systems like HAMLET [14, 15] and Affect Editor [12, 13]. These manipulated attributes which are known to correlate with affect, such as pitch or timing. These parameters were later investigated in a data-driven fashion [33], but, by and large, this first era of ESS largely depended on rules and expert knowledge.

The next generation featured concatenative synthesis [16, 17, 34, 35], which relied on selecting speech units uttered with the appropriate expressions from an existing corpus. We note that at this point, the ‘sister’ field of TTS had already progressed to statistical parametric speech synthesis (SPSS), where trainable modules are learnt from data, and this was readily co-opted for ESS too [18, 19]. Typically, these models were implemented with hidden Markov models (HMMs) and were trained to map prosodic and spectral features from a ‘neutral’ to an expressive state. Most often, this necessitated a cascade pipeline, with the initial synthesis made with a standard TTS tool and an extra conversion ESS module applied on top. Learning this transformation usually required parallel data – corpora containing audio pairs which only differed in the expressed state but everything else (speaker, text) remained the same.

With the advent of deep learning, HMMs were substituted with deep neural networks (DNNs) [20], but the key principles remained the same. A mapping from some neutral state to an expressive one was learnt from data. However, the capabilities of DNNs further allowed for a disentanglement between the different components of speech, thus no longer demanding parallel data. This enabled a radical scaling-up of the available speech that could be used for training, an increase which went hand-in-hand with the accompanying increase in model size and complexity.

The most recent “GenAI era” brought further advances, primarily with the introduction of denoising diffusion probabilistic models (DDPMs) and other generative methods [36]. Moreover, the rapid explosion in LLMs and, increasingly, multimodal foundation models (MFMs) (i. e., models which can jointly handle multiple modalities, usually including language) opened up new avenues in the controllability of ESS [20]. This is the state-of-play at the moment of writing. We next consider the applications where ESS can be deployed.

## 3.Application domains for ESS now and in the future

We consider application domains where use of ESS is already widespread, others, where text-based communication is already a given reality, with speech being the natural extension, and, finally, ones which have not yet seen much use of ESS, but are ripe for disruption. Some of the fields we discuss below already saw the widespread use of text-based technologies even before the introduction of LLMs. The unprecedented capabilities offered by those have naturally disrupted previous standard processes and made their integration a very active area of ongoing research. It is in this landscape that we discuss ESS applications.

An overview of the four types of application domains that are highly relevant for ESS research is given in Fig. 1. ESS systems are, so far, primarily used to facilitate human-computer interaction. In that scenario, it is typical to combine ESS with TTS, and create entirely artificial voices. However, ESS can also be used to transform existing human voices [37] – an application which is similar from a technical perspective but has different societal implications. We discuss these two scenarios below. Moreover, we briefly mention the exotic case of computer-computer interaction as an emerging new frontier for ESS research.

### 3.1.Human-Computer Interaction

First and foremost on the list of current and future applications are conversational agents. Several companies already employ text chatbots to offload some of their workload for customer support, service, or even sales [38], and this field is rapidly growing with the rise of LLMs. Moreover, different institutions see the promise of conversational agents in improving their services, like seen in the healthcare domain [39]. Finally, intelligent assistants (e. g., Siri, Alexa, or Google Assistant), by now pervasive in various consumer gadgets (like smartphones or even smartwatches), are essentially more advanced chatbots, with most of those already including TTS in their workflow.
Further integrating ESS capabilities to all these conversational agents is a straightforward extension of their present state [40, 41]. We thus expect this application domain to be both a key driver and an early adopter for future advances. In terms of expressivity, conversational agents may place an emphasis on interpersonal adaptation to the user, appearing helpful, empathetic, or show any other personality trait that is desirable to their creators.
3.2 Content creation
Besides interacting with humans, ESS can be used to facilitate the de novo creation of new content, especially when combined with the impressive capabilities of broader GenAI models. Marketing is a domain already seeing increased use of GenAI technologies [42], where employing ESS to accompany automatically generated illustrations or videos is already attracting community attention. In the extreme, this can go as far as creating entirely virtual personas, or artificial influencers, which populate digital spaces and promote marketing material in a more naturalistic way than a simple commercial can ever do [43].
On the darker side of speech science, ESS can vastly expand the capacity of bad actors to spread misinformation. This can be done as a straightforward case of “marketing”, albeit for an evil cause, with ESS being used for promotional material around fake news in the same way as a company may use it to promote its product. For example, vishing – the use of voice calls for phishing – is one area ripe for disruption from ESS software [44]. In its present form, performed by humans, it is already a major societal and legal problem2, and is bound to get worse as ESS facilitates a scaling up of resources available to bad actors that engage in this practice.
3.3 Voice enhancement
ESS can also be used to augment or enhance one’s own voice to attain specific expressive attributes that it is lacking. This can be done both short-term, e. g, when one sends a short voice message to their partner and wants to convey some additional affect they are not able to express at the moment, such as excitement for an upcoming dinner that they are not presently feeling due to fatigue, but also long-term, e. g., to manipulate one’s entire persona for a social media profile. For example, female politicians have sometimes undergone intentional training to change their manner of speaking,
with Hilary Clinton and Margaret Thatcher purportedly switching to a more masculine voice [45, 46]. In the future, this may be achieved by a simple application of voice conversion.
Beyond one’s self, however, ESS can be used to transform the voice of others – usually for malicious purposes. This is essentially a more subtle form of deep faking [47], where instead of using GenAI to fabricate a non-existent statement from an individual, one may use voice transformation to distort the original message. As a recent example, much debate is currently revolving around the age of the United States democratic candidate (and presidential incumbent) Joe Biden. A similar use of voice technology could make him appear older than he actually is, thus intensifying his opponents’ accusations regarding his suitability as a candidate. This more subtle form of manipulation is perhaps more subversive than outright fakes; while those can be vetted and refuted based on evidence and facts, minute changes to the voice of a speaker that cast them in a negative light will be much harder to identify. Broadly, this subtle transformation of one’s voice can be used to harm political or commercial opponents, or even entire social groups, and we expect it to become a major societal issue in the future.
3.4 Computer-computer communication
Finally, we want to highlight that in a world were conversational agents and intelligent assistants are increasingly deployed to perform more complex tasks autonomously, such as booking appointments or handling business transactions, we can expect that these systems will also encounter artificial interlocutors throughout their lifecycles. For example, one person’s intelligent assistant might attempt to book an appointment over the phone with a company’s artificial customer service agent. In that case, both systems might presumably use ESS to achieve the desirable outcome while remaining oblivious to the fact that they are communicating with another machine. This raises interesting implications both on how these systems will react and the types of affordances they will need to develop for success (assuming they are to some extent learning autonomously).
4 A taxonomy of expressive states and traits
Having reviewed the ecology where ESS algorithms operate, we now turn to the range of states and traits that can be synthesised. We begin with a basic assumption: that the things which can be recognised are those that will also be (eventually) synthesised. If humans, and by extension AI algorithms, can recognise particular affective states in speech, then there is nothing, in principle, preventing GenAI algorithms from simulating those states. This definition allows us to sidestep what has already been done by the community (due to various restrictions including the lack of available data) vs what can be done (based on evidence that humans can portray particular states).
Fig. 2 presents a non-exhaustive portrayal of expressive styles that can be recognised in humans – this serves as inspiration for all that can be potentially synthesised for computers. It is motivated by similar taxonomies outlined in Scherer [32] and Schuller and Batliner [30]. Importantly, we distinguish between two particular axes3: how long the underlying affective state lasts and how persistent it is in its appearance.
The first axis allows us to decompose affective behaviour into states and traits [30]. At the extreme, states are short-lived, transient conditions; these include concepts like emotion or interest. Traits, on the other hand, are long-term, immutable attributes, such as biological sex (barring medical intervention) or personality. Naturally, this taxonomy is not a black-and-white dichotomy, but a colourful spectrum: in-between the two extremes exists a variety of conditions that vary with respect to their duration, intensity, and (expected) rate of change.
The second axis instead differentiates by how prevalent a state or a trait is in a person’s speech. Some behaviours are persistent; at the extreme, they are ever-present, and ubiquitously colour almost every single utterance. On the other end are episodic behaviours; those are fleeting, appearing only momentarily and within the confines of a single expression. Crucially, this second distinction is independent of whether the behaviour is a state or a trait. Some traits, like gender or age, are both long-term and persistent (they are almost always to be detected in a speaker’s voice); others, like political orientation or depression are episodic4. Likewise, even though states themselves are fleeting, some, like politeness or sincerity, are to be found in a small set of utterances, while others, like emotion or respiratory diseases are constantly present – so long as they continue to be true for a speaker5. Temporality is important for understanding affective behaviour in humans, as, like Cowie and Cornelius [48] argues:
A real possibility is that there may be multiple scales at work even in the short term, with some signs building up over a period of seconds or minutes and others erupting briefly but tellingly.
This important aspect of timing also calls for different ESS capabilities. The first, simpler one, requires a mapping from one state to another; this is applied consistently to all utterances. We call this Stage I ESS, and it is primarily suitable for synthesising persistent behaviours. The second, more sophisticated one requires knowing which utterance to transform into a particular expression; often, the desired effect is achieved by combining multiple simple expressions from Stage I. We call this Stage II ESS, and it is geared towards long-term, episodic behaviours.
Stage II is much more challenging than Stage I. Specifically, while the generation of a persistent expression in situ is possible using a direct portrayal of it, utilising such a portrayal in context to reflect an episodic expression is substantially more challenging [49]. Seen in this light, contemporary ESS systems have mastered the synthesis of behavioural ‘primitives’ – affective states that can be portrayed fleetingly, oftentimes within a single utterance or even within a single word. However, these primitives form the basic building blocks for an array of more complex states and traits that have thus far remained elusive, like personality, ideology, stance, friendship, or compassion. We continue by reviewing how existing models handle the synthesis of these primitives in the next section, followed by a perspective on how we expect them to progress to the generation of more complex behaviours in Section 6.
5 Stage I: Synthesising expressive primitives
In this section, we discuss how ESS can be used to synthesise what we term expressive primitives – transient behaviours which are completely encompassed within a single episode. These primitives include very short states which dominate behaviour for a fixed period of time, like emotions, and immutable traits which are omnipresent, such as biological sex or age [30]. As such, they can be portrayed within the confines of a single utterance. This can be done by manipulating
the paralinguistic structure of the utterance, as well as by introducing vocal bursts as short interjections that convey a particular affect. Both tasks are achieved by following the principles outlined below – one needs to train a model on data that encompass the targeted expressive behaviours.
As discussed in Section 2, contemporary ESS is statistical in nature, falling under the auspices of machine learning (ML), and specifically statistical generative models (SGMs). SGMs constitute a model of the underlying data generation process; as such, they allow sampling from that process to generate new content. Traditionally, the generation process to be modelled was that of generating speech from text (i. e., TTS) and converting it to some expressive state. Early SGMs were specialists, focused exclusively on particular mappings, namely, the ones defined by the data and tasks they were trained on. We describe their inner workings in the following subsections.
5.1 Statistical generative models
In the present subsection, we begin with a quick overview of the mathematical underpinnings of SGMs followed by a discussion of the most crucial components that are needed to build ESS systems.
5.1.1 Preliminaries
Broadly, SGMs can be seen as a category of models fθθθ that aim to capture the data generating distribution:
Usually, the latter is expected to be conditioned by some additional information y, in which case it takes the form:
with y now taking arbitrary values (e. g., a class label denoted as integer or even text; see below).
In order to ensure that f (·) is a proper probability distribution, it is often thought of as the normalised form of an unnormalised energy distribution E7:
with xxx = (x1, ..., xN ), ccc = (c1, ..., cN ), D being the set of all possible data points, and β a normalisation (temperature) parameter which we will henceforth ignore. We note that the major bottleneck in computing f is the presence of the integral of D in the denominator; in the general case, this must be evaluated over all data points (i. e., the space of all possible speech utterances in our case). This denominator is often referred to as the partition function Zθθθ and is considered intractable for most practical applications. We return to this point when we discuss how these models are actually trained in Section 5.1.2.
Broadly, we distinguish between two main forms of ESS systems, depending on their input-output schemas:
1. Expressive TTS: these “end-to-end” models generate expressive speech directly from text; thus, they jointly
create the text content and the expressive style of an utterance.
2. Expressive voice conversion: these “cascade” models manipulate an input speech signal to change its expressive style; usually, they are combined with a ‘simple’ TTS frontend that creates a speech utterance in neutral style, which is then transformed by the ESS model.
An overview of both and their differences can be found in Triantafyllopoulos et al. [20].
There are three main challenges associated with fθθθ:
1. Training it to become a good approximation of p(·); 2. Being able to sample efficiently from it; 3. Achieving good levels of control for the different values of y.
We discuss each of them in the subsections that follow.
5.1.2 Training
SGMs are trained on (large) corpora of speech; in the case of expressive TTS they are trained to output speech from text (either graphemes or phonemes); in the case of expressive voice conversion, they are instead trained to map speech to speech. We note that our goal during training is to estimate the normalised energy function fθθθ(·) from Eq. (4). This is achieved by using a training set S and computing the function fθθθ(·) that maximises the likelihood over S; in layman’s term, this optimal f ∗ θθθ (·) is the model which best captures the variability over the observed data – the most “likely” model given the evidence. As the standard algorithm used for training (especially for neural networks) is (stochastic) gradient descent, in practice, we minimise the negative likelihood – and a logarithm is often taken to remove the exponent. Thus, we end up with the following negative loss-likelihood loss function LN LL:
Note that the first term above (often referred to as the “positive phase”) is computed over the training set S; the latter (the “negative phase”), is instead computed over the entire true distribution of data D. The positive phase increases the likelihood of the observed data; the negative phase in turn grounds that likelihood by keeping it limited over the entire space of possible data. Importantly, during training with gradient descent, both integrals are approximated using a sum (over the finite set of observed data); this is also the process used by the popular stochastic gradient descent algorithm and its variants. However, in each iteration, one must also evaluate the negative phase over D. There are two issues with this:
1. The computational overhead of always evaluating the value of the energy function over D is intractable. 2. More importantly, it is almost impossible to observe this D in practice; not only does it include all possible observable data points (e. g., all possible speech utterances that will ever be uttered in the entire history of mankind in our case), but in the strict sense, it also includes all ‘garbage’ sounds that fit into the embedding space defined by xxx; technically, even though these sounds will have a very low probability, they still need to be evaluated.
The above two bottlenecks make it very hard to identify a suitable D to integrate over. All modern variants of SGMs are explicitly aimed at overcoming this hurdle: variational autoencoders (VAEs) circumvent the need to approximate the partition function by optimising instead a lower bound, the so-called evidence lower bound (ELBO) [50]. Contrastive methods increase the likelihood on observed data and decrease it on fake data [51]; the difference between those two likelihoods eliminates the need to compute the partition function. DDPMs rely on score matching [36, 52], whereby the dependence on Zθθθ is lifted by substituting the estimated likelihood with its derivative. Our focus here, however, is not on thoroughly reviewing these (and other) methods, so, we instead refer the reader to relevant surveys [11, 53]. For our purposes, it is important to note that DDPMs [36] have emerged as the most recent class of methods with impressive generative results, and are nowadays the go-to method for most GenAI applications, including ESS [54, 55, 56], at least in terms of offline generation.
5.1.3 Sampling
After successfully training an approximation of p(·), it becomes necessary to sample from it during inference. This is also not a trivial problem, especially because the typical forms of fθθθ are complex and make sampling complicated. A key challenge arises from the fact that N is high-dimensional – particularly for ESS. For example, assuming we aim to generate a 1-second sample at 16 kHz, then a model needs to procure 16,000 samples. Algorithm 1 shows how this sampling can be achieved with a simplified8 version of a traditional algorithm, namely, Gibbs sampling, an instance of a Monte Carlo Markov Chain (MCMC) method [57]. Gibbs sampling relies on the iterative sampling of all variables by using the conditional distribution of that variable over all others. The iteration stops once all variables have
converged. It is evident that performing this procedure for 16,000 samples – let alone for longer sequences – easily becomes computationally prohibitive depending on the structure of fθθθ.
To overcome this crucial challenge, the community has focused on SGMs that can be efficiently sampled. Here, DDPMs suffer from an additional overhead imposed by iterating over the denoising distribution [36, 58] and are thus not suited for the real-time requirements of some ESS applications (cf. Section 3). While recent efforts have been targeted towards addressing this bottleneck [59], the current state-of-the-art relies on slightly older methods, primarily autoregressive models [11].
5.1.4 Controllability
The last hurdle to a successful SGM-based ESS system is achieving a satisfactory degree of controllability [20]. As mentioned, controllability corresponds to learning a conditional generation of the data distribution (p(x1, ..., xN |y)) for different values of y. We begin by discussing the different forms that y may take (see Triantafyllopoulos et al. [20] for a more detailed discussion):
One-hot encoding: The standard form of conditioning relies on a constrained label space. These labels encode the different states and traits that can be synthesised by the ESS system. The typical representation of those labels is a ‘one-hot’ encoding, i. e., a 1D vector with dimensionality equal to the number of labels, populated with zeros everywhere except a single 1 in the element corresponding to the target class (some more advanced forms of encoding allow mixed classes; cf. Section 6.2). In the simplest case, a different one-to-one model is trained and sampled for each state/trait combination; then, the label is simply used to select the appropriate model. However, most recent works prefer to inject the label as additional information to the generating module (e. g., the decoder in an encoder-decoder architecture) [20, 60].
The major downside of one-hot encoding is that it only covers a restricted amount of expressive attributes that can be synthesised. Moreover, given that it represents categories, it is only suitable for concepts that are categorical in nature. For example, in the case of synthesising emotions, it is mostly used to synthesise categorical emotions, e. g., relying on Ekman’s ‘big-6’ [61]. This is severely restricting the choices of ESS creators, which is why the community is transitioning to the following two forms of conditioning.
Audio prompts: A more natural form of conditioning relies on (short) speech snippets that are uttered in the desired style [62, 63, 64]. These short snippets are given as inputs along with an input text sequence or audio sample (synthesised in neutral voice) – or both. ESS models that are controlled via audio prompts feature an additional prompt encoder, which maps the input audio to a set of embeddings that only encode the required style9. The ESS model then learns to map the input utterance to the target style specified by the additional prompt. In the literature, this process is also known as reference encoding [20, 62] or style transfer [66].
During training, these audio prompts are drawn from a pool of available data (oftentimes the same data the input and target utterances are drawn from). During inference, they are instead given by the downstream user (though sometimes the user may select a style from some pool of references).
The main downside of auditory prompting is that the reference samples encompass a lot more information than the targeted expressive state [20]. For example, they additionally include information about the prompt speaker’s sex, age, ethnicity, or any other attribute that is encoded in the speech signal. It is thus particularly challenging to disentangle all the different components such that only the required style is propagated to the main synthesis network [67]. Recent works have taken aim at this challenge by introducing complementary losses to emphasise this aspect during training [65], but despite their success, disentanglement remains an open problem.
Linguistic prompts: Lately, and especially with the recent rise of LLMs, it has become possible to condition ESS models using linguistic prompts [68, 69, 70]. This is arguably the most natural form of controlling the models, as it
makes it more intuitive – and reproducible – for downstream users. The general layout is the same: the user gives an input prompt (“Generate a pleasing voice”), which is passed to a text encoder to generate embeddings that are then propagated to the synthesis module and the whole system is trained end-to-end (though some components might be frozen).
The intuition behind this form of conditioning is that the text encoder encapsulates knowledge about how a particular expression sounds. In the case of LLMs, this knowledge is incorporated through its pretraining on very large corpora and uncovered via prompting or finetuning. The downside is that the model may inherit the biases which accompany the text encoder [26] (as is always the case using transfer learning).
5.2 Expressions in speech
Synthesising expressive styles in speech entails the manipulation of those voice parameters which convey affective information: pitch, voice quality, rhythm, and pronunciation. As we saw in Section 2, in the early days of ESS, these parameters were explicitly manipulated by rule-based systems. With the later rise of SGMs, the community has primarily focused on learning mappings from one expressive style (usually neutral) to another, with the models implicitly transforming those parameters as needed.
Fig. 3 shows how this mapping can be achieved in practice: Typically, the input text is transformed to a neutral utterance using TTS; then, the appropriate style is applied to this utterance using a cascade voice conversion model. In recent years, end-to-end models that start from text and directly output an expressive utterance are increasingly becoming the norm. In either case, the expressive style is controlled as discussed in Section 5.1.4 – with a reference utterance, a linguistic prompt, a tag, or some combination of the above. The list of ESS models is very long and beyond our scope here – we refer to Triantafyllopoulos et al. [20] for a recent survey, although more and more models come out each year.
5.3 Vocal bursts
Non-verbal vocalisations, or “vocal bursts”, also play an eminent role in expressing affect [71]. A timely exclamation can convey agreement, compassion, understanding, or support more easily than a verbal message, especially in the
form of backchanelling during a real-time conversation [72, 73]. When the demo for Google Assistant was unveiled, the crowd first erupted in cheers at the assistant’s “mm-hmm” near the end of the video – a startling depiction of the importance we place on vocal bursts [20]. Until recently, however, their synthesis has received far less attention than their verbal counterparts. This is quickly changing. Notably, the Expressive Vocalisations (ExVo) series of workshops has called attention to their generation and provided the first challenge on synthesising vocal bursts, drawing increasing interest to this task [74].
In principle, the process for generating a vocal burst is similar to that of an expressive speech utterance and can be handled by a specialist SGM trained explicitly for this task. Perhaps even more so than verbal expressivity, the key challenge lies with knowing when to output such a vocalisation. Timing is essential to transmit the appropriate message – and this is where Stage II ESS becomes even more important. This is what we turn to next.
6 Stage II: Synthesising complex behaviours
We noted in Section 4 how temporality is a vital aspect of ESS. Section 5 discussed how GenAI models can be used to synthesise a set of behavioural primitives – simple affective states that can be understood within a singular utterance (or vocal burst). The particular primitives that can be synthesised constitute the set of affordances made available by a Stage I ESS system. We now turn to how an AI conversational agent can utilise these affordances to advance its Stage II capabilities.
6.1 Learnt expressive policies
A conceptual example for how a Stage II ESS agent operates is shown in Fig. 4. It illustrates how an intelligent assistant with the overall goal to ‘befriend’ an individual may utilise different expressive styles to achieve its goal depending on the context of an interaction. It may begin with a [CHEERY] message, then understand that the user is in a rather dejected mood, and choose to respond with [EMPATHY] and its own [DEJECTION], before proceeding with another
attempt for an [UPLIFTING] utterance. The choice of styles and their ordering is set by the agent’s policy, which takes into account the present interaction and the user’s overall preferences. The expressive primitives available by Stage I ESS can thus be considered as a set of available actions10 that an agent can choose from at each conversational turn (or even in-between, in the case of backchanneling). These actions must be combined over several turns to achieve a particular goal11. This high-level goal can be achieved by utilising an appropriate chain of actions.
Importantly, the policy for selecting the appropriate set of actions can be either hardcoded or learnt. In the latter case, our conceptualisation is amenable to a standard reinforcement learning (RL) framing [76]. Fig. 5 shows a blueprint for how a policy can be learnt from conversational interactions: An initial bootstrapping phase allows learning to generate appropriate responses from observing human-human conversations; in that phase, the agent is tasked with generating “shadow” responses for one or both of the interlocutors, and attempts to match the actual response of a human. The underlying assumption here is that humans more often than not pick the most suitable response in a conversation. This assumption can be relaxed by annotating some conversations with respect to how ‘successful’ they were (although we still expect a large benefit from pre-training on unlabelled large data before using annotations). On a second step, the agent is used for actual human-machine conversations, where it receives a positive reward when it achieves its goals. There, the agent actually partakes in a conversation, selects the most suitable action combination using its present policy, receives its reward in the form of feedback, and updates its policy accordingly.
Similar to contemporary RL systems, the process can involve human interlocutors (i. e., a reinforcement learning from human feedback (RLHF) setup [77]) or be bootstrapped with self-play using artificial agents [78] – both strategies which have proven successful in other RL problems. Moreover, the layering of system goals can be extended to more levels than two. In our example, [EMPATHY] may be a sub-goal of [BEFRIEND], which in turn might be a sub-goal of
10These actions are intentionally reminiscent of acts in speech act theory [75]. However, they are not entirely the same concept. While their aim is to help ‘achieve’ the agent’s goal(s), they do not have a direct mapping to the performative context of some speech acts.
[SUPPORT], or even, on a potentially more sinister turn, of [INFLUENCE] and [DECEIVE]. This framework thus allows for an extension of the affordances that an agent can employ – or even autonomously acquire throughout its life-cycle. We note that we have selected these styles for illustration purposes only; in fact, we expect ESS models to rely on internalised constructs that remain, to a smaller or greater extent, uninterpretable to humans (especially when incorporated in the inner space of foundation models; cf. Section 7).
Intriguingly, RL training may further lead ESS models to uncover entirely novel forms of expression that are not (presently) used by humans. We note that the emergence of new expressive styles is common for humans – and is further impacted by modern media [79]. In principle, there is nothing preventing artificial ESS models to introduce such styles, which humans may then choose to mimic, either explicitly or implicitly (i. e., simply owning to the popularity of those styles in social media and beyond). We find this an exciting proposition of self-learning systems and return to it in Section 8.1.
6.2 Mixed-state synthesis
Oftentimes, the state that an agent needs to express is mixed and comprises multiple simpler states that need to be synthesised jointly. Stress is a good example. Lazarus [80, p. 35] argued that: “When there is stress there are emotions... when there are emotions, even positively toned ones, there is often stress too...” Another one, perhaps more pertinent to affective agents, is compassion [81].
Some research efforts have already targeted the synthesis of mixed states [82]. Typically, these try to interpolate between two or more ‘clear-cut’ elements, thus resulting in a new construct that lands somewhere in between. For instance, happiness and sadness can be interpolated to obtain a bittersweet state.
Technically, this effect can be achieved by actually interpolating between the embedding spaces characterising the two initial states; assuming that this space is sufficiently well-behaved, a mathematical interpolation (e. g., averaging or taking the geodesic mean) will result in a point in that space that maps to an ‘in-between’ state. This is congruent with the semantic space theory recently proposed by Cowen and Keltner [83], which postulates that emotions exist on a well-behaved manifold, whose traversal yields smooth transitions between the different emotions.
However, there exists another side to synthesising mixed-states. Previous work has focused on expressing a single new state which characterises an entire utterance – this is equivalent to our Stage I ESS capabilities discussed previously. The corresponding Stage II implementation would require the interlacing of multiple states in a longer segment comprising multiple utterances, some of one state, some of the other – and some in-between. Crucially, planning the trajectory between successive states to achieve the required effect, as well as managing the corresponding transitions, requires the higher-level capabilities of a more advanced policy of the kind envisioned above. This still remains open-ground for ESS systems.
6.3 Personalised expressive speech synthesis
Expressivity exists in the ears of the beholder. How a speaker is perceived depends on the background and current affective state of the listener. Previous research suggests that personal effects mediate both the perception of expressive primitives and more complex behaviour. For example, different age, gender, and culture groups have been found to perceive emotions differently [84, 85, 86], while the relatively low to moderate inter-annotator agreements found in several contemporary speech emotion recognition (SER) datasets demonstrates how individualised the perception of affect may be (e. g., see [87]). Consequently, this means that AI conversational agents must learn to adapt in-context to their interlocutor, a feat that is already being pursued for LLMs [88].
To achieve this, it is necessary for AI agents to actively monitor their interlocutors during a conversation. On a first level, they may try to identify their demographics; this helps frame their interlocutor as belonging to particular groups with known preferences. On a deeper layer, the agents must identify how their interlocutors perceive expressivity. This can be a achieved by a trial-and-error process, where the agent makes attempts to express particular states and gauges the response they elicit. Essentially, this fits into the RL framework outlined in Section 6, where the agent first selects an appropriate action given its current policy and overarching goal, and subsequently updates that policy given the reward received by the ‘environment’ (i. e, the difference between the user state that the agent aimed for and the one it actually elicited).
Finally, we note that this interpersonal adaptation additionally subsumes entrainment [89, 90]. Entrainment is the process of convergence that happens between two conversational partners, a prerequisite for a successful and enjoyable conversation. It manifests as a gradual adaptation to the linguistic structure and the paralinguistic expressions of the other – and generally happens from both partners. Conversational agents must therefore include entrainment in their affordances. This requires an analysis (implicit or explicit) of their interlocutor’s manner of speaking. This analysis
can be coupled with the state and trait analysis mentioned above, and can broadly cover more granular paralinguistic markers such as pitch, tempo, and even timbre, or linguistic behaviour ranging from word-use to deeper grammatical structure.
Overall, we consider personalisation to be an exciting new frontier for ESS (and LLMs, see [88]), given that most existing systems lack a ‘feedback mechanism’ that allows them to adapt to each new user. Prior research has been largely focused on obtaining good, ‘universal’ expressivity, but with that goal now closer to sight, it might be time to switch to a more modular, malleable, and adaptive approach.
7 Foundation models and emergence
The introduction of foundation models (FMs), and especially generative large audio models (LAMs), has paved the way for a novel paradigm of synthesis, especially as it pertains to controllability. Specifically, while LAMs (so far) follow the same basic operating principles as traditional SGMs, their scale and amount of data they have consumed in training gives rise to emergent properties – properties that have not been explicitly trained for but are uncovered using appropriate prompting [91].
This phenomenon was first observed for LLMs, which were shown capable of performing tasks that were not part of their training simply by providing them with appropriate prompts [91]. In similar fashion, LAMs can be prompted to synthesise styles that were not part of their training. In practice, ‘all’ a LAM does is encapsulate all the individual steps shown in Fig. 3 in one singular architecture. Such models accept multimodal inputs in a more modular way; parts of them correspond to the output text that must be generated, and parts pertain to the style that needs to be synthesised. Crucially, inputs can also incorporate Stage II capabilities; for instance, the overall goal of the system or information about the user may be given as part of the prompt [88].
For example, an input prompt might be: “You are an intelligent assistant aiming to befriend the user. The user is a male computer science student that just returned home from the lab. Generate an uplifting message to start a conversation.” This modularity enables LAMs to benefit from the compositionality of language – rather than trained to synthesise specific styles explicitly, they learn to map longer text inputs which consolidate information about style, intent, and context into an output utterance. This allows a more flexible interface that can scale to novel situations by tapping into the world knowledge of the model. Namely, while a traditional generative model would need to be trained to generate happy, cheerful, or compassionate speech explicitly, a LAM only needs to exploit its understanding of each term (as well as its understanding of how to synthesise expressive speech) and achieve the task without having been trained for every style explicitly (though it will, of course, need to be trained for some of them). This feat alone unlocks a tremendous potential for scalability.
Naturally, given the success of LLMs in chatting but also longer-term planning, it will also be straightforward to include ESS along with the text-generating and dialogue management capabilities of existing models, and even let the model pick the suitable style on its own, thus resulting in a truly end-to-end artificial conversational agent. This means that the paradigm of foundation models has the potential to resolve a lot of the issues that are still open for ESS. Even though we are still in the early days of their development, the recent experience with LLMs and vision FMs points to a (near) future where LAMs become the norm [26].
Consequently, this state of affairs raises the same considerations as for FMs [26], namely, regarding fairness and the representation of different socio-demographic groups in the data; the alignment of those models with established ethical values; the models’ computational cost, both in terms of harm done to the environment and with respect to the limited access to that technology that the increase in computational complexity entails; the lack of interpretability; the appearance of hallucinations, with models failing to follow instructions but nevertheless producing outputs which sound plausible; and, finally, the potential that any advances introduced by FMs can be subverted by bad actors for nefarious purposes12. This is the topic of our last section.
8 Societal implications of advancing ESS systems
In this section, we discuss the societal implications that accompany the advancement of ESS research. This pertains both to its current state, but also to the expected advances we outlined in previous sections. We begin by discussing the expected limits on performance in Section 8.1. As we argue, these range beyond levels achieved by average humans. With that in mind, we continue with a discussion of a world where ESS systems are increasingly employed for a variety
of applications, starting with the negative implications of the technology. Following that, we end with a discussion of how ESS models may be aligned with the ethical values and regulatory requirements of contemporary societies.
8.1 Superhuman expressivity
Before we continue with the social implications of more advanced ESS systems, we briefly discuss potential limits to their performance. The expectations for AI success on a particular task is often bounded by estimates of human-level performance for that task. Similarly, the long-term goal of ESS has been to achieve ‘human-level’ expressivity.
However, the capacity of humans to express their desired affect varies wildly across individuals. This capacity falls partially in the premises of charisma, with different individuals being charismatic to different degrees. A prototypical example is political candidates: their speeches can be perceived as more or less charismatic by average listeners (albeit with large variability in individual perceptions) [92]. The same can be said of pundits, journalists, or influencers which increasingly dominate the public sphere, or, of course, anyone else we encounter in our daily lives.
This raises the question: what constitutes ‘superhuman’ performance? Is it beating the most charismatic of speakers? Or just being better than the ‘average Joe’?
This question becomes more pertinent when considering the different risks related to ESS systems [93]. When considering the ability of conversational agents to function as ‘artificial friends’, then, perhaps we can set the bar to the level of the average person. When thinking about autonomous AIs ruling the world, the required persuasion capabilities exceed those of even the most charismatic rulers. But the use of ESS systems for widespread misinformation campaigns orchestrated by potential bad actors might only require those systems to operate on the level of mediocre salesmen.
This forces us to acknowledge that ‘superhuman’ expressivity is around the corner, if not already there. That in turn leads us to consider societal repercussions in a world where almost anyone can deploy a small online army of influencers to spread their message – being a simple marketing campaign, a prank, or more malicious messaging such as misinformation or hate speech.
8.1.1 On the limits of measurement
On a side note, we call attention to the fact that there is nothing preventing autonomous ESS systems, especially ones trained via self-play (cf. Section 6), from truly progressing beyond human-level capabilities, in the sense of developing entirely novel, unseen modes of expression that are not employed by humans. This can be done either when conversing with humans or with other agents – and especially when combined with autonomous learning and the type of goal-seeking that Stage II ESS requires. In that process of an agent autonomously optimising its policy and learning to adapt its output in order to achieve that goal, it is plausible to manipulate its voice in ways humans are incapable of doing, and perhaps even of noticing.
An inspirational example can be found in the literature on adversarial attacks [94]. This has shown that it is possible to manipulate a signal in ways that go entirely unnoticed by humans but that can cause a neural network to change its decision in predictable ways. Indeed, this is perhaps the expected outcome in cases where artifical agents endowed with ESS abilities begin to communicate with similar agents, as exploiting the shortcuts arising due to DL’s susceptibility to adversarial examples presents an easy avenue to achieve the agent’s goal.
Perhaps more worryingly, the same is possible for human-computer interactions as well. Several studies have shown that biases in human thinking can be exploited simply by manipulating the framing of a question or even placing inconspicuous cues in the environment [95, 96] (see also Kahneman’s [96] notion of “fast” thinking). Similar fallacies can be co-opted by ESS agents in the pursuit of their goals. This can be done through a particular choice of words or by co-opting vocal cues, even ones that the user is not directly aware of. Beyond obvious ethical implications, this potential of ESS systems to progress beyond what humans are able to consciously perceive raises questions about their evaluation. This is even more pertinent for the average user of the technology – even though they might be aware of conversing with an AI, they might not be aware of the ways they are being manipulated.
Perhaps the only way to mitigate this apparent gap in evaluation is to deploy AI guardrails – systems that are explicitly trained to monitor ESS models for ‘deviant’ behaviour. In turn, this obviously raises the question of how to train these models. Generally, this remains an open problem which we expect to gain increasing prominence in the upcoming years (not only for ESS but also for all AI systems in general). With these caveats in mind, we now turn to the discussion of some negative effects that ESS can have on human societies.
8.2 Persuasion and manipulation
One of the most obvious downsides to ESS is its potential for misuse. Crafting more expressive artificial voices unlocks the possibility to scale up misinformation, unwarranted persuasion, manipulation, or outright fraud. Already, the use of voice cloning – a sister field of ESS where the goal is to simulate the identity of a particular human speaker – is raising increasing concerns. This technology can be misused to impersonate family members or persons of authority in order to manipulate the victim, and are already a pressing societal problem. However, ESS will allow fraudsters to progress even beyond that by leveraging more advanced intelligent agents to persuade their subjects.
Already, claims have emerged that text generated using commercial LLMs can be more persuasive than human-generated text [97, 98, 99]. While these findings are still preliminary, they nevertheless showcase the feasibility of using computergenerated speech for persuasion. We expect ESS to further advance this potential, as integrating paralinguistic cues can increase the effectiveness of the message [100]. Notably, personalisation (in the sense of adapting to user demographics or prior opinions) led to performance improvements, a theme we also highlighted in Section 6.3.
8.3 A metaverse of superhuman influencers
Beyond fraudulent or criminal behaviour, ESS may have negative effects even under lawful usage. Specifically, we expect that as the more advanced models we described in previous sections become increasingly available, they will be used by a number of actors to generate or manipulate digital content in order to make it more appealing. This means that the voices we encounter in digital spheres will come to be increasingly enhanced, or even fully generated, by AI. As such, society might soon find itself in a metaverse populated with superhuman ‘influencers’ – agents, human or artificial, who possess above-average charisma and expressivity. This calls into question the changes this might impart on expressivity and language itself. This much broader field of study falls under the premises of sociolinguistics, which studies the interaction between social and linguistic change, oftentimes in the landscape formed by modern media [79]. In the following paragraphs, we highlight some particular repercussions of ESS being deployed in the real-world.
The first frontier is attention. Commercials are already mired in what has been labelled a “loudness war” [101]. Yet the impact of such simplistic forms of manipulation that rely on a single cue (loudness) to draw our attention pales in comparison to the potential wave of information streams augmented with the use of advanced ESS systems. Given that attention has become a commodity in today’s “attention economy” [102], this could inspire renewed competition between commercials, news sites, and anyone else vying for our focused engagement in the digital sphere.
Especially for digital media, there are not many studies on the interplay between voice expressivity and social media dynamics (like engagement or outreach). We can, however, draw some insights from similar studies on visual aesthetics. For example, fashion brands that opt for a more expressive style in their posts (vibrant colours, modern design, energetic) have a bigger outreach than others who prefer more classic aesthetics (orderly and clear design) [103, 104]. Similarly, specific speech attributes can lead to improved visibility, which in turn creates incentives that ‘select’ for these attributes to be more widely used.
This becomes even more pertinent when considering overall social media use. Overexposure to social networking sites, especially when consumption is focused on perusing profiles heavy on visual content, has been linked to reduced self-esteem and negative self-evaluation [105, 106, 107]. While previous studies have focused on the visual component of social media, they have largely ignored the fact that they are also ripe with spoken content. Assuming the ubiquitous presence of ESS in the near future, and especially models optimised for human voice enhancement, we can expect that these models will be used to improve the outreach and appeal of social media content, similar to how facial ‘filters’ are used today. This raises the question of how social media users will react to a virtual world filled with oversaturated and overexpressive voices. Authentically charismatic speakers can use their voice to stand out of the crowd, but this ability may soon become available for everyone by simply using an off-the-shelf ESS model. On the one hand, this will level the playing field for people competing for our attention. On the other hand, it will lead to an overexposure to charismatic speakers, which will inadvertently feed into changes of our aesthetics. Whether this will need to a desensitisation effect, where people learn to ignore paralinguistic styles previously associated with charisma, or open up our senses to previously unappreciated modes of expression remains to be seen. In any case, we expect ESS to become a staple in the toolkit of professional influencers13.
Perhaps more alarmingly, they also have the potential to penetrate the sphere of ‘normal’ people. While coming in contact with overtly charismatic politicians, journalists, or celebrities may be relatively benign, even wanted, the constant use of this technology among peers might cause more harmful comparisons. To be more precise, it is well-established that voice disorders lead to reduced self-esteem [109]. While some of them are very pronounced phenomena that are readily apparent, the definition of a ‘disorder’ hinges on the definition of normality – the former is merely a (large)
deviation from the latter. With ESS technology about to move goalposts regarding what counts as normal, at least on the Internet, this might also anchor us to higher standards for normality – anything below the level of charisma attained artificially, but amply exhibited online by many of our peers, might now count as ‘problematic’. This might lead to the emergence of more negative self-evaluations of one’s voice, and, accordingly, to a reduced self-esteem, especially for groups that are considered high-risk for social anxiety, like teenagers. This is one of the areas we expect more future research to be needed in order to make ESS systems compliant with our values as a society. We outline some strategies for that in the next section.
8.4 Aligned artificial expressivity
Given all the negative social implications that ESS might cause, it is important to ensure that all ESS systems remain aligned with societal values – i. e., making ESS “friendly” [110]. Naturally, we expect this process to involve stringent regulatory guidelines, such as banning the use of deepfakes, and the accompanying effort to enforce them, which are beyond our scope here. Instead, we consider how model development can prevent even lawful uses of the technology from going awry.
We expect that ethical and regulatory guidelines will have to be ‘baked into’ a model’s behaviour during training, especially with regards to its Stage II capabilities. Inspiration for this can be drawn from the recent advances in physics-informed neural networks [111], which solve physics-related problems (e. g., material design) using DNNs but explicitly guide these DNNs to generate outputs that conform to physical laws. Similarly, we can envision ESS systems whose outputs conform to judicial laws and social ethics. For example, an ESS system that is deployed for online marketing could refrain from using persuasion techniques on particularly vulnerable users, such as children. Training ESS models – and especially the most recent foundation models – to conform to those norms is an area of active research. In terms of training, it boils down to additional constraints that need to be satisfied. These could be implemented by extending the loss function of a model or optimising its parameters in a constrained optimisation paradigm – similar to disentanglement (Section 5.1).
The most interesting open question is how to best translate ethical guidelines into constraints that can then be incorporated into training in this manner. Preliminary approaches could be based on common-sense and grounding datasets related to affective computing, such as ATOMIC [112]. However, these are so far geared towards describing what users expect in terms of a response that fits the underlying context, and do not encode ethical and legal requirements that actually correspond to what different stakeholders agree that an ESS system should and should not do.
On top of that, auditing whether ESS systems adhere to all guidelines in practice is a much more challenging endeavour. Even assuming that models are publicly accessible (e. g., through APIs available for research purposes), testing them rigorously, and periodically, to cover new releases, remains an open issue. In the most straightforward case, this would involve the use of human auditors – test users who interact with ESS systems and rate their abilities. However, this process does not scale well in practice due to the sheer number of vendors and open-source models that are even now available. Moreover, humans will inevitably bump into the measurement issues outlined in Section 8.1.1; layman users, in particular, might struggle with advanced ESS models that use nuanced strategies for manipulation. In the most extreme case, we can envision self-learning ESS systems developing capabilities that enable them to circumvent testing, a case of a so-called “Runaway AI” [113].
For all those reasons, we expect machine-based auditing to become increasingly more relevant as it offers better scalability and reproducibility. Such models are already being developed to identify spoofing attempts [114] – AIgenerated speech that is intended for malicious purposes such as identity theft. While these models are failing to capture all speech samples generated by contemporary TTS systems, they do work to some extent and are a useful tool in mitigating threats resulting from unlawful use of the technology (e. g., identity theft). A similar effort is required to monitor lawful but ethically dubious use of ESS technology. While challenging, we expect this pursuit to be fruitful and a critical step in ensuring the fair and ethical development of artificial expressivity.
9 Summary & Conclusion
We have presented an overview of the fundamental blocks required to build expressive speech synthesis systems, starting from nowadays standard statistical generative models and reaching to recently-introduced foundation models. We have also discussed open risks and highlighted areas which we expect are ripe for innovation. In summary, we see a consolidation of ESS into the broader move towards foundation models which encapsulate multiple, often wildly disparate, capabilities, as well as the emergence of longer-term planning and in-context synthesis (what we termed Stage II capabilities). This is an exciting time for ESS research, as the last handful of years have seen tremendous progress in fidelity and controllability for synthesising expressive primitives – elementary styles that can form the basic
building blocks for more complex behaviour. We anticipate that the landscape of ESS over the next decade will be largely defined by the pursuit of methods that can combine these primitives and translate them into more nuanced states, as well as a drive to ensure that models remain tethered to social and ethical norms.
