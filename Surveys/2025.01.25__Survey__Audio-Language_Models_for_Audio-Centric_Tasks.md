# Audio-Language Models for Audio-Centric Tasks: A Survey

<details>
<summary>基本信息</summary>

- 标题: "Audio-Language Models for Audio-Centric Tasks: A Survey"
- 作者:
  - 01 Yi Su (College of Computer Science and Technology, Changsha)
  - 02 Jisheng Bai (School of Marine Science and Technology, Northwestern Polytechnical University)
  - 03 Qisheng Xu (College of Computer Science and Technology, Changsha)
  - 04 Kele Xu (College of Computer Science and Technology, Changsha)
  - 05 Yong Dou (College of Computer Science and Technology, Changsha)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.15177)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv](2501.15177v1__Survey__Audio-Language_Models_for_Audio-Centric_Tasks.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Audio-Language Models (ALMs), which are trained on audio-text data, focus on the processing, understanding, and reasoning of sounds.
Unlike traditional supervised learning approaches learning from predefined labels, ALMs utilize natural language as a supervision signal, which is more suitable for describing complex real-world audio recordings.
ALMs demonstrate strong zero-shot capabilities and can be flexibly adapted to diverse downstream tasks.
These strengths not only enhance the accuracy and generalization of audio processing tasks but also promote the development of models that more closely resemble human auditory perception and comprehension.
Recent advances in ALMs have positioned them at the forefront of computer audition research, inspiring a surge of efforts to advance ALM technologies.
Despite rapid progress in the field of ALMs, there is still a notable lack of systematic surveys that comprehensively organize and analyze developments.
This deficiency not only limits researchers' comprehensive understanding and evaluation of existing technologies but also hinders the rapid adoption and improvement of new methods.
In this paper, we present a comprehensive review of ALMs with a focus on general audio tasks, aiming to fill this gap by providing a structured and holistic overview of ALMs.
Specifically, we cover:
(1) the background of computer audition and audio-language models;
(2) the foundational aspects of ALMs, including prevalent network architectures, training objectives, and evaluation methods;
(3) foundational pre-training and audio-language pre-training approaches;
(4) task-specific fine-tuning, multi-task tuning and agent systems for downstream applications;
(5) datasets and benchmarks;
(6) current challenges and future directions.

Our review provides a clear technical roadmap for researchers to understand the development and future trends of existing technologies, offering valuable references for implementation in real-world scenarios.

## 1·Introduction: 引言

Enabling machines to hear like humans and process audio-centric tasks has long been a significant challenge \cite{deshmukh2023pengi}.
Audio-Language Models (ALMs), which are trained on audio-text data, focus on the processing, understanding, and reasoning of sounds.
This area is emerging as a prominent research field at the intersection of audio processing and Natural Language Processing.
ALMs are not only applicable to basic audio tasks, such as audio classification \cite{elizalde2023clap}, but also show great potential for more complicated scenarios.
These include tasks such as audio-text retrieval \cite{yan2024MLCLAP}, audio generation \cite{liu2023audioldm}, automatic audio captioning \cite{kim2024enclap}, audio source separation \cite{liu2022_lass}, automatic speech translation \cite{rubenstein2023audiopalm}, and audio chatbots \cite{zhang2023speechgpt}.

In contrast to audio representation learning based on labeled data for specific tasks, ALM can learn from more descriptive textual information, expanding the scope of supervision to include human-annotated captions and readily available titles and descriptions from web sources \cite{laion_ai_audio_dataset}.
Natural language is well-suited for characterizing real-world audio, which frequently involves multiple overlapping sound events, thereby enabling models to learn their intrinsic relationships \cite{wu2019audio}.
Furthermore, using natural language as supervision avoids the model's reliance on task-specific predefined labels, enhancing the potential for models to generalize effectively to open-world scenarios.

As large language models (LLMs) exhibit remarkable comprehension capabilities, researchers have explored their integration as guiding components within ALMs.
However, pre-trained LLMs still face challenges in generalizing across a broad spectrum of downstream tasks \cite{zhao2023LLMsurvey}, necessitating additional transfer steps such as post-training and collaboration with other foundational models.
Within this research landscape, language provides a unified mechanism for constructing instances, enabling LLMs to undergo instruction tuning and in-context learning across diverse tasks.
This approach bridges the gap between auditory information and language understanding, facilitating the alignment of multiple components within ALMs.
Furthermore, language serves as a versatile human-machine interface, empowering users to instruct LLM agents to collaborate effectively with audio-language systems.

Despite the strong interest shown by the audio community in ALMs, there is still a lack of comprehensive surveys to review the current research status.
Existing relevant reviews include speech-language models \cite{cui2024speechlanguage, ji2024wavchat}, codec-based models \cite{wang2023codec}, ALMs for specific tasks such as audio-text retrieval \cite{koepke2022benchmarks}, automated audio captioning \cite{xu2023aac_survey}, speech-to-text translation \cite{xu2023S2TT}, and audio-language datasets \cite{wijngaard2024ald_survey}.
Here, we present the first comprehensive survey on ALMs, aiming to achieve an exhaustive coverage of the entire ALM research landscape from the perspective of model training.
Additionally, we adopt a perspective centered on general audio-centric tasks that encompasses a diverse range of audio types to provide a more detailed reflection of the current state and development of computer audition.
This survey method reflects mutual promotion and constraints among different research aspects from model to data, aids in systematically summarizes challenges and future directions, and serves as a guide for researchers and practitioners interested in ALM techniques, thereby facilitating further academic research and industrial applications in the field.

We first look at recent advances in ALM research and draw the timeline as shown in Fig.\ref{fig1: timeline}.
CLAP\cite{elizalde2023clap} is considered a significant milestone.
Previous work includes some audio-caption datasets \cite{kim2019audiocaps, drossos2020clotho, lipping2022clotho}, which were initially used for automatic audio caption model training and also served as data foundations for ALMs, inspiring subsequent work.
Since the introduction of pre-training and large-scale datasets \cite{wu2023large}, the advantages of ALMs have gradually gained attention.
Recently, numerous new works have emerged, primarily reflecting the intertwined development between pre-training and downstream models.
With increasing model research, recent studies have focused on the lack of unified evaluation standards and proposed various benchmarks.
It shows a high correlation between datasets, pre-training, downstream models, and benchmark research in ALMs.
Additionally, we observe that, driven by commercial applications, research interests have shifted more towards the speech domain.
However, audio typically encompasses a variety of environmental events, including human voices, natural sounds, music rhythms, etc., which presents significant challenges to general audio modeling \cite{chen2023beats}.

In the subsequent sections of this paper, we first introduce the background of audio-language pre-training and transfer paradigm (Section \ref{section: Background}).
We then describe the foundations of ALMs, including model architecture, training objectives, and evaluation methods (Section \ref{section: Foundations}).
Following this, we review the topics of representation pre-training (Section \ref{section: Pre-training}), downstream transfer (Section \ref{section: transfer}), and related data (Section \ref{section: Data}).
Building on these foundations, we discuss the challenges and future research directions (Section \ref{section: Challenges and future directions}), before concluding the paper (Section \ref{section: Conclusion}).

## 2·Background: 背景

This section begins by discussing the development of computer audition paradigms, with a particular focus on how ALMs are trained and transfer for downstream, as well as the reasons for the shift towards the audio-language paradigm.
We then introduce the training stages and establish a research landscape for ALMs, providing a structured basis for the comprehensive review in the following sections.

### Pre-training and Transfer Paradigm

The pre-training and transfer paradigm involves initially training on large-scale public datasets to get robust representations, and then applying knowledge gained from one context to another to enhance the performance on downstream tasks.
This approach accelerates supervised learning on downstream tasks.

However, as this paradigm evolves, two challenges emerge.
First, models may overfit by exploiting simple label mappings, achieving high performance on specific tasks without truly understanding the underlying audio content \cite{gong2024ltu}, leading to poor generalization to new data.
Second, the high cost of manual annotation exacerbates the difficulty of obtaining limited labeled datasets for learning audio representation \cite{sun2024AAA}.

To address these challenges, ALMs have been proposed to learn audio concepts through natural language supervision \cite{elizalde2023clap}.
Firstly, this form of supervision provides more details about the audio, enabling models to understand the meanings and make decisions accordingly like a human.
For example, natural language can describe the temporal order of multiple events using words such as `simultaneous`, `before`, and `after` \cite{ghosh2024compa}, better reflecting the complex composition of audio compared to predefined labels and helping models learn their intrinsic relationships \cite{wu2019audio}.
Additionally, audio-text data is easier to obtain than well-defined labeled datasets, effectively expanding the scale of datasets.
For instance, we can use `dog` or `barking` to label a dog barking, but inconsistencies among multiple annotators make it difficult to create a perfectly accurate audio dataset.
While ALMs are able to leverage the natural language processing capabilities of pre-trained models to extract similar semantic features from different forms of descriptions.
Besides human-annotated captions and translations, titles and descriptions related to audio found abundantly on the web can also serve as sources of text annotation.

### Audio-Language Training Stages

As data and model sizes grow, the training strategies for ALMs become more intricate.
From the viewpoints of representation learning and downstream task application, we first categorize the training stages aimed at enhancing task-independent audio representations as falling within the scope of pre-training, while fine-tuning and cooperating before the model is applied to downstream tasks are defined as part of the transfer process.

ALMs pre-training can be further divided into multiple stages, typically including the pre-training of foundational models, followed by audio-language pre-training on paired data.
Some may also involve further training on a broader range of data and tasks.

Although ALMs have achieved strong zero-shot capabilities in audio retrieval, transfer remains an important stage for applying models to downstream tasks.
Task-specific fine-tuning is one of the most widely used methods.
It involves supervised fine-tuning of pre-trained models on downstream task datasets and may require the addition of some adaptive modules.
Another category of methods includes transferring simultaneously on multiple tasks to make the model more universal or gain from multi-task knowledge sharing.
Unlike task-specific fine-tuning, which focuses directly on task performance, instruction tuning and in-context learning aim to enhance (or unlock) the LLM's ability to follow human instructions.
Essentially, it fine-tunes ALMs with a set of formatted instances in natural language form \cite{wei2022instruction}, thus helping the model generalize to various downstream tasks.
Multi-task transfer can also be achieved by cooperating multiple models to form an agent system.

### Research Landscape

Based on current research and our definition of audio-language training stages, we construct a research landscape for ALMs, as shown in Fig.
\ref{fig2: framework}.
From the training dimension, ALMs are divided into pre-training and transfer.
ALMs achieve multimodal perception by integrating pre-trained audio and language models, then undergo further pre-training on extensive audio-text data.
Transfer is crucial for combining these models with other networks and applying them to various downstream tasks.
Data is an essential element for model training and evaluation.
Different types of datasets can be utilized at various stages of training, and benchmarks provide unified and comprehensive standards for model evaluation, playing an important role in optimizing the models.
Therefore, research on ALMs can be developed in three fields: (a) pre-training for representation learning, (b) downstream transfer, and (c) datasets and benchmarks.

Within the scope of the research landscape, we designed a review outline as shown in Fig.\ref{fig3:outline}.
We first provide an overview of the foundation on ALMs, thereby comprehensively reviewing related work from three research fields.
According the progress across areas, we systematically propose the challenges and future directions for ALMs.

## 3·Foundations

In this section, we will introduce the general foundations of ALMs, including commonly-used architectures, training objectives, and evaluation methods.

### ALM Architectures

Audio-language models and systems typically comprising audio and language encoders, and may include other multimodal alignment mechanisms and language models.
As shown in Fig. \ref{fig4:arch}, current ALMs can generally be divided into four types: Two Towers, Two Heads, One Head and Cooperated Systems.

#### Two Towers

The basic form of ALMs, with one encoder and a projector for each modality, embeddings will be aligned in a joint space.
Among them, the most prominent landmark pretraining research is Contrastive Language-Audio Pretraining (CLAP), which incorporates a contrastive learning framework to bring audio and text descriptions into a joint multimodal space, learning the mapping relationship between the two modalities \cite{elizalde2023clap}.
Furthermore, based on the concept of modality alignment, mechanisms can be added between two independent encoders to facilitate communication, with the aim of achieving early-stage modality fusion during the representation phase \cite{li2021ctal}.

#### Two Heads

A mainstream form that utilizes one encoder and a projector for each modality, with a language model on top.
Here, `Head' refers to a network that unifies a certain modal representation space into a unified space \cite{wang2021simvlm, mustafa2022vimoe, jang2023oner}.
Language modeling has first been proven to possess strong capabilities in semantic feature extraction within the field of speech \cite{wang2023VALLE}, making it a natural design choice to incorporate language models into ALMs.
With the development of large language models, many works have utilized LLMs as the backbone for ALM inference, expanding the perceptual modalities of large language models and leveraging their emergent understanding capabilities.
This has led to classic works such as SpeechGPT \cite{zhang2023speechgpt}, Pengi \cite{deshmukh2023pengi}, and Qwen-Audio \cite{chu2023qwenaudio}, making Two Heads a unified architecture of Large Audio-Language Models.
In this structure, modality fusion can also be promoted through communication mechanisms between encoders \cite{zhao2024mint}.
It is important to note that in some works, text inputs may only undergo tokenization without the need for a dedicated text encoder, and these models can be considered under a special type of Two Heads framework.

#### One Head

A unified multimodal input form that uses one encoder to handle two different modalities simultaneously, with a language model on top.
In the vision community, a line of work has conducted research on the One Head architecture based on the view that the same multimodal processing module can achieve better alignment.
That is, using a unified space to represent two modalities.
However, there are relatively few related studies in audio-language \cite{sachidananda2022calm}.

#### Cooperated Systems

This system employs an LLM as a planning agent and comprises various model types mentioned above.
Its design facilitates the selection and utilization of each model's inherent complementary strengths, tailored to downstream task requirements.
Through the collaboration of these diverse models, the system can tackle a wider array of complex tasks compared to a solitary model alone \cite{li2024survey}.

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论