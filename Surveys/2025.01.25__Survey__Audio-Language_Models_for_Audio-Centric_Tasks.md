# Audio-Language Models for Audio-Centric Tasks: A Survey

<details>
<summary>基本信息</summary>

- 标题: "Audio-Language Models for Audio-Centric Tasks: A Survey"
- 作者:
  - 01 Yi Su (College of Computer Science and Technology, Changsha)
  - 02 Jisheng Bai (School of Marine Science and Technology, Northwestern Polytechnical University)
  - 03 Qisheng Xu (College of Computer Science and Technology, Changsha)
  - 04 Kele Xu (College of Computer Science and Technology, Changsha)
  - 05 Yong Dou (College of Computer Science and Technology, Changsha)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.15177)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv](2501.15177v1__Survey__Audio-Language_Models_for_Audio-Centric_Tasks.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Audio-Language Models (ALMs), which are trained on audio-text data, focus on the processing, understanding, and reasoning of sounds.
Unlike traditional supervised learning approaches learning from predefined labels, ALMs utilize natural language as a supervision signal, which is more suitable for describing complex real-world audio recordings.
ALMs demonstrate strong zero-shot capabilities and can be flexibly adapted to diverse downstream tasks.
These strengths not only enhance the accuracy and generalization of audio processing tasks but also promote the development of models that more closely resemble human auditory perception and comprehension.
Recent advances in ALMs have positioned them at the forefront of computer audition research, inspiring a surge of efforts to advance ALM technologies.
Despite rapid progress in the field of ALMs, there is still a notable lack of systematic surveys that comprehensively organize and analyze developments.
This deficiency not only limits researchers' comprehensive understanding and evaluation of existing technologies but also hinders the rapid adoption and improvement of new methods.
In this paper, we present a comprehensive review of ALMs with a focus on general audio tasks, aiming to fill this gap by providing a structured and holistic overview of ALMs.
Specifically, we cover:
(1) the background of computer audition and audio-language models;
(2) the foundational aspects of ALMs, including prevalent network architectures, training objectives, and evaluation methods;
(3) foundational pre-training and audio-language pre-training approaches;
(4) task-specific fine-tuning, multi-task tuning and agent systems for downstream applications;
(5) datasets and benchmarks;
(6) current challenges and future directions.

Our review provides a clear technical roadmap for researchers to understand the development and future trends of existing technologies, offering valuable references for implementation in real-world scenarios.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论