# Audio-Language Models for Audio-Centric Tasks: A Survey

<details>
<summary>基本信息</summary>

- 标题: "Audio-Language Models for Audio-Centric Tasks: A Survey"
- 作者:
  - 01 Yi Su (College of Computer Science and Technology, Changsha)
  - 02 Jisheng Bai (School of Marine Science and Technology, Northwestern Polytechnical University)
  - 03 Qisheng Xu (College of Computer Science and Technology, Changsha)
  - 04 Kele Xu (College of Computer Science and Technology, Changsha)
  - 05 Yong Dou (College of Computer Science and Technology, Changsha)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.15177)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv](2501.15177v1__Survey__Audio-Language_Models_for_Audio-Centric_Tasks.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Audio-Language Models (ALMs), which are trained on audio-text data, focus on the processing, understanding, and reasoning of sounds.
Unlike traditional supervised learning approaches learning from predefined labels, ALMs utilize natural language as a supervision signal, which is more suitable for describing complex real-world audio recordings.
ALMs demonstrate strong zero-shot capabilities and can be flexibly adapted to diverse downstream tasks.
These strengths not only enhance the accuracy and generalization of audio processing tasks but also promote the development of models that more closely resemble human auditory perception and comprehension.
Recent advances in ALMs have positioned them at the forefront of computer audition research, inspiring a surge of efforts to advance ALM technologies.
Despite rapid progress in the field of ALMs, there is still a notable lack of systematic surveys that comprehensively organize and analyze developments.
This deficiency not only limits researchers' comprehensive understanding and evaluation of existing technologies but also hinders the rapid adoption and improvement of new methods.
In this paper, we present a comprehensive review of ALMs with a focus on general audio tasks, aiming to fill this gap by providing a structured and holistic overview of ALMs.
Specifically, we cover:
(1) the background of computer audition and audio-language models;
(2) the foundational aspects of ALMs, including prevalent network architectures, training objectives, and evaluation methods;
(3) foundational pre-training and audio-language pre-training approaches;
(4) task-specific fine-tuning, multi-task tuning and agent systems for downstream applications;
(5) datasets and benchmarks;
(6) current challenges and future directions.

Our review provides a clear technical roadmap for researchers to understand the development and future trends of existing technologies, offering valuable references for implementation in real-world scenarios.

## 1·Introduction: 引言

Enabling machines to hear like humans and process audio-centric tasks has long been a significant challenge \cite{deshmukh2023pengi}.
Audio-Language Models (ALMs), which are trained on audio-text data, focus on the processing, understanding, and reasoning of sounds.
This area is emerging as a prominent research field at the intersection of audio processing and Natural Language Processing.
ALMs are not only applicable to basic audio tasks, such as audio classification \cite{elizalde2023clap}, but also show great potential for more complicated scenarios.
These include tasks such as audio-text retrieval \cite{yan2024MLCLAP}, audio generation \cite{liu2023audioldm}, automatic audio captioning \cite{kim2024enclap}, audio source separation \cite{liu2022_lass}, automatic speech translation \cite{rubenstein2023audiopalm}, and audio chatbots \cite{zhang2023speechgpt}.

In contrast to audio representation learning based on labeled data for specific tasks, ALM can learn from more descriptive textual information, expanding the scope of supervision to include human-annotated captions and readily available titles and descriptions from web sources \cite{laion_ai_audio_dataset}.
Natural language is well-suited for characterizing real-world audio, which frequently involves multiple overlapping sound events, thereby enabling models to learn their intrinsic relationships \cite{wu2019audio}.
Furthermore, using natural language as supervision avoids the model's reliance on task-specific predefined labels, enhancing the potential for models to generalize effectively to open-world scenarios.

As large language models (LLMs) exhibit remarkable comprehension capabilities, researchers have explored their integration as guiding components within ALMs.
However, pre-trained LLMs still face challenges in generalizing across a broad spectrum of downstream tasks \cite{zhao2023LLMsurvey}, necessitating additional transfer steps such as post-training and collaboration with other foundational models.
Within this research landscape, language provides a unified mechanism for constructing instances, enabling LLMs to undergo instruction tuning and in-context learning across diverse tasks.
This approach bridges the gap between auditory information and language understanding, facilitating the alignment of multiple components within ALMs.
Furthermore, language serves as a versatile human-machine interface, empowering users to instruct LLM agents to collaborate effectively with audio-language systems.

Despite the strong interest shown by the audio community in ALMs, there is still a lack of comprehensive surveys to review the current research status.
Existing relevant reviews include speech-language models \cite{cui2024speechlanguage, ji2024wavchat}, codec-based models \cite{wang2023codec}, ALMs for specific tasks such as audio-text retrieval \cite{koepke2022benchmarks}, automated audio captioning \cite{xu2023aac_survey}, speech-to-text translation \cite{xu2023S2TT}, and audio-language datasets \cite{wijngaard2024ald_survey}.
Here, we present the first comprehensive survey on ALMs, aiming to achieve an exhaustive coverage of the entire ALM research landscape from the perspective of model training.
Additionally, we adopt a perspective centered on general audio-centric tasks that encompasses a diverse range of audio types to provide a more detailed reflection of the current state and development of computer audition.
This survey method reflects mutual promotion and constraints among different research aspects from model to data, aids in systematically summarizes challenges and future directions, and serves as a guide for researchers and practitioners interested in ALM techniques, thereby facilitating further academic research and industrial applications in the field.

We first look at recent advances in ALM research and draw the timeline as shown in Fig.\ref{fig1: timeline}.
CLAP\cite{elizalde2023clap} is considered a significant milestone.
Previous work includes some audio-caption datasets \cite{kim2019audiocaps, drossos2020clotho, lipping2022clotho}, which were initially used for automatic audio caption model training and also served as data foundations for ALMs, inspiring subsequent work.
Since the introduction of pre-training and large-scale datasets \cite{wu2023large}, the advantages of ALMs have gradually gained attention.
Recently, numerous new works have emerged, primarily reflecting the intertwined development between pre-training and downstream models.
With increasing model research, recent studies have focused on the lack of unified evaluation standards and proposed various benchmarks.
It shows a high correlation between datasets, pre-training, downstream models, and benchmark research in ALMs.
Additionally, we observe that, driven by commercial applications, research interests have shifted more towards the speech domain.
However, audio typically encompasses a variety of environmental events, including human voices, natural sounds, music rhythms, etc., which presents significant challenges to general audio modeling \cite{chen2023beats}.

In the subsequent sections of this paper, we first introduce the background of audio-language pre-training and transfer paradigm (Section \ref{section: Background}).
We then describe the foundations of ALMs, including model architecture, training objectives, and evaluation methods (Section \ref{section: Foundations}).
Following this, we review the topics of representation pre-training (Section \ref{section: Pre-training}), downstream transfer (Section \ref{section: transfer}), and related data (Section \ref{section: Data}).
Building on these foundations, we discuss the challenges and future research directions (Section \ref{section: Challenges and future directions}), before concluding the paper (Section \ref{section: Conclusion}).

## 2·Background: 背景

This section begins by discussing the development of computer audition paradigms, with a particular focus on how ALMs are trained and transfer for downstream, as well as the reasons for the shift towards the audio-language paradigm.
We then introduce the training stages and establish a research landscape for ALMs, providing a structured basis for the comprehensive review in the following sections.

### Pre-training and Transfer Paradigm

The pre-training and transfer paradigm involves initially training on large-scale public datasets to get robust representations, and then applying knowledge gained from one context to another to enhance the performance on downstream tasks.
This approach accelerates supervised learning on downstream tasks.

However, as this paradigm evolves, two challenges emerge.
First, models may overfit by exploiting simple label mappings, achieving high performance on specific tasks without truly understanding the underlying audio content \cite{gong2024ltu}, leading to poor generalization to new data.
Second, the high cost of manual annotation exacerbates the difficulty of obtaining limited labeled datasets for learning audio representation \cite{sun2024AAA}.

To address these challenges, ALMs have been proposed to learn audio concepts through natural language supervision \cite{elizalde2023clap}.
Firstly, this form of supervision provides more details about the audio, enabling models to understand the meanings and make decisions accordingly like a human.
For example, natural language can describe the temporal order of multiple events using words such as `simultaneous`, `before`, and `after` \cite{ghosh2024compa}, better reflecting the complex composition of audio compared to predefined labels and helping models learn their intrinsic relationships \cite{wu2019audio}.
Additionally, audio-text data is easier to obtain than well-defined labeled datasets, effectively expanding the scale of datasets.
For instance, we can use `dog` or `barking` to label a dog barking, but inconsistencies among multiple annotators make it difficult to create a perfectly accurate audio dataset.
While ALMs are able to leverage the natural language processing capabilities of pre-trained models to extract similar semantic features from different forms of descriptions.
Besides human-annotated captions and translations, titles and descriptions related to audio found abundantly on the web can also serve as sources of text annotation.

### Audio-Language Training Stages

As data and model sizes grow, the training strategies for ALMs become more intricate.
From the viewpoints of representation learning and downstream task application, we first categorize the training stages aimed at enhancing task-independent audio representations as falling within the scope of pre-training, while fine-tuning and cooperating before the model is applied to downstream tasks are defined as part of the transfer process.

ALMs pre-training can be further divided into multiple stages, typically including the pre-training of foundational models, followed by audio-language pre-training on paired data.
Some may also involve further training on a broader range of data and tasks.

Although ALMs have achieved strong zero-shot capabilities in audio retrieval, transfer remains an important stage for applying models to downstream tasks.
Task-specific fine-tuning is one of the most widely used methods.
It involves supervised fine-tuning of pre-trained models on downstream task datasets and may require the addition of some adaptive modules.
Another category of methods includes transferring simultaneously on multiple tasks to make the model more universal or gain from multi-task knowledge sharing.
Unlike task-specific fine-tuning, which focuses directly on task performance, instruction tuning and in-context learning aim to enhance (or unlock) the LLM's ability to follow human instructions.
Essentially, it fine-tunes ALMs with a set of formatted instances in natural language form \cite{wei2022instruction}, thus helping the model generalize to various downstream tasks.
Multi-task transfer can also be achieved by cooperating multiple models to form an agent system.

### Research Landscape

Based on current research and our definition of audio-language training stages, we construct a research landscape for ALMs, as shown in Fig.
\ref{fig2: framework}.
From the training dimension, ALMs are divided into pre-training and transfer.
ALMs achieve multimodal perception by integrating pre-trained audio and language models, then undergo further pre-training on extensive audio-text data.
Transfer is crucial for combining these models with other networks and applying them to various downstream tasks.
Data is an essential element for model training and evaluation.
Different types of datasets can be utilized at various stages of training, and benchmarks provide unified and comprehensive standards for model evaluation, playing an important role in optimizing the models.
Therefore, research on ALMs can be developed in three fields: (a) pre-training for representation learning, (b) downstream transfer, and (c) datasets and benchmarks.

Within the scope of the research landscape, we designed a review outline as shown in Fig.\ref{fig3:outline}.
We first provide an overview of the foundation on ALMs, thereby comprehensively reviewing related work from three research fields.
According the progress across areas, we systematically propose the challenges and future directions for ALMs.

## 3·Foundations

In this section, we will introduce the general foundations of ALMs, including commonly-used architectures, training objectives, and evaluation methods.

### ALM Architectures

Audio-language models and systems typically comprising audio and language encoders, and may include other multimodal alignment mechanisms and language models.
As shown in Fig. \ref{fig4:arch}, current ALMs can generally be divided into four types: Two Towers, Two Heads, One Head and Cooperated Systems.

#### Two Towers

The basic form of ALMs, with one encoder and a projector for each modality, embeddings will be aligned in a joint space.
Among them, the most prominent landmark pretraining research is Contrastive Language-Audio Pretraining (CLAP), which incorporates a contrastive learning framework to bring audio and text descriptions into a joint multimodal space, learning the mapping relationship between the two modalities \cite{elizalde2023clap}.
Furthermore, based on the concept of modality alignment, mechanisms can be added between two independent encoders to facilitate communication, with the aim of achieving early-stage modality fusion during the representation phase \cite{li2021ctal}.

#### Two Heads

A mainstream form that utilizes one encoder and a projector for each modality, with a language model on top.
Here, `Head' refers to a network that unifies a certain modal representation space into a unified space \cite{wang2021simvlm, mustafa2022vimoe, jang2023oner}.
Language modeling has first been proven to possess strong capabilities in semantic feature extraction within the field of speech \cite{wang2023VALLE}, making it a natural design choice to incorporate language models into ALMs.
With the development of large language models, many works have utilized LLMs as the backbone for ALM inference, expanding the perceptual modalities of large language models and leveraging their emergent understanding capabilities.
This has led to classic works such as SpeechGPT \cite{zhang2023speechgpt}, Pengi \cite{deshmukh2023pengi}, and Qwen-Audio \cite{chu2023qwenaudio}, making Two Heads a unified architecture of Large Audio-Language Models.
In this structure, modality fusion can also be promoted through communication mechanisms between encoders \cite{zhao2024mint}.
It is important to note that in some works, text inputs may only undergo tokenization without the need for a dedicated text encoder, and these models can be considered under a special type of Two Heads framework.

#### One Head

A unified multimodal input form that uses one encoder to handle two different modalities simultaneously, with a language model on top.
In the vision community, a line of work has conducted research on the One Head architecture based on the view that the same multimodal processing module can achieve better alignment.
That is, using a unified space to represent two modalities.
However, there are relatively few related studies in audio-language \cite{sachidananda2022calm}.

#### Cooperated Systems

This system employs an LLM as a planning agent and comprises various model types mentioned above.
Its design facilitates the selection and utilization of each model's inherent complementary strengths, tailored to downstream task requirements.
Through the collaboration of these diverse models, the system can tackle a wider array of complex tasks compared to a solitary model alone \cite{li2024survey}.

### Training Objectives

Training objectives are used to guide model learning during pre-training and transfer.
As shown in Fig.
\ref{fig5: objectives}(a), pre-training contrastive, generative, or discriminative objectives guide the model to learn pretext tasks on audio, text, or audio-text paired data, aiming to learn audio semantic features and audio-language correlations.
As illustrated in Fig.
\ref{fig5: objectives}(b), task-specific fine-tuning as a commonly adopted transfer method, employs either generative or discriminative objectives depending on the context.
Another line of transfer methods with generative language models in Fig.
\ref{fig5: objectives}(c) aims to improve unlock pre-training models' generalization ability on downstream tasks through standard language modeling objectives.
Note that the above training objectives can be used in combination.

#### Contrastive Objectives

It is the most commonly used type of training objective in audio-language pre-training, which aims to train the model to bring positive sample pairs closer together and push negative sample pairs further apart within a shared embedding space for audio and text, thereby learning the audio-language correlations and obtaining distinguishable representations between audio samples.
The most widely implemented approach for this category of objective is using a symmetric audio-text infoNCE \cite{oord2018CPC} loss function to measure the similarity between audio and text embeddings.
Let the $i-th$ sample pair be ${x_{i}, t_{i}}$.
Given an audio encoder $h_a(\cdot)$ and a text encoder $h_t(\cdot)$, the embedding vectors for the audio sample $x_{i}$ and its corresponding caption $t_{i}$ can be represented as:

$$
z_{i}^{a}=h_a(x_i)
$$

$$
z_{i}^{t}=h_t(t_i)
$$

The similarity between audio and text embeddings is calculated using the dot product.
The infoNCE loss for the audio dimension, $l_{a}$, is defined as the average of a normalized function measuring the similarity of different texts to the same audio query.
Similarly, the contrastive loss for the text dimension, $l_{t}$, measures the similarity of different audios to the same text query.
For a batch with $B$ audio-text pairs, we have:

$$
l_{i}^{a}=-\log \frac{\exp \left(z_{i}^{a} \cdot z_{i}^{t} / \tau\right)}{\sum_{j=1}^{B} \exp \left(z_{i}^{a} \cdot z_{j}^{t} / \tau\right)}
$$

$$
l_{i}^{t}=-\log \frac{\exp \left(z_{i}^{t} \cdot z_{i}^{a} / \tau\right)}{\sum_{j=1}^{B} \exp \left(z_{i}^{t} \cdot z_{j}^{a} / \tau\right)}
$$

where $\tau$ represents a temperature parameter used to scale the range of logits.
When setting the contrastive objective to be completely symmetrical, the total loss for the audio-text pairs in one batch can be defined as:

$$
\mathcal{L}_{con}=\frac{1}{2B} \sum_{i=1}^{B}(l_{i}^{a} + l_{i}^{t})
$$

#### Generative Objectives

Generative methods have proven to be powerful and effective in audio representation learning.
They lead the network in learning semantic features of audio through pretext tasks such as masked reconstruction \cite{huang2022audiomae}.
In audio-language pre-training, similar approaches are introduced, guiding representation learning through audio or audio-related language generation tasks.
These methods are often combined with contrastive learning to bolster the robustness of learned audio embeddings or improve computational efficiency.
During transfer, these generative objectives can help the model adapt to corresponding generative tasks and are widely used in transfer with generative LLMs.


During pre-training, the most common method for audio mask reconstruction is based on the audio spectrogram. Let $M\left (\cdot \right )$ denote the masking operation, and let $f_{a}\left (\cdot \right )$ and $p_{ae}\left (\cdot \right )$ represent the spectrogram encoder and audio embedding projection layer, respectively. To achieve masked spectrogram prediction, an additional decoder $f_{a}^{-1} \left (\cdot \right )$ is added to the model. For an audio sample with the original spectrogram $a$, spectrogram reconstruction can be represented as $\hat{a} = f_{a}^{-1} (p_{ae}(f_{a} (M(a))))$. Using $\hat{a}_{n}$ and $a_{n}$ to denote the decoder prediction output of the $n-th$ masked spectrogram patch and the original true patch, respectively. For a spectrogram divided into $N$ patches, the audio reconstruction loss used for self-supervision can be defined as minimizing the $L2$ (mean squared error, MSE) loss:

$$
\mathcal{L}_{ar}=\frac{1}{N}\sum _{n=1}^{N}\left\|\hat{a}_{i}-a_{i}\right\|_{2}
$$

Since ALMs include both audio and language modalities as inputs, some works have similarly designed masked cross-modal reconstruction tasks, which typically involve methods such as cross-attention mechanisms to communicate between the encoders of the two modalities and perform reconstruction on the audio representation.

During audio generation transfer, training objectives essentially enhance the model's performance by minimizing the distance between the predicted embedding $\hat{z}$ and its corresponding ground truth $z$. This distance metric can be chosen based on the situation, with common options including $L1$ and $L2$ distances. The training objective can also be set as a weighted sum of multiple distances. For an audio sample, generative audio modeling objective can be represented as:

$$
\mathcal{L}_{am}=\frac{1}{T} \frac{1}{L} \sum_{t=1}^{T} \sum_{l=1}^{L} \alpha \left\| \hat{z}_{t, l}-{z}_{t, l}\right\|_{1}+\beta \left\| \hat{z}_{t, l}-{z}_{t, l}\right\|_{2}
$$

where $T$ denotes the total number of frames, $L$ denotes embedding dimension, and $\alpha$ and $\beta$ are weight hyperparameters. In addition to the method that uses embedding differences as a training objective, it is also possible to directly train jointly with the decoder network, designing the training objective directly on the predicted audio amplitude. For instance, aiming to learn a decoder net $h_{de}\left (\cdot \right )$ that maps known audio $x_{i}$ and query $t_{i}$ to a predicted audio $\hat{a}_i$. Denote $z_{i}^{t}$ as the embedding of the language query, the training objective could be to minimize the $L1$ (mean absolute error, MAE) loss between the amplitude spectrogram $|a_i|$ of the ground truth target audio source and the predicted $|\hat{a}_i|$:

$$
|\hat{a}_i| = h_{de}\left (z_{i}^{t} \right )
$$

$$
\mathcal{L}^{'}_{am} = \sum_{i=1}^B \left \| |{a}_i| - |\hat{a}_i| \right \|_{1}
$$

Generative language modeling objectives are used to guide ALM in generating audio-related text that is consistent with the ground truth. On one hand, they can be used to force the model to learn audio-language correlations to promote representation learning, and help improve the model's performance on corresponding downstream tasks (e.g., automatic caption generation). On the other hand, as a standard loss for generative language modeling, it is also commonly used during ALM transfer with language model \cite{JMLR2023palm}.

An additional text decoder (language pre-trained model) is required in language generation. When using an autoregressive language model to predict tokenized text associated with a given audio sample $x$, the language modeling objective is defined as minimizing the negative log-likelihood of the current ground-truth token (cross-entropy, CE loss), given the previous ground-truth tokens:

$$
\mathcal{L}_{lm}=-\frac{1}{T} \sum_{t=1}^{T} \log P_{\theta}\left(y_{t} \mid y_{1: t-1}, x\right)
$$

Here, $y_{t}$ is the $t-th$ ground-truth token of the given caption $y$, $T$ is the total length of the caption, and $\theta$ represents the model's learnable parameters. Non-autoregressive language models also adopt a similar negative log likelihood objective without temporal averaging.

#### Discriminative Objectives

They are used to guide the model in learning to predict the correct label, and can be broadly categorized into classification and retrieval objectives. Here, we take the cross-entropy function as an example to uniformly calculate the loss between the predicted output and the ground truth.

Audio classification is one of the most extensively studied downstream tasks. It aims to recognize patterns from specific audio inputs to predict given labels. For a batch of $B$ audio samples, the objective can be expressed as:

$$
\mathcal{L}_{cls} = -\frac{1}{B} \sum_{i=1}^{B} \sum_{c=1}^{C} y_{i,c} \log(\hat{p}_{i,c})
\label{eq:cls}
$$

where $C$ is the number of classes. $y_{i,c}$ is the true label of the $i-th$ sample in class $c$ (0 or 1). $\hat{p}_{i,c}$ is the predicted probability of the $i-th$ sample in class $c$.

Audio-Text Retrieval (ATR) aims to find matching items between audio clips and textual descriptions. Given a query in one modality (audio or text), the goal is to retrieve the corresponding item from a pool of candidates in the other modality. Here, we use a scoring function $S\left (\cdot \right )$ to represent the model's prediction output by measuring the correlation between audio and text.

Denote $Y$ as a set of $m$ possible caption texts, the correspondence caption of a given audio $x_i$ is

$$
\hat{y}_i = \arg\max_{y_j \in Y} \frac{\exp(S(z_{i}^{a}, z_{j}^{t}))}{\sum_{k=1}^{m} \exp(S(z_{i}^{a}, z_{k}^{t}))}
$$

Then, retrieval tasks can be considered as instance-level classification, so the objective can be formatted as:

$$
\mathcal{L}_{atr} = - \sum_{i=1}^B \log(\hat{y}_i)
$$

Specially, audio-text matching is pretext task designed to forcing a more fine-grained alignment between audio and text embeddings than contrastive pre-training. It train the model to predict whether a given text correctly describes a provided audio, can be seen as a binary classification task requiring the model to determine whether an audio-language pair is a match or not. The matching objective can be defined as:

$$
\mathcal{L}_{mat}=p \log \mathcal{S}\left(z^{a}, z^{t}\right)+(1-p) \log \left(1-\mathcal{S}\left(z^{a}, z^{t}\right)\right)
$$

Here, $p$ is 1 if the audio and text are paired, otherwise it is 0.

### Evaluation Methods

Model evaluation aims to fairly measure the performance of models under the same experimental setup and tasks. The evaluation methods for ALMs mainly include zero-shot (ZS), linear probe, supervised fine-tuning, and instruction-following evaluation. Each of these methods has their own focus, collectively forming the basis for a comprehensive performance evaluation of ALMs.

#### Zero-Shot Evaluation

It focuses on assessing the ability of contrastive ALMs in open-set retrieval. This zero-shot prediction is primarily conducted by measuring the similarity between audio and text embeddings. Notably, aside from direct text-to-audio or audio-to-text retrieval, considering that labels are also a special form of language. This allows for zero-shot evaluation on classification tasks such as sound event detection and emotion recognition.

#### Linear Probe Evaluation

It is a common experimental setup for evaluating pre-trained models, and it is used to assess the audio representation of ALMs. It involves adding a linear header (usually an MLP) to the frozen pre-trained model and training the header on downstream tasks, allowing the model to be adapted for specific tasks and datasets. Although this simple transfer learning setup may not achieve optimal performance on specific tasks, it minimizes the variables introduced, hence its widespread adoption for conducting fair representational evaluations. In linear probe evaluation, the selected tasks are usually fundamental linear tasks like classification.

#### Supervised Fine-tune Evaluation

It further examines the generalization ability of the pre-trained model to downstream tasks and its task-specific performance. For a given downstream task, the audio encoder is unfrozen and fine-tuned along with an attached head. The model's performance is then validated on the test set and compared with state-of-the-art (SOTA) models for that task. This evaluation approach not only covers classification tasks but often combines various types of tasks including detection and generation, thoroughly testing the model's adaptability and flexibility.

#### Instruction-Following Evaluation

It emphasizes the model's ability to understand complex instructions and respond accurately, serving as an important indicator of the task generalization ability of LALMs. This evaluation method can be considered a special type of zero-shot evaluation or supervised fine-tuning evaluation, depending on whether instruction tuning is performed.

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论