# 当大语言模型遇上语音: 集成方法综述

<details>
<summary>基本信息</summary>

- 标题: "When Large Language Models Meet Speech: A Survey on Integration Approaches"
- 作者:
  - 01 Zhengdong Yang,
  - 02 Shuichiro Shimizu,
  - 03 Yahan Yu,
  - 04 Chenhui Chu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.19548)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv](PDF/S20250226__When_LLMs_Meet_Speech_A_Survey_on_Integration_Approaches[2502.19548v1].pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Recent advancements in large language models (LLMs) have spurred interest in expanding their application beyond text-based tasks.
A large number of studies have explored integrating other modalities with LLMs, notably speech modality, which is naturally related to text.
This paper surveys the integration of speech with LLMs, categorizing the methodologies into three primary approaches: text-based, latent-representation-based, and audio-token-based integration.
We also demonstrate how these methods are applied across various speech-related applications and highlight the challenges in this field to offer inspiration for future research.

## 1·引言

In recent years, the field of natural language processing (NLP) has been greatly reshaped by the development of large language models (LLMs)[^brown-etal-2020], [^touvron-etal-2023-llama], [^geminiteam-2024-gemini], [^bai-2023-qwen], [^deepseekai-2024-deepseek].
These models have not only shown excellent ability in understanding and generating text but have also sparked interest in their potential applicability across other modalities, including speech.
The integration of speech and LLMs offers a wide range of potential applications, including speech translation, conversational chatbots, and enhanced human-computer interaction in robotics.

Survey papers have reviewed speech language models [^peng-etal-2024-speechllmsurvey], [^cui-etal-2025-speechlmsurvey], as well as audio language models[^latif-2023-large-audio] and multimodal language models[^ghosh-2024-vision-language-model], [^zhang-2024-mmllm].
However, there still lacks a survey specifically on the integration approaches of speech and LLMs, posing a challenge for researchers seeking to address this complex problem.

Distinct from other survey papers regarding speech and LLMs, this paper provides insight into the problem by specifically surveying the integration approaches of speech and LLMs.
Studies on LLM tokenization[^chai-etal-2024-tokenization], [^tao-etal-2024-scalingvocab] suggests that tokenization methods can affect the performance of LLMs.
On the other hand, studies on tokenization methods for speech language modeling[^gat-etal-2023-discreteslm], [^borsos-etal-2023-audiolm] also show that speech tokenization methods can affect the performance of speech language models.
In contrast to text processing, speech--LLM integration approaches are not limited to discrete tokenization, as presented in this paper.
Therefore, studying the integration between speech and LLMs can be a key to innovations.

In this paper, we systematically categorize a substantial body of research on speech--LLM integration,\footnote{One challenge that affects the scope of studies is the lack of standard definition for LLMs.
In this paper, we adopt the loose definition by [^zhao-etal-2023-survey-llm], focusing on models with over 10 billion parameters, while also including notable studies with smaller models.}
and provide a clear taxonomy of the integration approaches.
We broadly categorize the integration into the following three types:
(a) **Text-based integration**: LLMs process textual data, integrated with speech-to-text and/or text-to-speech models;
(b) **Latent-representation-based integration**: Latent vector representations that encode speech data are utilized, mainly as inputs to LLMs;
(c) **Audio-token-based integration**: Speech tokens, such as semantic tokens and/or acoustic tokens, are used as the inputs/outputs for LLMs.
The overview of these approaches is illustrated in Figure \ref{fig:overview}, and the detailed taxonomy with representative studies is presented in Figure \ref{fig:taxonomy}.

## 2·背景

### 2.1·语言建模

Language modeling dates back to statistical models like $N$-gram models[^brown-etal-1992-ngram], which were central to early NLP and automatic speech recognition (ASR) systems[^bahl-etal-1983].
Neural-network-based language modeling was introduced by[^bengio-etal-2000], formulating the probability
\[
 P(w_{1:T}) = \prod_{t=1}^T P(w_t | w_{1:t-1}),
\]
where $w_t$ is the $t$-th token (text subwords or speech tokens) and $w_{i:j} = (w_i, w_{i + 1}, \cdots, w_j)$ is the subsequence from $i$-th token to $j$-th token.

Advances in hardware enabled neural models to scale, leading to innovations such as sequence-to-sequence learning[^sutskever-etal-2014-seq2seq] and the attention mechanism[^bahdanau-etal-2015].
The Transformer architecture[^vaswani-etal-2017] revolutionalizes language modeling using self-attention by efficiently modeling long-range dependencies utilizing parallelized computation.
Decoder-only Transformer[^liu-etal-2018] gains prominence through the multi-task learning paradigm with generative pre-training and discrinative fine-tuning[^radford-etal-2018].
Scaling these models[^radford-etal-2019], [^brown-etal-2020] shows generalization to multiple tasks without explicit supervision and achieves comparative performance against task-specific models, which leads to the advent of LLM era.
Techniques such as instruction tuning[^wei-etal-2022] and alignment to human preference via reinforcement learning[^ouyang-etal-2022] further advance LLM capabilities.
For a comprehensive overview of LLMs, we refer the readers to LLM survey papers[^zhao-etal-2023-survey-llm], [^minaee-etal-2024].

### 2.2·语音表示

Speech is captured as waveform signals, sampled at a specific sampling rate and quantization value, and represented as a sequence of amplitude values.
For deep learning applications, they are often converted to speech representations with shorter sequence lengths, such as log Mel filterbanks, which we refer to as **filterbanks**.
Following the success of the unsupervised pre-training approach in other fields, self-supervised speech models (S3Ms)[^baevski-etal-2020-wav2vec2], [^hsu-etal-2021-hubert], [^chen-etal-2022-wavlm] were introduced.
These models predict masked frames to learn representations that capture phonetic information [^choi-etal-2024], along with a wide range of speech characteristics, such as speaker identity, paralinguistic information, and word-level information [^pasad-etal-2024].
We refer to these contextualized frame-level features as **latent representations**.

To integrate speech (which is naturally continuous) with LLMs (which are designed to handle discrete tokens), **audio tokens** that are more discrete than previous representations have been studied recently.
There are two mainstream approaches to modeling speech as audio tokens: **semantic tokens** and **acoustic tokens**.
> Different studies use different terms for these tokens (e.g., [^lakhotia-etal-2021-gslm] used the term ``units'' for semantic tokens).
> We use the terms semantic tokens and acoustic tokens as in [^borsos-etal-2023-audiolm] and the term audio tokens to represent either or both of them similar to [^defossez-etal-2024-moshi].
> We note that there are other efforts than semantic or acoustic tokens to discretize speech signals [^bai-etal-2024-dmel].

Semantic tokens can be obtained from S3Ms by discretizing latent representations using $k$-means.
Acoustic tokens have been studied in the line of audio codecs (e.g., MP3 [^iso11172-3], Opus [^rfc6716], etc.), which aim to compress audio signals.
Neural audio codecs [^zeghidour-etal-2021-soundstream], [^defossez-etal-2023-encodec], [^kumar-etal-2023-dac] have recently gained traction as powerful tools to improve coding efficiency as well as perceptual quality.
Because acoustic tokens exhibit better quality in producing output speech signals [^borsos-etal-2023-audiolm] but fail to capture semantic information like semantic tokens do, recent studies combine both to obtain better representations for speech-language modeling [^wang-etal-2023-viola], [^zhang-etal-2024-speechtokenizer], [^defossez-etal-2024-moshi].

## 3·基于文本的集成方法

This approach primarily utilizes text as both input and output for LLMs.
One of the most direct implementations of this method is through **cascaded integration**, which is highly adaptable to various tasks.
Within tasks like ASR, LLMs are not limited to working with the final recognition results but can also operate on intermediate hypotheses.
This allows for sophisticated operations like **LLM rescoring**, and **LLM GER** (Generative Error Correction).

### 3.1·级联式集成

Cascaded integration is the most simple and straightforward approach for integrating speech with LLMs.
This involves the backbone LLM being supported by ASR and Text-to-Speech (TTS) interfaces for handling speech inputs and outputs, as well as the LLM invoking external models to solve speech-related tasks.
This approach has been employed in models such as AudioGPT[^huang-2024-audio-gpt] and HuggingGPT[^shen-2023-hugginggpt] to expand the applications of LLMs.
The primary advantage of cascaded integration is its ease of implementation, allowing various tasks to be integrated into a single model with minimal effort.
However, this method suffers from accuracy problems due to error propagation and efficiency problems due to the latency of processing through multiple steps.

### 3.2·LLM Rescoring

Before research on LLMs explosively expanded, ASR models had already begun using rescoring mechanisms with external language models to improve transcription accuracy[^mcdermott-2019-lm-fusion], [^shin-2019-sentence-scoring], [^salazar-2020-mlm-scoring], [^xu-2022-rescorebert].
Recently, there have been efforts towards utilizing LLMs for rescoring, and it has been proven to be effective[^chen-2023-longform], [^udagawa-2022-rescoring].

As shown in Figure \ref{fig:rescore} (a), this process involves generating a list of $n$-best hypotheses $\mathcal{H} = \{Y_1,Y_2,...,Y_n\}$  from the speech input $X$ through the initial ASR decoding, which is then re-evaluated using an LLM to find the hypothesis with higher linguistic coherence to improve accuracy.
The rescoring process can be expressed as
\begin{align*}
Y^* &= \text{argmax}_{Y_i \in \mathcal{H}} \ [(1\!-\!\lambda) \log p_{\text{AM}}(Y_i|X) \\
&\quad + \lambda \log p_{\text{LLM}}(Y_i)]
\end{align*}
where $p_{\text{AM}}$ and $p_{\text{LLM}}$ is the probability according to the ASR model (Acoustic Modle, AM) and the LLM, and $\lambda$ is a weight balancing the contributions of two models.

### 3.3·LLM Generative Error Correction

Based on the concept of rescoring, LLM GER represents a new approach that fully utilizes the generative capability of LLMs.
As shown in Figure \ref{fig:rescore} (b), the method utilizes an instructional prompt together with the n-best hypotheses list to guide the LLM in generating transcription predictions.
[^chen-2023-hyporadise] described this task performed by the LLM as hypotheses-to-transcription (H2T), which they explore under both fine-tuning and few-shot settings.
Further studies have explored this direction with PEFT[^hu-2024-noise-robust] or in-context learning[^yang-2023-tap], [^ma-2023-error-correction].

Instead of merely selecting the best hypothesis from a pre-existing set,
this method allows the LLM to generate a new transcription based on the hypotheses.
By doing so, LLMs can potentially produce a result with a quality better than all initial hypotheses.
Such an approach has proven to be well-suited for other tasks such as speech-to-text translation (S2TT), which require more generative capabilities of the model[^hu-etal-2024-gentranslate].
[^lin-etal-2024-neko] adopted the mixture-of-experts architecture [^fedus-etal-2022], [^jiang-etal-2024-mixtral] to let different experts handle different types of generative errors produced by task-specific models including ASR and ST models.

## 4·基于潜在表示的集成方法

In this integration approach, a **speech encoder** is used to process the speech input and generate latent representations that are directly fed into the LLM, bypassing the embedding layer.
The speech encoder is usually a Transformer-based model, which can be pre-trained on large-scale speech data or trained from scratch for the speech-LLM integration.

A critical issue of this approach is the sequence length gap between the speech and text modalities.
Speech features, often sampled at rates of 50 to 100 frames per second, result in longer sequences compared to text tokens.
Consequently, some form of **modality adaptation** mechanism is necessary to bridge these modalities, ensuring that the latent representations align with the LLM’s embedding space.

### 4.1·语音编码器

For the speech encoder, various pre-trained models can be utilized to provide representations learned from large-scale speech data.
This includes S3Ms (e.g., HuBERT[^hsu-etal-2021-hubert]) as well as the encoder of pre-trained ASR models (e.g., Whisper[^radford-etal-2023-whisper]).
To connect the pre-trained speech encoder and the LLM, an \emph{adapter} (also referred to as a \emph{bridge network} or a \emph{module connector}) is usually adopted.

An alternative approach is to train a speech encoder from scratch, specifically for the LLM integration.
The structure of the speech encoder is usually a multi-layer Transformer[^vaswani-etal-2017] encoder or Conformer[^gulati-2020-conformer].

### 4.2·模态适配器

The modality adaptation mechanism is adopted for the speech encoder to map original frame-wise representations to token-wise representations.
This allows the LLM to process the speech sequence in a manner similar to how it processes the text sequence, without having to deal with overly long sequences caused by long-form speech inputs.

The adaptation is usually accomplished by the adapter between pre-trained models, which is introduced in \ref{sec:speech-encoder}, or any part of the self-designed trained-from-scratch speech encoder.
To this end, various adaptation methods have been proposed.
Apart from some rarely-used strategies such as random downsampling[^wang-2023-slm], most of the methods can be categorized into three groups, as shown in Figure \ref{fig:adapter}: convolutional downsampling, CTC compression, and Q-former.
According to comparisons conducted by [^hono-2023-integration] and [^yu-2024-connecting], the Q-former generally outperforms convolutional downsampling, which in turn outperforms CTC compression.
Next, we will detail these strategies in the following sections.

#### 卷积下采样

A basic strategy where the speech representation sequence is downsampled from its original length using convolutional layers[^hono-2023-integration].
When using the same number for kernel size and stride, it becomes equivalent to simply stacking multiple representation vectors together, which may only benefit parameter efficiency[^fathullah-2024-prompting].
For better alignment with the LLM's embeddings,
some variants add a fully connected layer or a multi-head Transformer network after the convolutional layers[^yu-2024-connecting].

#### CTC Compression

CTC compression involves two steps: (i) Training the speech encoder, or a part of it, on the ASR task based on Connectionist Temporal Classification (CTC)[^graves-2013-ctc]; (ii) Compressing the representation sequence based on the results of CTC predictions.
Several specific strategies for the step (ii) include:

Blank-removal: Discarding all the frames predicted as blanks[^hono-2023-integration], [^wu-2023-decoder-only].

Frame-averaging: Averaging the latent representations of consecutive frames whose CTC predictions are the same label[^hono-2023-integration], [^wu-2023-decoder-only].

Blank-probability: Discarding any frames with a CTC probability for blank exceeding a threshold[^ling-2024-fully-formatted].

#### Q-Former

Many studies[^yu-2024-connecting], [^tang-etal-2024-salmonn], [^pan-2023-cosmic] adopt Q-Former for the modality adaptation.
Q-Former is a Transformer-based module converting variable-length input sequences into fixed-length output query representations.
It is initially proposed for vision -- text modality alignment[^li-2023-blip].

### 4.3·训练策略

The optimal approach is to train the whole model consisting of the speech encoder and the LLM.
However, fully fine-tuning the LLM is computationally expensive.
Consequently, PEFT methods such as LoRA[^xu-2024-lora], are often adopted to reduce resource consumption.

In scenarios where a pre-trained speech encoder is utilized, the primary focus is training the adapter connecting two pre-trained models.
Whether to fine-tune the speech encoder or the LLM with PEFT seems optional as the decision differs across studies.
On this topic, several studies[^hono-2023-integration], [^pham-2024-comprehensive] have conducted systematic investigations by comparing the effects of fine-tuning versus freezing different modules.

As for training the speech encoder from scratch, it is commonly done jointly with fine-tuning LLM with PEFT.
[^wu-2023-decoder-only] proposed employing a two-stage training process where PEFT is not initiated until the speech encoder has undergone some initial training, ensuring a more stable training progression.

## 5·基于音频 Token 的集成方法

As explained in Section \ref{sec:background-speech-representations}, there are two types of audio tokens: semantic tokens and acoustic tokens.
Studies use either or both, treating them as independent tokens like text or converting them back to latent representations.
When semantic tokens are used for the output, a separate model to convert them into filterbanks and to waveform signals similar to TTS models is often used.

### 5.1·无 LLM 的语音语言模型

#### 语义 Token

[^lakhotia-etal-2021-gslm] introduced the concept of generative spoken language modeling, which uses semantic tokens as both the input and output of a language model.
[^polyak-etal-2021] and [^kharitonov-etal-2022-pgslm] extended the idea by incorporating prosodic information.
These models only used audio, and although they could output temporarily coherent contents, semantic understanding ability, and audio quality remained as challenges.

#### 声学 Token

The use of acoustic tokens for language modeling was popularized in the task of TTS [^wang-etal-2023-valle], [^chen-etal-2024-valle2], where a few seconds of sample speech is used to generate coherent speech as those samples given text.
[^wang-etal-2023-viola] extended the idea to cover multi-tasking of ASR, machine translation (MT), and TTS.

#### 语义和声学 Token 的结合

AudioLM [^borsos-etal-2023-audiolm] was introduced to incorporate both semantic and acoustic tokens.
They adopted a two-stage approach, where the model first predicts semantic tokens and then subsequently predicts acoustic tokens.
[^zhang-etal-2024-speechtokenizer] employed distillation from semantic representation to the first quantizer to combine semantic and acoustic tokens.
Their generation approach is similar to [^wang-etal-2023-valle], where an autoregressive model produces the first-layer token and a non-autoregressive model produces the rest.

### 5.2·LLM 集成到语音语言模型

The use of LLMs as an underlying model of speech language models has become more popular following the success of LLMs.
[^hassid-etal-2023-twist] showed that training speech language models using underlying pretrained text language models could improve performance.
VoxtLM [^maiti-etal-2024-voxtlm] uses OPT [^zhang-etal-2022-opt] as an underlying LLM and conducts multi-task fintuning it on ASR, TTS, and language modeling on text and semantic tokens.
AudioPaLM [^rubenstein-etal-2023-audiopalm] adopts a similar approach to [^borsos-etal-2023-audiolm] but also enables text input/output, leveraging PaLM [^chowdhery-etal-2023-palm] as the underlying LLM.
TWIST and SpeechGPT [^zhang-etal-2023-speechgpt] use Llama [^touvron-etal-2023-llama] as the underlying model and use semantic tokens as both input and output.
Spirit-LM [^nguyen-etal-2025-spiritlm] uses Llama 2 [^touvron-etal-2023-llama2] as the underlying model and uses a mixture of semantic and text tokens for both input and output.

[^defossez-etal-2024-moshi] introduced a new architecture for speech language modeling.
Similarly to previous studies, they trained a text LLM and a codec model similar to [^zhang-etal-2024-speechtokenizer].
Their architecture enables duplex ability of listening and generating speech by using a hierarchical autoregressive model consisting of two Transformer modules, namely Temporal Transformer and Depth Transformer [^lee-etal-2022-rqtransformer].
This architecture is also shown to be effective for speech-to-speech translation (S2ST)[^labiausse-etal-2025-hibiki].

## 6·对比分析

### 6.1·优点与缺点

It is important to understand the advantages and disadvantages of the three integration approaches when employing them for different applications.
We summarize their pros and cons as follows:
- **Degree of integration**: Latent-representation-based $>$ Audio-token-based $>$ Text-based.
- **Interpretability**: Text-based $>$ Audio-token-based $>$ Latent-representation-based.
- **Speech generation ability**: Text-based and audio-token-based approach can generate speech, whereas latent-representation-based approaches typically cannot.

Based on these characteristics, different approaches excel in different scenarios:
- Latent-representation-based and audio-token-based approaches are better than text-based approach, in scenarios where sufficient resources (data, computational power, and time) are available or when real-time processing is required.
Deeper integration reduces error propagation and latency but demands more extensive resources.
- Text-based approach is better than latent-representation-based and audio-token-based approaches, in scenarios where resources are limited or when greater interpretability is required.
- Latent-representation-based approach is better than audio-token-based, in scenarios where speech is only considered as input.
Latent-representation-based methods provide the deepest integration when generating textual or other downstream outputs.
- Audio-token-based is better than latent-representation-based, in scenarios where speech is also considered as output.
Generating speech from latent representations remains challenging, making audio-token-based approaches more suitable in these cases.

### 6.2·定量对比

We provide a comprehensive performance comparison in Table~\ref{tab:benchmarks}, comparing studies with different integration approaches across multiple tasks: ASR, S2TT, S2ST, TTS.
The table highlights key metrics such as word error rate (WER), BLEU score, and others, alongside each model’s integration method and backbone components.

While Table~\ref{tab:benchmarks} summarizes many state-of-the-art results, direct comparisons can be challenging due to variations in backbone LLMs, acoustic front-ends, and training protocols.
For instance, although BLEU scores on CoVoST~2[^wang-etal-2021-covost2] indicate that deeper integration methods (e.g., latent-representation-based) often yield improved performance, model size and training resources also play a significant role.
Future work should focus on more uniform experimental conditions to isolate the impact of each approach.

Nonetheless, the metrics in Table~\ref{tab:benchmarks} serve as a starting point for evaluating how well each method handles different tasks.
The results suggest that deeper integration (e.g., latent-representation-based) can be beneficial when sufficient computational resources and training data are available, whereas text-based methods often offer greater interpretability and simpler pipelines.

## 7·挑战

### 7.1·Text-based Integration

Text-based integration preserves the original LLM input modality, which is text.
Because the model is already optimized for textual inputs, this approach typically requires the least adaptation from the LLM.
However, transforming speech into text before feeding it into the LLM inevitably introduces a layer of abstraction and potential information loss such as prosody and emotion, which limits the downstream performance.

Therefore, the main challenge is how to convey the rich information of source speech through text.
In current practice, the intermediate text is often generated based on the highest probability[^chen-2023-hyporadise], [^yang-2023-tap], [^radhakrishnan2023whispering].
Although this strategy maximizes intermediate accuracy, it may not be optimal for the LLM and its final outputs.
One possible direction is to inject controlled randomness during decoding to ensure additional diversity.
However, degrading accuracy could also have a negative effect, creating a trade-off.
How to balance diversity and accuracy remains an open research question that requires future investigation.

### 7.2·Latent-representation-based Integration

In contrast to text-based integration, latent-representation-based integration employs an intermediate representation that is closer to the source speech but more distant from the LLM’s natural input space.
Consequently, the primary challenge lies in aligning these acoustic representations with the textual embedding space of the LLM.
Section~\ref{sec:modality-adaptation} introduces multiple modality adaptation mechanisms to address this and enable LLMs to handle speech more effectively.

While the primary focus of most research introduced in this paper has been on adapting LLMs to better handle speech data, there is comparatively less attention on how these adaptations affect performance on text-based tasks.
The alignment of text and speech data, along with their latent representations, remains underexplored, with only a few studies focusing on this issue[^wang2023blsp], [^lu-2024-desta].

A promising solution for improving speech-text alignment is the generation of high-quality synthetic speech data to supplement real datasets, thus closing the gap between the speech modality and text-based LLMs.
More thorough investigations into how to align speech and text representations in complex languages (beyond English) could also strengthen the multimodal capabilities of LLMs.

### 7.3·Audio-token-based Integration

As stated in Sections \ref{sec:background-speech-representations} and \ref{sec:audio-token-based-integration},
S3Ms (and their derived semantic tokens) mainly capture phonetic information[^choi-etal-2024], while acoustic tokens offer higher fidelity in generating speech signals[^borsos-etal-2023-audiolm].

Integrating both representations, as presented in Section \ref{sec:semantic-and-acoustic}, is one approach that requires further investigation.
While this approach achieves strong performance across various downstream tasks (Table~\ref{tab:benchmarks}), there are still limited studies on this approach, where the models are English-centric models that rely on millions of hours of speech data.
Minimizing computational requirements is essential to extend this approach to languages lacking the vast resources available for English.

Some studies suggest that latent-representation-based integration can outperform audio-token-based integration[^wang-etal-2024-comparativestudydiscretespeech].
However, it is possible that this is due to the suboptimal representation of audio tokens[^gat-etal-2023-discreteslm], requiring further improvement of tokenization methods and comparison with other integration approaches.

### 7.4·Fair Comparison Across Integration Approaches

Beyond these integration-approach-specific challenges, there is a notable gap in comparing the different integration approaches under a unified setting.
Most existing works focus on one approach, making it difficult to assess their relative merits consistently.
A fair comparison among these integration methods could clarify how different factors affect performance.
Developing standardized benchmarks, protocols, and reporting practices for speech-LLM research would help future work isolate the core differences between these approaches.

## 8·结论

In this survey, we systematically categorize the integration of LLMs with speech modality, outlining three primary approaches: text-based, latent-representation-based, and audio-token-based integration.
Each method demonstrates unique strengths in performing different speech-related tasks.
However, numerous challenges remain in this evolving field, presenting significant opportunities for future research.

## 局限性

In our survey of the integration of speech with LLMs, we have covered the significant developments within this field.
Despite our comprehensive approach, we acknowledge that the rapid pace of development in speech processing and LLM means that some recent advances or discussions might have escaped our attention and are not fully addressed within this paper.

[^brown-etal-2020]:
[^touvron-etal-2023-llama]:
[^geminiteam-2024-gemini]:
[^bai-2023-qwen]:
[^deepseekai-2024-deepseek]:
[^peng-etal-2024-speechllmsurvey]:
[^cui-etal-2025-speechlmsurvey]:
[^latif-2023-large-audio]:
[^ghosh-2024-vision-language-model]:
[^zhang-2024-mmllm]:
[^chai-etal-2024-tokenization]:
[^tao-etal-2024-scalingvocab]:
[^gat-etal-2023-discreteslm]:
[^borsos-etal-2023-audiolm]:
[^zhao-etal-2023-survey-llm]:
[^brown-etal-1992-ngram]:
[^bahl-etal-1983]:
[^bengio-etal-2000]:
[^sutskever-etal-2014-seq2seq]:
[^bahdanau-etal-2015]:
[^vaswani-etal-2017]:
[^liu-etal-2018]:
[^radford-etal-2018]:
[^radford-etal-2019]:
[^wei-etal-2022]:
[^ouyang-etal-2022]:
[^minaee-etal-2024]:
[^baevski-etal-2020-wav2vec2]:
[^hsu-etal-2021-hubert]:
[^chen-etal-2022-wavlm]:
[^choi-etal-2024]:
[^pasad-etal-2024]:
[^lakhotia-etal-2021-gslm]:
[^defossez-etal-2024-moshi]:
[^bai-etal-2024-dmel]:
[^iso11172-3]:
[^rfc6716]:
[^zeghidour-etal-2021-soundstream]:
[^defossez-etal-2023-encodec]:
[^kumar-etal-2023-dac]:
[^wang-etal-2023-viola]:
[^zhang-etal-2024-speechtokenizer]:
[^huang-2024-audio-gpt]:
[^shen-2023-hugginggpt]:
[^mcdermott-2019-lm-fusion]:
[^shin-2019-sentence-scoring]:
[^salazar-2020-mlm-scoring]:
[^xu-2022-rescorebert]:
[^chen-2023-longform]:
[^udagawa-2022-rescoring]:
[^chen-2023-hyporadise]:
[^hu-2024-noise-robust]:
[^yang-2023-tap]:
[^ma-2023-error-correction]:
[^hu-etal-2024-gentranslate]:
[^lin-etal-2024-neko]:
[^fedus-etal-2022]:
[^jiang-etal-2024-mixtral]:
[^radford-etal-2023-whisper]:
[^gulati-2020-conformer]:
[^wang-2023-slm]:
[^hono-2023-integration]:
[^yu-2024-connecting]:
[^fathullah-2024-prompting]:
[^graves-2013-ctc]:
[^wu-2023-decoder-only]:
[^ling-2024-fully-formatted]:
[^tang-etal-2024-salmonn]:
[^pan-2023-cosmic]:
[^li-2023-blip]:
[^xu-2024-lora]:
[^pham-2024-comprehensive]:
[^polyak-etal-2021]:
[^kharitonov-etal-2022-pgslm]:
[^wang-etal-2023-valle]:
[^chen-etal-2024-valle2]:
[^hassid-etal-2023-twist]:
[^maiti-etal-2024-voxtlm]:
[^zhang-etal-2022-opt]:
[^rubenstein-etal-2023-audiopalm]:
[^chowdhery-etal-2023-palm]:
[^zhang-etal-2023-speechgpt]:
[^nguyen-etal-2025-spiritlm]:
[^touvron-etal-2023-llama2]:
[^lee-etal-2022-rqtransformer]:
[^labiausse-etal-2025-hibiki]:
[^wang-etal-2021-covost2]:
[^radhakrishnan2023whispering]:
[^wang2023blsp]:
[^lu-2024-desta]:
[^wang-etal-2024-comparativestudydiscretespeech]:
