# 当大语言模型遇上语音: 集成方法综述

<details>
<summary>基本信息</summary>

- 标题: "When Large Language Models Meet Speech: A Survey on Integration Approaches"
- 作者:
  - 01 Zhengdong Yang,
  - 02 Shuichiro Shimizu,
  - 03 Yahan Yu,
  - 04 Chenhui Chu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.19548)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv](PDF/S20250226__When_LLMs_Meet_Speech_A_Survey_on_Integration_Approaches[2502.19548v1].pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Recent advancements in large language models (LLMs) have spurred interest in expanding their application beyond text-based tasks. A large number of studies have explored integrating other modalities with LLMs, notably speech modality, which is naturally related to text.
This paper surveys the integration of speech with LLMs, categorizing the methodologies into three primary approaches: text-based, latent-representation-based, and audio-token-based integration.
We also demonstrate how these methods are applied across various speech-related applications and highlight the challenges in this field to offer inspiration for future research.

## 1·引言

In recent years, the field of natural language processing (NLP) has been greatly reshaped by the development of large language models (LLMs)~\cite{brown-etal-2020,touvron-etal-2023-llama,geminiteam-2024-gemini,bai-2023-qwen,deepseekai-2024-deepseek}.
These models have not only shown excellent ability in understanding and generating text but have also sparked interest in their potential applicability across other modalities, including speech.
The integration of speech and LLMs offers a wide range of potential applications, including speech translation, conversational chatbots, and enhanced human-computer interaction in robotics.

Survey papers have reviewed speech language models \citep{peng-etal-2024-speechllmsurvey,cui-etal-2025-speechlmsurvey}, as well as audio language models~\cite{latif-2023-large-audio} and multimodal language models~\cite{ghosh-2024-vision-language-model,zhang-2024-mmllm}.
However, there still lacks a survey specifically on the integration approaches of speech and LLMs, posing a challenge for researchers seeking to address this complex problem.

Distinct from other survey papers regarding speech and LLMs, this paper provides insight into the problem by specifically surveying the integration approaches of speech and LLMs.
Studies on LLM tokenization~\citep{chai-etal-2024-tokenization,tao-etal-2024-scalingvocab} suggests that tokenization methods can affect the performance of LLMs.
On the other hand, studies on tokenization methods for speech language modeling~\citep{gat-etal-2023-discreteslm,borsos-etal-2023-audiolm} also show that speech tokenization methods can affect the performance of speech language models.
In contrast to text processing, speech--LLM integration approaches are not limited to discrete tokenization, as presented in this paper.
Therefore, studying the integration between speech and LLMs can be a key to innovations.

In this paper, we systematically categorize a substantial body of research on speech--LLM integration,\footnote{One challenge that affects the scope of studies is the lack of standard definition for LLMs. In this paper, we adopt the loose definition by \citet{zhao-etal-2023-survey-llm}, focusing on models with over 10 billion parameters, while also including notable studies with smaller models.}
and provide a clear taxonomy of the integration approaches.
We broadly categorize the integration into the following three types:
(a) \textbf{Text-based integration}: LLMs process textual data, integrated with speech-to-text and/or text-to-speech models;
(b) \textbf{Latent-representation-based integration}: Latent vector representations that encode speech data are utilized, mainly as inputs to LLMs;
(c) \textbf{Audio-token-based integration}: Speech tokens, such as semantic tokens and/or acoustic tokens, are used as the inputs/outputs for LLMs.
The overview of these approaches is illustrated in Figure \ref{fig:overview}, and the detailed taxonomy with representative studies is presented in Figure \ref{fig:taxonomy}.

## 2·背景

### 2.1·语言建模

Language modeling dates back to statistical models like $N$-gram models~\citep{brown-etal-1992-ngram}, which were central to early NLP and automatic speech recognition (ASR) systems~\citep{bahl-etal-1983}.
Neural-network-based language modeling was introduced by~\citet{bengio-etal-2000}, formulating the probability
\[
 P(w_{1:T}) = \prod_{t=1}^T P(w_t | w_{1:t-1}),
\]
where $w_t$ is the $t$-th token (text subwords or speech tokens) and $w_{i:j} = (w_i, w_{i + 1}, \cdots, w_j)$ is the subsequence from $i$-th token to $j$-th token.

Advances in hardware enabled neural models to scale, leading to innovations such as sequence-to-sequence learning~\citep{sutskever-etal-2014-seq2seq} and the attention mechanism~\citep{bahdanau-etal-2015}.
The Transformer architecture~\citep{vaswani-etal-2017} revolutionalizes language modeling using self-attention by efficiently modeling long-range dependencies utilizing parallelized computation.
Decoder-only Transformer~\citep{liu-etal-2018} gains prominence through the multi-task learning paradigm with generative pre-training and discrinative fine-tuning~\citep{radford-etal-2018}.
Scaling these models~\citep{radford-etal-2019,brown-etal-2020} shows generalization to multiple tasks without explicit supervision and achieves comparative performance against task-specific models, which leads to the advent of LLM era.
Techniques such as instruction tuning~\citep{wei-etal-2022} and alignment to human preference via reinforcement learning~\citep{ouyang-etal-2022} further advance LLM capabilities.
For a comprehensive overview of LLMs, we refer the readers to LLM survey papers~\citep{zhao-etal-2023-survey-llm,minaee-etal-2024}.

### 2.2·语音表示


\revised{
Speech is captured as waveform signals, sampled at a specific sampling rate and quantization value, and represented as a sequence of amplitude values.
For deep learning applications, they are often converted to speech representations with shorter sequence lengths, such as log Mel filterbanks, which we refer to as \textbf{filterbanks}.
Following the success of the unsupervised pre-training approach in other fields, self-supervised speech models (S3Ms)~\citep{baevski-etal-2020-wav2vec2,hsu-etal-2021-hubert,chen-etal-2022-wavlm} were introduced.
These models predict masked frames to learn representations that capture phonetic information \citep{choi-etal-2024}, along with a wide range of speech characteristics, such as speaker identity, paralinguistic information, and word-level information \citep{pasad-etal-2024}.}
We refer to these contextualized frame-level features as \textbf{latent representations}.

To integrate speech (which is naturally continuous) with LLMs (which are designed to handle discrete tokens), \textbf{audio tokens} that are more discrete than previous representations have been studied recently.
There are two mainstream approaches to modeling speech as audio tokens: \textbf{semantic tokens} and \textbf{acoustic tokens}.
> Different studies use different terms for these tokens (e.g., \citet{lakhotia-etal-2021-gslm} used the term ``units'' for semantic tokens).
> We use the terms semantic tokens and acoustic tokens as in \citet{borsos-etal-2023-audiolm} and the term audio tokens to represent either or both of them similar to \citet{defossez-etal-2024-moshi}.
> We note that there are other efforts than semantic or acoustic tokens to discretize speech signals \citep{bai-etal-2024-dmel}.

Semantic tokens can be obtained from S3Ms by discretizing latent representations using $k$-means.
Acoustic tokens have been studied in the line of audio codecs (e.g., MP3 \citep{iso11172-3}, Opus \cite{rfc6716}, etc.), which aim to compress audio signals.
Neural audio codecs \citep{zeghidour-etal-2021-soundstream,defossez-etal-2023-encodec,kumar-etal-2023-dac} have recently gained traction as powerful tools to improve coding efficiency as well as perceptual quality.
Because acoustic tokens exhibit better quality in producing output speech signals \cite{borsos-etal-2023-audiolm} but fail to capture semantic information like semantic tokens do, recent studies combine both to obtain better representations for speech-language modeling \citep{wang-etal-2023-viola,zhang-etal-2024-speechtokenizer,defossez-etal-2024-moshi}.
