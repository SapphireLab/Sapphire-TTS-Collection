# 当大语言模型遇上语音: 集成方法综述

<details>
<summary>基本信息</summary>

- 标题: "When Large Language Models Meet Speech: A Survey on Integration Approaches"
- 作者:
  - 01 Zhengdong Yang,
  - 02 Shuichiro Shimizu,
  - 03 Yahan Yu,
  - 04 Chenhui Chu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.19548)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv](PDF/S20250226__When_LLMs_Meet_Speech_A_Survey_on_Integration_Approaches[2502.19548v1].pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Recent advancements in large language models (LLMs) have spurred interest in expanding their application beyond text-based tasks. A large number of studies have explored integrating other modalities with LLMs, notably speech modality, which is naturally related to text.
This paper surveys the integration of speech with LLMs, categorizing the methodologies into three primary approaches: text-based, latent-representation-based, and audio-token-based integration.
We also demonstrate how these methods are applied across various speech-related applications and highlight the challenges in this field to offer inspiration for future research.

## 1·引言

In recent years, the field of natural language processing (NLP) has been greatly reshaped by the development of large language models (LLMs)~\cite{brown-etal-2020,touvron-etal-2023-llama,geminiteam-2024-gemini,bai-2023-qwen,deepseekai-2024-deepseek}.
These models have not only shown excellent ability in understanding and generating text but have also sparked interest in their potential applicability across other modalities, including speech.
The integration of speech and LLMs offers a wide range of potential applications, including speech translation, conversational chatbots, and enhanced human-computer interaction in robotics.

Survey papers have reviewed speech language models \citep{peng-etal-2024-speechllmsurvey,cui-etal-2025-speechlmsurvey}, as well as audio language models~\cite{latif-2023-large-audio} and multimodal language models~\cite{ghosh-2024-vision-language-model,zhang-2024-mmllm}.
However, there still lacks a survey specifically on the integration approaches of speech and LLMs, posing a challenge for researchers seeking to address this complex problem.

Distinct from other survey papers regarding speech and LLMs, this paper provides insight into the problem by specifically surveying the integration approaches of speech and LLMs.
Studies on LLM tokenization~\citep{chai-etal-2024-tokenization,tao-etal-2024-scalingvocab} suggests that tokenization methods can affect the performance of LLMs.
On the other hand, studies on tokenization methods for speech language modeling~\citep{gat-etal-2023-discreteslm,borsos-etal-2023-audiolm} also show that speech tokenization methods can affect the performance of speech language models.
In contrast to text processing, speech--LLM integration approaches are not limited to discrete tokenization, as presented in this paper.
Therefore, studying the integration between speech and LLMs can be a key to innovations.

In this paper, we systematically categorize a substantial body of research on speech--LLM integration,\footnote{One challenge that affects the scope of studies is the lack of standard definition for LLMs. In this paper, we adopt the loose definition by \citet{zhao-etal-2023-survey-llm}, focusing on models with over 10 billion parameters, while also including notable studies with smaller models.}
and provide a clear taxonomy of the integration approaches.
We broadly categorize the integration into the following three types:
(a) \textbf{Text-based integration}: LLMs process textual data, integrated with speech-to-text and/or text-to-speech models;
(b) \textbf{Latent-representation-based integration}: Latent vector representations that encode speech data are utilized, mainly as inputs to LLMs;
(c) \textbf{Audio-token-based integration}: Speech tokens, such as semantic tokens and/or acoustic tokens, are used as the inputs/outputs for LLMs.
The overview of these approaches is illustrated in Figure \ref{fig:overview}, and the detailed taxonomy with representative studies is presented in Figure \ref{fig:taxonomy}.
