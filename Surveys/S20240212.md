# Deep Learning-Based Expressive Speech Synthesis: A Systematic Review of Approaches, Challenges, and Resources

- 标题: Deep Learning-Based Expressive Speech Synthesis: A Systematic Review of Approaches, Challenges, and Resources
- 作者:
  - Huda Barakat
  - Oytun Turk
  - Cenk Demiroglu
- 发表:
  - EURASIP Journal on Audio, Speech, and Music Processing (2024)
  - DOI: https://doi.org/10.1186/s13636-024-00329-7


## Abstract·摘要

Speech synthesis has made significant strides thanks to the transition from machine learning to deep learning models.
Contemporary **Text-To-Speech (TTS)** models possess the capability to generate speech of exceptionally high quality, closely mimicking human speech.
Nevertheless, given the wide array of applications now employing TTS models, mere high-quality speech generation is no longer sufficient.
Present-day TTS models must also excel at producing expressive speech that can convey various speaking styles and emotions, akin to human speech.
Consequently,researchers have concentrated their efforts on developing more efficient models for expressive speech synthesis in recent years.
This paper presents a systematic review of the literature on expressive speech synthesis models published within the last 5 years, with a particular emphasis on approaches based on deep learning.
We offer a comprehensive classification scheme for these models and provide concise descriptions of models falling into each category.
Additionally, we summarize the principal challenges encountered in this research domain and outline the strategies employed to tackle these challenges as documented in the literature.
In the Section 8, we pinpoint some research gaps in this field that necessitate further exploration.
Our objective with this work is to give an all-encompassing overview of this hot research area to offer guidance to interested researchers and future endeavors in this field.

语音合成技术因从机器学习到深度学习模型的转变而取得了显著进步.
当代的文本转语音模型能够生成质量极高的, 几乎媲美人类的语音.
但是鉴于现在有众多应用正在采用文本转语音模型, 仅仅生成高质量的语音已经不再足够.
现今的文本转语音模型还必须擅长生成富有表现力的语音, 能够传达类似于人类语音的多种说话风格和情感.
因此, 近年来研究人员集中精力于开发更高效的富有表现力的语音合成模型.

本文对过去五年内发表的关于富有表现力的语音合成模型的文献进行了系统性的回顾, 特别关注基于深度学习的方法.
我们为这些模型提供了一个全面的分类方案, 并对每个类别中的模型进行了简要的描述.
此外, 我们总结了在这一研究领域遇到的主要挑战, 并概述了文献中记录的应对这些挑战的策略.

在第 8 节中, 我们指出了该领域中需要进一步探索的一些研究空白.

我们进行这项工作的目的是为这一热门研究领域提供一个全面的概述, 为对这一领域感兴趣的研究人员和未来研究提供指导.

文章内容:
- [ ] [Sec.01](Sec.01.md)
- [ ] [Sec.02](Sec.02.md)
- [ ] [Sec.03](Sec.03.md)
- [ ] [Sec.04](Sec.04.md)
- [ ] [Sec.05](Sec.05.md)
- [x] [Sec.06](Sec.06.md)
- [x] [Sec.07](Sec.07.md)
- [ ] [Sec.08](Sec.08.md)

## Section 01: Introduction

Since the late 1950s, computer-based text-to-speech systems (TTS) have undergone significant advancements, culminating in the production of models that generate speech almost indistinguishable from that of a human. This progress has followed a path consisting of several stages, beginning with conventional methods named as concatenative synthesis and progressing to more advanced approaches known as **Statistical Parametric Speech Synthesis (SPSS)**. Advanced approaches are mainly based on machine learning algorithms like <algo>hidden Markov models (HMMs)</algo> and <algo>gaussian mixture models (GMMs)</algo>. Despite this progress, speech generated by these methods was still noticeably artificial. However, the emergence of deep learning (DL) as a new branch under machine learning (ML) in 2006 has led to significant improvements. Speech synthesis researchers,like many in other research fields, started incorporating deep neural networks (DNN) in their models. Initially,DNNs replaced HMMs and GMMs in SPSS models while the main structure still follows the primary framework of SPSS models as shown in Fig. 1. As discussed in **"Statistical Parametric Speech Synthesis Using Deep Neural Networks"**, the deep learning-based models have overcome many limitations and problems associated with machine learning-based models.

Researchers continue to aim for improved speech quality and more human-like speech despite past advancements. Additionally, they seek to simplify the framework of the text-to-speech models due to the intricate nature of the SPSS structure, which limits progress in this field to those with extensive linguistic knowledge and expertise. Deep learning advancements have brought about the simple encoder-decoder structure for TTS models as <algo>sequence-to-sequence (Seq2Seq)</algo> approaches. The pro-posed approaches have simplified the structure of conventional TTS with multiple components into training a single network that converts a set of input text characters/phonemes into a set of acoustic features (mel-spectrograms). A main concern in these advanced TTS models is the mapping process between the input and output sequences, which is a one-to-many problem, as the single input text can have multiple speech variations as output. In fact, there are two groups of recent TTS models, as shown in Fig.02. The first group generates mel-spectrograms in a sequential (autoregressive) manner using soft and automatic attention alignments between input and output sequences, such as the Tacotron model ([Tacotron (2017)](../../Models/Acoustic/_PDF/2017.03.29_Tacotron.md) [Tacotron2 (2017)](../../Models/Acoustic/2017.12.16_Tacotron2.mdhe second group utilizes hard alignments between the phonemes/characters and mel-spectro-grams, and thus its speech generation process is parallel(non-autoregressive), as in the FastSpeech model ([FastSpeech (2019)](../../Models/Acoustic/2019.05.22_FastSpeech.mdstSpeech2](../../Models/TTS2_Acoustic/2020.06.08_FastSpeech2.md)).This improvement in the structure of the TTS model has encouraged rapid development in the field within the last few years, during which the proposed models produced speech that is nearly indistinguishable from human speech.

Human speech is highly expressive and reflects various factors, such as the speaker’s identity, emotion, and speaking style. In addition, there are many applications in which speech synthesis can be utilized, especially expressive speech synthesis. For instance, audiobooks and podcast applications that create audio versions of eBooks and podcasts, translation applications which provide real-time translation of foreign language text, dubbing applications that generate an alternative audio track for a video with different content, speaker, or language, and content creation applications which help produce audio versions of textual content, such as blogs and news articles. E-learning applications that allow for adding voice-over audio to e-learning courses, and conversational AI applications enable machines to communicate with users in a human-like manner, such as AI chatbots and virtual assistants.

As spoken language is a crucial component in such applications, users must feel as if they are communicating with a real human rather than a machine. Therefore, the speech generated by these applications should convey appropriate emotion, intonation, stress, and speaking style to match the ongoing conversation or the content type and context of the text being read.

As a result, there has been a recent attention towards building efficient expressive speech synthesis models as another step forward in achieving human-like speech. Therefore, many studies have been devoted to expressive speech synthesis (ETTS) as a hot research area, particularly over the last 5 years. In this work, we present the findings of our systematic literature review on ETTS field from the past 5 years. Firstly, we suggest a classification schema of deep learning-based ETTS models that are proposed during this period, based on structures,and learning methods followed in each study. A summary is then provided for each category in the classification schema and main papers related to this category. After that, we outline the main challenges in the ETTS area and solutions that have been proposed to solve them from literature. Finally, we conclude with a discussion of the implications of our work and a highlight of some gaps that require further research in this area.

During our work on this review of expressive speech synthesis literature, we came across several review papers that focus on different stages of development in the speech synthesis field. The majority of these reviews concentrate on DL-based TTS approaches, while only a few papers cover recent TTS approaches in addition to early conventional ones. However, to the best of our knowledge, there are no review papers that cover the fast growth in the (expressive) speech synthesis area, especially in the last few years. Therefore, our main goal in this review is to provide an overview of research trends, techniques, and challenges in this area during this period. We hope that our work will offer researchers a comprehensive understanding of how and what has been accomplished in this field and the gaps that need to be filled as guidance for their future efforts.

While we were writing this paper, we came across an interesting recent review paper **"An Overview of Affective Speech Synthesis and Conversion in the Deep Learning Era"** that is similar to our work. However, the review covers emotional speech synthesis (ESS) as a sub-field of voice transformation while our work is more comprehensive as a systematic literature review that discusses approaches,challenges, and resources. Furthermore, the taxonomy we provide for the reviewed approaches differs from the one given in as elaborated in the next section.

The remaining sections of this paper are structured as follows: Section 2 provides an explanation of the methodology employed for conducting this review. Sections 3and 4 describe the different main and sub-categories of the proposed classification schema for DL-based expressive TTS models. Main challenges facing ETTS models and how they have been tackled in the literature are covered in Section 5. We then give a brief description of ETTS datasets and applied evaluation metrics in Sections 6 and 7, respectively. Finally, Section 8 concludes the paper.

## 2.方法

The last few years have seen rapid growth in expressive and emotional speech synthesis approaches, resulting in a large number of papers and publications in this area. Here, we present the outcomes of a systematic literature review of the last 5 years’ publications within this active research area. This section describes the methodology used to conduct the review, illustrated by Fig.03, which consists of three main stages: paper selection, paper exclusion, and paper classification.

### 2.1.论文选择

For our review, we used the Scopus database to retrieve papers as it encompasses most of the significant journals and conferences pertaining to the speech synthesis field. Our query criteria to find relevant papers on Scopus were twofold: (1) the paper title must include at least one of four words (emotion* OR expressive OR prosod* OR style) that denote expressive speech, and (2) the paper title, abstract, or keywords must comprise the terms “speech” AND “synthesis,” in addition to at least one of the above-mentioned words for expressive speech.
We considered all papers written in English and published in journals or conferences since 2018. The search query was conducted in January 2023, and it yielded 356papers. Scopus provides an Excel file containing all the primary information of the retrieved papers, which we used in the second stage of our review.

### 2.2.论文排除

The exclusion of papers occurred in two phases. In the first phase, we screened the abstract text, while in the second phase, we screened the full text of the paper. Five main constraints were used to exclude papers, including (1) papers that were not related to the TTS field, (2)papers that were not DL-based models, (3) papers that did not focus on expressive or emotional TTS models, (4)papers that were too specific to non-English languages,and (5) papers that lacked details about the applied method. After screening the paper abstracts, we excluded180 papers, mostly based on the first exclusion criterion.
During the second exclusion phase, in which we read the full text of each paper, we identified another 65 papers that met at least one of the five exclusion criteria. Consequently, 111 papers were included in the third stage of our review. Additionally, a group of recently published papers in this area ([17,18,19,20] [InstructTTS (2023)](../../Models/_tmp/2023.01.31_InstructTTS.md), [22,23, [DiffProsody (2023)](../../Models/E2E/2023.07.31_DiffProsody.md), [VoiceBox (2023)](../../Models/Speech_LLM/2023.06.23_VoiceBox.md)) was hand-picked and added to the final set of selected papers. While most of the reviewed papers trained their models on English data, a few other papers used data in other languages as listed in Table 1.

### 2.3.论文分类

After summarizing the approach proposed for generating expressive speech in each selected paper, we categorized the papers based on the learning approach applied in each one. Accordingly, papers are divided into two main categories, including supervised and unsupervised approaches. Under the supervised category, where labeled data is utilized, we identified three subcategories based on how models are employed expressive speech synthesis. The three proposed subcategories are (1)labels as input features, (2) labels as separate layers or models, and (3) labels for emotion predictors/classifiers.

Papers in the unsupervised approaches category are grouped into four different subcategories based on the main structure or method used for modeling expressivity in these papers. From our observation, most of the proposed methods in the last 5 years are based on three main early works in this field, namely, reference encoder [74], global style tokens[75], and latent features via variational autoencoders (VAE)[76] [77]. Specifically, proposed models in most of the papers under this category can be considered as an extension or enhancement of one of the three previously mentioned methods. Besides, we identify a fourth subcategory that includes the recent TTS models representing the new trend in the TTS area, which utilizes in-context learning. There is one factor common to all these four unsupervised models,which is that they are all based on using an audio reference/prompt. Additionally, we added a fifth subcategory (named other approaches) in which we include approaches outside the previous four main unsupervised approaches. Fig.04 illustrates the proposed classification schema for the DL-based expressive speech synthesis models.


## 3.监督学习方法


Supervised approaches refer to models that are trained on datasets with emotion labels.
Those labels guide model training, enabling it to learn accurate weights.
Early deep learning-based expressive speech synthesis systems were primarily supervised models that utilized labeled speech exhibiting various emotions (such as sadness, happiness, and anger) or speaking styles (such as talk-show, newscaster, and call-center).
Note that the term style has also been used to refer to a set of emotions or a mixture of emotions and speaking styles ([ST-TTS](), [68] [78] [79].
**Generally, the structure of early conventional TTS models was built upon two primary networks: one for predicting duration and the other for predicting acoustic features.
These acoustic features were then converted to speech using vocoders.
Both networks receive linguistic features extracted from the input text.**
In supervised ETTS approaches, speech labels (emotions and/or styles) are represented in the TTS model as either input features or as separate layers, models, or sets of neurons for each specific label.
The following sections explain these three representations in detail then we provide a general summary of the supervised approaches reviewed in this work in Table 2.

监督学习方法是指在带有情感标签的数据集上训练的模型。这些标签指导模型训练以确保能够学习到准确的权重。
早期基于深度学习的表达性语音合成系统主要是监督模型，他们使用带有各种情感 （如悲伤、快乐和愤怒）或说话风格（如脱口秀、新闻播音和呼叫中心）标签的语音。注意，“风格”这一术语同样用于指代一组情感或情感和说话风格的混合。
**通常早期传统 TTS 模型的结构是建立在两个主要网络之上：一个用于预测时长；另一个用于预测声学特征，这些声学特征之后通过声码器转换为语音。这两个网络都接收从输入文本中提取出的的语义特征。**
在监督 ETTS 方法中，语音标签（情感与、或风格）在 TTS 模型中被表示为输入特征或为每个特定标签的单独层、模型或神经元集合。以下小节将详细解释这三种表示，然后在表格二中提供本文回顾的监督方法的概述。

|引用序号|算法简称|输入|情感标签表示|是否支持情绪转移|TTS 模型|
|:-:|:-:|---|---|:-:|:-:|
|80||语言特征+情感标签|独热编码||DL-SPSS, HMM|
|65||语言特征+情感标签|独热编码/单独层|√|DL-SPSS|
|66||语言特征+情感标签|感知向量/矩阵||DL-SPSS|
|41||语言特征+情感标签|独热编码||DL-SPSS|
|42||语言特征+情感标签|依赖层||DL-SPSS|
|81||语言特征+情感标签|独热编码/神经元集合|√|DL-SPSS|
|43||语言特征+情感标签|独热编码/依赖层/独立模型||DL-SPSS|
|82||语言特征+情感标签|独热编码|√|DL-SPSS|
|83||音素序列+语言模型特征+情感标签|嵌入向量||Encoder-Dttention-Decoder|
|28, 78||语言特征+情感标签|独热编码/依赖层/独立模型|√|DL-SPSS|
|26||音素序列+梅尔频谱+情感标签|独热编码 GSTs 权重的真实值||Tacotron2|
|27||音素序列+语言特征+情感标签|嵌入向量||Tacotron2|
|84||语言特征+情感标签|嵌入与其他数据标签联合||DL-SPSS|
|85||语言特征+韵律特征+情感标签|分类器的真实值||DL-SPSS|
|86||音素序列+情感标签|嵌入向量||Transformer TTS|
|32, 36||字符序列+梅尔频谱+情感标签|分类器的真实值||Tacotron2|
|69||语言特征+情感标签|独热编码/依赖层|√|DL-SPSS|
|34||音素序列+梅尔频谱+情感标签|分类器的真实值||Tacotron2|
|64||字符序列+语言模型特征+情感标签|分类器的真实值||Tacotron2|
|39, 87||音素序列+梅尔频谱+情感标签|分类器的真实值||Tacotron2|

### 3.1 Labels as Input Features 标签作为输入特征

The most straightforward method for representing emotion labels of annotated datasets as input to the TTS model is by using a one-hot vector.
This approach entails using a vector with a size equivalent to the number of available labels.
In this vector, a value of (1) is assigned to the index corresponding to the label ID, while all other values are set to (0).
Many early ETTS models [43] [56] [65] [69] [78] [80] [82] [84] advocated for this direct representation of emotion labels in order to generate speech encompassing various emotions.

用于表示带有注释的数据集的情感标签作为 TTS 模型输入的最直接方法是使用独热编码。这种方法需要使用和可用标签数量长度相同的向量。在这个向量内，将值为 1 分配给标签 ID 对应的索引，其他值为 0. 许多早期 ETTS 模型提倡这种直接表示情感标签的方法以生成包含各种情绪的语音。

The one-hot emotion vector, also referred to as a style/emotion code in some studies [43] [78] [80] [82], is concatenated with the input linguistic features of the model.

独热编码在某些文献中也被称为风格/情感编码，和模型的输入语言特征进行拼接。

When dealing with large number of labels, the one-hot representation becomes both high-dimensional and sparse.
Moreover, in other scenarios, merging label vectors with input features instead of concatenation can lead to length mismatch issues.

当处理大量标签时，独热编码表示变得高维且稀疏。而且在其他情况下，将标签向量和输入特征合并而不是拼接会导致长度不匹配问题。

In both situations, the embedding layer offers a solution by creating a continuous representation for each label, known as embedding vectors.
Unlike the one-hot vector, which is constrained in size based on the number of labels, an emotion embedding can have any dimension, regardless of the number of available labels.

在这两种情况下，嵌入层提供了一种解决方案：通过给每个标签创建一个连续的表示，即嵌入向量。和独热编码受到标签数量的限制不同，情感嵌入可以有任意的维度，和可用标签数量无关。

For instance, in [84], each sample in the training dataset has three separated labels including speaker, style(emotion), and cluster.
In this context, the cluster value indicates the consistency in speech quality of a given speaker and style pair.
If one-hot vector is used to represent each unique combined label of each sample, the resulting label vector will be high dimensional (which in this case is 67).
Therefore, the three one-hot vectors representing the given three labels are combined and passed as input to an embedding layer to reduce its dimension (in this case 15).
On a different note, [41] utilizes an embedding layer to expand concise binary one-hot label vectors to match with the dimensions of the input features to be added together as input to the TTS model.

例如, 在文献 [84] 中, 训练数据集的每个样本有三个独立的标签, 包括说话人, 风格 (情感) 和聚类类别. 在这种情况下, 聚类类别值表明了同时给定说话人和风格在语音质量方面的一致性. 如果独热编码用于表示每一个唯一拼接的标签, 那么标签向量会变得非常高维. 因此这三个独热编码向量分别表示给定的三个标签结合并输入到嵌入层进行降维. 而文献 [41] 使用嵌入层将简洁的二进制独热标签向量扩展到匹配输入特征的维度, 以便相加作为 TTS 模型的输入.

To address the potential disparities between a talker’s intent and a listener’s perception when annotating emotional samples, in [66], a different methodology for representing labels is introduced.
In the context of N emotion classes, each sample from the talker may be perceived by the listener as one of the N emotions.
In response to this, the paper suggests the adoption of a singular vector termed the ’perception vector,’ with N dimensions.
This vector represents how samples from a specific emotion class are distributed among the N emotions, based on the listener’s perception.
Furthermore, in the context of multiple listeners, each emotion class can be represented as a confusion matrix that captures the diverse perceptions of samples belonging to that emotion class by multiple listeners.

为了解决在注释情感样本时说话人意图和倾听者的感知之间的潜在差异, 文献 [66] 引入了表示标签的不同方法. 在具有 N 个情绪类别的情况下, 来自说话人的每个样本可能被倾听者感知为这 $N$ 个情绪的其中之一. 对此, 文献 [66] 建议采样一个名为"感知向量"的单个 N 维向量. 这一向量表示特定情绪类别的样本如何根据倾听者的感知在 N 个情绪上分布. 此外, 在多倾听者的情况下, 每个情绪类可以表示为一个混淆矩阵, 捕获由多个倾听者提供的属于该情绪类别的样本的多样性感知.

### 3.2 Labels as Separate Layers/Models 标签作为单独层/模型

In this approach, to represent emotion or style labels in TTS models, each label is associated with either a separate instance of the DNN model, an emotion-specific layers, or a set of emotion-specific neurons within a layer.
Initially, the model is trained using neutral data, which typically has larger size.
Subsequently, in the first approach, multiple copies of the trained model are fine-tuned using emotion-specific data of small size [43] [78].
In the second approach, instead of creating an individual model for each emotion, only specific model layers (usually the uppermost or final layers) from the employed DNN model are assigned to each emotion [43] [65] [69] [78] as shown by Fig. 5.
While shared layers are adjusted during training using neutral data, output layers corresponding to each emotion are modified exclusively when the model is trained with data from the respective emotion.

在这种方法中, TTS 模型内为了表示情感或风格标签, 每个标签要么和 DNN 模型的单独示例即一个特定情感层, 要么和一层内的特定情感神经元集合相关联.
首先, 模型使用通常尺寸较大的中性数据进行训练.
第一种方法, 对多个已经训练好的模型的副本分别使用小尺寸的特定情感数据进行微调;
第二种方法, 不为每种情感创建单独模型, 而是只将所使用的 DNN 模型中的特定层 (通常是最上层/最终层) 分配给每种情感. 如图五所示. 使用中性数据训练时共享层会进行调整, 对应每种情感的输出层仅在模型使用相应情感的数据进行训练是进行修改.

Alternatively, when dealing with limited data for certain emotions/styles, the model can initially undergo training for emotions with large amount of data.
Following this step, the weights of the shared layers within the model are fixed, and only the weights of the top layers are fine-tuned using the limited, emotion-specific data [42].

当处理某些情感或风格的有限数据时, 模型可以先为具有大量数据的情感进行训练. 完成后将共享层的权重固定, 只有最顶层的权重使用有限的, 特定情感的数据进行微调. 如文献 [42].

Another method for representing emotion labels involves allocating specific neurons from a layer within the DNN model for each emotion.
In this approach, the hidden layers of the model could be expanded by introducing new neurons.
Then, as outlined in [81], particular neurons from this expanded set are assigned to represent each distinct emotion.
Importantly, the associated weights of these specific neuron subsets are adjusted solely during the processing of data relevant to the corresponding emotion.
Furthermore, by substituting the subset of neurons dedicated to a particular emotional class with a different set, the model becomes capable of generating speech imbued with the desired emotional class.
This capability holds true even for new speakers who only possess neutral data, and in this case, it is known as **expression/emotion transplantation**.

其他表示情感标签的方法是将 DNN 模型层中特定的神经元分配给每种情感. 这种方法可以通过引入新的神经元来扩展模型的隐藏层. 如文献 [81] 从扩展的神经元集合中分配特定神经元来表示每种不同的情感. 重要的是只有在处理和相应情感相关的数据时, 这些特定神经元子集的关联权重才会进行调整. 此外, 通过加入专门用于某种特定情感类别的神经元, 模型可以生成具有所需情感类的语音.
这种能力对仅有中性数据的新说话人也成立, 这称为表达/情感移植.

### 3.3 Labels for Emotion Predictors/Classifiers 标签用于情感预测器/分类器

Another common approach to utilize emotion labels is to use them directly or via emotion predictor or classifier to support the process of extracting emotion/prosody embedding.

另一种常见的使用情感标签的方法是直接使用它们或者通过情感预测器/分类器以支持提取情感/韵律嵌入的过程.

For example, in **"End-to-End Emotional Speech Synthesis Using Style Tokens  and Semi-Supervised Training"** emotion labels represented as one-hot vectors are used as targets for the weight vectors of GSTs (explained in Section 4.3) where a cross entropy loss between the two vectors is added to the total loss function.
Yoon et al. [64] proposes a joint emotion predictor based on the Generative Pre-trained Transformer (GPT)-3 [88].
The proposed predictor produces two outputs including emotion class and emotion strength based on features extracted from input text by (GPT)-3.
A joint emotion encoder is then used to encode the predictor outputs into a joint emotion embedding.
The joint emotion predictor is trained with the guidance of the emotion labels and emotion strength values obtained via a <algo>ranking support vector machine (RankSVM)</algo[89].

文献 [026] 中情感标签表示成独热向量, 作为 GSTs 权重向量的目标值, 这两个向量之间的交叉熵损失被添加到总损失函数中;
文献 [64] 提出了基于 GPT-3 的联合情感预测器, 这个预测器基于 GPT-3 从输入文本中提取的特征产生两个输出包括情感类别和情感强度.
联合情感编码器将预测器的输出编码为一个联合情感嵌入.
联合情感预测器在通过 RankSVM 获得的情感标签和情感强度值得指导下进行训练.

In [32], an emotion classifier is used to produce more discriminative emotion embeddings.
Initially, the input Mel-spectrogram features from the reference-style audio and those predicted by the proposed TTS model are passed to two reference encoders (explained in Section 4.1) to generate reference embeddings.
Both embeddings are then fed to two emotion classifiers, which consist of intermediate fully connected (FC) layers.
The output of the second FC layer from both classifiers is considered as the emotion embedding.
Apart from the loss of the classifiers, an additional loss function is established between the resulting emotion embeddings from the two classifiers.
Similarly, an emotion classifier is also employed in [36] to reduce irrelevant information in the generated emotion embedding from an emotion encoder with reference speech (Mel-spectrogram) as input.

文献 [32] 使用情感分类器用于产生更具有区分性得情感嵌入.
首先从参考风格音频的输入梅尔频谱特征和 TTS 模型的相应预测传递到两个参考编码器中以生成参考嵌入.
两个嵌入之后都输入到两个情感分类器中, 由中间全连接层组成.
分类器的第二个全连接层的输出被视为情感嵌入.
除了分类器的损失之外, 还建立了两个分类器产生的情感嵌入结果之间的附加损失函数.
文献 [36] 使用了情感分类器用于减少带有参考语音 (梅尔频谱) 的情感编码器生成的情感嵌入的无关信息.

Several other studies [34] [36] [39] that support multiple speakers also suggest utilizing a speaker classifier in addition to the emotion classifier.
This approach aims to improved the speaker embedding derived from speaker encoders.
Moreover, these studies introduce an adversarial loss between the speaker encoder and the emotion classifier using a gradient reversal layer (GRL) [90].
The purpose of this is to minimize the potential transfer of emotion-related information into the speaker embedding.
The GRL technique involves updating the weights of the speaker encoder by utilizing the inverse of the gradient values obtained from the emotion classifier during the training process.

文献 [34] [36] [39] 支持多说话人的研究也建议使用说话人分类器以及情感分类器.
这一方法旨在改善说话人编码器导出的说话人嵌入.
此外这些研究通过使用一个 GRL 引入了说话人编码器和情感分类器的对抗损失.
目的是最小化情感相关的信息转移到说话人嵌入中.
GRL 技术涉及在训练过程中使用从情感分类器获得的梯度值的逆来更新说话人编码器的权重.

## Sec.09: Conclusions·结论

This paper presents the findings of our systematic literature review on expressive speech synthesis over the past 5 years.
The main contribution of this article is the development of a comprehensive taxonomy for DL-based approaches published in this field during that specific time frame.
The approaches are classified into three primary categories based on the learning method, followed by models within each category.
Further subcategories are identified at the lower levels of the taxonomy, considering the methods and structures applied to achieve expressiveness in synthesized speech.
In addition to the ETTS approaches taxonomy, we provide descriptions of the main challenges in the ETTS field and proposed solutions from the literature.
Furthermore, we support the reader with brief summaries of ETTS datasets, performance evaluation metrics, and some open-source implementations.
The significance of our work lies in its potential to serve as an extensive overview of the research conducted in this area from different aspects, benefiting both experienced researchers and newcomers in this active research domain.

本文介绍了我们对过去 5 年内表现力语音合成领域的系统性文献综述的研究结果.
本文的主要贡献是开发了一个全面的分类法, 用于描述在该特定时间段内发表的基于深度学习（DL）的方法.
这些方法根据学习方式被分为三个主要类别, 每个类别下又包含具体的模型.
在分类法的较低层次, 进一步确定了子类别, 这些子类别考虑了实现合成语音表现力的方法和结构.
除了ETTS方法的分类法之外, 我们还描述了ETTS领域的主要挑战以及文献中提出的解决方案.
此外, 我们为读者提供了ETTS数据集, 性能评估指标以及一些开源实现的简要概述.
我们工作的意义在于, 它有可能作为一个全面的概述, 从不同方面展示该领域的研究, 对经验丰富的研究人员和活跃研究领域的新手都有益处.

Some main directions for future work in this area involve collection of large expressive datasets in different languages, going from acted expressive style to realistic style.
Further evaluation metrics are still needed in this area for assessing models’ performance such as evaluation of prosody controllability.
Efficient metrics are also required for monitoring performance and guiding loss evaluation during the training process.
These need to be lightweight and fast in order not to slow down training but still reliable.
Another suggestion for future investigations is to take cultural differences in perception of expressions into account for multi-language, multi-speaker expressive TTS applications.
Moreover, as speech is just one modality for expressions, multi-modal approaches that combine facial expressions, eye movements, body movements, gestures, non-verbal clues, etc., will be required to reach human-level expressiveness.
Training several modalities together could be beneficial as the model can transfer useful information from one modality to another in a self-supervised fashion.

该领域未来工作的一些主要方向包括收集不同语言的大型表现力数据集, 从表演风格过渡到真实风格.
该领域仍然需要进一步的评估指标来评估模型的性能, 例如韵律可控性的评估.
还需要有效的指标来监控性能并在训练过程中指导损失评估.
这些指标需要轻量级且快速, 以免减慢训练速度, 但仍然可靠.
未来研究的另一个建议是考虑多语言, 多说话者表现力TTS应用中对表达感知的文化差异.
此外, 由于语音只是表达的一种方式, 因此需要多模态方法, 这些方法结合了面部表情, 眼神移动, 身体动作, 手势, 非言语线索等, 以达到人类水平的表现力.
同时训练多个模态可能是有益的, 因为模型可以在自我监督的方式下将一个模态的有用信息传递给另一个模态.