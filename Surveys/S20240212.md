# Deep Learning-Based Expressive Speech Synthesis: A Systematic Review of Approaches, Challenges, and Resources

- 标题: Deep Learning-Based Expressive Speech Synthesis: A Systematic Review of Approaches, Challenges, and Resources
- 作者:
  - Huda Barakat
  - Oytun Turk
  - Cenk Demiroglu
- 发表:
  - EURASIP Journal on Audio, Speech, and Music Processing (2024)
  - DOI: https://doi.org/10.1186/s13636-024-00329-7


## Abstract·摘要

Speech synthesis has made significant strides thanks to the transition from machine learning to deep learning models.
Contemporary **Text-To-Speech (TTS)** models possess the capability to generate speech of exceptionally high quality, closely mimicking human speech.
Nevertheless, given the wide array of applications now employing TTS models, mere high-quality speech generation is no longer sufficient.
Present-day TTS models must also excel at producing expressive speech that can convey various speaking styles and emotions, akin to human speech.
Consequently,researchers have concentrated their efforts on developing more efficient models for expressive speech synthesis in recent years.
This paper presents a systematic review of the literature on expressive speech synthesis models published within the last 5 years, with a particular emphasis on approaches based on deep learning.
We offer a comprehensive classification scheme for these models and provide concise descriptions of models falling into each category.
Additionally, we summarize the principal challenges encountered in this research domain and outline the strategies employed to tackle these challenges as documented in the literature.
In the Section 8, we pinpoint some research gaps in this field that necessitate further exploration.
Our objective with this work is to give an all-encompassing overview of this hot research area to offer guidance to interested researchers and future endeavors in this field.

语音合成技术因从机器学习到深度学习模型的转变而取得了显著进步.
当代的文本转语音模型能够生成质量极高的, 几乎媲美人类的语音.
但是鉴于现在有众多应用正在采用文本转语音模型, 仅仅生成高质量的语音已经不再足够.
现今的文本转语音模型还必须擅长生成富有表现力的语音, 能够传达类似于人类语音的多种说话风格和情感.
因此, 近年来研究人员集中精力于开发更高效的富有表现力的语音合成模型.

本文对过去五年内发表的关于富有表现力的语音合成模型的文献进行了系统性的回顾, 特别关注基于深度学习的方法.
我们为这些模型提供了一个全面的分类方案, 并对每个类别中的模型进行了简要的描述.
此外, 我们总结了在这一研究领域遇到的主要挑战, 并概述了文献中记录的应对这些挑战的策略.

在第 8 节中, 我们指出了该领域中需要进一步探索的一些研究空白.

我们进行这项工作的目的是为这一热门研究领域提供一个全面的概述, 为对这一领域感兴趣的研究人员和未来研究提供指导.

文章内容:
- [ ] [Sec.01](Sec.01.md)
- [ ] [Sec.02](Sec.02.md)
- [ ] [Sec.03](Sec.03.md)
- [ ] [Sec.04](Sec.04.md)
- [ ] [Sec.05](Sec.05.md)
- [x] [Sec.06](Sec.06.md)
- [x] [Sec.07](Sec.07.md)
- [ ] [Sec.08](Sec.08.md)

## Section 01: Introduction

Since the late 1950s, computer-based text-to-speech systems (TTS) have undergone significant advancements, culminating in the production of models that generate speech almost indistinguishable from that of a human. This progress has followed a path consisting of several stages, beginning with conventional methods named as concatenative synthesis and progressing to more advanced approaches known as **Statistical Parametric Speech Synthesis (SPSS)**. Advanced approaches are mainly based on machine learning algorithms like <algo>hidden Markov models (HMMs)</algo> and <algo>gaussian mixture models (GMMs)</algo>. Despite this progress, speech generated by these methods was still noticeably artificial. However, the emergence of deep learning (DL) as a new branch under machine learning (ML) in 2006 has led to significant improvements. Speech synthesis researchers,like many in other research fields, started incorporating deep neural networks (DNN) in their models. Initially,DNNs replaced HMMs and GMMs in SPSS models while the main structure still follows the primary framework of SPSS models as shown in Fig. 1. As discussed in **"Statistical Parametric Speech Synthesis Using Deep Neural Networks"**, the deep learning-based models have overcome many limitations and problems associated with machine learning-based models.

Researchers continue to aim for improved speech quality and more human-like speech despite past advancements. Additionally, they seek to simplify the framework of the text-to-speech models due to the intricate nature of the SPSS structure, which limits progress in this field to those with extensive linguistic knowledge and expertise. Deep learning advancements have brought about the simple encoder-decoder structure for TTS models as <algo>sequence-to-sequence (Seq2Seq)</algo> approaches. The pro-posed approaches have simplified the structure of conventional TTS with multiple components into training a single network that converts a set of input text characters/phonemes into a set of acoustic features (mel-spectrograms). A main concern in these advanced TTS models is the mapping process between the input and output sequences, which is a one-to-many problem, as the single input text can have multiple speech variations as output. In fact, there are two groups of recent TTS models, as shown in Fig.02. The first group generates mel-spectrograms in a sequential (autoregressive) manner using soft and automatic attention alignments between input and output sequences, such as the Tacotron model ([Tacotron (2017)](../../Models/Acoustic/_PDF/2017.03.29_Tacotron.md) [Tacotron2 (2017)](../../Models/Acoustic/2017.12.16_Tacotron2.mdhe second group utilizes hard alignments between the phonemes/characters and mel-spectro-grams, and thus its speech generation process is parallel(non-autoregressive), as in the FastSpeech model ([FastSpeech (2019)](../../Models/Acoustic/2019.05.22_FastSpeech.mdstSpeech2](../../Models/TTS2_Acoustic/2020.06.08_FastSpeech2.md)).This improvement in the structure of the TTS model has encouraged rapid development in the field within the last few years, during which the proposed models produced speech that is nearly indistinguishable from human speech.

Human speech is highly expressive and reflects various factors, such as the speaker’s identity, emotion, and speaking style. In addition, there are many applications in which speech synthesis can be utilized, especially expressive speech synthesis. For instance, audiobooks and podcast applications that create audio versions of eBooks and podcasts, translation applications which provide real-time translation of foreign language text, dubbing applications that generate an alternative audio track for a video with different content, speaker, or language, and content creation applications which help produce audio versions of textual content, such as blogs and news articles. E-learning applications that allow for adding voice-over audio to e-learning courses, and conversational AI applications enable machines to communicate with users in a human-like manner, such as AI chatbots and virtual assistants.

As spoken language is a crucial component in such applications, users must feel as if they are communicating with a real human rather than a machine. Therefore, the speech generated by these applications should convey appropriate emotion, intonation, stress, and speaking style to match the ongoing conversation or the content type and context of the text being read.

As a result, there has been a recent attention towards building efficient expressive speech synthesis models as another step forward in achieving human-like speech. Therefore, many studies have been devoted to expressive speech synthesis (ETTS) as a hot research area, particularly over the last 5 years. In this work, we present the findings of our systematic literature review on ETTS field from the past 5 years. Firstly, we suggest a classification schema of deep learning-based ETTS models that are proposed during this period, based on structures,and learning methods followed in each study. A summary is then provided for each category in the classification schema and main papers related to this category. After that, we outline the main challenges in the ETTS area and solutions that have been proposed to solve them from literature. Finally, we conclude with a discussion of the implications of our work and a highlight of some gaps that require further research in this area.

During our work on this review of expressive speech synthesis literature, we came across several review papers that focus on different stages of development in the speech synthesis field. The majority of these reviews concentrate on DL-based TTS approaches, while only a few papers cover recent TTS approaches in addition to early conventional ones. However, to the best of our knowledge, there are no review papers that cover the fast growth in the (expressive) speech synthesis area, especially in the last few years. Therefore, our main goal in this review is to provide an overview of research trends, techniques, and challenges in this area during this period. We hope that our work will offer researchers a comprehensive understanding of how and what has been accomplished in this field and the gaps that need to be filled as guidance for their future efforts.

While we were writing this paper, we came across an interesting recent review paper **"An Overview of Affective Speech Synthesis and Conversion in the Deep Learning Era"** that is similar to our work. However, the review covers emotional speech synthesis (ESS) as a sub-field of voice transformation while our work is more comprehensive as a systematic literature review that discusses approaches,challenges, and resources. Furthermore, the taxonomy we provide for the reviewed approaches differs from the one given in as elaborated in the next section.

The remaining sections of this paper are structured as follows: Section 2 provides an explanation of the methodology employed for conducting this review. Sections 3and 4 describe the different main and sub-categories of the proposed classification schema for DL-based expressive TTS models. Main challenges facing ETTS models and how they have been tackled in the literature are covered in Section 5. We then give a brief description of ETTS datasets and applied evaluation metrics in Sections 6 and 7, respectively. Finally, Section 8 concludes the paper.

## 2.方法

The last few years have seen rapid growth in expressive and emotional speech synthesis approaches, resulting in a large number of papers and publications in this area. Here, we present the outcomes of a systematic literature review of the last 5 years’ publications within this active research area. This section describes the methodology used to conduct the review, illustrated by Fig.03, which consists of three main stages: paper selection, paper exclusion, and paper classification.

### 2.1.论文选择

For our review, we used the Scopus database to retrieve papers as it encompasses most of the significant journals and conferences pertaining to the speech synthesis field. Our query criteria to find relevant papers on Scopus were twofold: (1) the paper title must include at least one of four words (emotion* OR expressive OR prosod* OR style) that denote expressive speech, and (2) the paper title, abstract, or keywords must comprise the terms “speech” AND “synthesis,” in addition to at least one of the above-mentioned words for expressive speech.
We considered all papers written in English and published in journals or conferences since 2018. The search query was conducted in January 2023, and it yielded 356papers. Scopus provides an Excel file containing all the primary information of the retrieved papers, which we used in the second stage of our review.

### 2.2.论文排除

The exclusion of papers occurred in two phases. In the first phase, we screened the abstract text, while in the second phase, we screened the full text of the paper. Five main constraints were used to exclude papers, including (1) papers that were not related to the TTS field, (2)papers that were not DL-based models, (3) papers that did not focus on expressive or emotional TTS models, (4)papers that were too specific to non-English languages,and (5) papers that lacked details about the applied method. After screening the paper abstracts, we excluded180 papers, mostly based on the first exclusion criterion.
During the second exclusion phase, in which we read the full text of each paper, we identified another 65 papers that met at least one of the five exclusion criteria. Consequently, 111 papers were included in the third stage of our review. Additionally, a group of recently published papers in this area ([17,18,19,20] [InstructTTS (2023)](../../Models/_tmp/2023.01.31_InstructTTS.md), [22,23, [DiffProsody (2023)](../../Models/E2E/2023.07.31_DiffProsody.md), [VoiceBox (2023)](../../Models/Speech_LLM/2023.06.23_VoiceBox.md)) was hand-picked and added to the final set of selected papers. While most of the reviewed papers trained their models on English data, a few other papers used data in other languages as listed in Table 1.

### 2.3.论文分类

After summarizing the approach proposed for generating expressive speech in each selected paper, we categorized the papers based on the learning approach applied in each one. Accordingly, papers are divided into two main categories, including supervised and unsupervised approaches. Under the supervised category, where labeled data is utilized, we identified three subcategories based on how models are employed expressive speech synthesis. The three proposed subcategories are (1)labels as input features, (2) labels as separate layers or models, and (3) labels for emotion predictors/classifiers.

Papers in the unsupervised approaches category are grouped into four different subcategories based on the main structure or method used for modeling expressivity in these papers. From our observation, most of the proposed methods in the last 5 years are based on three main early works in this field, namely, reference encoder [74], global style tokens[75], and latent features via variational autoencoders (VAE)[76] [77]. Specifically, proposed models in most of the papers under this category can be considered as an extension or enhancement of one of the three previously mentioned methods. Besides, we identify a fourth subcategory that includes the recent TTS models representing the new trend in the TTS area, which utilizes in-context learning. There is one factor common to all these four unsupervised models,which is that they are all based on using an audio reference/prompt. Additionally, we added a fifth subcategory (named other approaches) in which we include approaches outside the previous four main unsupervised approaches. Fig.04 illustrates the proposed classification schema for the DL-based expressive speech synthesis models.


## 3.监督学习方法


Supervised approaches refer to models that are trained on datasets with emotion labels.
Those labels guide model training, enabling it to learn accurate weights.
Early deep learning-based expressive speech synthesis systems were primarily supervised models that utilized labeled speech exhibiting various emotions (such as sadness, happiness, and anger) or speaking styles (such as talk-show, newscaster, and call-center).
Note that the term style has also been used to refer to a set of emotions or a mixture of emotions and speaking styles ([ST-TTS](), [68] [78] [79].
**Generally, the structure of early conventional TTS models was built upon two primary networks: one for predicting duration and the other for predicting acoustic features.
These acoustic features were then converted to speech using vocoders.
Both networks receive linguistic features extracted from the input text.**
In supervised ETTS approaches, speech labels (emotions and/or styles) are represented in the TTS model as either input features or as separate layers, models, or sets of neurons for each specific label.
The following sections explain these three representations in detail then we provide a general summary of the supervised approaches reviewed in this work in Table 2.

监督学习方法是指在带有情感标签的数据集上训练的模型。这些标签指导模型训练以确保能够学习到准确的权重。
早期基于深度学习的表达性语音合成系统主要是监督模型，他们使用带有各种情感 （如悲伤、快乐和愤怒）或说话风格（如脱口秀、新闻播音和呼叫中心）标签的语音。注意，“风格”这一术语同样用于指代一组情感或情感和说话风格的混合。
**通常早期传统 TTS 模型的结构是建立在两个主要网络之上：一个用于预测时长；另一个用于预测声学特征，这些声学特征之后通过声码器转换为语音。这两个网络都接收从输入文本中提取出的的语义特征。**
在监督 ETTS 方法中，语音标签（情感与、或风格）在 TTS 模型中被表示为输入特征或为每个特定标签的单独层、模型或神经元集合。以下小节将详细解释这三种表示，然后在表格二中提供本文回顾的监督方法的概述。

|引用序号|算法简称|输入|情感标签表示|是否支持情绪转移|TTS 模型|
|:-:|:-:|---|---|:-:|:-:|
|80||语言特征+情感标签|独热编码||DL-SPSS, HMM|
|65||语言特征+情感标签|独热编码/单独层|√|DL-SPSS|
|66||语言特征+情感标签|感知向量/矩阵||DL-SPSS|
|41||语言特征+情感标签|独热编码||DL-SPSS|
|42||语言特征+情感标签|依赖层||DL-SPSS|
|81||语言特征+情感标签|独热编码/神经元集合|√|DL-SPSS|
|43||语言特征+情感标签|独热编码/依赖层/独立模型||DL-SPSS|
|82||语言特征+情感标签|独热编码|√|DL-SPSS|
|83||音素序列+语言模型特征+情感标签|嵌入向量||Encoder-Dttention-Decoder|
|28, 78||语言特征+情感标签|独热编码/依赖层/独立模型|√|DL-SPSS|
|26||音素序列+梅尔频谱+情感标签|独热编码 GSTs 权重的真实值||Tacotron2|
|27||音素序列+语言特征+情感标签|嵌入向量||Tacotron2|
|84||语言特征+情感标签|嵌入与其他数据标签联合||DL-SPSS|
|85||语言特征+韵律特征+情感标签|分类器的真实值||DL-SPSS|
|86||音素序列+情感标签|嵌入向量||Transformer TTS|
|32, 36||字符序列+梅尔频谱+情感标签|分类器的真实值||Tacotron2|
|69||语言特征+情感标签|独热编码/依赖层|√|DL-SPSS|
|34||音素序列+梅尔频谱+情感标签|分类器的真实值||Tacotron2|
|64||字符序列+语言模型特征+情感标签|分类器的真实值||Tacotron2|
|39, 87||音素序列+梅尔频谱+情感标签|分类器的真实值||Tacotron2|

### 3.1 Labels as Input Features 标签作为输入特征

The most straightforward method for representing emotion labels of annotated datasets as input to the TTS model is by using a one-hot vector.
This approach entails using a vector with a size equivalent to the number of available labels.
In this vector, a value of (1) is assigned to the index corresponding to the label ID, while all other values are set to (0).
Many early ETTS models [43] [56] [65] [69] [78] [80] [82] [84] advocated for this direct representation of emotion labels in order to generate speech encompassing various emotions.

用于表示带有注释的数据集的情感标签作为 TTS 模型输入的最直接方法是使用独热编码。这种方法需要使用和可用标签数量长度相同的向量。在这个向量内，将值为 1 分配给标签 ID 对应的索引，其他值为 0. 许多早期 ETTS 模型提倡这种直接表示情感标签的方法以生成包含各种情绪的语音。

The one-hot emotion vector, also referred to as a style/emotion code in some studies [43] [78] [80] [82], is concatenated with the input linguistic features of the model.

独热编码在某些文献中也被称为风格/情感编码，和模型的输入语言特征进行拼接。

When dealing with large number of labels, the one-hot representation becomes both high-dimensional and sparse.
Moreover, in other scenarios, merging label vectors with input features instead of concatenation can lead to length mismatch issues.

当处理大量标签时，独热编码表示变得高维且稀疏。而且在其他情况下，将标签向量和输入特征合并而不是拼接会导致长度不匹配问题。

In both situations, the embedding layer offers a solution by creating a continuous representation for each label, known as embedding vectors.
Unlike the one-hot vector, which is constrained in size based on the number of labels, an emotion embedding can have any dimension, regardless of the number of available labels.

在这两种情况下，嵌入层提供了一种解决方案：通过给每个标签创建一个连续的表示，即嵌入向量。和独热编码受到标签数量的限制不同，情感嵌入可以有任意的维度，和可用标签数量无关。

For instance, in [84], each sample in the training dataset has three separated labels including speaker, style(emotion), and cluster.
In this context, the cluster value indicates the consistency in speech quality of a given speaker and style pair.
If one-hot vector is used to represent each unique combined label of each sample, the resulting label vector will be high dimensional (which in this case is 67).
Therefore, the three one-hot vectors representing the given three labels are combined and passed as input to an embedding layer to reduce its dimension (in this case 15).
On a different note, [41] utilizes an embedding layer to expand concise binary one-hot label vectors to match with the dimensions of the input features to be added together as input to the TTS model.

例如, 在文献 [84] 中, 训练数据集的每个样本有三个独立的标签, 包括说话人, 风格 (情感) 和聚类类别. 在这种情况下, 聚类类别值表明了同时给定说话人和风格在语音质量方面的一致性. 如果独热编码用于表示每一个唯一拼接的标签, 那么标签向量会变得非常高维. 因此这三个独热编码向量分别表示给定的三个标签结合并输入到嵌入层进行降维. 而文献 [41] 使用嵌入层将简洁的二进制独热标签向量扩展到匹配输入特征的维度, 以便相加作为 TTS 模型的输入.

To address the potential disparities between a talker’s intent and a listener’s perception when annotating emotional samples, in [66], a different methodology for representing labels is introduced.
In the context of N emotion classes, each sample from the talker may be perceived by the listener as one of the N emotions.
In response to this, the paper suggests the adoption of a singular vector termed the ’perception vector,’ with N dimensions.
This vector represents how samples from a specific emotion class are distributed among the N emotions, based on the listener’s perception.
Furthermore, in the context of multiple listeners, each emotion class can be represented as a confusion matrix that captures the diverse perceptions of samples belonging to that emotion class by multiple listeners.

为了解决在注释情感样本时说话人意图和倾听者的感知之间的潜在差异, 文献 [66] 引入了表示标签的不同方法. 在具有 N 个情绪类别的情况下, 来自说话人的每个样本可能被倾听者感知为这 $N$ 个情绪的其中之一. 对此, 文献 [66] 建议采样一个名为"感知向量"的单个 N 维向量. 这一向量表示特定情绪类别的样本如何根据倾听者的感知在 N 个情绪上分布. 此外, 在多倾听者的情况下, 每个情绪类可以表示为一个混淆矩阵, 捕获由多个倾听者提供的属于该情绪类别的样本的多样性感知.

### 3.2 Labels as Separate Layers/Models 标签作为单独层/模型

In this approach, to represent emotion or style labels in TTS models, each label is associated with either a separate instance of the DNN model, an emotion-specific layers, or a set of emotion-specific neurons within a layer.
Initially, the model is trained using neutral data, which typically has larger size.
Subsequently, in the first approach, multiple copies of the trained model are fine-tuned using emotion-specific data of small size [43] [78].
In the second approach, instead of creating an individual model for each emotion, only specific model layers (usually the uppermost or final layers) from the employed DNN model are assigned to each emotion [43] [65] [69] [78] as shown by Fig. 5.
While shared layers are adjusted during training using neutral data, output layers corresponding to each emotion are modified exclusively when the model is trained with data from the respective emotion.

在这种方法中, TTS 模型内为了表示情感或风格标签, 每个标签要么和 DNN 模型的单独示例即一个特定情感层, 要么和一层内的特定情感神经元集合相关联.
首先, 模型使用通常尺寸较大的中性数据进行训练.
第一种方法, 对多个已经训练好的模型的副本分别使用小尺寸的特定情感数据进行微调;
第二种方法, 不为每种情感创建单独模型, 而是只将所使用的 DNN 模型中的特定层 (通常是最上层/最终层) 分配给每种情感. 如图五所示. 使用中性数据训练时共享层会进行调整, 对应每种情感的输出层仅在模型使用相应情感的数据进行训练是进行修改.

Alternatively, when dealing with limited data for certain emotions/styles, the model can initially undergo training for emotions with large amount of data.
Following this step, the weights of the shared layers within the model are fixed, and only the weights of the top layers are fine-tuned using the limited, emotion-specific data [42].

当处理某些情感或风格的有限数据时, 模型可以先为具有大量数据的情感进行训练. 完成后将共享层的权重固定, 只有最顶层的权重使用有限的, 特定情感的数据进行微调. 如文献 [42].

Another method for representing emotion labels involves allocating specific neurons from a layer within the DNN model for each emotion.
In this approach, the hidden layers of the model could be expanded by introducing new neurons.
Then, as outlined in [81], particular neurons from this expanded set are assigned to represent each distinct emotion.
Importantly, the associated weights of these specific neuron subsets are adjusted solely during the processing of data relevant to the corresponding emotion.
Furthermore, by substituting the subset of neurons dedicated to a particular emotional class with a different set, the model becomes capable of generating speech imbued with the desired emotional class.
This capability holds true even for new speakers who only possess neutral data, and in this case, it is known as **expression/emotion transplantation**.

其他表示情感标签的方法是将 DNN 模型层中特定的神经元分配给每种情感. 这种方法可以通过引入新的神经元来扩展模型的隐藏层. 如文献 [81] 从扩展的神经元集合中分配特定神经元来表示每种不同的情感. 重要的是只有在处理和相应情感相关的数据时, 这些特定神经元子集的关联权重才会进行调整. 此外, 通过加入专门用于某种特定情感类别的神经元, 模型可以生成具有所需情感类的语音.
这种能力对仅有中性数据的新说话人也成立, 这称为表达/情感移植.

### 3.3 Labels for Emotion Predictors/Classifiers 标签用于情感预测器/分类器

Another common approach to utilize emotion labels is to use them directly or via emotion predictor or classifier to support the process of extracting emotion/prosody embedding.

另一种常见的使用情感标签的方法是直接使用它们或者通过情感预测器/分类器以支持提取情感/韵律嵌入的过程.

For example, in **"End-to-End Emotional Speech Synthesis Using Style Tokens  and Semi-Supervised Training"** emotion labels represented as one-hot vectors are used as targets for the weight vectors of GSTs (explained in Section 4.3) where a cross entropy loss between the two vectors is added to the total loss function.
Yoon et al. [64] proposes a joint emotion predictor based on the Generative Pre-trained Transformer (GPT)-3 [88].
The proposed predictor produces two outputs including emotion class and emotion strength based on features extracted from input text by (GPT)-3.
A joint emotion encoder is then used to encode the predictor outputs into a joint emotion embedding.
The joint emotion predictor is trained with the guidance of the emotion labels and emotion strength values obtained via a <algo>ranking support vector machine (RankSVM)</algo[89].

文献 [026] 中情感标签表示成独热向量, 作为 GSTs 权重向量的目标值, 这两个向量之间的交叉熵损失被添加到总损失函数中;
文献 [64] 提出了基于 GPT-3 的联合情感预测器, 这个预测器基于 GPT-3 从输入文本中提取的特征产生两个输出包括情感类别和情感强度.
联合情感编码器将预测器的输出编码为一个联合情感嵌入.
联合情感预测器在通过 RankSVM 获得的情感标签和情感强度值得指导下进行训练.

In [32], an emotion classifier is used to produce more discriminative emotion embeddings.
Initially, the input Mel-spectrogram features from the reference-style audio and those predicted by the proposed TTS model are passed to two reference encoders (explained in Section 4.1) to generate reference embeddings.
Both embeddings are then fed to two emotion classifiers, which consist of intermediate fully connected (FC) layers.
The output of the second FC layer from both classifiers is considered as the emotion embedding.
Apart from the loss of the classifiers, an additional loss function is established between the resulting emotion embeddings from the two classifiers.
Similarly, an emotion classifier is also employed in [36] to reduce irrelevant information in the generated emotion embedding from an emotion encoder with reference speech (Mel-spectrogram) as input.

文献 [32] 使用情感分类器用于产生更具有区分性得情感嵌入.
首先从参考风格音频的输入梅尔频谱特征和 TTS 模型的相应预测传递到两个参考编码器中以生成参考嵌入.
两个嵌入之后都输入到两个情感分类器中, 由中间全连接层组成.
分类器的第二个全连接层的输出被视为情感嵌入.
除了分类器的损失之外, 还建立了两个分类器产生的情感嵌入结果之间的附加损失函数.
文献 [36] 使用了情感分类器用于减少带有参考语音 (梅尔频谱) 的情感编码器生成的情感嵌入的无关信息.

Several other studies [34] [36] [39] that support multiple speakers also suggest utilizing a speaker classifier in addition to the emotion classifier.
This approach aims to improved the speaker embedding derived from speaker encoders.
Moreover, these studies introduce an adversarial loss between the speaker encoder and the emotion classifier using a gradient reversal layer (GRL) [90].
The purpose of this is to minimize the potential transfer of emotion-related information into the speaker embedding.
The GRL technique involves updating the weights of the speaker encoder by utilizing the inverse of the gradient values obtained from the emotion classifier during the training process.

文献 [34] [36] [39] 支持多说话人的研究也建议使用说话人分类器以及情感分类器.
这一方法旨在改善说话人编码器导出的说话人嵌入.
此外这些研究通过使用一个 GRL 引入了说话人编码器和情感分类器的对抗损失.
目的是最小化情感相关的信息转移到说话人嵌入中.
GRL 技术涉及在训练过程中使用从情感分类器获得的梯度值的逆来更新说话人编码器的权重.


## 4.无监督学习方法

Due to the limited availability and challenges associated with collecting or preparing labeled datasets of expressive speech, as discussed in Section 6, many researchers tend to resort to unsupervised approaches for generating expressive speech. Within these approaches, models are trained to extract speaking styles or emotions from expressive speech data through unsupervised methods.
Unsupervised models typically utilize reference speech as an input to the TTS model, which extracts a style or prosody embedding which is then used to synthesize speech resembling the input style reference. In the literature, three primary structures emerge as baseline models for unsupervised ETTS models: including reference encoders, global style tokens, and variational autoencoders, which are explained in the following three sections. In addition, we identify the recent TTS models that utilize in-context learning as another group of unsupervised approaches. The last subcategory under the unsupervised approaches involves other individual approaches. We then provide a general summary of all the unsupervised approaches reviewed in this work in Table 3.

由于表达性语音的标注数据集的收集或准备相关的有限可用性和挑战, 许多研究人员倾向于采用无监督方法用于生成表达性语音. 在这些方法中, 模型通过无监督方法被训练用于从表达性语音数据中提取说话风格或情感. 无监督模型通常使用参考语音作为输入传递给 TTS 模型, 该模型提取风格或韵律嵌入, 之后用于合成类似于输入风格参考的语音.
现有的文献中出现了三个主要结构作为无监督 ETTS 模型的基线模型: Reference Encoders, Global Style Tokens, VAEs.
此外我们认为近期使用上下文学习的 TTS 模型为其他一组无监督学习方法.最后一个子类别还涉及到其他个别方法.
我们在表格三种提供了本文回顾的所有无监督方法的一般性总结.

|序号|组别|TTS 模型|韵律级别|


### 4.1 Direct Reference Encoding

The main approach, based on a reference or prosody encoder, can be traced back to an early Google paper[74]. The paper suggests using a reference encoder to produce a low-dimensional embedding for a given style reference audio, which is called a prosody embedding.
This encoder takes spectrograms as input to represent the reference audio. The generated prosody embedding is then concatenated with the text embedding derived from the text encoder of a Seq2Seq TTS model such as [Tacotron](../../Models/Acoustic/_PDF/2017.03.29_Tacotron.md), [Tacotron2](../../Models/Acoustic/2017.12.16_Tacotron2.md)ure 6 shows reference encoder integrated to the TTS model.

基于参考或韵律编码器的主要方法可以回溯到 Google 的一篇论文 [74] Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron. 该文献建议使用参考编码器以为给定风格参考音频生成低维嵌入, 称为韵律嵌入. 这一编码器将频谱作为输入来表示参考音频. 生成的韵律嵌入会与 Seq2Seq TTS 模型如 Tacotron 的文本编码器导出的文本嵌入相拼接. 图六展示了参考编码器集成到 TTS 模型.

Various features have been employed in the literature as inputs for the reference encoder. For example, in the work [85], MFCC features extracted using the openSMILE toolkit [139] are fed into one of the encoders within its style extraction model, which is composed of a multi-modal dual recurrent encoder (MDRE). In another study [31], the reference encoder is proposed as a ranking function model, aimed at learning emotion strength at the phoneme level. This model leverages the OpenSMILE toolkit to extract 384-dimensional emotion-related features from segments of reference audio, derived using a forced alignment model for phoneme boundaries. Furthermore, in work [63], a word-level prosody embedding is generated. This is achieved by extracting phoneme-level F0 features from reference speech using the WORLD vocoder [140] and an internal aligner operating with the input text.

文献中已经使用了各种特征作为参考编码器的输入.
- 文献 [85] 使用 OpenSMILE 工具箱提取 MFCC 特征被输入到其风格提取模型的一个编码器中. 该模型有一个多模态对偶循环编码器组成.
- 文献 [31] 参考编码器被作为一个排序函数模型, 旨在音素级别学习情感强度. 这个模型利用 OpenSMILE 工具箱从参考音频片段中提取和情感相关的 384 维特征, 这些片段是对音素边界采用强制对齐模型获得的.
- 文献 [63] 生成了基于单词级别的韵律嵌入. 这是通过使用 WORLD 声码器和与输入问题一起操作的内置对齐器从参考音频中提取音素级别的 F0 特征来实现的.

A prosody-aware module is proposed in [37] which extracts other prosody-related features. The prosody-aware module consists of an encoder, an extractor, and a predictor. The encoder receives the three phoneme-level features including logarithmic fundamental frequency(LF0), intensity, and duration from the extractor as input and generates the paragraph prosody embedding with the assistance of an attention unit. Simultaneously, the predictor is trained to predict these features at inference time based on the input text embedding only.

文献 [37] 提出了韵律感知模块用于提取其他韵律相关特征. 该模块由一个编码器, 一个提取器和一个预测器组成. 编码器接收来自提取器的三个音素级别特征包括对数基频 LF0, 强度和时长作为输入, 并借助注意力单元生成段落韵律嵌入. 同时预测器被训练用于在推理时仅输入文本嵌入来预测这些特征.

In Daft-Exprt TTS model [118], the prosody encoder receives pitch, energy and spectrogram as input. The prosody encoder then uses FiLM conditioning layers[141] to carry out affine transformations to the intermediate features of specific layers in the TTS model. A slightly modified version of the [FastSpeech2](../../Models/Acoustic/2020.06.08_FastSpeech2.mdel is utilized in this work where the phoneme encoder,prosody predictor and the decoder are the conditioned components. The prosody predictor is similar to the variance adaptor of [FastSpeech2](../..../../Models/Acoustic/2020.06.08_FastSpeech2.mdout the length regulator, and it estimates pitch, energy and duration at phoneme-level.

在 Draft-Exprt TTS 模型中, 韵律编码器接受音高, 能量和频谱作为输入. 然后韵律编码器使用 FiLM 条件层对 TTS 模型的特定层的中间特征执行仿射变换. 这项工作使用了 FastSpeech2 的稍微修改版本, 其中音素编码器, 韵律预测器和解码器都是条件化组件. 韵律预测器和 FastSpeech2 的方差适配器相似但没有长度调节器, 且其在音素水平估计音高, 能量和时长.

A pre-trained Wav2Vec model [142] has also been utilized for extracting features from the reference waveform.

文献 [142] 采用预训练 Wav2Vec 模型用于从参考波形中提取特征.

These features serve as input to the reference encoders of the proposed [Emo-VITS]() model, which integrates an emotion network into the VITS model [143] to enhance expressive speech synthesis. In fact, the emotion network in the Emo-VITS model comprises two reference encoders. The resulting emotion embeddings from these encoders are then combined through a feature fusion module that employs an attention mechanism. Wav2vec2.0-derived features from the reference waveform in this work are particularly suitable for attention-based fusion and contribute to reducing the textual content within the resulting embeddings.

Emo-VITS 将这些特征作为 Emo-VITS 的参考编码器的输入, 它将情感网络集成到 VITS 模型中用于增强表达性语音合成. 实际上, 情感网络包含了两个参考编码器. 这些编码器的情感嵌入输出之后通过一个特征融合模块进行结合然后应用注意力机制. 由 Wave2Vec 2.0 从参考音频导出的特征尤其适合基于注意力的融合, 且有助于在结果嵌入中减少文本内容.

In contrast, [60] proposes a an image style transfer module to generate input for reference encoder. The concept of image style transfer involves altering the artistic style of an image from one domain to another while retaining the image’s original content [144]. In specific research, the style reconstruction module from VGG-19[145], a deep neural network primarily used for image classification, is employed to extract style-related information from the Mel-spectrogram used as input image. Subsequently, the output of this module is fed into the reference encoder to generate the style embedding.

文献 [60] 提出了一种图像风格转换模块用于生成参考编码器的输入. 图像风格迁移的概念涉及到图像的艺术风格从一个领域的转化到另一个领域, 同时保留图像原始内容. 具体而言来自 VGG-19 的风格重构模块, 一个主要用于图像分类的神经网络, 将梅尔频谱视为输入图像从中提取和风格相关的信息. 之后这个模块的输出传递到参考编码器中以生成风格嵌入.

### 4.2 Latent Features via Variational Auto‑Encoders 通过 VAE 获取隐特征

The goal of TTS models under this is to map input speech from the higher dimensional space to a well-organized and lower-dimensional latent space utilizing variational auto-encoders (VAEs) [146]. VAE is a generative model that is trained to learn the mapping between observed data x and continuous random vectors z in an unsupervised manner. In detail, VAEs learn a Gaussian distribution denoted as the latent space from which the latent vectors representing the given data x can be sampled. A typical variational autoencoder consists of two components. First, the encoder learns the parameters of the z vectors (latent distribution), namely the mean $\mu(x)$ and variance $\sigma^2(x)$, based on the input data x. Second,the decoder regenerates the input data x based on latent vectors z sampled from the distribution learned by the encoder. In addition to the reconstruction loss between the model input and the data, variational autoencoders are also trained to minimize a latent loss, which ensures that the latent space follows a Gaussian distribution.

在这类 TTS 模型中, 目标是利用 VAE 将来自于高维空间的语音映射到组织良好且维度较低的隐空间. VAE 是一种生成模型, 通过无监督学习的方式学习观察数据 $x$ 和连续随机向量 $z$ 之间的映射. 具体来说, VAE 学习一个记为隐空间的高斯分布, 从中可以采样到能表示给定数据 $x$ 的隐向量. 一个典型的变分自编码器由两部分组成: 一是编码器基于输入数据学习 $z$ 向量即隐分布的参数: 均值和方差, 二是解码器基于解码器学习到的分布中采样的隐变量 $z$ 重新生成输入数据. 除了模型输出和数据之间的重构损失, 变分自编码器还需要最小化一个潜在损失, 使得隐空间服从高斯分布.

Utilizing VAEs in expressive TTS models as shown by Fig. 7, allows for mapping the various speech styles within the given dataset to be encoded as latent vectors,often referred to as prosody vectors, within this latent space. During inference, these latent vectors can be sampled directly or with the guidance of reference audio from the VAE’s latent space. Furthermore, the latent vectors offer the advantage of disentangling prosody features,meaning that some specific dimensions of these vectors independently represent single prosody features such as pitch variation or speaking rate. Disentangled prosody features allow for better prosody control via manipulating the latent vectors with different operations such as interpolation and scaling [77].

在表达性 TTS 中使用 VAEs 如图七所示, 允许将给定数据集中各种语音风格编码为隐变量, 通常称为韵律向量. 在推理时这些隐变量可以从隐空间直接采样或在参考音频的指导下采样. 此外, 隐变量还提供了分离韵律特征的优势, 意味着这些向量某些特定维度独立地表示单个韵律特征, 如音高变化或语速. 分离的韵律特征通过某些操作以进行更好的韵律控制, 如插值和缩放.

The two early papers, [76] [77], can be regarded as the baseline for latent feature-based approaches. The former study [76] introduces VAE within the VoiceLoop model [147], while the latter [77]incorporates VAE into [Tacotron2](../../Models/Acoustic/2017.12.16_Tacotron2.md)n end-to-end TTS model for expressive speech synthesis.

文献 [76] [77] 可以视为基于隐特征方法的基线模型. [76] 将 VAE 引入 VoiceLoop 模型, [77] 将 VAE 集成到 Tacotron2 中作为表达性语音合成的端到端 TTS 模型.

In the same direction of modeling the variation of the prosodic features in expressive speech, studies [109] [110] propose a hierarchical structure for the baseline variational autoencoder, known as Clockwork Hierarchical Variational AutoEncoder (CHiVE). Both the encoder and decoder in the CHiVE model have several layers to capture prosody at different levels based on the input text’s hierarchical structure. Accordingly, linguistic features are also used alongside acoustic features as input to the model’s encoder. The model’s layers are dynamically clocked at specific rates: sentence, words, syllables, and phones. The encoder hierarchy goes from syllables to the sentence level, while the decoder hierarchy is in the reversed order.

表达性语音中建模韵律特征变化的方面, 文献 [109], [110] 为基线 VAE 提出了一个层次结构, 即 CHiVE. 编码器和解码器都有数层, 基于输入文本的层次结构在不同级别捕获韵律. 因此除了声学特征之外, 语言特征也作为模型编码器的输入. 模型的层以特定的速率动态计时: 句子, 单词, 音节和音素. 编码器的层次结构从音节到句子, 而解码器则相反;

The CHiVE-BERT model in [110], differs from the main model in [109] as it utilizes BERT [148] features for input text at the word-level. Since the features extracted by the BERT model incorporate both syntactic and semantic information from a large language model, CHiVE-BERT model is expected to have improved the prosody generation.

ChiVE-BERT 模型使用 BERT 特征作为单词级别的输入文本. 由于 BERT 模型提取的特征包含大型语言模型的语法和语义信息, 所以 CHiVE-BERT 模型预计将提高韵律的生成.

Other studies [DiffProsody](../../Models/E2E/2023.07.31_DiffProsody.md) [53] propose Vector-Quantized Variational Auto-Encoder (VQ-VAE) to achieve discretized latent prosody vectors. In vector quantization(VQ) [149], latent representations are mapped from the prosody latent space to a codebook of a limited number of prosody codes. Specifically, during training, the nearest neighbor lookup algorithm is applied to find the nearest codebook vector to the output of the reference encoder and used to condition TTS decoder.

DiffProsody [53] 提出矢量量化 VAE 用于实现离散的隐韵律向量. 在 VQ 中, 隐表示从韵律潜在空间映射到有限数量的韵律代码的码本. 特别地在训练时, 最近邻查找算法应用于查找参考编码器输出最近的码本向量, 并用于条件 TTS 模型.

To further improve the quality of latent prosody vectors and consequently the expressiveness of the generated speech, [DiffProsody](../../Models/E2E/2023.07.31_DiffProsody.md) proposes a diffusion-based VQ-VAE model.

为了进一步提高隐韵律向量的质量和后续生成语音的表达性, Diff-Prosody 提出了一种基于扩散的 VQ-VAE 模型.

In the proposed model a prosody generator that utilizes a denoising diffusion generative adversarial networks (DDGANs) [150] is trained to generate the prosody latent vectors based only on text and speaker information. At inference time, the prosody generator is used to produce prosody vectors based on input text and with no need for an audio reference which improves both quality and speed of speech synthesis.

在提出的模型中, 一个韵律生成器使用去噪扩散对抗生成模型仅使用文本和说话人信息生成韵律隐变量. 在推理时, 韵律生成器用于基于输入文本的韵律向量而无需音频参考, 从而提升语音合成的质量和速度.

While most of the studies in this category follow the baseline model and use mel-spectrograms to represent the reference audio, other studies extract correlated prosody features as input to the VAE. For instance, frame-level F0, energy, and duration features are extracted from the reference speech as basic input for the hierarchical encoder of the CHiVE model [109]. These same features are also used as input for the VAE encoder in work [35], but at the phoneme level. In work [68], multi-resolution VAEs are employed, each with acoustic and linguistic input vectors. The acoustic feature vectors for each encoder include 70 mel-cepstral coefficients, log F0value, a voiced/unvoiced value, and 35 mel-cepstral analysis aperiodicity measures.

此类的大部分研究都遵循基线模型并使用梅尔频谱表示参考音频, 其他研究提取相关韵律特征作为 VAE 的输入. 例如 帧级别的 F0, 能量, 时长特征从参考音频中提取, 作为 ChiVE 模型层次编码器的输入. 这些相同特征同样在文献 [35] 中使用, 但是是音素级别.

文献 [68] 采用了多分辨率 VAEs, 每个都有声学和语言输入向量. 声学特征包括 70 个梅尔频谱系数, 对数 F0 值, 有声/无声值和 35 个 mel-cepstral 分析非周期度量.

### 4.3 Global Style Tokens

The Global Style Tokens (GST) approach for expressive synthesis was first introduced in [75]. The paper proposes a framework to learn various speaking styles (referred to as style tokens) in an unsupervised manner within an end-to-end TTS model. The proposed approach can be seen as a soft clustering method that learns soft style clusters for expressive styles in an unlabeled dataset. In detail, GST, as shown by Fig. 8, extends the approach introduced in [74] by passing the resulting style embedding from the reference encoder to an attention unit,which functions as a similarity measure between the style embedding and a bank of randomly initialized tokens.
During training, the model learns the style tokens and a set of weights, where each style embedding is generated via a weighted sum of the learned tokens. In fact, the obtained weights represent how each token contributes to the final style embedding. Therefore, each token will represent a single style or a single prosody-related feature, such as pitch, intensity, or speaking rate.
At inference time, a reference audio can be passed to the model to generate its corresponding style embedding via a weighted sum of the style tokens. Alternatively, each individual style token can be used as a style embedding.
In addition, GSTs offer an enhanced control over the speaking style through various operations. These include manual weight refinement, token scaling with different values, or the ability to condition different parts of the input text with distinct style tokens.

用于表达性语音合成的全局风格标记方法在文献 [75] 中被首次提出. 该文献提出了一个框架用于端到端 TTS 模型中以无监督学习的方式用于学习各种说话风格 (称为风格标记). 该方法可以视为一种软聚类方法用于无标签数据集中学习软风格簇. 具体地 GST 如图八所示, 将文献 [74] 的方法进行了扩展, 将参考编码器输出的风格嵌入传递给注意力单元, 作为风格嵌入和一组随机初始标记的相似性度量. 在训练时, 模型学习风格标记和一组权重, 其中每个风格嵌入可以通过学习到的标记进行加权和获得. 实际上, 获得的权重表示每个标记对于最终风格嵌入的贡献. 因此每个标记将表示单个风格或单个韵律相关的特征, 例如音高, 强度或语速. 在推理时, 参考音频传递给模型通过风格标记的加权和用于生成对应风格嵌入. 或者每个单独的风格标记作为风格嵌入.
此外 GSTs 提供了通过各种操作增强对说话风格的控制, 包括手动权重细化, 标记按不同值缩放或使用不同风格标记条件话输入文本的不同部分.

The GST-TTS model can be further enhanced by modeling different levels of prosody to improve both expressiveness and control over the generated speech. For instance, [46] proposes a fine-grained GST-TTS model where word-level GSTs are generated to capture local style variations (WSVs) through a prosody extractor. The WSV extractor consists of a reference encoder and a style token layer, as described in [75], along with an attention unit to produce the word-level style token.

In [133] a hierarchical structure of multi-layer GSTs with residuals is proposed. The model employs three GST layers, each with 10 tokens, resulting in a better interpretation of the tokens of each level. Upon tokens analysis, it was found that the first-layer tokens learned speaker representations, while the second-layer tokens captured various speaking style features such as pause position, duration, and stress. The third-layer tokens, however, were able to generate higher-quality samples with more distinct and interpretable styles. Similarly, in[50], a multi-scale GST extractor is proposed to extract speaking style at different levels. This extractor extracts style embeddings from the reference mel-spectrogram using three style encoders at global, sentence, and sub word levels, and combines their outputs to form the multi-scale style embedding.

GST-TTS 模型能够通过建模不同级别的韵律来提升生成语音的表达性和控制.

- 文献 [46] 提出了细粒度 GST-TTS 模型, 通过韵律提取器生成了词级别的 GSTs 以捕获局部风格变化 (WSVs). WSV 提取器由参考编码器和风格标记层组成, 如文献 [75] 所述, 以及用于生成单词级别的注意力单元.
- 文献 [133] 具有残差的多层 GSTs 的层次结构, 应用三层 GST 层, 每个有 10 个标记, 从而得到各个级别标记的更佳解释. 标记分析发现第一层标记学习到说话人表示, 第二层捕获了各种说话风格例如停顿位置, 时长和强调. 第三层标题能够生成高质量的样本, 具有更明显和可解释的风格.
- 类似地 [50], 一个多尺度 GST 提取器用于提取不同级别的说话风格, 该提取器使用三种风格编码器按全局, 句子和单词级别从参考梅尔频谱中提取风格嵌入, 并将它们的输出结合以形成多尺度的风格嵌入.

With only a small portion of the training dataset labeled with emotions, **"End-to-End Emotional Speech Synthesis Using Style Tokens  and Semi-Supervised Training"** proposes a semi-supervised GST model for generating emotional speech. The model applies a cross-entropy loss between the one-hot vectors representing the emotion labels and the weights of GSTs,in addition to the GST-TTS reconstruction loss. The semi-GST model is trained on a dataset in which only 5%of the samples are labeled with emotion classes, while the rest of the dataset is unlabeled. After training, each style token represents a specific emotion class from the training dataset and can be used to generate speech in the corresponding emotion.

当训练集只有一小部分带有情感标签时, 文献 [026] 提出了一种半监督的 GST 模型用于生成情感语音. 该模型应用表示情感标签的独热编码和 GSTs 的权重之间的交叉熵损失, 以及 GST-TTS 重构损失. 这个 semi-GST 模型在训练集只有 5% 的样本具有情感类别的情况下训练, 训练后每个风格标记表示训练集中一个特定的情感类别并且能用于对应情感生成语音.

Furthermore, in [92], a speech emotion recognition(SER) model is proposed with the GST-TTS to generate emotional speech while acquiring only a small labeled dataset for training. The paper formulates the training process as reinforcement learning (RL). In this frame-work, the GST-TTS model is treated as the agent, and its parameters serve as the policy. The policy aims to predict the emotional acoustic features at each time step, where these features represent the actions. The pre-trained SER model then provides feedback on the predicted features through emotion recognition accuracy, which represents the reward. The policy gradient strategy is employed to perform backpropagation and optimize the TTS model to achieve the maximum reward.

文献 [92] 一个语音情感识别 SER 模型被提出和 GST-TTS 模型结合用于生成情感语音, 只需要很小部分带标签的数据集用于训练. 该文献将训练过程形式化为强化学习. 在此架构下, GST-TTS 模型视为智能体, 其参数作为策略. 策略旨在预测每个时间步的情感声学特征, 这些特征表示动作. 预训练 SER 模型通过情感识别精度提供反馈, 即奖励. 策略梯度策略用于优化 TTS 模型以达到最大奖励.

In contrast, the Mellotron model [114] introduces a unique structure for the GSTs, enabling Mellotron to generate speech in various styles, including singing styles, based on pitch and duration information extracted from the reference audio. This is achieved by obtaining a set of explicit and latent variables from the reference audio. Explicit variables (text, speaker, and F0contour) capture explicit audio information, while latent variables (style tokens and attention maps) capture the latent characteristics of speech that are hard to extract explicitly.

文献 [114] Mellotron 引入 GSTs 独特结构, 使得 Mellotron 能够基于从参考音频中提取的音高和时长信息生成各种风格的语音, 包括歌唱风格. 这通过从参考音频中获取显式和隐式变量实现. 显式变量捕获显式音频信息, 隐式变量捕获语音中难以显式提取的隐藏特征.

### 4.4.基于上下文学习的方法

These is a group of recent TTS models that are trained on a large amounts of data using in-context learning strategy. During in-context learning (also called prompt engineering), the model is trained to predict missing data based its context. In other words, the model is trained with a list of input-output pairs formed in a way that represents the in-context learning task. After training, the model should be able to predict the output based on a given input.

近期有一组 TTS 模型通过上下文学习策略在大量数据上进行训练. 在上下文学习 (或提示工程) 中, 模型被训练用于基于上下文预测缺失数据. 换句话说模型通过表示上下文学习任务的输入输出对列表进行训练, 训练后模型能够预测给定输入的输出.

For the TTS task, the provided style reference (referred to as prompt) is considered as part of the entire utterance to be synthesized. The TTS model training task is to generate the rest of this utterance following the style of the provided prompt as shown by Fig. 9. By employing this training strategy, recent TTS models such as [VALL-E (2023)](../../Models/SpeechLM/ST2S/2023.01.05_VALL-E.md-E.md), [NaturalSpeech2 (2022)](../../Models/Diffusion/2023.04.18_NaturalSpeech2.md), and [Voicebox (2023)](../../Models/SpeechLM/2023.06.23_VoiceBox.md) are capable of producing zero-shot speech synthesis using only a single acoustic prompt. Furthermore, these models demonstrate the ability to replicate speech style/emotion from a provided prompt ([NaturalSpeech2 (2022)](../../Models/Diffusion/2023.04.18_NaturalSpeech2.md), [VALL-E (20../../Models/SpeechLM/ST2S/2023.01.05_VALL-E.md_VALL-E.md)) or reference ([Voicebox (2023)](../../Models/SpeechLM/2023.06.23_VoiceBox.md)) to the synthesized speech.

对于 TTS 任务, 提供的风格参考 (即提示) 被考虑为要合成的整个发言的一部分. TTS 模型训练任务即遵循提示的风格生成这个发言剩下的部分. 通过应用这种训练策略, 近期 TTS 模型例如 VALL-E, NaturalSpeech2 和 VoiceBox 能够使用单个声学提示进行零次语音合成. 此外, 这些模型说明了从提供的提示或参考复制语音风格/情感到合成语音的能力.

In [VALL-E (2023)](../../Models/SpeechLM/ST2S/2023.01.05_VALL-E.md-E.md), a language model is trained on tokens from [Encodec (2022)](../../Models/SpeechCodec/2022.10.24_EnCodec.md), and the input text is used to condi-tion the language model. Specifically, the Encodec model tokenizes audio frames into discrete latent vectors/codes,where each audio frame is encoded with eight codebooks.
VALL-E employs two main models: the first one is an auto-regressive (AR) model that predicts the first code of each frame, and the second is non-auto-regressive (NAR)model that predicts the other seven codes of the frame.

VALL-E 一个语言模型在 Encodec 的标记上训练, 且输入文本用于条件化语言模型. 特别地, Encodec 模型将音频帧离散化为离散的隐向量/代码, 每个音频帧由八个码本进行编码. VALL-E 应用两个主要模型一个是自回归模型能够预测每帧的第一个编码, 第二个是非自回归模型用于预测其他七个编码.

Instead of discrete tokens used in VALL-E, [NaturalSpeech2 (2022)](../../Models/Diffusion/2023.04.18_NaturalSpeech2.md) represents speech as latent vectors from a neural audio codec with residual vector quantizers. The latent vectors are then predicted via a diffusion model,conditioned on input text, pitch from a pitch predictor,and input speech prompt.
和 VALL-E 不同, NaturalSpeech 2 使用具有残差向量量化的神经音频编解码器将语音表示为隐向量. 隐向量之后通过扩散模型进行预测, 根据输入问题, 音高预测器的音高和输入语音提示进行条件化.

Another example of in-context training is [Voicebox (2023)](../../Models/SpeechLM/2023.06.23_VoiceBox.md) which is a versatile generative model for speech trained on a large amount of multilingual speech data. The model is trained on a text-guided speech infilling task, which gives it the flexibility to perform various speech tasks such as zero-shot TTS, noise removal, content editing,and diverse speech sampling. Voicebox is modeled as a non-autoregressive (NAR) flow-matching model with the ability to consider future context.

另一个基于上下文训练的例子是 Voicebox, 是一个在大量多语言语音数据上训练的语音的通用生成模型. 该模型在文本引导的语音填充任务上进行训练, 这使其能够执行各种语音任务, 例如零次 TTS, 去噪, 内容编辑和多样化的语音采样. Voicebox 被建模为一个非自回归的流匹配模型, 能够考虑未来上下文.


### 4.5 Other Approaches 其他方法

This category containes reviewed papers that propose individual techniques or methods which cannot be categorized under any of the previously mentioned unsupervised approaches.

这个类别包含了一些不能归类为之前提到的任何无监督方法的单独技术和方法的总结.

For instance, in [121], a neural encoder is introduced to encode the residual error between the predictions of a trained average TTS model and the ground truth speech. The encoded error is then used as a style embedding that conditions the decoder of the TTS model to guide the synthesis process.

文献 [121] 引入神经编码器来编码一个训练好的平均 TTS 模型预测和真实语音之间的残差误差. 然后编码的误差被用作风格嵌入用于条件化 TTS 模型的解码器以指导合成过程.

Raitio and Seshadri [128] improves prosody modeling of [FastSpeech2](../../Models/Acoustic/2020.06.08_FastSpeech2.mdel with an additional variance adaptor for utterance-wise prosody modeling.

文献 [128] 通过使用额外的方差适配器用于语调韵律建模, 以提升 FastSpeech2 模型的韵律建模.

As context information is strongly related to speech expressivity, [45] proposes using multiple self-attention layers in [Tacotron2](../../Models/Acoustic/2017.12.16_Tacotron2.md)der to better capture the con-text information in the input text. The outputs of these layers in the encoder are combined through either direct aggregation (concatenation) or weighted aggregation using a multi-head attention layer.

由于上下文信息和语音表达性强相关, 文献 [45] 在 Tacotron 编码器中使用多个自注意力层用于更好地捕获输入文本的内容信息. 这些层的输出通过直接聚合 (拼接) 或加权聚合 (多头注意力层) 进行结合.

Additionally, there are some papers that propose using only input text to obtain prosody-related representations/embeddings without any style references, and those are further discussed in Section 5.2.4.

此外, 有些文献提出只使用输入文本用于获得韵律相关表示/嵌入, 无需风格参考, 这在后续的 5.2.4 中进行讨论.

## Sec.09: Conclusions·结论

This paper presents the findings of our systematic literature review on expressive speech synthesis over the past 5 years.
The main contribution of this article is the development of a comprehensive taxonomy for DL-based approaches published in this field during that specific time frame.
The approaches are classified into three primary categories based on the learning method, followed by models within each category.
Further subcategories are identified at the lower levels of the taxonomy, considering the methods and structures applied to achieve expressiveness in synthesized speech.
In addition to the ETTS approaches taxonomy, we provide descriptions of the main challenges in the ETTS field and proposed solutions from the literature.
Furthermore, we support the reader with brief summaries of ETTS datasets, performance evaluation metrics, and some open-source implementations.
The significance of our work lies in its potential to serve as an extensive overview of the research conducted in this area from different aspects, benefiting both experienced researchers and newcomers in this active research domain.

本文介绍了我们对过去 5 年内表现力语音合成领域的系统性文献综述的研究结果.
本文的主要贡献是开发了一个全面的分类法, 用于描述在该特定时间段内发表的基于深度学习（DL）的方法.
这些方法根据学习方式被分为三个主要类别, 每个类别下又包含具体的模型.
在分类法的较低层次, 进一步确定了子类别, 这些子类别考虑了实现合成语音表现力的方法和结构.
除了ETTS方法的分类法之外, 我们还描述了ETTS领域的主要挑战以及文献中提出的解决方案.
此外, 我们为读者提供了ETTS数据集, 性能评估指标以及一些开源实现的简要概述.
我们工作的意义在于, 它有可能作为一个全面的概述, 从不同方面展示该领域的研究, 对经验丰富的研究人员和活跃研究领域的新手都有益处.

Some main directions for future work in this area involve collection of large expressive datasets in different languages, going from acted expressive style to realistic style.
Further evaluation metrics are still needed in this area for assessing models’ performance such as evaluation of prosody controllability.
Efficient metrics are also required for monitoring performance and guiding loss evaluation during the training process.
These need to be lightweight and fast in order not to slow down training but still reliable.
Another suggestion for future investigations is to take cultural differences in perception of expressions into account for multi-language, multi-speaker expressive TTS applications.
Moreover, as speech is just one modality for expressions, multi-modal approaches that combine facial expressions, eye movements, body movements, gestures, non-verbal clues, etc., will be required to reach human-level expressiveness.
Training several modalities together could be beneficial as the model can transfer useful information from one modality to another in a self-supervised fashion.

该领域未来工作的一些主要方向包括收集不同语言的大型表现力数据集, 从表演风格过渡到真实风格.
该领域仍然需要进一步的评估指标来评估模型的性能, 例如韵律可控性的评估.
还需要有效的指标来监控性能并在训练过程中指导损失评估.
这些指标需要轻量级且快速, 以免减慢训练速度, 但仍然可靠.
未来研究的另一个建议是考虑多语言, 多说话者表现力TTS应用中对表达感知的文化差异.
此外, 由于语音只是表达的一种方式, 因此需要多模态方法, 这些方法结合了面部表情, 眼神移动, 身体动作, 手势, 非言语线索等, 以达到人类水平的表现力.
同时训练多个模态可能是有益的, 因为模型可以在自我监督的方式下将一个模态的有用信息传递给另一个模态.