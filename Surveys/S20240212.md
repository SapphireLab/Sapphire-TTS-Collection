# Deep Learning-Based Expressive Speech Synthesis: A Systematic Review of Approaches, Challenges, and Resources

- 标题: Deep Learning-Based Expressive Speech Synthesis: A Systematic Review of Approaches, Challenges, and Resources
- 作者:
  - Huda Barakat
  - Oytun Turk
  - Cenk Demiroglu
- 发表:
  - EURASIP Journal on Audio, Speech, and Music Processing (2024)
  - DOI: https://doi.org/10.1186/s13636-024-00329-7


## Abstract·摘要

Speech synthesis has made significant strides thanks to the transition from machine learning to deep learning models.
Contemporary **Text-To-Speech (TTS)** models possess the capability to generate speech of exceptionally high quality, closely mimicking human speech.
Nevertheless, given the wide array of applications now employing TTS models, mere high-quality speech generation is no longer sufficient.
Present-day TTS models must also excel at producing expressive speech that can convey various speaking styles and emotions, akin to human speech.
Consequently,researchers have concentrated their efforts on developing more efficient models for expressive speech synthesis in recent years.
This paper presents a systematic review of the literature on expressive speech synthesis models published within the last 5 years, with a particular emphasis on approaches based on deep learning.
We offer a comprehensive classification scheme for these models and provide concise descriptions of models falling into each category.
Additionally, we summarize the principal challenges encountered in this research domain and outline the strategies employed to tackle these challenges as documented in the literature.
In the Section 8, we pinpoint some research gaps in this field that necessitate further exploration.
Our objective with this work is to give an all-encompassing overview of this hot research area to offer guidance to interested researchers and future endeavors in this field.

语音合成技术因从机器学习到深度学习模型的转变而取得了显著进步.
当代的文本转语音模型能够生成质量极高的, 几乎媲美人类的语音.
但是鉴于现在有众多应用正在采用文本转语音模型, 仅仅生成高质量的语音已经不再足够.
现今的文本转语音模型还必须擅长生成富有表现力的语音, 能够传达类似于人类语音的多种说话风格和情感.
因此, 近年来研究人员集中精力于开发更高效的富有表现力的语音合成模型.

本文对过去五年内发表的关于富有表现力的语音合成模型的文献进行了系统性的回顾, 特别关注基于深度学习的方法.
我们为这些模型提供了一个全面的分类方案, 并对每个类别中的模型进行了简要的描述.
此外, 我们总结了在这一研究领域遇到的主要挑战, 并概述了文献中记录的应对这些挑战的策略.

在第 8 节中, 我们指出了该领域中需要进一步探索的一些研究空白.

我们进行这项工作的目的是为这一热门研究领域提供一个全面的概述, 为对这一领域感兴趣的研究人员和未来研究提供指导.

文章内容:
- [ ] [Sec.01](Sec.01.md)
- [ ] [Sec.02](Sec.02.md)
- [ ] [Sec.03](Sec.03.md)
- [ ] [Sec.04](Sec.04.md)
- [ ] [Sec.05](Sec.05.md)
- [x] [Sec.06](Sec.06.md)
- [x] [Sec.07](Sec.07.md)
- [ ] [Sec.08](Sec.08.md)

## Section 01: Introduction

Since the late 1950s, computer-based text-to-speech systems (TTS) have undergone significant advancements, culminating in the production of models that generate speech almost indistinguishable from that of a human. This progress has followed a path consisting of several stages, beginning with conventional methods named as concatenative synthesis and progressing to more advanced approaches known as **Statistical Parametric Speech Synthesis (SPSS)**. Advanced approaches are mainly based on machine learning algorithms like <algo>hidden Markov models (HMMs)</algo> and <algo>gaussian mixture models (GMMs)</algo>. Despite this progress, speech generated by these methods was still noticeably artificial. However, the emergence of deep learning (DL) as a new branch under machine learning (ML) in 2006 has led to significant improvements. Speech synthesis researchers,like many in other research fields, started incorporating deep neural networks (DNN) in their models. Initially,DNNs replaced HMMs and GMMs in SPSS models while the main structure still follows the primary framework of SPSS models as shown in Fig. 1. As discussed in **"Statistical Parametric Speech Synthesis Using Deep Neural Networks"**, the deep learning-based models have overcome many limitations and problems associated with machine learning-based models.

Researchers continue to aim for improved speech quality and more human-like speech despite past advancements. Additionally, they seek to simplify the framework of the text-to-speech models due to the intricate nature of the SPSS structure, which limits progress in this field to those with extensive linguistic knowledge and expertise. Deep learning advancements have brought about the simple encoder-decoder structure for TTS models as <algo>sequence-to-sequence (Seq2Seq)</algo> approaches. The pro-posed approaches have simplified the structure of conventional TTS with multiple components into training a single network that converts a set of input text characters/phonemes into a set of acoustic features (mel-spectrograms). A main concern in these advanced TTS models is the mapping process between the input and output sequences, which is a one-to-many problem, as the single input text can have multiple speech variations as output. In fact, there are two groups of recent TTS models, as shown in Fig.02. The first group generates mel-spectrograms in a sequential (autoregressive) manner using soft and automatic attention alignments between input and output sequences, such as the Tacotron model ([Tacotron (2017)](../../Models/Acoustic/_PDF/2017.03.29_Tacotron.md) [Tacotron2 (2017)](../../Models/Acoustic/2017.12.16_Tacotron2.mdhe second group utilizes hard alignments between the phonemes/characters and mel-spectro-grams, and thus its speech generation process is parallel(non-autoregressive), as in the FastSpeech model ([FastSpeech (2019)](../../Models/Acoustic/2019.05.22_FastSpeech.mdstSpeech2](../../Models/TTS2_Acoustic/2020.06.08_FastSpeech2.md)).This improvement in the structure of the TTS model has encouraged rapid development in the field within the last few years, during which the proposed models produced speech that is nearly indistinguishable from human speech.

Human speech is highly expressive and reflects various factors, such as the speaker’s identity, emotion, and speaking style. In addition, there are many applications in which speech synthesis can be utilized, especially expressive speech synthesis. For instance, audiobooks and podcast applications that create audio versions of eBooks and podcasts, translation applications which provide real-time translation of foreign language text, dubbing applications that generate an alternative audio track for a video with different content, speaker, or language, and content creation applications which help produce audio versions of textual content, such as blogs and news articles. E-learning applications that allow for adding voice-over audio to e-learning courses, and conversational AI applications enable machines to communicate with users in a human-like manner, such as AI chatbots and virtual assistants.

As spoken language is a crucial component in such applications, users must feel as if they are communicating with a real human rather than a machine. Therefore, the speech generated by these applications should convey appropriate emotion, intonation, stress, and speaking style to match the ongoing conversation or the content type and context of the text being read.

As a result, there has been a recent attention towards building efficient expressive speech synthesis models as another step forward in achieving human-like speech. Therefore, many studies have been devoted to expressive speech synthesis (ETTS) as a hot research area, particularly over the last 5 years. In this work, we present the findings of our systematic literature review on ETTS field from the past 5 years. Firstly, we suggest a classification schema of deep learning-based ETTS models that are proposed during this period, based on structures,and learning methods followed in each study. A summary is then provided for each category in the classification schema and main papers related to this category. After that, we outline the main challenges in the ETTS area and solutions that have been proposed to solve them from literature. Finally, we conclude with a discussion of the implications of our work and a highlight of some gaps that require further research in this area.

During our work on this review of expressive speech synthesis literature, we came across several review papers that focus on different stages of development in the speech synthesis field. The majority of these reviews concentrate on DL-based TTS approaches, while only a few papers cover recent TTS approaches in addition to early conventional ones. However, to the best of our knowledge, there are no review papers that cover the fast growth in the (expressive) speech synthesis area, especially in the last few years. Therefore, our main goal in this review is to provide an overview of research trends, techniques, and challenges in this area during this period. We hope that our work will offer researchers a comprehensive understanding of how and what has been accomplished in this field and the gaps that need to be filled as guidance for their future efforts.

While we were writing this paper, we came across an interesting recent review paper **"An Overview of Affective Speech Synthesis and Conversion in the Deep Learning Era"** that is similar to our work. However, the review covers emotional speech synthesis (ESS) as a sub-field of voice transformation while our work is more comprehensive as a systematic literature review that discusses approaches,challenges, and resources. Furthermore, the taxonomy we provide for the reviewed approaches differs from the one given in as elaborated in the next section.

The remaining sections of this paper are structured as follows: Section 2 provides an explanation of the methodology employed for conducting this review. Sections 3and 4 describe the different main and sub-categories of the proposed classification schema for DL-based expressive TTS models. Main challenges facing ETTS models and how they have been tackled in the literature are covered in Section 5. We then give a brief description of ETTS datasets and applied evaluation metrics in Sections 6 and 7, respectively. Finally, Section 8 concludes the paper.

## Sec.09: Conclusions·结论

This paper presents the findings of our systematic literature review on expressive speech synthesis over the past 5 years.
The main contribution of this article is the development of a comprehensive taxonomy for DL-based approaches published in this field during that specific time frame.
The approaches are classified into three primary categories based on the learning method, followed by models within each category.
Further subcategories are identified at the lower levels of the taxonomy, considering the methods and structures applied to achieve expressiveness in synthesized speech.
In addition to the ETTS approaches taxonomy, we provide descriptions of the main challenges in the ETTS field and proposed solutions from the literature.
Furthermore, we support the reader with brief summaries of ETTS datasets, performance evaluation metrics, and some open-source implementations.
The significance of our work lies in its potential to serve as an extensive overview of the research conducted in this area from different aspects, benefiting both experienced researchers and newcomers in this active research domain.

本文介绍了我们对过去 5 年内表现力语音合成领域的系统性文献综述的研究结果.
本文的主要贡献是开发了一个全面的分类法, 用于描述在该特定时间段内发表的基于深度学习（DL）的方法.
这些方法根据学习方式被分为三个主要类别, 每个类别下又包含具体的模型.
在分类法的较低层次, 进一步确定了子类别, 这些子类别考虑了实现合成语音表现力的方法和结构.
除了ETTS方法的分类法之外, 我们还描述了ETTS领域的主要挑战以及文献中提出的解决方案.
此外, 我们为读者提供了ETTS数据集, 性能评估指标以及一些开源实现的简要概述.
我们工作的意义在于, 它有可能作为一个全面的概述, 从不同方面展示该领域的研究, 对经验丰富的研究人员和活跃研究领域的新手都有益处.

Some main directions for future work in this area involve collection of large expressive datasets in different languages, going from acted expressive style to realistic style.
Further evaluation metrics are still needed in this area for assessing models’ performance such as evaluation of prosody controllability.
Efficient metrics are also required for monitoring performance and guiding loss evaluation during the training process.
These need to be lightweight and fast in order not to slow down training but still reliable.
Another suggestion for future investigations is to take cultural differences in perception of expressions into account for multi-language, multi-speaker expressive TTS applications.
Moreover, as speech is just one modality for expressions, multi-modal approaches that combine facial expressions, eye movements, body movements, gestures, non-verbal clues, etc., will be required to reach human-level expressiveness.
Training several modalities together could be beneficial as the model can transfer useful information from one modality to another in a self-supervised fashion.

该领域未来工作的一些主要方向包括收集不同语言的大型表现力数据集, 从表演风格过渡到真实风格.
该领域仍然需要进一步的评估指标来评估模型的性能, 例如韵律可控性的评估.
还需要有效的指标来监控性能并在训练过程中指导损失评估.
这些指标需要轻量级且快速, 以免减慢训练速度, 但仍然可靠.
未来研究的另一个建议是考虑多语言, 多说话者表现力TTS应用中对表达感知的文化差异.
此外, 由于语音只是表达的一种方式, 因此需要多模态方法, 这些方法结合了面部表情, 眼神移动, 身体动作, 手势, 非言语线索等, 以达到人类水平的表现力.
同时训练多个模态可能是有益的, 因为模型可以在自我监督的方式下将一个模态的有用信息传递给另一个模态.