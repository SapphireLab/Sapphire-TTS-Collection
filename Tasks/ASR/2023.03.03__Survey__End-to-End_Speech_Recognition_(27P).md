# End-to-End Speech Recognition: A Survey

<details>
<summary>基本信息</summary>

- 标题: "End-to-End Speech Recognition: A Survey"
- 作者:
  - 01 Rohit Prabhavalkar
  - 02 Takaaki Hori
  - 03 Tara N.Sainath
  - 04 Ralf Schluter
  - 05 Shinji Watanabe
- 链接:
  - [ArXiv](https://arxiv.org/abs/2303.03329v1)
  - [Publication](https://doi.org/10.1109/TASLP.2023.3328283)
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv](PDF/2023.03.03__2303.03329v1__Survey__End-to-End_Speech_Recognition__A_Survey.pdf)
  - [Publication](PDF/2023.02.21__2303.03329p0__Survey__End-to-End_Speech_Recognition__A_Survey_TASLP2023.pdf)

</details>

## Abstract: 摘要

<table>
<tr>
<td>

In the last decade of **automatic speech recognition (ASR)** research, the introduction of deep learning has brought considerable reductions in **word error rate** of more than 50% relative, compared to modeling without deep learning.
In the wake of this transition, a number of all-neural ASR architectures have been introduced.
These so-called **end-to-end (E2E)** models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, with lower dependence on ASR domain-specific experience.
The success and enthusiastic adoption of deep learning, accompanied by more generic model architectures has led to E2E models now becoming the prominent ASR approach.
The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relationship to classical **hidden Markov model (HMM)** based ASR architectures.
All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, discussions of performance and deployment opportunities, as well as an outlook into potential future developments.

</td>
<td>

在过去十年的**自动语音识别 (Automatic Speech Recognition, ASR)** 研究中, 深度学习的引入使得**词错误率 (Word Error Rate, WER)** 相比非深度学习模型相对降低了超过 50%.
随着这一转变, 许多全神经 ASR 架构被提出.
这些所谓的**端到端 (End-to-End, E2E)** 模型提供了高度集成, 完全神经网络化的 ASR 模型, 它高度依赖于通用的机器学习知识, 从数据中更一致地学习, 并且对 ASR 领域特定经验的依赖更低.
深度学习的成功和狂热应用, 以及更通用模型架构的出现, 已经使得 E2E 模型现已成为主流 ASR 方法.
本综述的目标是提供 E2E ASR 模型的分类体系和对应改进, 并讨论它们的性质和经典的基于**隐马尔可夫模型 (Hidden Markov Model, HMM)** 的 ASR 模型架构之间的关系.
本工作涵盖了 E2E ASR 的所有相关方面: 建模, 训练, 解码, 外部语言模型集成, 对性能和部署机遇的讨论, 以及潜在的未来发展方向的展望.

</td>
</tr>
</table>

## 1·Introduction: 引言

<table>
<tr>
<td>

The classical statistical architecture decomposes an ASR system into four main components: acoustic feature extraction from speech audio signals, acoustic modeling, language modeling and search based on Bayes decision rule \cite{Bayes:1763,jelinek1997statistical}.
The term "classical" here refers to the former long-term state-of-the-art ASR architecture based on the decomposition into acoustic and language model and with acoustic modeling based on hidden Markov models.
Classical acoustic modeling is based on hidden Markov models (HMM) to account for speaking rate variation.
Within the classical approach, deep learning has been introduced to acoustic and language modeling.
In acoustic modeling, deep learning replaced Gaussian mixture distributions (hybrid HMM \cite{Bourlard+Morgan:1993,Seide+:2011}) or augmented the acoustic feature set (nonlinear disciminant/tandem approach \cite{Fontaine+:1997,Hermansky+:2000}).
In language modeling, deep learning replaced count-based approaches \cite{Nakamura+:1989,Bengio+:2000,Schwenk+Gauvain:2002}.
However, when introducing deep learning, the classical ASR architecture was not yet touched.
Classical state-of-the-art ASR systems today are composed of many separate components and knowledge sources, especially speech signal preprocessing, methods for robustness w.r.t.\ recording conditions, phoneme inventories and pronunciation lexica, phonetic clustering, handling of out-of-vocabulary words, various methods for adaptation/normalization, elaborate training schedules with different objectives and incl.\ sequence discriminative training, etc.
The potential of deep learning on the other hand initiated successful approaches to integrate formerly separate modeling steps, e.g.\ integrating speech signal preprocessing and feature extraction into acoustic modeling \cite{Tuske+:2014,Sainath+:2015}.

</td>
<td>

</td>
</tr>

<tr>
<td>

More consequently, the introduction of deep learning to ASR also initiated research to replace classical ASR architectures based on hidden Markov models (HMM) with more integrated joint neural network model structures \cite{graves2006connectionist,graves2012sequence,chorowski2015attention,chan2016listen}.
These ventures might be seen as trading specific speech processing models for more generic machine learning approaches to sequence-to-sequence processing, maybe in a similar way as statistical approaches to natural language processing used to replace more linguistically oriented models.
For these all-neural approaches recently the term \emph{end-to-end} (\EtoE) \cite{Liang+:2006,Collobert+:2011,graves2012sequence,GravesJaitly14} has been established.
Therefore, first of all an attempt to defining the term \emph{end-to-end} in the context of ASR is due in this survey.
According to the Cambridge Dictionary, the adjective "end-to-end" is defined by: "including all the stages of a process" \cite{CambridgeDictEndToEnd}.
We therefore propose the following definition of end-to-end ASR: an integrated ASR model that enables joint training from scratch; avoids separately obtained knowledge sources; and, provides single-pass recognition consistent with the objective to optimize the task-specific evaluation measure, i.e., usually label (word, character, subword, etc.) error rate.
While this definition suffices for the present discussion, we note that such an idealized definition hides many nuances involved in the term E2E and lacks distinctiveness; we elaborate on some of these nuances in Section II to discuss the various connotations of the term E2E in the context of ASR.

</td>
<td>

</td>
</tr>

<tr>
<td>

What are potential benefits of E2E approaches to ASR? The primary objective when developing an ASR systems is to minimize the expected word error rate; secondary objectives are to reduce time and memory complexity of the resulting decoder, and – assuming a constrained development budget – genericity, and ease of modeling.
First of all, an integrated ASR system, defined in terms of a single neural network structure supports genericity of modeling and may allow for faster development cycles when building ASR systems for new languages or domains.
Similarly, ASR models defined by a single neural network structure may become more ‘lean’ compared to classical modeling, with a simpler decoding process, obviating the need to integrate separate models.
The resulting reduction in memory footprint and power consumption supports embedded ASR applications [21], [22].
Furthermore, end-to-end joint training may help to avoid spurious optima from intermediate training stages.
Avoiding secondary knowledge sources like pronunciation lexica may be helpful for languages/domains where such resources are not easily available.
Also, secondary knowledge sources may themselves be erroneous; avoiding these may improve models trained directly from data, provided that sufficient amounts of task-specific training data are available.

</td>
<td>

</td>
</tr>

<tr>
<td>

With the current surge of interest in E2E ASR models and an increasing diversity of corresponding work, the authors of this review think it is time to provide an overview of this rapidly evolving domain of research.
The goal of this survey is to provide an in-depth overview of the current state of research on E2E ASR systems, covering all relevant aspects of E2E ASR, with a contrastive discussion of the different E2E and classical ASR architectures.

</td>
<td>

</td>
</tr>

<tr>
<td>

This survey of E2E speech recognition is structured as follows.
Section II discusses the nuances in the term E2E as it applies to ASR.
Section III describes the historical evolution of E2E speech recognition, with specific focus on the input-output alignment and an overview of prominent E2E ASR models.
Section IV discusses improvements of the basic E2E models, including E2E model combination, training loss functions, context, encoder/decoder structures and endpointing.
Section V provides an overview of E2E ASR model training.
Decoding algorithms for the different E2E approaches are discussed in Section VI.
Section VII discusses the role and integration of (separate) language models in E2E ASR.
Section VIII reviews experimental comparisons of the different E2E as well as classical ASR approaches.
Section IX provides an overview of applications of E2E ASR.
Section X investigates future directions of E2E research in ASR, before concluding in Section XI.
Finally, we note that this survey paper also includes comparative discussions between novel E2E models and classical HMM-based ASR approaches in terms of various aspects; most sections end with a summarization of the relationship between E2E models and HMM-based ASR approaches in relation to the topics covered within the respective sections.

</td>
<td>

</td>
</tr>
</table>

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论