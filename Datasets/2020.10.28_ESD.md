# ESD (Emotional Speech Dataset)

<details>
<summary>基本信息</summary>

- 标题: Seen and Unseen emotional style transfer for voice conversion with a new emotional speech dataset
- 作者:
  1. Kun Zhou, 
  2. Berrak Sisman, 
  3. Rui Liu, 
  4. Haizhou Li
- 机构:
  1.
- 时间:
  - 预印时间: 2020.10.28 ArXiv v1
  - 预印时间: 2021.02.11 ArXiv v2
  - 更新笔记: 2024.09.16
- 发表:
  - ICASSP 2021
- 链接:
  - [ArXiv](https://arxiv.org/abs/2010.14794)
  - [DOI](https://doi.org/10.1109/ICASSP39728.2021.9413391)
  - [Github]()
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=14762481511534763494)
- 标签:
  - ?
- 页数: ?
- 引用: ?
- 被引: 195
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Emotional voice conversion aims to transform emotional prosody in speech while preserving the linguistic content and speaker identity. 
> Prior studies show that it is possible to disentangle emotional prosody using an encoder-decoder network conditioned on discrete representation, such as one-hot emotion labels. 
> Such networks learn to remember a fixed set of emotional styles. 
> In this paper, we propose a novel framework based on variational auto-encoding Wasserstein generative adversarial network (VAW-GAN), which makes use of a pre-trained speech emotion recognition (SER) model to transfer emotional style during training and at run-time inference. 
> In this way, the network is able to transfer both seen and unseen emotional style to a new utterance. 
> We show that the proposed framework achieves remarkable performance by consistently outperforming the baseline framework. 
> This paper also marks the release of an emotional speech dataset (ESD) for voice conversion, which has multiple speakers and languages.

## 1.Introduction: 引言

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
