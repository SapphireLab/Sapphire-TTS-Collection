# Emilia

<details>
<summary>基本信息</summary>

- 标题: Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for Large-Scale Speech Generation
- 作者:
  1. Haorui He, 
  2. Zengqiang Shang, 
  3. Chaoren Wang, 
  4. Xuyuan Li, 
  5. Yicheng Gu, 
  6. Hua Hua, 
  7. Liwei Liu, 
  8. Chen Yang, 
  9. Jiaqi Li, 
  10. Peiyang Shi, 
  11. Yuancheng Wang, 
  12. Kai Chen, 
  13. Pengyuan Zhang, 
  14. Zhizheng Wu
- 机构:
  1. 
- 时间:
  - 预印时间: 2024.07.07 ArXiv v1
  - 预印时间: 2024.07.13 ArXiv v2
  - 预印时间: 2024.09.07 ArXiv v3
  - 更新笔记: 2024.09.16
- 发表:
  - SLT 2024 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2407.05361)
  - [DOI]()
  - [Hugging Face](https://huggingface.co/datasets/amphion/Emilia-Dataset)
  - [Demo](https://emilia-dataset.github.io/Emilia-Demo-Page/)
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: ?
- 引用: ?
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Recent advancements in speech generation models have been significantly driven by the use of large-scale training data. 
> However, producing highly spontaneous, human-like speech remains a challenge due to the scarcity of large, diverse, and spontaneous speech datasets. 
> In response, we introduce Emilia, the first large-scale, multilingual, and diverse speech generation dataset. 
> Emilia starts with over 101k hours of speech across six languages, covering a wide range of speaking styles to enable more natural and spontaneous speech generation. 
> To facilitate the scale-up of Emilia, we also present Emilia-Pipe, the first open-source preprocessing pipeline designed to efficiently transform raw, in-the-wild speech data into high-quality training data with speech annotations. 
> Experimental results demonstrate the effectiveness of both Emilia and Emilia-Pipe. 
> Demos are available at: this https URL.

## 1.Introduction: 引言

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
