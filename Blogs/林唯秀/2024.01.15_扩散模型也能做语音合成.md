# 论文解读：扩散模型也能做语音合成吗？

- 作者：林唯秀
- 链接: [知乎](https://zhuanlan.zhihu.com/p/672986204)
- 日期：2024.01.15

注: 基于原文整理

VALL-E 借助音频编解码器为语音生成离散的声学 Tokens, 结合 GPT 自回归的思路, 基于上万小时的语音数据来训练语音合成的语言模型, 训练出的模型展现出了惊艳的上下文学习能力.
对于这类模型, 一个问题是语言合成任务的中间表征使用离散形式的表征就一定是好的吗? 虽然 GPT 系列在自然语言处理领域取得了巨大成功, 用 GPT 自回归的思路训练出的模型上限是很高的, 但同样的思路放在语音领域就没有问题吗? 毕竟语音和文本在模态特性上还是有很大不同的.
除此之外还能够考虑其他的生成模型吗? 特别是非自回归的生成模型, 采用这种模型, 那么 VALL-E 这类自回归模型面临的模型结构和鲁棒性的问题也将迎刃而解.

说到非自回归模型, 就会想到在图像领域大放异彩的扩散模型. 那么扩散模型也能够用来做语音合成吗?

## 2021.05.[NaturalSpeech2](../../Models/Diffusion/2023.04.18_NaturalSpeech2.md)

微软的 ***NaturalSpeech2*** 提出: **使用连续特征作为语音数据的中间表征, 生成模型采用扩散模型, 训练出的模型同样也能具备经验的上下文学习能力**.

本文要解决的问题依然是: 用生成模型来做零样本/少样本的语义合成.
既然像 VALL-E 这样的方法已经可以完成少样本的合成, 那为什么还要另寻他法?
这是因为 VALL-E 是将连续的语音波形量化为离散的声学 Tokens, 然后使用自回归的语言模型来建模这些声学 Token 的上下文分布.
而这么做有几点局限性:
- 语音数据对应的声学 Tokens 序列非常长, 自回归模型在解码时非常容易出现错误传播, 从而导致模型的输出不稳定.
- EnCodec 和语言模型之间存在矛盾: EnCodec 将音频数据压缩为低比特率的声学 Tokens, 虽然降低了语言模型的生成难度, 但也会丢失高频的细节信息. 一个补救的方法是使用多个声学 Tokens 来表示一个音频帧, 但这也增加了声学 Tokens 的长度.

文章针对以上问题提出了相应的改进:
- 建模对象由离散的声学 Tokens 改成连续的隐向量, 减少语音数据对应的序列长度;
- 自回归建模改成非自回归建模, 避免输出不稳定的问题.

那么需要解决两个问题:
- 连续隐向量是什么, 如何获得?
- 非自回归建模怎么做, 用哪种生成模型?

第一点可以用自编码器/变分自编码器这种压缩模型就能做到;
第二点也有现成的解决方案, 比如在图像合成领域初显效果的扩散模型, 既然图像这种低信息密度的长序列数据都可以, 那对于具有相同性质的语音数据应该也行.

由此确定出 ***NaturalSpeech2*** 的基本架构, 这个架构和后来图像领域的 LDM 架构相似.

NaturalSpeech2 在训练时, 输入语音 $x$ 和文本 $y$, 文本 $y$ 通过 Phoneme 编码器转换成音素, 再加上预测的时长 (Duration) 和音高 (Pitch), 对齐到帧级别然后一起作为条件 $c$ 输入到扩散模型中. 扩散模型的扩散过程和生成在隐空间中进行, 而隐空间由 Codec 模型得到.

下面来看 ***NaturalSpeech2*** 各个部分是如何实现的, 重点关注连续向量怎么获得和扩散模型如何设计的问题.

### 连续向量的获得

要想获得音频波形对应的连续向量, 最容易想到的是使用自编码器/变分自编码器, 和 [潜在扩散模型 (Latent Diffusion Model, LDM)](../../Models/Diffusion/2021.12.20_LDM.md) 一样. 可参阅前文 [解读扩散模型的经典论文](2023.12.30_解读扩散模型的经典论文.md).

考虑到压缩效率的问题和正则化的好处, 本文使用了 Encoder + RVQ + Decoder 的架构, 即往 Encoder 和 Decoder 之间加入一个 RVQ 模块. 这种结构就是 VALL-E 使用的 EnCodec.
而和 VALL-E 不同的是, 本文用的是各个 RVQ 层量化向量的求和, 将求和后的量化向量作为扩散模型的训练目标.
VALL-E 的做法是利用离散的 Codebook ID 作为声学 Tokens ID, 将其建模成语言模型的问题.

这种做法相比 (变分) 自编码器有一个好处: 在扩散模型预测连续向量时, 可以基于模型预测的连续向量和量化向量之间的差距添加一个正则化损失, 辅助扩散模型的训练.
文章还提出了存储效率提升的优点, 但笔者不太认同, 就算是利用 VAE 这样的模型进行压缩, 其实也不用把音频波形对应的连续向量保存下来, 只需要执行一次 VAE 的编码即可.

### 扩散模型的设计

扩散模型本质上就是一个回归预测模型.
在语言合成任务的设置下, 它是一个基于文本输入的回归预测模型. 因此需要解决两个问题:
- 文本如何进行编码;
- 回归预测模型如何建模, 即损失函数如何设计.

#### 条件输入

基于扩散模型的 TTS 系统的条件输入仍然是文本.
TTS 系统通常会通过一个 Phoneme Encoder (音素编码器) 将输入的文本转换为音素序列.
为了降低模型学习的难度, 减少文本模态和语音模态之间的 Gap, 还会在音素序列的基础上增加时长 (Duration) 和音高 (Pitch) 信息.
时长信息和音高信息通过模型预测得到, 分别给出一个损失函数: $L_{duration}$ 和 $L_{pitch}$.

#### 训练目标

文章对于扩散模型的论述, 沿用了宋飏博士的基于 SDE 的理论框架, 比较难理解.
此处只做简单解释, 不做具体展开.

为什么扩散模型能用 SDE 来描述?
扩散模型中的扩散过程其实是一个 $T$ 步的迭代式加噪过程.
加噪过程相邻步骤之间发生的变化是非常微小的, 所以适合用高斯分布来描述.
可以认为, 两个相邻步骤在时间尺度上非常接近.
进一步地, 扩散过程可以理解为在时间上连续的变化过程, 因而可以用随机微分方程来描述.

前向的扩散过程和反向的生成过程可以分别用两个 SDE 来描述.
扩散模型做的工作就是用神经网络来预测 $\nabla_x \log p_t(x)$.

有了预测结果之后, Reverse SDE 中就没有未知项, 离散化之后就能逐步反推.

虽然文章的论述比较复杂, 但建模思想还是沿用了 DDPM, 不过稍有区别:
DDPM 提到回归模型的建模目标有多种选择, 可以是噪声, 后验均值, 也可以是原始数据.
在图像合成中比较好用的是预测噪声.
但文章实验后发现同时预测隐空间 $z_0$ 和后验均值的效果会更好.

#### 上下文学习能力

### 模型训练

### 实验结果

### 算法总结

### 疑问

## AudioLDM 1/2

## VoiceLDM

## 总结

