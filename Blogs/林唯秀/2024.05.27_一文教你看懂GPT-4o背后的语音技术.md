# 别慌! 一文教你看懂 GPT-4o 背后的语音技术

- 作者：林唯秀
- 链接: [知乎](https://zhuanlan.zhihu.com/p/698725358)
- 日期：2024.05.27

## 导语

05 月 14 日凌晨, OpenAI 推出了最新的生成模型 GPT-4o, 带来了一系列震撼的功能, 用技术彻底颠覆了产品形态.
产品最大的亮点在于：以近乎完美的交互方式, 为每位用户带来GPT-4级别的智能体验.
在语音方面, GPT-4o做到了实时低延迟, 平均响应时间与人类反应速度相当, 输出的语音能够理解极度贴合对话上下文, 能够理解人类的情感情绪, 听觉质量上佳, 与真人无异.

OpenAI的生肉博客：https://openai.com/index/hello-gpt-4o/

GPT-4o 是一个 any2any 的多模态模型, 能够接受文本, 音频, 图像, 视频等多模态输入, 也能够生成包含文本, 语音, 图像和视频等混合内容的多模态输出.
**限于篇幅, 本文主要谈谈语音多模态的实现, 并分享一些对于语音研究未来发展的看法**.

当我们主要关注文本和语音模态时, GPT-4o 其实就是一个**语音语言模型 (Speech Language Model, SLM)**.
该 SLM 同时具备语音理解能力和语音合成能力, 输入端和输出端均支持文本和语音的混合多模态.
那么, 这一 SLM 应该如何实现呢? 
在**大语言模型 (Large Language Model, LLM)** 滥觞的今日, 不难想到这样一种方法：将连续的语音数据离散化成如同单词 (或者称 Token, 词元) 一样的表示, 并入到 LLM 的词表中, 再走一遍训练 LLM 的老路.

基于上述思想来构建 SLM, 需要解决以下几个问题：
- 语音如何离散化?
- 如何让 LLM 理解语音的 token? 加入语音 token 之后, LLM 在语音数据的理解上是否具有涌现性? 
- LLM 如何合成/解码语音? 

接下来, 我们按图索骥, 分别看看上述三个问题应该如何解决.

## 语音的离散化：向 LLM 看齐! 

在谈及语音离散化之前, 我们先来看看语音和文本作为两种不同的模态, 有什么区别, 有什么联系.
这直接关系到后文建模方法的选择以及离散化特征的关注点.

语音和文本的差别主要体现在：
- 文本离散, 序列短, 信息密度高 (几乎每个词都包含语义); 
- 语音连续, 序列长, 信息密度低.

语音序列长, 信息密度低的特点, 意味着语音数据有很大的压缩空间, 这一点和图像非常类似.
因此, 一些用于图像的离散化压缩方法也可以用在语音上.

除了差异, 语音和文本也有一定的联系：
语音是文本的超集, 既包含文本内容(说话人说了什么, 也就是语义信息), 也包含语音特有的音色, 韵律, 语速等声学信息(也叫做副语言).
既然语音包含文本, 那么在 NLP 中预训练语言模型也可以用来建模语音中的上下文依赖关系, 从而得到语音的离散化 token.

基于这些方法得到的 token 主要包含语音的**语义信息**.

我们先来看看语音的语义 token 如何获取.

### 语音的语义 Token 

语音的语义建模方法, 最常用到的就是 BERT 的 MLM 方法, 比较经典的工作有三个：
- [Wav2Vec 2.0](../../Models/Speech_Representaion/2020.06.20_Wav2Vec2.0.md) 
- [HuBERT](../../Models/Speech_Representaion/2021.06.14_HuBERT.md)
- [W2V-BERT](../../Models/Speech_Representaion/2021.08.07_W2V-BERT.md)

#### Wav2Vec 2.0

类似于 BERT, [Wav2Vec 2.0](../../Models/Speech_Representaion/2020.06.20_Wav2Vec2.0.md) 在隐空间随机 mask 了一定比例的语音输入, 然后用基于对比学习的训练目标学习帧的表征.
值得注意的一点是, 对比学习中目标帧的离散化处理是一个非常巧妙的操作, 它将无限的连续特征空间坍缩为有限的离散空间, 让帧特征的鲁棒性更强了.
这在语音领域上非常有用的技巧, 允许模型接受带有噪声的语音作为输入.

[Wav2Vec 2.0](../../Models/Speech_Representaion/2020.06.20_Wav2Vec2.0.md) 只是借用了 BERT 中 mask 的操作, 训练目标大体上是基于对比学习的范式.

#### HuBERT

那么, 能直接用 BERT 的 MLM 建模目标来得到高质量的语音表征吗? 
其后的 [HuBERT](../../Models/Speech_Representaion/2021.06.14_HuBERT.md) 做的就是这个事情.

[HuBERT](../../Models/Speech_Representaion/2021.06.14_HuBERT.md) 的核心点在于使用简单的 K Means 聚类方法为语音数据抽取离散化的分类标签, 也就是文中所说的 hidden unit/acoustic unit.
有了分类标签, 然后就是用 BERT 的 MLM loss 来学习语音数据中内在的上下文依赖关系.
对于 KMeans 聚类对初始值和 K 值高灵敏的特点, 作者设计了 ensemble 和 iterative refinement 方法予以解决.
前者就是多个聚类模型继承, 后者就是先在基于 MFCC 的聚类标签上进行学习, 学习到一定程度时, 在模型学习到的表征重新聚类, 再做一次 BERT 的学习.

#### W2V-BERT

既然对比学习可以学习语音的语义表征, BERT 的 MLM 也可以, 那将二者结合起来, 会不会有互补的效果呢? 
[W2V-BERT](../../Models/Speech_Representaion/2021.08.07_W2V-BERT.md) 做的就是这个事情.

注意到：[HuBERT](../../Models/Speech_Representaion/2021.06.14_HuBERT.md)中语音的离散 token 不是端到端获得的, 需要用 KMeans 算法对特征进行离线聚类, 而 [Wav2Vec 2.0](../../Models/Speech_Representaion/2020.06.20_Wav2Vec2.0.md) 又正好提供了音频帧的量化离散表征, [HuBERT](../../Models/Speech_Representaion/2021.06.14_HuBERT.md) 和 [Wav2Vec 2.0](../../Models/Speech_Representaion/2020.06.20_Wav2Vec2.0.md) 很容易就能缝合在一起.
缝合的方法也是显然的：前面若干层做类似 [Wav2Vec 2.0](../../Models/Speech_Representaion/2020.06.20_Wav2Vec2.0.md) 的对比学习, 学习出 [HuBERT](../../Models/Speech_Representaion/2021.06.14_HuBERT.md) 要用的离散表征, 然后在后面若干层做类似 [HuBERT](../../Models/Speech_Representaion/2021.06.14_HuBERT.md) 的 MLM 训练.

### 语音的声学 Token

上一部分介绍的预训练模型做的是上下文关系的预训练, 学习到的表征主要包含与上下文相关的语义信息.

要想将语音的 Token 还原成为真正具有真人表现力的信号, 还需要有包含音色, 韵律, 语速等副语言信息的声学特征.
声学特征的学习在很大程度上参考了图像领域的工作, 用到的主要是类似于 [VQ-VAE](../../Modules/VQ/2017.11.02_VQ-VAE.md), [VQGAN](../../Models/_Basis/2020.12.17_VQGAN.md) 等的离散化压缩方法, 并针对语音数据的特性做了优化.
这一部分比较经典的工作就是 [SoundStream](../../Models/Speech_Neural_Codec/2021.07.07_SoundStream.md) 和 [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md), 二者的工作高度类似, 我们放在一起来看.

说到压缩, 最先想到的模型当然就是 AutoEncoder (自编码器).
为了提升压缩效率, 有利于数字传输和存储, 以及离散化建模的要求, 压缩模型中还需要包含**量化 (Quantization)**, 将连续的音频信号转换为离散的数值.
基于上述考虑, 模型大体上应该是 [VQ-VAE](../../Modules/VQ/2017.11.02_VQ-VAE.md) 的结构.

为了平衡 **VQ (Vector Quantization, 向量量化)** 与音频实时高保真传输的矛盾, 通常采用多个残差连接的 CodeBook 来进行量化, 这个就是所谓的 RVQ.
采用 RVQ 的好处主要有两个：
- 区分不同 Quantization Block 的分工, 第一个 Block 包含最重要的语义信息, 后续的 Block 包含还原语音的副语言信息; 
- 模型训练时可随机采样前面若干个 Block 来训练, 保持一定精度, 实现对比特率的动态适应.

总而言之, [SoundStream](../../Models/Speech_Neural_Codec/2021.07.07_SoundStream.md)/[EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md) 其实就是一个 RVQ-VAE, 它们所建模的语音离散化 token 包含了层次化的语义信息和声学信息.

### 语音的统一表征? 

不难发现, 虽然说 [SoundStream](../../Models/Speech_Neural_Codec/2021.07.07_SoundStream.md) 和 [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md) 这样的基于 RVQ-VAE 的压缩建模方法包含了语音的声学特征, 但其中也不可避免地带入了语义特征.
二者提取的实际上更像是一种语义特征和声学特征的混合体.
基于此, [SpeechTokenizer](../../Models/Speech_Neural_Codec/2023.08.31_SpeechTokenizer.md) 在二者的基础上, 引入了语义引导信息来解耦语义特征和声学特征.
语义特征和声学特征的解耦对于最终的语音合成有着相当的重要性.
[SpeechTokenizer](../../Models/Speech_Neural_Codec/2023.08.31_SpeechTokenizer.md) 的具体做法是：使用 [HuBERT](../../Models/Speech_Representaion/2021.06.14_HuBERT.md) 的特征对 RVQ1 的特征做语义蒸馏, 其余部分保留声学信息.

### 语音的其他表征

上述的语音离散表征, 不管是基于 [HuBERT](../../Models/Speech_Representaion/2021.06.14_HuBERT.md) 的语义 Token, 还是基于 [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md) 的声学 Token, 它们都是直接基于原始的音频波形抽取的.
除此之外, 也可以基于语音的中间表征来抽取.
最典型的语音中间表征就是**梅尔频谱 (MEL Spectrogram, 下文简称 MEL)**.
梅尔频谱本身就对语音进行了压缩, 将梅尔频谱类比于图像, 使用单码本的 VQ 也可以达到与 [SoundStream](../../Models/Speech_Neural_Codec/2021.07.07_SoundStream.md) 和 [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md) 那样类似的压缩程度.
这种 MEL+VQ 的做法在各种语音合成模型中也相当常见.

## 让 LLM 理解语音 Token

有了上面所说的语义 Token 和声学 Token 之后, 其实就可以利用它们来构建语音层面的语言模型了.
比较经典的工作有：
- 谷歌: [AudioLM](../../Models/Speech_LLM/2022.09.07_AudioLM.md), [AudioPaLM](../../Models/Speech_LLM/2023.06.22_AudioPaLM.md);
- 字节: [SALMONN](../../Models/Speech_LLM/2023.10.20_SALMONN.md);
- 复旦: [SpeechGPT](../../Models/Speech_LLM/2023.05.18_SpeechGPT.md)/[SpeechGPT-Gen](../../Models/Speech_LLM/2024.01.24_SpeechGPT-Gen.md)/[SpeechAlign](../../Models/Speech_LLM/SpeechAlign.md);
- 阿里: [LauraGPT](../../Models/Speech_LLM/2023.10.07_LauraGPT.md)
- 新加坡国立大学: [NextGPT](../../Models/Speech_LLM/2023.09.11_NExT-GPT.md).

它们的做法其实都大差不差, 我们看几个就知道是怎么回事了.

### AudioLM：最初的 SLM

见名知义, [AudioLM](../../Models/Speech_LLM/2022.09.07_AudioLM.md) 构建的是语音层面的语言模型, 即给定一段语音, 模型预测后续的语音.
输入侧和输出侧都只有语音模态.

这个任务形式和 GPT-4o 非常类似, 不会经历 ASR → LM → TTS 的过程, 直接从语音上下文中推理语义信息, 再结合声学信息合成贴合上下文的高表现力语音.
而上文所述的语义 Token 和声学 Token 正好就能满足这个任务的要求.

[AudioLM](../../Models/Speech_LLM/2022.09.07_AudioLM.md) 的具体做法是：
- 用 [SoundStream](../../Models/Speech_Neural_Codec/2021.07.07_SoundStream.md) 提取声学 Token, 
- 用 [W2V-BERT](../../Models/Speech_Representaion/2021.08.07_W2V-BERT.md) 提取语义 Token, 
- 模型主体就是一个常规的 GPT, 词表包含所有的声学 Token 和语义 Token.

它的建模过程也相当有意思, 有很大的参考意义：
- 先做最重要的语义建模, 
- 然后先预测 [SoundStream](../../Models/Speech_Neural_Codec/2021.07.07_SoundStream.md) 的前若干层特征, 建模粗糙的声学特征, 
- 再预测 [SoundStream](../../Models/Speech_Neural_Codec/2021.07.07_SoundStream.md) 的剩余层特征, 建模声音的细节信息, 
- 最后基于所有的声学 Token 还原为语音.

这种层次化的建模在诸如 [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md) 这样的语音合成模型中也非常常见.

当然, [AudioLM](../../Models/Speech_LLM/2022.09.07_AudioLM.md) 仅仅关注语音模态, LM 也很常规, 不具备如同 GPT-4o 一样强悍的指令遵循能力和对话能力, 语音对话的连贯性和表现力都相当弱.
但这一工作仍然具有相当的启发性和开拓性, 证明了：即使是常规的LM, 照样也能理解语音 Token .

### AudioPaLM：整合LLM

这个就是 [AudioLM](../../Models/Speech_LLM/2022.09.07_AudioLM.md) 的后续了, 谷歌将常规的 LM 替换成已经训练好的, 具有强大文本理解能力和生成能力的大语言模型 [PaLM-2](../../Models/LLM/2023.05.17_PaLM2.md), 既继承了 [AudioLM](../../Models/Speech_LLM/2022.09.07_AudioLM.md) 保留副语言的能力, 又融合了 [PaLM-2](../../Models/LLM/2023.05.17_PaLM2.md) 强大的语义理解能力和推理能力.
而且, 该模型的词表同时包含大语言模型的 Token 和语音 Token , 可以同时做语音理解任务和合成生成任务, 第一次将这些任务整合在一个模型中进行解决.

不过, 需要指出的是, 文中的语音 Token Embedding 是直接输入到 Transformer 中的, 并没有使用音频编码器做一次转换.
而且, [AudioPaLM](../../Models/Speech_LLM/2023.06.22_AudioPaLM.md) 的训练更加接近文本多任务的 T5, 并未用到复杂的, 丰富多样的指令来表达任务的意图, 还不能算是真正严格的 Instruction Fine-Tuning.

### SALMONN

[SALMONN](../../Models/Speech_LLM/2023.10.20_SALMONN.md) 是字节跳动和清华大学电子系 (也是我们实验室) 的合作成果.
虽然这个工作的目的是让 LLM 能够理解语音, 还不能生成语音, 但它的训练方法和 LLM 比较接近, 而且在诸多语音相关的任务上都显示出了涌现性, 可以用作通用的特征提取器, 这对于构建高质量的, 包含语音-文本多模态的指令微调数据集具有相当大的意义.

### SpeechGPT/SpeechGPT-Gen/SpeechAlign：向 LLM 的训练方法看齐

这算是复旦大学邱锡鹏组在这个领域一个成系列的工作, 我们一个一个来看.

#### SpeechGPT

[SpeechGPT](../../Models/Speech_LLM/2023.05.18_SpeechGPT.md) 做的也是兼具语音理解能力和语音生成能力的多模态模型.
在模型的训练上, [SpeechGPT](../../Models/Speech_LLM/2023.05.18_SpeechGPT.md) 大幅度向 LLM 看齐, 使用了三段式的训练方法：
- 第一阶段先做模态适应的预训练, 其实就是拿 ASR 的语音数据来做预训练; 
- 第二阶段和第三阶段都是指令微调, 不过根据指令模态的不同, 细分为了跨模态的指令微调和模态链指令微调. 指令微调的数据集都是来自ASR数据集. 描述任务需求的指令由GPT-4生成.

在我看来, 这个工作还是相当偏学术化的作品, 文中有不少点都有值得商榷的地方：
- 第一, 语音的离散化仅仅用了 [HuBERT](../../Models/Speech_Representaion/2021.06.14_HuBERT.md), 模型只能看到语音的语义特征, 这对模型合成语音的音质和表现力有非常大的影响, demo 的语音也验证了我的判断; 
- 第二, 指令微调数据集的构造上有问题. 他们用的是 ASR 数据集, 其实更好的选择应该是 TTS 数据集, 可惜高质量的 TTS 数据集实在是太少了. ASR 数据集中的文本和语音可能并不是严格对齐的, GPT-4 产生的 meta-prompt 和语音本身的特征也有可能是对不上的, 比如 prompt 要求大声朗读, 但语音本身可能是特定低沉的.
meta-prompt 本身就无法做到足够复杂丰富, 不能描述到语音的一些细粒度信息.
这一部分, 最好要有像诸如 [SALMONN](../../Models/Speech_LLM/2023.10.20_SALMONN.md) 这样的多模态语音理解模型的介入, 像 DALLE3 一样丰富指令的多样性.
至于语音方面, 可以考虑引入 zero-shot 的语音合成模型或者变声模型来做合成数据.
- 第三, 文中的训练方法也没有与人类偏好做对齐.

#### SpeechGPT-Gen

对于上面的第一个问题, 作者在其后的 [SpeechGPT-Gen](../../Models/Speech_LLM/2024.01.24_SpeechGPT-Gen.md) 中做了解决.

解决思路的核心点就是：让模型不仅看到语音的语义 Token, 也要看到语音的声学 Token .
具体做法是：
[SpeechGPT](../../Models/Speech_LLM/2023.05.18_SpeechGPT.md) 的 [HuBERT](../../Models/Speech_Representaion/2021.06.14_HuBERT.md) 特征替换成了 [SpeechTokenizer](../../Models/Speech_Neural_Codec/2023.08.31_SpeechTokenizer.md) 中的语义特征, 用 SpeechGPT 这一 LLM 来自回归地建模语义特征, 有了语义特征之后, 再使用 Flow-Matching 这样的扩散模型来建模声学特征.
这里选用 Flow-Matching 扩散模型, 可能是受了 SD3 和 [VoiceBox](../../Models/Speech_LLM/2023.06.23_VoiceBox.md)/[AudioBox](../../Models/Speech_LLM/2023.12.25_Audiobox.md) 的影响.
为了增强两阶段建模的依赖关系, 作者将语义特征的先验信息注入到第二阶段扩散模型的先验分布中.
可以看到, 这里语音的解码其实也是一种层次化渐进式解码.

#### SpeechAlign

[SpeechAlign](../../Models/Speech_LLM/SpeechAlign) 做的则是 SLM 与人类偏好的对齐, 彻底地向 LLM 的训练方法看齐.
该工作构建了对比 gold token 和合成 Token 的 encodec 数据集, 然后进行偏好优化来进行改进.
使用的偏好优化方法包括 RLHF 和 Chain of Hindsight.

#### 总结

简单总结一下上面这些工作中值得关注的点：

要想让 LLM 输出上下文连贯的高表现力语音, 必须要让 LLM 看到语义 Token 和声学 Token, 
- 只有语义 Token, 那语音就会显得呆板机械, 
- 只有声学 Token , 那语音就不知所云; 

LLM 的指令微调同样可以迁移到语音-文本多模态领域中, LLM 的指令微调同样可以带来如同 NLP 一样的涌现性; 
高质量指令微调数据集的构建应该是最大的瓶颈! 
一下子让 LLM 同时做语音理解和语音生成, 难度非常大.
不如分步进行.

如果要分步进行的话, 要先实现一个类似于 [SALMONN](../../Models/Speech_LLM/2023.10.20_SALMONN.md) 那样的多模态理解模型和一个强大的 Zero-shot TTS 模型.

- 前者用于给语音数据打上丰富的标签, 可以是情感情绪, 韵律, 音高, 语速, 也可以是口音, 意图和说话环境; 
- 后者则用于生成高质量的语音数据.

毕竟, 高质量的, 文本和语音严格对齐的TTS数据实在是太少了, 尤其是中文领域.
有了这两个模型的加持, 我们其实就能够构造出高质量的指令微调数据集.
我不知道 OpenAI 是否有 [SALMONN](../../Models/Speech_LLM/2023.10.20_SALMONN.md) 这样的模型, 但 OpenAI 的 OpenVoice 模型应该足够为其提供高质量的语音数据了.

既然我们在上面的篇幅中论述了语音理解多模态模型的构建, 那我们在下一部分就重点关注 zero-shot TTS 模型, 它对高质量指令微调数据集的构建同样至关重要.
同时, LLM 解码语音的方法也能从 zero-shot TTS 方案中得到不少的启发.

## LLM 如何合成语音

前面说到, SLM 词表中包含了语音的语义 Token 和声学 Token.
- 语义 Token 保证生成语音与对话上下文的连贯性, 
- 声学 Token 保证了合成语音的质量和表现力.

要想做到合成上下文连贯的高自然度语音, 有两个问题必须要解决：

- 语音既有语义 Token, 又有声学 Token, 应该要如何解码成语音? 
- SLM 在合成语音的过程中是否能够遵循多轮对话中的文本指令和语音指令? 这个很重要! 这允许模型根据用户的即时要求来生成语音回复. 比如说, OpenAI 演示视频中出现的：“将语速提高两倍”, “采用更加机械化的语气” 这样的要求.

对于第一个问题, 以 [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md) 为代表的诸多 zero-shot TTS 模型给出了不同的解决方案, 这些方案虽有不同, 但也有不可忽视的共同点; 
对于第二个问题, 以 [VoiceLDM](../../Models/Diffusion/2023.09.24_VoiceLDM.md) 和 [ParlerTTS](../../Models/Speech_LLM/2024.02.02_Parler-TTS.md) 为代表的 Text/Prompt-Guided Zero-Shot TTS 工作给出了肯定的答案.

简单解释一下 Text/Prompt-Guided Zero-Shot TTS 是怎么回事, 通常的语音合成就是将文本转换成声音, 该任务在 Transcription 之外, 又增加了 Description 的输入, 来描述合成语音的情感情绪, 口音, 语气, 语速, 音高, 说话环境, 氛围等等信息.
我们逐个来看这些工作.

### Zero-shot TTS

2023 年以来, 学术界和工业界出了不少具备 In-Context Learning (Zero-Shot/Few-Shot) 能力的 TTS 模型.
这些 TTS 模型通常会将低信息密度, 长序列的连续语音数据压缩为高信息密度的 Tokens 或者 Latents (其实就是码本中具体的 Token Embedding).

这些模型本质上做的事情就是：如何高效实现语音 Tokens/Latents到音频波形的映射.

这些模型给出的解决方案基本上都遵循一个准则：语义 Token 和声学 Token 层次化解码, 先语义后声学, 或者先解码成 MEL 再后接声码器, 并且非必要不做自回归 (毕竟自回归上线虽高, 但太吃数据了)! 我们一个个来看.

#### 基于声学 Token 或语义 Token 的工作

##### VALL-E

先是微软的 [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md).

这是 Zero-Shot TTS 的开山之作, 首次在 TTS 任务上采用了上万小时的数据.
它采用 [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md) 将语音转换为离散的 Token, 然后用 GPT 在 Token 上做语言模型的任务.
但是, 语音毕竟不是文本, 如果直接在语音的所有特征上都做自回归的话, 那训练的成本会相当高.
考虑到 [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md) RVQ 特征的层次性, 低层特征表示语义内容这样的重要特征, 高层特征则表征声学细节.
前者具有比较强的上下文依赖关系, 适合用自回归来建模, 后者诸如音色这样的特征, 具有全局性, 用非自回归特征也可以搞定, 所以就有了 [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md) 中自回归+非自回归的层次建模方式.

尽管 [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md) 在用 GPT 建模 Token 的上下文关系的时候, 基于  Token 的层次化特性做了分治处理, 可能是限于当前语音数据集的规模 (几万小时可能不够), 这种 GPT 自回归的难度还是相当大的, 解码过程存在常见的错误传播现象, 鲁棒性非常差, 极其不稳定.

根据 Ilya Sutskever 此前对于自回归的论述, GPT 自回归相比于 BERT 这种双向结构是非常 Data-Hungry 的, 万小时的数据可能不够.
根据本人以及一些同行的经验, [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md) 模型这一类的自回归模型, 也包括 Tortoise-TTS 和 XTTS v2, 要想显出威力, 至少要有十几万小时的数据才行.

既然 GPT 自回归的难度这么大, 就有不少人想方设法地来降低 GPT 学习的难度了.
他们的解决方案也非常类似：给 GPT 提供额外的条件信息不就行了.

比较典型的工作就是微软的 [RALL-E](../../Models/Speech_LLM/2024.04.04_RALL-E.md) 和吉利的 [HAM-TTS](../../Models/Speech_LLM/2024.03.09_HAM-TTS.md).
- [RALL-E](../../Models/Speech_LLM/2024.04.04_RALL-E.md) 先生成了时长信息和音高信息, 作为 GPT 自回归的先验, 之所以会补充时长和音高, 这大概是受到 [FastSpeech2](../../Models/TTS2_Acoustic/2020.06.08_FastSpeech2.md) 这样的非自回归模型的启发, 这两个指标的引入, 有助于提升合成的鲁棒性; 
- [HAM-TTS](../../Models/Speech_LLM/2024.03.09_HAM-TTS.md) 则是补充了基于 [HuBERT](../../Models/Speech_Representaion/2021.06.14_HuBERT.md) 的语义信息.
值得注意地是, [HAM-TTS](../../Models/Speech_LLM/2024.03.09_HAM-TTS.md) 将模型的训练数据扩充到了 65 万小时, 其中有 50 万小时的数据是合成数据.
合成数据也能大幅度提升合成语音的音质.

说到 [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md) 的后续改进, [VoiceCraft](../../Models/Speech_LLM/2024.03.25_VoiceCraft.md) 不得不提.
我愿意称之为 "优雅的 [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md)".
它的优雅主要体现在两个方面：Casual Masking 和 Delayed Stacking.
- 所谓的 Causal Masking, 是为了用自回归 GPT 架构来做语音编辑任务, 就是把被 Mask 的部分移动到序列末尾去预测, 一套架构同时做合成和编辑任务; 
- 所谓的 Delay Stacking, 是为了适配自回归和 RVQ, 通过 Delay 错位让当前码本的 Token 预测正好可以利用前面那些  Token 的预测结果, 比起 [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md) 那样自回归和非自回归缝合在一起的结构要优雅不少.

##### Diffusion-Based

我们通常所说的语音 Token 是离散的.
如果使用对应码本中的 Embedding 来表示语音的话, 它也可以是连续的低维度的隐变量.
既然是低维度的连续隐变量, 那图像合成领域中大火的 LDM (Latent Diffusion Model, 其实就是 Stable Diffsion 1&2 采用的模型) 自然也可以用到语音的合成上.
这方面的经典工作有很多, 比如说：[NaturalSpeech2](../../Models/Diffusion/2023.04.18_NaturalSpeech2.md); [NaturalSpeech3](../../Models/Diffusion/2024.03.05_NaturalSpeech3.md); [AudioLDM 2](); [VoiceLDM].

但这里面只有 [NaturalSpeech2](../../Models/Diffusion/2023.04.18_NaturalSpeech2.md) 用到了语音离散化部分提及的声学/语义 Token, [NaturalSpeech3](../../Models/Diffusion/2024.03.05_NaturalSpeech3.md) 的属性分解形式的 VQ 更像是另一种形式的 RVQ.

我们先来看 [NaturalSpeech2](../../Models/Diffusion/2023.04.18_NaturalSpeech2.md); [NaturalSpeech3](../../Models/Diffusion/2024.03.05_NaturalSpeech3.md), 其他的工作后面再来看.

首先是 [NaturalSpeech2](../../Models/Diffusion/2023.04.18_NaturalSpeech2.md), 它基本上就是 [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md) 的连续版本.
它用的 Latent 也是来自 [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md), 对其中不同层次的 Latent 做了求和, 然后将其作为扩散模型的训练目标.
值得注意的是, 扩散模型和 [FastSpeech2](../../Models/TTS2_Acoustic/2020.06.08_FastSpeech2.md) 一样也用了时长和音高作为合成的先验条件.
这一点也被后来的 [RALL-E](../../Models/Speech_LLM/2024.04.04_RALL-E.md) 采用.
该工作中的扩散模型采用 [WaveNet](../../Models/TTS3_Vocoder/2016.09.12_WaveNet.md) 实现, 同时预测不加噪的 Latent 和后验均值, 和图像合成领域的扩散模型在实现方式上还是有所不同的.

[NaturalSpeech3](../../Models/Diffusion/2024.03.05_NaturalSpeech3.md), 还是非自回归的, 而且非自回归的正统性味道更加浓厚, 借用了不少 [FastSpeech2](../../Models/TTS2_Acoustic/2020.06.08_FastSpeech2.md) 和 [Mega-TTS](../../Models/Speech_LLM/2023.06.06_Mega-TTS.md), [Mega-TTS2](../../Models/Speech_LLM/2023.07.14_Mega-TTS2.md) 的设计思想.
像 [Mega-TTS](../../Models/Speech_LLM/2023.06.06_Mega-TTS.md), [Mega-TTS2](../../Models/Speech_LLM/2023.07.14_Mega-TTS2.md) 一样, 同样采用(自)监督信号对语音 Token 编码的内容做了限制, 而不再像是 [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md)/[NaturalSpeech2](../../Models/Diffusion/2023.04.18_NaturalSpeech2.md) 那样一把抓.
相应地, 语音 Token 化的方法也用 VQ 就行.

具体而言, 文章将语音信号分解为时长, 内容, 韵律和细节四个部分, 然后每个部分用离散化的扩散模型来建模.
不过, 原文使用 GRL 来促进语音属性的分解, 这一点的靠谱程度存疑.
我也尝试过文章的 FACodec, 但效果很差.
三级扩散模型级联的结构, 预测起来似乎也非常麻烦.

#### 基于 MEL 频谱 + VQ 的 TOKEN 的工作

当然, 也有不少工作用了 MEL 频谱作为中间特征, 然后在梅尔频谱的基础上, 或是用 VQ 提供离散 Token, 或是用 CNN 来提取连续 Latent.

对于 MEL+VQ 的工作, 有 Tortoise-TTS, XTTS2, [Mega-TTS](../../Models/Speech_LLM/2023.06.06_Mega-TTS.md), [Mega-TTS2](../../Models/Speech_LLM/2023.07.14_Mega-TTS2.md), [BASE TTS](../../Models/Speech_LLM/2024.02.12_BASE-TTS.md).

##### Tortoise-TTS

该工作是著名的开源英文TTS模型.
其作者目前在 OpenAI 就职, 同时也是 GPT-4o 的重要 Contributor (他自个儿在博客中说的).
Tortoise-TTS 使用 MEL+[VQ-VAE](../../Modules/VQ/2017.11.02_VQ-VAE.md) 的方法得到语音的 MEL Token, 然后对 MEL  Token 以及 Text Token 做 GPT 自回归建模.
对于语音的解码, 自然也是分为两步：先是用扩散模型将 MEL Token 转换为 MEL 谱, 这一步和文生图很像, 用扩散模型是很自然的选择; 然后用声码器将 MEL 频谱转换为音频波形.
Tortoise-TTS 和 [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md) 的主体都是自回归建模, 二者的不同主要在于 Token 的不同.

##### Mega-TTS 系列

字节跳动的 Mega-TTS 系列对语音 Token 编码信息做了显式的信息压缩处理, 让语音 Token 仅编码上下文依赖强的韵律信息, 然后用 GPT 自回归来建模语音的韵律.
对于其他方面的信息, 模型的处理显得较为常规：
- 音色一般具有全局性, 使用单一的音色编码器从参考音频中提取就行; 
- 对于文本语义内容的处理, 模型在很大程度上参考了非自回归的FastSpeech 2.

对于语音的解码, 也是分为两步：先通过 MEL Decoder 还原为 MEL 谱, 然后通过声码器解码为音频波形.

[Mega-TTS 2](../../Models/Speech_LLM/2023.07.14_Mega-TTS2.md) 和 [Mega-TTS](../../Models/Speech_LLM/2023.06.06_Mega-TTS.md) 总体上类似, 在音色编码 (音素级编码, 多条参考音频), 语音提示长度 (扩展同 Speaker 语音上下文长度硬Train, 音频 Prompt 长度更长)和时长建模 (也用 GPT 自回归) 上做了改进, 同时堆了更大规模的数据.

剪映的后端 TTS 模型用的就是 [Mega-TTS 2](../../Models/Speech_LLM/2023.07.14_Mega-TTS2.md).

该工作在各论文的评测中表现也都不错.

#### 基于 MEL 频谱 + VAE 的工作

对于 MEL+Latents 的工作, 有：[AudioLDM](../../Models/Diffusion/2023.01.29_AudioLDM.md), [AudioLDM](../../Models/Diffusion/2023.08.10_AudioLDM2.md), [StyleTTS](../../Models/TTS2_Acoustic/2022.05.30_StyleTTS.md), [StyleTTS2](../../Models/Speech_LLM/2023.06.13_StyleTTS2.md).

##### AudioLDM 系列.

[AudioLDM](../../Models/Diffusion/2023.01.29_AudioLDM.md), [AudioLDM](../../Models/Diffusion/2023.08.10_AudioLDM2.md) 使用的语音 Latents 是一致的, 均通过 MEL+VAE 获得.
既然是连续的 Latents, 使用扩散模型来建模也合情合理.
解码过程也相当简单：VAE Decoder 获得梅尔频谱, 然后用声码器转换为音频波形.

该系列工作的核心创新点是利用多模态模型统一了扩散模型条件输入侧的信息：
[AudioLDM](../../Models/Diffusion/2023.01.29_AudioLDM.md) 用 CLAP 统一了文本模态和音频模态, 用单模态的音频数据就能完成模型的训练; 
[AudioLDM](../../Models/Diffusion/2023.08.10_AudioLDM2.md) 则包含了图像, 文本, 转录文本等更多模态, 模型泛用性也更强, 既能做语音合成, 也能做音乐生成, 音频事件生成.

##### StyleTTS 系列.

StyleTTS 系列的模型一众 Zero-Shot TTS 模型显得比较老派, 整体结构基本上沿袭了非自回归的 [FastSpeech2](../../Models/TTS2_Acoustic/2020.06.08_FastSpeech2.md), 不同之处在于增加了基于参考音频抽取的风格信息.
说是风格, 其实跟 [Mega-TTS 2](../../Models/Speech_LLM/2023.07.14_Mega-TTS2.md) 的音色很像.

[StyleTTS 2](../../Models/Speech_LLM/2023.06.13_StyleTTS2.md) 的工作则将风格进一步拆分成声学风格和韵律风格.
训练时的风格信息由音频提供, 推断时的风格信息则由扩散模型提供.

[StyleTTS 2](../../Models/Speech_LLM/2023.06.13_StyleTTS2.md) 通过一个扩散模型桥接了文本韵律和语音风格之间的联系, 摆脱推断时对参考音频的依赖.
不用参考音频其实对产品的意义还挺大的, 要都用现实世界中真人尤其是名人的声音作为参考音频, 那这势必会引起版权纠纷.
这种纠纷在国内国外都有相关的事件.
最近寡姐投诉 OpenAI 的事件就是一例.

### TTS 对指令的遵循

SLM 不仅要合成合乎上下文语义的高表现力语音, 合成的语音还要符合用户的即时要求.
一些 Text-Guided Zero-Shot TTS 的工作值得参考.
这些工作一般都是在已有的 Zero-Shot TTS 模型或者 Text-to-Audio 模型上改造而来, 同时吸收 Transcription 和 Description 两路条件.
其中的重点还是在于数据集的构建.

这方面的工作有：[PromptTTS](../../Models/Prompt/2022.11.22_PromptTTS.md), [InstructTTS](../../Models/Prompt/2023.01.31_InstructTTS.md), [ParlerTTS](../../Models/Speech_LLM/2024.02.02_Parler-TTS.md), [VoiceLDM](../../Models/Diffusion/2023.09.24_VoiceLDM.md) 和 [Audiobox](../../Models/Speech_LLM/2023.12.25_Audiobox.md).

我们主要谈谈 [ParlerTTS](../../Models/Speech_LLM/2024.02.02_Parler-TTS.md) 和 [VoiceLDM](../../Models/Diffusion/2023.09.24_VoiceLDM.md).

#### ParlerTTS

[VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md)/[VoiceCraft](../../Models/Speech_LLM/2024.03.25_VoiceCraft.md) 的增强版, 通过 T5 编码器和 Cross-Attention 旁路引入了描述性文本的信息.

该工作的目的是想使用自然语言 Prompt 来指定说话风格和环境信息, 摆脱对参考音频的依赖.
描述性标签文本的收集过程也显得相当朴素：通过定制化的监督式模型获取语音数据的口音特征, 录音质量特征, 音高语速特征.
然后用 LLM 将这些特征转换为自然语言的描述.
在我看来, 这个工作有这么几点局限性：
- 其一, 缺乏情绪标签; 
- 其二, 语音描述性标签的收集并不具备通用性, 较为繁琐, 远不如一个强大的多模态语音理解模型来得实在.

文章demo虽然达到了预期的效果, 但场景似乎局限在朗读的情景中.

#### VoiceLDM.

[VoiceLDM](../../Models/Diffusion/2023.09.24_VoiceLDM.md) 在 [AudioLDM](../../Models/Diffusion/2023.01.29_AudioLDM.md) 的基础上增加了转录文本的输入.
这个工作和 [AudioLDM](../../Models/Diffusion/2023.01.29_AudioLDM.md) 很像, 同样使用 CLAP 注入语音的描述性信息.
不同地是, 为了做 TTS 任务, 该工作通过 Cross-Attention 旁路增加了 Transcription 的信息.

### TTS总结

林林总总说了这么多 Zero-Shot 的 TTS 方法, 我想说明的结论有这么几点：

- 在 LLM 大行其道, Scaling Law 大显神威的时代, TTS 模型的训练数据规模已经突破了万小时, 甚至达到了数十万小时的级别.
在大数据的加持下, TTS 任务上也涌现出了 In-Context Learning能力.
- 语音信息的解码通常都要层次化或者多步进行, 不能一步到位. 自回归, 扩散模型和流匹配都能在 TTS 中发挥作用; 
- 借鉴 NLP Instruction Fine-Tuning 和文生图的经验, TTS 模型同样可以遵循文本指令或者语音指令, 合成符合用户即时要求的语音, 摆脱对参考音频的依赖, 这或许也能规避一些知识产权的困扰. 同时, 用户也能在对话过程中随时切换语音回复的风格, 这一点在 OpenAI 的 demo 中有很明确的体现. 另外, 不知道大家有没有注意, GPT-4o 合成的语音是可以是放映所处的声学环境的：有一段语音背后似乎是有钢琴声的.
- Text-Guided Zero-Shot TTS 在模型架构上和 Zero-Shot TTS 有非常大的相似性. 但训练数据可能较为缺乏.
  先开发 Zero-Shot TTS, 再用类似 [SALMONN](../../Models/Speech_LLM/2023.10.20_SALMONN.md) 那样的多模态理解模型来打标签 (类似 DALLE3 的做法), 这样数据集构造方式, 可能会是更好的选择.

另外, 对于语音的解码方案, 我倾向于是这样的：
- 如果要做流式推理, 外接类似 [HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md) 这样的声码器的方式可能不是好的选择. [HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md) 并不天然支持流式解码. 
  相反地, 诸如 [SoundStream](../../Models/Speech_Neural_Codec/2021.07.07_SoundStream.md) 和 [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md) 这样的方法, 同时有流式变体和非流式变体; 
- 先做语义 Token 的解码, 这个解码大概率是自回归解码. 语义 Token 毕竟是建模上下文依赖关系, 自回归方法已经在 NLP 上证明了这一点; 
- 然后做声学 Token 的解码, 扩散或者 Flow-Matching 可能是更好的选择. 扩散模型或者流匹配可以很好地修补语音的细节; 

当然, 除了上面讲到的, zero-shot TTS还有很多值得研究的方法.
限于篇幅, 仅列举于此, 不再详述:
- [HierSpeech++](../../Models/Speech_Neural_Codec/2023.11.21_HierSpeechpp.md), 
- [BASE-TTS](../../Models/Speech_LLM/2024.02.12_BASE-TTS.md), 
- [Voicebox](../../Models/Speech_LLM/2023.06.23_VoiceBox.md)/[Audiobox](../../Models/Speech_LLM/2023.12.25_Audiobox.md)
- [UniAudio](../../Models/Speech_LLM/2023.10.01_UniAudio.md), 
- [Make-a-Voice](../../Models/_tmp/2023.05.30_Make-A-Voice.md).

## 其他问题

对于 GPT-4o 模型, 如果仅仅聚焦于语音多模态, 还有下面的问题值得关注：

- 语音交互如何做到低延迟? 
  大概率要求流式切片处理, 主要工作在于工程优化, 用 C++ 重写算子. 推理框架的话, 用 Tensorrt, mnn 这些都行. 上下文所述的音频离散化方法, 诸如 [SoundStream](../../Models/Speech_Neural_Codec/2021.07.07_SoundStream.md) 和 [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md), 其实也支持流式处理.
- 语音对话中的打断如何实现? 
  个人认为有两种可能的方案：Turn-Based 和流式处理.
  - 所谓的 Turn-Based 方案, 是比较工程化的, 简答概括一下就是：检测是否有停顿, 如果一段时间内没有声音, 模型就开始返回语音回复.
  - 另一种流式方案, 则是：模型一直在接受用户的流式语音输入, 判断是否应该输出语音回复, 一个充分训练的模型应该是能够准确预测出语音词表中的 `[START]` 和 `[END]` 的.


## 全文总结

总结一下本文所谈的内容, 我认为 GPT-4o 语音多模态的实现可能是走了以下的技术路线：

- Audio & Text Tokenizer 的实现应该是语音离散化部分所用的技术, 例如 [SoundStream](../../Models/Speech_Neural_Codec/2021.07.07_SoundStream.md), [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md), [SpeechTokenizer](../../Models/Speech_Neural_Codec/2023.08.31_SpeechTokenizer.md), 或者是 MEL+VQ 最后配合声码器来解码; 
  - 参考 Zero-Shot TTS, [AudioLM](../../Models/Speech_LLM/2022.09.07_AudioLM.md)/[AudioPaLM](../../Models/Speech_LLM/2023.06.22_AudioPaLM.md), [SpeechGPT-Gen](../../Models/Speech_LLM/2024.01.24_SpeechGPT-Gen.md) 等工作的结果, LLM 中语音 Token 的解码应该是要走层次化或者多步的方法, 先解码语义特征, 再解码声学特征, 
  - 或者是先解码 MEL, 再加一个 [HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md) 这样的声码器.
  - 另外, 如果做 Audio/Speech/Music 这样的通用声合成的话, 可能也能通过 Prompt 来控制.
  [AudioLDM2](../../Models/Diffusion/2023.08.10_AudioLDM2.md) 虽然做了这方面的工作, 但 Audio/Music 和 Speech 的参数其实是不一样的, 说到底还不是同一个模型.
- 对于指令微调, 数据集的构造非常重要, 大概率要用到合成数据.
  - 其一, 网络上高质量语音数据的量级远远不及文本, 直接拿ASR数据来做肯定会影响模型合成语音的音质; 
  - 其二, 大语言模型合成的instruction往往触及不到语音的细粒度特征, 这样的instruction其实无法准确详尽地描述text和speech之间的关系. 因而, 需要引入强大的zero-shot TTS模型合成高质量语音, 然后用多模态语音理解模型来为合成语音打标签, 当然也可以评分做筛选什么的.
- 最后是要让大模型的输出对齐人类的偏好. 
  这方面的方法有很多, 有DPO, PPO什么的, 都可以用.
