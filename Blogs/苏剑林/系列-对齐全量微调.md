# 对齐全量微调! 最精彩的 LoRA 改进

注: 基于原博客进行内容重排.

## 前言

众所周知, LoRA 是一种常见的参数高效的微调方法, 在文章 "梯度视角下的 LoRA: 简介, 分析, 猜测及推广" 中做过简单介绍.
LoRA 使用低秩分解来降低微调参数量, 节省微调显存, 同时训练好的权重可以合并到原始权重上, 推理架构不需要作出改变, 是一种训练和推理都比较友好的微调方案.

此外, 在文章 "配置不同的学习率, LoRA 还能再涨一点?" 中讨论了 LoRA 的不对称性, 指出给矩阵 $A$, $B$ 设置不同的学习率能取得更好的效果, 该结论被称为 LoRA+.

为了进一步提升效果, 研究人员还提出了不少其他 LoRA 变体: AdaLoRA, rsLoRA, DoRA, PiSSA 等, 这些改动都有一定道理, 但没有特别让人印象深刻的地方.

## LoRA

假设预训练参数为 $W_0\in \mathbb{R}^{n\times m}$, 那么**全量微调**时的更新量 $\delta W$ 自然也是一个 $n\times m$ 矩阵.
**LoRA** 将更新量约束为低秩矩阵来降低训练时的参数量, 即设 $\delta W= AB, W = W_0 + AB$, 其中 $A\in \mathbb{R}^{n\times r}, B\in \mathbb{R}^{r\times m}, r<<\min(n,m)$. 实际上会用新的参数 $W$ 替换模型原参数, 然后固定 $W_0$ 不变, 只训练 $A,B$.

为了使得 LoRA 的初始状态和预训练模型一致, 通常会将 $A$ 和 $B$ 其中之一进行全零初始化, 这样可以得到 $A_0B_0 = \mathbf{0}$, 从而初始的 $W$ 就是 $W_0$. 
注意这不是必须的, 如果 $A,B$ 都是非零初始化, 那么只需要设 $W=(W_0-A_0B_0)+AB$ 即可, 同样可以满足初始 $W$ 等于 $W_0$.

**需要指出的是, LoRA 往往只是显存不足的无奈之选, 因为一般情况下全量微调的效果都会优于 LoRA.**
所以若算力足够并且要追求最佳效果, 应优先选择全量微调. 
这一点也是 LoRA-GA 的假设之一, 因为它的改进方向就是向全量微调对齐.

**使用 LoRA 的另一个场景是有大量微型定制化需求**, 需要存下非常多的微调结果, 此时使用 LoRA 能够减少储存成本.

## LoRA-GA

于 2024 年 07 月 06 日提出的 [LoRA-GA: Low-Rank Adaptation with Gradient Approximation](../../Modules/LoRA/2024.07.06_LoRA-GA.md) 让笔者眼前一亮, 仅仅看了摘要就有种必然有效的感觉, 仔细阅读后更觉得是至今最精彩的 LoRA 改进.

LoRA-GA 提出了一个非常深刻的优化点: 通过 $W=(W_0-A_0B_0)+AB$ 可以保证 $W$ 的初始值等于 $W_0$. 那么能否**进一步调整 $A_0$ 和 $B_0$ 使得 LoRA 和全量微调在后续训练中也尽可能地近似**? 比如让经过第一步优化后的 $W_1$ 尽可能相等.

仔细思考越觉得这个优化角度直击本质, LoRA 的目标就是以小博大, 希望接近全量微调的效果. 那么尽可能对齐全量微调的后续更新结果就是最正确的改进方向.
从逼近的角度看, $W$ 的初始值等于 $W_0$ 相当于全量微调的零阶近似, 保持后面的 $W_1, W_2, \cdots$ 接近则相当于更高阶的近似, 是合情合理的选择.

具体来说, 假设优化器是 SGD, 那么对于全量微调有:
$$
    W_1 = W_0 - \eta \dfrac{\partial \mathcal{L}}{\partial W_0}.
$$

其中 $\mathcal{L}$ 是损失函数, $\eta$ 是学习率.

如果是 LoRA 则有:

$$
    A_1 = A_0 - \eta\dfrac{\partial \mathcal{L}}{\partial A_0} = A_0 - \eta\dfrac{\partial \mathcal{L}}{\partial W_0}B_0^{\mathsf{T}},
$$

$$
    B_1 = B_0 - \eta\dfrac{\partial \mathcal{L}}{\partial B_0} = B_0 - \eta A_0^{\mathsf{T}} \dfrac{\partial \mathcal{L}}{\partial W_0},
$$

$$
    W_1 = W_0 - A_0B_0 + A_1B_1\approx W_0 - \eta (A_0A_0^{\mathsf{T}}\dfrac{\partial \mathcal{L}}{\partial W_0} + \dfrac{\partial \mathcal{L}}{\partial W_0} B_0^{\mathsf{T}}B_0).
$$

最后这步近似省略了 $\eta$ 的二阶项.

现在两个 $W_1$ 具有相似的形式, 为了让它们尽可能近似, 可以考虑最小化

$$
    \arg\min_{A_0,B_0}\left\| A_0 A_0^{\mathsf{T}} \dfrac{\partial \mathcal{L}}{\partial W_0} + \dfrac{\partial \mathcal{L}}{\partial W_0} B_0^{\mathsf{T}}B_0 - \dfrac{\partial \mathcal{L}}{\partial W_0} \right\|_F^2.
$$

其中 $\| \cdot \|_F^2$ 表示 Frobenius 范数的平方, 即矩阵每个元素的平方和.

下面用 $G_0$ 代替 $\dfrac{\partial \mathcal{L}}{\partial W_0}$, 则目标函数可以写为:

$$
    \arg\min_{A_0,B_0}\left\| A_0 A_0^{\mathsf{T}} G_0 + G_0 B_0^{\mathsf{T}}B_0 - G_0 \right\|_F^2.
$$

注意到 $A_0A_0^{\mathsf{T}}G_0$, $G_0B_0^{\mathsf{T}}B_0$ 的秩顶多为 $r$, 相加之后的秩顶多为 $2r$.
下面假设 $2r < \min(n,m)$, 所以上述目标相当于寻找 $G_0$ 的一个秩不超过 $2r$ 的最优近似.

先考虑 $G_0$ 是非负对角矩阵的情形, 并且对角线元素已经按照从大到小的顺序排列.
这个例子很简单, 它的秩不超过 $2r$ 的最优近似就是只保留对角线前 $2r$ 个元素的新对角矩阵, 这一结论叫做 **Eckart-Young 定理**, 而能让 $A_0A_0^{\mathsf{T}}G_0 + G_0B_0^{\mathsf{T}}B_0$ 只保留 $G_0$ 的前 $2r$ 个对角线元素的 $A_0, B_0$ 可以是分块矩阵: $A_0= (I_n)_{[:,r]}$, $B_0=(I_m)_{[r:2r,:]}$, 下标 $[:,r]$ 表示前 $r$ 列, $[r:2r,:]$ 表示第 $r+1$ 到第 $2r$ 列.

注意这里是 "可以是", 即解不唯一. 也就是把 $G_0$ 的前 $2r$ 个对角元挑出来, $A_0A_0^{\mathsf{T}}G_0$ 和 $G_0B_0^{\mathsf{T}}B_0$ 各挑一半, 至于如何分配就无所谓了.
上面给出的解对应的是 $A_0A_0^{\mathsf{T}}G_0$ 取前 $r$ 个, $G_0B_0^{\mathsf{T}}B_0$ 取 $r+1 \sim 2r$ 个.

当 $G_0$ 不是对角矩阵时, 将之进行 SVD 分解为 $U\Sigma V$, 其中 $U\in \mathbb{R}^{n\times n}, V\in \mathbb{R}^{m\times m}$ 为正交矩阵, $\Sigma\in \mathbb{R}^{n\times m}$ 为对角矩阵, 对角线元素非负且从大到小排列, 代入得到

$$
\begin{aligned}
    &\left\| A_0 A_0^{\mathsf{T}} G_0 + G_0 B_0^{\mathsf{T}}B_0 - G_0 \right\|_F^2\\
    =& \left\| A_0 A_0^{\mathsf{T}} U\Sigma V + U\Sigma V B_0^{\mathsf{T}}B_0 - U\Sigma V \right\|_F^2\\
    =& \left\| \textcolor{red}{U U^{\mathsf{T}}} A_0 A_0^{\mathsf{T}} U\Sigma V + U\Sigma V B_0^{\mathsf{T}}B_0 \textcolor{red}{V^{\mathsf{T}}V} - U\Sigma V \right\|_F^2\\
    =& \left\| \textcolor{red}{U}(\textcolor{red}{U^{\mathsf{T}}} A_0) (U^{\mathsf{T}}A_0)^{\mathsf{T}}\Sigma V + U\Sigma (B_0 V^{\mathsf{T}})^{\mathsf{T}}(B_0 \textcolor{red}{V^{\mathsf{T}}})\textcolor{red}{V} - U\Sigma V \right\|_F^2\\
    =& \left\| U \textcolor{cyan}{[}(\textcolor{red}{U^{\mathsf{T}}} A_0) (U^{\mathsf{T}}A_0)^{\mathsf{T}}\Sigma + \Sigma (B_0 V^{\mathsf{T}})^{\mathsf{T}}(B_0 \textcolor{red}{V^{\mathsf{T}}}) - \Sigma\textcolor{cyan}{]} V \right\|_F^2\\
    =& \left\| (\textcolor{red}{U^{\mathsf{T}}} A_0) (U^{\mathsf{T}}A_0)^{\mathsf{T}}\Sigma + \Sigma (B_0 V^{\mathsf{T}})^{\mathsf{T}}(B_0 \textcolor{red}{V^{\mathsf{T}}}) - \Sigma \right\|_F^2\\
\end{aligned}
$$

上述推导有因为正交变换不改变 Frobenius 范数.
经过这样的转换, 发现逼近的对象重新转变为对角阵 $\Sigma$, 自变量则变成了 $U^{\mathsf{T}}A_0$, $B_0 V^{\mathsf{T}}$, 那么按照 $G_0$ 是对角阵时所给出的解, 得到:

$$
    A_0 = U(I_n)_{[:,r]} = U_{[:,r]}
$$

$$
    B_0 = (I_m)_{[r:2r,:]}V = V_{[r:2r,:]}
$$

现在就得到了 LoRA 的一种初始化方法:

LoRA-GA 选取一批样本, 计算初始梯度 $G_0=\nabla_{W_0}\mathcal{L}$, 对梯度进行 SVD 得到 $G_0=U\Sigma V$, 取 $U$ 的前 $r$ 列初始化 $A$, 取 $V$ 的第 $r+1\sim 2r$ 行初始化 $B$.

这样 LoRA + SGD 得到的 $W_1$ 就和全量微调的 $W_1$ 尽可能相近.

此外梯度最重要的是方向, 模长不太重要, 所以初始化结果时还可以乘以 `scale`, LoRA 本身也可以乘以 `scale`, 即 $W=(W_0-\lambda A_0B_0) + \lambda AB$, 这些都是 LoRA 常见的超参数.

形式上和 LoRA-GA 比较相似的时 PiSSA, 它是对 $W_0$ 进行 SVD 来初始化 $A,B$, 这在理论支持上就不如 LoRA-GA 了, 是一个纯粹的经验选择.

当然目前的推导都基于 SGD 优化器的假设, 对于更常用的 Adam 优化器, 结论理论上需要做出一些改变.
这在文章 "配置不同的学习率, LoRA 还能再涨一点?" 中讨论过, 对于 Adam 来说, 第一步优化结果是
$$
    W_1 = W_0 -\eta \text{sign}(G_0)
$$

此时的优化目标为

$$
    \arg\min_{A_0,B_0}\left\| A_0 \text{sign}(A_0^{\mathsf{T}} G_0) + \text{sign}(G_0 B_0^{\mathsf{T}})B_0 - \text{sign}(G_0) \right\|_F^2.
$$

由于符号函数的存在, 无法求出其解析解, 所以对于 Adam 的理论分析只能止步于此.

此时对于 Adam 优化器有三种选择:
1. 直接引用 SGD 的结果, 期望能在 Adam 中发挥出相同效果;
2. 用优化器直接去优化上述优化目标, 计算量尚能接受;
3. 将 $G_0$ 替换为 $\text{sign}(G_0)$ 然后代入 SGD 的结论, 可能更贴合 Adam.

原文选择的是第一种, 实验结果也支持这一选择.

### 结果

实验结果还是比较好的, 在 GLUE 上取得了最接近全量微调的效果.
平均来说, 训练数据量越少, 相对提升的幅度越大, 这表明 LoRA-GA 对齐全量微调的策略, 不仅有助于提高最终效果, 还能提高训练效率, 即可以以更少的训练步数达到更优的效果.

注意使用 LoRA 的主要场景是显存不足, 但 LoRA 的初始化需要求出所有训练参数的完整梯度, 这可能会由于显存不足而无法实现.
为此, 原论文提出的技巧是可以一个个参数串行地求梯度, 而不是同时求所有训练参数的梯度, 这样就可以把单步计算的显存降下来. 串行求梯度虽然会降低效率, 但是初始化本身也是一次性的工作, 因此稍微慢点也无妨.
至于如何实现这一操作, 不同框架有不同的方法, 此处不展开讨论.

---

LoRA-GA 从理论上来说只能尽量对齐第一步更新后的 $W_1$, 那么后面的 $W_2, W_3, \cdots$ 就可以不管了吗?
下面介绍的 LoRA-Pro 正好能够回答这一问题, 它对齐的是每一步的梯度, 从而对齐整条优化轨迹.

---

## LoRA-Pro

于 2024 年 07 月 25 日提出的 [LoRA-Pro: Are Low-Rank Adapters Properly Optimized?](../../Modules/LoRA/2024.07.25_LoRA-Pro.md) 希望对齐全量微调和 LoRA 的每一个 $W_t$.

并不是对每一步都最小化 $\| A_tA_t^{\mathsf{T}}G_t + B_tB_t^{\mathsf{T}}G_t - G_t \|_F^2$ 就能实现这一点的, 因为 $A_t$ 和 $B_t$ 都是由优化器根据 $A_{t-1}, B_{t-1}$ 和梯度确定的, 并不是可以自由调节的参数.

LoRA-Pro 的思路则是既然 $A_t$ 和 $B_t$ 是由优化器根据 $A_{t-1}, B_{t-1}$ 和梯度确定的, 既然不能修改所依据的量, 那就修改优化器本身.

具体来说, 将 $A_t$ 和 $B_t$ 的更新规则改为:

$$
    A_{t+1} = A_t - \eta H_{A,t}, B_{t+1} = B_t - \eta H_{B,t}
$$

其中 $H_{A,t}$ 和 $H_{B,t}$ 待定, 但形状和 $A,B$ 一致.

现在可以写出:

$$
    W_{t+1} = W_t - A_t B_t + A_{t+1}B_{t+1}\approx W_t - \eta (H_{A,t}B_t + A_t H_{B,t})
$$

此时就可以调整 $H_{A,t}, H_{B,t}$ 让这个 $W_{t+1}$ 和 SGD 的 $W_{t+1}$ 尽可能地接近.

$$
    \arg\min_{H_{A,t}, H_{B,t}} \| H_{A,t}B_t + A_t H_{B,t} - G_t \|_F^2
$$

下面求解这个优化问题.