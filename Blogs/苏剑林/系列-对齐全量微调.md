# 对齐全量微调! 最精彩的 LoRA 改进

- 原文: https://spaces.ac.cn/archives/10226
- 时间: 2024.07.12

注: 基于原博客进行内容重排.

## 前言

众所周知, LoRA 是一种常见的参数高效的微调方法, 在文章 "梯度视角下的 LoRA: 简介, 分析, 猜测及推广" 中做过简单介绍.
LoRA 使用低秩分解来降低微调参数量, 节省微调显存, 同时训练好的权重可以合并到原始权重上, 推理架构不需要作出改变, 是一种训练和推理都比较友好的微调方案.

此外, 在文章 "配置不同的学习率, LoRA 还能再涨一点?" 中讨论了 LoRA 的不对称性, 指出给矩阵 $A$, $B$ 设置不同的学习率能取得更好的效果, 该结论被称为 LoRA+.

为了进一步提升效果, 研究人员还提出了不少其他 LoRA 变体: AdaLoRA, rsLoRA, DoRA, PiSSA 等, 这些改动都有一定道理, 但没有特别让人印象深刻的地方.

## LoRA

假设预训练参数为 $W_0\in \mathbb{R}^{n\times m}$, 那么**全量微调**时的更新量 $\delta W$ 自然也是一个 $n\times m$ 矩阵.
**LoRA** 将更新量约束为低秩矩阵来降低训练时的参数量, 即设 $\delta W= AB, W = W_0 + AB$, 其中 $A\in \mathbb{R}^{n\times r}, B\in \mathbb{R}^{r\times m}, r<<\min(n,m)$. 实际上会用新的参数 $W$ 替换模型原参数, 然后固定 $W_0$ 不变, 只训练 $A,B$.

为了使得 LoRA 的初始状态和预训练模型一致, 通常会将 $A$ 和 $B$ 其中之一进行全零初始化, 这样可以得到 $A_0B_0 = \mathbf{0}$, 从而初始的 $W$ 就是 $W_0$. 
注意这不是必须的, 如果 $A,B$ 都是非零初始化, 那么只需要设 $W=(W_0-A_0B_0)+AB$ 即可, 同样可以满足初始 $W$ 等于 $W_0$.

**需要指出的是, LoRA 往往只是显存不足的无奈之选, 因为一般情况下全量微调的效果都会优于 LoRA.**
所以若算力足够并且要追求最佳效果, 应优先选择全量微调. 
这一点也是 LoRA-GA 的假设之一, 因为它的改进方向就是向全量微调对齐.

**使用 LoRA 的另一个场景是有大量微型定制化需求**, 需要存下非常多的微调结果, 此时使用 LoRA 能够减少储存成本.

## LoRA-GA

原文地址: 

于 2024 年 07 月 06 日提出的 [LoRA-GA: Low-Rank Adaptation with Gradient Approximation](../../Modules/LoRA/2024.07.06_LoRA-GA.md) 让笔者眼前一亮, 仅仅看了摘要就有种必然有效的感觉, 仔细阅读后更觉得是至今最精彩的 LoRA 改进.

LoRA-GA 提出了一个非常深刻的优化点: 通过 $W=(W_0-A_0B_0)+AB$ 可以保证 $W$ 的初始值等于 $W_0$. 那么能否**进一步调整 $A_0$ 和 $B_0$ 使得 LoRA 和全量微调在后续训练中也尽可能地近似**? 比如让经过第一步优化后的 $W_1$ 尽可能相等.

仔细思考越觉得这个优化角度直击本质, LoRA 的目标就是以小博大, 希望接近全量微调的效果. 那么尽可能对齐全量微调的后续更新结果就是最正确的改进方向.
从逼近的角度看, $W$ 的初始值等于 $W_0$ 相当于全量微调的零阶近似, 保持后面的 $W_1, W_2, \cdots$ 接近则相当于更高阶的近似, 是合情合理的选择.

具体来说, 假设优化器是 SGD, 那么对于全量微调有:
$$
    W_1 = W_0 - \eta \dfrac{\partial \mathcal{L}}{\partial W_0}.
$$

其中 $\mathcal{L}$ 是损失函数, $\eta$ 是学习率.

如果是 LoRA 则有:

$$
    A_1 = A_0 - \eta\dfrac{\partial \mathcal{L}}{\partial A_0} = A_0 - \eta\dfrac{\partial \mathcal{L}}{\partial W_0}B_0^{\mathsf{T}},
$$

$$
    B_1 = B_0 - \eta\dfrac{\partial \mathcal{L}}{\partial B_0} = B_0 - \eta A_0^{\mathsf{T}} \dfrac{\partial \mathcal{L}}{\partial W_0},
$$

$$
    W_1 = W_0 - A_0B_0 + A_1B_1\approx W_0 - \eta (A_0A_0^{\mathsf{T}}\dfrac{\partial \mathcal{L}}{\partial W_0} + \dfrac{\partial \mathcal{L}}{\partial W_0} B_0^{\mathsf{T}}B_0).
$$

最后这步近似省略了 $\eta$ 的二阶项.

现在两个 $W_1$ 具有相似的形式, 为了让它们尽可能近似, 可以考虑最小化

$$
    \arg\min_{A_0,B_0}\left\| A_0 A_0^{\mathsf{T}} \dfrac{\partial \mathcal{L}}{\partial W_0} + \dfrac{\partial \mathcal{L}}{\partial W_0} B_0^{\mathsf{T}}B_0 - \dfrac{\partial \mathcal{L}}{\partial W_0} \right\|_F^2.
$$

其中 $\| \cdot \|_F^2$ 表示 Frobenius 范数的平方, 即矩阵每个元素的平方和.

下面用 $G_0$ 代替 $\dfrac{\partial \mathcal{L}}{\partial W_0}$, 则目标函数可以写为:

$$
    \arg\min_{A_0,B_0}\left\| A_0 A_0^{\mathsf{T}} G_0 + G_0 B_0^{\mathsf{T}}B_0 - G_0 \right\|_F^2.
$$

注意到 $A_0A_0^{\mathsf{T}}G_0$, $G_0B_0^{\mathsf{T}}B_0$ 的秩顶多为 $r$, 相加之后的秩顶多为 $2r$.
下面假设 $2r < \min(n,m)$, 所以上述目标相当于寻找 $G_0$ 的一个秩不超过 $2r$ 的最优近似.

先考虑 $G_0$ 是非负对角矩阵的情形, 并且对角线元素已经按照从大到小的顺序排列.
这个例子很简单, 它的秩不超过 $2r$ 的最优近似就是只保留对角线前 $2r$ 个元素的新对角矩阵, 这一结论叫做 **Eckart-Young 定理**, 而能让 $A_0A_0^{\mathsf{T}}G_0 + G_0B_0^{\mathsf{T}}B_0$ 只保留 $G_0$ 的前 $2r$ 个对角线元素的 $A_0, B_0$ 可以是分块矩阵: $A_0= (I_n)_{[:,r]}$, $B_0=(I_m)_{[r:2r,:]}$, 下标 $[:,r]$ 表示前 $r$ 列, $[r:2r,:]$ 表示第 $r+1$ 到第 $2r$ 列.

注意这里是 "可以是", 即解不唯一. 也就是把 $G_0$ 的前 $2r$ 个对角元挑出来, $A_0A_0^{\mathsf{T}}G_0$ 和 $G_0B_0^{\mathsf{T}}B_0$ 各挑一半, 至于如何分配就无所谓了.
上面给出的解对应的是 $A_0A_0^{\mathsf{T}}G_0$ 取前 $r$ 个, $G_0B_0^{\mathsf{T}}B_0$ 取 $r+1 \sim 2r$ 个.

当 $G_0$ 不是对角矩阵时, 将之进行 SVD 分解为 $U\Sigma V$, 其中 $U\in \mathbb{R}^{n\times n}, V\in \mathbb{R}^{m\times m}$ 为正交矩阵, $\Sigma\in \mathbb{R}^{n\times m}$ 为对角矩阵, 对角线元素非负且从大到小排列, 代入得到

$$
\begin{aligned}
    &\left\| A_0 A_0^{\mathsf{T}} G_0 + G_0 B_0^{\mathsf{T}}B_0 - G_0 \right\|_F^2\\
    =& \left\| A_0 A_0^{\mathsf{T}} U\Sigma V + U\Sigma V B_0^{\mathsf{T}}B_0 - U\Sigma V \right\|_F^2\\
    =& \left\| \textcolor{red}{U U^{\mathsf{T}}} A_0 A_0^{\mathsf{T}} U\Sigma V + U\Sigma V B_0^{\mathsf{T}}B_0 \textcolor{red}{V^{\mathsf{T}}V} - U\Sigma V \right\|_F^2\\
    =& \left\| \textcolor{red}{U}(\textcolor{red}{U^{\mathsf{T}}} A_0) (U^{\mathsf{T}}A_0)^{\mathsf{T}}\Sigma V + U\Sigma (B_0 V^{\mathsf{T}})^{\mathsf{T}}(B_0 \textcolor{red}{V^{\mathsf{T}}})\textcolor{red}{V} - U\Sigma V \right\|_F^2\\
    =& \left\| U \textcolor{cyan}{[}(\textcolor{red}{U^{\mathsf{T}}} A_0) (U^{\mathsf{T}}A_0)^{\mathsf{T}}\Sigma + \Sigma (B_0 V^{\mathsf{T}})^{\mathsf{T}}(B_0 \textcolor{red}{V^{\mathsf{T}}}) - \Sigma\textcolor{cyan}{]} V \right\|_F^2\\
    =& \left\| (\textcolor{red}{U^{\mathsf{T}}} A_0) (U^{\mathsf{T}}A_0)^{\mathsf{T}}\Sigma + \Sigma (B_0 V^{\mathsf{T}})^{\mathsf{T}}(B_0 \textcolor{red}{V^{\mathsf{T}}}) - \Sigma \right\|_F^2\\
\end{aligned}
$$

上述推导有因为正交变换不改变 Frobenius 范数.
经过这样的转换, 发现逼近的对象重新转变为对角阵 $\Sigma$, 自变量则变成了 $U^{\mathsf{T}}A_0$, $B_0 V^{\mathsf{T}}$, 那么按照 $G_0$ 是对角阵时所给出的解, 得到:

$$
    A_0 = U(I_n)_{[:,r]} = U_{[:,r]}
$$

$$
    B_0 = (I_m)_{[r:2r,:]}V = V_{[r:2r,:]}
$$

现在就得到了 LoRA 的一种初始化方法:

LoRA-GA 选取一批样本, 计算初始梯度 $G_0=\nabla_{W_0}\mathcal{L}$, 对梯度进行 SVD 得到 $G_0=U\Sigma V$, 取 $U$ 的前 $r$ 列初始化 $A$, 取 $V$ 的第 $r+1\sim 2r$ 行初始化 $B$.

这样 LoRA + SGD 得到的 $W_1$ 就和全量微调的 $W_1$ 尽可能相近.

此外梯度最重要的是方向, 模长不太重要, 所以初始化结果时还可以乘以 `scale`, LoRA 本身也可以乘以 `scale`, 即 $W=(W_0-\lambda A_0B_0) + \lambda AB$, 这些都是 LoRA 常见的超参数.

形式上和 LoRA-GA 比较相似的时 PiSSA, 它是对 $W_0$ 进行 SVD 来初始化 $A,B$, 这在理论支持上就不如 LoRA-GA 了, 是一个纯粹的经验选择.

当然目前的推导都基于 SGD 优化器的假设, 对于更常用的 Adam 优化器, 结论理论上需要做出一些改变.
这在文章 "配置不同的学习率, LoRA 还能再涨一点?" 中讨论过, 对于 Adam 来说, 第一步优化结果是
$$
    W_1 = W_0 -\eta \text{sign}(G_0)
$$

此时的优化目标为

$$
    \arg\min_{A_0,B_0}\left\| A_0 \text{sign}(A_0^{\mathsf{T}} G_0) + \text{sign}(G_0 B_0^{\mathsf{T}})B_0 - \text{sign}(G_0) \right\|_F^2.
$$

由于符号函数的存在, 无法求出其解析解, 所以对于 Adam 的理论分析只能止步于此.

此时对于 Adam 优化器有三种选择:
1. 直接引用 SGD 的结果, 期望能在 Adam 中发挥出相同效果;
2. 用优化器直接去优化上述优化目标, 计算量尚能接受;
3. 将 $G_0$ 替换为 $\text{sign}(G_0)$ 然后代入 SGD 的结论, 可能更贴合 Adam.

原文选择的是第一种, 实验结果也支持这一选择.

### 结果

实验结果还是比较好的, 在 GLUE 上取得了最接近全量微调的效果.
平均来说, 训练数据量越少, 相对提升的幅度越大, 这表明 LoRA-GA 对齐全量微调的策略, 不仅有助于提高最终效果, 还能提高训练效率, 即可以以更少的训练步数达到更优的效果.

注意使用 LoRA 的主要场景是显存不足, 但 LoRA 的初始化需要求出所有训练参数的完整梯度, 这可能会由于显存不足而无法实现.
为此, 原论文提出的技巧是可以一个个参数串行地求梯度, 而不是同时求所有训练参数的梯度, 这样就可以把单步计算的显存降下来. 串行求梯度虽然会降低效率, 但是初始化本身也是一次性的工作, 因此稍微慢点也无妨.
至于如何实现这一操作, 不同框架有不同的方法, 此处不展开讨论.

---

- 原文: https://spaces.ac.cn/archives/10266
- 时间: 2024.07.29

LoRA-GA 从理论上来说只能尽量对齐第一步更新后的 $W_1$, 那么后面的 $W_2, W_3, \cdots$ 就可以不管了吗?
下面介绍的 LoRA-Pro 正好能够回答这一问题, 它对齐的是每一步的梯度, 从而对齐整条优化轨迹.

---

## LoRA-Pro

于 2024 年 07 月 25 日提出的 [LoRA-Pro: Are Low-Rank Adapters Properly Optimized?](../../Modules/LoRA/2024.07.25_LoRA-Pro.md) 希望对齐全量微调和 LoRA 的每一个 $W_t$.

并不是对每一步都最小化 $\| A_tA_t^{\mathsf{T}}G_t + B_tB_t^{\mathsf{T}}G_t - G_t \|_F^2$ 就能实现这一点的, 因为 $A_t$ 和 $B_t$ 都是由优化器根据 $A_{t-1}, B_{t-1}$ 和梯度确定的, 并不是可以自由调节的参数.

LoRA-Pro 的思路则是**既然 $A_t$ 和 $B_t$ 是由优化器根据 $A_{t-1}, B_{t-1}$ 和梯度确定的, 既然不能修改所依据的量, 那就修改优化器本身**.

具体来说, 将 $A_t$ 和 $B_t$ 的更新规则改为:

$$
    A_{t+1} = A_t - \eta H_{A,t}, B_{t+1} = B_t - \eta H_{B,t}
$$

其中 $H_{A,t}$ 和 $H_{B,t}$ 待定, 但形状和 $A,B$ 一致.

现在可以写出:

$$
    W_{t+1} = W_t - A_t B_t + A_{t+1}B_{t+1}\approx W_t - \eta (H_{A,t}B_t + A_t H_{B,t})
$$

此时就可以调整 $H_{A,t}, H_{B,t}$ 让这个 $W_{t+1}$ 和 SGD 的 $W_{t+1}$ 尽可能地接近.

$$
    \arg\min_{H_{A,t}, H_{B,t}} \| H_{A,t}B_t + A_t H_{B,t} - G_t \|_F^2
$$

下面求解这个优化问题.

由于 $H_{A,t}$ 和 $H_{B,t}$ 之间没有约束, 所以这两者的优化是相互独立的, 因此可以采取先优化 $H_{A,t}$ 再优化 $H_{B,t}$ 的策略.
当我们优化 $H_{A,t}$ 时, $H_{B,t}$ 相当于是一个常数, 可以先考虑简化的等价命题:

$$
    \arg\min_{H} \| HB-X \|_F^2
$$

其中 $H\in \mathbb{R}^{n\times r}, B\in :\mathbb{R}^{r\times m}, X\in \mathbb{R}^{n\times m}$.

- 如果 $r=m$ 且 $B$ 可逆, 那么可以直接变为解方程组 $HB=X$, 即 $H=XB^{-1}$.

- 如果 $r<m$, 就需要用其他优化手段, 注意到 $HB-X$ 关于 $H$ 是线性的, 所以这实质上就是线性回归的最小二乘问题, 它是有解析解的.
对于目标函数 $l = \| HB-X \|_F^2$, 直接求 $H$ 的导数得到:
$$
    \dfrac{\partial l}{\partial H} = 2(HB-X)B^{\mathsf{T}} = 2(HBB^{\mathsf{T}}-XB^{\mathsf{T}})
$$
令偏导为零, 可以得到
$$
    H = XB^{\mathsf{T}}(BB^{\mathsf{T}})^{-1}
$$
$B^{\mathsf{T}}(BB^{\mathsf{T}})^{-1}$ 为矩阵 $B$ 的伪逆.
同理, $\| AH-X \|_F^2$ 对 $H$ 的导数就是 $2A^{\mathsf{T}}(AH-X)$, 由此可以得到
$$
    H = (A^{\mathsf{T}}A)^{-1}A^{\mathsf{T}}X
$$

---

有了上面两个等式, 就可以着手求解
$$
    \arg\min_{H_{A,t}, H_{B,t}} \| H_{A,t}B_t + A_t H_{B,t} - G_t \|_F^2
$$

注意: 下面忽略下标 $t$

首先固定 $H_{B}$, 那么根据第一个等式, 得到

$$
    H_{A} = (G_t - AH_{B})B^{\mathsf{T}}(BB^{\mathsf{T}})^{-1}
$$

注意目标函数具有一个不变性:

$$
    \| H_{A}B + A H_{B} - G \|_F^2 = \| (H_{A}+AC)B + A(H_{B}-CB) - G\|_F^2
$$

其中 $C\in\mathbb{R}^{r\times r}$.

这个不变性的意思是 $H_A$ 的解可以加减任意具有 $AC$ 形式的矩阵, 只需要 $H_B$ 减加相应的 $CB$ 即可.

根据这个不变性, 可以将 $H_{A}$ 的求解结果简化为

$$
    H_{A} = G B^{\mathsf{T}}(BB^{\mathsf{T}})^{-1}
$$

然后代回目标函数中得到

$$
\begin{aligned}
    &\arg\min_{H_{B}} \| H_{A}B + A H_{B} - G \|_F^2\\
    =&\arg\min_{H_{B}} \| \textcolor{blue}{(G B^{\mathsf{T}}(BB^{\mathsf{T}})^{-1})}B + A H_{B} - G \|_F^2\\
    =&\arg\min_{H_{B}} \| A H_{B} + G [\textcolor{blue}{(B^{\mathsf{T}}(BB^{\mathsf{T}})^{-1})}B - I] \|_F^2\\
\end{aligned}
$$

那么对应的 $H_B$ 结果为

$$
    H_B = (A^{\mathsf{T}}A)^{-1}A^{\mathsf{T}} G [I - B^{\mathsf{T}}(BB^{\mathsf{T}})^{-1}B]
$$

可以发现 $GB^{\mathsf{T}}$ 是 $A$ 的梯度 $G_A$, $A^{\mathsf{T}}G$ 是 $B$ 的梯度 $G_B$, 再次利用不变性, 可以得到:

$$
    H_A = G_A (BB^{\mathsf{T}})^{-1} + AC\\
    H_B = (A^{\mathsf{T}}A)^{-1} G_B (I-B^{\mathsf{T}}(BB^{\mathsf{T}})^{-1}B) - CB 
$$

### C 的选择

可以发现虽然求解出了 $H_A, H_B$ 的形式, 但解不是唯一的, 它有一个可以自由选择的参数矩阵 $C$.
那么就可以选择合适的 $C$ 来使得最终的 $H_A, H_B$ 具备一些期望的特性.

---

例如现在的 $H_A,H_B$ 是不大对称的, $H_B$ 多了前面一项, 可以将之平均分配到 $H_A$ 和 $H_B$ 中, 使得更对称一些, 等价于 $C = -\dfrac{1}{2} (A^{\mathsf{T}}A)^{-1} G_B B^{\mathsf{T}}(BB^{\mathsf{T}})^{-1}$:

$$
\begin{aligned}
    H_A &= G_A (BB^{\mathsf{T}})^{-1} + AC\\
    &= G_A (BB^{\mathsf{T}})^{-1} + A\cdot -\dfrac{1}{2} (A^{\mathsf{T}}A)^{-1} G_B B^{\mathsf{T}}(BB^{\mathsf{T}})^{-1} \\
    &= G_A (BB^{\mathsf{T}})^{-1} -\dfrac{1}{2} A(A^{\mathsf{T}}A)^{-1} G_B B^{\mathsf{T}}(BB^{\mathsf{T}})^{-1} \\
    &= G_A (BB^{\mathsf{T}})^{-1} -\dfrac{1}{2} A(A^{\mathsf{T}}A)^{-1} A^{\mathsf{T}}G B^{\mathsf{T}}(BB^{\mathsf{T}})^{-1} \\
    &= G_A (BB^{\mathsf{T}})^{-1} -\dfrac{1}{2} A(A^{\mathsf{T}}A)^{-1} A^{\mathsf{T}}G_A (BB^{\mathsf{T}})^{-1} \\
    &=[I-\dfrac{1}{2} A(A^{\mathsf{T}}A)^{-1} A^{\mathsf{T}} ] G_A (BB^{\mathsf{T}})^{-1}
\end{aligned}
$$

$$
\begin{aligned}
    H_B &= (A^{\mathsf{T}}A)^{-1} G_B (I-B^{\mathsf{T}}(BB^{\mathsf{T}})^{-1}B) - CB\\
    &=(A^{\mathsf{T}}A)^{-1} G_B (I-B^{\mathsf{T}}(BB^{\mathsf{T}})^{-1}B) +\dfrac{1}{2} (A^{\mathsf{T}}A)^{-1} G_B B^{\mathsf{T}}(BB^{\mathsf{T}})^{-1} B\\
    &=(A^{\mathsf{T}}A)^{-1} G_B (I-\dfrac{1}{2}B^{\mathsf{T}}(BB^{\mathsf{T}})^{-1}B)
\end{aligned}
$$

这个 $C$ 也是以下两个优化问题的解:

优化目标一:

$$
    \arg\min_{C} \| H_A B - A H_B \|_F^2
$$

这一目标可以理解为让 $A,B$ 对最终效果的贡献尽可能一样. 这和 "配置不同的学习率, LoRA 还能再涨一点?" 中的假设有一定的异曲同工之妙.

对这个优化目标进行求导得到:

$$
\begin{aligned}
    \dfrac{\partial l}{\partial C} &= 4A^{\mathsf{T}}(H_A B - A H_B) B^{\mathsf{T}}\\
    &= 4 A^{\mathsf{T}} [G_A (BB^{\mathsf{T}})^{-1}B + 2ACB] B^{\mathsf{T}}
\end{aligned}
$$

令偏导为零, 则可以得到相同的 $C$.

优化目标二:
$$
    \arg\min_{C} \| H_A B - G \|_F^2 + \| A H_B - G \|_F^2
$$

第二个目标则是让 $H_A B$, $A H_B$ 都尽可能逼近完整的梯度 $G$.

---

LoRA-Pro 选择的 $C$ 略有不同, 是如下目标函数的最优解:

$$
    \arg\min_{C} \| H_A - G_A \|_F^2 + \| H_B - G_B \|_F^2
$$

意图是 $H_A, H_B$ 用于替代 $G_A, G_B$, 如果在能够达到相同效果的前提下, 相比 $G_A, G_B$ 的改动尽可能小, 不失为一个合理的选择.

同样求导令其为零, 化简得到:

$$
    A^{\mathsf{T}}AC + CBB^{\mathsf{T}} = - A^{\mathsf{T}}G_A (BB^{\mathsf{T}})^{-1}
$$

现在得到一个关于 $C$ 的一个方程, 称为 Sylvester 方程. 可以通过外积符号写出解析解, 但没有必要. 因为数值求解的复杂度比解析解的复杂度更低.

总的来说, $C$ 的选择都是让 $H_A, H_B$ 在某种视角下更加对称一些. 笔者认为不同选择之间不会有太明显的区别.

### 一般讨论

梳理一下前面讨论的内容.

现在模型还是常规的 LoRA, 目标则是希望每一步更新都能逼近全量微调的结果.
为此, 假设优化器为 SGD, 然后对比了同样 $W_t$ 下全量微调和 LoRA 所得到的 $W_{t+1}$, 发现要实现这个目标, 需要把更新过程中 $A,B$ 的梯度 $G_A,G_B$ 换成前面求解得到的 $H_A,H_B$.

前面讨论的内容是基于 SGD 优化器的, 但实践中更常用 Adam.
如果对 Adam 优化器重复前面的推导, 结果就是 $G$ 要换成全量微调下 Adam 的更新方向 $U$, 而 $U$ 需要用全量微调的梯度 $G$ 按照 Adam 的更新规则计算而来. 但 LoRA 是无法获得全量微调的梯度的, 只有 $G_A, G_B$.

可以考虑一个近似的方案, 用 $H_A B + AH_B$ 逼近 $G$, 来执行 Adam.

LoRA-Pro 使用的是 AdamW 优化器, 结果稍微复杂一些, 但更新算法无本质不同.

$$
\begin{aligned}
    G_A &= \dfrac{\partial L}{\partial A_{t-1}}\\
    G_B &= \dfrac{\partial L}{\partial B_{t-1}}\\
    H_A &= G_A (BB^{\mathsf{T}})^{-1}\\
    H_B &= (A^{\mathsf{T}}A)^{-1} G_B (I-B^{\mathsf{T}}(BB^{\mathsf{T}})^{-1}B)\\
    \hat{G} &= H_A B + AH_B\\
    M_t &= \beta_1 M_{t-1} + (1-\beta_1)\hat{G}_t\\
    V_t &= \beta_2 V_{t-1} + (1-\beta_2)\hat{G}_t^2\\
    \hat{M}_t &= \dfrac{M_t}{1-\beta_1^t}\\
    \hat{V}_t &= \dfrac{V_t}{1-\beta_2^t}\\
    U &= \dfrac{\hat{M}_t}{\sqrt{\hat{V}_t}+\epsilon}\\
    U_A &= UB^{\mathsf{T}}\\
    U_B &= A^{\mathsf{T}}U\\
    \hat{H}_A &= U_A (BB^{\mathsf{T}})^{-1} + AC\\
    \hat{H}_B &= (A^{\mathsf{T}}A)^{-1} U_B (I-B^{\mathsf{T}}(BB^{\mathsf{T}})^{-1}B) - CB\\
    A_t &= A_{t-1} - \eta \hat{H}_A\\
    B_t &= B_{t-1} - \eta \hat{H}_B
\end{aligned}
$$

然而, 这样的改动且不说引入的额外复杂度如何, 最大的问题在于里面的滑动更新变量 $M,V$ 和全量微调一样都是满秩的, 也就是说它的优化器相比全量微调并不节省内存, 仅仅是通过低秩分解节省了参数和梯度的部分显存, 相比常规的 LoRA 显存消耗还会有明显增加.

一个比较简单的方案是直接用 $H_A, H_B$ 替代 $G_A, G_B$, 这样节省的显存达到了最大.
不过此时的 Adam 理论基础不如 LoRA-Pro 的结果.

从实验结果来看, 包含了 $A^{\mathsf{T}}A$ 和 $BB^{\mathsf{T}}$ 的求逆, 所以很明显 $A,B$ 之一就不能用全零初始化, 比较符合直觉的是正交初始化, 即初始的 $A^{\mathsf{T}}A$ $BB^{\mathsf{T}}$ 是单位矩阵.
LoRA-GA 给出的初始化正好是正交初始化, 所以 LoRA-GA 和 LoRA-Pro 的结果互补了.