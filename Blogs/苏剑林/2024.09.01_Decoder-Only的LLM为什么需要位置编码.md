# Decoder-Only 的 LLM 为什么需要位置编码?

- 原文: https://kexue.fm/archives/10347
- 时间: 2024.09.01

总所周知, 目前主流的大语言模型, 都是基于因果注意力 (Causal Attention) 的仅解码器 (Decoder-Only) 模型.
在 ["2023.03.17 17_为什么现在的LLM都是Decoder-Only架构?"](2023.03.17_为什么现在的LLM都是Decoder-Only架构.md) 中也有过相关讨论.
而对于因果注意力, 已经有不少工作表明它不需要额外的位置编码 (简称 NoPE) 就可以取得非平凡的结果.
然而, 事实是主流的 Decoder-Only LLM 都还是加上了额外的位置编码, 如 [RoPE](../../Modules/PositionEmb/RoPE.md), [ALIBI](../../Modules/PositionEmb/ALIBI.md) 等.

那么问题来了, 既然不加位置编码也可以, 为什么主流的大语言模型反而都加上了呢?

本文以三个角度给出相应的解释.
1. 位置编码对于注意力机制的作用是什么? → 打破置换不变性, 引入先验认知.
2. NoPE 的因果注意力是如何实现位置编码的? → 将位置信息隐藏在计算注意力时的方差中.
3. NoPE 实现的位置编码有什么不足? → 对于长文本可能会存在位置分辨率不足, 效率较低, 注意力弥散等问题.

## 位置编码

在 BERT 盛行的年代, 有不少位置编码的工作被提出, 在 ["让研究人员绞尽脑汁的Transformer位置编码"]() 中总结过一些.
后来, 在 [系列-Transformer升级之路 1](系列-Transformer升级之路.md) 中试图以更贴近原理的视角来理解位置编码, 并得到了最早的正弦位置编码的一种理论解释, 直接启发了之后的旋转位置编码 RoPE.

简单来说, 位置编码最根本的作用是**打破注意力机制的置换不变性**.

在 BERT 时代, 主要用的是双向注意力, 基本形式为:

$$
    y_n = f(q_n; x_1,\cdots,x_L) = \dfrac{\sum_{m=1}^L \exp(q_n\cdot k_m) v_m}{\sum_{m=1}^L \exp(q_n\cdot k_m)}, \quad k_n/v_n = x_n W_{k/v} + b_{k/v}.
$$

假设 $\sigma_1,\cdots,\sigma_L$ 是 $\{1,\cdots,L\}$ 的任意排列, 那么置换不变性是指:

$$
    y_n = f(q_n; x_1,\cdots,x_L) = f(q_n;x_{\sigma_1},\cdots,x_{\sigma_L})
$$

即 $y_n$ 和 Key-Value 的顺序无关, 这和自然语言的特性不符, 所以要打破这种置换不变性.

用数据库类比, 没有位置编码的注意力机制就像是没有时间标签的数据库, 检索结果只和 Query 有关, 而位置编码相当于给数据库的 Item 按顺序打上时间标签, 使得检索结果还可以和 Item 顺序有关.

## 先验认知

位置编码的另一个作用是加入对注意力的先验认知, 或者赋予注意力学习到这些先验认知性质的能力.

首先是绝对位置编码:
- 正弦位置编码是由三角函数生成的绝对位置编码, 并且相邻的位置向量之间的相似度更高, 这隐含了相近的 Token 应该具有相近的 Embedding 的先验.
- BERT 所用的位置编码同样是绝对位置编码, 但它是由随机初始化然后作为参数来学习的, 也就是没有作出相近的假设, 但允许模型学到这种性质 (如果模型认为有必要).

更流行的相对位置编码, 其先验假设是 "相对位置比绝对位置更加重要".
- 早期的相对位置编码通常会做一个截断 (大于某个数值后的相对位置直接取同一个值), 这里的假设是 "远距离的相对位置可以不那么精确".
- T5 的位置编码更进一步, 将相对位置按照对数形式分桶处理, 实现了 "越远的相对位置越模糊" 的效果.
- 有些相对位置会直接给 Token 的重要性加上先验, 比如 ALIBI 隐含了 "越远的 Token 平均而言越不重要" 的假设 (远程衰减).

诸如 RNN, CNN 之类的模型, 本质上就是把 "越近的 Token 越重要" 的先验融入到了架构之中, 使其可以不用位置编码并且将复杂度降低到线性.
然而, 先验都是人为的, 有偏的, 即不够准确.
而目前看来 LLM 的目标是碾压人类而不是模仿人类, 这也就可以解释为什么主流架构都用注意力机制, 因为架构的先验更少, 即人为的偏见和误区更少, 从而上限更高.

## 单向注意

下面介绍 NoPE 是如何工作的, 或者说它多大程度上能实现上面这些位置编码的作用.

双向注意力机制具有置换不变性, 所以需要位置编码, 因此 NoPE 不适用于双向注意力机制, 应用的前提是单项注意力或因果注意力.

$$
    y_n = f(q_n; x_1,\cdots,x_L) = \dfrac{\sum_{m=1}^{\textcolor{red}{n}} \exp(q_n\cdot k_m) v_m}{\sum_{m=1}^{\textcolor{red}{n}} \exp(q_n\cdot k_m)}, \quad k_n/v_n = x_n W_{k/v} + b_{k/v}
$$

它和双向注意力机制的区别, 只是求和符号的上限从 $L$ 改为了 $n$, 由此可见类似于累积求和, 结果依赖于 $x_1,\cdots,x_L$ 的顺序, 即本身就不具有置换不变性.

所以 Causal + NoPE 的组合原则上不需要位置编码, 也能取得非平凡的效果 (非平凡指的是和使用位置编码的效果在同一级别).

首先指出该结论的论文应该是: Transformer Language Models without Positional Encodings Still Learn Positional Information (https://arxiv.org/abs/2203.16634). 
这主要是说作者第一次以实验+论文这种比较规范的方式来宣称该结论, 事实上在这篇论文前该结论就已经被不少人默认.

此外, 后来的工作还探讨了 NoPE 的长度泛化能力. 
- The Impact of Positional Encoding on Length Generalization in Transformers (https://arxiv.org/abs/2305.19466) 
- Length Generalization of Causal Transformers without Position Encoding (https://arxiv.org/abs/2404.12224) 

## 方差辨位

进一步地, 因果注意力+无位置编码 (Causal + NoPE) 是通过什么机制来识别位置信息的呢?
可以通过一个极简的例子来体会.

直观来看, 因果注意力所定义得 $y_n$ 是 $n$ 个 $v$ 的加权平均, $y_{n+1}$ 就是 $n+1$ 个 $v$ 的加权平均, 以此类推, 可以先尝试最简单的情形: 均匀分布, 也就是考虑如下的注意力矩阵.

$$
    A = \begin{pmatrix}
        1 \\
        \frac{1}{2} & \frac{1}{2} \\
        \frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\
        \vdots & \vdots & \vdots & \ddots \\
        \frac{1}{n} & \frac{1}{n} & \frac{1}{n} & \cdots & \frac{1}{n}\\
        \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\
    \end{pmatrix}
$$

在这个假设下, 我们有

$$
    y_n = \dfrac{1}{n}\sum_{m=1}^n v_m
$$

然后, 假设每个 $v$ 的每个分量, 都是从同一个均值为 0, 标准差为 $\sigma$ 的分布中独立重复采样出来.

在这个假设下, 可以计算出 $y_n$ 的均值和方差.

$$
    \dfrac{1}{d}\sum_{i=1}^d y_{n,i}\approx \mathbb{E}[y_{n,i}] = \mathbb{E}[\dfrac{1}{n}\sum_{m=1}^n v_{n,i}] = \dfrac{1}{n}\sum_{m=1}^n \mathbb{E}[v_{n,i}] = 0
$$

$$
    \dfrac{1}{d}\sum_{i=1}^d y^2_{n,i}\approx \mathbb{E}[y_{n,i}^2] = \mathbb{E}[(\dfrac{1}{n}\sum_{m=1}^n v_{n,i})^2] = \dfrac{1}{n^2}\sum_{m=1}^n \mathbb{E}[v_{n,i}^2] = \dfrac{\sigma^2}{n}
$$

第二个等式就是 RMS Norm 的 Mean Square, 可以看到它和位置 $n$ 有关, 由于均值为零, 所以 Mean Square 等价于方差.
由此得出 Causal + NoPE 实际上是将位置信息隐藏在了 $y$ 的分量方差之中, 或者等价地, 隐藏在 $y$ 的 $l_2$ 范数之中.

当然这两个假设顶多适用于初始化的模型, 但用来理解一下 NoPE 识别位置的原理其实足够了.
各个 $y_n$ 的直观区别就是求平均 $v_m$ 的个数, 而不同数量的平均导致最直接的变化量就是方差.

同样的结果也出现在: Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings (https://arxiv.org/abs/2305.13571) 中, 并且作者在预训练过的 NoPE 模型上作了进一步的验证, 肯定了该结论的普适性.

## 不足之处

总结前面的结果:
- 位置编码的两个作用:
  - 主要作用: 打破注意力机制的置换不变性;
  - 次要作用: 为注意力机制注入一些先验.
- 因果注意力本身不具备置换不变性, 原则上不需要位置编码.
- NoPE 主要是通过隐藏状态向量的方差来表达位置信息.

那么为什么基于 Causal Attention 的 Decoder-Only 模型通常还加上位置编码呢?
答案是 NoPE 原则上还行, 但加上位置编码更好.

NoPE 通过向量的方差来表达位置信息, 相当于说 $y_n$ 是由某个不带位置信息的向量 $z_n$ 乘以某个和 $n$ 相关的变量函数 $p(n)$ 得到.
这意味着:
1. NoPE 实现的是类似乘性的绝对位置编码, 并且只是将位置信息压缩到单个变量中, 所以位置编码的效果很弱.
2. 单个标量所能表示的信息有限, 当输入长度增加时, 位置编码会越来越紧凑以至于难以区分.
3. 主流观点人为相对位置编码更适合自然语言, NoPE 实现的是绝对位置编码, 自然效率上不如再给模型额外补充相对位置编码.
4. NoPE 既没有给模型添加诸如远程衰减之类的先验, 也没有赋予模型学习到这种先验的能力, 那么输出长度足够大时可能会出现注意力不集中的问题.

综上 NoPE 对于长文本可能会存在位置分辨率不足, 效率较低, 注意力弥散等问题, 所以即便是 Decoder-Only 模型, 仍然需要补充上额外的位置编码 (特别是相对位置编码), 以完善上述种种不足指出.

以上分析主要是针对单头注意力, 事实上哪怕每个头的位置信息只有一个标量, 但在多头和多层的加持下, 总的位置信息也是一个比较可观的大向量了, 所以实际上 NoPE 没有那么糟糕, 只是加上位置编码后会更好一些, 因为这可以让 LLM 更聚焦于整体的推理能力, 而不是还要复现一些使用位置编码就可以实现的能力.
