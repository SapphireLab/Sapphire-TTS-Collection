# 低秩近似之路

可能很多读者和笔者一样, 对矩阵的低秩近似有种熟悉而又陌生的感觉.
熟悉是因为低秩近似的概念和意义都不难理解, 加之目前诸如 LoRA 等基于低秩近似的微调技术遍地开花, 让低秩近似的概念在耳濡目染之间就已经深入人心.
然而低秩近似所覆盖的内容非常广, 在低秩近似相关的论文中时常能看到一些不熟悉但又让人叹为观止的新技巧, 这就导致了一种似懂非懂的陌生感.

因此, 在这个系列文章中, 笔者将试图系统梳理一下矩阵低秩近似相关的理论内容, 以补全对低秩近似的了解.

## 伪逆 (Pseudo Inverse)

<a id="01"></a>

原文: <https://kexue.fm/archives/10366>
时间: 2024.09.16

**伪逆** 也称为 **广义逆 (Generalized Inverse)**, 顾名思义是"广义的逆矩阵". 它实际上是逆矩阵的概念对于不可逆矩阵的推广.

对于矩阵方程 $AB=M$, 如果 $A$ 是方阵且可逆, 那么直接得到 $B=A^{-1}M$.
可如果 $A$ 不可逆或者不是方阵呢? 这种情况下很可能找不到 $B$ 满足 $AB=M$.
此时如果还想继续求解, 通常是转化为优化问题:

$$
    \arg\min_{B} \| AB-M \|_F^2
$$

- $A\in\mathbb{R}^{n\times r}$, 
- $B\in\mathbb{R}^{r\times m}$, 
- $M\in\mathbb{R}^{n\times m}$,
- $\| \cdot \|_F^2$ 表示 F 范数 (Frobenius Norm), 用来衡量矩阵 $AB-M$ 与全零矩阵的距离, 其定义为:
  $$
    \| X\|_F^2 = \sqrt{\sum_{i}^{n}\sum_{j}^{m} X_{i,j}^2}
  $$

意思就是从求解精确的逆矩阵修改为最小化 $AB$ 和 $M$ 的平方误差.

本系列的主题是低秩近似, 所以假设 $r<< \min(n,m)$, 其机器学习意义就是通过低维的, 有损的输入矩阵 $A$ 和线性变换 $B$ 来重建完整的 $M$ 矩阵.

当 $m=n$ 且 $M$ 为单位矩阵时, 就得到一个只依赖于 $A$ 的结果, 记为:

$$
    A^\dagger = \arg\min_{B} \| AB-I_n \|_F^2
$$

它的作用类似于 $A$ 的逆矩阵, 所以称为 $A$ 的 (右) 伪逆.

类似地, 如果给定的是 $B$ 矩阵, 也可以将优化参数改为 $A$, 得到 $B$ 的 (左) 伪逆:

$$
    B^\dagger = \arg\min_{A} \| A B-I_n \|_F^2
$$

### 范数相关

对于向量 $x=(x_1,\cdots,x_m)$, 其 $p$-范数定义为:

$$
    \| x \|_p = \left( \sum_{i=1}^m x_i^p \right)^{1/p}
$$

$p$-范数中最常见的就是 $p=2$ 的情形, 即常说的向量模长, 也叫**欧几里得范数**.
通常范数忽略下标时, 基本上就是默认 $p=2$.

对于矩阵, 至少有两种不同但都常用的范数:
其中一种就是 $F$ 范数, 它是将矩阵展平为向量来计算的范数:

$$
    \| X \|_F = \| \text{vec}(X)\|_2 = \sqrt{\sum_{i=1}^n\sum_{j=1}^m X_{i,j}^2}
$$

由于矩阵范数的多样性, 所以 $\|X\|_F$ 的下标通常不能省略, 避免引起混淆.

$F$ 范数是将矩阵当成向量然后照搬向量范数的定义而来的, 由此启发, 我们可以尝试把更多的向量运算搬到矩阵上.

例如内积:

$$
    \langle P,Q \rangle_F = \langle \text{vec}(P), \text{vec}(Q) \rangle = \sqrt{\sum_{i=1}^n\sum_{j=1}^m P_{i,j} Q_{i,j}}
$$

其中 $P$ 和 $Q$ 是 $n\times m$ 矩阵.

这称为矩阵 $P$ $Q$ 的 $F$ 内积 (Frobenius Inner Product). 它可以使用向量的迹运算来表示:

$$
    \langle P,Q \rangle_F = \text{Tr}(P^\top Q)
$$

这可以直接由矩阵乘法和迹的定义来证明.

> #TODO: 补充证明.

当 $P$, $Q$ 是由多个矩阵连乘而来时, 转换为等价的迹运算通常能帮助我们进行化简.

例如利用它可以证明正交变换不改变 $F$ 范数:
假设 $U$ 是一个正交矩阵, 可以有

$$
\begin{aligned}
    \| UM \|_F^2 &= \langle UM, UM \rangle_F \\
    &= \text{Tr}[(UM)^{\mathsf{T}}UM]\\
    &= \text{Tr}(M^{\mathsf{T}}U^{\mathsf{T}} UM)\\
    &= \text{Tr}(M^{\mathsf{T}}M)\\
    &= \| M \|_F^2
\end{aligned}
$$

### 矩阵求导

对于一个优化目标, 最理想的结果自然是能够通过求导求出解析解.
可以发现对于前文定义的优化问题:

$$
    \arg\min_{B} \| AB-M \|_F^2
$$

其中 $AB-M$ 是关于 $B$ 的线性函数, 所以优化目标是关于 $A$ 或者 $B$ 的二次函数, 二次函数的最小值是有解析解的.

求出 $\mathcal{L}=\| AB-M \|_F^2$ 关于 $B$ 的导数, 由链式法则:

$$
    \dfrac{\partial \mathcal{L}}{\partial B_{i,j}} = \sum_{k,l} \dfrac{\partial \mathcal{L}}{\partial E_{k,l}}\dfrac{\partial E_{k,l}}{\partial B_{i,j}}
$$

其中 $\mathcal{L}=\|E\|_F^2=\sum_{i,j} E_{i,j}^2$, $E=AB-M$.

显然在求和的众多平方项中只有当 $(i,j)=(k,l)$ 时, 关于 $E_{k,l}$ 的偏导才非零, 其他项都为零.
所以 $\mathcal{L}$ 关于 $E_{k,l}$ 的导数就是 $E_{k,l}^2$ 关于 $E_{k,l}$ 的导数.
所以:

$$
    \dfrac{\partial \mathcal{L}}{\partial E_{k,l}} = 2E_{k,l} = 2[(\sum_{\alpha} A_{k,\alpha}B_{\alpha,l})-M_{k,l}]
$$

类似地, 只有当 $(\alpha,l)=(i,j)$ 时, 关于 $B_{i,j}$ 的导数才会产生非零的结果 $A_{k,i}$.
所以:

$$
    \dfrac{\partial E_{k,l}}{\partial B_{i,j}} = A_{k,i}\delta_{l,j}
$$

其中 $\delta_{l,j}$ 表示 Kronecker 符号, 表示只有当 $l=j$ 时值为 $1$, 否则为 $0$.

综上, 对于 $B_{i,j}$ 的导数:

$$
\begin{aligned}
    \dfrac{\partial \mathcal{L}}{\partial B_{i,j}} &= \sum_{k,l} \dfrac{\partial \mathcal{L}}{\partial E_{k,l}}\dfrac{\partial E_{k,l}}{\partial B_{i,j}}\\
    &= \sum_{k,l} 2E_{k,l} A_{k,i}\delta_{l,j}\\
    &= 2\sum_{k}E_{k,j}A_{k,i}
\end{aligned}
$$

如果我们约定, 标量对矩阵的梯度形状和矩阵形状本身一致, 那么上式右端项可以理解为 $A$ 的第 $i$ 行的转置乘以 $E$ 的第 $j$ 列, 可以写出:

$$
    \dfrac{\partial \mathcal{L}}{\partial B} = 2A^{\mathsf{T}}E = 2A^{\mathsf{T}}(AB-M)
$$

从直觉上, 按照通常的标量求导方式, $\mathcal{L}$ 关于 $B$ 的偏导数就是 $2(AB-M)$ 和 $A$ 的乘积.
又约定了 $\mathcal{L}$ 关于 $B$ 的偏导数的形状和 $B$ 形状一致, 即 $(r,m)$.
所以右端的两项 $2(AB-M)$ 为 $(n,m)$ 矩阵, 而 $A$ 为 $(n,r)$ 矩阵, 需要凑出一个 $(r,m)$ 的结果.

类似地, 可以得到 $\mathcal{L}$ 关于 $A$ 的偏导数:

$$
    \dfrac{\partial \mathcal{L}}{\partial A} = 2 (AB-M)B^{\mathsf{T}}
$$
