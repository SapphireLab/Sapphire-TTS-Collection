# 低秩近似之路

可能很多读者和笔者一样, 对矩阵的低秩近似有种熟悉而又陌生的感觉.
熟悉是因为低秩近似的概念和意义都不难理解, 加之目前诸如 LoRA 等基于低秩近似的微调技术遍地开花, 让低秩近似的概念在耳濡目染之间就已经深入人心.
然而低秩近似所覆盖的内容非常广, 在低秩近似相关的论文中时常能看到一些不熟悉但又让人叹为观止的新技巧, 这就导致了一种似懂非懂的陌生感.

因此, 在这个系列文章中, 笔者将试图系统梳理一下矩阵低秩近似相关的理论内容, 以补全对低秩近似的了解.

## 伪逆 (Pseudo Inverse)

<a id="01"></a>

原文: <https://kexue.fm/archives/10366>
时间: 2024.09.16

**伪逆** 也称为 **广义逆 (Generalized Inverse)**, 顾名思义是"广义的逆矩阵". 它实际上是逆矩阵的概念对于不可逆矩阵的推广.

对于矩阵方程 $AB=M$, 如果 $A$ 是方阵且可逆, 那么直接得到 $B=A^{-1}M$.
可如果 $A$ 不可逆或者不是方阵呢? 这种情况下很可能找不到 $B$ 满足 $AB=M$.
此时如果还想继续求解, 通常是转化为优化问题:

$$
    \arg\min_{B} \| AB-M \|_F^2
$$

- $A\in\mathbb{R}^{n\times r}$, 
- $B\in\mathbb{R}^{r\times m}$, 
- $M\in\mathbb{R}^{n\times m}$,
- $\| \cdot \|_F^2$ 表示 F 范数 (Frobenius Norm), 用来衡量矩阵 $AB-M$ 与全零矩阵的距离, 其定义为:
  $$
    \| X\|_F^2 = \sqrt{\sum_{i}^{n}\sum_{j}^{m} X_{i,j}^2}
  $$

意思就是从求解精确的逆矩阵修改为最小化 $AB$ 和 $M$ 的平方误差.

本系列的主题是低秩近似, 所以假设 $r<< \min(n,m)$, 其机器学习意义就是通过低维的, 有损的输入矩阵 $A$ 和线性变换 $B$ 来重建完整的 $M$ 矩阵.

当 $m=n$ 且 $M$ 为单位矩阵时, 就得到一个只依赖于 $A$ 的结果, 记为:

$$
    A^\dagger = \arg\min_{B} \| AB-I_n \|_F^2
$$

它的作用类似于 $A$ 的逆矩阵, 所以称为 $A$ 的 (右) 伪逆.

类似地, 如果给定的是 $B$ 矩阵, 也可以将优化参数改为 $A$, 得到 $B$ 的 (左) 伪逆:

$$
    B^\dagger = \arg\min_{A} \| A B-I_n \|_F^2
$$
