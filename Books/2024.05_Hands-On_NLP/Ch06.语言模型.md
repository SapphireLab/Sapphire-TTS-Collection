# 06.语言模型

**语言模型 (Language Model)** 用于计算一个文字序列的概率, 评估该序列作为一段文本出现在通用或特定场景中的可能性.
每个人的语言能力蕴含了一个语言模型, 当我们说出或写下一段话的时候, 已经在不自觉地应用语言模型来帮助我们决定这段话中的每个词, 使之通顺合理.
语言模型在自然语言处理中也有诸多应用, 例如, 当我们使用拼音中文输入法输入 `ziranyuyan`, 输出的候选文字中 `自然语言` 比 `孜然鱼雁` 更靠前, 这是因为中文输入法所使用的语言模型判断出前者的概率更高.
类似地, 在机器翻译, 拼写检查, 语音识别等应用中, 语言模型也被用来在多项候选文字中选择更合理, 更有可能出现的文字.

本章将首先概述语言模型, 然后介绍不同的方法来实现语言模型, 包括最简单的 $n$ 元语法模型, 更复杂但效果更好的循环神经网络, 在循环神经网络的基础上引入的注意力机制, 以及纯粹基于注意力机制的 Transformer 模型.

## 6.1.概述

要想得到一个语言模型, 最简单的想法是从一个大型语料库中直接统计不同文字序列出现的频率.
然而由于文字序列的排列组合空间极大, 不可能找到一个包含所有合理的文字序列的语料库, 因此这个想法是不可行的.
既然序列的概率无法通过经验频率估计, 那么是否可以通过概率乘法公式将其转换为一系列条件概率的乘积, 转而估算这些条件概率呢?

$$
    P(w_1,\cdots,w_n) = \prod_{i=1}^n P(w_i|w_1,\cdots,w_{i-1})
$$

其中 $w_i$ 表示输入文字序列的第 $i$ 个词.

那么这个序列 `自然语言` 的概率可以分解为 $P(自然语言) = P(自) P(然|自) P(语|自然) P(言|自然语)$.
这种分解方式的一个潜在好处在于, 一旦能够成功估算所有可能的条件概率, 就可以用它来生成文本.
具体而言, 首先根据第一个词的概率分布采样出第一个词, 再根据给定第一个词时第二个词的条件概率分布采样出第二个词, 以此类推.
这种逐个词依次输出, 每一步根据已输出的词决定下一个词的过程称为**自回归 (Auto-Regressive)** 过程.

---

那么如何估算这些条件概率呢?
最直接的想法是最大似然估计: $P(言|自然语) = \dfrac{count(自然语言)}{count(自然语)}$.
但这显然也是不可行的, 同样因为我们无法找到一个足够大的包含所有合理文字序列的语料库来估算频率.

因此, 人们发展出 $n$ 元语法模型, 循环神经网络, Transformer 模型等一系列方法来计算这些条件概率.

- $n$ 元 ($n$-gram) 语法模型: 每个词的概率仅以前 $n-1$ 个词为条件.
- 循环神经网络 (RNN): 每个词的概率以一个包含前置序列全部信息的稠密向量为条件.
- Transformer: 每个词的概率通过对前置所有词使用注意力机制得到.

本章将对这些方法的细节进行详细介绍.
除了这些方法, 还存在一些更复杂的方法, 如基于句法结构的生成式文法等, 后面的章节会介绍其中一些方法, 这里不再展开.

---

所有这些方法所共同面临的一个问题是, 如何处理在模型训练时没有见过的词, 即所谓**未登录 (Out-Of-Vocabulary, OOV)** 词.
一个常用的方法是引入一个特殊词 `[UNK]`: 
- 在训练时, 创建一个固定的词表 (如所有高频词), 将训练语料库中所有未在词表中出现的词都替换为 `[UNK]`, 并将 `[UNK]` 作为一个正常词估算概率;
- 在测试时, 使用 `[UNK]` 的概率来代替任何未登录词的概率.

除了这个方法之外, 还可以在字符或者子词级建立语言模型.
因为任何词都可以拆解为若干字符或子词的组合, 而字符或子词的个数较少. (注: 一般不超过几万, 例如大小写英文字符只有 52 个, BERT 的词表包含约 3 万个子词).
所以这样的语言模型能够涵盖所有字符或子词, 从而涵盖所有可能的文字序列.

---

本节最后讨论如何评估一个语言模型的质量.
一种方式是在下游任务 (如机器翻译, 语音识别等) 中检验语言模型的性能, 但这往往比较费时费力, 并且不同下游任务的评估结果有可能大相径庭.
因此评估语言模型的通用方法是使用**困惑度 (Perplexity)**, 即评估模型是否给训练语料库之外的真实测试语言语料库赋予较大的概率.
对于测试语料库 $\bar{x}_{1:m}$ ($m$ 个序列), 使用待评估模型计算每个词的平均负对数概率:

$$
    l=-\dfrac{1}{M}\sum_{i=1}^m \log P(\bar{x}_i)
$$

其中 $M$ 为测试语料库中的总词数.

该评价指标相当于编码每个词平均所需的比特数, 其二次幂 $2^l$ 就被称为测试数据的困惑度.
困惑度越小则测试语料库的概率越大, 因此可认为被评估模型的质量越高.
困惑度的最小值是 1, 这仅当所有测试语料的概率均为 1 的极端情况下才能取得.

需要特别注意的是, 词表不同的两个语言模型, 其困惑度是不可比较的, 显然, 词表较小的语言模型平均会给每个词更高的概率, 因而更有可能具有较低的困惑度, 极端情况下, 如果词表只包含 `[UNK]` 这一个词, 那么模型的困惑度可以达到完美的 1.
但词表过小的语言模型会将过多的词当作 `[UNK]`, 缺乏区分度, 因而不是一个好的语言模型.
因此, 要比较不同的语言模型方法时, 需要使用统一的词表.

## 6.2.$n$ 元语法模型

前一节使用概率乘法公式分解文字序列的概率, 但无法对分解得到的条件概率进行估算.
为了估算这些条件概率, 可以引入马尔可夫假设, 即假设每个词只依赖它之前的 $n-1$ 个词.

$$
    P(w_i|w_1,\cdots,w_{i-1}) = P(w_i|w_{i-n+1},\cdots,w_{i-1})
$$

上述方法被称为 $n$ 元语法模型.
所谓的 $n$ 元语法是指文本中的连续 $n$ 个词.
- 最简单的情况为**一元语法 (Unigram) 模型**: $P(w_1,\cdots,w_n) = \prod_{i} P(w_i)$.
一元语法模型假设每个词出现的概率独立于其他词, 这类似于朴素贝叶斯模型所做的假设.
- **二元语法 (Bigram) 模型** 则假设每个词只与上一个词有关, 而和其他词无关: $P(w_i|w_1,\cdots,w_{i-1}) = P(w_i|w_{i-1})$.
- 类似地, 可以定义三元语法模型, 四元语法模型等.

由于 $n$ 元语法模型对于条件概率的限制条件是只包含 $n-1$ 个词的序列, 因此当 $n$ 较小时, 可能的条件序列也相对较少, 可以从语料库中通过统计频率来估算.

---

$n$ 元语法模型的**缺点**:
- 无法建模所谓的长距离依赖 (即距离大于 $n$ 的两个词之间的依赖关系).
长距离依赖在自然语言中很常见, 例如英语中动词所采用的单复数形式取决于主语, 但动词与其主语之间可能间隔任意多个词 (如对于一个很长的定语从句).
- 需要存储大量的条件概率, 当 $n$ 较大时模型会非常巨大.

尽管有这些缺点, $n$ 元语法模型仍有很不错的性能, 在神经网络语言模型出现之前是最为成功的语言模型.

---

下面讨论如何从语料库中估算 $n$ 元语法模型的条件概率.

最简单的方式是最大似然估计:

$$
    P(w_i|w_{i-n+1},\cdots,w_{i-1}) = \dfrac{count(w_{i-n+1},\cdots,w_{i-1},w_i)}{count(w_{i-n+1},\cdots,w_{i-1})}
$$

但这样做的一个常见问题是**数据稀疏性问题**: 尽管限制了序列的长度, 但是不同的 $n$ 元语法仍然是非常多的, 因此一些合理的 $n$ 元语法可能不会在训练语料库中出现, 从而导致相应的条件概率估算为 0. 模型会将包含这些 $n$ 元语法的文本的概率计算为 0, 这是不合理的.

常见的处理数据稀疏性问题的方法有以下几类: 
- 平滑法 (如拉普拉斯平滑), 
- 回退法: 找不到对应的 $n$ 元语法时使用 $n-1$ 元语法, 依次类推,
- 插值法: 将 $n$ 元语法模型, $n-1$ 元语法模型等一系列模型加权平均.

注: 其中最为成功的方法是改良型 Kneser-Ney 平滑, 参阅 "Scalable Modified Kneser-Ney Language Model Estimation (2013)".

一个著名的 $n$ 元语法数据集和可视化界面是 Google Ngram Viewer, 包含几百年来多种语言的公开文献中 $n$ 元语法的出现频率统计, 可以查询官网了解详细信息.

## 6.3.循环神经网络

基于神经网络的语言模型可以避免 $n$ 元语法模型的各种缺点.
神经网络语言模型中最基础的模型之一是**循环神经网络 (Recurrent Neural Network, RNN)**, 它使用隐状态 (Hidden State) 来保存历史信息, 并使用循环结构逐一处理输入序列中的每一个元素.
接下来首先介绍最基本的循环神经网络语言模型, 然后介绍循环神经网络的两个著名变体: **长短期记忆 (Long Short-Term Memory, LSTM)** 和**门控循环单元 (Gated Recurrent Unit, GRU)**, 最后介绍多层双向循环神经网络.

### 6.3.1.RNN

循环神经网络的基本思想是在计算文字序列中每个词的条件概率时, 计算一个稠密向量来表示条件 (也就是这个词的前置序列) 所包含的信息, 然后用该向量来计算条件概率分布.

在循环神经网络的模型结构中, 每一步网络的输入包含两部分:
- 历史输入 $\mathbf{h}^{(t-1)}$, 是前置序列的总结, 称为隐状态;
- 新的输入 $\mathbf{x}^{(t)}$, 此处为词 $w_t$ 所对应的词嵌入;

模型的输出经过 Softmax 函数处理得到下一步的词的分布, 例如 `自然语言` 的下一个词可能是 `处 (理)`, `生 (成)` 等.
计算公式如下:

隐状态: $\mathbf{h}^{(t)} = \sigma(\mathbf{W}_x \mathbf{x}^{(t)} + \mathbf{W}_h \mathbf{h}^{(t-1)} + \mathbf{b}_h)$;
输出: $\hat{y}^{(t)} = \text{softmax}(\mathbf{W}_o \\mathbf{h}^{(t)} + \mathbf{b}_o)$
 
- $\mathbf{W}_x$ 为输入的参数; 
- $\mathbf{W}_h, \mathbf{b}_h$ 为隐藏层的参数;
- $\mathbf{W}_o, \mathbf{b}_o$ 为输出层的参数;
- $\sigma$ 为激活函数;
- $\mathbf{h}^{(0)}$ 为初始隐状态, 在没有其他信息的情况下一般设置为全零.

循环神经网络的显著好处在于, 解除了 $n$ 元语法模型中对条件序列长度的限制, 即不再使用马尔可夫假设, 因而每一时刻的输出 (即条件概率分布) 都基于整个前置序列作为条件.
并且由于不需要像 $n$ 元语法模型那样存储大量的条件概率, 循环神经网络往往比典型的 $n$ 元语法模型小, 即具有更少的模型参数.

下面讨论循环神经网络的训练.
在每一步 $t$, 训练损失函数为下一个词的预测分布 $\hat{y}^{(t)}$ 与真实分布 $y^{(t)}$ (即下一个词 $w_{t+1}$ 的独热编码) 的交叉熵 (Cross Entropy):

$$
    J^{(t)}(\theta) = -\sum_{w\in V} y_w^{(t)}\log \hat{y}_w^{(t)} = -\log \hat{y}_{w_{t+1}}^{(t)}
$$

### 6.3.2.LSTM

### 6.3.3.多层双向 RNN

## 6.4.注意力机制



## 6.5.Transformer

## 总结

本章介绍了语言模型的基本概念, 然后介绍了一系列语言模型方法, 包括传统的 $n$ 元语法模型, 循环神经网络及其变体, 可以建模长距离依赖的注意力机制, 以及纯粹基于注意力机制的 Transformer 模型.

值得一提的是, 本章介绍的 RNN 与 Transformer 模型除了用于语言模型, 还可以用于很多其他任务, 例如文本表示, 序列标注, 文本生成, 其中文本表示任务中这些模型的使用已经在 [Ch03.文本表示](Ch03.文本表示.md) 的 4.3 节讨论, 其余任务将在后面的章节中讨论.