# ç¬¬04ç« .ä½¿ç”¨è¯„ä»·æŒ‡æ ‡å·¥å…·

[ä»£ç æ–‡ä»¶](PythonFiles/Ch04.Metrics.py)

## 4.1.ä»‹ç»

åœ¨è®­ç»ƒå’Œæµ‹è¯•ä¸€ä¸ªæ¨¡å‹æ—¶å¾€å¾€éœ€è¦è®¡ç®—ä¸åŒçš„è¯„ä»·æŒ‡æ ‡, å¦‚æ­£ç¡®ç‡, æŸ¥å‡†ç‡, æŸ¥å…¨ç‡, F1 å€¼ç­‰ç­‰, å…·ä½“çš„æŒ‡æ ‡å¾€å¾€å’Œå¤„ç†çš„æ•°æ®é›†, ä»»åŠ¡ç±»å‹æœ‰å…³.
HuggingFace æä¾›äº†ç»Ÿä¸€çš„è¯„ä»·æŒ‡æ ‡å·¥å…·, èƒ½å¤Ÿå°†å…·ä½“åœ°è®¡ç®—è¿‡ç¨‹éšè—, è°ƒç”¨è€…åªéœ€æä¾›è®¡ç®—ç»“æœ, ç”±è¯„ä»·æŒ‡æ ‡å·¥å…·ç»™å‡ºç»“æœ.

## 4.2.ä½¿ç”¨

### 4.2.1.åˆ—å‡ºå¯ç”¨æŒ‡æ ‡

`list_metrics()` å‡½æ•°è·å–å¯ç”¨çš„è¯„ä»·æŒ‡æ ‡åˆ—è¡¨.
ä»£ç å¦‚ä¸‹:
```python
from datasets import list_metrics

metrics_list = list_metrics()
print(f"{len(metrics_list)=}")
print(metrics_list[:10])
è¿è¡Œç»“æœ:
<ipython-input-1-57908563e8de>:4: FutureWarning: list_metrics is deprecated and will be removed in the next major version of datasets. Use 'evaluate.list_evaluation_modules' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metrics_list = list_metrics()
len(metrics_list)=243
['accuracy', 'bertscore', 'bleu', 'bleurt', 'brier_score', 'cer', 'character', 'charcut_mt', 'chrf', 'code_eval']
```

åç»­è¦ä½¿ç”¨å…¶ä»–æ–¹æ³•æ›¿ä»£æ‰ (? æŒ‡æ ‡æ•°é‡ä¸ä¸€è‡´).

```python
!pip install evaluate
from evaluate import list_evaluation_modules

metrics_list = list_evaluation_modules()
print(f"{len(metrics_list)=}")
print(metrics_list[:10])
è¿è¡Œç»“æœ:
len(metrics_list)=167
['lvwerra/test', 'jordyvl/ece', 'angelina-wang/directional_bias_amplification', 'cpllab/syntaxgym', 'lvwerra/bary_score', 'hack/test_metric', 'yzha/ctc_eval', 'codeparrot/apps_metric', 'mfumanelli/geometric_mean', 'daiyizheng/valid'
```

### 4.2.2.åŠ è½½æŒ‡æ ‡

ä½¿ç”¨ `load_metric()` å‡½æ•°åŠ è½½ä¸€ä¸ªè¯„ä»·æŒ‡æ ‡.
è¯„ä»·æŒ‡æ ‡å¾€å¾€å’Œå¯¹åº”çš„æ•°æ®é›†é…å¥—ä½¿ç”¨, æ­¤å¤„ä»¥ `glue` æ•°æ®é›†çš„ `mrpc` å­é›†ä¸ºä¾‹.
ä»£ç å¦‚ä¸‹:
```python
from datasets import load_metric
metric = load_metric(path='glue', config_name='mrpc')
print(metric)
è¿è¡Œç»“æœ:
<ipython-input-4-32635ea1b629>:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. 
Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate
    metric = load_metric(path='glue', config_name='mrpc')
~\site-packages\datasets\load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
Downloading builder script: 5.76kB [00:00, 5.79MB/s]                   
Metric(name: "glue", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = datasets.load_metric('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = datasets.load_metric('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
```

åç»­å°†æ›¿æ¢ä¸º
```python
from evaluate import load
metric = load(path='glue', config_name='mrpc')
print(metric)
è¿è¡Œç»“æœ:
Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.75k/5.75k [00:00<?, ?B/s]
EvaluationModule(name: "glue", module_type: "metric", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: """
Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
    predictions: list of predictions to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
    "accuracy": Accuracy
    "f1": F1 score
    "pearson": Pearson Correlation
    "spearmanr": Spearman Correlation
    "matthews_correlation": Matthew Correlation
Examples:

    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of ["mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0}

    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'accuracy': 1.0, 'f1': 1.0}

    >>> glue_metric = evaluate.load('glue', 'stsb')
    >>> references = [0., 1., 2., 3., 4., 5.]
    >>> predictions = [0., 1., 2., 3., 4., 5.]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print({"pearson": round(results["pearson"], 2), "spearmanr": round(results["spearmanr"], 2)})
    {'pearson': 1.0, 'spearmanr': 1.0}

    >>> glue_metric = evaluate.load('glue', 'cola')
    >>> references = [0, 1]
    >>> predictions = [0, 1]
    >>> results = glue_metric.compute(predictions=predictions, references=references)
    >>> print(results)
    {'matthews_correlation': 1.0}
""", stored examples: 0)
```

æ³¨æ„: ä¸æ˜¯æ¯ä¸ªæ•°æ®é›†éƒ½æœ‰å¯¹åº”çš„è¯„ä»·æŒ‡æ ‡, åœ¨å®é™…ä½¿ç”¨æ—¶ä»¥æ»¡è¶³éœ€è¦ä¸ºå‡†åˆ™é€‰æ‹©åˆé€‚çš„è¯„ä»·æŒ‡æ ‡å³å¯.

### 4.2.3.è·å–è¯´æ˜

```python
print(metric.inputs_description)
```
å’Œä¸Šé¢çš„è¾“å‡ºä¸€è‡´, åŒ…æ‹¬äº†å¯¹è¯„ä»·æŒ‡æ ‡çš„ä»‹ç», è¦æ±‚è¾“å…¥æ ¼å¼çš„è¯´æ˜, è¾“å‡ºæŒ‡æ ‡çš„è¯´æ˜, ä»¥åŠéƒ¨åˆ†ç¤ºä¾‹ä»£ç .

### 4.2.4.è®¡ç®—æŒ‡æ ‡

æŒ‰ç…§ä¸Šé¢çš„ç¤ºä¾‹ä»£ç , å¯ä»¥å®é™…è®¡ç®—å‡ºè¯„ä»·æŒ‡æ ‡.
ä»£ç å¦‚ä¸‹:
```python
predictions = [0,1,0]
references  = [0,1,1]
metric.compute(predictions=predictions, references=references)
è¿è¡Œç»“æœ:
{'accuracy': 0.6666666666666666, 'f1': 0.6666666666666666}
```

è¯¥æŒ‡æ ‡çš„è¾“å‡ºåŒ…æ‹¬äº†å‡†ç¡®ç‡å’Œ F1 åˆ†å€¼.

## æ€»ç»“

æœ¬ç« è®²è§£äº† HuggingFace è¯„ä»·æŒ‡æ ‡å·¥å…·çš„ä½¿ç”¨, åœ¨å®é™…ä½¿ç”¨æ—¶è¯„ä»·æŒ‡æ ‡å·¥å…·å¾€å¾€å’Œè®­ç»ƒå·¥å…·ä¸€èµ·ä½¿ç”¨, èƒ½å¤Ÿéšç€è®­ç»ƒæ­¥éª¤è¿›è¡Œ, åŒæ—¶ç›‘æ§è¯„ä»·æŒ‡æ ‡, ä»¥ç¡®å®šæ¨¡å‹ç¡®å®æ­£å‘ç€ä¸€ä¸ªç†æƒ³çš„ç›®æ ‡è¿›æ­¥.