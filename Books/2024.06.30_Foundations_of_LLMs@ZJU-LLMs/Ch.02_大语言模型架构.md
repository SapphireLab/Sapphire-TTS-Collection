# 第 2 章·大语言模型架构

- [原文](https://github.com/ZJU-LLMs/Foundations-of-LLMs/blob/main/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E6%95%99%E6%9D%90/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E5%88%86%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9/%E7%AC%AC2%E7%AB%A0%20%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84.pdf)

## 引言

随着数据资源和计算能力的爆发式增长, 语言模型的参数规模和性能表现实现了质的飞跃, 迈入了**大语言模型 (Large Language Model, LLM)** 的新时代.
凭借着庞大的参数量和丰富的训练数据, 大语言模型不仅展现出了强大的泛化能力, 还催生了新智能的涌现, 勇立生成式人工智能(Artificial Intelligence Generated Content, AIGC) 的浪潮之巅.

当前, 大语言模型技术蓬勃发展, 各类模型层出不穷.
这些模型在广泛的应用场景中已经展现出与人类比肩甚至超过人类的能力, 引领着由 AIGC 驱动的新一轮产业革命.

本章将深入探讨大语言模型的相关背景知识, 并分别介绍 **Encoder-Only**, **Encoder-Decoder** 以及 **Decoder-Only** 三种主流模型架构.
通过列举每种架构的代表性模型, 深入分析它们在网络结构, 训练方法等方面的主要创新之处.
最后, 本章还将简单介绍一些非 Transformer 架构的模型, 以展现当前大语言模型研究百花齐放的发展现状.

## 2.1.大数据+大模型→新智能

#TODO

## 2.2.大语言模型架构概览

在语言模型的发展历程中, [Transformer](../../Models/_Transformer/2017.06.12_Transformer.md) 框架的问世代表着一个划时代的转折点.
其独特的**自注意力 (Self-Attention) 机制**极大地提升了模型**对序列数据的处理能力**, 在**捕捉长距离依赖关系**方面尤为出色.
此外, Transformer 框架**对并行计算的支持**极大地加速了模型的训练过程.

当前绝大多数大语言模型均以 Transformer 框架为核心, 并进一步演化出了三种经典架构: **Encoder-Only**, **Encoder-Decoder**, **Decoder-Only**.
这三种架构在设计和功能上各有不同.

### 2.2.1.基本概念

#### Encoder-Only 架构

Encoder-Only 架构仅选取了 Transformer 中的编码器 Encoder 部分, 用于接受输入文本并生成与上下文相关的特征.
Encoder-Only 架构包含三个部分:
- 输入编码部分: 分词, 向量化, 添加位置编码三个过程;
  - 原始输入文本被分词器 Tokenizer 拆解为 Token 序列;
  - 通过词表和词嵌入 (Embedding) 矩阵映射为向量序列, 确保文本信息得以数字化表达;
  - 位置编码 (Positional Encoding) 添加到每个向量序列中, 以保留文本中单词的顺序信息.
- 特征编码部分:
  - 由多个相同的编码器块堆叠而成, 每个编码器块包含自注意力模块和全连接前馈模块;
  - 前一部分得到的向量序列依次通过这些编码模块, 进一步提取和深化文本特征.
- 任务处理部分:
  - 在预训练阶段使用全连接层作为输出头, 用于完成掩码预测等任务;
  - 在下游任务适配阶段, 输出头会根据具体任务需求进行定制.
    - 情感分析/主题分类等判别任务: 只需添加一个分类器直接输出判别结果;
    - 文本摘要生成等生成任务: 只需添加一个全连接层逐个预测后续的 Token. (注: 每次生成新 Token 时需要重新计算整个输入序列的表示, 增加了计算成本, 也可能导致生成文本缺乏连贯性)

#### Encoder-Decoder 架构

Encoder-Decoder 架构在 Encoder-Only 架构的基础上引入了一个解码器, 并采用交叉注意力机制来实现编码器和解码器之间的有效交互, 弥补 Encoder-Only 架构在生成任务上的不足.
Decoder 部分包含三个部分:
- 输出编码部分: 和 Encoder 的输入编码结构相同, 含分词, 向量化, 位置编码三个过程.
- 特征解码部分: 和特征编码部分高度相似, 包括**掩码自注意力 (Masked Self-Attention) 模块**, 交叉注意力模块, 全连接前馈模块.
  - 掩码自注意力模块: 确保模型只关注上文, 不会预见未来信息, 从而在无下文泄露的条件下进行自回归的训练和推理;
  - 交叉注意力模块: 负责处理从编码模块向解码模块传递相关信息.
- 输出生成部分:
  - 线性层 + Softmax 层将特征解码后的向量转换为词表上的概率分布, 并从这个分布中采样得到最合适的 Token 作为输出.

训练阶段:
- 数据样本: 输入文本和真实输出文本 (Ground Truth).
- 输入文本首先被输入编码部分转化为向量序列, 在特征编码模块进一步处理转化为上下文表示.
- 输出文本添加特殊的开始 Token `[START]` 在输出编码部分转化为向量序列, 并行输入到特征解码模块.
- 使用 Teacher Forcing 技术, 在每轮预测时, 使用真实输出文本中的已知部分作为输入, 并结合从最后一个编码块得到的上下文信息来预测下一个 Token, 计算预测的 Token 和真实 Token 之间的损失, 通过反向传播更新参数.

推理阶段:
- 无真实输出文本, 输出序列原始状态只有开始 Token `[START]`, 无需分词器.
- 模型通过自回归的方式, 在每轮采样生成 Token 后拼接到输出序列中, 用于下一轮预测.
- 这一过程循环进行直到生成特定的结束 Token `[END]` 或达到模型设定的最大输出长度.
- 每轮的输入依赖上一轮的采样结果, 只能一步步串行输出.

#### Decoder-Only 架构

为了有效缩减模型的规模以及降低整体的计算复杂度, Decoder-Only 架构摒弃了 Encoder 部分以及交叉注意力模块.
在这种架构下, 模型仅使用解码器来构建语言模型, 利用自回归机制在给定上文的情况下生成流畅且连贯的下文.

Decoder-Only 架构包含三个部分:
- 输入编码部分: 分词器, 词嵌入矩阵, 位置编码;
- 特征解码部分: 忽略交叉注意力子模块;
- 输出生成部分: 线性层 + Softmax 层, 用于生成下一个 Token.

### 2.2.2.功能对比

#### 注意力矩阵

注意力矩阵 (Attention Matrix) 是 Transformer 中的核心组件, 用于计算输入序列中各个 Token 之间的依赖关系.
通过注意力机制, 模型可以在处理当前 Token 时, 灵活地关注序列中其他 Token 所携带的信息, 决定了这一过程中哪些 Token 能够相互影响.

- Encoder-Only 架构: 注意力矩阵来自自注意力模块, 用于捕捉输入序列中各个 Token 之间的关系. 整个矩阵呈现出完全的注意力, 即对每个 Token 的理解都依赖于整个输入序列中的所有 Token, 即**双向注意力机制**. 能够同时利用前后文信息, 深入理解复杂的语义联系和上下文依赖.
- Encoder-Decoder 架构: 注意力矩阵有编码器的自注意力, 解码器的掩码自注意力, 交叉注意力三种机制.
  - 编码器自注意力: 完全的注意力, 用于生成输入序列的全面上下文表示.
  - 解码器掩码自注意力: 下三角注意力矩阵, 确保生成当前 Token 时模型只关注之前生成的 Token.
  - 交叉注意力: 解码器始终能够动态地参考编码器生成的完整上下文表示, 确保输入和输出序列高度相关且连贯. 即生成 $y_{i}$ 时参考 $x_{1}\sim x_{n} + y_{1}\sim y_{i-1}$.
- Decoder-Only 架构: 掩码自注意力模块, 只能依赖于已经生成的历史 Token 信息, 整个矩阵呈现出下三角的注意力, 即**单向注意力机制**.

#### 适用任务

由于模型设计和注意力矩阵的差距, 在同等参数规模下, 三种架构的模型在适用任务上各有倾向.
- Encoder-Only 架构:
  - 特点: 双向注意力机制允许预测每个 Token 时充分考虑序列的上下文信息, 捕捉丰富的语义和依赖关系.
  - 任务: 适合**自然语言理解 (Natural Language Understanding, NLU) 任务**, 如情感分析和文本分类等任务.
  - 缺点: 缺少解码器组件, 无法直接生成所需目标序列. 在**自然语言生成 (Natural Language Generation, NLG) 任务**上表现可能不如专门设计的的生成模型.
- Encoder-Decoder 架构:
  - 特点: 添加解码器能够基于编码器输出的上下文表示逐步生成序列, 使得模型可以有效地处理复杂的输入条件, 并生成相关且连贯的高质量内容.
  - 任务: 适合处理各种复杂的**有条件生成任务**. 如机器翻译, 文本摘要, 问答系统等需要同时理解输入并生成相应输出的场景.
  - 缺点: 模型规模以及参数量庞大的问题.
- Decoder-Only 架构:
  - 特点: 删除编码器部分, 降低模型本身计算复杂度. 使用掩码操作确保在每个时间步生成当前 Token 时只能访问先前的 Token, 通过自回归生成机制从起始 Token 开始逐步生成文本. 大规模预训练数据使得该架构的模型能够生成高质量连贯的文本.
  - 任务: 适合处理**无条件文本生成任务**, 如自动故事生成, 新闻文章生成等.
  - 缺点: 模型规模有限时由于缺乏编码器的双向上下文信息, 模型在理解复杂输入数据时存在一定局限性, 表现可能不如 Encoder-Decoder 架构.

### 2.2.3.历史演变

在不同的历史阶段, 暗中模型架构分别展示了自身的优势.
随着模型规模以及数据规模的显著增长, Decoder-Only 架构模型逐渐占据上风, 以其强大的任务泛化性能展现出成为大一统架构的潜力.
以 GPT-3, GPT-4 等为代表的大型 Decoder-Only 语言模型已经发展出了与人类媲美甚至超越人类的记忆, 推理以及处理复杂任务的能力.

在大语言模型的早期发展阶段 (2018 年), BERT 和 GPT-1 分别作为 Encoder-Only 和 Decoder-Only 架构的代表几乎同时出现. 但受限于当时的模型参数规模, **BERT 的强大上下文理解能力比 GPT-1 初阶的文本生成能力更为亮眼**. 这使得 Encoder-Only 架构得到更广泛的探索和应用.

但随着用户对机器翻译等生成任务需求的增加, Encoder-Only 架构逐渐无法满足直接生成的需求, 因而被逐渐冷落.
2019 年末诞生了一众 Encoder-Decoder 架构的模型, 能够有效处理序列到序列的生成任务, 逐渐成为主流.
随着算力资源的急速发展, 研究人员开始寻求不断提升参数量来激发更强的生成能力.
得益于参数易扩展性, Decoder-Only 架构的模型参数量开始急剧扩充, 文本生成能力大幅提升.
2021 年后, 在 GPT-3 等模型的推动下, Decoder-Only 架构开始占据主流, 甚至主导了大语言模型的发展.
尽管如此, Encoder-Decoder 架构仍然活跃于开源社区中, 不断被探索和改进.
Encoder-Only 架构在 BERT 带来最初的爆炸性增长后, 关注度有所下降, 但仍在部分判别任务中发挥重要作用.

总的来讲, 大语言模型的主流架构经历了从 Encoder-Only 到 Encoder-Decoder 再到 Decoder-Only 的演变, 而引发这种更迭趋势的原因可能是模型本身生成能力以及计算效率上的差异.
