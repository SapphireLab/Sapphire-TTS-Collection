# 第 2 章·大语言模型架构

- [原文](https://github.com/ZJU-LLMs/Foundations-of-LLMs/blob/main/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E6%95%99%E6%9D%90/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E5%88%86%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9/%E7%AC%AC2%E7%AB%A0%20%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84.pdf)

## 引言

随着数据资源和计算能力的爆发式增长, 语言模型的参数规模和性能表现实现了质的飞跃, 迈入了**大语言模型 (Large Language Model, LLM)** 的新时代.
凭借着庞大的参数量和丰富的训练数据, 大语言模型不仅展现出了强大的泛化能力, 还催生了新智能的涌现, 勇立生成式人工智能(Artificial Intelligence Generated Content, AIGC) 的浪潮之巅.

当前, 大语言模型技术蓬勃发展, 各类模型层出不穷.
这些模型在广泛的应用场景中已经展现出与人类比肩甚至超过人类的能力, 引领着由 AIGC 驱动的新一轮产业革命.

本章将深入探讨大语言模型的相关背景知识, 并分别介绍 **Encoder-Only**, **Encoder-Decoder** 以及 **Decoder-Only** 三种主流模型架构.
通过列举每种架构的代表性模型, 深入分析它们在网络结构, 训练方法等方面的主要创新之处.
最后, 本章还将简单介绍一些非 Transformer 架构的模型, 以展现当前大语言模型研究百花齐放的发展现状.

## 2.1.大数据+大模型→新智能

#TODO

## 2.2.大语言模型架构概览

在语言模型的发展历程中, [Transformer](../../Models/_Transformer/2017.06.12_Transformer.md) 框架的问世代表着一个划时代的转折点.
其独特的**自注意力 (Self-Attention) 机制**极大地提升了模型**对序列数据的处理能力**, 在**捕捉长距离依赖关系**方面尤为出色.
此外, Transformer 框架**对并行计算的支持**极大地加速了模型的训练过程.

当前绝大多数大语言模型均以 Transformer 框架为核心, 并进一步演化出了三种经典架构: **Encoder-Only**, **Encoder-Decoder**, **Decoder-Only**.
这三种架构在设计和功能上各有不同.

### 2.2.1.基本概念

#### Encoder-Only 架构

Encoder-Only 架构仅选取了 Transformer 中的编码器 Encoder 部分, 用于接受输入文本并生成与上下文相关的特征.
Encoder-Only 架构包含三个部分:
- 输入编码部分: 分词, 向量化, 添加位置编码三个过程;
  - 原始输入文本被分词器 Tokenizer 拆解为 Token 序列;
  - 通过词表和词嵌入 (Embedding) 矩阵映射为向量序列, 确保文本信息得以数字化表达;
  - 位置编码 (Positional Encoding) 添加到每个向量序列中, 以保留文本中单词的顺序信息.
- 特征编码部分:
  - 由多个相同的编码器块堆叠而成, 每个编码器块包含自注意力模块和全连接前馈模块;
  - 前一部分得到的向量序列依次通过这些编码模块, 进一步提取和深化文本特征.
- 任务处理部分:
  - 在预训练阶段使用全连接层作为输出头, 用于完成掩码预测等任务;
  - 在下游任务适配阶段, 输出头会根据具体任务需求进行定制.
    - 情感分析/主题分类等判别任务: 只需添加一个分类器直接输出判别结果;
    - 文本摘要生成等生成任务: 只需添加一个全连接层逐个预测后续的 Token. (注: 每次生成新 Token 时需要重新计算整个输入序列的表示, 增加了计算成本, 也可能导致生成文本缺乏连贯性)

#### Encoder-Decoder 架构

Encoder-Decoder 架构在 Encoder-Only 架构的基础上引入了一个解码器, 并采用交叉注意力机制来实现编码器和解码器之间的有效交互, 弥补 Encoder-Only 架构在生成任务上的不足.
Decoder 部分包含三个部分:
- 输出编码部分: 和 Encoder 的输入编码结构相同, 含分词, 向量化, 位置编码三个过程.
- 特征解码部分: 和特征编码部分高度相似, 包括**掩码自注意力 (Masked Self-Attention) 模块**, 交叉注意力模块, 全连接前馈模块.
  - 掩码自注意力模块: 确保模型只关注上文, 不会预见未来信息, 从而在无下文泄露的条件下进行自回归的训练和推理;
  - 交叉注意力模块: 负责处理从编码模块向解码模块传递相关信息.
- 输出生成部分:
  - 线性层 + Softmax 层将特征解码后的向量转换为词表上的概率分布, 并从这个分布中采样得到最合适的 Token 作为输出.

训练阶段:
- 数据样本: 输入文本和真实输出文本 (Ground Truth).
- 输入文本首先被输入编码部分转化为向量序列, 在特征编码模块进一步处理转化为上下文表示.
- 输出文本添加特殊的开始 Token `[START]` 在输出编码部分转化为向量序列, 并行输入到特征解码模块.
- 使用 Teacher Forcing 技术, 在每轮预测时, 使用真实输出文本中的已知部分作为输入, 并结合从最后一个编码块得到的上下文信息来预测下一个 Token, 计算预测的 Token 和真实 Token 之间的损失, 通过反向传播更新参数.

推理阶段:
- 无真实输出文本, 输出序列原始状态只有开始 Token `[START]`, 无需分词器.
- 模型通过自回归的方式, 在每轮采样生成 Token 后拼接到输出序列中, 用于下一轮预测.
- 这一过程循环进行直到生成特定的结束 Token `[END]` 或达到模型设定的最大输出长度.
- 每轮的输入依赖上一轮的采样结果, 只能一步步串行输出.

#### Decoder-Only 架构

为了有效缩减模型的规模以及降低整体的计算复杂度, Decoder-Only 架构摒弃了 Encoder 部分以及交叉注意力模块.
在这种架构下, 模型仅使用解码器来构建语言模型, 利用自回归机制在给定上文的情况下生成流畅且连贯的下文.

Decoder-Only 架构包含三个部分:
- 输入编码部分: 分词器, 词嵌入矩阵, 位置编码;
- 特征解码部分: 忽略交叉注意力子模块;
- 输出生成部分: 线性层 + Softmax 层, 用于生成下一个 Token.
