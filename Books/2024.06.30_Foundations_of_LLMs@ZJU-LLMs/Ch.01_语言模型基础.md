# 第 1 章·语言模型基础

- [原文](https://github.com/ZJU-LLMs/Foundations-of-LLMs/blob/main/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E6%95%99%E6%9D%90/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E5%88%86%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9/%E7%AC%AC1%E7%AB%A0%20%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80.pdf)

语言是一套复杂的符号系统.

语言符号通常在**音韵 (Phonology)**, **词法 (Morphology)**, **句法 (Syntax)** 的约束下构成, 并承载不同的**语义 (Semantics)**.

**语言是概率的, 且概率性和认知的概率性也存在密不可分的关系**:
语言符号具有不确定性, 例如
- 同样的语义可以由不同的音韵/词法/句法构成的符号来表达;
- 同样的音韵/词法/句法构成的符号也可以在不同语境下表达不同的语义;

**语言模型 (Language Models, LMs)** 旨在**准确预测语言符号的概率**.
- 从语言学角度: 语言模型可以赋能计算机掌握语法/理解语义, 以完成自然语言处理任务.
- 从认知科学的角度: 准确预测语言符号的概率可以赋能计算机描摹认知, 演化智能.

从 ELIZA 到 GPT-4, 语言模型经历了从**规则模型到统计模型, 再到神经网络模型**的发展历程, 逐步从呆板的机械式问答程序成长为具有强大泛化能力的多任务只能模型.

本章按照语言模型发展的顺序依次讲解
- 基于统计方法的 N-Grams 语言模型;
- 基于循环神经网络的语言模型;
- 基于 Transformer 的语言模型.

本章还介绍
- 将语言模型输出概率值解码为目标文本;
- 对语言模型的性能进行评估.

## 1.1.基于统计方法的语言模型

语言模型通过对**语料库 (Corpus)** 中的语料进行统计或学习来获得预测语言符号概率的能力.

通常, 基于统计的语言模型通过**直接统计语言符号在语料库中出现的频率**来预测语言符号的概率.

其中 N-Grams 最具代表性.

**N-Grams 语言模型基于马尔可夫假设和离散变量的极大似然估计给出语言符号的概率.**

假设包含有 $N$ 个元素的语言符号可以表示为 $w_{1:N}=\{w_1, w_2, w_3, \cdots w_N\}$.
该集合可以代表文本, 也可以代表音频序列等承载了语义信息的序列.
在此处代表文本, 每个元素代表一个词.
在真实语言模型中, 每个元素可以是 Token 等其他形式.

N-Grams 语言模型中的 "N-Grams" 指的是长度为 N 的词序列. 模型通过依次统计文本中的 N-Gram 和对应的 (N-1)-Gram 在语料库中出现的相对频率来计算文本 $w_{1:N}$ 出现的概率.

其计算公式为:

$$
    P_{N-Grams}(w_{1:N}) = \prod_{i=n}^{N}\dfrac{C(w_{i-n+1:i})}{C(w_{i-n+1:i-1})}
$$

- $C(w_{i-n+1:i})$ 表示 $w_{i-n+1:i}$ 在语料库中出现的频率;
- $C(w_{i-n+1:i-1})$ 表示 $w_{i-n+1:i-1}$ 在语料库中出现的频率.

- 当 N=1 时, 称为 UniGram, 不考虑文本的上下关系, 此时的分母变为 $C_{total}$ 表示所有词的总数.
- 当 N=2 时, 称为 BiGrams, 考虑前一个词.
- 当 N=3 时, 称为 TriGrams, 考虑前两个词.

> 下面给出 BiGrams 语言模型的例子:
>
> 假设语料库里有五个句子:
> A. 脖子长是长颈鹿最醒目的特征之一.
> B. 脖子长使得长颈鹿看起来非常优雅, 并为其在获取食物上带来便利.
> C. 有了长脖子的加持, 长颈鹿可以观察到动物园里那些隐蔽的角落里发生的事情.
> D. 长颈鹿脖子和人类脖子一样, 只有七节颈椎, 也容易患劲椎病.
> E. 如同长颈鹿脖子由短变长的进化历程一样, 语言模型也在不断进化.
>
> 对于文本 `长颈鹿脖子长` 其出现的概率为: P(长颈鹿, 脖子, 长) = C(长颈鹿, 脖子) / C(长颈鹿) × C(脖子, 长) / C(脖子) = 2/5×2/6 = 2/15.
>
> 虽然这一文本并未出现在语料库中, 但 BiGrams 语言模型仍然可以得出其出现概率, 说明其具备**对未知文本的泛化能力**.
> 这是对传统基于规则的方法的优势.
> 但这种泛化能力会随着 N 的增大而逐渐减弱.
>
> 例如, 使用 TriGrams 语言模型同样对文本 `长颈鹿脖子长` 的出现概率进行计算:
> P(长颈鹿, 脖子, 长) = C(长颈鹿, 脖子, 长) / C(长颈鹿, 脖子) = 0.

N 代表了拟合语料库的能力和对未知文本的泛化能力之间的权衡.
- N 过大时, 很难在语料库中找到和 N-Grams 一样的序列, 可能出现大量的概率为 0 的情况 (可以通过平滑技术进行改善).
- N 过小时, 模型承载的语言信息不足, 不足以反映出语料库的特性.

***N-Grams 语言模型是在 N 阶马尔可夫假设下, 对语料库中出现的长度为 N 的词序列出现概率的极大似然估计***.
- **N 阶马尔可夫假设**: 当前状态出现的概率仅与前 N 个状态有关.
- **极大似然估计**: 离散随机变量 X 的分布律为 P(θ), 若有 N 个样本 Xi 来自 X, xi 为相应的观察值, 在参数 θ 下, 分布函数随机取到这 N 的观察值的概率为 P(x|θ) = Prod(p(xi;θ)), 构造似然函数为 L(θ|x) = Prod(p(xi;θ)), 极大似然估计即找到 θ 使得 L(θ|x) 最大.

基于上述定义, 设文本 $w_{1:K}$ 出现的概率为 $P(w_{1:K})$.
那么首先由条件概率链式法则:
$$
    P(w_{1:K}) = P(w_1) P(w_2|w_1) \cdots P(w_{K}|w_{1:K-1}) = \prod_{i=1}^{K} P(w_i|w_{1:i-1})
$$

然后根据 N 阶马尔可夫假设:
$$
    P(w_{1:K}) = \prod_{i=1}^{K} P(w_i|w_{1:i-1})\approx \prod_{i=1}^{K} P(w_i|w_{i-N:i-1})
$$

然后根据离散型随机变量的极大似然估计:
$$
    P(w_{1:K}) \approx P_{N-Grams}(w_{1:K}) = \prod_{i=1}^{K} \frac{C(w_{i-N:i})}{C(w_{i-N:i-1})}
$$

## 1.2.基于 RNN 的语言模型

## 1.3.基于 Transformer 的语言模型

## 1.4.语言模型的采样方法

## 1.5.语言模型的评测
