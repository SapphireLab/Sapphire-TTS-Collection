# 第 1 章 语言模型基础

- [原文](https://github.com/ZJU-LLMs/Foundations-of-LLMs/blob/main/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E6%95%99%E6%9D%90/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E5%88%86%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9/%E7%AC%AC1%E7%AB%A0%20%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80.pdf)

语言是一套复杂的符号系统.

语言符号通常在**音韵 (Phonology)**, **词法 (Morphology)**, **句法 (Syntax)** 的约束下构成, 并承载不同的**语义 (Semantics)**.

**语言是概率的, 且概率性和认知的概率性也存在密不可分的关系**:
语言符号具有不确定性, 例如
- 同样的语义可以由不同的音韵/词法/句法构成的符号来表达;
- 同样的音韵/词法/句法构成的符号也可以在不同语境下表达不同的语义;

**语言模型 (Language Models, LMs)** 旨在**准确预测语言符号的概率**.
- 从语言学角度: 语言模型可以赋能计算机掌握语法/理解语义, 以完成自然语言处理任务.
- 从认知科学的角度: 准确预测语言符号的概率可以赋能计算机描摹认知, 演化智能.

从 ELIZA 到 GPT-4, 语言模型经历了从规则模型到统计模型, 再到神经网络模型的发展历程, 逐步从呆板的机械式问答程序成长为具有强大泛化能力的多任务只能模型.

本章按照语言模型发展的顺序依次讲解
- 基于统计方法的 N-Grams 语言模型;
- 基于循环神经网络的语言模型;
- 基于 Transformer 的语言模型.

本章还介绍
- 将语言模型输出概率值解码为目标文本;
- 对语言模型的性能进行评估.
