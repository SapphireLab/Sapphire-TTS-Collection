# RIO

<details>
<summary>基本信息</summary>

- 标题: Robust Zero-Shot Text-to-Speech Synthesis with Reverse Inference Optimization
- 作者:
  - 01 [Yuchen Hu](../../Authors/Yuchen_Hu.md)
  - 02 [Chen Chen](../../Authors/Chen_Chen.md)
  - 03 [Siyin Wang](../../Authors/Siyin_Wang.md)
  - 04 [Eng Siong Chng](../../Authors/Eng_Siong_Chng.md)
  - 05 [Chao Zhang](../../Authors/Chao_Zhang.md)
- 机构:
  - [Nanyang Technological University](../../Institutions/SGP-NTU_新加坡南洋理工大学.md)
  - [清华大学](../../Institutions/CHN-THU_清华大学.md)
- 时间:
  - 预印时间: 2024.07.02 ArXiv v1
  - 更新笔记: 2024.07.07
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2407.02243)
  - [DOI]()
  - [Github]()
  - [Demo](https://github.com/YUCHEN005/RIO-TTS-demos)
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: 12
- 引用: 52
- 被引: 0
- 数据:
  - [GigaSpeech](../../Datasets/2021.06.13_GigaSpeech.md) 
  - [LibriTTS](../../Datasets/2019.04.05_LibriTTS.md)
  - [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md)
- 对比:
  - Backbone: [VoiceCraft](../../Models/Speech_LLM/2024.03.25_VoiceCraft.md)
  - RIO Framework + DPO
  - RIO Framework + ODPO
  - UNO-null
- 复现:
  - ?

</details>

## Abstract: 摘要

> In this paper, we propose ***Reverse Inference Optimization (RIO)***, a simple and effective method designed to enhance the robustness of autoregressive-model-based zero-shot text-to-speech (TTS) systems using reinforcement learning from human feedback (RLHF).
> To assess the quality of speech produced by the TTS system without human annotations, RIO introduces a novel concept termed as reverse inference based on the Bayesian principle, which suggests that a high-quality generated speech should be able to be used as a prompt for subsequent generation using the same TTS model. By leveraging reverse inference as the standard to select exemplars used in RLHF from the speech samples generated by the TTS system itself, RIO steers the subsequent optimization towards a direction of enhancing the TTS robustness. 
> The RIO framework, comprising sampling, automatic annotating, and learning, obviates the need for a reward model or pairwise preference data, and significantly improves the stability of zero-shot TTS performance by reducing the discrepancies between training and inference conditions. 
> Our experimental results verify that RIO can effectively improve both subjective and objective metrics, including mean opinion scores, word error rates, and speaker similarity. Remarkably, RIO can also diminish the incidence of bad outputs to nearly zero percent, rivalling the robustness when using ground-truth speech as the prompt.

## 1.Introduction: 引言

Large language models (LLMs),  capable of zero-shot and in-context learning, have transformed and unified the research and development of natural language processing tasks~\cite{brown2020language,chatgpt,gpt4,touvron2023llama,touvron2023llama2}. 
Inspired by the success of text LLMs, the next-token prediction paradigm has been generalised to tasks of other modalities~\cite{zhan2024anygpt,wu2023next,koh2024generating}, in particular, text-to-speech (TTS) synthesis implemented as an auto-regressive language model based on the neural codec tokens~\cite{neuralcodec,soundstream} extracted by discretizing the speech signals~\cite{audiolm,Audiopalm,speechgpt,wang2023neural}. 
With extensive text-speech training pairs, the in-context learning capabilities are also emergent in such TTS systems, enabling the model to perform transcript-conditioned speech continuation tasks by providing a short speech prompt~\cite{wang2023neural}. Since the speaker in the speech prompt can be unseen during training, this capability is often termed \textit{zero-shot TTS} and has attracted a surge of research interest in the speech community~\cite{vallex,wang2023viola,yang2023instructtts,basetts}.

Despite the significant advancements, the zero-shot TTS often has sub-optimal robustness that results in unstable speech synthesis performance~\cite{yao2012adaptation,huang2021context}, with an unignorable rate of abrupt truncation, repetition, and unnatural prosody~\cite{xin2024ralle}. 
We attribute this problem to the incomplete training of the codec-based auto-regressive language models, which suffer from the issue of \textit{exposure bias} well-known in speech recognition and other sequence modelling problems~\cite{bengio2015scheduled,pang2020text,gu2023minillm}. Specifically, the codec language model is only trained with the teacher-forcing strategy that predicts the next token by depending on the golden history sequence without considering the errors accumulated from the history in auto-regressive decoding at test time~\cite{ji2023survey}.

Recently, there has been a growing interest in integrating human evaluation into TTS optimization through reinforcement learning from human feedback (RLHF)~\cite{rafailov2024direct}, which effectively enhances the zero-shot capacity of pre-trained TTS models~\cite{speechalign,chen2024uno}.
RLHF typically follows a sampling-annotating-learning pipeline, in which human evaluation is applied to model-generated outputs to ensure they align with subjective human preferences. We suggest that this pipeline offers a viable strategy to address the discrepancy in history sequence between TTS training and test, as it exposes the model with the samples with self-generated history during training~\cite{ethayarajh2024kto}. 
To effectively handle such exposure bias issue in zero-shot TTS using RLHF, it is crucial to find a suitable ``preference'' function that can efficiently determine whether selecting a specific speech sample as a positive exemplar can lead to improved TTS robustness or not in RLHF framework. 

In this paper, we introduce \textbf{r}everse \textbf{i}nference \textbf{o}ptimization (RIO), an RLHF-related method tailored to improve the robustness of zero-shot TTS, which alleviates the train-inference mismatch through a sampling-annotating-learning pipeline based on a novel self ``preference'' function. 
Our proposed self ``preference'' function is built on \textit{reverse inference} defined based on the Bayesian formula, which relies on the assumption that a satisfactory speech sample generated by a robust zero-shot TTS system should serve as a good speech prompt to \textit{reversely} generate the original speech prompt using the same TTS system. %This assumption imposes an underlying requirement of \textit{perceptual consistency} that requires the TTS-produced speech samples to be perceived consistently as the human-produced speech samples by the TTS system as a speech prompt. 
This assumption imposes an underlying requirement of \textit{production-perception consistency} (PPC) that enforces the TTS-produced speech samples to be consistent with the human-produced speech samples when perceived by the TTS system as the speech prompt, which we humans can not notice. To this end, the self ``preference'' is defined as selecting the speech samples satisfying both forward and reverse TTS inference as the positive exemplars for the subsequent learning process. 

Besides the target TTS model, a pre-trained mean opinion score (MOS) estimator is used to assess the quality of a speech sample, which can thereby avoid the intensive human annotation in traditional RLHF. 
In addition, RIO eliminates the requirement of either reward model or pairwise preference data by directly maximizing the utility of speech generations to improve PPC in both the forward and reverse inference. 
Experimental results show that RIO considerably enhances the robustness of zero-shot TTS, and the quality of the synthesized speech is improved consistently in terms of MOS, word error rate (WER), speaker similarity (SIM), and the ratio of bad cases.

Our main contributions are summarized as follows:
\begin{itemize}
    \item By investigating the impact of synthesized speech prompts on zero-shot TTS, we propose and verify an assumption based on Bayesian reverse inference that PPC samples are correlated well with TTS robustness, providing new insights for understanding the difference between human and machine speech perception.      
    \item RIO, a sampling-annotating-learning pipeline is proposed to improve the TTS robustness by mitigating training-inference mismatch. The novelty of RIO lies in the exemplar selection strategy that utilizes the self ``preference'' provided by the TTS model to determine whether a speech sample is suitable for subsequent RLHF optimization. 
    %, instead of relying on human evaluation.  
    \item Intensive experiments are conducted to demonstrate the efficacy of RIO in both subjective and objective metrics. 
    In particular, RIO reduces the ratio of bad cases to nearly 0\%, which effectively resolves the robustness issue of codec-based auto-regressive zero-shot TTS. 
    
    %demonstrates its superior robustness over baselines.
\end{itemize}

## 2.Related Works: 相关工作

> **Neural codec language modeling** formulates TTS generation as a next-token prediction task and has gained increasing popularity in recent years~\cite{audiolm,Audiopalm,speechgpt}. Under this setup, the speech signal is firstly tokenized into sequences of discrete units based on vector quantization~\cite{neuralcodec,soundstream,zhang2023speechtokenizer,huang2023repcodec}, and then a decoder-only language model is trained based on these acoustic tokens~\cite{mohamed2022self,TortoiseTTS}. This approach has been demonstrated promising scalability to large data and model sizes, resulting in high-quality synthesized speech~\cite{wang2023neural,Spear-TTS}, emergent zero-shot capacity on unseen speakers~\cite{voicecraft,basetts,xin2024ralle}, style control~\cite{lyth2024natural,ji2024textrolspeech,liu2023promptstyle,yang2023instructtts}, and cross-lingual TTS~\cite{vallex,wang2023viola}.
>
> **RLHF-based optimization** is widely utilized for LLM alignment, where a reward model is trained with human-annotated data to calibrate the generative content~\cite{christiano2017deep,bai2022training, achiam2023gpt,dai2023safe}. Recent advancements focus on closed-form losses that directly operate on preference data, such as direct preference optimization (DPO)~\cite{rafailov2024direct} and its extensions~\cite{amini2024direct,zeng2024token,liu2024enhancing}. Moreover, the idea of ``self-rewarding'' is also proposed to infer preference data based on the model itself~\cite{chen2024self,yuan2024self}. 
> In the realm of TTS optimization, SpeechAlign~\cite{speechalign} presents the first method based on DPO that views ground truth as preferred samples while model generations as dispreferred samples. 
> UNO~\cite{chen2024uno} eliminates the dependence on such pairwise preference data and considers the annotation uncertainty due to subjective variability.

## 3.Methodology: 方法

### 3.1.Problem Formulation of Zero-shot TTS

> Given speech prompt $\mathbf{X}$ and its paired text prompt $\textbf{T}_\text{X}$, zero-shot TTS aims to synthesize a target speech $\mathbf{Y}$ based on target text $\textbf{T}_\text{Y}$ that clones the voice of the speaker in $\mathbf{X}$. With neural codec modelling, both $\mathbf{X}$ and $\mathbf{Y}$ are represented as a sequence of discrete acoustic tokens, and the auto-regressive inference process can be formulated as speech continuation that a trained codec language model predicts most possible speech sequence $\hat{\mathbf{Y}}$:

$$
    \hat{\mathbf{Y}} = \arg\max\nolimits_\textbf{Y}P (\mathbf{Y} | \textbf{T}_\text{Y}, \textbf{T}_\text{X}, \mathbf{X}),
$$

> and $\hat{\mathbf{Y}}$ typically consists of tokens of several codebooks from different residual vector quantizers (RVQs)~\cite{neuralcodec}. 
The tokens from the first quantizer are predicted in an auto-regressive manner, and then refined in a non-autoregressive manner to reconstruct the time-domain waveform~\cite{wang2023neural}.  

### 3.2.Reverse Inference

> For speech $\hat{\mathbf{Y}}$ generated by a text-to-speech (TTS) model, it is pertinent to explore whether $\hat{\mathbf{Y}}$ can serve as a prompt for further TTS inference. This leads us to investigate a ``reverse'' inference process in which $\hat{\mathbf{Y}}$ is used as input to predict the original speech prompt $\mathbf{X}$.
> Hereby, we propose a reverse inference that takes $\hat{\mathbf{Y}}$ and $\textbf{T}_\text{X}$ as the speech prompt and target text to synthesize a speech sample $\hat{\mathbf{X}}$: 

$$
    \hat{\mathbf{X}} = \arg\max\nolimits_{\textbf{X}} P (\mathbf{X} | \textbf{T}_\text{X}, \textbf{T}_\text{Y}, \hat{\mathbf{Y}}),
$$

> where the original target text $\textbf{T}_\text{Y}$ and the transcription of original speech prompt $\textbf{T}_\text{X}$ are swapped accordingly. Eq.~\eqref{eq-tts-infer} and Eq.~\eqref{eq-tts-reverse} can be connected using Bayes’ theorem as:

$$
\begin{aligned}
P(\mathbf{Y} | \textbf{T}_\text{Y}, \textbf{T}_\text{X}, \mathbf{X}) &= \frac{P(\mathbf{X}| \textbf{T}_\text{X}, \textbf{T}_\text{Y},\mathbf{Y}) \, P(\mathbf{Y} |\textbf{T}_\text{Y}, \textbf{T}_\text{X})}{P(\mathbf{X}| \textbf{T}_\text{X}, \textbf{T}_\text{Y})} \\
&= \frac{P(\mathbf{Y} |\textbf{T}_\text{Y})}{P(\mathbf{X}| \textbf{T}_\text{X})}P(\mathbf{X}| \textbf{T}_\text{X}, \textbf{T}_\text{Y}, \mathbf{Y})
\end{aligned}
$$

> where $(\textbf{T}_\text{X}, \mathbf{X})$ and $(\textbf{T}_\text{Y}, \mathbf{Y})$ are independent text-speech pairs, and  
> $P(\mathbf{X}|\textbf{T}_\text{X})$ and $P(\mathbf{Y}|\textbf{T}_\text{Y})$ are the priors of these pairs that can be viewed as constant. 
> From Eq.~\eqref{eq-bayes2}, training the TTS model to maximize $P(\mathbf{Y}|\textbf{T}_\text{Y},\textbf{T}_\text{X}, \mathbf{X})$ should maximize $P(\mathbf{X}| \textbf{T}_\text{X},\textbf{T}_\text{Y}, \mathbf{Y})$ simultaneously, which should naturally enable $P(\mathbf{X}| \textbf{T}_\text{X},\textbf{T}_\text{Y}, \hat{\mathbf{Y}})$ if $\hat{\mathbf{Y}}$ is a sufficiently good approximation to $\mathbf{Y}$. 

> \textbf{Empirical observation} shows that when applying pre-trained zero-shot TTS models to reverse inference, the $\hat{\mathbf{X}}$ generated based on Eq.~\eqref{eq-tts-reverse} sometimes has very low quality even if the corresponding $\hat{\mathbf{Y}}$ has high quality, in both subjective and objective metrics. 
> Specifically, two TTS models, VoiceCraft with 330 million (M) and 830M parameters referred to as $\bm{\theta}_1$ and $\bm{\theta}_2$, with similar zero-shot performances on a small validation dataset $\mathcal{D}_{\text{val}}$ are examined. $\mathcal{D}_{\text{val}}$ consists of 1000 data samples selected from the LibriSpeech dataset~\cite{panayotov2015librispeech}, whose corresponding synthetic speech samples produced by both $\bm{\theta}_1$ and $\bm{\theta}_2$ satisfy MOS > 3 (estimated by the MOSNet). 
> The results of the samples $\hat{\mathbf{X}}$ obtained by reverse inference are shown in Figure~\ref{f2} and Table~\ref{table:mos_330_830}. 

> Although both $\bm{\theta}_1$ and $\bm{\theta}_2$ generate good $\hat{\mathbf{Y}}$ with high MOS on $\mathcal{D}_{\text{val}}$, $\bm{\theta}_2$ generates much fewer bad $\hat{\mathbf{X}}$ (with MOS $\leqslant$ 3) compared to $\bm{\theta}_1$ ($9\%$ {vs.} $35\%$). This phenomenon reveals the underlying difference between human-produced speech samples and high-quality TTS-produced speech samples, which may be attributed to subtle changes that human listeners cannot perceive. %To resolve this issue, we propose to enhance TTS robustness 
> Therefore, to further reduce the discrepancy between the speech samples produced by humans and TTS, we propose to further optimize the TTS towards generating high-quality $\hat{\mathbf{X}}$, which requires the TTS speech production to be consistent with its own perception of the speech prompt. The next section will present how to achieve such consistency using RLHF with the PPC samples, which also reduces exposure bias.  

### 3.3.Optimization without Pairwise Preference Data

> With $K$ times of zero-shot TTS sampling, $I$ PPC speech samples with both good inference and reverse inference results are selected as exemplars. Conversely, $J$ bad cases are also selected as negative examples to inhibit generating such undesirable samples. In this process, {MOSNet} is employed as the discriminator according to a threshold (details are in Sec.~\ref{dataset}), hence $K$ can be large to provide representative samples. The bad cases are independent of the exemplars since they are obtained with different inputs, which are stored in a positive pool $\mathcal{P}_{\text{pos}}$ and a negative pool $\mathcal{P}_{\text{neg}}$ separately by: 

$$
\begin{aligned}
    \mathcal{P}_{\text{pos}} &= \{(\mathbf{X}_i, \hat{\mathbf{Y}}_i, \textbf{T}_{\text{X},i}, \textbf{T}_{\text{Y},i}) \ | \ \hat{\mathbf{Y}}_i \sim \pi_{\text{ref}} (\mathbf{X}_i, \textbf{T}_{\text{X},i}, \textbf{T}_{\text{Y},i}),i = 1,2, \ldots, I\} \\
    \mathcal{P}_{\text{neg}} &= \{(\mathbf{X}_j, \hat{\mathbf{Y}}_j, \textbf{T}_{\text{X},j}, \textbf{T}_{\text{Y},j} \ | \ \hat{\mathbf{Y}}_j \sim \pi_{\text{ref}} (\mathbf{X}_j, \textbf{T}_{\text{X},j}, \textbf{T}_{\text{Y},j}),j = 1,2, \ldots, J\},
\end{aligned}
$$

> where $\pi_{\text{ref}}$ denotes a frozen reference model that prevents the optimized model $\pi_{\theta}$ from making radical update. Since the samples in $\mathcal{P}_{\text{pos}}$ and $\mathcal{P}_{\text{pos}}$ are \textit{not} pairwise preference data, a ``reference point'' is used following prior works~\cite{ethayarajh2024kto}, which is estimated by a KL divergence item $\mathcal{Z}_\text{kl}$. That is,

$$
\begin{aligned}
\mathcal{Z}_{\text{kl}} = \mathbb{E}_{(\mathbf{X}, \hat{\mathbf{Y}}, \textbf{T}_{\text{X}}, \textbf{T}_{\text{Y}})\sim \mathcal{P}_{\text{pos}}\cup \mathcal{P}_{\text{neg}}} [ \text{KL}(\pi_{\theta}(\hat{\mathbf{Y}}|\mathbf{X}, \textbf{T}_{\text{X}}, \textbf{T}_{\text{Y}}) \Vert \pi_{\text{ref}}(\hat{\mathbf{Y}}|\mathbf{X}, \textbf{T}_{\text{X}}, \textbf{T}_{\text{Y}}) )],
\end{aligned}
$$

> where $(\mathbf{X}, \hat{\mathbf{Y}}, \textbf{T}_{\text{X}}, \textbf{T}_{\text{Y}})$ samples from each batch during training. Based on $\mathcal{Z}_\text{kl}$, the final optimization loss for the TTS system is written as:   

$$ 
\begin{aligned}
\mathcal{L}_{\text{tts}} (\pi_{\theta}, \pi_{\text{ref}})&= \mathbb{E}_{(\mathbf{X}, \hat{\mathbf{Y}}, \textbf{T}_{\text{X}}, \textbf{T}_{\text{Y}})\sim \mathcal{P}_{\text{pos}}\cup \mathcal{P}_{\text{neg}}} (1-\mathcal{V}_{\text{tts}}(\mathbf{X}, \hat{\mathbf{Y}}, \textbf{T}_{\text{X}}, \textbf{T}_{\text{Y}}))\\
\mathcal{V}_{\text{tts}}(\mathbf{X}, \hat{\mathbf{Y}}, \textbf{T}_{\text{X}}, \textbf{T}_{\text{Y}}) & = 
\begin{cases}
\sigma(\mathcal{R}(\mathbf{X}, \hat{\mathbf{Y}}, \textbf{T}_{\text{X}}, \textbf{T}_{\text{Y}}) - \mathcal{Z}_{\text{kl}}), & \text{if } (\mathbf{X}, \hat{\mathbf{Y}}, \textbf{T}_{\text{X}}, \textbf{T}_{\text{Y}}) \sim \mathcal{P}_{\text{pos}} \\
\sigma(\mathcal{Z}_{\text{kl}} - \mathcal{R}(\mathbf{X}, \hat{\mathbf{Y}}, \textbf{T}_{\text{X}}, \textbf{T}_{\text{Y}})), & \text{if } (\mathbf{X}, \hat{\mathbf{Y}}, \textbf{T}_{\text{X}}, \textbf{T}_{\text{Y}}) \sim \mathcal{P}_{\text{neg}} 
\end{cases} \\
\mathcal{R}(\mathbf{X}, \hat{\mathbf{Y}}, \textbf{T}_{\text{X}}, \textbf{T}_{\text{Y}}) &= \beta \cdot \log \frac{\pi_\theta(\hat{\mathbf{Y}} | \mathbf{X}, \textbf{T}_{\text{X}}, \textbf{T}_{\text{Y}})}{\pi_{\text{ref}}(\hat{\mathbf{Y}} | \mathbf{X}, \textbf{T}_{\text{X}}, \textbf{T}_{\text{Y}})}.
\end{aligned}
$$

> In Eq.~\eqref{eq-reward}, $\mathcal{R}(\mathbf{X}, \hat{\mathbf{Y}}, \textbf{T}_{\text{X}}, \textbf{T}_{\text{Y}})$ is the implicit reward modeling proposed in DPO~\cite{rafailov2024direct}, where $\beta$ serves as a factor controlling update step from $\pi_{\text{ref}}$. Eq.~\eqref{eq-value} reflects the core idea of optimization: Given a desirable sample from $\mathcal{P}_{\text{pos}}$, the corresponding implicit reward is maximized to boost the probability of $\pi_{\theta}$, conversely, an undesirable sample is suppressed to avoid such kind of inference. In this process, $\mathcal{Z}_\text{kl}$ is not involved in the backpropagation but it stabilizes the training process. $\sigma(\cdot)$ is the logistic function to keep $\mathcal{V}_{\text{tts}}$ less than 1, and minimizing the $\mathcal{L}_{\text{tts}}$ in Eq.~\eqref{eq-loss} is equal to maximize the $\mathcal{V}_{\text{tts}}$. \par

> Here the advantages of RIO in summary:
> (1) RIO only requires a desirable/undesirable label for each sample, thus supporting flexible annotating and eliminating the need for pairwise preference data. Based on the sample input $(\mathbf{X}, \textbf{T}_\text{X}, \textbf{T}_\text{Y})$, it is challenging for the same TTS model to simultaneously generate both a PPC exemplar and a bad sample to constitute the pairwise data required by DPO.
> (2) Since no human evaluation is required in this process, the sampling frequency $K$ can be increased significantly to ensure that the $\mathcal{P}_{\text{pos}}$ and $\mathcal{P}_{\text{neg}}$ contain a sufficient number of high/bad-quality examples.

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

> In this work, we present an RLHF-based optimization approach called RIO tailored to the zero-shot TTS system with neural codec modelling. RIO is novel for its reverse inference that aims to select PPC speech samples to enable the use of TTS-generated speech samples as prompt for further TTS generation. 
> Updating the TTS model using RLHF with these PPC samples helps to enhance the TTS robustness. 
> Experimental results show that without human annotation, RIO effectively enhances the capacity of an advanced zero-shot TTS system and surpasses other optimization baselines in terms of MOS, WER, and SIM, as well as considerably reduces the ratio of bad cases. Moreover, RIO proposes a new insight to analyze the robustness of zero-shot TTS inference, thereby providing a potential scheme for the deployment of the TTS system in practice.    
