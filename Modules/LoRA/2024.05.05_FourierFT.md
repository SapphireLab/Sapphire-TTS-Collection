# FourierFT

<details>
<summary>基本信息</summary>

- 标题: Parameter-Efficient Fine-Tuning with Discrete Fourier Transform
- 作者:
  - 01 [Ziqi Gao](../../Authors/Ziqi_Gao.md)
  - 02 [Qichao Wang](../../Authors/Qichao_Wang.md)
  - 03 [Aochuan Chen](../../Authors/Aochuan_Chen.md)
  - 04 [Zijing Liu](../../Authors/Zijing_Liu.md)
  - 05 [Bingzhe Wu](../../Authors/Bingzhe_Wu.md)
  - 06 [Liang Chen](../../Authors/Liang_Chen.md)
  - 07 [Jia Li](../../Authors/Jia_Li.md)
- 机构:
  - 机构 
- 时间:
  - 预印时间: 2024.05.05 ArXiv v1
  - 更新笔记: 2024.08.16
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2405.03003)
  - [DOI]()
  - [Github](https://github.com/chaos96/fourierft)
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=9524360008038380215)
- 标签:
  - [开源](../../Tags/OpenSource.md)
- 页数: ?
- 引用: ?
- 被引: ?
- 数据:
  - ?
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Low-rank adaptation~(LoRA) has recently gained much interest in fine-tuning foundation models. It effectively reduces the number of trainable parameters by incorporating low-rank matrices $A$ and $B$ to represent the weight change, i.e., $\Delta W=BA$. Despite LoRA's progress, it faces storage challenges when handling extensive customization adaptations or larger base models. In this work, we aim to further compress trainable parameters by enjoying the powerful expressiveness of the Fourier transform. Specifically, we introduce FourierFT, which treats $\Delta W$ as a matrix in the spatial domain and learns only a small fraction of its spectral coefficients. With the trained spectral coefficients, we implement the inverse discrete Fourier transform to recover $\Delta W$. Empirically, our FourierFT method shows comparable or better performance with fewer parameters than LoRA on various tasks, including natural language understanding, natural language generation, instruction tuning, and image classification. For example, when performing instruction tuning on the LLaMA2-7B model, FourierFT surpasses LoRA with only 0.064M trainable parameters, compared to LoRA's 33.5M. Our code is released at [Github](https://github.com/Chaos96/fourierft).

## 1.Introduction: 引言

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

> In this paper, we aim to achieve an extremely low storage memory for a single fine-tuning of large foundation models. This will enable the customization of multiple fine-tunings for different domains, tasks, or user preferences. To achieve this, we propose a simple yet powerful fine-tuning method that treats weight changes as spatial-domain matrices and only learns the sparse coefficients in the spectral domain. Compared to the LoRA-style baselines, our approach reduces the number of trainable parameters by about $8\sim500\times$ on a wide range of tasks in the NLP and CV domains.
