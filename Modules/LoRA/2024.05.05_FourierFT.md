# FourierFT

<details>
<summary>基本信息</summary>

- 标题: Parameter-Efficient Fine-Tuning with Discrete Fourier Transform
- 作者:
  - 01 [Ziqi Gao](../../Authors/Ziqi_Gao.md)
  - 02 [Qichao Wang](../../Authors/Qichao_Wang.md)
  - 03 [Aochuan Chen](../../Authors/Aochuan_Chen.md)
  - 04 [Zijing Liu](../../Authors/Zijing_Liu.md)
  - 05 [Bingzhe Wu](../../Authors/Bingzhe_Wu.md)
  - 06 [Liang Chen](../../Authors/Liang_Chen.md)
  - 07 [Jia Li](../../Authors/Jia_Li.md)
- 机构:
  - 机构 
- 时间:
  - 预印时间: 2024.05.05 ArXiv v1
  - 更新笔记: 2024.08.16
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2405.03003)
  - [DOI]()
  - [Github](https://github.com/chaos96/fourierft)
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=9524360008038380215)
- 标签:
  - [开源](../../Tags/OpenSource.md)
- 页数: ?
- 引用: ?
- 被引: ?
- 数据:
  - ?
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Low-rank adaptation~(LoRA) has recently gained much interest in fine-tuning foundation models. 
> It effectively reduces the number of trainable parameters by incorporating low-rank matrices $A$ and $B$ to represent the weight change, i.e., $\Delta W=BA$. 
> Despite LoRA's progress, it faces storage challenges when handling extensive customization adaptations or larger base models. 
> In this work, we aim to further compress trainable parameters by enjoying the powerful expressiveness of the Fourier transform. 
> Specifically, we introduce FourierFT, which treats $\Delta W$ as a matrix in the spatial domain and learns only a small fraction of its spectral coefficients. 
> With the trained spectral coefficients, we implement the inverse discrete Fourier transform to recover $\Delta W$. 
> Empirically, our FourierFT method shows comparable or better performance with fewer parameters than LoRA on various tasks, including natural language understanding, natural language generation, instruction tuning, and image classification. 
> For example, when performing instruction tuning on the LLaMA2-7B model, FourierFT surpasses LoRA with only 0.064M trainable parameters, compared to LoRA's 33.5M. 
> Our code is released at [Github](https://github.com/Chaos96/fourierft).

## 1.Introduction: 引言

> Large foundation models~(LFMs) have demonstrated exceptional performance on tasks of multiple domains, including natural language processing (NLP)~\cite{roberta,deberta,gpt2,gpt3,licommunity} and computer vision (CV)~\cite{cv_2,cv_1,cv_3,sd}. 
> Owing to their impressive capabilities, fine-tuning LFMs for a wide range of downstream tasks has become prevalent~\cite{gpt35,alpaca,ft_3}. 
> Under the full fine-tuning paradigm, the new model adapted to each customized task typically contains as many parameters as the original model~\cite{ft_3,ft_4,graphwiz,promptmsp}. 
> As models grow larger and customization needs expand, the demand for storing fine-tuned checkpoints rises, resulting in both costly storage and memory consumption.

> As a popular way to address this issue, LoRA \cite{lora} represents the weight change with two low-rank matrices $A$ and $B$, i.e., $W_0+\Delta W = W_0+BA$.
> Despite LoRA's superb performance, its large size of trainable parameters still brings high IT infrastructure consumption, which affects both ends of public communities and individual users. 
> For the former, an intuitive example is that a LoRA adapter~(fine-tuned weights) for a specific style of the stable diffusion model~\cite{sd} requires about 40MB of memory. 
> This necessitates the LFM communities (e.g., Civitai~\cite{civitai}) to bear high storage and bandwidth costs to cater to a large user base. 
> For the latter, fewer parameters mean direct RAM savings when loading fine-tuned weights in mobile APPs, enabling sufficient customization for individual users~\cite{pets1}. 
> To this end, we naturally ask the question: \textit{How can we aggressively compress trainable parameters even further for fine-tuning LFMs?}

> Previous works have demonstrated the powerful expressiveness of Fourier basis in data compression, where extremely sparse spectral information can be used to recover high-fidelity data~(e.g., 1D signal vectors \cite{fft_signal_1,fft_signal_2,fft_signal_3} and 2D image matrices \cite{fft_cv_1,fft_cv_2,fft_cv_3}). 
> More importantly, when dealing with more general (non-image) matrices that lack strong spatial semantics and are not frequency-sparse, Fourier transform can still handle recovery effectively \cite{evi_2,evi_3}. 
> Motivated by this, we investigate the potential for updating the weight change $\Delta W$ with its sparse spectral coefficients for fine-tuning LFMs. 

> In this paper, we aim to aggressively reduce the number of trainable parameters for fine-tuning LFMs. 
> To this end, we propose \textit{FourierFT} (\underline{Fourier} Transform for \underline{F}ine-\underline{T}uning), which treats the weight change $\Delta W$ as a matrix in the spatial domain, and learns its sparse spectral coefficients. 
> Specifically, we first randomly select $n$ spectral entries that are shared across all layers. 
> For each layer, FourierFT learns $n$ spectral coefficients located at these $n$ selected entries and then directly applies inverse discrete Fourier transform to compute the updated $\Delta W$. 
> Therefore, fine-tuning a pre-trained model with $L_{t}$ layers only requires storing $2n$ entry parameters and $nL_{t}$ coefficient parameters for FourierFT. 

> Empirically, we compare our method with state-of-the-art LoRA variants and other parameter-efficient fine-tuning methods on various tasks including (1) natural language understanding~(on the GLUE benchmark), (2) natural language generation~(on the E2E benchmark), (3) instruction tuning (with LLaMA-family models), and (4) image classification~(with vision transformers). 
> FourierFT can always achieve comparable or even better performance than LoRA, with about 6.0\%, 9.4\%, 0.2\% and 9.2\% of LoRA's trainable parameters for these 4 tasks, respectively. 
> For example in Figure \ref{fig:motivation}, on the instruction tuning task, our FourierFT method outperforms LoRA with only 64K trainable parameters. 
> Moreover, it achieves a comparable score to Full Fine-tuning with only 128K parameters.

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

> In this paper, we aim to achieve an extremely low storage memory for a single fine-tuning of large foundation models. 
> This will enable the customization of multiple fine-tunings for different domains, tasks, or user preferences. 
> To achieve this, we propose a simple yet powerful fine-tuning method that treats weight changes as spatial-domain matrices and only learns the sparse coefficients in the spectral domain. 
> Compared to the LoRA-style baselines, our approach reduces the number of trainable parameters by about $8\sim500\times$ on a wide range of tasks in the NLP and CV domains.
