# GPT-4o 背后可能的语音技术猜测

链接: [Youtube](https://www.youtube.com/watch?v=CgQ3lUOpXgc)
时间: 2024-05-20 (录制时间: 2024-05-19 晚)
文件: [Slide](Slide_2024.05.20_GPT-4o.pdf)

## 引言

GPT-4o (Generative Pre-trained Transformer for All) 其中一项特别令人瞩目的技术, 就是语音互动的功能. Demo 链接: [OpenAI](https://openai.com/index/hello-got-4o/) [Youtube](https://www.youtube.com/watch?v=DQacCB9tDaw)
Google 也出了 Project Astra 其中也打造了语音互动的技术, 这说明现在语音互动的技术是各个大厂都非常重视的一块. Demo 链接: [Youtube](https://www.youtube.com/watch?v=nXVvvRhiGjl)

GPT-4o 语音模式 (Voice Mode) 的特别之处:
- 丰富的语音风格; → 用文字指令要求使用不同的语调来说话, 例如轻声细语;
- 理解语音内容之外的信息; → 察言观色, 例如喘气;
- 发出非语言性的声音; → 例如笑声;
- 自然而即时的互动.

## 误解

截止 2024.05.19, GPT-4o 还没有正式发布, 所以现在的语音交互仍是旧版的 ChatGPT 语音界面.

旧版本 ChatGPT 语音界面的实现: 原始波形 → 语音识别 → 文本 → ChatGPT → 文本生成 → 语音合成 → 最终的语音输出. 当语音识别和语言合成运行速度足够快的话, 这种方式也能够达到还不错的实时互动.
但和新版本的 GPT-4o 比起来还是有一段差距. 例如语音识别将原始语音转换为文字, 那么语言模型可能无法获知说话人的情绪, 然后语音合成将之转化为音频时, 也就只有某一种说话风格.

原本的猜测: 直接在原有系统加上一些额外的模块. 
- 在语音识别之外添加一些事件侦测, 语音情绪识别的模块, 识别出语音的情绪, 然后将结果放在语音识别的结果后一同交给语言模型, 使得语言模型能够了解语音的相关情绪.
  参考文献:
  - [Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue](https://arxiv.org/abs/2312.15316)
  - [Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations](https://arxiv.org/abs/2402.12786)
- 让语言模型的输出除了文字内容外输出一些符号, 再输入语音合成系统中.
  如 Suno AI 开发的 [Bark](https://github.com/suno-ai/bark) 系统能够识别到 `(笑)` 产生笑声.
- 让语言模型产生一些文本指令, 告诉语音合成系统用什么样的语气进行合成.
  如 Meta 开发的 [AudioBox](https://audiobox.metademolab.com)
  参考文献:
  - [PromptTTS: Controllable Text-to-Speech with Text Descriptions](https://arxiv.org/abs/2211.12171)
  - [Towards General-Purpose Text-Instruction-Guided Voice Conversion](https://arxiv.org/abs/2309.14324)

所以结合现有的技术不是没可能打造类似 GPT-4o 的语音交互模型, 只是许多系统串联的实时性可能不够好, 需要通过大量的工程手段加快模型运行速度来获得实时性.
但 GPT-4o 在博客中说明其语音模式是一个**端到端**模型, 也就是只使用一个模型来处理所有事情, 而不是系统的串接. 这个模型是一个多模态模型, 但这里仅讨论声音部分.

## 类比

首先回顾文本模态的语言模型是如何训练的:
1. 预训练 (Pretrain): 用大量无标签数据训练模型, 参考课程 [Youtube](https://youtu.be/cCpErV7To2o?si=lfsIfaV7PwYqWNFg)
2. 微调 (Finetune): 用少量含标签数据微调模型, [Youtube](https://youtu.be/Q9cNkUPXUB8?si=qj573p9Ohl74qYk5)
3. 反馈微调 (RLHF): 用使用者反馈数据微调模型, [Youtube](https://youtu.be/v12IKvF6Cj8?si=hqaXTn1A5iSjy8Ig)

Finetune 和 RLHF 合起来又称为 Alignment.

那么语音模态的语言模型可能的原理是什么呢?

我们知道文本语言模型是做文字接龙, 给定一个未完成的句子, 它猜测接下来应该输出哪一个文字的 Token.
那么类比一下, 语音语言模型可能是做声音接龙, 可以是给定一段语音, 然后猜测接下来应该产生什么样的声音.

这样看来运行逻辑类似, 但语音语言模型有着自己独特挑战, 即**语音相较于文本更为复杂**. 

### 处理输入

对于一段原始波形, 其采样率为 16kHz, 即一秒钟的波形由 16000 个采样点构成. 而如果一次生成一个采样点, 那么以声音接龙的角度看要生成一秒钟的音频就需要运行 16000 次及预测, 显然这耗时过多. 
所以目前要做声音接龙不是直接对原始波形进行处理, 常见的做法是将原始波形进行压缩, 即通过使用一个内含 Codebook 的编码器 Encoder, Codebook 内有许多预定义的 code, 每个 code 用于指代某种类型的声音. 那么编码器会使用 Codebook 中的 code 来表示输入的原始波形, 所以一段语音会被表示为一个 code 序列, 这些 code 又被称为 Speech Unit. 然后训练一个解码器 Decoder, 读取这些语音单位转换为波形. 编码器和解码器同样是神经网络, 需要经过训练数据进行训练. 关于编码器和解码器可以参阅 [Overview](https://arxiv.org/abs/2402.13236)和 [Codec-SUPERB](https://arxiv.org/abs/2402.13071).
因此语音语言模型是运行在这些 Speech Units 上的, 即语音输入后会通过编码器转化为 Speech Unit 作为语音语言模型的输入, 然后语音语言模型预测下一个 Unit 是什么, 然后通过解码器产生波形. 故语音语言模型不需要直接生成复杂的原始波形. 
代表性的语音语言模型: Meta 的 [GSLM: Generative Spoken Language Modeling from Raw Audio](https://arxiv.org/abs/2102.01192), Google 的 [AudioLM: A Language Modeling Approach to Audio Generation](https://arxiv.org/abs/2209.03143), 但这已经不是最新的技术.

其实我们可以
- 将编码器视为一种特殊的语音识别, 不过其输出不是文字, 而是 Speech Unit. 
- 将解码器视为一种特殊的语音合成, 不过其输入不是文字, 而是 Speech Unit. 

那么为什么不直接将语音识别和语音合成作为编码器和解码器呢?

其实可以将语音识别视为一种特别的压缩方式, 即将语音信号转换为文字时本质上也是在做压缩, 文字本来就可以视为语音的浓缩版, 但问题在于文本仅仅只代表了语音中某个方面的信息, 即内容.
假如有一个人说了好好笑然后哈哈哈了几声, 那么语音识别系统可能只能够将代表内容的好好笑识别出来, 而丢失掉笑声, 从而导致合成的时候也只有好好笑这部分, 而没有笑声.

那么用 Speech Unit 的好处就在于可能可以保留文字所无法表达的信息.

但 Speech Unit 也有缺点, 即里面可能有很多 Unit 本来就是对应到某个文字的 Token, 这样会使得编码器重复学习这些已经有文字可以直接作为 Unit 的信号. 那么这样就有一个自然的想法, 就是将编码器和语音识别相结合, 即混合编码器, 解码器和语音合成相结合, 即混合解码器. 也就是能够识别成文字的部分就通过语音识别处理, 其他不能识别的就用 Speech Unit 表示.

例如前面提到的好好笑后接哈哈哈几声, 就会被识别为 `好好笑(笑声符号)`.
当然 GPT-4o 也许并没有采取这样的混合策略, 只是个人认为有额外的优势.

除此之外 GPT-4o 还能够分辨出不同说话人, 所以应该还采用了说话人自动分段标记的技术 Speaker Diarization 用于识别出不同说话人对应的语音段.

以上是语音语言模型在处理语音输入时可能采用的技术的猜想.

### 预训练

语言模型是用大量文字资料所训练出来的, 那语音语言模型也可以用大量声音的资料来进行训练. 我们完全可以用大量的声音资料来训练一个语音版的语言模型来做 Speech Unit 的接龙. 
那哪里去找大量的声音资料呢？那这个想起来非常的直觉, 网络上有这么多的影音平台, 从这些影音平台上就可以收集到大量的声音资料. OpenAI 显然在做这件事情, 因为在四月的时候纽约时报报道了 OpenAI 用了超过一百万小时的 Youtube 视频来训练他们的声言模型. 那时候就疑惑为什么要用视频来训练语言模型, 难道现在文字的资料已经全部都用完了, 真的没有文字的资料可以用了吗? 但是如果 OpenAI 是在训练语音语言模型那就可以理解了.

网络上的视频五花八门又不是全部都是干净的语音, 很多语音背后都有音效有背景音乐的, 那我们拿这些声音讯号来训练我们的语音版语言模型. 这个语音版语言模型会不会把音效背景, 音乐通通都学进去了呢？非常有可能.
GPT-4o 讲话的时候, 背景是有一个钢琴声的, 但我们不能排除现场正好有人在弹钢琴的可能. 但是如果说 GPT-4o 讲话的时候就是有可能会产生一些音效或者是背景音乐, 这也完全不意外. 那不过我觉得可以如果今天一个语音模型在讲话的时候就是会自带音效或自带BGM, 其实也是一个很酷的事情. 所以我们可以说这不是一个 Bug, 这是一个 Feature.

除此之外, GPT-4o 可以产生非常多样化和戏剧性的声音, 这是怎么办到的呢?

Amazon 在今年二月发表的论文 [BASE TTS](https://arxiv.org/abs/2402.08093), 用了超过十万个小时的语音来训练参数量为 1B 即十亿的语音合成模型. 当然这样大小的模型在文字领域并不是很大, 但对于语音合成模型来说已经非常大了. 发现是先前的语音合成系统, 因为他用的资料不够多, 所以往往合出来的声音非常的平淡. 但如果有大量训练数据时, 这些语音合成的模型就可以理解要读的内容, 而且根据要读的内容给予适当的变化. 
举例来说, `A profound sense of realization washed overmatter as he whispered, you've been there for me all along, haven't you? I never truly appreciated you until now.` 根据这篇论文说明并没有对输入的文字做任何特殊的处理, 但模型在读到 `whisper` 这个词时, 会用比较轻的声音来念这个句子. 这是模型根据数据学到的, 当然此处合出来的声音没有非常的戏剧化. 而 GPT-4o 声音的合成是可以更戏剧化的. 也许把资料从十万小时扩展到一百万小时的时候, 就是会有这种效果. 

另一方面, 单单使用语音数据来训练非常显然是不够的, 必须要利用文字信息. 

为什么用语音资料训练是不够的? 
例如一百万小时的语音, 即六千万分钟, 而一般人一分钟可以大概讲一百多个文字的 token, 那六千万分钟就可以讲六十亿个文字的 token. 看起来不少, 但例如 LLaMA3 使用了十五兆文字 token 进行训练, 所以一百万小时的语音只有 LLaMA3 预训练数据的两百五十分之一而已. 所以假设一个模型, 他只听了一百万小时的语音进行学习. 也许他可以学到一些语音信息, 但是他的知识可能会非常的不足.
所以模型不能单单只用语音声音的数据训练, 还得利用文字的信息. 

那怎么利用文字的信息呢? 

一个可能是语音语言模型, 是以原来的语言模型作为初始化的, OpenAI 已经创建了许多高性能语言模型, 那他们可能不会浪费这些语言模型已经学到的知识. 所以他们可以用原来已有的语言模型作为初始化去打造语音语言模型, 或者是用通俗的讲法, 就是我们可以去教原有的语言模型让他听懂语音.

前面提到语音其实会被表示成一堆语音单元, 而这些 Unit 对于语言模型来说, 就是一种全新的语言, 它有一套自己独特的符号系统. 所以当我们要教一个原来的语言模型听懂语音的时候, 其实对语言模型来说, 它就是在学一个全新的语言. 
当然另外有一个可能是把语音的符号跟文字全部放在一起训练.

总之, 单凭语音数据来训练, 一定是不行的. 一定要想办法利用文字信息. 

这个地方可能就可以展现混合模式的好处, 因为假设我们今天表示一段声音不是只用 Speech Unit, 而是有用到文字的话, 那对于语言模型来说, 学习起来可能会更为容易. 因为这些文字符号的意思早就了解了, 只需要花力气去了解这些新的符号是什么意思就好.

如何利用语音信息, 其实并不是只有固定一种方法而已. 还有很多其他让语音语言模型利用文字信息的方法, 可以参考下列文献.
- [Toward Joint Language Modeling for Speech Units and Text](https://arxiv.org/abs/2310.08715)
- [SpiRit-LM: Interleaved Spoken and Written Language Model](https://arxiv.org/abs/2402.05755)

## 对齐

除了预训练部分, 还需要进行 Alignment, 根据有标注的数据进行微调.
那在文本模型上, 你需要收集一些对话数据来微调你的模型, 让它能够好好跟人说话.
而对于语音模型, 我们可能就需要收集语音对话来训练它, 这就要收集语音的对话. 然后在这个对话里, 让某一个人去当做使用者, 某一个人扮演 AI 的角色, 用于微调语音语言模型, 让它听这个人说话, 然后要产生这个人的回应. 

但光是这样, 可能还不够.
GPT-4o 是固定某一个说话人的声音, 在做 Alignment 时想要让模型学会用某个人的声音来讲话, 那你可能需要收集该说话人和其他人的对话数据. 

那是不是需要录很多特定说话人跟其他人的对话呢?
也许不一定需要. 文字模型其实在微调时往往不需要太多数据, 因为模型在预训练时已经拥有非常丰富的知识, 微调只是只是画龙点睛. 那会不会做完语音的预训练以后, 模型已经很会模仿各种不同的人说话, 他只要听过几句特定说话人的话, 就可以很好的学会某个人说话的方式. 
那另外一个可能是就算没有很多对话语音, 也许可以用语音转换的技术. 把对话中各种不同人的声音都转成所需说话人的声音. 
但讲到目前为止, 看起来这个语音语言模型跟原来文字的语言模型非常的类似, 但是其实**语音模型跟文本模型还是有很多本质上的区别**.
举例来说现在用文字来跟 AI 互动的时候, 你有很明确的开始跟结束.
比如你输入一句话, 打完以后你会按 Enter, ChatGPT 就知道该轮到他讲话了, 他不用猜什么时候它该开始讲话, 结束也是一样. 你想打断也可以通过停止按钮停止生成. 但是语音界面不一样, 当有一个人对 AI 说我们来做一件有趣的事, 那稍微停顿一下. 这个时候 AI 必须要猜他到底应该接话, 说什么事, 还是这个人还没有讲完, 应该要等他继续讲完再进行答复. 
另外一方面, 也许今天有人叫语言模型讲一个故事, 他开始讲一个无聊的故事, 那人听了就很厌烦, 跟他说停停停, 这不是我要听的, 那语音版的语言模型, 他知不知道要停下来呢? 那也许说反正只要人说话我们就停下来. 那这个不是最好的解决问题的方法. 因为也许这个人讲话并不是要语言模型停下来, 所以并不是只要听到人的声音就必须停下来. 

所以你假设今天要让一个语音语言模型跟人有自然的互动, 就要让模型可以同时听和说.

原来的文字接龙的方式听跟说是分开的, 在说的时候, 它就很难去进行听这件事情.
那一个可能的解决方法是把听跟说这两件件事情分开来. 那这个技术过去也是有过的, 有一个模型叫做 [Dialogue GSLM: Generative Spoken Dialogue Language Modeling](https://arxiv.org/abs/2203.16502) 把听跟说分成两个不同的频道. 
所以有一个听的频道, 那这个是模型的麦克风, 它会在听人在说什么, 外界发生了什么声音.
另外一个频道是模型会记录自己发出过什么样的声音.
但这两个频道应该要是分开的, 而不是直接被混在一起的.

此时语音语言模型需要同时听这两个频道的内容, 包括现在人在说什么, 他之前讲过什么, 现在人说了个 `我` 那感觉这句话还没有说完, 所以语音版语言模型就输出 silent, 然后接下来人继续说, 也许人说到某一个段落, 那语音语言模型根据人现在说的话, 觉得那他说完了, 轮到我说了. 例如人说`我们来做个有趣的尝试`, 那语言模型也许就输出一个 `wow`, 代表说他很兴奋, 可以做这个尝试啊. 那接下来呢, 假设人是安静的没有进行发言, 语言模型就知道它可以继续说, 也许它就会说 `我好期待`. 但如果人仍然有发出声音, 就 `我要你做某件事情`. 那语言模型知道人还没有说完, 也许它就会输出代表安静的符号, 继续保持安静. 当然这并不是唯一让语言模型可以同时听跟说的方法了, 但可能还有其他的解法.

另外也许语言不只要同时听跟说, 还要同时听 + 说 + 看.
因为 GPT-4o 是可以一边看着东西一边说话的, 所以可能不只有听跟说两个频道, 还有一个视觉的频道, 专门读取外界的影像输入. 举例来说, 语言模型正在描述房间中的灯光, 讲的非常的起劲. 这时候有一个人跳出来比了一个 Yeah 的动作, 而语言模型无视了这个影像, 继续谈他的灯光, 不知道他有没有看到这个比 Yeah 的人. 这时候人打断语言模型, 说等一下你有没有看到什么奇怪的东西, 那语言模型显然是有看到比 Yeah 的这个人的, 只是在讲灯光的时候没有觉得需要对这个人发表任何的评论. 但是当有人问他说你有看到什么怪怪的东西的时候, 这个时候语言模型会做的事情, 比如会对每一个通道不只是他现在生成的通道, 还有其他听的通道, 看的通道, 通通去做注意力. 所以它会收集所有的信息, 然后得到一个答案, 那可能就会知道说有人比了一个 Yeah. 
那其实在 Google 的 Project Astra 里面也有一个非常类似的例子, 有一个人拿着手机随便走随便拍, 然后中间拍摄的过程中呢有看到了一个眼镜, 但是那个时候语言模型并没有对眼镜做出任何评论, 人也没有提到跟眼镜有关的事情, 然后人就跟语言模型聊了一下, 我们现在在哪里? 语言模型说在某某车站附近, 然后接下来人问说有没有看到我的眼镜? 这个时候眼镜已经不在画面里面了. 但是语言模型会对过去的信息去做 attention. 从过去看到的影像, 声音收集信息, 就得到它有看到眼镜在桌子上的结论. 这是一个蛮自然的互动的模式.


如果想知道更多有关语音语言模型的论文, 可以参阅 [Github](https://github.com/ga642381/speech-trident/).