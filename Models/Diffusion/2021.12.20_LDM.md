# LDM/Stable Diffusion

- 标题: High-Resolution Image Synthesis with Latent Diffusion Models
- 作者: 
  - 01 Robin Rombach
  - 02 Andreas Blattmann
  - 03 Dominik Lorenz
  - 04 Patrick Esser
  - 05 Bjorn Ommer
- 链接: 
  - [ArXiv](https://arxiv.org/abs/2112.10752)
  - [Publication](https://doi.org/10.1109/CVPR52688.2022.01123)
  - [Github](https://github.com/CompVis/latent-diffusion)
- 文件: 
  - [ArXiv](_PDF/2112.10752v2__LDM__High-Resolution_Image_Synthesis_with_Latent_Diffusion_Models.pdf)
  - [Publication] #TODO

## Abstract: 摘要

<details>
<summary>展开原文</summary>

> By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. 
> Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. 
> However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. 
> To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. 
> In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. 
> By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. 
> Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. 
> Code is available at this https URL.

</details>
<br>

通过将图像形成过程分解为降噪自编码器的序列应用, **扩散模型 (Diffusion Model, DM)** 实现了图像数据及其他领域实现了当前最佳合成结果.
此外, 它们的公式允许在不重新训练的情况下控制图像生成过程的引导机制.

然而, 由于这些模型通常直接在像素空间中操作, 强大的扩散模型的优化通常需要消耗数百 GPU 天数, 并且由于序列式评估导致推理成本高昂.

为了在有限的计算资源上训练扩散模型, 同时保留其质量和灵活性, 我们将它们应用于强大的预训练自编码器的潜在空间.

与先前的工作相比, 在这种表示上训练扩散模型首次能够在复杂性降低和细节保留之间达到近乎最优的平衡点, 大大提高了视觉保真度.
通过在模型架构中引入交叉注意力层, 我们将扩散模型转变为强大且灵活的生成器, 适用于一般的条件输入, 如文本或边界框, 而且以卷积方式实现高分辨率合成成为可能.

我们的***潜在扩散模型 (Latent Diffusion Model, LDM)*** 在图像修复方面实现了新的最先进水平, 并在各种任务中展现出色, 包括无条件图像生成, 语义场景合成和超分辨率, 同时相比基于像素的扩散模型显著降低了计算需求.
