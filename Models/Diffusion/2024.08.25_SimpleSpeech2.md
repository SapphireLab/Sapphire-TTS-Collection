# SimpleSpeech2

<details>
<summary>基本信息</summary>

- 标题: SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with Flow-based Scalar Latent Transformer Diffusion Models
- 作者:
  - 01 [Dongchao Yang](../../Authors/Dongchao_Yang_(杨东超).md)
  - 02 [Rongjie Huang](../../Authors/Rongjie_Huang_(黄融杰).md)
  - 03 [Yuanyuan Wang](../../Authors/Yuanyuan_Wang.md)
  - 04 [Haohan Guo](../../Authors/Haohan_Guo_(郭浩翰).md)
  - 05 [Dading Chong](../../Authors/Dading_Chong.md)
  - 06 [Songxiang Liu](../../Authors/Songxiang_Liu.md)
  - 07 [Xixin Wu](../../Authors/Xixin_Wu.md)
  - 08 [Helen Meng](../../Authors/Helen_Meng_(蒙美玲).md)
- 机构:
  - 机构 
- 时间:
  - 预印时间: 2024.08.25 ArXiv v1
  - 更新笔记: 2024.08.27
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2408.13893)
  - [DOI]()
  - [Github]()
  - [Demo](https://dongchaoyang.top/SimpleSpeech2_demo)
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: 13
- 引用: 73
- 被引: 0
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Scaling Text-to-speech (TTS) to large-scale datasets has been demonstrated as an effective method for improving the diversity and naturalness of synthesized speech. 
> At the high level, previous large-scale TTS models can be categorized into either Auto-regressive (AR) based (e.g., [VALL-E](../Speech_LLM/2023.01.05_VALL-E.md)) or Non-auto-regressive (NAR) based models (e.g., [NaturalSpeech2](2023.04.18_NaturalSpeech2.md); [NaturalSpeech3](2024.03.05_NaturalSpeech3.md)). 
> Although these works demonstrate good performance, they still have potential weaknesses. 
> For instance, AR-based models are plagued by unstable generation quality and slow generation speed; meanwhile, some NAR-based models need phoneme-level duration alignment information, thereby increasing the complexity of data pre-processing, model design, and loss design.
>  
> In this work, we build upon our previous publication by implementing a simple and efficient non-autoregressive (NAR) TTS framework, termed ***SimpleSpeech2***. 
> ***SimpleSpeech2*** effectively combines the strengths of both autoregressive (AR) and non-autoregressive (NAR) methods, offering the following key advantages: (1) simplified data preparation; (2) straightforward model and loss design; and (3) stable, high-quality generation performance with fast inference speed.
> Compared to our previous publication, we present 
> (1) a detailed analysis of the influence of speech tokenizer and noisy label for TTS performance; 
> (2) four distinct types of sentence duration predictors; 
> (3) a novel flow-based scalar latent transformer diffusion model. 
> 
> With these improvement, we show a significant improvement in generation performance and generation speed compared to our previous work and other state-of-the-art (SOTA) large-scale TTS models. 
> Furthermore, we show that SimpleSpeech 2 can be seamlessly extended to multilingual TTS by training it on multilingual speech datasets. 
> Demos are available on: https://dongchaoyang.top/SimpleSpeech2_demo/.

## 1.Introduction: 引言

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
