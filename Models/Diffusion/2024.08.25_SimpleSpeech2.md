# SimpleSpeech2

<details>
<summary>基本信息</summary>

- 标题: SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with Flow-based Scalar Latent Transformer Diffusion Models
- 作者:
  - 01 [Dongchao Yang](../../Authors/Dongchao_Yang_(杨东超).md)
  - 02 [Rongjie Huang](../../Authors/Rongjie_Huang_(黄融杰).md)
  - 03 [Yuanyuan Wang](../../Authors/Yuanyuan_Wang.md)
  - 04 [Haohan Guo](../../Authors/Haohan_Guo_(郭浩翰).md)
  - 05 [Dading Chong](../../Authors/Dading_Chong.md)
  - 06 [Songxiang Liu](../../Authors/Songxiang_Liu.md)
  - 07 [Xixin Wu](../../Authors/Xixin_Wu.md)
  - 08 [Helen Meng](../../Authors/Helen_Meng_(蒙美玲).md)
- 机构:
  - 机构 
- 时间:
  - 预印时间: 2024.08.25 ArXiv v1
  - 更新笔记: 2024.08.27
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2408.13893)
  - [DOI]()
  - [Github]()
  - [Demo](https://dongchaoyang.top/SimpleSpeech2_demo)
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: 13
- 引用: 73
- 被引: 0
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

> Scaling Text-to-speech (TTS) to large-scale datasets has been demonstrated as an effective method for improving the diversity and naturalness of synthesized speech. 
> At the high level, previous large-scale TTS models can be categorized into either Auto-regressive (AR) based (e.g., [VALL-E](../Speech_LLM/2023.01.05_VALL-E.md)) or Non-auto-regressive (NAR) based models (e.g., [NaturalSpeech2](2023.04.18_NaturalSpeech2.md); [NaturalSpeech3](2024.03.05_NaturalSpeech3.md)). 
> Although these works demonstrate good performance, they still have potential weaknesses. 
> For instance, AR-based models are plagued by unstable generation quality and slow generation speed; meanwhile, some NAR-based models need phoneme-level duration alignment information, thereby increasing the complexity of data pre-processing, model design, and loss design.
>  
> In this work, we build upon [our previous publication](2024.06.04_SimpleSpeech.md) by implementing a simple and efficient non-autoregressive (NAR) TTS framework, termed ***SimpleSpeech2***. 
> ***SimpleSpeech2*** effectively combines the strengths of both autoregressive (AR) and non-autoregressive (NAR) methods, offering the following key advantages: (1) simplified data preparation; (2) straightforward model and loss design; and (3) stable, high-quality generation performance with fast inference speed.
> Compared to [our previous publication](2024.06.04_SimpleSpeech.md), we present 
> (1) a detailed analysis of the influence of speech tokenizer and noisy label for TTS performance; 
> (2) four distinct types of sentence duration predictors; 
> (3) a novel flow-based scalar latent transformer diffusion model. 
> 
> With these improvement, we show a significant improvement in generation performance and generation speed compared to [our previous work](2024.06.04_SimpleSpeech.md) and other state-of-the-art (SOTA) large-scale TTS models. 
> Furthermore, we show that ***SimpleSpeech2*** can be seamlessly extended to multilingual TTS by training it on multilingual speech datasets. 
> Demos are available on: https://dongchaoyang.top/SimpleSpeech2_demo/.

</details>
<br>

将文本转语音 (Text-to-Speech, TTS) 扩展到大规模数据集已经被证明是提高合成语音多样性和自然性的有效方法.
先前的大规模文本转语音模型从高层次上可以分为基于自回归 (Auto-Regressive, AR) 的模型 (如 [VALL-E](../Speech_LLM/2023.01.05_VALL-E.md)) 和基于非自回归 (Non-Auto-Regressive, NAR) 的模型 (如 [NaturalSpeech2](2023.04.18_NaturalSpeech2.md); [NaturalSpeech3](2024.03.05_NaturalSpeech3.md)) 两类.
尽管这些工作展现了良好的性能, 但仍然存在潜在的弱点.
例如, 基于自回归的模型受到生成质量不稳定和生成速度慢的困扰; 而一些基于非自回归的模型需要音素级别的时长对齐信息, 从而增加了数据预处理, 模型设计和损失函数设计的复杂度.

在这项工作中, 在[我们之前工作](2024.06.04_SimpleSpeech.md)的基础上实现了一个简单高效的非自回归文本转语音框架, 称为 ***SimpleSpeech2***.
***SimpleSpeech2*** 有效结合了自回归 (AR) 和非自回归 (NAR) 方法的优势, 提供了以下关键优势: (1) 简化的数据准备; (2) 直观的模型和损失函数设计; (3) 稳定, 高质量的生成性能和快速推理速度.

相比[我们之前的工作](2024.06.04_SimpleSpeech.md), 我们展示了
(1) 语音分词器和噪声标签对文本转语音性能的影响的详细分析;
(2) 四种不同的句子时长预测器;
(3) 一种新的基于流的标量隐 Transformer 扩散模型.

通过这些改进, 我们展示了和[先前工作](2024.06.04_SimpleSpeech.md)以及其他最先进的大规模文本转语音相比, 生成性能和生成速度的显著提升.
此外, 我们表明, ***SimpleSpeech2*** 可以通过在多语言语音数据集上训练来无缝地扩展到多语言文本转语音.
示例在: https://dongchaoyang.top/SimpleSpeech2_demo/

## 1.Introduction: 引言

> Text-to-Speech synthesis (TTS) endeavors to synthesize speech that is both intelligible and natural at a human level, a field that has witnessed significant advancements over the past few years due to the proliferation of deep learning technologies \cite{tecatron2,fastspeech2,vits,glow-tts,grad-tt,transformer-tts,ns1}. These TTS models have successfully synthesized speech that is not only intelligible but also of high quality, albeit generally limited to specific styles, speakers, and languages. This limitation primarily stems from the fact that they are trained on small-scale, high-quality, labeled speech datasets.
> At a high level of modeling, previous TTS models are usually categorized into two types: auto-regressive (AR) and non-auto-regressive (NAR) models. Generally, NAR-based models outperform AR-based models in terms of generation speed and robustness \cite{fastspeech2}. However, AR-based models exhibit superior diversity, prosody, expressiveness, and flexibility, attributable to their implicit duration modeling and AR sampling strategy \cite{tecatron2}. Furthermore, many NAR-based models \cite{fastspeech2} depend on fine-grained alignment information (phoneme-level duration), which complicates data pre-processing. Although numerous studies \cite{grad-tt,glow-tts,vits} have suggested employing the Monotonic Alignment Search (MAS) strategy to learn alignment information, this approach not only increases training complexity but also results in alignments that may not always be suitable. Most critically, the rigid boundary between phoneme and speech representation can lead to unnatural prosody \cite{seedtss}.
>
> Recently, an increasing number of researchers have concentrated on synthesizing diverse and natural speech, particularly focusing on the diversity of speaking styles, achieving human-level naturalness, and zero-shot speaker cloning. To achieve these objectives, they propose utilizing large-scale datasets to train Text-to-Speech (TTS) models \cite{borsos2023audiolm,valle,speartts,uniaudio,make-a-voice}.
> For example, language model (LM)-based TTS systems, such as VALL-E \cite{valle}, employ a pre-trained audio codec model (Encodec \cite{encodec}) to convert the speech signal into a sequence of discrete tokens. Subsequently, an auto-regressive (AR) language model is trained to predict these speech tokens based on phonemes and speaker prompts. LM-based TTS models streamline data pre-processing, and their structure and training strategies draw from the successes of large language models in natural language processing (NLP) \cite{llama2,gpt4}. Moreover, these models are capable of generating expressive and diverse speech, thanks to their implicit duration modeling and sampling strategy. However, they suffer from slow and unstable generation due to the nature of AR sampling. To mitigate these drawbacks, large-scale non-autoregressive (NAR) models \cite{soundstorm,ns2,voicebox,megatts} have been proposed, such as NaturalSpeech 2/3. Although NAR-based large-scale TTS models exhibit higher inference speeds and enhanced stability, they require extra effort in data pre-processing, such as using their internal alignment tools to secure the alignment between speech and phonemes \cite{ns2,ns3}. Additionally, the persistent issue of hard boundary alignment continues to limit the natural prosody in NAR-based models \cite{seedtss}. We summarize the strengths and potential weaknesses for current AR-based and NAR-based large-scale TTS models in Table \ref{tab1}. We can see that both AR-based and NAR-based large TTS models still have considerable room for improvement in terms of performance, efficiency, or data requirements. Consequently, the primary research question is \textbf{whether we can develop a simple, efficient, and stable TTS system capable of synthesizing diverse, natural, and controllable speech}.
>
> Considering the complexity and long sequence of the speech signal, most large TTS models use a speech tokenizer \footnote{In this study, we refer to the intermediate representation extractor as a speech tokenizer. In the audio field, the speech tokenizer can be a codec model or Variational Auto Encoder (VAE).} to extract the intermediate representation from speech, \textit{e.g.} discrete tokens of audio codec model \cite{valle,uniaudio} or continuous embeddings from VAE models \cite{ns2}, then applying generative models on the intermediate representation. However, previous models pay more attention to improving the performance of TTS systems by scaling the training data and model size. The research about the influence of speech tokenizers on the TTS models is scarce. In other words, previous works tell us how to utilize a pre-trained speech tokenizer for large TTS systems, but do not disclose what is the most suitable speech tokenizer for TTS systems. We think the speech tokenizer is a key point for the success of large TTS models. Thus, the second research problem is \textbf{what is a good speech tokenizer for large TTS models}. In this study, we propose to focus on two aspects of the speech tokenizer: 
> - Completeness: The speech tokenizer should recover the compressed speech without loss.
> - Compactness: The speech tokenizer should encode the speech into fewer parameters or a smaller space.
>
> In Section \ref{sub:good tokenizer}, we first give a comprehensive performance comparison for different speech tokenizers, including our proposed SQ-Codec, Encodec, and Variational Auto Encoder (VAE). Then we give a detailed analysis of the influence of completeness and compactness on speech generation. 
>
> Large-scale datasets are crucial for the development of advanced Text-to-Speech (TTS) systems \cite{valle,ns2,ns3,uniaudio,voicebox,voicecraft}. While prior studies have demonstrated that large-scale Automatic Speech Recognition (ASR) datasets can be employed to train TTS systems, the details of obtaining text-speech pairs are usually neglected. In this study, we utilize publicly available ASR models (Whisper \cite{whisper}), to generate the transcriptions. However, these transcriptions may be inaccurate and are thus considered noisy labels. The third research question we address is: \textbf{Can a dataset comprising noisy labels effectively train a high-quality TTS system?} In Section \ref{sec:cfc}, we give a theoretical analysis that when a large-scale dataset includes a small number of noisy labels, which is equal to introducing the classifier-free guidance training \cite{cfc} for the model optimization.
>
> Our contributions can be summarized as follows:
> - We introduce SimpleSpeech 2, a simple and efficient NAR framework for text-to-speech, building upon our previous work \cite{simplespeech}. This framework effectively combines the advantages of current AR-based and NAR-based models. 
> - We conduct a comprehensive series of contrast experiments to explore the impact of different tokenizers on TTS systems, and investigate the influence of completeness and compactness of speech tokenizer for TTS performance.
> - We explore the influence of sentence duration predictor, and show 4 distinct duration predictors.
> - We propose a novel flow-based scalar transformer diffusion model that is faster and more stable than the previous DDPM-based diffusion \cite{simplespeech}. Furthermore, we also introduce an advanced transformer backbone from large language models, \textit{e.g.} time mixture-of-experts.
>
> The rest of this paper is organized as follows: In Section 2, we motivate our study by introducing the background and related work. In Section~\ref{sec4:Proposed method}, we introduce the details of our proposed methods. The experimental setting, evaluation metrics, and results are presented from Section~\ref{sec:exp set} to Section~\ref{exp: experiments}. The study is concluded in Section~\ref{sec:conclusion}.

## 2.Related Works: 相关工作

### 2.1.Speech Tokenizer

> In this study, we define an \textit{audio/speech tokenizer} as a model that maps complex audio data into a latent space (latent representation), which can subsequently be reconstructed from this latent space back into audio. The latent representation may take the form of either discrete tokens or continuous vectors. Within the existing literature, the concept of an audio tokenizer has been extensively investigated \cite{yang2023hifi,encodec,soundstream,dac,make-an-audio}. These studies generally fall into two categories: Audio Codecs and Variance AutoEncoders (VAEs). Audio Codecs aim to quantize audio data into a set of discrete tokens, whereas VAEs are designed to model the mean and variance distributions of audio data.
>
> Although audio codec models have been widely used in audio generation tasks, they face the problem of codebook collapse and need complex training loss combinations. Furthermore, the audio codec quantizes each speech frame into multiple tokens with RVQ, this will ensure high-quality audio reconstruction but will cause difficulty in autoregressive model generation \cite{valle,speartts} (error propagation and robust issues) due to the increased length of the token sequence. The VAE model or the continuous representations from audio codec are also can be used as the prediction target of generative models. For instance, previous works \cite{ns2,ditto-tts} follow the Latent Diffusion Model (LDM) \cite{ldm} to train a diffusion model to fit the distribution of continuous representations. However, developing a good VAE model also requires balancing the variance level with the reconstruction performance. A typical example is that data compressed with AE might reconstruct better than those with VAE, but the complexity retained in the compressed data distribution still poses challenges for the generative model. Thus, current VAE models in the audio and image generation fields imposes a slight Kullback-Leibler (KL) penalty towards a standard normal on the learned latent \cite{ldm,make-an-audio}. 
> In this study, we propose to use scalar quantization to replace the Residual Vector Quantization (RVQ) in previous audio codec works, named SQ-Codec. We demonstrate that SQ-Codec not only achieves good reconstruction performance but also provides a finite, compact latent space, which is highly suitable for generative models.

### 2.2.Large-Scale Text-To-Speech

> Large-scale text-to-speech (TTS) research has made significant strides. These methods can be divided into two categories: autoregressive (AR) TTS based on large language models (LLMs) and non-autoregressive (NAR) TTS. AR-based TTS models got great attention, for instance, AudioLM \cite{borsos2023audiolm}, VALL-E \cite{valle}, SpearTTS \cite{speartts}, VoiceCraft \cite{voicecraft}, and UniAudio \cite{uniaudio} use audio codec (\textit{e.g.} Encodec \cite{encodec} or SoundStream \cite{soundstream}) for mapping speech to tokens, framing text-to-speech tasks as AR language modeling tasks, thus enabling zero-shot capabilities in the speech domain. AR-based TTS models are easy to train because they only rely on speech-transcription pairs data, but they also face the problem of slow generation speed and unstable generation quality. To improve its robustness, VALLE-R \cite{valle-r}, RALL-E \cite{rall-e} proposes to use more alignment information (\textit{e.g} phone duration) in the training stage, but such strategy also increases the difficulty of data preparation. 
>
> The other type relies on NAR generative models, for instance, NaturalSpeech 2 \cite{ns2} proposes to model the continuous representation of the audio codec model by latent diffusion models. NaturalSpeech 2 is an NAR-based model, which has faster generation speed and better robustness than previous AR-based models. NaturalSpeech 3 \cite{ns3} proposes to encode different attributes (\textit{e.g.} content and prosody) into different codebooks, then applying discrete diffusion models \cite{diffsound,soundstorm,yang2024instructtts} on them. One of the drawbacks in NaturalSpeech 2/3 is that the system training needs fine-grained alignment information (\textit{e.g.} phone-level duration), which significantly hinders the use of large-scale in the wild data. Furthermore, the hard boundary alignment between phoneme and speech representation may result in unnatural prosody. Similarly, Voicebox \cite{voicebox} and Audiobox \cite{audiobox} propose a pre-training and fine-tuning strategy based on flow matching \cite{flow-matching}. HierSpeech ++ \cite{hierspeech++} proposes a hierarchical variational inference method. E3-TTS proposes to generate fixed-length audio based on text using diffusion models without any duration information, which significantly simplifies the data preparation for NAR TTS training. However, the fixed length generation also limits the flexibility of the model. Our recent publication SimpleSpeech \cite{simplespeech} proposes to use a sentence duration to control the length of audio, and we demonstrate that the sentence duration is a effective way for NAR TTS. In this study, we follow the line of our previous publication SimpleSpeech uses sentence duration to control the speech length. In addition, we investigate more strategies to obtain the sentence duration, refer to Section \ref{sec:sentence duration} to find the details. 
>
> We notice that some concurrent works, SeedTTS \cite{seedtss} and DiTTo-TTS \cite{ditto-tts} also adopt a similar idea to use the sentence duration to control the generated speech length in an NAR framework. We claim that our study is one of the earliest works to adopt this idea, and our proposed method is different from theirs.

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
