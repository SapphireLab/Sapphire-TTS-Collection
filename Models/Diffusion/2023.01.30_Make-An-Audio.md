# Make-An-Audio

<details>
<summary>基本信息</summary>

- 标题: "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models"
- 作者:
  - 01 Rongjie Huang,
  - 02 Jiawei Huang,
  - 03 Dongchao Yang,
  - 04 Yi Ren,
  - 05 Luping Liu,
  - 06 Mingze Li,
  - 07 Zhenhui Ye,
  - 08 Jinglin Liu,
  - 09 Xiang Yin,
  - 10 Zhou Zhao
- 链接:
  - [ArXiv](https://arxiv.org/abs/2301.12661)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv] #TODO
  - [Publication] #TODO

</details>

## Abstract: 摘要

Large-scale multimodal generative modeling has created milestones in text-to-image and text-to-video generation.
Its application to audio still lags behind for two main reasons: the lack of large-scale datasets with high-quality text-audio pairs, and the complexity of modeling long continuous audio data.
In this work, we propose Make-An-Audio with a prompt-enhanced diffusion model that addresses these gaps by 1) introducing pseudo prompt enhancement with a distill-then-reprogram approach, it alleviates data scarcity with orders of magnitude concept compositions by using language-free audios; 2) leveraging spectrogram autoencoder to predict the self-supervised audio representation instead of waveforms.
Together with robust contrastive language-audio pretraining (CLAP) representations, Make-An-Audio achieves state-of-the-art results in both objective and subjective benchmark evaluation.
Moreover, we present its controllability and generalization for X-to-Audio with "No Modality Left Behind", for the first time unlocking the ability to generate high-definition, high-fidelity audios given a user-defined modality input.
Audio samples are available at [this https URL](https://text-to-audio.github.io/).

## 1.Introduction: 引言

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
