# E2 TTS

<details>
<summary>基本信息</summary>

- 标题: "E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS"
- 作者:
  - 01 Sefik Emre Eskimez
  - 02 Xiaofei Wang
  - 03 Manthan Thakker
  - 04 Canrun Li
  - 05 Chung-Hsien Tsai
  - 06 Zhen Xiao
  - 07 Hemin Yang
  - 08 Zirun Zhu
  - 09 Min Tang
  - 10 Xu Tan (谭旭)
  - 11 Yanqing Liu
  - 12 Sheng Zhao (赵胜)
  - 13 Naoyuki Kanda
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.18009)
  - [Publication]()
  - [Github]
    - 复现 2024.07.10 [lucidrains/e2-tts-pytorch](https://github.com/lucidrains/e2-tts-pytorch)
  - [Demo](https://aka.ms/e2tts/)
- 文件:
  - [ArXiv](_PDF/2406.18009v2__E2_TTS__Embarrassingly_Easy_Fully_Non-AutoRegressive_Zero-Shot_TTS.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

This paper introduces ***Embarrassingly Easy Text-to-Speech (E2 TTS)***, a fully non-autoregressive zero-shot text-to-speech system that offers human-level naturalness and state-of-the-art speaker similarity and intelligibility.
In the ***E2 TTS*** framework, the text input is converted into a character sequence with filler tokens.
The flow-matching-based mel spectrogram generator is then trained based on the audio infilling task.
Unlike many previous works, it does not require additional components (e.g., duration model, grapheme-to-phoneme) or complex techniques (e.g., monotonic alignment search).
Despite its simplicity, ***E2 TTS*** achieves state-of-the-art zero-shot TTS capabilities that are comparable to or surpass previous works, including Voicebox and NaturalSpeech 3.
The simplicity of ***E2 TTS*** also allows for flexibility in the input representation.
We propose several variants of ***E2 TTS*** to improve usability during inference.
See this https [URL](https://aka.ms/e2tts/) for demo samples.

## 1·Introduction: 引言

In recent years, text-to-speech (TTS) systems have seen significant improvements (**FastSpeech** [^1], **FastSpeech2** [^2], **Glow-TTS** [^3], **HiFi-GAN** [^4]), achieving a level of naturalness that is indistinguishable from human speech (**NaturalSpeech** [^5]).
This advancement has further led to research efforts to generate natural speech for any speaker from a short audio sample, often referred to as an audio prompt.
Early studies of zero-shot TTS used speaker embedding to condition the TTS system [^6] [^7].
More recently, **VALL-E** [^8] proposed formulating the zero-shot TTS problem as a language modeling problem in the neural codec domain, achieving significantly improved speaker similarity while maintaining a simplistic model architecture.
Various extensions were proposed to improve stability (**SpeechX** [^9], **VALL-T** [^10], **RALL-E** [^11]), and **VALL-E 2** [^12] recently achieved human-level zero-shot TTS with techniques including repetition-aware sampling and grouped code modeling.

While the neural codec language model-based zero-shot TTS achieved promising results, there are still a few limitations based on its auto-regressive (AR) model-based architecture.
Firstly, because the codec token needs to be sampled sequentially, it inevitably increases the inference latency.
Secondly, a dedicated effort to figure out the best tokenizer (both for text tokens and audio tokens) is necessary to achieve the best quality (**SpeechX** [^9]).
Thirdly, it is required to use some tricks to stably handle long sequences of audio codecs, such as the combination of AR and non-autoregressive (NAR) modeling (**VALL-E** [^8],**VALL-E 2** [^12]), multi-scale transformer (**UniAudio** [^13]), grouped code modeling (**VALL-E 2** [^12]).

Meanwhile, several fully NAR zero-shot TTS models have been proposed with promising results.
Unlike AR-based models, fully NAR models enjoy fast inference based on parallel processing.
**NaturalSpeech2** [^14] and **NaturalSpeech3** [^15] estimate the latent vectors of a neural audio codec based on diffusion models (**DDPM** [^16], [^17]).
**Voicebox** [^18] and **MatchaTTS** [^19] used a **flow-matching** [^20] model conditioned by an input text.
However, one notable challenge for such NAR models is how to obtain the alignment between the input text and the output audio, whose length is significantly different.
**NaturalSpeech2** [^14], **NaturalSpeech3** [^15], and **Voicebox** used a frame-wise phoneme alignment for training the model.
**MatchaTTS** [^19], on the other hand, used monotonic alignment search (MAS) (**Grad-TTS** [^21], **Glow-TTS** [^3]) to automatically find the alignment between input and output.
While MAS could alleviate the necessity of the frame-wise phoneme aligner, it still requires an independent duration model to estimate the duration of each phoneme during inference.
More recently, **E3 TTS** [^22] proposed using cross-attention from the input sequence, which required a carefully designed **U-Net** [^23] architecture.
As such, fully NAR zero-shot TTS models require either an explicit duration model or a carefully designed architecture.
One of our findings in this paper is that such techniques are not necessary to achieve high-quality zero-shot TTS, and they are sometimes even harmful to naturalness.
<u>Footnote: Concurrent with our work, **Seed-TTS** [^24] proposed a diffusion model-based zero-shot TTS, named **Seed-TTS**$_{DiT}$.
Although it appears to share many similarities with our approach, the authors did not elaborate on the details of their model, making it challenging to compare with our work.</u>

Another complexity in TTS systems is the choice of the text tokenizer.
As discussed above, the AR-model-based system requires a careful selection of tokenizer to achieve the best result.
On the other hand, most fully NAR models assume a monotonic alignment between text and output, with the exception of E3 TTS, which uses cross-attention.
These models impose constraints on the input format and often require a text normalizer to avoid invalid input formats.
When the model is trained based on phonemes, a grapheme-to-phoneme converter is additionally required.

In this paper, we propose ***Embarrassingly Easy TTS (E2 TTS)***, a fully NAR zero-shot TTS system with a surprisingly simple architecture.
***E2 TTS*** consists of only two modules: a flow-matching-based mel spectrogram generator and a vocoder.
The text input is converted into a character sequence with filler tokens to match the length of the input character sequence and the output mel-filterbank sequence.
The mel spectrogram generator, composed of a vanilla Transformer with U-Net style skip connections, is trained using a speech-infilling task (**Voicebox** [^18]).
Despite its simplicity, ***E2 TTS*** achieves state-of-the-art zero-shot TTS capabilities that are comparable to, or surpass, previous works, including Voicebox and NaturalSpeech 3.
The simplicity of ***E2 TTS*** also allows for flexibility in the input representation.
We propose several variants of ***E2 TTS*** to improve usability during inference.

注: 对原文内容进行了重排
- 第三章 方法 对应 原文第二章
- 第四章 实验 对应 原文第三章前三节
- 第五章 结果 对应 原文第三章后三节
- 第六章 结论 对应 原文第四章

## 2·Related Works: 相关工作

~~None~~

## 3·Methodology: 方法

<!-- Fig.01. An overview of the training (left) and the inference (right) processes of ***E2 TTS***. -->

### 3.1·Training: 训练

<details>
<summary>原文</summary>

Fig.01 (a) provides an overview of ***E2 TTS*** training.
Suppose we have a training audio sample $s$ with transcription $y=(c_1, c_2, ..., c_M)$, where $c_i$ represents the $i$-th character of the transcription.
<u>Footnote: Alternatively, we can represent $y$ as a sequence of Unicode bytes [^25].</u>
First, we extract its mel-filterbank features $\hat{s}\in\mathbb{R}^{D\times T}$, where $D$ denotes the feature dimension and $T$ represents the sequence length.
We then create an extended character sequence $\tilde{y}$, where a special filler token $\langle F\rangle$ is appended to $y$ to make the length of $\tilde{y}$ equal to $T$.
<u>Footnote: We assume $M\le T$, which is almost always valid.</u>

</details>
<br>

![Images/2024.06.26.E2_TTS.Fig.01a.png](Images/2024.06.26_E2_TTS_Fig.01a.png)

图 01 (a) 展示了 ***E2 TTS*** 的训练过程.
假设我们有训练用的音频样本 $s$, 对应的转写文本为 $y=(c_1, c_2, ..., c_M)$, 其中 $c_i$ 表示第 $i$ 个字符. <u>注: 也可以用 Unicode 字节序列来表示 $y$.</u>

首先, 我们提取其梅尔滤波器组特征 $\hat{s}\in\mathbb{R}^{D\times T}$, 其中 $D$ 表示特征维度, $T$ 表示序列长度.
然后, 我们创建扩展的字符序列 $\tilde{y}$, 其中一个特殊填充符号 $\langle F\rangle$ 被添加到 $y$ 后, 使得 $\tilde{y}$ 的长度等于 $T$. <u>注: 我们假设文本字符长度小于音频序列长度 $M\le T$, 这几乎总是成立的.</u>

$$
\hat{y} = (c_1, c_2, \ldots, c_M, \underbrace{\langle F\rangle, \ldots, \langle F\rangle}_{(T-M) \text{ times}}).\tag{01}
$$

<details>
<summary>原文</summary>

A spectrogram generator, consisting of a vanilla Transformer [^26] with **U-Net** [^23] style skip connection, is then trained based on the speech infilling task (**Voicebox** [^18]).
More specifically, the model is trained to learn the distribution $P(m\odot\hat{s}|(1-m)\odot\hat{s}, \hat{y})$, where $m\in\{0,1\}^{D\times T}$ represents a binary temporal mask, and $\odot$ is the Hadamard product.
***E2 TTS*** uses the conditional **flow-matching** [^20] to learn such distribution.

</details>
<br>

由原始 Transformer 和 U-Net 类型的跳跃连接构成的频谱生成器, 基于语音填充任务进行训练.
具体来说, 模型被训练于学习概率分布 $P(m\odot\hat{s}|(1-m)\odot\hat{s}, \hat{y})$, 其中 $m\in\{0,1\}^{D\times T}$ 表示二值时序掩码, $\odot$ 为 Hadamard 积.
***E2 TTS*** 使用条件化流匹配来学习这样的分布.

### 3.2·Inference: 推理

<details>
<summary>原文</summary>

Fig.01 (b) provides an overview of the inference with ***E2 TTS***.
Suppose we have an audio prompt $s^{aud}$ and its transcription $y^{aud}=(c'_1, c'_2, ..., c'_{M^{aud}})$ to mimic the speaker characteristics.
We also suppose a text prompt $y^{text}=(c''_1, c''_2, ..., c''_{M^{text}})$.
In the ***E2 TTS*** framework, we also require the target duration of the speech that we want to generate, which may be determined arbitrarily.
The target duration is internally represented by the frame length $T^{gen}$.

First, we extract the mel-filterbank features $\hat{s}^{aud}\in\mathbb{R}^{D\times T^{aud}}$ from $s^{aud}$.
We then create an extended character sequence $\hat{y}'$ by concatenating $y^{aud}$, $y^{text}$, and repeated $\langle F\rangle$, as follows:

</details>
<br>

![Images/2024.06.26.E2_TTS.Fig.01b.png](Images/2024.06.26_E2_TTS_Fig.01b.png)

图 01 (b) 展示了 ***E2 TTS*** 的推理过程.
假设有参考音频 $s^{aud}$ 和它的转写文本 $y^{aud}=(c'_1, c'_2, ..., c'_{M^{aud}})$, 用来模仿说话人的特征.
假设有文本提示 $y^{text}=(c''_1, c''_2, ..., c''_{M^{text}})$.

在 ***E2 TTS*** 框架中, 我们还需要指定生成语音的目标时长, 这可能是任意给定的.
目标时长是由内部表示的帧长度 $T^{gen}$ 给定的.

首先, 我们提取参考音频 $s^{aud}$ 的梅尔滤波器组特征 $\hat{s}^{aud}\in\mathbb{R}^{D\times T^{aud}}$.
然后我们将 $y^{aud}$, $y^{text}$, 以及重复 $\langle F\rangle$ 连接起来, 得到扩展的字符序列 $\hat{y}'$.

$$
  \hat{y}' = (c'_1, c'_2, \ldots, c'_{M^{aud}}, c''_1, c''_2, \ldots, c''_{M^{text}}, \underbrace{\langle F\rangle, \ldots, \langle F\rangle}_{\mathcal{T} \text{ times}}),\tag{02}
$$

<details>
<summary>原文</summary>

where $\mathcal{T}=T^{aud}+T^{gen}-M^{aud}-M^{text}$, which ensures the length of $\hat{y}'$ is equal to $T^{aud}+T^{gen}$.
<u>Footnote: To ensure $\mathcal{T}\ge 0$, $T^{gen}$ needs to satisfy $T^{gen}\ge M^{aud}+M^{text}-T^{aud}$, which is almost always valid in the TTS scenario.</u>

The mel spectrogram generator then generates mel-filterbank features $\tilde{s}$ based on the learned distribution of $P(\tilde{s}|[\hat{s}^{aud}; z^{gen}], \hat{y}')$, where $z^{gen}$ is an all-zero matrix with a shape of ${D\times T^{gen}}$, and $[;]$ is a concatenation operation in the dimension of $T^*$.
The generated part of $\tilde{s}$ are then converted to the speech signal based on the vocoder.

</details>
<br>

其中 $\mathcal{T} = T^{aud} + T^{gen} - M^{aud} - M^{text}$, 这保证 $\hat{y}'$ 的长度等于 $T^{aud}+T^{gen}$.
<u>注: 为了确保 $\mathcal{T}\ge 0$, $T^{gen}$ 需要满足 $T^{gen}\ge M^{aud}+M^{text}-T^{aud}$, 这在 TTS 场景下几乎总是成立的.</u>

然后梅尔频谱生成器基于学习到的分布 $P(\tilde{s}|[\hat{s}^{aud}; z^{gen}], \hat{y}')$ 生成梅尔滤波器组特征 $\tilde{s}$.
其中 $z^{gen}$ 为全零矩阵, 形状为 ${D\times T^{gen}}$, $[;]$ 为在 $T^*$ 维度上的连接操作.
$\tilde{s}$ 的生成部分基于声码器被转换为语音信号.

### 3.3·Flow-Matching-Based Mel Spectrogram Generator: 基于流匹配的梅尔频谱生成器

***E2 TTS*** leverages conditional **flow-matching** [^20], which incorporates the principles of continuous normalizing flows [^27].
This model operates by transforming a simple initial distribution $p_0$ into a complex target distribution $p_1$ that characterizes the data.
The transformation process is facilitated by a neural network, parameterized by $\theta$, which is trained to estimate a time-dependent vector field, denoted as $v_t(x;\theta)$, for $t \in [0, 1]$.
From this vector field, we derive a flow, $\phi_{t}$, which effectively transitions $p_0$ into $p_1$.
The neural network's training is driven by the conditional flow matching objective:

$$
\mathcal{L}^{\text{CFM}}(\theta) = \mathbb{E}_{t,q(x_1), p_t(x|x_1)} \left\| u_t(x|x_1) - v_t(x;\theta) \right\|^2,\tag{03}
$$

where $p_t$ is the probability path at time $t$, $u_t$ is the designated vector field for $p_t$, $x_1$ symbolizes the random variable corresponding to the training data, and $q$ is the distribution of the training data.
In the training phase, we construct both a probability path and a vector field from the training data, utilizing an optimal transport path: $p_t(x|x_1)=\mathcal{N}(x|tx_1, (1-(1-\sigma_{min})t)^2I)$ and $u_t(x|x_1)=(x_1-(1-\sigma_{min})x)/(1-(1-\sigma_{min})t)$.
For inference, we apply an ordinary differential equation solver [^27] to generate the log mel-filterbank features starting from the initial distribution $p_0$.

We adopt the same model architecture with the audio model of Voicebox (Fig.02 of **Voicebox** [^18]) except that the frame-wise phoneme sequence is replaced into $\hat{y}$.
Specifically, Transformer with U-Net style skip connection (**Voicebox** [^18]) is used as a backbone.
The input to the mel spectrogram generator is $m\odot\hat{s}$, $\hat{y}$, the flow step $t$, and noisy speech $s_t$.
$\hat{y}$ is first converted to character embedding sequence $\tilde{y}\in\mathbb{R}^{E\times T}$.
Then, $m\odot\hat{s}$, $s_t$, $\tilde{y}$ are all stacked to form a tensor with a shape of $(2\cdot D+E)\times T$, followed by a linear layer to output a tensor with a shape of $D\times T$.
Finally, an embedding representation, $\hat{t}\in\mathbb{R}^D$, of $t$ is appended to form the input tensor with a shape of $\mathbb{R}^{D\times (T+1)}$ to the Transformer.
The Transformer is trained to output a vector field $v_t$ with the conditional flow-matching objective $\mathcal{L}^{CFM}$.

### 3.4·Relationship to Voicebox: 与 Voicebox 的关联

***E2 TTS*** has a close relationship with the Voicebox.
From the perspective of the Voicebox, ***E2 TTS*** replaces a frame-wise phoneme sequence used in conditioning with a character sequence that includes a filler token.
This change significantly simplifies the model by eliminating the need for a grapheme-to-phoneme converter, a phoneme aligner, and a phoneme duration model.
From another viewpoint, the mel spectrogram generator of ***E2 TTS*** can be viewed as a joint model of the grapheme-to-phoneme converter, the phoneme duration model, and the audio model of the Voicebox.
This joint modeling significantly improves naturalness while maintaining speaker similarity and intelligibility, as will be demonstrated in our experiments.

### 3.5·Extension of E2 TTS: E2 TTS 的扩展

![Images/2024.06.26.E2_TTS.Fig.02a.png](Images/2024.06.26_E2_TTS_Fig.02.png)
Fig. 2. An overview of the training (left) and the inference (right) processes of E2 TTS X1.

#### 3.5.1·Extension 1: Eliminating the need for transcription of audio prompts in inference

In certain application contexts, obtaining the transcription of the audio prompt can be challenging.
To eliminate the requirement of the transcription of the audio prompt during inference, we introduce an extension, illustrated in Fig.02.
This extension, referred to as ***E2 TTS X1***, assumes that we have access to the transcription of the masked region of the audio, which we use for $y$.
During inference, the extended character sequence $\tilde{y}'$ is formed without $y^{aud}$, namely,

$$
    \tilde{y}' = (c''_1, c''_2, \ldots, c''_{M^{text}}, \underbrace{\langle F\rangle, \ldots, \langle F\rangle}_{\mathcal{T} \text{ times}}).\tag{04}
$$

The rest of the procedure remains the same as in the basic ***E2 TTS***.

The transcription of the masked region of the training audio can be obtained in several ways.
One method is to simply apply ASR to the masked region during training, which is straightforward but costly.
In our experiment, we employed the **Montreal Forced Aligner** [^28] to determine the start and end times of words within each training data sample.
The masked region was determined in such a way that we ensured not to cut the word in the middle.

#### 3.5.2·Extension 2: Enabling explicit indication of pronunciation for parts of words in a sentence

In certain scenarios, users want to specify the pronunciation of a specific word such as unique foreign names.
Retraining the model to accommodate such new words is both expensive and time-consuming.

To tackle this challenge, we introduce another extension that enables us to indicate the pronunciation of a word during inference.
In this extension, referred to as ***E2 TTS X2***, we occasionally substitute a word in $y$ with a phoneme sequence enclosed in parentheses during training, as depicted in Fig.~\ref{fig:extension_2_fig}.
In our implementation, we replaced the word in $y$ with the phoneme sequence from the CMU pronouncing dictionary [^29] with a 15\% probability.
During inference, we simply replace the target word with phoneme sequences enclosed in parentheses.

It's important to note that $y$ is still a simple sequence of characters, and whether the character represents a word or a phoneme is solely determined by the existence of parentheses and their content.
It's also noteworthy that punctuation marks surrounding the word are retained during replacement, which allows the model to utilize these punctuation marks even when the word is replaced with phoneme sequences.

## 4·Experiments: 实验

### 4.1·Training Data: 训练数据

We utilized the **Libriheavy** [^30] dataset to train our models.
The Libriheavy dataset comprises 50,000 hours of read English speech from 6,736 speakers, accompanied by transcriptions that preserve case and punctuation marks.
It is derived from the **Librilight** [^31] dataset contains 60,000 hours of read English speech from over 7,000 speakers.
For ***E2 TTS*** training, we used the case and punctuated transcription without any pre-processing.
We also used a proprietary 200,000 hours of training data to investigate the scalability of ***E2 TTS*** model.

### 4.2·Model Configurations: 模型配置

We constructed our proposed ***E2 TTS*** models using a Transformer architecture.
The architecture incorporated **U-Net** [^23] style skip connections, 24 layers, 16 attention heads, an embedding dimension of 1024, a linear layer dimension of 4096, and a dropout rate of 0.1.
The character embedding vocabulary size was 399.
<u>Footnote: We used all characters and symbols that we found in the training data without filtering.</u>
The total number of parameters amounted to 335 million.
We modeled the 100-dimensional log mel-filterbank features, extracted every 10.7 milliseconds from audio samples with a 24 kHz sampling rate.
A **BigVGAN** [^32]-based vocoder was employed to convert the log mel-filterbank features into waveforms.
The masking length was randomly determined to be between 70\% and 100\% of the log mel-filterbank feature length during training.
In addition, we randomly dropped all the conditioning information with a 20\% probability for **classifier-free guidance (CFG)** [^33].
All models were trained for 800,000 mini-batch updates with an effective mini-batch size of 307,200 audio frames.
We utilized a linear decay learning rate schedule with a peak learning rate of $7.5\times10^{-5}$ and incorporated a warm-up phase for the initial 20,000 updates.
We discarded the training samples that exceeded 4,000 frames.

In a subset of our experiments, we initialized ***E2 TTS*** models using a pre-trained model in an unsupervised manner.
This pre-training was conducted on an anonymized dataset, which consisted of 200,000 hours of unlabeled data.
The pre-training protocol, which involved 800,000 mini-batch updates, followed the scheme outlined in [^34].

In addition, we trained a regression-based duration model by following that of **Voicebox** [^18].
It is based on a Transformer architecture, consisted of 8 layers, 8 attention heads, an embedding dimension of 512, a linear layer dimension of 2048, and a dropout rate of 0.1.
The training process involved 75,000 mini-batch updates with 120,000 frames.
We used this duration model for estimating the target duration for the fair comparison to the Voicebox baseline.
Note that we will also show that ***E2 TTS*** is robust for different target duration in Section 5.3.

### 4.3·Evaluation Data & Metrics: 评估数据及指标

In order to assess our models, we utilized the test-clean subset of the **LibriSpeech-PC** dataset [^35], which is an extension of **LibriSpeech** [^36] that includes additional punctuation marks and casing.
We specifically filtered the samples to retain only those with a duration between 4 and 10 seconds.
Since **LibriSpeech-PC** [^35] lacks some of the utterances from LibriSpeech, the total number of samples was reduced to 1,132, sourced from 39 speakers.
For the audio prompt, we extracted the last three seconds from a randomly sampled speech file from the same speaker for each evaluation sample.

We carried out both objective and subjective evaluations.
For the objective evaluations, we generated samples using three random seeds, computed the objective metrics for each, and then calculated their average.
We computed the word error rate (WER) and speaker similarity (SIM-o).
The WER is indicative of the intelligibility of the generated samples, and for its calculation, we utilized a **HuBERT**-large-based [^37] ASR system.
The SIM-o represents the speaker similarity between the audio prompt and the generated sample, which is estimated by computing the cosine similarity between the speaker embeddings of both.
For the calculation of SIM-o, we used a **WavLM**-large-based [^38] speaker verification model.

For the subjective assessments, we conducted two tests: the Comparative Mean Opinion Score (CMOS) and the Speaker Similarity Mean Opinion Score (SMOS).
We evaluated 39 samples for both tests, with one sample per speaker from our test-clean set.
Each sample was assessed by 12 Native English evaluators.
In the CMOS test (**NaturalSpeech3** [^15]), evaluators were shown the ground-truth sample and the generated sample side-by-side without knowing which was the ground-truth, and were asked to rate the naturalness on a 7-point scale (from -3 to 3), where a negative value indicates a preference for the ground-truth and a positive value indicates the opposite.
In the SMOS test, evaluators were presented with the audio prompt and the generated sample, and asked to rate the speaker similarity on a scale of 1 (not similar at all) to 5 (identical), with increments of 1.

## 5·Results: 结果

### 5.1·Main Results: 主要结果

In our experiments, we conducted a comparison between our ***E2 TTS*** models and four other models: **Voicebox** [^18], **VALL-E** [^8], and **NaturalSpeech3** [^15].
We utilized our own reimplementation of the Voicebox model, which was based on the same model configuration with ***E2 TTS*** except that the Voicebox model is trained with frame-wise phoneme alignment.
During the inference, we used CFG with a guidance strength of 1.0 for both ***E2 TTS*** and Voicebox.
We employed the midpoint ODE solver with a number of function evaluations of 32.

Table~\ref{tab:main_results} presents the objective evaluation results for the baseline and ***E2 TTS*** models across various configurations.
By comparing the (B4) and (P1) systems, we observe that the ***E2 TTS*** model achieved better WER and SIM-o than the Voicebox model when both were trained on the Libriheavy dataset.
This trend holds even when we initialize the model with unsupervised pre-training [^34] ((B5) vs. (P2)), where the (P2) system achieved the best WER (1.9\%) and SIM-o (0.708) which are better than those of the ground-truth audio.
Finally, by using larger training data (P3), ***E2 TTS*** achieved the same best WER (1.9\%) and the second best SIM-o (0.707) even when the model is trained from scratch, showcasing the scalability of ***E2 TTS***.
It is noteworthy that ***E2 TTS*** achieved superior performance compared to all strong baselines, including VALL-E, NaturalSpeech 3, and Voicebox, despite its extremely simple framework.

Table~\ref{tab:subj_results} illustrated the subjective evaluation results for NaturalSpeech 3, Voicebox, and ***E2 TTS***.
Firstly, all variants of ***E2 TTS***, from (P1) to (P3), showed a better CMOS score compared to NaturalSpeech 3 and Voicebox.
In particular, the (P2) model achieved a CMOS score of -0.05, which is considered to have a level of naturalness indistinguishable from the ground truth (**NaturalSpeech3** [^15]; **Seed-TTS** [^24]).
<u>Footnote: In the evaluation, ***E2 TTS*** was judged to be better in 33\% of samples, while the ground truth was better in another 33\% of samples.
The remaining samples were judged to be of equal quality.</u>
The comparison between (B4) and (P1) suggests that the use of phoneme alignment was the major bottleneck in achieving better naturalness.
Regarding speaker similarity, all the models we tested showed a better SMOS compared to the ground truth, a phenomenon also observed in **NaturalSpeech3** [^15].
<u>Footnote: In LibriSpeech, some speakers utilized varying voice characteristics for different characters in the book, leading to a low SMOS for the ground truth.</u>
Among the tested systems, ***E2 TTS*** achieved a comparable SMOS to Voicebox and NaturalSpeech 3.

Overall, ***E2 TTS*** demonstrated a robust zero-shot TTS capability that is either superior or comparable to strong baselines, including Voicebox and NaturalSpeech 3.
The comparison between Voicebox and ***E2 TTS*** revealed that the use of phoneme alignment was the primary obstacle in achieving natural-sounding audio.
With a simple training scheme, ***E2 TTS*** can be easily scaled up to accommodate large training data.
This resulted in human-level speaker similarity, intelligibility, and a level of naturalness that is indistinguishable from a human's voice in the zero-shot TTS setting, despite the framework’s extreme simplicity.

### 5.2·Evaluation of E2 TTS Extensions: E2 TTS 扩展的评估

#### 5.2.1·Evaluation of the Extension 1

The results for the ***E2 TTS X1*** models are shown in Table~\ref{tab:e2ttse1}.
These results indicate that the ***E2 TTS X1*** model has achieved results nearly identical to those of the ***E2 TTS*** model, especially when the model was initialized by unsupervised pre-training [^34].
***E2 TTS X1*** does not require the transcription of the audio prompt, which greatly enhances its usability.

#### 5.2.2·Evaluation of the Extension 2

In this experiment, we trained the ***E2 TTS X2*** models with a phoneme replacement rate of 15\%.
During inference, we randomly replaced words in the test-clean dataset with phoneme sequences, with a probability ranging from 0\% up to 50\%.

Table~\ref{tab:e2ttse2} shows the result.
We first observed that ***E2 TTS X2*** achieved parity results when no words were replaced with phoneme sequences.
This shows that we can introduce extension 2 without any drawbacks.
We also observed that the ***E2 TTS X2*** model showed only marginal degradation of WER, even when we replaced 50\% of words with phoneme sequences.
This result indicates that we can specify the pronunciation of a new term without retraining the ***E2 TTS*** model.

### 5.3·Analysis of the System Behavior: 系统行为分析

#### 5.3.1·Training Progress

Fig.~\ref{fig:abl_prg_p1} illustrates the training progress of the (B4)-Voicebox, (B5)-Voicebox, (P1)-E2 TTS, and (P2)-***E2 TTS*** models.
The upper graphs represent the training progress as measured by WER, while the lower graphs depict the progress as measured by SIM-o.
We present a comparison between (B4)-Voicebox and (P1)-E2 TTS, as well as between (B5)-Voicebox and (P2)-E2 TTS.
The former pair was trained from scratch, while the latter was initialized by unsupervised pre-training [^34].

From the WER graphs, we observe that the Voicebox models demonstrated a good WER even at the 10\% training point, owing to the use of frame-wise phoneme alignment.
On the other hand, ***E2 TTS*** required significantly more training to converge.
Interestingly, ***E2 TTS*** achieved a better WER at the end of the training.
We speculate this is because the ***E2 TTS*** model learned a more effective grapheme-to-phoneme mapping based on the large training data, compared to what was used for Voicebox.

From the SIM-o graphs, we also observed that ***E2 TTS*** required more training iteration, but it ultimately achieved a better result at the end of the training.
We believe this suggests the superiority of ***E2 TTS***, where the audio model and duration model are jointly learned as a single flow-matching Transformer.

#### 5.3.2·Impact of Audio Prompt Length

During the inference of ***E2 TTS***, the model needs to automatically identify the boundary of $y^{aud}$ and $y^{text}$ in $\hat{y}$ based on the audio prompt $\hat{s}^{aud}$.
Otherwise, the generated audio may either contain a part of $y^{aud}$ or miss a part of $y^{text}$.
This is not a trivial problem, and ***E2 TTS*** could show a deteriorated result when the length of the audio prompt $\hat{s}^{aud}$ is long.

To examine the capability of ***E2 TTS***, we evaluated the WER and SIM-o with different audio prompt lengths.
The result is shown in Fig.~\ref{fig:abl_prompt_dur}.
In this experiment, we utilized the entire audio prompts instead of using the last 3 seconds, and categorized the result based on the length of the audio prompts.
As shown in the figure, we did not observe any obvious pattern between the WER and the length of the audio prompt.
This suggests that ***E2 TTS*** works reasonably well even when the prompt length is as long as 10 seconds.
On the other hand, SIM-o significantly improved when the audio prompt length increased, which suggests the scalability of ***E2 TTS*** with respect to the audio prompt length.

#### 5.3.3·Impact of Changing the Speech Rate

We further examined the model's ability to produce suitable content when altering the total duration input.
In this experiment, we adjusted the total duration by multiplying it by $\frac{1}{sr}$, where $sr$ represents the speech rate.
The results are shown in Fig.~\ref{fig:abl_spd}.
As depicted in the graphs, the ***E2 TTS*** model exhibited only a moderate increase in WER while maintaining a high SIM-o, even in the challenging cases of $sr=0.7$ and $sr=1.3$.
This result suggests the robustness of ***E2 TTS*** with respect to the total duration input.

## 6·Conclusions: 结论

We introduced ***E2 TTS***, a novel fully NAR zero-shot TTS.
In the ***E2 TTS*** framework, the text input is converted into a character sequence with filler tokens to match the length of the input character sequence and the output mel-filterbank sequence.
The flow-matching-based mel spectrogram generator is then trained based on the audio infilling task.
Despite its simplicity, ***E2 TTS*** achieved state-of-the-art zero-shot TTS capabilities that were comparable to or surpass previous works, including Voicebox and NaturalSpeech 3.
The simplicity of ***E2 TTS*** also allowed for flexibility in the input representation.
We proposed several variants of ***E2 TTS*** to improve usability during inference.

[^1]: [FastSpeech](../../Models/TTS2_Acoustic/2019.05.22_FastSpeech.md)
[^2]: [FastSpeech2](../../Models/TTS2_Acoustic/2020.06.08_FastSpeech2.md)
[^3]: [Glow-TTS](../../Models/TTS2_Acoustic/2020.05.22_Glow-TTS.md)
[^4]: [HiFi-GAN](../../Models/TTS3_Vocoder/2020.10.12_HiFi-GAN.md)
[^5]: [NaturalSpeech](../../Models/E2E/2022.05.09_NaturalSpeech.md)
[^6]: [Neural Voice Cloning with a Few Samples]
[^7]: [Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis]
[^8]: [VALL-E](../SpeechLM/2023.01.05_VALL-E.md)
[^9]: [SpeechX](../SpeechLM/2023.08.14_SpeechX.md)
[^10]: [VALL-T](../SpeechLM/2024.01.25_VALL-T.md)
[^11]: [RALL-E](../SpeechLM/2024.04.04_RALL-E.md)
[^12]: [VALL-E 2](../SpeechLM/2024.06.08_VALL-E_2.md)
[^13]: [UniAudio](../SpeechLM/2023.10.01_UniAudio.md)
[^14]: [NaturalSpeech2](../../Models/Diffusion/2023.04.18_NaturalSpeech2.md)
[^15]: [NaturalSpeech3](../../Models/Diffusion/2024.03.05_NaturalSpeech3.md)
[^16]: [DDPM](2020.06.19_DDPM.md)
[^17]: []
[^18]: [Voicebox](../../Models/SpeechLM/2023.06.23_VoiceBox.md)
[^19]: [Matcha-TTS](2023.09.06_Matcha-TTS.md)
[^20]: [Flow-Matching](../Diffusion/2022.10.06_Flow_Matching.md)
[^21]: [Grad-TTS](../../Models/TTS2_Acoustic/2021.05.13_Grad-TTS.md)
[^22]: [E3 TTS](../../Models/Diffusion/2023.11.02_E3_TTS.md)
[^23]: [U-Net](../../Models/_Basis/U-Net.md)
[^24]: [Seed-TTS](../SpeechLM/2024.06.04_Seed-TTS.md)
[^25]: [Bytes are All You Need: End-To-End Multilingual Speech Recognition and Synthesis with Bytes]
[^26]: [Transformer](../../Models/_Transformer/2017.06.12_Transformer.md)
[^27]: [NeuralODE]
[^28]: [MFA]
[^29]: [The Carnegie Mellon University Pronouncing Dictionary](../../Models/TTS1_Text_Analysis/CMUdict.md)
[^30]: [LibriHeavy](../../Datasets/2023.09.15_Libriheavy.md)
[^31]: [Libri-Light](../../Datasets/2019.12.17_Libri-Light.md)
[^32]: [BigVGAN](../../Models/TTS3_Vocoder/2022.06.09_BigVGAN.md)
[^33]: [Classifier-Free Diffusion Guidance](2022.07.26_Classifier-Free_Guidance.md)
[^34]: [An Investigation of Noise Robustness for Flow-Matching-Based Zero-Shot TTS]
[^35]: [LibriSpeech-PC](../../Datasets/2023.10.04_LibriSpeech-PC.md)
[^36]: [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md)
[^37]: [HuBERT](../SpeechRepresentation/2021.06.14_HuBERT.md)
[^38]: [WavLM](../SpeechRepresentation/2021.10.26_WavLM.md)