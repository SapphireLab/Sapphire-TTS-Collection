# VoiceDiT

<details>
<summary>基本信息</summary>

- 标题: "VoiceDiT: Dual-Condition Diffusion Transformer for Environment-Aware Speech Synthesis"
- 作者:
  - 01 Jaemin Jung,
  - 02 Junseok Ahn,
  - 03 Chaeyoung Jung,
  - 04 Tan Dat Nguyen,
  - 05 Youngjoon Jang,
  - 06 Joon Son Chung
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.19259)
  - [Publication]
  - [Github]
  - [Demo](https://mm.kaist.ac.kr/projects/voicedit/)
- 文件:
  - [ArXiv](_PDF/2412.19259v1__VoiceDiT__Dual-Condition_Diffusion_Transformer_for_Environment-Aware_Speech_Synthesis.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

We present VoiceDiT, a multi-modal generative model for producing environment-aware speech and audio from text and visual prompts.
While aligning speech with text is crucial for intelligible speech, achieving this alignment in noisy conditions remains a significant and underexplored challenge in the field.
To address this, we present a novel audio generation pipeline named VoiceDiT.
This pipeline includes three key components: (1) the creation of a large-scale synthetic speech dataset for pre-training and a refined real-world speech dataset for fine-tuning, (2) the Dual-DiT, a model designed to efficiently preserve aligned speech information while accurately reflecting environmental conditions, and (3) a diffusion-based Image-to-Audio Translator that allows the model to bridge the gap between audio and image, facilitating the generation of environmental sound that aligns with the multi-modal prompts.
Extensive experimental results demonstrate that VoiceDiT outperforms previous models on real-world datasets, showcasing significant improvements in both audio quality and modality integration.

## 1·Introduction: 引言

The demand for generating realistic audio, including sound effects, music, and speech, has rapidly increased across various industries, such as film and gaming.
In response, the fields of Text-to-Audio (TTA)~\cite{yang2023diffsound, kreuk2022audiogen, huang2023make, ghosal2023text, evans2024fast,  liu2024audiolcm, xue2024auffusion} and Text-to-Speech (TTS)~\cite{popov2021grad, kim2020glow, kim22guidedtts, huang2023prosody, kim2023crossspeech, nguyen2024fregrad, Ju24naturalspeech, lee2024ditto} have garnered significant attention for their ability to generate natural sounds and speech from text prompts.

Recently, diffusion models, which have been extensively studied in the field of image generation~\cite{rombach2022high, song2020score, ho2020denoising}, have emerged as a transformative approach to audio synthesis.
Grad-TTS~\cite{popov2021grad} leverages diffusion processes to produce high-quality speech from text, and AudioLDM~\cite{liu2023audioldm} enables the versatile generation of environmental sounds and music from text prompts.

While these models perform well at their respective tasks, audio generation often requires the simultaneous generation of speech and environmental sounds.
For example, to enhance realism in extended reality (XR) applications, the character's voice must blend naturally with environmental sounds and reflect the acoustics of the space.
Recent studies~\cite{lee2024voiceldm, vyas2023audiobox, liu2024audioldm, yang2023uniaudio} have explored specialized model architectures capable of jointly generating both audio and speech.

VoiceLDM~\cite{lee2024voiceldm} conditions the U-Net backbone of AudioLDM with speech transcriptions through cross-attention, enabling the simultaneous generation of both speech and environmental sounds.

It uses the contrastive language-audio pre-training (CLAP)~\cite{wu2023large} model to train with sound clips instead of text annotations, addressing the issue of data scarcity.
However, VoiceLDM often produces repetitive and mumbled speech due to a lack of temporal alignment between speech and text.
More recently, AudioBox~\cite{vyas2023audiobox} introduces a unified model based on flow-matching, capable of generating various audio modalities.

AudioBox is trained on data annotated by a large language model or human experts, encompassing diverse speech attributes such as age, gender, and accent.
However, as the labels primarily focus on speaker style rather than environmental context, AudioBox's ability to generate a wide range of environmental sounds remains limited.

In this paper, we present VoiceDiT, a novel audio generation pipeline that not only produces natural speech but also allows for conditioning on the acoustic environment.
The proposed pipeline features three key components: (1) data synthesis and refinement for limited training data, (2) a transformer-based architecture designed for multi-task performance, and (3) an image-to-audio translator~(I2A-Translator) to enhance flexibility in handling diverse inputs.

Current efforts to integrate TTA and TTS systems are constrained by the lack of large speech datasets that are both accurately transcribed and reflective of the diverse audio conditions encountered in real-world scenarios~\cite{ zen2019libritts, ardila2019common, nagrani2017voxceleb, chung2018voxceleb2, kwak2024voxmm}.
To address this issue, we propose a practical strategy for data acquisition by first constructing a large-scale synthetic dataset, where environmental noise and reverberation are added to clean speech for pre-training the model.

Subsequently, we refine the real-world speech dataset\cite{lee2024voiceldm} for fine-tuning, which helps bridge the domain gap between synthetic and real-world data.

We then introduce a Dual-condition Diffusion Transformer~(Dual-DiT), designed to generate environment-aware speech by incorporating two distinct conditions: one for speech and another for environmental sound.

For stable and efficient speech generation, we develop a TTS module that meticulously aligns text with speech using alignment information~\cite{kim2020glow}, along with a Latent Mapper that compresses long text conditions into a latent space.
Additionally, to ensure the model generates sounds suitable for the given environmental conditions, we integrate a cross-attention module into the Dual-DiT blocks.
This module injects environmental features, enabling the model to generate sounds that align with the conditions.

Finally, to broaden our model’s applicability, we introduce a diffusion-based I2A-Translator that converts image embeddings into audio embeddings.
Unlike text, images provide a more intuitive representation of environments, especially for complex or abstract scenarios.
Through the I2A-Translator, our model can generate diverse and nuanced audio conditioned on both text and images.

Our comprehensive experimental results demonstrate that VoiceDiT excels at generating environment-aware speech that aligns closely with user prompts, achieving state-of-the-art performance across both qualitative and quantitative metrics.
This success underscores the potential of VoiceDiT for a wide range of applications.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论