# VoiceDiT

<details>
<summary>基本信息</summary>

- 标题: "VoiceDiT: Dual-Condition Diffusion Transformer for Environment-Aware Speech Synthesis"
- 作者:
  - 01 Jaemin Jung,
  - 02 Junseok Ahn,
  - 03 Chaeyoung Jung,
  - 04 Tan Dat Nguyen,
  - 05 Youngjoon Jang,
  - 06 Joon Son Chung
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.19259)
  - [Publication]
  - [Github]
  - [Demo](https://mm.kaist.ac.kr/projects/voicedit/)
- 文件:
  - [ArXiv](_PDF/2412.19259v1__VoiceDiT__Dual-Condition_Diffusion_Transformer_for_Environment-Aware_Speech_Synthesis.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

We present VoiceDiT, a multi-modal generative model for producing environment-aware speech and audio from text and visual prompts.
While aligning speech with text is crucial for intelligible speech, achieving this alignment in noisy conditions remains a significant and underexplored challenge in the field.
To address this, we present a novel audio generation pipeline named VoiceDiT.
This pipeline includes three key components: (1) the creation of a large-scale synthetic speech dataset for pre-training and a refined real-world speech dataset for fine-tuning, (2) the Dual-DiT, a model designed to efficiently preserve aligned speech information while accurately reflecting environmental conditions, and (3) a diffusion-based Image-to-Audio Translator that allows the model to bridge the gap between audio and image, facilitating the generation of environmental sound that aligns with the multi-modal prompts.
Extensive experimental results demonstrate that VoiceDiT outperforms previous models on real-world datasets, showcasing significant improvements in both audio quality and modality integration.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论