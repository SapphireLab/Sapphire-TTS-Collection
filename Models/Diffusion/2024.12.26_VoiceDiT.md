# VoiceDiT

<details>
<summary>基本信息</summary>

- 标题: "VoiceDiT: Dual-Condition Diffusion Transformer for Environment-Aware Speech Synthesis"
- 作者:
  - 01 Jaemin Jung,
  - 02 Junseok Ahn,
  - 03 Chaeyoung Jung,
  - 04 Tan Dat Nguyen,
  - 05 Youngjoon Jang,
  - 06 Joon Son Chung
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.19259)
  - [Publication]
  - [Github]
  - [Demo](https://mm.kaist.ac.kr/projects/voicedit/)
- 文件:
  - [ArXiv](_PDF/2412.19259v1__VoiceDiT__Dual-Condition_Diffusion_Transformer_for_Environment-Aware_Speech_Synthesis.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

We present VoiceDiT, a multi-modal generative model for producing environment-aware speech and audio from text and visual prompts.
While aligning speech with text is crucial for intelligible speech, achieving this alignment in noisy conditions remains a significant and underexplored challenge in the field.
To address this, we present a novel audio generation pipeline named VoiceDiT.
This pipeline includes three key components: (1) the creation of a large-scale synthetic speech dataset for pre-training and a refined real-world speech dataset for fine-tuning, (2) the Dual-DiT, a model designed to efficiently preserve aligned speech information while accurately reflecting environmental conditions, and (3) a diffusion-based Image-to-Audio Translator that allows the model to bridge the gap between audio and image, facilitating the generation of environmental sound that aligns with the multi-modal prompts.
Extensive experimental results demonstrate that VoiceDiT outperforms previous models on real-world datasets, showcasing significant improvements in both audio quality and modality integration.

## 1·Introduction: 引言

The demand for generating realistic audio, including sound effects, music, and speech, has rapidly increased across various industries, such as film and gaming.
In response, the fields of Text-to-Audio (TTA)~\cite{yang2023diffsound, kreuk2022audiogen, huang2023make, ghosal2023text, evans2024fast,  liu2024audiolcm, xue2024auffusion} and Text-to-Speech (TTS)~\cite{popov2021grad, kim2020glow, kim22guidedtts, huang2023prosody, kim2023crossspeech, nguyen2024fregrad, Ju24naturalspeech, lee2024ditto} have garnered significant attention for their ability to generate natural sounds and speech from text prompts.

Recently, diffusion models, which have been extensively studied in the field of image generation~\cite{rombach2022high, song2020score, ho2020denoising}, have emerged as a transformative approach to audio synthesis.
Grad-TTS~\cite{popov2021grad} leverages diffusion processes to produce high-quality speech from text, and AudioLDM~\cite{liu2023audioldm} enables the versatile generation of environmental sounds and music from text prompts.

While these models perform well at their respective tasks, audio generation often requires the simultaneous generation of speech and environmental sounds.
For example, to enhance realism in extended reality (XR) applications, the character's voice must blend naturally with environmental sounds and reflect the acoustics of the space.
Recent studies~\cite{lee2024voiceldm, vyas2023audiobox, liu2024audioldm, yang2023uniaudio} have explored specialized model architectures capable of jointly generating both audio and speech.

VoiceLDM~\cite{lee2024voiceldm} conditions the U-Net backbone of AudioLDM with speech transcriptions through cross-attention, enabling the simultaneous generation of both speech and environmental sounds.

It uses the contrastive language-audio pre-training (CLAP)~\cite{wu2023large} model to train with sound clips instead of text annotations, addressing the issue of data scarcity.
However, VoiceLDM often produces repetitive and mumbled speech due to a lack of temporal alignment between speech and text.
More recently, AudioBox~\cite{vyas2023audiobox} introduces a unified model based on flow-matching, capable of generating various audio modalities.

AudioBox is trained on data annotated by a large language model or human experts, encompassing diverse speech attributes such as age, gender, and accent.
However, as the labels primarily focus on speaker style rather than environmental context, AudioBox's ability to generate a wide range of environmental sounds remains limited.

In this paper, we present VoiceDiT, a novel audio generation pipeline that not only produces natural speech but also allows for conditioning on the acoustic environment.
The proposed pipeline features three key components: (1) data synthesis and refinement for limited training data, (2) a transformer-based architecture designed for multi-task performance, and (3) an image-to-audio translator~(I2A-Translator) to enhance flexibility in handling diverse inputs.

Current efforts to integrate TTA and TTS systems are constrained by the lack of large speech datasets that are both accurately transcribed and reflective of the diverse audio conditions encountered in real-world scenarios~\cite{ zen2019libritts, ardila2019common, nagrani2017voxceleb, chung2018voxceleb2, kwak2024voxmm}.
To address this issue, we propose a practical strategy for data acquisition by first constructing a large-scale synthetic dataset, where environmental noise and reverberation are added to clean speech for pre-training the model.

Subsequently, we refine the real-world speech dataset\cite{lee2024voiceldm} for fine-tuning, which helps bridge the domain gap between synthetic and real-world data.

We then introduce a Dual-condition Diffusion Transformer~(Dual-DiT), designed to generate environment-aware speech by incorporating two distinct conditions: one for speech and another for environmental sound.

For stable and efficient speech generation, we develop a TTS module that meticulously aligns text with speech using alignment information~\cite{kim2020glow}, along with a Latent Mapper that compresses long text conditions into a latent space.
Additionally, to ensure the model generates sounds suitable for the given environmental conditions, we integrate a cross-attention module into the Dual-DiT blocks.
This module injects environmental features, enabling the model to generate sounds that align with the conditions.

Finally, to broaden our model’s applicability, we introduce a diffusion-based I2A-Translator that converts image embeddings into audio embeddings.
Unlike text, images provide a more intuitive representation of environments, especially for complex or abstract scenarios.
Through the I2A-Translator, our model can generate diverse and nuanced audio conditioned on both text and images.

Our comprehensive experimental results demonstrate that VoiceDiT excels at generating environment-aware speech that aligns closely with user prompts, achieving state-of-the-art performance across both qualitative and quantitative metrics.
This success underscores the potential of VoiceDiT for a wide range of applications.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

### A·Data Preparation

#### Pre-Training Dataset

To train the TTS components that generate intelligible speech, it is essential to have a speech dataset aligned with corresponding transcripts.
However, large-scale real-world transcribed speech datasets are currently scarce.
To address this shortage of data, we construct a large-scale synthetic dataset by adding various noises to clean speech.
For the noise data, we utilize WavCaps~\cite{mei2024wavcaps}, a sound event dataset with audio captions.
Since WavCaps contains a notable amount of speech data alongside environmental sounds, we filter out speech-related samples, retaining 340K environmental sound samples.
We then mix clean speech from LibriTTS-R~\cite{koizumi2023libritts} with the noise data, using a signal-to-noise ratio value randomly selected from a uniform distribution within the range of 2 to 10.
Furthermore, to simulate various environmental conditions, we apply Room Impulse Response filters with a certain probability.

#### Fine-Tuning Dataset

To bridge the domain gap between synthetic and real-world data, we further fine-tune our model on the in-the-wild speech dataset, AudioSet-speech~\cite{lee2024voiceldm}.
This is a speech subset of AudioSet transcribed with ASR models.
Despite a carefully designed ASR process for accurate transcription, as detailed in~\cite{lee2024voiceldm}, AudioSet-speech still contains many inaccurate transcriptions.
Additionally, since the audio length is fixed at 10 seconds, non-speech segments are present before and after the speech segments within the audio sample.
Such misaligned data hinders the alignment of speech and text during the training of TTS model.
To address these issues, we implement a two-step preprocessing approach~\cite{jung2023metric}.
First, we compute the word error rate (WER) on AudioSet-speech dataset using the Whisper \textit{large-v3} model~\cite{radford2023robust}, filtering out samples where the WER exceeds 20\%.
This process reduces the original 597K speech data to a refined dataset of 400K, with most of the discarded data consisting of songs or audio featuring multiple speakers.
Next, we use a forced aligner to align the transcript of spoken words with the corresponding audio recording.
Then, we truncate non-speech segments from the beginning and end of the speech segment, ensuring proper alignment between the speech and transcripts.

### B·Model Architecture

VoiceDiT is a latent diffusion model with two components: the TTS module, which generates text-aligned acoustic features, and Dual-condition Diffusion Transformer (Dual-DiT), a decoder that synthesizes environment-aware speech from two input conditions.
An overview of the architecture is provided in \Fref{fig:framework}.

#### TTS Module

The TTS module consists of a text encoder and a duration predictor, both adopted from Glow-TTS~\cite{kim2020glow}.
This module is responsible for extracting time-aligned linguistic information to generate the mel-spectrogram \(y_{1:F}\) from the input text sequence \(x_{1:L}\), where \(F\) and \(L\) represent the lengths of the respective sequences.

First, the text encoder extracts linguistic information \(\tilde{\mu}_{1:L}\) from the input text sequence \(x_{1:L}\).
Then, the upsampled encoded text $Up(\tilde{\mu}_{1:L}) = \tilde{\mu}_{1:L, A^*}$ is obtained according to the best alignment \(A^*\) between \(\tilde{\mu}_{1:L}\) and the mel-spectrogram \(y_{1:F}\).
During training, the alignment \(A^*\) is determined using MAS.

The text encoder is trained to maximize the probability $p(y_{1:F}; \tilde{\mu}_{1:L, A^*}, \boldsymbol{I})$ by minimizing the encoder loss ($L_{enc}$).
Furthermore, the optimal alignment is used to extract ground truth duration $d$, which supervises the training process of duration predictor by loss function $\mathcal{L}_{dp} = \Vert \log(d) - \log(\hat{d})\Vert_2^2$, where $\hat{d}$ is predicted duration conditioned on $\tilde{\mu}_{1:L}$.

During inference, the duration predictor replaces MAS to provide alignment since only text is provided at this stage.

#### Dual-Condition Diffusion Transformer

We present the Dual-DiT, a model designed to fuse two distinct conditions to generate natural and coherent speech.
We begin by adopting a Transformer-based architecture~\cite{peebles2023scalable} as our base model, utilizing its multi-head attention-based fusion mechanism, which is effective at modeling long-range dependencies and facilitating efficient modality fusion~\cite{chen2023pixartalpha}.

Subsequently, we explore two methods for incorporating the acoustic feature into the DiT model.
When long acoustic features are integrated via cross-attention, the computational complexity scales quadratically with the sequence length~\cite{beltagy2020longformer}.
Alternatively, we concatenate the acoustic features with the noisy input in the mel-spectrogram space before passing them through the DiT model.
This method yields more stable training and produces natural-sounding speech.
However, training the diffusion model in the high-resolution mel-spectrogram space remains computationally demanding.
To address this, we propose a Latent Mapper, consisting of two 2D convolutional layers, to map the long acoustic features into the latent space of the DiT.
Specifically, the acoustic feature \(\mu_{1:F}\) of size $T\times F$ is mapped into a latent representation \(\mu_{latent}\) with a resolution of $8 \times T/4 \times F/4$, aligning with the resolution of the noisy latent.
By employing the Latent Mapper, we enable the diffusion model to be trained in the latent space, significantly reducing the computational cost by 94\% compared to pixel-space-based DiT models.

To enable the model to generate sounds appropriate for the given environment conditions, we design the DiT block by incorporating a cross-attention module~\cite{vaswani2017attention}.
In our implementation, environmental conditions are extracted using the Contrastive Language-Audio Pre-training (CLAP) audio encoder.

The original DiT processes this additional conditional information through adaptive layer norm (adaLN) modules where the scale and shift parameters are regressed from the sum of the time embedding and condition label.
However, due to adaLN's reliance on affine transformations, there is a potential risk of losing detailed conditioning information~\cite{perez2018film}.
To address this issue, we introduce a cross-attention module between the self-attention and feed-forward layers.
This design choice ensures that CLAP embeddings are directly injected into the DiT model via cross-attention, thereby facilitating a more precise representation of detailed environmental sounds.

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论