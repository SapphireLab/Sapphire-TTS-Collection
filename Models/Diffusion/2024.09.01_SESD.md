# SESD

<details>
<summary>基本信息</summary>

- 标题: Sample-Efficient Diffusion for Text-To-Speech Synthesis
- 作者:
  | 序号 | 作者 | 机构 |
  | :-: | --- | --- |
  | 01 | [Justin Lovelace](../../Authors/Justin_Lovelace.md) | [Cornell University](../../Institutions/USA-Cornell_康奈尔大学.md) <br> [ASAPP Inc.](../../Institutions/USA-ASAPP.Inc.md) | 
  | 02 | [Soham Ray](../../Authors/Soham_Ray.md) | [ASAPP Inc.](../../Institutions/USA-ASAPP.Inc.md) |
  | 03 | [Kwangyoun Kim](../../Authors/Kwangyoun_Kim.md) | [ASAPP Inc.](../../Institutions/USA-ASAPP.Inc.md) |
  | 04 | [Kilian Q. Weinberger](../../Authors/Kilian_Q._Weinberger.md) | [Cornell University](../../Institutions/USA-Cornell_康奈尔大学.md) <br> [ASAPP Inc.](../../Institutions/USA-ASAPP.Inc.md) | 
  | 05 | [Felix Wu](../../Authors/Felix_Wu.md) | [ASAPP Inc.](../../Institutions/USA-ASAPP.Inc.md) <br> [Character.AI](../../Institutions/USA-Character.AI.md)|
- 机构:
  | 序号 | 机构 | 占比 |
  | :-: | --- | :-: |
  | 01 | [Cornell University](../../Institutions/USA-Cornell_康奈尔大学.md) | 02/05 |
  | 02 | [ASAPP Inc.](../../Institutions/USA-ASAPP.Inc.md) | 05/05 |
  | 03 | [Character.AI](../../Institutions/USA-Character.AI.md) | 01/05 |
- 时间:
  - 预印时间: 2024.09.01 ArXiv v1
  - 更新笔记: 2024.09.06
- 发表:
  - [InterSpeech 2024](../../Publications/InterSpeech.md)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2409.03717)
  - [DOI]()
  - [Github](https://github.com/justinlovelace/SESD) 尚未开放
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: 5
- 引用: 32
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

> This work introduces ***Sample-Efficient Speech Diffusion (SESD)***, an algorithm for effective speech synthesis in modest data regimes through latent diffusion. 
> It is based on a novel diffusion architecture, that we call ***U-Audio Transformer (U-AT)***, that efficiently scales to long sequences and operates in the latent space of a pre-trained audio autoencoder. 
> Conditioned on character-aware language model representations, ***SESD*** achieves impressive results despite training on less than 1k hours of speech – far less than current state-of-the-art systems. 
> In fact, it synthesizes more intelligible speech than the state-of-the-art auto-regressive model, [VALL-E](../Speech_LLM/2023.01.05_VALL-E.md), while using less than 2 % the training data. 
> Our implementation is available at [Github](https://github.com/justinlovelace/SESD).

</details>

本项工作介绍了 ***样本高效语音扩散 (Sample-Efficient Speech Diffusion, SESD)***, 这是一种通过潜在扩散在适度数据条件下实现有效语音合成的算法.
它基于一种新颖的扩散架构, 我们称为 ***U-Audio Transformer (U-AT)***，它可以有效地扩展到长序列并在预训练的音频自编码器的潜在空间中运行.
以字符感知语言模型表示作为条件, ***SESD*** 能在不足 1k 小时的语音上训练并获得令人印象深刻的结果, 远少于现有的最先进系统.
事实上, 它合成的语音比当前最先进的自回归模型 [VALL-E](../Speech_LLM/2023.01.05_VALL-E.md) 更易理解, 而使用的训练数据不到后者的 2%.
我们的实现可在 [Github](https://github.com/justinlovelace/SESD) 上获得.

## 1.Introduction: 引言

> Neural approaches have revolutionized generative speech modeling, with recent advances driven by auto-regressive and diffusion-based systems~\cite{le2023voicebox,wang2023neural}. These improvements, however, come with a cost. Generative models are data hungry, and state-of-the-art systems have used increasingly large volumes of annotated data. This poses challenges for the application of these methods to low-resource domains and languages. Learning effective generative models with limited data has so far remained an open challenge.
>
> To address this data bottleneck, we develop a latent diffusion model that can exploit abundant  \textit{unlabeled} speech data and therefore requires only a fraction of the \textit{labeled} data~\cite{rombach2021highresolution}.  We utilize a pre-trained autoencoder to map high-dimensional speech waveforms to compact latent representations. By training a diffusion model to generate samples in the lower-dimensional latent space, we offload modeling of fine-grained data characteristics to the unsupervised autoencoder. This allows the diffusion model to focus on the more tractable latent space, thereby improving data efficiency. 
>
> In speech synthesis, the generated audio must align with the text transcript. This makes diffusion models a proper fit, because they can incorporate complex conditioning information into the generative process. However, with limited training data it is challenging to generalize across diverse transcripts. To address this issue, we condition our model on representations from a pre-trained language model. These representations, learned through self-supervised pre-training, contain the rich linguistic information necessary for natural speech synthesis and help our model generalize effectively to diverse text inputs.
>
> Building on these insights, we introduce ***Sample-Efficient Speech Diffusion (SESD)***, a sample-efficient latent diffusion framework that achieves impressive results with less than 1k hours of speech data. We develop a diffusion architecture, the U-Audio Transformer (U-AT), that scales efficiently to long audio sequences. It consists of a 1D U-Net that downsamples the audio features before applying a transformer backbone to model global speech characteristics. Crucially, we propose a position-aware cross-attention mechanism to condition the model on representations from a frozen character-aware language model, ByT5-base \cite{xue2022byt5}. To increase our model's alignment with the transcript, we adjust the diffusion loss weighting to emphasize performance at high noise levels where the global structure of the speech (e.g. word placement) is being determined.
>
> With these innovations, ***SESD*** can synthesize highly intelligible speech directly from text transcripts, without the explicit phoneme alignment required by current TTS diffusion models \cite{le2023voicebox, shen2023naturalspeech}. For text-only TTS, our framework achieves a word error rate (WER) of 2.3\%, nearly matching the 2.2\% WER of natural human speech. For speaker-prompted synthesis, ***SESD*** generates audio with a WER rate of 2.3\% and a speaker similarity score of 0.617, outperforming the state-of-the-art autoregressive model VALL-E (WER 5.9\%, similarity 0.580) which uses 62.5x times more training data \cite{wang2023neural}. 

## 2.Related Works: 相关工作

> Most related are the diffusion TTS models, NaturalSpeech2 (NS2) \cite{shen2023naturalspeech} and VoiceBox \cite{le2023voicebox}. 
> They depend on phonemizers and aligners for frame level phonetic transcripts, which can introduce errors \cite{mcauliffe17_interspeech}. 
> Both need phoneme duration annotations for generation, necessitating an external model for phoneme duration prediction. Our system, however, can synthesize varied speech with just the utterance duration and transcript.
> NS2 also requires pitch annotations and a speech prompt, unlike our system which supports text-only generation. Importantly, our method is more data-efficient, requiring far less annotated data than NS2 and VoiceBox by 45.8x and 62.6x, respectively.

### Background

> Diffusion models \cite{sohl2015deep, ddpm, kingma2021variational} are latent variable models with latents $\mathbf{z}  = \{\mathbf{z}_t | t\in [0,1] \}$ given by a forward diffusion process $q(\mathbf{z}|\mathbf{x})$, which defines a gradual transition from the data distribution, $\mathbf{x} \sim p(\mathbf{x})$, to a Gaussian distribution. The Markovian forward process iteratively adds Gaussian noise to the data over time and satisfies

$$
\begin{aligned}
    q(\mathbf{z}_t|\mathbf{z}_s)=\mathcal{N}(\mathbf{z}_t; \alpha_{t|s}\mathbf{z}_s, (1-\alpha_{t|s}^2)\mathbf{I}),\\
    q(\mathbf{z}_t|\mathbf{x}) = \mathcal{N}(\mathbf{z}_t; \alpha_t\mathbf{x}, (1-\alpha_t^2)\mathbf{I})
\end{aligned}
$$

> where $\alpha_{t|s} = \alpha_t/\alpha_s$ and $0 \leq s < t \leq 1$. The noise schedule, determined by $\alpha_t\in [0,1]$, monotonically decreases the signal-to-noise ratio (SNR), $\lambda_t =\frac{\alpha_t^2}{1-\alpha_t^2}$ as a function of the time, $t$, such that the final latent becomes approximately Gaussian, $q(\mathbf{z}_1) \approx \mathcal{N}(\mathbf{0}, \mathbf{I})$. The forward process therefore defines a transition from the data distribution to a Gaussian distribution.
>
> Diffusion models define a generative process to invert the forward process. This specifies a transition from Gaussian noise, which can be sampled analytically, to the unknown data distribution. Inverting this process can be reduced to learning a \textit{denoising network}, $\hat{\mathbf{x}}_\theta(\mathbf{z}_t, t, \mathbf{c}) \approx \mathbf{x}$, that reconstructs the clean data given some noisy latent, the time, and (optionally) some conditioning information, $\mathbf{c}$, about the data. The conditioning information could be a textual description of an image \cite{saharia2022photorealistic} or, in our case, a textual transcription of some speech. 
>
> In practice, the denoising network is often parameterized as a noise prediction network \cite{ddpm} or a velocity prediction network \cite{salimans2022progressive}, where the velocity is given as $\mathbf{v} = {\alpha_t}\bm{\epsilon} - \sqrt{1-\alpha^2_t} \mathbf{x}$, to improve training stability and performance \cite{salimans2022progressive}.  We adopt the $\mathbf{v}$-parameterization throughout this work and therefore train the denoising network with the regression objective

\[ 
\mathcal{L}(\theta) = \mathbb{E}_{t,\mathbf{x}, \epsilon} [  w(\lambda_t) \lVert\hat{\mathbf{v}}_{\theta}(\mathbf{z}_t, t, \mathbf{c}) - \mathbf{v} \rVert_2^2 ] 
\]

> with some time-dependent weighting, $w(\lambda_t)$, that is set empirically to emphasize noise levels that are important for downstream perceptual quality \cite{ddpm, nichol2021improved}. This loss function is the weighted variational lower bound of the log likelihood of the data under the forward diffusion process \cite{sohl2015deep, ddpm, kingma2021variational}.

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

> We present ***SESD***, a highly sample-efficient latent diffusion framework for text-to-speech synthesis that achieves strong results in a modest data regime. 
> The key ingredients in the success of ***SESD*** are: a novel diffusion architecture that efficiently models long audio sequences, incorporating representations from a byte-level language model that capture linguistic properties critical for natural speech synthesis, and modifying the diffusion loss weighting to improve text-speech alignment.
> Together, these innovations enable ***SESD*** to perform speech synthesis directly from text without explicit phoneme alignment. 
> ***SESD*** generates intelligible speech near human-level word error rates with less than 1k hours of training data.
