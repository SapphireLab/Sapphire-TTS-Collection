# SESD

<details>
<summary>基本信息</summary>

- 标题: Sample-Efficient Diffusion for Text-To-Speech Synthesis
- 作者:
  | 序号 | 作者 | 机构 |
  | :-: | --- | --- |
  | 01 | [Justin Lovelace](../../Authors/Justin_Lovelace.md) | [Cornell University](../../Institutions/USA-Cornell_康奈尔大学.md) <br> [ASAPP Inc.](../../Institutions/USA-ASAPP.Inc.md) | 
  | 02 | [Soham Ray](../../Authors/Soham_Ray.md) | [ASAPP Inc.](../../Institutions/USA-ASAPP.Inc.md) |
  | 03 | [Kwangyoun Kim](../../Authors/Kwangyoun_Kim.md) | [ASAPP Inc.](../../Institutions/USA-ASAPP.Inc.md) |
  | 04 | [Kilian Q. Weinberger](../../Authors/Kilian_Q._Weinberger.md) | [Cornell University](../../Institutions/USA-Cornell_康奈尔大学.md) <br> [ASAPP Inc.](../../Institutions/USA-ASAPP.Inc.md) | 
  | 05 | [Felix Wu](../../Authors/Felix_Wu.md) | [ASAPP Inc.](../../Institutions/USA-ASAPP.Inc.md) <br> [Character.AI](../../Institutions/USA-Character.AI.md)|
- 机构:
  | 序号 | 机构 | 占比 |
  | :-: | --- | :-: |
  | 01 | [Cornell University](../../Institutions/USA-Cornell_康奈尔大学.md) | 02/05 |
  | 02 | [ASAPP Inc.](../../Institutions/USA-ASAPP.Inc.md) | 05/05 |
  | 03 | [Character.AI](../../Institutions/USA-Character.AI.md) | 01/05 |
- 时间:
  - 预印时间: 2024.09.01 ArXiv v1
  - 更新笔记: 2024.09.06
- 发表:
  - [InterSpeech 2024](../../Publications/InterSpeech.md)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2409.03717)
  - [DOI]()
  - [Github](https://github.com/justinlovelace/SESD) 尚未开放
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: 5
- 引用: 32
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

> This work introduces ***Sample-Efficient Speech Diffusion (SESD)***, an algorithm for effective speech synthesis in modest data regimes through latent diffusion. 
> It is based on a novel diffusion architecture, that we call ***U-Audio Transformer (U-AT)***, that efficiently scales to long sequences and operates in the latent space of a pre-trained audio autoencoder. 
> Conditioned on character-aware language model representations, ***SESD*** achieves impressive results despite training on less than 1k hours of speech – far less than current state-of-the-art systems. 
> In fact, it synthesizes more intelligible speech than the state-of-the-art auto-regressive model, [VALL-E](../Speech_LLM/2023.01.05_VALL-E.md), while using less than 2 % the training data. 
> Our implementation is available at [Github](https://github.com/justinlovelace/SESD).

</details>
<br>

本项工作介绍了 ***样本高效语音扩散 (Sample-Efficient Speech Diffusion, SESD)***, 这是一种通过潜在扩散在适度数据条件下实现有效语音合成的算法.
它基于一种新颖的扩散架构, 我们称为 ***U-Audio Transformer (U-AT)***，它可以有效地扩展到长序列并在预训练的音频自编码器的潜在空间中运行.
以字符感知语言模型表示作为条件, ***SESD*** 能在不足 1k 小时的语音上训练并获得令人印象深刻的结果, 远少于现有的最先进系统.
事实上, 它合成的语音比当前最先进的自回归模型 [VALL-E](../Speech_LLM/2023.01.05_VALL-E.md) 更易理解, 而使用的训练数据不到后者的 2%.
我们的实现可在 [Github](https://github.com/justinlovelace/SESD) 上获得.

## 1.Introduction: 引言

<details>
<summary>展开原文</summary>

> Neural approaches have revolutionized generative speech modeling, with recent advances driven by auto-regressive and diffusion-based systems ([VoiceBox [1]](../Speech_LLM/2023.06.23_VoiceBox.md); [VALL-E [2]](../Speech_LLM/2023.01.05_VALL-E.md)). 
> These improvements, however, come with a cost. 
> Generative models are data hungry, and state-of-the-art systems have used increasingly large volumes of annotated data. 
> This poses challenges for the application of these methods to low-resource domains and languages. 
> Learning effective generative models with limited data has so far remained an open challenge.
>
> To address this data bottleneck, we develop a latent diffusion model that can exploit abundant  unlabeled speech data and therefore requires only a fraction of the labeled data ([LDM [3]](2021.12.20_LDM.md)).  
> We utilize a pre-trained autoencoder to map high-dimensional speech waveforms to compact latent representations. 
> By training a diffusion model to generate samples in the lower-dimensional latent space, we offload modeling of fine-grained data characteristics to the unsupervised autoencoder. 
> This allows the diffusion model to focus on the more tractable latent space, thereby improving data efficiency. 
>
> In speech synthesis, the generated audio must align with the text transcript. 
> This makes diffusion models a proper fit, because they can incorporate complex conditioning information into the generative process. 
> However, with limited training data it is challenging to generalize across diverse transcripts. 
> To address this issue, we condition our model on representations from a pre-trained language model. 
> These representations, learned through self-supervised pre-training, contain the rich linguistic information necessary for natural speech synthesis and help our model generalize effectively to diverse text inputs.
>
> Building on these insights, we introduce ***Sample-Efficient Speech Diffusion (SESD)***, a sample-efficient latent diffusion framework that achieves impressive results with less than 1k hours of speech data. 
> We develop a diffusion architecture, the ***U-Audio Transformer (U-AT)***, that scales efficiently to long audio sequences. 
> It consists of a 1D U-Net that downsamples the audio features before applying a transformer backbone to model global speech characteristics. 
> Crucially, we propose a position-aware cross-attention mechanism to condition the model on representations from a frozen character-aware language model, [ByT5 [4]](../LLM/2021.05.28_ByT5.md)-base. 
> To increase our model's alignment with the transcript, we adjust the diffusion loss weighting to emphasize performance at high noise levels where the global structure of the speech (e.g. word placement) is being determined.
>
> With these innovations, ***SESD*** can synthesize highly intelligible speech directly from text transcripts, without the explicit phoneme alignment required by current TTS diffusion models ([VoiceBox [1]](../Speech_LLM/2023.06.23_VoiceBox.md); [NaturalSpeech2 [5]](2023.04.18_NaturalSpeech2.md)). 
> For text-only TTS, our framework achieves a word error rate (WER) of 2.3\%, nearly matching the 2.2\% WER of natural human speech. 
> For speaker-prompted synthesis, ***SESD*** generates audio with a WER rate of 2.3\% and a speaker similarity score of 0.617, outperforming the state-of-the-art autoregressive model VALL-E (WER 5.9\%, similarity 0.580) which uses 62.5x times more training data ([VALL-E [2]](../Speech_LLM/2023.01.05_VALL-E.md)). 

</details>
<br>

神经方法已经彻底改变了生成语音建模, 最近的进展由自回归和基于扩散的系统 ([VoiceBox [1]](../Speech_LLM/2023.06.23_VoiceBox.md); [VALL-E [2]](../Speech_LLM/2023.01.05_VALL-E.md)) 推动.
然而这些改进是有代价的.

生成式模型需要大量数据, 而最先进的系统使用了越来越多的标注数据.
这对于这些方法在低资源领域和语言中的应用带来了挑战.
在有限数据的情况下学习有效的生成模型至今仍然是一个开放挑战.

为了解决这一数据瓶颈, 我们开发了一种潜在扩散模型 ([LDM [3]](2021.12.20_LDM.md)), 该模型可以利用丰富的未标注语音数据, 因此只需要一部分标注数据.
我们利用预训练的自编码器将高维语音波形映射到紧凑的潜在表示.
通过训练扩散模型在低维潜在空间中生成样本, 我们将细粒度数据特征的建模任务卸载给无监督的自编码器.
这使得扩散模型能够专注于更易处理的隐空间, 从而提高数据效率.

在语音合成中, 生成的音频必须和转录文本对齐.
这使得扩散模型成为一个合适的模型, 因为它们可以将复杂的条件信息纳入生成过程中.
然而在有限的训练数据下, 要跨多样化的转录进行泛化是具有挑战性的.
为了解决这个问题, 我们使用预训练语言模型的表示来条件化我们的模型.
这些通过自监督预训练学到的表示包含了自然语音合成所需的丰富语言信息, 并帮助我们的模型有效地泛化到多样化的文本输入.

基于这些见解, 我们引入了***样本高效语言扩散 (Sample-Efficient Speech Diffusion, SESD)***, 这是一种样本高效的潜在扩散框架, 使用不到一千小时的语音数据就能取得令人印象深刻的结果.
我们开发了一种扩散架构 ***U-Audio Transformer (U-AT)***，它可以有效地扩展到长音频序列.
它由一个 1D U-Net 组成, 该 U-Net 在应用 Transformer 主干模型以建模全局语音特征之前对音频特征进行下采样.
至关重要的是, 我们提出了一种位置感知的交叉注意力机制, 以条件化来自冻结的字符感知语言模型 [ByT5 [4]](../LLM/2021.05.28_ByT5.md)-base 的表示.
为了增强模型与转录的对齐, 我们调整了扩散损失权重, 以强调在高噪声水平下的性能, 此时语音的全局结构（例如词语位置）正在被确定.

通过这些创新, ***SESD*** 可以直接从文本转录生成高质量的语音, 而不需要显式的音素对齐, 这与当前的 TTS 扩散模型 ([VoiceBox [1]](../Speech_LLM/2023.06.23_VoiceBox.md); [NaturalSpeech2 [5]](2023.04.18_NaturalSpeech2.md)) 所需的音素对齐不同.
对于纯文本 TTS, 我们的框架能生成 2.3\% 的词错误率 (WER), 与自然人语音的 2.2\% WER 相当.
对于说话人提示的合成，***SESD*** 生成的音频的 WER 率为 2.3%, 说话人相似度得分为 0.617, 优于使用 62.5 倍更多训练数据的最先进的自回归模型 [VALL-E [2]](../Speech_LLM/2023.01.05_VALL-E.md) (WER 5.9%，相似度0.580).

## 2.Related Works: 相关工作

> Most related are the diffusion TTS models, [NaturalSpeech2 (NS2) [5]](2023.04.18_NaturalSpeech2.md) and [VoiceBox [1]](../Speech_LLM/2023.06.23_VoiceBox.md). 
> They depend on phonemizers and aligners for frame level phonetic transcripts, which can introduce errors ([Montreal Forced Aligner [6]](../Tricks/Montreal_Forced_Aligner.md)). 
> Both need phoneme duration annotations for generation, necessitating an external model for phoneme duration prediction. 
> Our system, however, can synthesize varied speech with just the utterance duration and transcript.
> NS2 also requires pitch annotations and a speech prompt, unlike our system which supports text-only generation. 
> Importantly, our method is more data-efficient, requiring far less annotated data than NS2 and VoiceBox by 45.8x and 62.6x, respectively.

### Background

> Diffusion models ([DM [7]](DM.md); [DDPM [8]](DDPM.md); [VDM [9]](VDM.md)) are latent variable models with latents $\mathbf{z}  = \{\mathbf{z}_t | t\in [0,1] \}$ given by a forward diffusion process $q(\mathbf{z}|\mathbf{x})$, which defines a gradual transition from the data distribution, $\mathbf{x} \sim p(\mathbf{x})$, to a Gaussian distribution. 
> The Markovian forward process iteratively adds Gaussian noise to the data over time and satisfies

$$
\begin{aligned}
    q(\mathbf{z}_t|\mathbf{z}_s)=\mathcal{N}(\mathbf{z}_t; \alpha_{t|s}\mathbf{z}_s, (1-\alpha_{t|s}^2)\mathbf{I}),\\
    q(\mathbf{z}_t|\mathbf{x}) = \mathcal{N}(\mathbf{z}_t; \alpha_t\mathbf{x}, (1-\alpha_t^2)\mathbf{I})
\end{aligned}
$$

> where $\alpha_{t|s} = \alpha_t/\alpha_s$ and $0 \leq s < t \leq 1$. 
> The noise schedule, determined by $\alpha_t\in [0,1]$, monotonically decreases the signal-to-noise ratio (SNR), $\lambda_t =\frac{\alpha_t^2}{1-\alpha_t^2}$ as a function of the time, $t$, such that the final latent becomes approximately Gaussian, $q(\mathbf{z}_1) \approx \mathcal{N}(\mathbf{0}, \mathbf{I})$. 
> The forward process therefore defines a transition from the data distribution to a Gaussian distribution.
>
> Diffusion models define a generative process to invert the forward process. 
> This specifies a transition from Gaussian noise, which can be sampled analytically, to the unknown data distribution. 
> Inverting this process can be reduced to learning a \textit{denoising network}, $\hat{\mathbf{x}}_\theta(\mathbf{z}_t, t, \mathbf{c}) \approx \mathbf{x}$, that reconstructs the clean data given some noisy latent, the time, and (optionally) some conditioning information, $\mathbf{c}$, about the data. 
> The conditioning information could be a textual description of an image ([Imagen [10]](../_Basis/Imagen.md)) or, in our case, a textual transcription of some speech. 
>
> In practice, the denoising network is often parameterized as a noise prediction network ([DDPM [8]](DDPM.md)) or a velocity prediction network ([Progressive_Distillation [11]](Progressive_Distillation.md)), where the velocity is given as $\mathbf{v} = {\alpha_t}\bm{\epsilon} - \sqrt{1-\alpha^2_t} \mathbf{x}$, to improve training stability and performance ([Progressive_Distillation [11]](Progressive_Distillation.md)).  
> We adopt the $\mathbf{v}$-parameterization throughout this work and therefore train the denoising network with the regression objective

\[ 
\mathcal{L}(\theta) = \mathbb{E}_{t,\mathbf{x}, \epsilon} [  w(\lambda_t) \lVert\hat{\mathbf{v}}_{\theta}(\mathbf{z}_t, t, \mathbf{c}) - \mathbf{v} \rVert_2^2 ] 
\]

> with some time-dependent weighting, $w(\lambda_t)$, that is set empirically to emphasize noise levels that are important for downstream perceptual quality ([DDPM [8]](DDPM.md); [Improved_DDPM [12]](iDDPM.md)). 
> This loss function is the weighted variational lower bound of the log likelihood of the data under the forward diffusion process ([DM [7]](DM.md); [DDPM [8]](DDPM.md); [VDM [9]](VDM.md)).

## 3.Methodology: 方法

![](Images/2024.09.01_SESD_Fig.01.png)

### Latent Audio Diffusion.

> While auto-regressive approaches require discrete tokens, diffusion models are effective at generating continuous representations. 
> This avoids potential information loss from quantization and and enables modeling long sequences more efficiently. 
> To leverage these benefits, we train our diffusion model on the continuous latent embeddings from a pretrained audio auto-encoder.
> Specifically, we utilize the publicly available EnCodec autoencoder to map 24kHz waveforms to sequences of 75 latent vector representations per second of audio ([EnCodec [13]](../Speech_Neural_Codec/2022.10.24_EnCodec.md)). 
> EnCodec applies residual vector quantization to map each continuous latent vector to multiple discrete tokens capturing increasingly fine details. 
> Instead of modeling these discrete tokens, we train our diffusion model to generate the 75Hz 128-dimensional continuous embeddings from the EnCodec encoder prior to quantization.
>
> This continuous latent diffusion approach significantly reduces the effective sequence length compared to modeling tokens - a 10 second clip consists of just 750 latent vectors rather than 24,000 tokens after quantization (a 32x reduction). 
> The continuous latents generated during inference can then be quantized and decoded by EnCodec to produce the waveform.

### U-Audio Transformer (U-AT).

> For our diffusion network, we propose the U-Audio Transformer (U-AT), a hybrid architecture that combines the strengths of U-Nets and transformers (see \autoref{fig:u-audt}). 
> U-Nets are well-suited for high-resolution data, while transformers excel at capturing long-range dependencies and incorporating conditioning information.
> In the U-AT, we first use a 1D U-Net to downsample the lengthy audio features from a maximum length of 1504 frames to 188 frames. 
> This downsampling step allows us to apply a deep transformer backbone to the compressed sequence, incorporating information from the transcript ([Transformer [14]](../_Transformer/2017.06.12_Transformer.md)). 
> Processing the full-resolution input with a transformer would be computationally prohibitive.
>
> To enhance the transformer's capacity for modeling global information, we incorporate a recent advance from vision transformers ([ViT_Registers [15]](../_Transformer/ViT_Registers.md)) and prepend 8 learnable register tokens to the downsampled features. 
> These tokens act as global memory slots, enabling the transformer to better process global information. 
> After applying the transformer, the register tokens are discarded, and the U-Net decoder upsamples only the corresponding audio features back to the original sequence length for the final prediction.
> Hybrid U-Net/transformer architectures have shown promise for high-resolution image diffusion ([Sample_Diffusion [16]](2023.01.26_Sample_Diffusion.md)), motivating our adaptation to the audio domain.

### Position-Aware Cross-Attention.

> Properly aligning the generated speech with the input transcript is a critical challenge in text-to-speech synthesis. 
> To improve alignment, we introduce position-aware cross-attention layers in the transformer model that attend to transcript representations from a frozen [ByT5 [4]](../LLM/2021.05.28_ByT5.md)-base encoder. 
> To explicitly incorporate positional information about the tokens in the transcript, we introduce a neural Position Encoder that maps the relative positions of the transcript tokens to key vectors. 
> We sum these positional key vectors with the corresponding key vectors from the ByT5 embedding in the cross-attention mechanism. 
> This allows the model to directly search for and attend to the relevant positions within the transcript when generating each audio frame.
>
> Specifically, we compute the cross-attention logits as:

\[\mathbf{A}_{ij}=\mathbf{q}_i^\top (\mathbf{k}_j + f_{\theta}(j/m))\]

> where $\mathbf{q}_i \in \mathbb{R}^d$ is the query vector for audio frame $i$, $\mathbf{k}_j \in \mathbb{R}^d$ is the key vector for the ByT5 text embedding $j$ in the sequence of $m$ bytes, and $f_{\theta}(j/m)\in \mathbb{R}^d$ is a relative position embedding computed with a lightweight MLP. 
> This positional encoding is critical for generating speech aligned with the transcript.

### Diffusion Loss Weighting.

> Properly emphasizing the diffusion noise levels that are most important for perceptual quality is critical ([VDM++ [17]](VDM_PP.md)). 
> Previous work has utilized symmetric weightings like the V-Weighting or unimodal distributions centered around moderate noise levels ([VDM++ [17]](VDM_PP.md); [VoiceBox [1]](../Speech_LLM/2023.06.23_VoiceBox.md)). 
> However, in text-to-speech synthesis, the input transcript and speech prompt provide valuable signal even at high noise levels where the signal in the corrupted latent itself is limited. 
> These high noise levels are precisely where the conditioning information is most beneficial for resolving the global speech structure and aligning it with the provided transcript.
>
> We therefore propose an asymmetric diffusion loss weighting that emphasizes performance at high noise levels where the transcript and prompt are relied upon to estimate the original speech. 
> We visualize our proposed asymmetric weighting, a symmetric weighting baseline, and the V-weighting in \autoref{fig:noise_schedules}. 
> Our proposed weighting dedicates more model capacity to resolving aspects like word placement and positioning compared to symmetric weightings.
> Specifically, we parameterize the weighting $w(\lambda_t)$ with a heavy-tailed Cauchy distribution for high noise levels $\lambda_t < -1$, combined with a unimodal normal for lower noise levels:

$$
\begin{aligned}
w(\lambda_t)= \begin{cases}
\frac{1}{Z_c} \text{Cauchy}(\lambda_t;-1, 4.8) & \text{if } \lambda_t < -1\\
\frac{1}{Z_n} \mathcal{N}(\lambda_t;-1,2.4) & \text{if } \lambda_t \geq -1
\end{cases}
\end{aligned}
$$

> where $Z_c$ and $Z_n$ normalize the densities to 1 at $\mu=-1$. 
> This asymmetric weighting improves transcript alignment in our generations compared to symmetric alternatives. 
> For training efficiency, we utilize the adaptive noise scheduler ([VDM++ [17]](VDM_PP.md)) to reduce loss estimate variance. 

### Duration Prediction.

> Our approach avoids the need for predicting phoneme durations as an intermediate step, which can introduce errors. 
> During training, we provide the diffusion network with noisy latents of the correct sequence length corresponding to the full utterance duration. 
> At inference time, we only specify the overall duration, not individual phoneme durations. 
> In contrast, models like NaturalSpeech2 and VoiceBox require an external phoneme duration prediction model.
>
> Our model instead learns to resolve the phoneme durations in an end-to-end manner from only the text transcript during diffusion training. 
> For duration prediction at inference time, we simply fine-tune the ByT5-base model as a stochastic duration predictor in a sequence-to-sequence manner. 
> Conditioned on the transcript, it generates utterance durations (e.g. "4.51") auto-regressively with [nucleus sampling [18]](../Tricks/Nucleus_Sampling.md) (p=0.95), achieving a 1.4 second RMSE. 
> Importantly, our approach is agnostic to the duration selection method, avoiding cascaded errors from explicit phoneme duration modeling.

### Speaker-Prompted Generation.

> The ability to perform speaker-prompted generation, where a short sample of reference audio conditions the generation on the desired speaker's voice characteristics, is a valuable capability for TTS systems. 
> Diffusion models can perform speaker-prompted TTS through audio inpainting ([VoiceBox [1]](../Speech_LLM/2023.06.23_VoiceBox.md)). 
> We train our denoising network for both text-only and speaker-prompted TTS synthesis in a multi-task fashion. 
> With probability $p=0.5$, we train the network to perform audio inpainting by concatenating a clean audio latent with a noisy latent vector. 
> We sample a duration $d$ and concatenate the start of the latent audio representation $\mathbf{x}[{:}d]$ with the end of the noisy latent $\mathbf{z}_t[d{:}]$ to construct the input. 
> We also introduce a binary embedding to identify corrupted frames, which we sum with the input after the initial projection. 
> When calculating the loss, we mask out frames corresponding to the clean audio. 
>
> For the prompt duration, we sample the proportion of the input, $d \in [0,1] $, to hold out as the clean prompt. 
> For instance, if we sample $d=0.1$ for a 10 second clip of audio, then we use the frames corresponding to the first second of audio as the clean prompt. 
> For sampling the duration, we use a Beta distribution with a mode of $.01$ and a concentration of $5$ to emphasize challenging cases with very short prompts. 
> During inference, we prepend a speaker's reference audio and the associated text to perform speaker-prompted TTS synthesis.

### Classifier-Free Guidance.

> To enable [classifier-free guidance [19]](../Diffusion/Classifier-Free_Guidance.md), we drop the text with probability $p=0.1$ and jointly train a conditional and unconditional diffusion model. 
> During inference, we introduce a sampling parameter $w$, and compute 

\[\hat{\mathbf{v}}_{\theta}^w(\mathbf{z}_t, t, \mathbf{c}) = \hat{\mathbf{v}}_{\theta}(\mathbf{z}_t, t) + w*(\hat{\mathbf{v}}_{\theta}(\mathbf{z}_t, t, \mathbf{c}) -\hat{\mathbf{v}}_{\theta}(\mathbf{z}_t, t)).\]

> When $w=1.0$, this reduces to the conditional diffusion model, and setting $w>1.0$ increases the influence of the conditioning information.
> For the cross-attention layers, we concatenate a learnable null embedding with the text features along the sequence dimension. 
> We mask out the text features to drop conditioning information.

### Implementation Details.

> We begin with the 2D U-Net design used by [iDDPM [12]](iDDPM.md) for image diffusion and replace its 2D convolutions with corresponding 1D convolutions to adapt the U-Net to 1D sequences. 
> For instance, we substitute each 2D convolution of size 3x3 with a 1D convolution of size 3. 
> We make similar substitutions for the downsampling and upsampling operations. 
> Our U-Net has 4 stages which downsample the input from 1504 frames to 188 frames. 
> We utilize a feature dimensionality of 512 throughout the network. 
>
> We use a transformer backbone ([Transformer [14]](../_Transformer/2017.06.12_Transformer.md); [DiT [20]](DiT.md)) with 8 layers and a dimension of 512. 
> We encode positional information with a 1D Dynamic Position Bias (DPB) ([Crossformer [21]](../_Transformer/Crossformer.md)). 
> This introduces a lightweight MLP that maps relative offsets between locations, $\Delta x_{i,j} \in \{..., -1, 0, 1, 2,...\} $, to head-specific bias terms that are added to the self-attention logits before the softmax.
> To condition the diffusion network on the level of noise, we utilize $\alpha$-conditioning ([WaveGrad [22]](../TTS3_Vocoder/2020.09.02_WaveGrad.md); [Noise Scheduling [23]](../Diffusion/Noise_Scheduling.md)). 
> We map $\alpha_t$ to a sinusoidal position embedding ([Transformer [14]](../_Transformer/2017.06.12_Transformer.md)) and pass it through an MLP to obtain a time embedding. 
> We condition the U-Net residual blocks and transformer feedforward layers on the time embedding following standard practice from prior image diffusion work ([DiT [20]](DiT.md)). 
> We pad the audio with silence up to 20 seconds (i.e., 1504 latents), and mask out the silence from the network.
>
> Our final model has 137M trainable parameters. 
> We train ***SESD*** for 250k steps with a batch size of 64 utterances on one Nvidia A6000 GPU.  We use the [AdamW [24]](../../Modules/Optimization/2017.11.14_AdamW.md) optimizer with a 1000-step linear warmup, a peak learning rate of 2e-4, a cosine learning rate decay, and independent weight decay of 2e-4. 
> We apply dropout of 0.1 to the feedfoward, self-attention, and cross-attention layers in the transformer. 
> We compute an exponential moving average (momentum of 0.9999) of the training model. 
> For our ablation study, we train all models for 100k steps without dropout or weight decay. 
> For generation, we use 250 sampling steps with the scaled cosine noise schedule with a scale factor of 0.5 ([Noise Scheduling [23]](../Diffusion/Noise_Scheduling.md); [Sample_Diffusion [16]](2023.01.26_Sample_Diffusion.md)). 
> We use the [DDPM [8]](DDPM.md) sampler with $w=5.0$ for text-only synthesis and the [DDIM [25]](../Diffusion/DDIM.md) sampler with $w=8.0$ for speaker-prompted synthesis. 
> Full implementation details and model checkpoints are provided in our code release to enable exact reproduction of our work.

## 4.Experiments: 实验

> We utilize the clean and other training splits of the [LibriSpeech (LS) dataset [26]](../../Datasets/2015.04.19_LibriSpeech.md), totaling 960 hours of speech, to train ***SESD***. 
> For evaluation, we follow prior work ([VALL-E [2]](../Speech_LLM/2023.01.05_VALL-E.md); [VoiceBox [1]](../Speech_LLM/2023.06.23_VoiceBox.md)) and consider a filtered subset of LS test-clean consisting of clips between four and ten seconds in length. 
> For speaker-prompted TTS, we utilize a 3 second clip from another sample of the same speaker.

### Baselines.

> For text-only synthesis, we compare against [VITS [27]](../E2E/2021.06.11_VITS.md), a variational autoencoder with adversarial training. 
> We consider both VITS variants: the single-speaker VITS-LJ trained on LJ Speech, and the multi-speaker VITS-VCTK trained on VCTK. 
> We also compare against English [MMS-TTS [28]](../_tmp/2023.05.22_MMS-TTS.md), a recent single-speaker model. 
> For speaker-prompted TTS, we compare against [YourTTS [29]](../E2E/2021.12.04_YourTTS.md), a VITS model conditioned on a speech prompt.
> We follow the evaluation protocol of the recent state-of-the-art generative models such as [VALL-E [2]](../Speech_LLM/2023.01.05_VALL-E.md) and [VoiceBox [1]](../Speech_LLM/2023.06.23_VoiceBox.md) and compare against their reported metrics. 
> We also compare against the speech-to-speech baselines [GSLM [30]](../Speech_LLM/2021.02.01_GSLM.md) and [AudioLM [31]](../Speech_LLM/2022.09.07_AudioLM.md).

### Evaluation Metrics. 

> To evaluate the intelligibility of the synthesized audio, we transcribe the speech with a pre-trained ASR model and compute the WER between the transcribed text and original transcript. 
> We use the [HuBERT [32]](../Speech_Representaion/2021.06.14_HuBERT.md)-L model employed by prior work ([VALL-E [2]](../Speech_LLM/2023.01.05_VALL-E.md); [VoiceBox [1]](../Speech_LLM/2023.06.23_VoiceBox.md)) ([HuggingFace](https://huggingface.co/facebook/hubert-large-ls960-ft)). 
> For speaker-prompted TTS, we evaluate the similarity between the prompt and synthesized speech by utilizing the pre-trained speaker verification model from prior work ([VALL-E [2]](../Speech_LLM/2023.01.05_VALL-E.md); [VoiceBox [1]](../Speech_LLM/2023.06.23_VoiceBox.md)). 
> The WavLM-Large model released at [Github](https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification).
> We report the cosine similarity between speaker embeddings for the re-synthesized prompt and synthesized speech.

## 5.Results: 结果

> Our results in \autoref{tab:text-only} demonstrate that our method can generate intelligible speech in a text-only setting, nearly matching the word error rate of the ground truth audio. 
> 
> Our text-only WER surpasses that of the single-speaker models while providing the additional capability of multi-speaker synthesis. 
> In the speaker-prompted setting, our model generates speech that maintains the characteristics of the prompt. 
> Notably, ***SESD*** outperforms the SoTA auto-regressive system, VALL-E, in terms of both the WER and the neural speaker similarity metric, with less than 2\% the training data. 
> We also match the performance of the latent diffusion NS2 system using 2.2\% of the training data. 
> 
> We demonstrate the importance of our various design decisions in \autoref{fig:ablation}. 
> Our position-aware cross attention mechanism, model architecture, text encoder, and diffusion loss weighting are critical for generating intelligible speech.

## 6.Conclusions: 结论

> We present ***SESD***, a highly sample-efficient latent diffusion framework for text-to-speech synthesis that achieves strong results in a modest data regime. 
> The key ingredients in the success of ***SESD*** are: a novel diffusion architecture that efficiently models long audio sequences, incorporating representations from a byte-level language model that capture linguistic properties critical for natural speech synthesis, and modifying the diffusion loss weighting to improve text-speech alignment.
> Together, these innovations enable ***SESD*** to perform speech synthesis directly from text without explicit phoneme alignment. 
> ***SESD*** generates intelligible speech near human-level word error rates with less than 1k hours of training data.
