# LatentSpeech

<details>
<summary>基本信息</summary>

- 标题: "LatentSpeech: Latent Diffusion for Text-To-Speech Generation"
- 作者:
  - 01 Haowei Lou (UNSW Sydney, Kensington, Australia)
  - 02 Helen Paik (UNSW Sydney, Kensington, Australia)
  - 03 Pari Delir Haghighi (UNSW Sydney, Kensington, Australia)
  - 04 Wen Hu (UNSW Sydney, Kensington, Australia)
  - 05 Lina Yao (UNSW Sydney, Kensington, Australia)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.08117)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2412.08117v1__LatentSpeech__Latent_Diffusion_for_Text-To-Speech_Generation.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Diffusion-based Generative AI gains significant attention for its superior performance over other generative techniques like Generative Adversarial Networks and Variational Autoencoders.
While it has achieved notable advancements in fields such as computer vision and natural language processing, their application in speech generation remains under-explored.
Mainstream Text-to-Speech systems primarily map outputs to Mel-Spectrograms in the spectral space, leading to high computational loads due to the sparsity of MelSpecs.
To address these limitations, we propose LatentSpeech, a novel TTS generation approach utilizing latent diffusion models.
By using latent embeddings as the intermediate representation, LatentSpeech reduces the target dimension to 5% of what is required for MelSpecs, simplifying the processing for the TTS encoder and vocoder and enabling efficient high-quality speech generation.
This study marks the first integration of latent diffusion models in TTS, enhancing the accuracy and naturalness of generated speech.
Experimental results on benchmark datasets demonstrate that LatentSpeech achieves a 25% improvement in Word Error Rate and a 24% improvement in Mel Cepstral Distortion compared to existing models, with further improvements rising to 49.5% and 26%, respectively, with additional training data.
These findings highlight the potential of LatentSpeech to advance the state-of-the-art in TTS technology

## 1·Introduction: 引言

Generative AI has made significant strides in recent years.
It revolutionises various fields with its ability to generate high-quality data.
Among numerous GAI techniques, diffusion-based generative models have garnered increased attention for their superior performance compared to other methods such as Generative Adversarial Networks~\cite{goodfellow2020generative} and Variational Autoencoders~\cite{kingma2013auto}.
Diffusion models demonstrate remarkable advancements in areas like image generation~\cite{rombach2022high}, large language models~\cite{ramesh2022hierarchical}, and video generation~\cite{ho2022imagen}.

Mainstream Text-to-Speech (TTS) systems, which convert linguistic context to speech using deep learning approaches, have explored the application of advanced deep learning techniques in speech generation.
For instance, Tacotron~\cite{wang2017tacotron} employs a sequence-to-sequence framework for speech generation, FastSpeech~\cite{ren2019fastspeech} uses a transformer architecture to enable parallel computation and address issues like word skipping, and StyleSpeech~\cite{lou2024stylespeechparameterefficientfinetuning} enhances phoneme and style embedding efficiency to improve speech quality.

One challenge for mainstream TTS methods is their reliance on MelSpec as an intermediate representation.
MelSpecs are characterized by high sparsity, which leads to significant computational and parameter demands to process the sparse content.
Each MelSpec represents the frequency content of a speech over time, resulting in a large and mostly empty matrix where only a few values carry significant information.
This sparsity requires models to allocate extensive computational resources and memory to process and store these large matrices.

There are methods that attempt to generate MelSpecs using diffusion models \cite{zhang2023survey}, and approaches like DiffVoice \cite{liu2023diffvoice} that employ latent diffusion with MelSpecs as an intermediate representation.
Some approaches, such as FastSpeech 2~\cite{ren2020fastspeech}, have explored direct speech generation without relying on MelSpec.
The potential of using latent embeddings directly in the audio space as the intermediate representation for TTS systems remains underexplored.

In this study, we propose LatentSpeech, a novel diffusion-based TTS framework that operates in the latent space.
Our method leverages the advantages of diffusion methods in capturing intricate details in latent embeddings.
It results in a more effective learning process, thereby enhancing the quality of generated speech.
The main contributions are:
1. LatentSpeech is the first approach to leverage latent diffusion in TTS for directly generating high-quality speech in the audio space.
Unlike other methods that apply latent diffusion on Mel-Spectrogram, LatentSpeech applies it directly on raw audio.
2. LatentSpeech reduces the intermediate representation dimension to 5\% of MelSpecs by using latent embeddings.
This reduction simplifies the processing for the TTS encoder and vocoder and enables efficient high-quality speech generation.
3. LatentSpeech achieves a 25\% improvement in Word Error Rate and a 24\% improvement in Mel Cepstral Distortion, with improvements rising to 49.5\% and 26\%, respectively, with more training data.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论