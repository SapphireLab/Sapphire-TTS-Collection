# LatentSpeech

<details>
<summary>基本信息</summary>

- 标题: "LatentSpeech: Latent Diffusion for Text-To-Speech Generation"
- 作者:
  - 01 Haowei Lou (UNSW Sydney, Kensington, Australia)
  - 02 Helen Paik (UNSW Sydney, Kensington, Australia)
  - 03 Pari Delir Haghighi (UNSW Sydney, Kensington, Australia)
  - 04 Wen Hu (UNSW Sydney, Kensington, Australia)
  - 05 Lina Yao (UNSW Sydney, Kensington, Australia)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.08117)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2412.08117v1__LatentSpeech__Latent_Diffusion_for_Text-To-Speech_Generation.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Diffusion-based Generative AI gains significant attention for its superior performance over other generative techniques like Generative Adversarial Networks and Variational Autoencoders.
While it has achieved notable advancements in fields such as computer vision and natural language processing, their application in speech generation remains under-explored.
Mainstream Text-to-Speech systems primarily map outputs to Mel-Spectrograms in the spectral space, leading to high computational loads due to the sparsity of MelSpecs.
To address these limitations, we propose LatentSpeech, a novel TTS generation approach utilizing latent diffusion models.
By using latent embeddings as the intermediate representation, LatentSpeech reduces the target dimension to 5% of what is required for MelSpecs, simplifying the processing for the TTS encoder and vocoder and enabling efficient high-quality speech generation.
This study marks the first integration of latent diffusion models in TTS, enhancing the accuracy and naturalness of generated speech.
Experimental results on benchmark datasets demonstrate that LatentSpeech achieves a 25% improvement in Word Error Rate and a 24% improvement in Mel Cepstral Distortion compared to existing models, with further improvements rising to 49.5% and 26%, respectively, with additional training data.
These findings highlight the potential of LatentSpeech to advance the state-of-the-art in TTS technology

## 1·Introduction: 引言

Generative AI has made significant strides in recent years.
It revolutionises various fields with its ability to generate high-quality data.
Among numerous GAI techniques, diffusion-based generative models have garnered increased attention for their superior performance compared to other methods such as Generative Adversarial Networks~\cite{goodfellow2020generative} and Variational Autoencoders~\cite{kingma2013auto}.
Diffusion models demonstrate remarkable advancements in areas like image generation~\cite{rombach2022high}, large language models~\cite{ramesh2022hierarchical}, and video generation~\cite{ho2022imagen}.

Mainstream Text-to-Speech (TTS) systems, which convert linguistic context to speech using deep learning approaches, have explored the application of advanced deep learning techniques in speech generation.
For instance, Tacotron~\cite{wang2017tacotron} employs a sequence-to-sequence framework for speech generation, FastSpeech~\cite{ren2019fastspeech} uses a transformer architecture to enable parallel computation and address issues like word skipping, and StyleSpeech~\cite{lou2024stylespeechparameterefficientfinetuning} enhances phoneme and style embedding efficiency to improve speech quality.

One challenge for mainstream TTS methods is their reliance on MelSpec as an intermediate representation.
MelSpecs are characterized by high sparsity, which leads to significant computational and parameter demands to process the sparse content.
Each MelSpec represents the frequency content of a speech over time, resulting in a large and mostly empty matrix where only a few values carry significant information.
This sparsity requires models to allocate extensive computational resources and memory to process and store these large matrices.

There are methods that attempt to generate MelSpecs using diffusion models \cite{zhang2023survey}, and approaches like DiffVoice \cite{liu2023diffvoice} that employ latent diffusion with MelSpecs as an intermediate representation.
Some approaches, such as FastSpeech 2~\cite{ren2020fastspeech}, have explored direct speech generation without relying on MelSpec.
The potential of using latent embeddings directly in the audio space as the intermediate representation for TTS systems remains underexplored.

In this study, we propose LatentSpeech, a novel diffusion-based TTS framework that operates in the latent space.
Our method leverages the advantages of diffusion methods in capturing intricate details in latent embeddings.
It results in a more effective learning process, thereby enhancing the quality of generated speech.
The main contributions are:
1. LatentSpeech is the first approach to leverage latent diffusion in TTS for directly generating high-quality speech in the audio space.
Unlike other methods that apply latent diffusion on Mel-Spectrogram, LatentSpeech applies it directly on raw audio.
2. LatentSpeech reduces the intermediate representation dimension to 5\% of MelSpecs by using latent embeddings.
This reduction simplifies the processing for the TTS encoder and vocoder and enables efficient high-quality speech generation.
3. LatentSpeech achieves a 25\% improvement in Word Error Rate and a 24\% improvement in Mel Cepstral Distortion, with improvements rising to 49.5\% and 26\%, respectively, with more training data.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

In this section, we introduce the architecture of LatentSpeech.
We first encode speech~$A$ into latent space using an Autoencoder~(AE).
Then, we set latent embeddings as the intermediate representation~$Z$ and train a diffusion-based TTS model to map embeddings.
In the end, we generate speech directly from the latent space to the audio space using the trained decoder.
An overview of the entire system is provided in Figure~\ref{fig:latent_speech}.

### A·Latent Encoder: 隐变量编码器

To lower the computation demand of training TTS system and sparsity of intermediate representation.

We follow a similar training setup to RAVE~\cite{caillon2021rave} to train an Autoencoder to encode speech from audio space to latent space.
Specifically, given a raw waveform~$A \in \mathbb{R}^{L_{audio}}$ where $L_{audio}$ is the number of time points in the speech.
We first apply a multi-band decomposition to the raw speech using Pseudo Quadrature Mirror Filters (PQMF)~\cite{nguyen1994near}.

$$
\mathbf{PQMF}(A) = \mathbb{R}^{N \times L_{sub}}, \, L_{audio}=N \times L_{sub}
$$

$N$ is the number of frequency sub-bands and $L_{sub}$ is the number of time points in each sub-band.
An encoder is applied~$\mathbf{E}(\cdot)$ to encode $\mathbf{PQMF}(A)$ into latent space~$Z \in \mathbb{R}^{N \times L_{\text{latent}}}$.
Here, $N$ denotes the number of channels $L_{\text{latent}}$ represents the latent space temporal resolution.

The latent embeddings are passed into a decoder~$\mathbf{D}(\cdot)$ to reconstruct $\mathbf{PQMF}(A)$, yielding $\mathbf{D}(Z)$.
The resultant multi-band speech is then processed using the inverse PQMF function to produce the reconstructed speech, \(A' = \mathbf{PQMF}^{-1}(\mathbf{D}(Z))\).

We use the multiscale spectral distance in the multi-band speech as the loss function~\cite{engel2020ddsp} to train the encoder and decoder.
$N$ and $L$ will be used in the following sections to denote the number of channels and time resolution in the latent space.

### B·Text-to-Speech Encoder: TTS 编码器

TTS encoder transforms linguistic inputs to TTS embedding, which serves as conditions for the diffusion model to map latent embedding.
In this work, we adopt the transformer-based TTS system StyleSpeech~\cite{lou2024stylespeechparameterefficientfinetuning} as our TTS encoder.
It includes the following key components: an acoustic pattern encoder, a duration adapter, and an integration encoder, each consisting of multiple layers of Feed-Forward Transformers (FFT Blocks)~\cite{ren2019fastspeech}.

Given sequences of phonemes~\(P\) and styles~\(S\) linguistic input, the Acoustic Pattern Encoder (APE) transforms input text into sequences of phoneme \(H_P = (h_{P1}, \ldots, h_{Pn})\) and style embeddings \(H_S = (h_{S1}, \ldots, h_{Sn})\).
The phoneme and style embeddings are fused to produce acoustic embedding \(H = H_P + H_S\).
The Duration Adapter controls the duration of acoustic embeddings to align acoustic embedding with real speech.
It has two main components: the \textit{duration predictor} and the \textit{length regulator}.
The duration predictor estimates the duration of each acoustic feature~\(L = \{l_1, \ldots, l_n\}\), where \(m = \sum_{i=0}^{N} l_i\).
These durations adjust the length of each acoustic embedding to the adaptive embedding~\(H_L = \{h_{l1}, \ldots, h_{lm}\}\).
The adaptive embeddings~\(H_L\) are then passed through the embedding generator to generate the TTS embedding~\(H_{TTS}\).
This is followed by a linear layer to broadcast~\(H_{TTS}\) to the dimensions of the latent embedding~\(Z\).

### C·Latent Diffusion: 隐变量扩散

Diffusion model is a probabilistic generative model that learns to produce data that match latent embedding distribution~$p(Z)$, by denoising a normally distributed variable through a reverse Markov Chain of length~$T$.
We define $q(Z_0)$ as the data distribution of the latent embedding $Z \in \mathbb{R}^{N \times L}$.
Let $Z_t \in \mathbb{R}^{N \times L}$ for $t = 0, 1, \ldots, T$ represent the forward diffusion process:

$$
    q(Z_{1:T} | Z_0) = \prod_{t=1}^{T} q(Z_t | Z_{t-1})
$$

where Gaussian noise~$\mathcal{N}(\cdot)$ is gradually added to the Markov chain from $Z_0$ to $Z_T$ until $q(Z_T) \sim \mathcal{N}(0,I)$.

$$
    q(Z_t | Z_{t-1}) = \mathcal{N}(Z_t;\sqrt{\alpha_t}Z_{t-1}, (1-\alpha_t)\mathbf{I})
$$

Here, $\alpha_t$ refers to the scaling factor that controls the amount of noise added at diffusion step $t$.
Then, we apply a conditional denoiser~$P_\theta(\cdot)$ parameterized by~$\theta$ to reverse the diffusion process and gradually reconstruct the original latent embeddings~$Z_0$ from the noisy latent embeddings~$Z_T$, as illustrated in Figure~\ref{fig:conditional_denoiser}.

$$
    p_{\theta}(Z_0 | Z_{T:1}) = \prod_{t=1}^{T} p_\theta(Z_{t-1}|Z_{t},t_{embed},H_{TTS})
$$

Specifically, we apply a 128-dimensional positional encoding~\cite{vaswani2017attention} at diffusion step~$t$ to represent the diffusion-step embedding~$t_{embed}$.

$t_{embed}$ is broadcasted to the $L$-dimension, $t_{embed} \in \mathbb{R}^{L}$, to match the temporal resolution of latent embedding~$Z$.
The TTS embedding~$H_{TTS}$, obtained in Section~\ref{sec:tts_encode}, serves as a conditional input for the denoiser to guide the reverse diffusion process.
The dimension of $H_{TTS}$ is as same as the latent embedding dimension~$Z \in \mathbb{R}^{N \times L}$.
The denoiser is constructed using several layers of residual blocks built with bidirectional dilated convolution kernels, similar to those applied in diffusion-based neural vocoders \cite{kong2020diffwave}.
More details on the architecture of the denoiser and the residual blocks can be found in Figures~\ref{fig:conditional_denoiser} and \ref{fig:residual_block} respectively.

#### Training: 训练

In the training stage, we define the transition probability $p_\theta(Z_{t-1}|Z_{t})$ is parameterized as

$$
\mathcal{N}(Z_{t-1};\mu_{\theta}(Z_{t},t), \sigma_{\theta}(Z_{t},t)^2I)
$$

$\mu_\theta$ is the mean embedding and $\sigma_\theta$ is a real number as the standard deviation.
We follow a closed-form diffusion model calculation method proposed in \cite{ho2020denoising} to accelerate computation and avoid Monte Carlo estimates. Specifically, we first define the variance schedule~${\beta}_{t=1}^{T}$:

$$
\begin{aligned}
    \alpha_t = 1 - \beta_t, \quad \hat{\alpha}_{t} = \prod_{s=1}^{t}\alpha_s, \quad \\
    \hat{\beta_{t}} = \frac{1-\hat{\alpha}_{t-1}}{1-\hat{\alpha}_{t}}\beta_{t}, \quad t > 1, \quad \hat{\beta_{1}} = \beta_1
\end{aligned}
$$

Then, the parameterizations of $\mu_\theta$ and $\sigma_\theta$ are defined by:

$$
    \mu_{\theta}(\cdot) = \frac{1}{\sqrt{\alpha_t}}(Z_t - \frac{\beta_t}{\sqrt{1-\hat{\alpha}}}f_{\theta}(Z_{t},t,H_{TTS})),\, \sigma_{\theta}(\cdot) = \sqrt{\beta_{t}}
$$

Here, $f_{\theta}(Z_{t},t,H_{TTS})$ is our proposed conditional denoiser r, which takes the diffusion step embedding~$t_{embed}$ and TTS embedding~$H_{TTS}$ as conditional inputs to predict the noise $\epsilon_t$ added in the forward diffusion process at step $t$. The training objective is to optimize the parameters to reduce the following loss function:

$$
    L = \mathbb{E}_{Z_0,\epsilon,t,H_{TTS}} \left\| \epsilon - f_\theta(Z_t, t, H_{TTS}) \right\|^2
$$

#### Inference: 推理

In the inference stage, we sample $Z_T \sim \mathcal{N}(0,I)$. We use the trained denoiser~$f_{\theta}({\cdot})$ predicts the noise~$\epsilon_{t}$ added to the latent embeddings at $t$ for $t=T,T-1,\dots,1$. This noise is iteratively subtracted from~$Z_T$ until the latent embedding~$Z_0$ is reconstructed.
$$
    \epsilon_{t} = f_{\theta}(Z_{t-1},t_{embed},H_{TTS})
$$

### D·Vocoder: 声码器

The trained decoder $\mathbf{D}(\cdot)$ described in Section~\ref{sec:lfe} serves as a vocoder to reconstruct speech using the latent embeddings produced by the diffusion denoising process outlined in Section~\ref{sec:ld}. Specifically, the denoised latent embeddings \(Z \in \mathbb{R}^{N \times T}\) are input into the decoder $\mathbf{D}(\cdot)$. The decoder converts these features back into multi-band speech, which is then processed using the inverse PQMF function, $\mathbf{PQMF}^{-1}(\cdot)$. This function combines the sub-band speech signals back into a single speech waveform to generate the final reconstructed speech signal \(A'\).

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论