# SimpleSpeech

<details>
<summary>基本信息</summary>

- 标题: "Title"
- 作者:
  - 01 Dongchao Yang - CUHK
  - 02 Dingdong Wang - CUHK
  - 03 Haohan Guo - CUHK
  - 04 Xueyuan Chen - CUHK
  - 05 Xixin Wu - CUHK
  - 06 Helen Meng - CUHK
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.02328)
  - [Publication](https://doi.org/10.21437/Interspeech.2024-1392) InterSpeech2024
  - [Github](https://github.com/yangdongchao/SimpleSpeech)
  - [Demo](https://simplespeech.github.io/simplespeechDemo/)
- 文件:
  - [ArXiv](_PDF/2406.02328v3__SimpleSpeech__Towards_Simple_&_Efficient_TTS_with_Scalar_Latent_Transformer_Diffusion_Models.pdf)
  - [Publication](_PDF/2406.02328p0__SimpleSpeech__InterSpeech2024.pdf)

</details>

## Abstract·摘要

In this study,we propose a simple and efficient **Non-AutoRegressive (NAR)** **Text-To-Speech (TTS)** system based on diffusion, named ***SimpleSpeech***.
Its simpleness shows in three aspects:

1. It can be trained on the speech-only dataset, without any alignment information;
2. It directly takes plain text as input and generates speech through an NAR way;
3. It tries to model speech in a finite and compact latent space, which alleviates the modeling difficulty of diffusion.

More specifically, we propose a novel speech codec model (***SQ-Codec***) with scalar quantization, ***SQ-Codec*** effectively maps the complex speech signal into a finite and compact latent space, named scalar latent space.
Benefits from ***SQ-Codec***, we apply a novel Transformer diffusion model in the scalar latent space of ***SQ-Codec***.
We train ***SimpleSpeech*** on 4k hours of a speech-only dataset, it shows natural prosody and voice cloning ability.
Compared with previous large-scale TTS models, it presents significant speech quality and generation speed improvement.
Demos are released at this [URL](https://simplespeech.github.io/simplespeechDemo/).

## 1.Introduction·引言

Text-to-speech synthesis (TTS) aims to synthesize intelligible and natural speech given text, which has made great progress in the past years [1, 2].
Most previous TTS systems trained on small-scale high-quality labeled speech datasets.
In terms of model training, they rely on a relatively complicated pipeline (e.g.prosody prediction, G2P conversion), and fine-grained alignment information (e.g. phone-level duration) is needed.
However, the recently proposed approaches based on large-scale speech data significantly simplify the TTS system ([AudioLM](../../Models/Speech_LLM/2022.09.07_AudioLM.md), [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md), [SPEAR-TTS](../../Models/Speech_LLM/2023.02.07_SPEAR-TTS.md), [UniAudio](../../Models/Speech_LLM/2023.10.01_UniAudio.md), [Make-A-Voice](../Speech_LLM/2023.05.30_Make-A-Voice.md)).
For instance, language model (LM) based TTS, e.g. [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md) uses a pre-trained audio codec model e.g. [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md) to map the speech signal into a sequence of discrete tokens, then an auto-regressive language model is trained to translate the phoneme into speech tokens.
It can generate more expressive speech, but also is troubled by the slow and unstable inference.
To address these issues, Non-autoregressive (NAR) models, e.g. [NaturalSpeech 2](../../Models/Diffusion/2023.04.18_NaturalSpeech2.md), [SoundStorm](../../Models/Speech_LLM/2023.05.16_SoundStorm.md), and [VoiceBox](../../Models/Speech_LLM/2023.06.23_VoiceBox.md), are proposed.
They have higher inference speed and better stability, but with the cost of (1) relying on phoneme-acoustic alignment information, and (2) a more complicated training process.
In this work, we propose a simple and efficient TTS system, ***SimpleSpeech***, which does not rely on any alignment information and generates high-quality speech in a NAR way.2To build such a TTS system, where we meet with the following research problems:

- (1) how to use the large-scale speech-only dataset to train a TTS model without any alignment information e.g. phoneme-level duration;
- (2) how to design a generative model that can generate high-quality speech in a NAR way.
- (3) how to solve the duration alignment problem without using a specific duration model when we train the NAR model;

The main contributions of this study are summarized as follows:

- (1) We demonstrate that large-scale unlabeled speech data can be used to build an NAR TTS system.
- (2) We propose a novel generation model, a scalar latent transformer diffusion model, which models the speech data in a finite and compact latent space.
More specifically, we propose a speech codec model (SQ-Codec) based on scalar quantization, which maps the complex speech signal into a finite and compact latent space, named scalar latent space.
Then we apply the diffusion model in the scalar latent space of ***SQ-Codec***.
- (3) We propose to use sentence duration instead of phone-level duration for the NAR-based TTS model.
The sentence duration is used to determine the length of the target sequence, then an in-context conditioning strategy is introduced to learn the fine-grained alignment between the condition and target sequence implicitly.
Compared to previous works that predict the duration of each phoneme, the sentence duration gives more diversity in the generation process.
More importantly, the sentence duration is easy to obtain in both the training and inference stages.
Please refer to Section 3.2 finds more details.
- (4) Extensive experimental results show the effectiveness of ***SimpleSpeech***.
We also conduct a lot of ablation studies to explore the effectiveness of each part in ***SimpleSpeech***.

## 2.Related Works·相关工作

`None`

## 3.Methodology·方法论

The overall architecture of the ***SimpleSpeech*** framework is demonstrated in Figure 1 (b), which mainly consists of two parts: ***SQ-Codec*** and scalar transformer diffusion.
The detailed design of each part will be introduced in this section.

### 3.1.Text Encoder & Speaker Encoder

This work explores to use the large-scale speech-only datasets to train a TTS system.
The first step is to obtain the text label for these speech samples.
We propose to use the open available ASR model to get transcripts, Whisper-base model [12] is used in this study.
To take advantage of the large language model and simply the traditional TTS frontend, we use a pre-trained language model to extract the textual representations.
Then we directly take these textual representations as the conditional information for TTS.
To realize zero-shot voice cloning, we use the 1st layer of XLSR-53 [13] to extract global embedding to represent the speaker timbre.

### 3.2.Sentence Duration

Previous works [2, 9] try to model the phone-level duration in TTS.
In general, a duration predictor is built to predict the duration of each phone.
Training such modules increases the complexity of data pre-processing and training pipeline.In this study, we propose to model the sentence-level duration prior by using the in-context learning of LLMs (gpt-3.5-turbo is used).
Our motivation is that LLMs can easily estimate how much time to read a sentence based on the number of words in the sentence and the prior knowledge.
The prompt for ChatGPT can be found on the demo page.
After we obtain the sentence-level duration, we let the model learn the alignment between words and latent features implicitly.
Such a design will bring more diversity to speech synthesis.
In the training stage, we can directly get the duration based on the length of the waveform.
We follow Stable
Audio [14] uses a timing module to encode the duration into a global embedding.
In the inference stage, the predicted duration by LLMs first determines the length of the noisy sequence, then input into the timing module.

### 3.3.SQ-Codec

Although residual vector quantization (RVQ) based audio codec models have shown effectiveness for audio compression, training a good codec model needs a lot of tricks and complicated loss design [15].
In this study, we propose to use scalar quantization [16, 17] to replace the residual vector quantization in audio codec models, which can be trained with the reconstruction loss and adversarial loss without any training tricks.
Furthermore, we also find that the scalar quantization effectively maps the complex speech signal into a finite and compact latent space, which is suitable for diffusion model (refer to Section 2.4 and 3.2.4 for more details).
Assuming h ∈ RT ∗ddenotes the output features of Encoder in the codec model.
T and d denote the number of frames and the dimension of each vector.
For any vector hi, we use a parameter-free scalar quantization module to quantize hiinto a fixed scalar space: hi= torch.tanh(hi),si= torch.round(hi∗ S)/S,(1) where S is a hyper-parameter that determines the scope of scalar space.
To get gradients through the rounding operation, we use a straight-through estimator like VQ-VAE [18].
We can see that the scalar quantization first uses a tanh function to map the value of features into [−1, 1], then a round operation further reduces the value of range into 2*S+1 different numbers.
We named such value domain as scalar latent space.
We note that previous works [17, 19] also try to use scalar quantization as the image tokenizer for image generation.
We claim that our implementation is different from theirs, and better adapts audio codec tasks in our experiments.

#### Encoder and Decoder
Our encoder consists of 5 convolution blocks, each block includes 2 causal 1D-convolutional layers and one down-sample layer.
The down-sample strides are set as [2, 2, 4, 4, 5], resulting in 320 times down-sample along the time dimension.
The decoder mirrors the encoder and uses transposed convolutions instead of stride convolutions.

#### Discriminator and Training Loss
Following [6], a multi-scale discriminator is used.
The training loss of ***SQ-Codec*** consists of two parts: (1) reconstruction loss Lrec, which includes time domain and frequency domain losses: the L1 loss between the reconstructed waveform and the original waveform and the MSE loss for the STFT spectrogram. (2) adversarial loss, which is calculated based on the results of the discriminator.

### 3.4.Scalar Latent Transformer Diffusion Models

Based on the previous discussion, we can map the speech data into a scalar latent space based on our proposed ***SQ-Codec*** model.
Inspired by the success of latent diffusion models [20] in both image and audio generation [9,21], we propose to model the speech data in the scalar latent space.
Our motivation is that the sampling space of scalar latent space is simple because SQ effectively limits the value range of each element.
The details of the scalar latent transformer diffusion model are as follows.

#### Transformer-Based Diffusion Backbone
U-Net backbone has been widely used in diffusion models, especially in the speech synthesis field [9,22–24].
We also noted that many transformer-based diffusion models, such as DiT [25] and Sora [26] have been used in image/video generation.
Inspired by the success of the transformer-based audio language models [4–6] and DiT, we propose a transformer-based diffusion backbone for speech synthesis.
Specifically, a GPT2-like transformer backbone is used: 12 attention layers, 8 attention heads, and the model dimension is 768.

#### In-Context Conditioning
Inspired by LLMs and previous audio language models [4–6], we simply append the features of time step t and condition c as the prefix sequence in the input sequence.
Such a condition way allows us to use a standard GPT-like structure without modification.
After the final block, we remove the conditioning sequence from the output sequence.

#### Scalar Latent Diffusion
Latent diffusion models (LDM) have been demonstrated to fit complex data distributions, including VAE latent features [9, 20–22], Mel-spectrogram features [27], and waveform [23, 24].
We speculate that these data distributions are very complex because their search space is infinite.
Instead, ***SQ-Codec*** provides a finite and compact scalar latent space, thus we can consider modeling speech data in this space.
Specifically, a network is trained to transfer the Gaussian distribution to the scalar latent space.
We follow the training strategy of DDPM [28], and the mean squared error (MSE) loss is used.
To make sure the final output belongs to the scalar latent space, we use the scalar quantization (SQ) operation to limit the final prediction.

$$
$$

where θ denotes the parameter of the neural network.
T denotes the timestep, xTdenotes the sampling features from the Gaussian distribution. c denotes the condition information.

## 4.Experiments·实验

## 5.Conclusion·结论

In this study, we propose a simple and efficient TTS model named ***SimpleSpeech***.
***SimpleSpeech*** can be trained on large-scale speech-only datasets without any additional data pre-processing, which significantly simplifies the efforts to train a TTS model.We propose a novel speech codec (SQ-Codec) based on scalar quantization.Then we apply a novel latent transformer diffusion model to the scalar latent space of ***SQ-Codec***.
Experimental results show the proposed model has better performance than the previous U-Net-based latent diffusion model.
Due to the NAR generation strategy, ***SimpleSpeech*** significantly improves the generation efficiency compared to previous LM-based TTS models.
In the future, we will explore to scale the model and data size.