# DualSpeech

<details>
<summary>基本信息</summary>

- 标题: "DualSpeech: Enhancing Speaker-Fidelity and Text-Intelligibility Through Dual Classifier-Free Guidance"
- 作者:
  - 01 Jinhyeok Yang,
  - 02 Junhyeok Lee,
  - 03 Hyeong-Seok Choi,
  - 04 Seunghun Ji,
  - 05 Hyeongju Kim,
  - 06 Juheon Lee
- 链接:
  - [ArXiv](https://arxiv.org/abs/2408.14423)
  - [Publication](https://doi.org/10.21437/Interspeech.2024-2005)
  - [Github]
  - [Demo](https://bit.ly/48Ewoib)
- 文件:
  - [ArXiv](_PDF/2408.14423v2__DualSpeech__Enhancing_Speaker-Fidelity_&_Text-Intelligibility_through_Dual_Classifer-Free_Guidance.pdf)
  - [Publication](_PDF/2408.14423p0__DualSpeech__InterSpeech2024.pdf)

</details>

## Abstract: 摘要

Text-to-Speech (TTS) models have advanced significantly, aiming to accurately replicate human speech's diversity, including unique speaker identities and linguistic nuances.
Despite these advancements, achieving an optimal balance between speaker-fidelity and text-intelligibility remains a challenge, particularly when diverse control demands are considered.
Addressing this, we introduce DualSpeech, a TTS model that integrates phoneme-level latent diffusion with dual classifier-free guidance.
This approach enables exceptional control over speaker-fidelity and text-intelligibility.
Experimental results demonstrate that by utilizing the sophisticated control, DualSpeech surpasses existing state-of-the-art TTS models in performance.
Demos are available at https://bit.ly/48Ewoib.

## 1.Introduction: 引言

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
