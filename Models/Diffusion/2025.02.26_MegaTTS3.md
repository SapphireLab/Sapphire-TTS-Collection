# MegaTTS3

<details>
<summary>基本信息</summary>

- 标题: "MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot Speech Synthesis"
- 作者:
  - 01 Ziyue Jiang
  - 02 Yi Ren
  - 03 Ruiqi Li
  - 04 Shengpeng Ji
  - 05 Boyang Zhang
  - 06 Zhenhui Ye
  - 07 Chen Zhang
  - 08 Bai Jionghao
  - 09 Xiaoda Yang
  - 10 Jialong Zuo
  - 11 Yu Zhang
  - 12 Rui Liu
  - 13 Xiang Yin
  - 14 Zhou Zhao
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.18924)
  - [Publication]()
  - [Github]()
  - [Demo](https://sditdemo.github.io/sditdemo/)
- 文件:
  - [ArXiv](_PDF/2502.18924v4__MegaTTS3__Sparse_Alignment_Enhanced_Latent_Diffusion_Transformer_for_Zero-Shot_Speech_Synthesis.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

While recent zero-shot text-to-speech (TTS) models have significantly improved speech quality and expressiveness,
mainstream systems still suffer from issues related to speech-text alignment modeling:
(1) models without explicit speech-text alignment modeling exhibit less robustness, especially for hard sentences in practical applications;
(2) predefined alignment-based models suffer from naturalness constraints of forced alignments.

This paper introduces ***MegaTTS3***, a TTS system featuring an innovative sparse alignment algorithm that guides the latent diffusion transformer (DiT).
Specifically, we provide sparse alignment boundaries to MegaTTS 3 to reduce the difficulty of alignment without limiting the search space, thereby achieving high naturalness.
Moreover, we employ a multi-condition classifier-free guidance strategy for accent intensity adjustment and adopt the piecewise rectified flow technique to accelerate the generation process.
Experiments demonstrate that MegaTTS 3 achieves state-of-the-art zero-shot TTS speech quality and supports highly flexible control over accent intensity.
Notably, our system can generate high-quality one-minute speech with only 8 sampling steps.
Audio samples are available at https://sditdemo.github.io/sditdemo/.

</td><td>

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

In recent years, neural codec language models~\citep{wang2023neural,zhang2023speak,song2024ella,xin2024rall} and large-scale diffusion models ~\citep{shen2023naturalspeech,le2023Voicebox,lee2024ditto,eskimez2024e2,ju2024naturalspeech,yang2024simplespeech,yang2024simplespeech2} have brought considerable advancements to the field of speech synthesis.
Unlike traditional text-to-speech (TTS) systems~\citep{shen2018natural,jia2018transfer,li2019neural,kim2020glow,ren2019fastspeech,kim2021conditional,kim2022guided}, these models are trained on large-scale, multi-domain speech corpora, which contributes to notable improvements in the naturalness and expressiveness of synthesized audio.
Given only seconds of speech prompt, they can synthesize identity-preserving speech in a zero-shot manner.

To generate high-quality speech with clear and expressive pronunciation, a TTS model must establish an alignment mapping from text to speech signals~\citep{kim2020glow,tan2021survey}.
However, from the perspective of speech-text alignment, current solutions suffer from the following issues:

- \textbf{Models with implicit speech-text alignment} achieve the soft alignment paths through attention mechanisms~\citep{wang2023neural,chen2024vall,du2024cosyvoice}.
These models can be categorized into: 1) autoregressive codec language models (AR LM), which are inefficient and lack robustness.
The lengthy discrete speech codes, which typically require a bit rate of 1.5 kbps~\citep{kumar2024high,wu2024towards}, impose a significant burden on these autoregressive language models; 2) diffusion-based models without explicit duration modeling~\citep{lee2024ditto,eskimez2024e2,lovelace2023simple,gao2023e3,cambara2024mapache,yang2024simplespeech,yang2024simplespeech2}, which significantly speeds up the speech generation process.
However, when compared with methods that adopt forced alignment, these models exhibit a decline in speech intelligibility.
Besides, these methods cannot provide fine-grained control over the duration of specific pronunciations and can only adjust the overall speech rate.

- \textbf{Predefined alignment-based methods} have prosodic naturalness constraints of forced alignments.
During training, alignment paths~\citep{ren2020fastspeech,kim2020glow} are directly introduced into their models~\citep{le2023Voicebox,shen2023naturalspeech,ju2024naturalspeech} to reduce the complexity of text-to-speech generation, which achieves higher intelligibility.
Nevertheless, they suffer from the following limitations: 1) predefined alignments constrain the model's search space to produce more natural-sounding speech~\citep{anastassiou2024seed,chen2024vall}; 2) the overall naturalness is highly dependent on the performance of duration models.

Intuitively, we can integrate the two aforementioned diffusion-based methods to pursue optimal performance.
To be specific, we propose a novel sparse speech-text alignment strategy to enhance the latent diffusion transformer (DiT), termed MegaTTS 3.
In our approach, phoneme tokens are sparsely distributed within the corresponding forced alignment regions to provide coarse pronunciation information that is then refined by the latent DiT model.
Experimental results demonstrate that MegaTTS 3 achieves nearly state-of-the-art speech intelligibility and speaker similarity on the LibriSpeech test-clean set~\citep{panayotov2015librispeech} with only 8 sampling steps, while also exhibiting high speech naturalness.
The main contributions of this work are summarized as follows:

- We design a sparse alignment enhanced latent diffusion transformer model, which effectively integrates the strengths of the two aforementioned speech-text alignment approaches.
Notably, our model also demonstrates greater robustness to duration prediction errors compared to methods with forced alignment.

- To achieve higher generation quality and more flexible control, we propose a multi-condition CFG strategy to adjust the guidance scales for speaker timbre and text content separately.
Furthermore, we discover that the text guidance scale can also be used to modulate the intensity of personal accents, offering a new direction for enhancing speech expressiveness.

- We successfully reduce the inference steps from 25 to 8 with the piecewise rectified flow (PeRFLow) technique, achieving highly efficient zero-shot TTS with minimal quality degradation.
We also visualize the attention matrices across various layers of MegaTTS 3 and obtain insightful findings in Appendix~\ref{app:vis_diff_attn}.

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

**Zero-Shot TTS**

Zero-shot TTS~\citep{casanova2022yourtts,wang2023neural,zhang2023speak,shen2023naturalspeech,le2023Voicebox,jiang2024mega,liu2024autoregressive,lee2024ditto,li2024styletts,lee2023hierspeech++,ju2024naturalspeech,meng2024autoregressive,chen2024f5} aims to synthesize unseen voices with speech prompts.
Among them, neural codec language models~\citep{chen2024vall} are the first that can autoregressively synthesize speech that rivals human recordings in naturalness and expressiveness.
However, they still face several challenges, such as the lossy compression in discrete audio tokenization and the time-consuming nature of autoregressive generation.
To address these issues, some subsequent works explore solutions based on continuous vectors and non-autoregressive diffusion models~\citep{shen2023naturalspeech,le2023Voicebox,lee2024ditto,eskimez2024e2,yang2024simplespeech,yang2024simplespeech2,chen2024f5}.
These works can be categorized into two main types: 1) the first type directly models speech-text alignments using attention mechanisms without explicit duration modeling~\citep{lee2024ditto,eskimez2024e2}.
Although these models perform well in terms of generation speed and quality, their robustness, especially in challenging cases, still requires enhancement.
The second category~\citep{shen2023naturalspeech,le2023Voicebox} utilizes predefined alignments to simplify alignment learning.
However, the search space of the generated speech of these models is limited by predefined alignments.
To address these limitations, we propose a sparse alignment mechanism to reduce the constraints of predefined alignment-based methods while also reducing the difficulty of speech-text alignment learning.

</td><td>

</td></tr>
<tr><td>

**Accented TTS**

While accented TTS is not yet mainstream in the field of speech synthesis, it offers valuable potential for customized TTS services, by enhancing the expressiveness of speech synthesis systems and improving listeners' comprehension of speech content~\citep{tan2021survey,melechovsky2022accented,badlani2023multilingual,zhou2024multi,shah2024parrottts,ma2023accent,inoue2024macst,zhong2024accentbox}.
With the emergence of conversational AI systems, accented TTS technology has even broader application scenarios.
In this paper, we focus on a specific task of accented TTS: adjusting the accent intensity of speakers to make them sound like native English speakers or accented speakers who use English as a second language~\citep{liu2024controllable}.
Unlike previous work, our approach does not require paired data or accurate accent labels; instead, it allows for flexible control over the accent intensity using the proposed multi-condition CFG mechanism.
In addition, we describe the CFG mechanism used in zero-shot TTS systems in Appendix~\ref{app:CFG_in_zs_tts}.

</td><td>

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

This section introduces MegaTTS 3.
To begin with, we describe the architecture design of MegaTTS 3.
Then, we provide detailed explanations of the sparse alignment mechanism, the piecewise rectified flow acceleration technique, and the multi-condition classifier-free guidance strategy.

</td><td>

</td></tr></table>

### Architecture

<table><tr><td width="50%">

**WaveVAE**

As shown in Figure~\ref{fig:arch_overview} (a), given a speech waveform $s$, the VAE encoder $E$ encodes $s$ into a latent vector $z$, and the wave decoder $D$ reconstructs the waveform $x = D(z) = D(E(s))$.
To reduce the computational burden of the model and simplify speech-text alignment learning, the encoder $E$ downsamples the waveform by a factor of $d$ in length.
The encoder $E$ is similar to the one used in~\citet{ji2024wavtokenizer}, and the decoder $D$ is based on~\citet{kong2020hifi}.
We also adopt the multi-period discriminator (MPD), multi-scale discriminator (MSD), and multi-resolution discriminator (MRD) ~\citep{kong2020hifi,jang2021univnet} to model the high-frequency details in waveforms, which ensure perceptually high-quality reconstructions.
The training loss of the speech compression model can be formulated as $\mathcal{L} = \mathcal{L}_{\mathrm{rec}} + \mathcal{L}_{\mathrm{KL}} + \mathcal{L}_{\mathrm{Adv}}$, where $\mathcal{L}_{\mathrm{rec}}=\|s-\hat{s}\|^2$ is the spectrogram reconstruction loss, $\mathcal{L}_{\mathrm{KL}}$ is the slight KL-penalty loss~\citep{rombach2022high}, and $\mathcal{L}_{\mathrm{Adv}}$ is the LSGAN-styled adversarial loss~\citep{mao2017least}.
After training, a one-second speech clip can be encoded into 25 vector frames.
For more details, please refer to Appendix~\ref{app:model_config} and~\ref{app:evaluation_speech_compression}.

</td><td>

</td></tr>
<tr><td>

**Latent Diffusion Transformer with Masked Speech Modeling**

The latent diffusion transformer is used to predict speech that matches the style of the given speaker and the content of the provided text.
Given the random variables $Z_{0}$ sampled from a standard Gaussian distribution $\pi_{0}$ and $Z_{1}$ sampled from the latent space given by the speech compression model with data density $\pi_{1}$, we adopt the rectified flow~\citet{liu2022flow} to implicitly learn the transport map $T$, which yields $Z_{1} := T(Z_{0})$.
The rectified flow learns $T$ by constructing the following ordinary differential equation (ODE):
$$
    \mathrm{d}Z_t = v(Z_t, t)\,\mathrm{d}t,
$$
where $t\in[0,1]$ denotes time and $v$ is the drift force.
Equation~\ref{eq:1} converts $Z_{0}$ from $\pi_{0}$ to $Z_{1}$ from $\pi_{1}$.
The drift force $v$ drives the flow to follow the direction $(Z_{1}-Z_{0})$.
The latent diffusion transformer,  parameterized by $\theta$, can be trained by estimating $v(Z_{t}, t)$ with $v_{\theta}(Z_{t}, t)$ through minimizing the least squares loss with respect to the line directions $(Z_{1}-Z_{0})$:
$$
    \min_v \int_0^1 \mathbb{E} \left[ \| (Z_1 - Z_0) - v(Z_t, t) \|^2 \right] \, \mathrm{d}t.
$$
We use the standard transformer block from LLAMA~\citep{dubey2024llama} as the basic structure for MegaTTS 3 and adopt the Rotary Position Embedding (RoPE)~\citep{su2024roformer} as the positional embedding.
During training, we randomly divide the latent vector sequence into a prompt region $z_{prompt}$ and a masked target region $z_{target}$, with the proportion of $z_{prompt}$ being $\gamma \sim U(0.1, 0.9)$.
We use $v_{\theta}$ to predict the masked target vector $\hat{z}_{target}$ conditioned on $z_{prompt}$ and the phoneme embedding $p$, denoted as $v_{\theta}(\hat{z}_{target}|z_{prompt}, p)$.
The loss is calculated using only the masked region $z_{target}$.
MegaTTS 3 learns the average pronunciation from $p$ and the specific characteristics such as timbre, accent, and prosody of the corresponding speaker from $z_{prompt}$.

</td><td>

</td></tr></table>

### Sparse Alignment Enhanced Latent Diffusion Transformer (MegaTTS 3)

<table><tr><td width="50%">

In this subsection, we describe the sparse alignment strategy as the foundation of MegaTTS 3, followed by the piecewise rectified flow and multi-condition CFG strategies to further enhance MegaTTS 3's capacity.

</td><td>

</td></tr>
<tr><td>

**Sparse Alignment Strategy**

Let’s first analyze the reasons behind the characteristics of different speech-text alignment modeling methods in depth.
Implicitly modeling speech-text alignment is a relatively challenging task, which consequently leads to suboptimal speech intelligibility, particularly in hard cases.
On the other hand, employing predefined hard alignment paths constrains the model's search space to produce more natural-sounding speech.
The characteristics of these systems motivate us to design an approach that combines the advantages of both: we first provide a rough alignment to MegaTTS 3 and then use attention mechanisms in Transformer blocks to construct the fine-grained implicit alignment path.
The visualizations of the implicit alignment paths are included in Appendix~\ref{app:vis_diff_attn}.
In specific, denote the latent speech vector sequence as $z=[z_1, z_2, \cdots, z_n]$, the phoneme sequence as $p=[p_1, p_2, \cdots, p_m]$, and the phoneme duration sequence as $d=[d_1, d_2, \cdots, d_m]$, where $n$, $m$ is the length of the sequence.
The length of the speech vector that corresponds to a phoneme $p_i$ is the duration $d_i$.
Given $d=[2, 2, 3]$, the hard speech-text alignment path can be denoted as $a=[p_1, p_1, p_2, p_2, p_3, p_3, p_3]$.
To construct the rough alignment $\tilde{a}$, we randomly retain only one anchor for each phoneme: $\tilde{a} = [\underline{M}, p_1, p_2, \underline{M}, \underline{M}, \underline{M}, P_3]$, where $\underline{M}$ represents the mask token.
$\tilde{a}$ is downsampled with convolution layers to match the length of the latent sequence $z$.
Then, we directly concatenate the downsampled $\tilde{a}$ and $z$ along the channel dimension.
The anchors in $\tilde{a}$ provide MegaTTS 3 with approximate positional information for each phoneme, simplifying the learning process of speech-text alignment.
At the same time, the rough alignment information does not limit MegaTTS 3's search space and also enables fine-grained control over each phoneme's duration.

</td><td>

</td></tr>
<tr><td>

**Piecewise Rectified Flow Acceleration**

We adopt Piecewise Rectified Flow (PeRFlow)~\citep{yan2024perflow} to distill the pretrained MegaTTS 3 model into a more efficient generator.
Although our MegaTTS 3 is non-autoregressive in terms of the time dimension, it requires multiple iterations to solve the Flow ODE.
The number of iterations (i.e., number of function evaluations, NFE) has a great impact on inference efficiency, especially when the model scales up further.
Therefore, we adopt the PeRFlow technique to further reduce NFE by segmenting the flow trajectories into multiple time windows.
Applying reflow operations within these shortened time intervals, PeRFlow eliminates the need to simulate the full ODE trajectory for training data preparation, allowing it to be trained in real-time alongside large-scale real data during the training process.
Given number of windows $K$, we divide the time $t\in[0,1]$ into $K$ time windows $\{ (t_{k-1}, t_{k}] \}^{K}_{k=1}$.
Then, we randomly sample $k\in\{1,\cdots,K\}$ uniformly.
We use the startpoint of the sampled time window $z_{t_{k-1}} = \sqrt{1 - \sigma^2(t_{k-1})} z_1 + \sigma(t_{k-1}) \epsilon$ to solve the endpoint of the time window $\hat{z}_{t_k} = \phi_{\theta}(z_{t_{k-1}}, t_{k-1}, t_{k})$, where $\epsilon \sim \mathcal{N}(0, I) $ is the random noise, $\sigma(t)$ is the noise schedule, and $\phi_{\theta}$ is the ODE solver of the teacher model.
Since $z_{t_{k-1}}$ and $\hat{z}_{t_{k}}$ is available, the student model $\hat{\theta}$ can be trained via the following objectives:
$$
    \small
    \ell = \left\lVert v_{\hat{\theta}}(z_t, t) - \frac{\hat{z}_{t_k} - z_{t_{k-1}}}{t_k - t_{k-1}} \right\lVert^2,
$$
where $v_{\hat{\theta}}$ is the estimated drift force with parameter $\hat{\theta}$ and $t$ is uniformly sampled from $(t_{k-1},t_{k}]$.
We provide details of PeRFlow training for MegaTTS 3 in Appendix~\ref{app:details_perflow_training}.

</td><td>

</td></tr>
<tr><td>

**Multi-condition Classifier-Free Guidance (CFG)**

We employ classifier-free guidance approach~\citep{ho2022classifier} to steer the model $g_{\theta}$'s output towards the conditional generation $g_{\theta}(z_t,c)$ and away from the unconditional generation $g_{\theta}(z_t,\varnothing)$:
$$
    \small
    \hat{g}_{\theta}(z_t, c) = g_{\theta}(z_t, \varnothing) + \alpha \cdot \left[ g_{\theta}(z_t, c) - g_{\theta}(z_t, \varnothing) \right],
$$
where $c$ denotes the conditional state, $\varnothing$ denotes the unconditional state, and $\alpha$ is the guidance scale selected based on experimental results.
Unlike standard classifier-free guidance, MegaTTS 3's conditional states $c$ consist of two components: phoneme embeddings $p$ and the speaker prompt $z_{prompt}$.
In the experiments, as the text guidance scale increases, we observe that the pronunciation changes according to the following pattern: 1) starting with improper pronunciation; 2) then shifting to pronouncing with the current speaker's accent; 3) and finally approaching the standard pronunciation of the target language.
The detailed experimental setup is described in Appendix~\ref{app:additional_detials_for_mt_cfg}.
This allows us to use the text guidance scale $\alpha_{txt}$ to control the accent intensity.
At the same time, the speaker guidance scale $\alpha_{spk}$ should be a relatively high value to ensure a high speaker similarity.
Therefore, we adopt the multi-condition classifier-free guidance technique to separately control $\alpha_{txt}$ and $\alpha_{spk}$:
$$
    \small
    \begin{split}
    \hat{g}_{\theta}(z_t, p, z_{prompt}) = & \alpha_{spk} \left[ g_{\theta}(z_t, p, z_{prompt}) - g_{\theta}(z_t, p, \varnothing) \right] \\
    & + \alpha_{txt} \left[ g_{\theta}(z_t, p, \varnothing) - g_{\theta}(z_t, \varnothing, \varnothing) \right] \\
    & + g_{\theta}(z_t, \varnothing, \varnothing) \\
    \end{split}
$$
In training, we randomly drop condition $z_{prompt}$ with a probability of $p_{spk} = 0.10$.
Only when $z_{prompt}$ is dropped, we randomly drop condition $p$ with a probability of 50\%.
Therefore, our model is able to handle all three types of conditional inputs described in Equation~\ref{eq:5}.
We select the guidance scale $\alpha_{spk}$ and $\alpha_{txt}$ based on experimental results.

</td><td>

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

In this subsection, we describe the datasets, training, inference, and evaluation metrics.
We provide the model configuration and detailed hyper-parameter setting in Appendix~\ref{app:model_config}.

</td><td>

</td></tr>
<tr><td>

**Datasets**

We train MegaTTS 3 on the LibriLight~\citep{kahn2020libri} dataset, which contains 60k hours of unlabeled speech derived from LibriVox audiobooks.
All speech data are sampled at 16KHz.
We transcribe the speeches using an internal ASR system and extract the predefined speech-text alignment using the external alignment tool~\citep{mcauliffe2017montreal}.
We utilize three benchmark datasets:
(1) the librispeech~\citep{panayotov2015librispeech} test-clean set following NaturalSpeech 3~\citep{ju2024naturalspeech} for zero-shot TTS evaluation;
(2) the LibriSpeech-PC test-clean set following F5-TTS~\citep{chen2024f5} for zero-shot TTS evaluation;
(3) the L2-arctic dataset~\citep{zhao2018l2arctic} following~\citep{melechovsky2022accented,liu2024controllable} for accented TTS evaluation.

</td><td>

</td></tr>
<tr><td>

**Training and Inference**

We train the WaveVAE model and MegaTTS 3 on 8 NVIDIA A100 GPUs.
The batch sizes, optimizer settings, and learning rate schedules are described in Appendix~\ref{app:model_config}.
It takes 2M steps for the WaveVAE model's training and 1M steps for MegaTTS 3's training until convergence.
The pre-training of MegaTTS 3 requires 800k steps and PeRFlow distillation requires 200k steps.

</td><td>

</td></tr>
<tr><td>

**Objective Metrics**

(1) For zero-shot TTS, we evaluate speech intelligibility using the word error rate (WER) and speaker similarity using SIM-O~\citep{ju2024naturalspeech}.
To measure SIM-O, we utilize the WavLM-TDCNN speaker embedding model\footnote{\url{https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification}} to calculate the cosine similarity score between the generated samples and the prompt.
As SIM-R~\citep{le2023Voicebox} is not comparable across baselines using different acoustic tokenizers, we recommend focusing on SIM-O in our experiments.
The similarity score is in the range of $\left[-1,1\right]$, where a higher value indicates greater similarity.
In terms of WER, we use the publicly available HuBERT-Large model~\citep{hsu2021hubert}, fine-tuned on the 960-hour LibriSpeech training set, to transcribe the generated speech.
The WER is calculated by comparing the transcribed text to the original target text.
All samples from the test set are used for the objective evaluation;
(2) For accented TTS, we evaluate the Mel Cepstral Distortion (MCD) in dB level and the moments (standard deviation ($\sigma$), skewness ($\gamma$) and kurtosis ($\kappa$))~\citep{andreeva2014differences,niebuhr2019measuring} of the pitch distribution to evaluate whether the model accurately captures accent variance.

</td><td>

</td></tr>
<tr><td>

**Subjective Metrics**

We conduct the MOS (mean opinion score) evaluation on the test set to measure the audio naturalness via Amazon Mechanical Turk.
We keep the text content and prompt speech consistent among different models to exclude other interference factors.
We randomly choose 40 samples from the test set of each dataset for the subjective evaluation, and each audio is listened to by at least 10 testers.
We analyze the MOS in three aspects: CMOS (quality, clarity, naturalness, and high-frequency details), SMOS (speaker similarity in terms of timbre reconstruction and prosodic pattern), and ASMOS (accent similarity).
We tell the testers to focus on one corresponding aspect and ignore the other aspect when scoring.

</td><td>

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td><td>

</td></tr></table>
