# MegaTTS3

<details>
<summary>基本信息</summary>

- 标题: "MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot Speech Synthesis"
- 作者:
  - 01 Ziyue Jiang
  - 02 Yi Ren
  - 03 Ruiqi Li
  - 04 Shengpeng Ji
  - 05 Boyang Zhang
  - 06 Zhenhui Ye
  - 07 Chen Zhang
  - 08 Bai Jionghao
  - 09 Xiaoda Yang
  - 10 Jialong Zuo
  - 11 Yu Zhang
  - 12 Rui Liu
  - 13 Xiang Yin
  - 14 Zhou Zhao
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.18924)
  - [Publication]()
  - [Github]()
  - [Demo](https://sditdemo.github.io/sditdemo/)
- 文件:
  - [ArXiv](_PDF/2502.18924v4__MegaTTS3__Sparse_Alignment_Enhanced_Latent_Diffusion_Transformer_for_Zero-Shot_Speech_Synthesis.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

While recent zero-shot text-to-speech (TTS) models have significantly improved speech quality and expressiveness,
mainstream systems still suffer from issues related to speech-text alignment modeling:
(1) models without explicit speech-text alignment modeling exhibit less robustness, especially for hard sentences in practical applications;
(2) predefined alignment-based models suffer from naturalness constraints of forced alignments.

This paper introduces ***MegaTTS3***, a TTS system featuring an innovative sparse alignment algorithm that guides the latent diffusion transformer (DiT).
Specifically, we provide sparse alignment boundaries to ***MegaTTS3*** to reduce the difficulty of alignment without limiting the search space, thereby achieving high naturalness.
Moreover, we employ a multi-condition classifier-free guidance strategy for accent intensity adjustment and adopt the piecewise rectified flow technique to accelerate the generation process.
Experiments demonstrate that ***MegaTTS3*** achieves state-of-the-art zero-shot TTS speech quality and supports highly flexible control over accent intensity.
Notably, our system can generate high-quality one-minute speech with only 8 sampling steps.
Audio samples are available at https://sditdemo.github.io/sditdemo/.

</td><td>

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

In recent years, neural codec language models~\citep{wang2023neural,zhang2023speak,song2024ella,xin2024rall} and large-scale diffusion models ~\citep{shen2023naturalspeech,le2023Voicebox,lee2024ditto,eskimez2024e2,ju2024naturalspeech,yang2024simplespeech,yang2024simplespeech2} have brought considerable advancements to the field of speech synthesis.
Unlike traditional text-to-speech (TTS) systems~\citep{shen2018natural,jia2018transfer,li2019neural,kim2020glow,ren2019fastspeech,kim2021conditional,kim2022guided}, these models are trained on large-scale, multi-domain speech corpora, which contributes to notable improvements in the naturalness and expressiveness of synthesized audio.
Given only seconds of speech prompt, they can synthesize identity-preserving speech in a zero-shot manner.

To generate high-quality speech with clear and expressive pronunciation, a TTS model must establish an alignment mapping from text to speech signals~\citep{kim2020glow,tan2021survey}.
However, from the perspective of speech-text alignment, current solutions suffer from the following issues:

- \textbf{Models with implicit speech-text alignment} achieve the soft alignment paths through attention mechanisms~\citep{wang2023neural,chen2024vall,du2024cosyvoice}.
These models can be categorized into:
(1) autoregressive codec language models (AR LM), which are inefficient and lack robustness.
The lengthy discrete speech codes, which typically require a bit rate of 1.5 kbps~\citep{kumar2024high,wu2024towards}, impose a significant burden on these autoregressive language models;
(2) diffusion-based models without explicit duration modeling~\citep{lee2024ditto,eskimez2024e2,lovelace2023simple,gao2023e3,cambara2024mapache,yang2024simplespeech,yang2024simplespeech2}, which significantly speeds up the speech generation process.
However, when compared with methods that adopt forced alignment, these models exhibit a decline in speech intelligibility.
Besides, these methods cannot provide fine-grained control over the duration of specific pronunciations and can only adjust the overall speech rate.

- \textbf{Predefined alignment-based methods} have prosodic naturalness constraints of forced alignments.
During training, alignment paths~\citep{ren2020fastspeech,kim2020glow} are directly introduced into their models~\citep{le2023Voicebox,shen2023naturalspeech,ju2024naturalspeech} to reduce the complexity of text-to-speech generation, which achieves higher intelligibility.
Nevertheless, they suffer from the following limitations:
(1) predefined alignments constrain the model's search space to produce more natural-sounding speech~\citep{anastassiou2024seed,chen2024vall};
(2) the overall naturalness is highly dependent on the performance of duration models.

Intuitively, we can integrate the two aforementioned diffusion-based methods to pursue optimal performance.
To be specific, we propose a novel sparse speech-text alignment strategy to enhance the latent diffusion transformer (DiT), termed ***MegaTTS3***.
In our approach, phoneme tokens are sparsely distributed within the corresponding forced alignment regions to provide coarse pronunciation information that is then refined by the latent DiT model.
Experimental results demonstrate that ***MegaTTS3*** achieves nearly state-of-the-art speech intelligibility and speaker similarity on the LibriSpeech test-clean set~\citep{panayotov2015librispeech} with only 8 sampling steps, while also exhibiting high speech naturalness.
The main contributions of this work are summarized as follows:

- We design a sparse alignment enhanced latent diffusion transformer model, which effectively integrates the strengths of the two aforementioned speech-text alignment approaches.
Notably, our model also demonstrates greater robustness to duration prediction errors compared to methods with forced alignment.

- To achieve higher generation quality and more flexible control, we propose a multi-condition CFG strategy to adjust the guidance scales for speaker timbre and text content separately.
Furthermore, we discover that the text guidance scale can also be used to modulate the intensity of personal accents, offering a new direction for enhancing speech expressiveness.

- We successfully reduce the inference steps from 25 to 8 with the piecewise rectified flow (PeRFLow) technique, achieving highly efficient zero-shot TTS with minimal quality degradation.
We also visualize the attention matrices across various layers of ***MegaTTS3*** and obtain insightful findings in Appendix~\ref{app:vis_diff_attn}.

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

**Zero-Shot TTS**

Zero-shot TTS~\citep{casanova2022yourtts,wang2023neural,zhang2023speak,shen2023naturalspeech,le2023Voicebox,jiang2024mega,liu2024autoregressive,lee2024ditto,li2024styletts,lee2023hierspeech++,ju2024naturalspeech,meng2024autoregressive,chen2024f5} aims to synthesize unseen voices with speech prompts.
Among them, neural codec language models~\citep{chen2024vall} are the first that can autoregressively synthesize speech that rivals human recordings in naturalness and expressiveness.
However, they still face several challenges, such as the lossy compression in discrete audio tokenization and the time-consuming nature of autoregressive generation.
To address these issues, some subsequent works explore solutions based on continuous vectors and non-autoregressive diffusion models~\citep{shen2023naturalspeech,le2023Voicebox,lee2024ditto,eskimez2024e2,yang2024simplespeech,yang2024simplespeech2,chen2024f5}.
These works can be categorized into two main types:
The first type directly models speech-text alignments using attention mechanisms without explicit duration modeling~\citep{lee2024ditto,eskimez2024e2}.
Although these models perform well in terms of generation speed and quality, their robustness, especially in challenging cases, still requires enhancement.
The second category~\citep{shen2023naturalspeech,le2023Voicebox} utilizes predefined alignments to simplify alignment learning.
However, the search space of the generated speech of these models is limited by predefined alignments.
To address these limitations, we propose a sparse alignment mechanism to reduce the constraints of predefined alignment-based methods while also reducing the difficulty of speech-text alignment learning.

</td><td>

</td></tr>
<tr><td>

**Accented TTS**

While accented TTS is not yet mainstream in the field of speech synthesis, it offers valuable potential for customized TTS services, by enhancing the expressiveness of speech synthesis systems and improving listeners' comprehension of speech content~\citep{tan2021survey,melechovsky2022accented,badlani2023multilingual,zhou2024multi,shah2024parrottts,ma2023accent,inoue2024macst,zhong2024accentbox}.
With the emergence of conversational AI systems, accented TTS technology has even broader application scenarios.
In this paper, we focus on a specific task of accented TTS: adjusting the accent intensity of speakers to make them sound like native English speakers or accented speakers who use English as a second language~\citep{liu2024controllable}.
Unlike previous work, our approach does not require paired data or accurate accent labels; instead, it allows for flexible control over the accent intensity using the proposed multi-condition CFG mechanism.
In addition, we describe the CFG mechanism used in zero-shot TTS systems in Appendix~\ref{app:CFG_in_zs_tts}.

</td><td>

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

This section introduces ***MegaTTS3***.
To begin with, we describe the architecture design of ***MegaTTS3***.
Then, we provide detailed explanations of the sparse alignment mechanism, the piecewise rectified flow acceleration technique, and the multi-condition classifier-free guidance strategy.

</td><td>

</td></tr></table>

### Architecture

<table><tr><td width="50%">

**WaveVAE**

As shown in Figure~\ref{fig:arch_overview} (a), given a speech waveform $s$, the VAE encoder $E$ encodes $s$ into a latent vector $z$, and the wave decoder $D$ reconstructs the waveform $x = D(z) = D(E(s))$.
To reduce the computational burden of the model and simplify speech-text alignment learning, the encoder $E$ downsamples the waveform by a factor of $d$ in length.
The encoder $E$ is similar to the one used in~\citet{ji2024wavtokenizer}, and the decoder $D$ is based on~\citet{kong2020hifi}.
We also adopt the multi-period discriminator (MPD), multi-scale discriminator (MSD), and multi-resolution discriminator (MRD) ~\citep{kong2020hifi,jang2021univnet} to model the high-frequency details in waveforms, which ensure perceptually high-quality reconstructions.
The training loss of the speech compression model can be formulated as $\mathcal{L} = \mathcal{L}_{\mathrm{rec}} + \mathcal{L}_{\mathrm{KL}} + \mathcal{L}_{\mathrm{Adv}}$, where $\mathcal{L}_{\mathrm{rec}}=\|s-\hat{s}\|^2$ is the spectrogram reconstruction loss, $\mathcal{L}_{\mathrm{KL}}$ is the slight KL-penalty loss~\citep{rombach2022high}, and $\mathcal{L}_{\mathrm{Adv}}$ is the LSGAN-styled adversarial loss~\citep{mao2017least}.
After training, a one-second speech clip can be encoded into 25 vector frames.
For more details, please refer to Appendix~\ref{app:model_config} and~\ref{app:evaluation_speech_compression}.

</td><td>

</td></tr>
<tr><td>

**Latent Diffusion Transformer with Masked Speech Modeling**

The latent diffusion transformer is used to predict speech that matches the style of the given speaker and the content of the provided text.
Given the random variables $Z_{0}$ sampled from a standard Gaussian distribution $\pi_{0}$ and $Z_{1}$ sampled from the latent space given by the speech compression model with data density $\pi_{1}$, we adopt the rectified flow~\citet{liu2022flow} to implicitly learn the transport map $T$, which yields $Z_{1} := T(Z_{0})$.
The rectified flow learns $T$ by constructing the following ordinary differential equation (ODE):
$$
\mathrm{d}Z_t = v(Z_t, t)\,\mathrm{d}t,
$$

where $t\in[0,1]$ denotes time and $v$ is the drift force.
Equation~\ref{eq:1} converts $Z_{0}$ from $\pi_{0}$ to $Z_{1}$ from $\pi_{1}$.
The drift force $v$ drives the flow to follow the direction $(Z_{1}-Z_{0})$.
The latent diffusion transformer,  parameterized by $\theta$, can be trained by estimating $v(Z_{t}, t)$ with $v_{\theta}(Z_{t}, t)$ through minimizing the least squares loss with respect to the line directions $(Z_{1}-Z_{0})$:
$$
\min_v \int_0^1 \mathbb{E} \left[ \| (Z_1 - Z_0) - v(Z_t, t) \|^2 \right] \, \mathrm{d}t.
$$

We use the standard transformer block from LLAMA~\citep{dubey2024llama} as the basic structure for ***MegaTTS3*** and adopt the Rotary Position Embedding (RoPE)~\citep{su2024roformer} as the positional embedding.
During training, we randomly divide the latent vector sequence into a prompt region $z_{prompt}$ and a masked target region $z_{target}$, with the proportion of $z_{prompt}$ being $\gamma \sim U(0.1, 0.9)$.
We use $v_{\theta}$ to predict the masked target vector $\hat{z}_{target}$ conditioned on $z_{prompt}$ and the phoneme embedding $p$, denoted as $v_{\theta}(\hat{z}_{target}|z_{prompt}, p)$.
The loss is calculated using only the masked region $z_{target}$.
***MegaTTS3*** learns the average pronunciation from $p$ and the specific characteristics such as timbre, accent, and prosody of the corresponding speaker from $z_{prompt}$.

</td><td>

</td></tr></table>

### Sparse Alignment Enhanced Latent Diffusion Transformer (MegaTTS3)

<table><tr><td width="50%">

In this subsection, we describe the sparse alignment strategy as the foundation of ***MegaTTS3***, followed by the piecewise rectified flow and multi-condition CFG strategies to further enhance ***MegaTTS3***'s capacity.

</td><td>

</td></tr>
<tr><td>

**Sparse Alignment Strategy**

Let’s first analyze the reasons behind the characteristics of different speech-text alignment modeling methods in depth.
Implicitly modeling speech-text alignment is a relatively challenging task, which consequently leads to suboptimal speech intelligibility, particularly in hard cases.
On the other hand, employing predefined hard alignment paths constrains the model's search space to produce more natural-sounding speech.
The characteristics of these systems motivate us to design an approach that combines the advantages of both: we first provide a rough alignment to ***MegaTTS3*** and then use attention mechanisms in Transformer blocks to construct the fine-grained implicit alignment path.
The visualizations of the implicit alignment paths are included in Appendix~\ref{app:vis_diff_attn}.
In specific, denote the latent speech vector sequence as $z=[z_1, z_2, \cdots, z_n]$, the phoneme sequence as $p=[p_1, p_2, \cdots, p_m]$, and the phoneme duration sequence as $d=[d_1, d_2, \cdots, d_m]$, where $n$, $m$ is the length of the sequence.
The length of the speech vector that corresponds to a phoneme $p_i$ is the duration $d_i$.
Given $d=[2, 2, 3]$, the hard speech-text alignment path can be denoted as $a=[p_1, p_1, p_2, p_2, p_3, p_3, p_3]$.
To construct the rough alignment $\tilde{a}$, we randomly retain only one anchor for each phoneme: $\tilde{a} = [\underline{M}, p_1, p_2, \underline{M}, \underline{M}, \underline{M}, P_3]$, where $\underline{M}$ represents the mask token.
$\tilde{a}$ is downsampled with convolution layers to match the length of the latent sequence $z$.
Then, we directly concatenate the downsampled $\tilde{a}$ and $z$ along the channel dimension.
The anchors in $\tilde{a}$ provide ***MegaTTS3*** with approximate positional information for each phoneme, simplifying the learning process of speech-text alignment.
At the same time, the rough alignment information does not limit ***MegaTTS3***'s search space and also enables fine-grained control over each phoneme's duration.

</td><td>

</td></tr>
<tr><td>

**Piecewise Rectified Flow Acceleration**

We adopt Piecewise Rectified Flow (PeRFlow)~\citep{yan2024perflow} to distill the pretrained ***MegaTTS3*** model into a more efficient generator.
Although our ***MegaTTS3*** is non-autoregressive in terms of the time dimension, it requires multiple iterations to solve the Flow ODE.
The number of iterations (i.e., number of function evaluations, NFE) has a great impact on inference efficiency, especially when the model scales up further.
Therefore, we adopt the PeRFlow technique to further reduce NFE by segmenting the flow trajectories into multiple time windows.
Applying reflow operations within these shortened time intervals, PeRFlow eliminates the need to simulate the full ODE trajectory for training data preparation, allowing it to be trained in real-time alongside large-scale real data during the training process.
Given number of windows $K$, we divide the time $t\in[0,1]$ into $K$ time windows $\{ (t_{k-1}, t_{k}] \}^{K}_{k=1}$.
Then, we randomly sample $k\in\{1,\cdots,K\}$ uniformly.
We use the startpoint of the sampled time window $z_{t_{k-1}} = \sqrt{1 - \sigma^2(t_{k-1})} z_1 + \sigma(t_{k-1}) \epsilon$ to solve the endpoint of the time window $\hat{z}_{t_k} = \phi_{\theta}(z_{t_{k-1}}, t_{k-1}, t_{k})$, where $\epsilon \sim \mathcal{N}(0, I) $ is the random noise, $\sigma(t)$ is the noise schedule, and $\phi_{\theta}$ is the ODE solver of the teacher model.
Since $z_{t_{k-1}}$ and $\hat{z}_{t_{k}}$ is available, the student model $\hat{\theta}$ can be trained via the following objectives:
$$
\ell = \left\lVert v_{\hat{\theta}}(z_t, t) - \frac{\hat{z}_{t_k} - z_{t_{k-1}}}{t_k - t_{k-1}} \right\lVert^2,
$$

where $v_{\hat{\theta}}$ is the estimated drift force with parameter $\hat{\theta}$ and $t$ is uniformly sampled from $(t_{k-1},t_{k}]$.
We provide details of PeRFlow training for ***MegaTTS3*** in Appendix~\ref{app:details_perflow_training}.

</td><td>

</td></tr>
<tr><td>

**Multi-condition Classifier-Free Guidance (CFG)**

We employ classifier-free guidance approach~\citep{ho2022classifier} to steer the model $g_{\theta}$'s output towards the conditional generation $g_{\theta}(z_t,c)$ and away from the unconditional generation $g_{\theta}(z_t,\varnothing)$:
$$
\hat{g}_{\theta}(z_t, c) = g_{\theta}(z_t, \varnothing) + \alpha \cdot \left[ g_{\theta}(z_t, c) - g_{\theta}(z_t, \varnothing) \right],
$$

where $c$ denotes the conditional state, $\varnothing$ denotes the unconditional state, and $\alpha$ is the guidance scale selected based on experimental results.
Unlike standard classifier-free guidance, ***MegaTTS3***'s conditional states $c$ consist of two components: phoneme embeddings $p$ and the speaker prompt $z_{prompt}$.
In the experiments, as the text guidance scale increases, we observe that the pronunciation changes according to the following pattern: 1) starting with improper pronunciation; 2) then shifting to pronouncing with the current speaker's accent; 3) and finally approaching the standard pronunciation of the target language.
The detailed experimental setup is described in Appendix~\ref{app:additional_detials_for_mt_cfg}.
This allows us to use the text guidance scale $\alpha_{txt}$ to control the accent intensity.
At the same time, the speaker guidance scale $\alpha_{spk}$ should be a relatively high value to ensure a high speaker similarity.
Therefore, we adopt the multi-condition classifier-free guidance technique to separately control $\alpha_{txt}$ and $\alpha_{spk}$:
$$
\begin{split}
\hat{g}_{\theta}(z_t, p, z_{prompt}) = & \alpha_{spk} \left[ g_{\theta}(z_t, p, z_{prompt}) - g_{\theta}(z_t, p, \varnothing) \right] \\
& + \alpha_{txt} \left[ g_{\theta}(z_t, p, \varnothing) - g_{\theta}(z_t, \varnothing, \varnothing) \right] \\
& + g_{\theta}(z_t, \varnothing, \varnothing) \\
\end{split}
$$

In training, we randomly drop condition $z_{prompt}$ with a probability of $p_{spk} = 0.10$.
Only when $z_{prompt}$ is dropped, we randomly drop condition $p$ with a probability of 50\%.
Therefore, our model is able to handle all three types of conditional inputs described in Equation~\ref{eq:5}.
We select the guidance scale $\alpha_{spk}$ and $\alpha_{txt}$ based on experimental results.

</td><td>

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

In this subsection, we describe the datasets, training, inference, and evaluation metrics.
We provide the model configuration and detailed hyper-parameter setting in Appendix~\ref{app:model_config}.

</td><td>

</td></tr>
<tr><td>

**Datasets**

We train ***MegaTTS3*** on the LibriLight~\citep{kahn2020libri} dataset, which contains 60k hours of unlabeled speech derived from LibriVox audiobooks.
All speech data are sampled at 16KHz.
We transcribe the speeches using an internal ASR system and extract the predefined speech-text alignment using the external alignment tool~\citep{mcauliffe2017montreal}.
We utilize three benchmark datasets:
(1) the librispeech~\citep{panayotov2015librispeech} test-clean set following NaturalSpeech 3~\citep{ju2024naturalspeech} for zero-shot TTS evaluation;
(2) the LibriSpeech-PC test-clean set following F5-TTS~\citep{chen2024f5} for zero-shot TTS evaluation;
(3) the L2-arctic dataset~\citep{zhao2018l2arctic} following~\citep{melechovsky2022accented,liu2024controllable} for accented TTS evaluation.

</td><td>

</td></tr>
<tr><td>

**Training and Inference**

We train the WaveVAE model and ***MegaTTS3*** on 8 NVIDIA A100 GPUs.
The batch sizes, optimizer settings, and learning rate schedules are described in Appendix~\ref{app:model_config}.
It takes 2M steps for the WaveVAE model's training and 1M steps for ***MegaTTS3***'s training until convergence.
The pre-training of ***MegaTTS3*** requires 800k steps and PeRFlow distillation requires 200k steps.

</td><td>

</td></tr>
<tr><td>

**Objective Metrics**

(1) For zero-shot TTS, we evaluate speech intelligibility using the word error rate (WER) and speaker similarity using SIM-O~\citep{ju2024naturalspeech}.
To measure SIM-O, we utilize the WavLM-TDCNN speaker embedding model\footnote{\url{https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification}} to calculate the cosine similarity score between the generated samples and the prompt.
As SIM-R~\citep{le2023Voicebox} is not comparable across baselines using different acoustic tokenizers, we recommend focusing on SIM-O in our experiments.
The similarity score is in the range of $\left[-1,1\right]$, where a higher value indicates greater similarity.
In terms of WER, we use the publicly available HuBERT-Large model~\citep{hsu2021hubert}, fine-tuned on the 960-hour LibriSpeech training set, to transcribe the generated speech.
The WER is calculated by comparing the transcribed text to the original target text.
All samples from the test set are used for the objective evaluation;
(2) For accented TTS, we evaluate the Mel Cepstral Distortion (MCD) in dB level and the moments (standard deviation ($\sigma$), skewness ($\gamma$) and kurtosis ($\kappa$))~\citep{andreeva2014differences,niebuhr2019measuring} of the pitch distribution to evaluate whether the model accurately captures accent variance.

</td><td>

</td></tr>
<tr><td>

**Subjective Metrics**

We conduct the MOS (mean opinion score) evaluation on the test set to measure the audio naturalness via Amazon Mechanical Turk.
We keep the text content and prompt speech consistent among different models to exclude other interference factors.
We randomly choose 40 samples from the test set of each dataset for the subjective evaluation, and each audio is listened to by at least 10 testers.
We analyze the MOS in three aspects: CMOS (quality, clarity, naturalness, and high-frequency details), SMOS (speaker similarity in terms of timbre reconstruction and prosodic pattern), and ASMOS (accent similarity).
We tell the testers to focus on one corresponding aspect and ignore the other aspect when scoring.

</td><td>

</td></tr></table>

## 5·Results: 结果

### Results of Zero-Shot Speech Synthesis

<table><tr><td width="50%">

**Evaluation Baselines**

We compare the zero-shot speech synthesis performance of ***MegaTTS3*** with 11 strong baselines, including:
(1) VALL-E 2~\citep{chen2024vall};
(2) VoiceBox~\citep{le2023Voicebox};
(3) DiTTo-TTS~\citep{lee2024ditto};
(4) NaturalSpeech 3~\citep{ju2024naturalspeech};
(5) CosyVoice~\citep{du2024cosyvoice};
(6) MaskGCT~\citep{wang2024maskgct};
(7) F5-TTS~\citep{chen2024f5};
(8) E2 TTS~\citep{eskimez2024e2}.

Explanation and details of the selected baseline systems are provided in Appendix~\ref{app:detail_zs_tts_baseline}.

</td><td>

</td></tr>
<tr><td>

**Analysis**

As shown in Table~\ref{table:en_zs_tts}, we can see that 1) ***MegaTTS3*** achieves state-of-the-art SIM-O, SMOS, and WER scores, comparable to NaturalSpeech 3 (the counterpart with forced alignment), and significantly surpasses other baselines without explicit alignments.
The improved SIM-O and SMOS suggest that the proposed sparse alignment effectively simplifies the text-to-speech mapping challenge like predefined forced duration information, allowing the model to focus more on learning timbre information.
And the improved WER indicates that ***MegaTTS3*** also enjoys strong robustness; 2) ***MegaTTS3*** significantly surpasses all baselines in terms of CMOS, demonstrating the effectiveness of the proposed sparse alignment strategy; 3) After the PeRFlow acceleration, the student model of ***MegaTTS3*** shows on par quality with the teacher model and enjoys fast inference speed.
We also conduct the experiments on the LibriSpeech-PC test-clean set provided by F5-TTS and the results are shown in Table~\ref{table:en_zs_tts_valle}, which also demonstrates that our method achieves state-of-the-art performance in terms of speaker similarity and speech intelligibility.
The duration controllability of ***MegaTTS3*** is verified in Appendix~\ref{app:dur_contol}.
In the demo page, we also demonstrate that our method can maintain high naturalness even when the performance of the duration predictor is suboptimal (while ***MegaTTS3*** with forced alignment fails).

</td><td>

</td></tr></table>

### Experiments of Prosodic Naturalness

<table><tr><td width="50%">

We also measure the objective metrics MCD, SSIM, STOI, GPE, VDE, and FFE following InstructTTS~\citep{yang2024instructtts} to evaluate the prosodic naturalness of our method.
The results are presented in Table~\ref{table:expressiveness_exp_40}.
Specifically, our method with sparse alignment (Ours w/ S.A.) achieves the best performance across all metrics, with an MCD of 4.42, GPE of 0.31, VDE of 0.29, and FFE of 0.34.
These results indicate a significant improvement in prosodic naturalness compared to the baseline NaturalSpeech 3 and our method with forced alignment (Ours w/ F.A.), further validating the effectiveness of our sparse alignment strategy.
Our method provides a noval and effective solution for speech synthesis applications that require high robustness and exceptional expressiveness, such as audiobook narration and virtual assistants.

</td><td>

</td></tr></table>

### Results of Accented TTS

<table><tr><td width="50%">

In this subsection, we evaluate the accented TTS performance of our model on the L2-ARCTIC dataset~\citep{zhao2018l2arctic}.
This corpus includes recordings from non-native speakers of English whose first languages are Hindi, Korean, etc.
In this experiment, we focus on verifying whether our model and baseline can synthesize natural speech with different accent types (standard English or English with specific accents) while maintaining consistent vocal timbre.
We compare our ***MegaTTS3*** model with CTA-TTS~\citep{liu2024controllable}.
More details of the baseline model are provided in Appendix~\ref{app:detail_accented_tts_experiment}.
1) First, we evaluate whether the models can synthesize high-quality speeches with accents.
As shown in Table~\ref{table:accent-tts-result}, our ***MegaTTS3*** model significantly outperforms the CTA-TTS baseline in terms of the subjective accent similarity MOS core, the MCD (dB) values, and the statistical moments ($\sigma$, $\gamma$, and $\kappa$) of pitch distributions.
These results demonstrate the superior accent learning capability of ***MegaTTS3*** compared to the baseline system.
Besides, the ***MegaTTS3*** model achieves higher CMOS and SMOS scores compared to CTA-TTS, indicating a significant improvement in speech quality and speaker similarity; 2) Secondly, we evaluate whether the models can accurately control the accent types of the generated speeches.
We follow CTA-TTS to conduct the intensity classification experiment~\citep{liu2024controllable}.
At
run-time, we generate speeches with two accent types, and the listeners are instructed to classify the perceived accent categories, including ``standard'' and ``accented''.
Figure~\ref{exp:accent_confusion_matrices} shows that our ***MegaTTS3*** significantly surpasses CTA-TTS in terms of accent controllability.

</td><td>

</td></tr></table>

### Evaluation of WaveVAE

<table><tr><td width="50%">

First, we evaluate the reconstruction quality of the WaveVAE model, with results presented in Table~\ref{app:table_recon_speech_compression}.
We report the objective metrics, including Perceptual Evaluation of Speech Quality (PESQ), Virtual Speech Quality Objective Listener (ViSQOL), and Mel-Cepstral Distortion (MCD).
We select the following codec models as baselines: 1) EnCodec~\citep{defossez2022high}, a representative and pioneering work in the field of speech codec; 2) DAC~\citep{kumar2024high}, a high-bitrate audio codec model with high reconstruction quality; 3) WavTokenizer~\citep{ji2024wavtokenizer}, a low-bitrate speech codec model that focuses more on perceptual reconstruction quality; 4) X-codec2~\citep{ye2025llasa}, a low-bitrate speech codec model, leveraging the representations of a pre-trained model to further enhance overall quality.
The results demonstrates that, despite applying higher compression rate, our solution achieves superior performance on various reconstruction metrics, such as MCD and ViSQOL.

Second, to demonstrate the impact of different speech compression models on the overall performance of the TTS system, we extracted the latents from Encodec and DAC, respectively, for training our ***MegaTTS3*** model.
We report the experimental results in Table~\ref{app:table_zs_tts_different_codec}.
It can be seen that our method outperforms ``w/ DAC'' and ``w/ Encodec'', due to the fact that the latent space of our speech compression model is more compact (only 25 tokens per second).
The results demonstrate the importance of our WaveVAE, a high-compression, high-reconstruction-quality speech codec model, for TTS systems.
This conclusion is also verified by a previous work~\citep{lee2024ditto}, which shows compact target latents facilitate learning in diffusion models.

</td><td>

</td></tr></table>

### Ablation Studies

<table><tr><td width="50%">

We test the following four settings: 1) \textit{w/ FA}, which replaces the sparse alignment in ***MegaTTS3*** with forced alignment used in ~\citep{le2023Voicebox,shen2023naturalspeech}; 2) \textit{w/o A.}, we do not use the predefined alignments and modeling the duration information implicitly; 3) \textit{w/ CFG}, we use the standard CFG following the common practice in Diffusion-based TTS; 4) \textit{w/o CFG}, we do not use the CFG mechanism.
All tests follow the experimental setup described in Section~\ref{exp_zero-shot}.
The results are shown in Table~\ref{table:ablation_alignments_cfg}.
For settings 1) and 2), it can be observed that both forced alignment and sparse alignment can enhance the performance of speech synthesis models.
However, compared to forced alignment, sparse alignment does not constrain the model's search space, leading to a prosodic naturalness (see Section~\ref{exp:prosodic_naturalness}).
Therefore, the sparse alignment strategy achieves $+0.17$ CMOS compared to the forced alignment strategy.
For setting 3), compared with the standard CFG, our multi-condition CFG performs slightly better as it allows for flexible control over the weights between the text prompt and the speaker prompt.
Setting 4) proves that the CFG mechanism is crucial for ***MegaTTS3***.
Additionally, we visualize the attention score matrices from different transformer layers in ***MegaTTS3*** in Appendix~\ref{app:vis_diff_attn}, leading to some interesting observations.

</td><td>

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

In this paper, we introduce ***MegaTTS3***, a zero-shot TTS framework that leverages novel sparse alignment boundaries to ease the difficulty of alignment learning while retaining the naturalness of the generated speeches.
This strategy allows ***MegaTTS3*** to combine the strengths of methods with both implicit alignments and predefined hard alignments.
Additionally, we employ the PeRFlow technique to further accelerate the generation process and design a multi-condition CFG strategy to offer more flexible control over accents.
Experimental results show that ***MegaTTS3*** achieves state-of-the-art zero-shot TTS speech quality while maintaining a more efficient pipeline.
Moreover, the sparse alignment strategy also shows enhanced prosodic naturalness and higher robustness against a suboptimal duration predictor.
Due to space constraints, further discussions are provided in the appendix.

</td><td>

</td></tr></table>
