# MegaTTS3

<details>
<summary>基本信息</summary>

- 标题: "MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot Speech Synthesis"
- 作者:
  - 01 Ziyue Jiang
  - 02 Yi Ren
  - 03 Ruiqi Li
  - 04 Shengpeng Ji
  - 05 Boyang Zhang
  - 06 Zhenhui Ye
  - 07 Chen Zhang
  - 08 Bai Jionghao
  - 09 Xiaoda Yang
  - 10 Jialong Zuo
  - 11 Yu Zhang
  - 12 Rui Liu
  - 13 Xiang Yin
  - 14 Zhou Zhao
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.18924)
  - [Publication]()
  - [Github]()
  - [Demo](https://sditdemo.github.io/sditdemo/)
- 文件:
  - [ArXiv](_PDF/2502.18924v4__MegaTTS3__Sparse_Alignment_Enhanced_Latent_Diffusion_Transformer_for_Zero-Shot_Speech_Synthesis.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

While recent zero-shot text-to-speech (TTS) models have significantly improved speech quality and expressiveness, mainstream systems still suffer from issues related to speech-text alignment modeling:
(1) models without explicit speech-text alignment modeling exhibit less robustness, especially for hard sentences in practical applications;
(2) predefined alignment-based models suffer from naturalness constraints of forced alignments.

This paper introduces ***MegaTTS3***, a TTS system featuring an innovative **sparse alignment algorithm** that guides the **latent diffusion transformer (DiT)**.
Specifically, we provide sparse alignment boundaries to ***MegaTTS3*** to reduce the difficulty of alignment without limiting the search space, thereby achieving high naturalness.
Moreover, we employ a multi-condition classifier-free guidance strategy for accent intensity adjustment and adopt the piecewise rectified flow technique to accelerate the generation process.
Experiments demonstrate that ***MegaTTS3*** achieves state-of-the-art zero-shot TTS speech quality and supports highly flexible control over accent intensity.
Notably, our system can generate high-quality one-minute speech with only 8 sampling steps.
Audio samples are available at https://sditdemo.github.io/sditdemo/.

</td><td>

尽管最近的零样本文本转语音模型已经显著提升了语音质量和表达力, 主流系统仍然受到和语音-文本对齐建模相关的问题的困扰:
1. 无显式语音-文本对齐建模的模型表现出较低的鲁棒性, 尤其是在实际应用中遇到困难句子时;
2. 基于预定义对齐的模型受到强制对齐的自然性约束.

本文介绍了 ***MegaTTS3***, 一个采用创新的**稀疏对齐算法**来引导**潜在扩散 Transformer (DiT)** 的 TTS 系统.
具体来说, 我们给 ***MegaTTS3*** 提供了稀疏对齐边界, 以减少对齐的困难, 且不限制搜索空间, 从而实现高自然度.
此外, 我们采用了多条件无分类器引导策略来进行口音强度调节, 采用分段 Rectified Flow 技术来加速生成过程.

实验表明 ***MegaTTS3*** 实现了最先进的零样本文本转语音质量, 并支持对口音强度的高度灵活的控制能力.
值得注意的是, 我们的系统仅需 8 步采样就可以生成 1 分钟的高质量语音.
音频示例可在 https://sditdemo.github.io/sditdemo/ 获得.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

In recent years, neural codec language models (**VALL-E**[^01], **SPEAR-TTS**[^02], **ELLA-V**[^03], **RALL-E**[^04]) and large-scale diffusion models (**NaturalSpeech2**[^05], **Voicebox**[^06], **DiTTo-TTS**[^07], **E2 TTS**[^08], **NaturalSpeech3**[^09], **SimpleSpeech**[^10], **SimpleSpeech2**[^11]) have brought considerable advancements to the field of speech synthesis.
Unlike traditional text-to-speech (TTS) systems (**Tacotron2**[^12], [^13], **TransformerTTS**[^14], ***Glow-TTS**[^15], **FastSpeech**[^16], **VITS**[^17], **Guided-TTS**[^18]), these models are trained on large-scale, multi-domain speech corpora, which contributes to notable improvements in the naturalness and expressiveness of synthesized audio.
Given only seconds of speech prompt, they can synthesize identity-preserving speech in a zero-shot manner.

To generate high-quality speech with clear and expressive pronunciation, a TTS model must establish an alignment mapping from text to speech signals (**Glow-TTS**[^15], **Survey by Tan**[^19]).
However, from the perspective of speech-text alignment, current solutions suffer from the following issues:

- **Models with implicit speech-text alignment** achieve the soft alignment paths through attention mechanisms (**VALL-E**[^01], **VALL-E 2**[^20], **CosyVoice**[^21]).
These models can be categorized into:
(1) autoregressive codec language models (AR LM), which are inefficient and lack robustness.
The lengthy discrete speech codes, which typically require a bit rate of 1.5 kbps (**DAC**[^22], **Survey by Wu**[^23]), impose a significant burden on these autoregressive language models;
(2) diffusion-based models without explicit duration modeling (**DiTTo-TTS**[^07], **E2 TTS**[^08], **Simple-TTS**[^24], **E3 TTS**[^25], **Mapache**[^26], **SimpleSpeech**[^10], **SimpleSpeech2**[^11]), which significantly speeds up the speech generation process.
However, when compared with methods that adopt forced alignment, these models exhibit a decline in speech intelligibility.
Besides, these methods cannot provide fine-grained control over the duration of specific pronunciations and can only adjust the overall speech rate.

- **Predefined alignment-based methods** have prosodic naturalness constraints of forced alignments.
During training, alignment paths (**FastSpeech2**[^27], **Glow-TTS**[^15]) are directly introduced into their models **Voicebox**[^06], **NaturalSpeech2**[^05], **NaturalSpeech3**[^09] to reduce the complexity of text-to-speech generation, which achieves higher intelligibility.
Nevertheless, they suffer from the following limitations:
(1) predefined alignments constrain the model's search space to produce more natural-sounding speech (**Seed-TTS**[^28], **VALL-E 2**[^20]);
(2) the overall naturalness is highly dependent on the performance of duration models.

Intuitively, we can integrate the two aforementioned diffusion-based methods to pursue optimal performance.
To be specific, we propose a novel sparse speech-text alignment strategy to enhance the latent diffusion transformer (DiT), termed ***MegaTTS3***.
In our approach, phoneme tokens are sparsely distributed within the corresponding forced alignment regions to provide coarse pronunciation information that is then refined by the latent DiT model.
Experimental results demonstrate that ***MegaTTS3*** achieves nearly state-of-the-art speech intelligibility and speaker similarity on the **LibriSpeech test-clean set**[^29] with only 8 sampling steps, while also exhibiting high speech naturalness.
The main contributions of this work are summarized as follows:

- We design a sparse alignment enhanced latent diffusion transformer model, which effectively integrates the strengths of the two aforementioned speech-text alignment approaches.
Notably, our model also demonstrates greater robustness to duration prediction errors compared to methods with forced alignment.

- To achieve higher generation quality and more flexible control, we propose a multi-condition CFG strategy to adjust the guidance scales for speaker timbre and text content separately.
Furthermore, we discover that the text guidance scale can also be used to modulate the intensity of personal accents, offering a new direction for enhancing speech expressiveness.

- We successfully reduce the inference steps from 25 to 8 with the piecewise rectified flow (PeRFLow) technique, achieving highly efficient zero-shot TTS with minimal quality degradation.
We also visualize the attention matrices across various layers of ***MegaTTS3*** and obtain insightful findings in Appendix~\ref{app:vis_diff_attn}.

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

**Zero-Shot TTS**

Zero-shot TTS (**YourTTS**[^30], **VALL-E**[^01], **SPEAR-TTS**[^02], **NaturalSpeech2**[^05], **Voicebox**[^06], **Mega-TTS2**[^31], **ARDiT**[^32], **DiTTo-TTS**[^07], **StyleTTS2**[^33], **HierSpeech++**[^34], **NaturalSpeech3**[^09], **MELLE**[^35], **F5-TTS**[^36]) aims to synthesize unseen voices with speech prompts.
Among them, neural codec language models (**VALL-E 2**[^20]) are the first that can autoregressively synthesize speech that rivals human recordings in naturalness and expressiveness.
However, they still face several challenges, such as the lossy compression in discrete audio tokenization and the time-consuming nature of autoregressive generation.
To address these issues, some subsequent works explore solutions based on continuous vectors and non-autoregressive diffusion models (**NaturalSpeech2**[^05], **Voicebox**[^06], **DiTTo-TTS**[^07], **E2 TTS**[^08], **SimpleSpeech**[^10], **SimpleSpeech2**[^11], **F5-TTS**[^36]).
These works can be categorized into two main types:
The first type directly models speech-text alignments using attention mechanisms without explicit duration modeling (**DiTTo-TTS**[^07], **E2 TTS**[^08]).
Although these models perform well in terms of generation speed and quality, their robustness, especially in challenging cases, still requires enhancement.
The second category (**NaturalSpeech2**[^05], **Voicebox**[^06]) utilizes predefined alignments to simplify alignment learning.
However, the search space of the generated speech of these models is limited by predefined alignments.
To address these limitations, we propose a sparse alignment mechanism to reduce the constraints of predefined alignment-based methods while also reducing the difficulty of speech-text alignment learning.

</td><td>

</td></tr>
<tr><td>

**Accented TTS**

While accented TTS is not yet mainstream in the field of speech synthesis, it offers valuable potential for customized TTS services, by enhancing the expressiveness of speech synthesis systems and improving listeners' comprehension of speech content ([^19], [^37], [^38], [^39], [^40], [^41], [^42], [^43]).
With the emergence of conversational AI systems, accented TTS technology has even broader application scenarios.
In this paper, we focus on a specific task of accented TTS: adjusting the accent intensity of speakers to make them sound like native English speakers or accented speakers who use English as a second language[^44].
Unlike previous work, our approach does not require paired data or accurate accent labels; instead, it allows for flexible control over the accent intensity using the proposed multi-condition CFG mechanism.
In addition, we describe the CFG mechanism used in zero-shot TTS systems in Appendix~\ref{app:CFG_in_zs_tts}.

</td><td>

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

This section introduces ***MegaTTS3***.
To begin with, we describe the architecture design of ***MegaTTS3***.
Then, we provide detailed explanations of the sparse alignment mechanism, the piecewise rectified flow acceleration technique, and the multi-condition classifier-free guidance strategy.

</td><td>

</td></tr></table>

### Architecture

<table><tr><td width="50%">

**WaveVAE**

As shown in Figure~\ref{fig:arch_overview} (a), given a speech waveform $s$, the VAE encoder $E$ encodes $s$ into a latent vector $z$, and the wave decoder $D$ reconstructs the waveform $x = D(z) = D(E(s))$.
To reduce the computational burden of the model and simplify speech-text alignment learning, the encoder $E$ downsamples the waveform by a factor of $d$ in length.
The encoder $E$ is similar to the one used in **WavTokenizer**[^45], and the decoder $D$ is based on **HiFi-GAN**[^46].
We also adopt the multi-period discriminator (MPD), multi-scale discriminator (MSD), and multi-resolution discriminator (MRD) (**HiFi-GAN**[^46], **UnivNet**[^47]) to model the high-frequency details in waveforms, which ensure perceptually high-quality reconstructions.
The training loss of the speech compression model can be formulated as $\mathcal{L} = \mathcal{L}_{\mathrm{rec}} + \mathcal{L}_{\mathrm{KL}} + \mathcal{L}_{\mathrm{Adv}}$, where $\mathcal{L}_{\mathrm{rec}}=\|s-\hat{s}\|^2$ is the spectrogram reconstruction loss, $\mathcal{L}_{\mathrm{KL}}$ is the slight KL-penalty loss[^48], and $\mathcal{L}_{\mathrm{Adv}}$ is the LSGAN-styled adversarial loss[^49].
After training, a one-second speech clip can be encoded into 25 vector frames.
For more details, please refer to Appendix~\ref{app:model_config} and~\ref{app:evaluation_speech_compression}.

</td><td>

</td></tr>
<tr><td>

**Latent Diffusion Transformer with Masked Speech Modeling**

The latent diffusion transformer is used to predict speech that matches the style of the given speaker and the content of the provided text.
Given the random variables $Z_{0}$ sampled from a standard Gaussian distribution $\pi_{0}$ and $Z_{1}$ sampled from the latent space given by the speech compression model with data density $\pi_{1}$, we adopt the rectified flow[^50] to implicitly learn the transport map $T$, which yields $Z_{1} := T(Z_{0})$.
The rectified flow learns $T$ by constructing the following ordinary differential equation (ODE):
$$
\mathrm{d}Z_t = v(Z_t, t)\,\mathrm{d}t,
$$

where $t\in[0,1]$ denotes time and $v$ is the drift force.
Equation~\ref{eq:1} converts $Z_{0}$ from $\pi_{0}$ to $Z_{1}$ from $\pi_{1}$.
The drift force $v$ drives the flow to follow the direction $(Z_{1}-Z_{0})$.
The latent diffusion transformer,  parameterized by $\theta$, can be trained by estimating $v(Z_{t}, t)$ with $v_{\theta}(Z_{t}, t)$ through minimizing the least squares loss with respect to the line directions $(Z_{1}-Z_{0})$:
$$
\min_v \int_0^1 \mathbb{E} \left[ \| (Z_1 - Z_0) - v(Z_t, t) \|^2 \right] \, \mathrm{d}t.
$$

We use the standard transformer block from **LLAMA3**[^51] as the basic structure for ***MegaTTS3*** and adopt the **Rotary Position Embedding (RoPE)**[^52] as the positional embedding.
During training, we randomly divide the latent vector sequence into a prompt region $z_{prompt}$ and a masked target region $z_{target}$, with the proportion of $z_{prompt}$ being $\gamma \sim U(0.1, 0.9)$.
We use $v_{\theta}$ to predict the masked target vector $\hat{z}_{target}$ conditioned on $z_{prompt}$ and the phoneme embedding $p$, denoted as $v_{\theta}(\hat{z}_{target}|z_{prompt}, p)$.
The loss is calculated using only the masked region $z_{target}$.
***MegaTTS3*** learns the average pronunciation from $p$ and the specific characteristics such as timbre, accent, and prosody of the corresponding speaker from $z_{prompt}$.

</td><td>

</td></tr></table>

### Sparse Alignment Enhanced Latent Diffusion Transformer (MegaTTS3)

<table><tr><td width="50%">

In this subsection, we describe the sparse alignment strategy as the foundation of ***MegaTTS3***, followed by the piecewise rectified flow and multi-condition CFG strategies to further enhance ***MegaTTS3***'s capacity.

</td><td>

</td></tr>
<tr><td>

**Sparse Alignment Strategy**

Let’s first analyze the reasons behind the characteristics of different speech-text alignment modeling methods in depth.
Implicitly modeling speech-text alignment is a relatively challenging task, which consequently leads to suboptimal speech intelligibility, particularly in hard cases.
On the other hand, employing predefined hard alignment paths constrains the model's search space to produce more natural-sounding speech.
The characteristics of these systems motivate us to design an approach that combines the advantages of both: we first provide a rough alignment to ***MegaTTS3*** and then use attention mechanisms in Transformer blocks to construct the fine-grained implicit alignment path.
The visualizations of the implicit alignment paths are included in Appendix~\ref{app:vis_diff_attn}.
In specific, denote the latent speech vector sequence as $z=[z_1, z_2, \cdots, z_n]$, the phoneme sequence as $p=[p_1, p_2, \cdots, p_m]$, and the phoneme duration sequence as $d=[d_1, d_2, \cdots, d_m]$, where $n$, $m$ is the length of the sequence.
The length of the speech vector that corresponds to a phoneme $p_i$ is the duration $d_i$.
Given $d=[2, 2, 3]$, the hard speech-text alignment path can be denoted as $a=[p_1, p_1, p_2, p_2, p_3, p_3, p_3]$.
To construct the rough alignment $\tilde{a}$, we randomly retain only one anchor for each phoneme: $\tilde{a} = [\underline{M}, p_1, p_2, \underline{M}, \underline{M}, \underline{M}, P_3]$, where $\underline{M}$ represents the mask token.
$\tilde{a}$ is downsampled with convolution layers to match the length of the latent sequence $z$.
Then, we directly concatenate the downsampled $\tilde{a}$ and $z$ along the channel dimension.
The anchors in $\tilde{a}$ provide ***MegaTTS3*** with approximate positional information for each phoneme, simplifying the learning process of speech-text alignment.
At the same time, the rough alignment information does not limit ***MegaTTS3***'s search space and also enables fine-grained control over each phoneme's duration.

</td><td>

</td></tr>
<tr><td>

**Piecewise Rectified Flow Acceleration**

We adopt **Piecewise Rectified Flow (PeRFlow)**[^53] to distill the pretrained ***MegaTTS3*** model into a more efficient generator.
Although our ***MegaTTS3*** is non-autoregressive in terms of the time dimension, it requires multiple iterations to solve the Flow ODE.
The number of iterations (i.e., number of function evaluations, NFE) has a great impact on inference efficiency, especially when the model scales up further.
Therefore, we adopt the PeRFlow technique to further reduce NFE by segmenting the flow trajectories into multiple time windows.
Applying reflow operations within these shortened time intervals, PeRFlow eliminates the need to simulate the full ODE trajectory for training data preparation, allowing it to be trained in real-time alongside large-scale real data during the training process.
Given number of windows $K$, we divide the time $t\in[0,1]$ into $K$ time windows $\{ (t_{k-1}, t_{k}] \}^{K}_{k=1}$.
Then, we randomly sample $k\in\{1,\cdots,K\}$ uniformly.
We use the startpoint of the sampled time window $z_{t_{k-1}} = \sqrt{1 - \sigma^2(t_{k-1})} z_1 + \sigma(t_{k-1}) \epsilon$ to solve the endpoint of the time window $\hat{z}_{t_k} = \phi_{\theta}(z_{t_{k-1}}, t_{k-1}, t_{k})$, where $\epsilon \sim \mathcal{N}(0, I) $ is the random noise, $\sigma(t)$ is the noise schedule, and $\phi_{\theta}$ is the ODE solver of the teacher model.
Since $z_{t_{k-1}}$ and $\hat{z}_{t_{k}}$ is available, the student model $\hat{\theta}$ can be trained via the following objectives:
$$
\ell = \left\lVert v_{\hat{\theta}}(z_t, t) - \frac{\hat{z}_{t_k} - z_{t_{k-1}}}{t_k - t_{k-1}} \right\lVert^2,
$$

where $v_{\hat{\theta}}$ is the estimated drift force with parameter $\hat{\theta}$ and $t$ is uniformly sampled from $(t_{k-1},t_{k}]$.
We provide details of PeRFlow training for ***MegaTTS3*** in Appendix~\ref{app:details_perflow_training}.

</td><td>

</td></tr>
<tr><td>

**Multi-condition Classifier-Free Guidance (CFG)**

We employ classifier-free guidance approach[^54] to steer the model $g_{\theta}$'s output towards the conditional generation $g_{\theta}(z_t,c)$ and away from the unconditional generation $g_{\theta}(z_t,\varnothing)$:
$$
\hat{g}_{\theta}(z_t, c) = g_{\theta}(z_t, \varnothing) + \alpha \cdot \left[ g_{\theta}(z_t, c) - g_{\theta}(z_t, \varnothing) \right],
$$

where $c$ denotes the conditional state, $\varnothing$ denotes the unconditional state, and $\alpha$ is the guidance scale selected based on experimental results.
Unlike standard classifier-free guidance, ***MegaTTS3***'s conditional states $c$ consist of two components: phoneme embeddings $p$ and the speaker prompt $z_{prompt}$.
In the experiments, as the text guidance scale increases, we observe that the pronunciation changes according to the following pattern: 1) starting with improper pronunciation; 2) then shifting to pronouncing with the current speaker's accent; 3) and finally approaching the standard pronunciation of the target language.
The detailed experimental setup is described in Appendix~\ref{app:additional_detials_for_mt_cfg}.
This allows us to use the text guidance scale $\alpha_{txt}$ to control the accent intensity.
At the same time, the speaker guidance scale $\alpha_{spk}$ should be a relatively high value to ensure a high speaker similarity.
Therefore, we adopt the multi-condition classifier-free guidance technique to separately control $\alpha_{txt}$ and $\alpha_{spk}$:
$$
\begin{split}
\hat{g}_{\theta}(z_t, p, z_{prompt}) = & \alpha_{spk} \left[ g_{\theta}(z_t, p, z_{prompt}) - g_{\theta}(z_t, p, \varnothing) \right] \\
& + \alpha_{txt} \left[ g_{\theta}(z_t, p, \varnothing) - g_{\theta}(z_t, \varnothing, \varnothing) \right] \\
& + g_{\theta}(z_t, \varnothing, \varnothing) \\
\end{split}
$$

In training, we randomly drop condition $z_{prompt}$ with a probability of $p_{spk} = 0.10$.
Only when $z_{prompt}$ is dropped, we randomly drop condition $p$ with a probability of 50\%.
Therefore, our model is able to handle all three types of conditional inputs described in Equation~\ref{eq:5}.
We select the guidance scale $\alpha_{spk}$ and $\alpha_{txt}$ based on experimental results.

</td><td>

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

In this subsection, we describe the datasets, training, inference, and evaluation metrics.
We provide the model configuration and detailed hyper-parameter setting in Appendix~\ref{app:model_config}.

</td><td>

</td></tr>
<tr><td>

**Datasets**

We train ***MegaTTS3*** on the LibriLight[^55] dataset, which contains 60k hours of unlabeled speech derived from LibriVox audiobooks.
All speech data are sampled at 16KHz.
We transcribe the speeches using an internal ASR system and extract the predefined speech-text alignment using the external alignment tool[^56].
We utilize three benchmark datasets:
(1) the **LibriSpeech**[^29] test-clean set following **NaturalSpeech3**[^09] for zero-shot TTS evaluation;
(2) the LibriSpeech-PC test-clean set following F5-TTS[^36] for zero-shot TTS evaluation;
(3) the L2-arctic dataset[^57] following[^37], [^44] for accented TTS evaluation.

</td><td>

</td></tr>
<tr><td>

**Training and Inference**

We train the WaveVAE model and ***MegaTTS3*** on 8 NVIDIA A100 GPUs.
The batch sizes, optimizer settings, and learning rate schedules are described in Appendix~\ref{app:model_config}.
It takes 2M steps for the WaveVAE model's training and 1M steps for ***MegaTTS3***'s training until convergence.
The pre-training of ***MegaTTS3*** requires 800k steps and PeRFlow distillation requires 200k steps.

</td><td>

</td></tr>
<tr><td>

**Objective Metrics**

(1) For zero-shot TTS, we evaluate speech intelligibility using the word error rate (WER) and speaker similarity using SIM-O (**NaturalSpeech3**[^09]).
To measure SIM-O, we utilize the WavLM-TDCNN speaker embedding model\footnote{\url{https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification}} to calculate the cosine similarity score between the generated samples and the prompt.
As SIM-R (**Voicebox**[^06]) is not comparable across baselines using different acoustic tokenizers, we recommend focusing on SIM-O in our experiments.
The similarity score is in the range of $\left[-1,1\right]$, where a higher value indicates greater similarity.
In terms of WER, we use the publicly available HuBERT-Large model[^58], fine-tuned on the 960-hour LibriSpeech training set, to transcribe the generated speech.
The WER is calculated by comparing the transcribed text to the original target text.
All samples from the test set are used for the objective evaluation;
(2) For accented TTS, we evaluate the Mel Cepstral Distortion (MCD) in dB level and the moments (standard deviation ($\sigma$), skewness ($\gamma$) and kurtosis ($\kappa$))[^59], [^60] of the pitch distribution to evaluate whether the model accurately captures accent variance.

</td><td>

</td></tr>
<tr><td>

**Subjective Metrics**

We conduct the MOS (mean opinion score) evaluation on the test set to measure the audio naturalness via Amazon Mechanical Turk.
We keep the text content and prompt speech consistent among different models to exclude other interference factors.
We randomly choose 40 samples from the test set of each dataset for the subjective evaluation, and each audio is listened to by at least 10 testers.
We analyze the MOS in three aspects: CMOS (quality, clarity, naturalness, and high-frequency details), SMOS (speaker similarity in terms of timbre reconstruction and prosodic pattern), and ASMOS (accent similarity).
We tell the testers to focus on one corresponding aspect and ignore the other aspect when scoring.

</td><td>

</td></tr></table>

## 5·Results: 结果

### Results of Zero-Shot Speech Synthesis

<table><tr><td width="50%">

**Evaluation Baselines**

We compare the zero-shot speech synthesis performance of ***MegaTTS3*** with 11 strong baselines, including:
(1) **VALL-E 2**[^20];
(2) **Voicebox**[^06];
(3) **DiTTo-TTS**[^07];
(4) **NaturalSpeech3**[^09];
(5) CosyVoice[^21];
(6) MaskGCT[^61];
(7) F5-TTSa[^36];
(8) **E2 TTS**[^08].

Explanation and details of the selected baseline systems are provided in Appendix~\ref{app:detail_zs_tts_baseline}.

</td><td>

</td></tr>
<tr><td>

**Analysis**

As shown in Table~\ref{table:en_zs_tts}, we can see that
(1) ***MegaTTS3*** achieves state-of-the-art SIM-O, SMOS, and WER scores, comparable to NaturalSpeech 3 (the counterpart with forced alignment), and significantly surpasses other baselines without explicit alignments.
The improved SIM-O and SMOS suggest that the proposed sparse alignment effectively simplifies the text-to-speech mapping challenge like predefined forced duration information, allowing the model to focus more on learning timbre information.
And the improved WER indicates that ***MegaTTS3*** also enjoys strong robustness;

(2) ***MegaTTS3*** significantly surpasses all baselines in terms of CMOS, demonstrating the effectiveness of the proposed sparse alignment strategy;

(3) After the PeRFlow acceleration, the student model of ***MegaTTS3*** shows on par quality with the teacher model and enjoys fast inference speed.

We also conduct the experiments on the LibriSpeech-PC test-clean set provided by F5-TTS and the results are shown in Table~\ref{table:en_zs_tts_valle}, which also demonstrates that our method achieves state-of-the-art performance in terms of speaker similarity and speech intelligibility.
The duration controllability of ***MegaTTS3*** is verified in Appendix~\ref{app:dur_contol}.
In the demo page, we also demonstrate that our method can maintain high naturalness even when the performance of the duration predictor is suboptimal (while ***MegaTTS3*** with forced alignment fails).

</td><td>

</td></tr></table>

### Experiments of Prosodic Naturalness

<table><tr><td width="50%">

We also measure the objective metrics MCD, SSIM, STOI, GPE, VDE, and FFE following InstructTTS[^62] to evaluate the prosodic naturalness of our method.
The results are presented in Table~\ref{table:expressiveness_exp_40}.
Specifically, our method with sparse alignment (Ours w/ S.A.) achieves the best performance across all metrics, with an MCD of 4.42, GPE of 0.31, VDE of 0.29, and FFE of 0.34.
These results indicate a significant improvement in prosodic naturalness compared to the baseline NaturalSpeech 3 and our method with forced alignment (Ours w/ F.A.), further validating the effectiveness of our sparse alignment strategy.
Our method provides a noval and effective solution for speech synthesis applications that require high robustness and exceptional expressiveness, such as audiobook narration and virtual assistants.

</td><td>

</td></tr></table>

### Results of Accented TTS

<table><tr><td width="50%">

In this subsection, we evaluate the accented TTS performance of our model on the L2-ARCTIC dataset[^57].
This corpus includes recordings from non-native speakers of English whose first languages are Hindi, Korean, etc.
In this experiment, we focus on verifying whether our model and baseline can synthesize natural speech with different accent types (standard English or English with specific accents) while maintaining consistent vocal timbre.
We compare our ***MegaTTS3*** model with CTA-TTS[^44].
More details of the baseline model are provided in Appendix~\ref{app:detail_accented_tts_experiment}.

(1) First, we evaluate whether the models can synthesize high-quality speeches with accents.
As shown in Table~\ref{table:accent-tts-result}, our ***MegaTTS3*** model significantly outperforms the CTA-TTS baseline in terms of the subjective accent similarity MOS core, the MCD (dB) values, and the statistical moments ($\sigma$, $\gamma$, and $\kappa$) of pitch distributions.
These results demonstrate the superior accent learning capability of ***MegaTTS3*** compared to the baseline system.
Besides, the ***MegaTTS3*** model achieves higher CMOS and SMOS scores compared to CTA-TTS, indicating a significant improvement in speech quality and speaker similarity;

(2) Secondly, we evaluate whether the models can accurately control the accent types of the generated speeches.
We follow CTA-TTS to conduct the intensity classification experiment[^44].
At run-time, we generate speeches with two accent types, and the listeners are instructed to classify the perceived accent categories, including "standard" and "accented".
Figure~\ref{exp:accent_confusion_matrices} shows that our ***MegaTTS3*** significantly surpasses CTA-TTS in terms of accent controllability.

</td><td>

</td></tr></table>

### Evaluation of WaveVAE

<table><tr><td width="50%">

First, we evaluate the reconstruction quality of the WaveVAE model, with results presented in Table~\ref{app:table_recon_speech_compression}.
We report the objective metrics, including Perceptual Evaluation of Speech Quality (PESQ), Virtual Speech Quality Objective Listener (ViSQOL), and Mel-Cepstral Distortion (MCD).
We select the following codec models as baselines:
(1) **EnCodec**[^63], a representative and pioneering work in the field of speech codec;
(2) **DAC**[^22], a high-bitrate audio codec model with high reconstruction quality;
(3) **WavTokenizer**[^45], a low-bitrate speech codec model that focuses more on perceptual reconstruction quality;
(4) X-codec2[^64], a low-bitrate speech codec model, leveraging the representations of a pre-trained model to further enhance overall quality.

The results demonstrates that, despite applying higher compression rate, our solution achieves superior performance on various reconstruction metrics, such as MCD and ViSQOL.

Second, to demonstrate the impact of different speech compression models on the overall performance of the TTS system, we extracted the latents from Encodec and DAC, respectively, for training our ***MegaTTS3*** model.
We report the experimental results in Table~\ref{app:table_zs_tts_different_codec}.
It can be seen that our method outperforms "w/ DAC" and "w/ Encodec", due to the fact that the latent space of our speech compression model is more compact (only 25 tokens per second).
The results demonstrate the importance of our WaveVAE, a high-compression, high-reconstruction-quality speech codec model, for TTS systems.
This conclusion is also verified by a previous work[^07], which shows compact target latents facilitate learning in diffusion models.

</td><td>

</td></tr></table>

### Ablation Studies

<table><tr><td width="50%">

We test the following four settings:
(1) w/FA, which replaces the sparse alignment in ***MegaTTS3*** with forced alignment used in **Voicebox**[^06], **NaturalSpeech2**[^05];
(2) w/o A., we do not use the predefined alignments and modeling the duration information implicitly;
(3) w/ CFG, we use the standard CFG following the common practice in Diffusion-based TTS;
(4) w/o CFG, we do not use the CFG mechanism.

All tests follow the experimental setup described in Section~\ref{exp_zero-shot}.
The results are shown in Table~\ref{table:ablation_alignments_cfg}.
For settings (1) and (2), it can be observed that both forced alignment and sparse alignment can enhance the performance of speech synthesis models.
However, compared to forced alignment, sparse alignment does not constrain the model's search space, leading to a prosodic naturalness (see Section~\ref{exp:prosodic_naturalness}).
Therefore, the sparse alignment strategy achieves $+0.17$ CMOS compared to the forced alignment strategy.
For setting (3), compared with the standard CFG, our multi-condition CFG performs slightly better as it allows for flexible control over the weights between the text prompt and the speaker prompt.
Setting (4) proves that the CFG mechanism is crucial for ***MegaTTS3***.
Additionally, we visualize the attention score matrices from different transformer layers in ***MegaTTS3*** in Appendix~\ref{app:vis_diff_attn}, leading to some interesting observations.

</td><td>

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

In this paper, we introduce ***MegaTTS3***, a zero-shot TTS framework that leverages novel sparse alignment boundaries to ease the difficulty of alignment learning while retaining the naturalness of the generated speeches.
This strategy allows ***MegaTTS3*** to combine the strengths of methods with both implicit alignments and predefined hard alignments.
Additionally, we employ the PeRFlow technique to further accelerate the generation process and design a multi-condition CFG strategy to offer more flexible control over accents.
Experimental results show that ***MegaTTS3*** achieves state-of-the-art zero-shot TTS speech quality while maintaining a more efficient pipeline.
Moreover, the sparse alignment strategy also shows enhanced prosodic naturalness and higher robustness against a suboptimal duration predictor.
Due to space constraints, further discussions are provided in the appendix.

</td><td>

</td></tr></table>

## Appendix

### A·Detailed Experimental Settings

<table><tr><td width="50%">

#### Model Configuration

- **The WaveVAE model** consists of a VAE encoder, a wave decoder, and discriminators;
The VAE encoder follows the architecture used in **WavTokenizer**[^45].
The wave decoder consists of a convolutional upsampler and a **HiFi-GAN** decoder[^46].
The latent channel size is set to 32.
The weight of the KL loss is set to $1\times10^{-3}$, which only imposes a slight KL penalty on the learned latent.
In training, we use batches of fixed length, consisting of 72,000 wavform frames, with a batch size set to 40 for each GPU.
We use the Adam optimizer with a learning rate of $1\times10^{-4}$, $\beta_{1} = 0.9$, $\beta_{2} = 0.999$, and 10K warmup steps.

- **The MegaTTS 3 model** use the standard transformer block from LLAMA[^51] as the basic structure, which comprises a 24-layer Transformer with 16 attention heads and 1024 embedding dimensions.
It contains 339M parameters in total.
We adopt the Rotary Position Embedding (RoPE)[^52] as the positional embedding following the common practice in LLAMA implementations.
For simplicity, we do not use the phoneme encoder and style encoder like previous works.
We only use a linear projection layer to transform these features to the same dimension.
During training, we use 8 A100 80GB GPUs with a batch size of 10K latent frames per GPU for 1M steps.
We use the Adam optimizer with a learning rate of $5\times10^{-5}$, $\beta_{1} = 0.9$, $\beta_{2} = 0.999$, and 10K warmup steps.
In zero-shot TTS experiments, we set the text guidance scale $\alpha_{txt}$ and the speaker guidance scale $\alpha_{spk}$ to 2.5 and 3.5, respectively.
In accented TTS experiments, we set $\alpha_{spk}=6.5$, $\alpha_{txt}=1.5$ to generate the accented speech and set $\alpha_{spk}=2.0$, $\alpha_{txt}=5.0$ to generate the speech with standard English.

</td><td>

</td></tr>
<tr><td>

#### Random Seeds

We ran objective experiments 10 times with 10 different random seeds and obtained the averaged results.
The chosen random seeds are [4475, 5949, 6828, 6744, 3954, 3962, 6837, 1237, 3824, 3163].

</td><td>

</td></tr>
<tr><td>

#### Sampling Strategy

For MegaTTS 3, we applied the Euler sampler with a fixed step size following the common practice in flow ODE sampling.
We use 25 and 8 sampling steps for \textit{MegaTTS 3} and \textit{MegaTTS 3-accelerated}, respectively.

</td><td>

</td></tr>
<tr><td>

#### Details about Zero-Shot TTS Baselines

In this subsection, we provide the details about the baselines in our zero-shot TTS experiments:

- **VALL-E 2**[^20], based on VALL-E, introduces Repetition Aware Sampling to stabilize the decoding process and proposes the Grouped Code Modeling to effectively address the challenges of long sequence modeling.

- **VoiceBox**[^06] is a non-autoregressive flow-matching model designed to infill mel-spectrograms based on provided speech context and text.
We obtained the samples by contacting the authors.

- **DiTTo-TTS**[^07] addresses
the challenge of text-speech alignment via cross-attention mechanisms with the prediction of the total length of speech representations.
We directly obtain the results of objective evaluations from their paper.

- **NaturalSpeech3**[^09] designs a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details and propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt.
We obtained the samples by contacting the authors.

- **CosyVoice**[^21] utilizes an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis.
We use the official code and the model snapshot named ``CosyVoice-300M'' in our experiments\footnote{\url{https://github.com/FunAudioLLM/CosyVoice}}.

- **MaskGCT**[^61] proposes a fully non-autoregressive codec-based TTS model that eliminates the need for explicit alignment information between text
and speech supervision, as well as phone-level duration prediction.
We directly obtain the results of objective evaluations from their paper.

- **F5-TTS**[^36] proposes a fully non-autoregressive text-to-speech system
based on flow matching with Diffusion Transformer (DiT).
We use the official code and pretrained model in our experiments\footnote{\url{https://github.com/SWivid/F5-TTS}}.

- **E2 TTS**[^08] proposes an easy non-autoregressive zero-shot TTS system, that offers human-level naturalness and state-of-the-art speaker similarity and intelligibility.
We use the code implemented by F5-TTS authors in our experiments\footnote{\url{https://github.com/SWivid/F5-TTS}}.
\end{itemize}

The evaluation is conducted on a server with 1 NVIDIA V100 GPU and batch size 1.
RTF denotes the real-time factor, i.e., the seconds required for the system (together with the vocoder) to synthesize one-second audio.

</td><td>

</td></tr>
<tr><td>

#### Details about the Accented TTS Baseline

CTA-TTS[^44] is a TTS framework that uses a phoneme recognition model to quantify the accent intensity in phoneme level for accent intensity control.
CTA-TTS first trains the phoneme recognition model on the standard pronunciation LibriSpeech dataset, and then uses the output probability distribution of the model to assess the accent intensity and create accent labels on the accented L2Arctic dataset.
These labels were input into the TTS model to enable control over accent intensity.

Systems like CTA-TTS require precise accent annotations during training, so we trained them on the L2-ARCTIC dataset.
However, our model does not require accent annotations and learns different accent patterns from large-scale data, using only the multi-condition CFG mechanism to achieve accent intensity control.
Therefore, we directly compare the zero-shot results of our model with the baselines, which is a more challenging task.

</td><td>

</td></tr>
<tr><td>

#### Details in Subjective Evaluations

We conduct evaluations of audio quality, speaker similarity, and accent similarity on Amazon Mechanical Turk (MTurk).
We inform the participants that the data will be utilized for scientific research purposes.
For each dataset, 40 samples are randomly selected from the test set, and the TTS systems are then used to generate corresponding audio samples.
Each audio sample is listened to by a minimum of 10 listeners.
For CMOS, following the approach of [^65], listeners are asked to compare pairs of audio generated by systems A and B and indicate their preference between the two.
They are then asked to choose one of the following scores: 0 indicating no difference, 1 indicating a slight difference, 2 indicating a significant difference and 3 indicating a very large difference.
We instruct listeners to ``\textit{Please focus on speech quality, particularly in terms of clarity, naturalness, and high-frequency details, while disregarding other factors}''.
For SMOS and ASMOS, each participant is instructed to rate the sentence on a 1-5 Likert scale based on their subjective judgment.
For speaker similarity evaluations (SMOS), we instruct listeners to ``\textit{Please focus solely on the timbre and prosodic similarity between the reference speech and the generated speech, while disregarding differences in content, grammar, audio quality, and other factors}''.
For accent similarity evaluations (ASMOS), we instruct listeners to ``\textit{Please focus solely on the accent similarity between the ground-truth speech and the generated speech, while disregarding other factors}''.
The screenshots of instructions for testers are shown in Figure~\ref{screenshots_subjective_evaluations}.
Additionally, we insert audio samples with known quality levels (e.g., reference recordings with no artifacts or intentionally corrupted audio with noticeable distortions) into the evaluation set to verify whether evaluators are attentive and professional.
We also randomly repeat some audio clips in the evaluation set to check whether evaluators provide consistent ratings for the same sample.
If large deviations in scores (larger than 1.0) for repeated clips occurs, we will select a new rater to evaluate this audio clip.
We paid \$8 to participants hourly and totally spent about \$500 on participant compensation.

</td><td>

</td></tr></table>

### B·Classifier-Free Guidance Used in Zero-Shot TTS

<table><tr><td width="50%">

Classifier-Free Guidance (CFG)[^54] is a technique that balances sample fidelity and mode coverage in diffusion models by combining the score estimates from both a conditional and an unconditional model.
The unconditional model is trained alongside the conditional model by randomly omitting the conditioning variable $c$ with a certain probability, allowing the same model to provide score estimates for both $p(x)$ and $p(x|c)$.
In large-scale zero-shot TTS, **Voicebox**[^06] and **NaturalSpeech2**[^05] achieve CFG mechanism by dropping the text and prompt speech features.
However, these works overlook that text and timbre should be controlled separately.
Inspired by VoiceLDM[^66] that introduces separate control of environmental conditions and speech contents, a concurrent work[^67] proposes separately controlling the speaker fidelity and text intelligibility.
However, this work is limited to improving the audio quality of TTS and does not explore the impact of CFG on accent.

</td><td>

</td></tr></table>

### C·Details of PeRFlow Training Procedure

<table><tr><td width="50%">

Once the pretrained ODE solver of the teacher model $\phi_{\theta}$ is available, we perform the PeRFlow technique to train an accelerated solver in real time.
When training, we only consider the shortened segments of the ODE trajectories, reducing the computational load of inference for the teacher model at each training step, and accelerating the training process.

At each training step, given a data sample $z_1$ and a sample $z_0$ drawn from the source distribution (in this case, $z_0 \sim \mathcal{N}(0, I)$, i.e., Gaussian distribution), we randomly select a time window $(t_{k-1}, t_{k}]$ and compute the standpoint of the segmented probability path $z_{t_{k-1}} = \sqrt{1 - \sigma^2(t_{k-1})} z_1 + \sigma(t_{k-1}) z_0$, where $K$ is a hyperparameter indicating the total number of segments, $k\in\{1,\cdots,K\}$, $t_k = k / K$, and $\sigma(t)$ is the noise schedule.
The teacher solver only needs to infer the endpoint of this segmented path, $\hat{z}_{t_k} = \phi_{\theta}(z_{t_{k-1}}, t_{k-1}, t_{k})$, with a remarkably smaller number of iterations $\widehat{T}$, comparing to that of a full trajectory, $T$.
Finally, the student model is optimized on the segmented trajectory from $z_{t_{k-1}}$ to $\hat{z}_{t_k}$.
We set $T$ to 25 and $\widehat{T}$ to 8, achieving a non-negligible acceleration of the training process.

</td><td>

</td></tr></table>

### D·Details about Data and Model Scaling Experiments

<table><tr><td width="50%">

**Training Corpus**

The data/model scalability is crucial for practical TTS systems.
To evaluate the scalability of MegaTTS 3 in Section~\ref{exp:ablation_studies}, we construct a 600kh internal multilingual training corpus comprising both English and Chinese speech.
Most of the audiobook recordings are crawled from YouTube and online podcasts like novelfm\footnote{\url{https://novelfm.changdunovel.com/}}.
We also include the academic datasets like LibriLight[^55], WenetSpeech[^68], and GigaSpeech[^69].
Since the crawled corpus may contain unlabelled speeches.
We transcribe them using an internal ASR model.

</td><td>

</td></tr>
<tr><td>

**Test Set**

Most prior studies of zero-shot TTS evaluate performances using the reading-style LibriSpeech test set, which may be different from real-world speech generation scenarios.
In section~\ref{exp:ablation_studies}, we evaluate our model using the test sets collected from various sources, including:
(1) CommonVoice[^70], a large voice corpus containing noisy speeches from various scenarios;
(2) RAVDESS[^71], an emotional TTS dataset featuring 8 emotions and 2 emotional intensity.
We follow **NaturalSpeech3**[^09] and use strong-intensity samples to validate the model’s ability to handle emotional variance;
(3) LibriTTS[^72], a high-quality speech corpus;
(4) we collect samples from videos, movies, and animations to test whether our model can simulate timbres with distinctly strong individual characteristics.
The test set consists of 40 audio samples extracted from each source.

</td><td>
</td></tr>
<tr><td>

**Experimental Setup**

We scale up MegaTTS 3 from 0.5B to 7.0B following the hyper-parameter settings in Qwen 2[^73].
In this experiment, we only increase the parameters of the MegaTTS 3 model to verify its scalability.
The parameters of the speech compression VAE remained unchanged.
In theory, expanding the parameters of both models could yield the optimal results, which we leave for future work.

</td><td>
</td></tr>
<tr><td>

**Speech-Text Alignment Labels for Large-Scale Data**

Training an MFA model directly on a 600k-hour dataset is impractical.
Therefore, we randomly sampled a 10k-hour subset from the dataset to train a robust MFA model, which is then used to align the full dataset.
Since data processing inherently requires some alignment model (such as an ASR model) for speech segmentation, using a pretrained MFA model for alignment extraction does not limit the system's data scalability.

</td><td>
</td></tr>
<tr><td>

**Results**

We evaluate the effectiveness of data and model scaling for the proposed MegaTTS 3 model.
In this experiment, we train models with 0.5B parameters on multilingual internal datasets with data sizes of 2kh, 40kh, 200kh, and 600kh, respectively.
We also train models with 0.5B, 1.5B, and 7.0B parameters on the 600kh dataset.
We evaluate the zero-shot TTS performance in terms of speaker similarity (Sim-O) and speech intelligibility (WER) on an internal test set consisting of 400 speech samples from various sources.
Based on Table~\ref{table:ablation_scalability}, we conclude that:
(1) as the data size increases from 2kh to 600kh, both the model's speaker similarity and speech intelligibility improve consistently, demonstrating strong data scalability of our model;
(2) as the model size scales from 0.5B to 7.0B parameters, SIM-O improves by 12.1\% and WER decreases by 9.52\%, validating the model scalability of MegaTTS 3.
Additionally, we find that increasing the model parameters enhances its para-linguistic capabilities, with specific audio examples available on the demo page.

</td><td>

</td></tr></table>

### E·Duration Controllability of MegaTTS3

<table><tr><td width="50%">

In this section, we aim to verify MegaTTS 3's duration control capabilities through case studies.
We randomly selected a speech prompt from the test set and used the sentence ``Notably, raising questions about both the size of the perimeter and efforts to sweep and secure.'' as the target sentence to generate speeches.
In the generation process, we first control the sentence-level duration by multiplying the time coordinates of the phoneme anchors described in Section~\ref{method:sec_3_2} by a fixed value.
As shown in Figure~\ref{app:vis_sent_dur_control}, our MegaTTS 3 demonstrates good sentence-level duration control.
Moreover, our MegaTTS 3 is also capable of fine-grained phoneme-level duration control.
As illustrated in Figure~\ref{app:vis_ph_dur_control}, we multiplied the anchor coordinates of the phoneme within the red box by a fixed value while keeping the relative positions of other phoneme anchors unchanged.
The figure shows that our MegaTTS 3 also exhibits good fine-grained phoneme-level duration controllability.

</td><td>

</td></tr></table>

### F·Visualization of Attention Matrices

<table><tr><td width="50%">

We visualize the attention matrices from all layers in the 1.4B MegaTTS 3 model, using 8 sampling steps.
From Figure~\ref{app:vis_attn}, we observe:
(1) within the same layer, despite different timesteps, the attention matrices remain identical.
In other words, the function of each layer stays consistent across timesteps;
(2) the functions of the transformer layers can be categorized into three types.
As shown in Figure~\ref{app:vis_attn} (a), the bottom layers handle text and audio feature extraction; in Figure~\ref{app:vis_attn} (b), the middle layers focus on speech-text alignment; and in Figure~\ref{app:vis_attn} (c), the top layers refine the target latent features.

</td><td>

</td></tr></table>

### G·About Different Lengths of Context

<table><tr><td width="50%">

An imbalanced distribution of prompt and target lengths during training can lead to unstable generation performance during inference.
For example, if the majority of the sampled data during training consists of 20-second targets, the generation performance for audio with a 40-second target will be worse than that of 20-second targets in inference.
To solve the imbalanced distribution issue, we recommend using the following multi-sentence data sampling strategy: we concatenate all audio recordings of the same speaker in the dataset in time order, and then randomly extract audio segments of length $t\sim U(t_{min}, t_{max})$ from the concatenated audio, where $t_{min}$ is the minimum sampling time and $t_{max}$ is the maximum sampling time.
Then, following Section~\ref{method:main_arch}, we randomly divide the sampled sequence into a prompt region and a target region.
Although we do not use this strategy in our experiments in order to make a fair comparison with other methods, this strategy is effective in practical scenarios.

</td><td>

</td></tr></table>

### H·Experiments of Prosodic Naturalness for Zero-Shot TTS

<table><tr><td width="50%">

We also conduct the ablation studies using the objective metrics MCD, SSIM, STOI, GPE, VDE, and FFE following InstructTTS[^62] to evaluate the prosodic naturalness of our proposed method.
We conduct experiments on the LibriSpeech test-clean 2.2-hour subset (following the setup in VALL-E 2 and Voicebox).
The results are shown in the Table below.
We compare MegaTTS 3 with the following baselines:
(1) ``Ours w/ Forced Alignment'', we replace the sparse alignment with the forced alignment;
(2) ``Ours w/ Standard CFG'', we replace the multi-condition CFG with standard CFG;
(3) ``Ours w/ Standard AR Duration'', we replace the duration from F-LM with the duration from standard AR duration predictor following **SimpleSpeech2**[^11].
The results in Table~\ref{table:expressiveness_exp_libritest_all} show that sparse alignment brings significant improvements, and both multi-condition CFG and F-LM duration contribute positively to the performance.

</td><td>

</td></tr></table>

### I·Experiments with Longer Samples

<table><tr><td width="50%">

To directly compare MegaTTS 3's robustness to long sequences against other AR models, we have conducted experiemnts for a test set with longer samples.
Specifically, we randomly select 10 sentences, each containing more than 50 words.
For each speaker in the LibriSpeech test-clean set, we randomly chose a 3-second clip as a prompt, resulting in 400 target samples in total.
To make our results more convincing, we include strong-performing TTS models, VoiceCraft[^74] and CosyVoice (AR+NAR)[^21], as our baselines.
The results for longer samples are presented in Table~\ref{table:cmp_longer_samples}.
As shown, compared to the baseline systems, MegaTTS 3 does not exhibit a significant decline in speech intelligibility when generating longer sentences, illustrating the effectiveness of the combination of F-LM and MegaTTS 3.

</td><td>

</td></tr></table>

### J·Experiments with Hard Sentences

<table><tr><td width="50%">

The transcriptions on the LibriSpeech test-clean set are relatively simple since they come from audiobooks.
To further indicate the speech intelligibility of different methods, we evaluate our model on the challenging set containing 100 difficult textual patterns from **ELLA-V**[^03].
Since the speech prompts used by ELLA-V are not publicly available, we randomly sample 3-second-long speeches in the LibriSpeech test-clean set as speech prompts.
For this evaluation, we used the official checkpoint of F5-TTS[^36] and the **E2 TTS**[^08] inference API provided on F5-TTS's Hugging Face page.
We employ Whisper-large-v3 for WER calculation.
Based on the results presented in Table~\ref{table:cmp_hard_sentences}, our model shows stronger robustness against hard transcriptions.

</td><td>

</td></tr></table>

### K·Additional Details for Multi-Condition CFG

<table><tr><td width="50%">

In Section~\ref{method:sec_3_2}, regarding the multi-condition CFG technique, the experimental setup for the preliminary experiment for accent control is: fixing $\alpha_{spk}$ at 2.5 and varying $\alpha_{txt}$ from 1.0 to 6.0.
Specifically, as $\alpha_{txt}$ increases from 1.0 to 1.5, the generated speeches contains improper pronunciations and distortions.
When $\alpha_{txt}$ ranges from 1.5 to 2.5, the pronunciations align with the speaker's accent.
Finally, once $\alpha_{txt}$ exceeds 4.0, the generated speech converges toward the standard pronunciation of the target language.
Notably, the optimal values for parameters $\alpha_{txt}$ and $\alpha_{spk}$ may vary across different models.
The values reported here are specific to the model used in our experiments.

</td><td>

</td></tr></table>

### L·Ethics Statement

<table><tr><td width="50%">

The proposed model, MegaTTS 3, is designed to advance zero-shot TTS technologies, making it easier for users to generate personalized speech.
When used responsibly and legally, this technique can enhance applications such as movies, games, podcasts, and various other services, contributing to increasing convenience in everyday life.
However, we acknowledge the potential risks of misuse, such as voice cloning for malicious purposes.
To mitigate this risk, solutions like building a corresponding deepfake detection model will be considered.
Additionally, we plan to incorporate watermarks and verification methods for synthetic audio to ensure ethical use in real-world applications.
Restrictions will also be included in the licensing of our project to further prevent misuse.
By addressing these ethical concerns, we aim to contribute to the development of responsible and beneficial AI technologies, while remaining conscious of the potential risks and societal impact.

</td><td>

</td></tr></table>

### M·Reproducibility Statement

<table><tr><td width="50%">

We have taken several steps to ensure the reproducibility of the experiments and results presented in this paper:
(1) the architecture and algorithm of the MegaTTS 3 model are described in Section~\ref{method} and and relevant hyperparameters are fully described in Appendix~\ref{app:model_config};
(2) The evaluation metrics, including WER, SIM-O, MCD (dB), the moments of the pitch distribution, alignment error, CMOS, SMOS, and ASMOS, are described in detail in Section~\ref{Experimental_Setup};
(3) For most of the key experiments, we utilize publicly available datasets such as LibriLight, LibriSpeech, and L2Arctic.
The selection of the test sets is identical to that used in previous zero-shot TTS research.
However, as the publicly available datasets are insufficient for our data scaling experiments, we construct a larger dataset, which is described in detail in Appendix~\ref{app:data_model_scaling};
(4) To ensure reproducibility of the results, we have carefully set random seeds in our experiments and the random seeds are provided in Appendix~\ref{app:random_seeds}.
All objective results reported are based on the average performance across multiple runs.

</td><td>

</td></tr></table>

## References: 参考文献

[^01]: [VALL-E](../SpeechLM/ST2S/2023.01.05_VALL-E.md)
[^02]: [SPEAR-TTS](../SpeechLM/ST2S/2023.02.07_SPEAR-TTS.md)
[^03]: [ELLA-V](../SpeechLM/ST2S/2024.01.14_ELLA-V.md)
[^04]: [RALL-E](../SpeechLM/ST2S/2024.04.04_RALL-E.md)
[^05]: [NaturalSpeech2](2023.04.18_NaturalSpeech2.md)
[^06]: [Voicebox](2023.06.23_Voicebox.md)
[^07]: [DiTTo-TTS](2024.06.17_DiTTo-TTS.md)
[^08]: [E2 TTS](2024.06.26_E2_TTS.md)
[^09]: [NaturalSpeech3](2024.03.05_NaturalSpeech3.md)
[^10]: [SimpleSpeech](2024.06.04_SimpleSpeech.md)
[^11]: [SimpleSpeech2](2024.08.25_SimpleSpeech2.md)
[^12]: [Tacotron2](../Acoustic/2017.12.16_Tacotron2.md)
[^13]: [Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis](../_Full/2018.06.12_Transfer_Learning_from_Speaker_Verification_to_Multispeaker_Text-To-Speech_Synthesis.md)
[^14]: [TransformerTTS](../Acoustic/2018.09.19_TransformerTTS.md)
[^15]: [Glow-TTS](../Acoustic/2020.05.22_Glow-TTS.md)
[^16]: [FastSpeech](../Acoustic/2019.05.22_FastSpeech.md)
[^17]: [VITS](../E2E/2021.06.11_VITS.md)
[^18]: [Guided-TTS](../TTS2_Acoustic/2021.11.23_Guided-TTS.md)
[^19]: [Survey by Tan](../../Surveys/2021.06.29__Survey__A_Survey_on_Neural_Speech_Synthesis_(63P).md)
[^20]: [VALL-E2](../SpeechLM/ST2S/2024.06.08_VALL-E_2.md)
[^21]: [CosyVoice](../SpeechLM/ST2S/2024.07.07_CosyVoice.md)
[^22]: [DAC](../SpeechCodec/2023.06.11_Descript-Audio-Codec.md)
[^23]: [Survey by Wu](../../Surveys/2024.02.20__Survey__Towards_Audio_Language_Modeling_(5P).md)
[^24]: [Simple-TTS](2023.09.23_Simple-TTS.md)
[^25]: [E3 TTS](2023.11.02_E3_TTS.md)
[^26]: [Mapache](2024.03.18_Mapache.md)
[^27]: [FastSpeech2](../Acoustic/2020.06.08_FastSpeech2.md)
[^28]: [Seed-TTS](../SpeechLM/2024.06.04_Seed-TTS.md)
[^29]: [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md)
[^30]: [YourTTS](../E2E/2021.12.04_YourTTS.md)
[^31]: [Mega-TTS2](../SpeechLM/ST2S/2023.07.14_Mega-TTS2.md)
[^32]: [ARDiT](2024.06.08_ARDiT.md)
[^33]: [StyleTTS2](../SpeechLM/2023.06.13_StyleTTS2.md)
[^34]: [HierSpeech++](../E2E/2023.11.21_HierSpeech++.md)
[^35]: [MELLE](../SpeechLM/ST2S/2024.07.11_MELLE.md)
[^36]: [F5-TTS](2024.10.09_F5-TTS.md)
[^37]: melechovsky2022accented
[^38]: badlani2023multilingual
[^39]: zhou2024multi
[^40]: shah2024parrottts
[^41]: ma2023accent
[^42]: inoue2024macst
[^43]: zhong2024accentbox
[^44]: liu2024controllable
[^45]: [WavTokenizer](../SpeechCodec/2024.08.29_WavTokenizer.md)
[^46]: [HiFi-GAN](../Vocoder/2020.10.12_HiFi-GAN.md)
[^47]: [UnivNet](../Vocoder/2021.06.15_UnivNet.md)
[^48]: rombach2022high
[^49]: mao2017least
[^50]: liu2022flow
[^51]: [LLaMA3](../TextLM/2024.07.31_LLaMA3.md)
[^52]: [RoPE](../../Modules/PositionEmb/RoPE.md)
[^53]: [PeRFlow](../Flow/PeRFlow.md)
[^54]: [CFG](2022.07.26_Classifier-Free_Guidance.md)
[^55]: [Libri-Light](../../Datasets/2019.12.17_Libri-Light.md)
[^56]: mcauliffe2017montreal
[^57]: zhao2018l2arctic
[^58]: [HuBERT](../SpeechRepresentation/2021.06.14_HuBERT.md)
[^59]: andreeva2014differences
[^60]: niebuhr2019measuring
[^61]: [MaskGCT](../SpeechLM/ST2S/2024.09.01_MaskGCT.md)
[^62]: [InstructTTS](../Acoustic/2023.01.31_InstructTTS.md)
[^63]: [EnCodec](../SpeechCodec/2022.10.24_EnCodec.md)
[^64]: [LLaSA](../SpeechLM/2025.02.06_LLaSA.md)
[^65]: loizou2011speech
[^66]: [VoiceLDM](2023.09.24_VoiceLDM.md)
[^67]: [DualSpeech](2024.08.26_DualSpeech.md)
[^68]: [WenetSpeech](../../Datasets/2021.10.07_WenetSpeech.md)
[^69]: [GigaSpeech](../../Datasets/2021.06.13_GigaSpeech.md)
[^70]: [CommonVoice](../../Datasets/CommonVoice.md)
[^71]: livingstone2018ryerson
[^72]: [LibriTTS](../../Datasets/2019.04.05_LibriTTS.md)
[^73]: [Qwen2](../TextLM/Qwen2.md)
[^74]: [VoiceCraft](../SpeechLM/2024.03.25_VoiceCraft.md)
