# UnitSpeech

<details>
<summary>基本信息</summary>

- 标题: "UnitSpeech: Speaker-Adaptive Speech Synthesis with Untranscribed Data"
- 作者:
  - 01 Heeseung Kim
  - 02 Sungwon Kim
  - 03 Jiheum Yeom
  - 04 Sungroh Yoon
- 链接:
  - [ArXiv](https://arxiv.org/abs/2306.16083) v1
  - [Publication](https://doi.org/10.21437/Interspeech.2023-2326) InterSpeech2023Oral
  - [Github](https://github.com/gmltmd789/UnitSpeech)
  - [Demo](https://unitspeech.github.io/)
- 文件:
  - [ArXiv](_PDF/2306.16083v1__UnitSpeech__Speaker-Adaptive_Speech_Synthesis_with_Untranscribed_Data.pdf)
  - [Publication](_PDF/2306.16083p0__UnitSpeech__InterSpeech2023.pdf)

</details>

## Abstract: 摘要

We propose UnitSpeech, a speaker-adaptive speech synthesis method that fine-tunes a diffusion-based text-to-speech (TTS) model using minimal untranscribed data.
To achieve this, we use the self-supervised unit representation as a pseudo transcript and integrate the unit encoder into the pre-trained TTS model.
We train the unit encoder to provide speech content to the diffusion-based decoder and then fine-tune the decoder for speaker adaptation to the reference speaker using a single `<unit, speech>` pair.
UnitSpeech performs speech synthesis tasks such as TTS and voice conversion (VC) in a personalized manner without requiring model re-training for each task.
UnitSpeech achieves comparable and superior results on personalized TTS and any-to-any VC tasks compared to previous baselines.
Our model also shows widespread adaptive performance on real-world data and other tasks that use a unit sequence as input.

## 1.Introduction: 引言

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论