# F5-TTS

<details>
<summary>基本信息</summary>

- 标题: "F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching"
- 作者:
  - 01 Yushen Chen - Shanghai Jiao Tong University
  - 02 Zhikang Niu - Shanghai Jiao Tong University
  - 03 Ziyang Ma - Shanghai Jiao Tong University
  - 04 Keqi Deng - University of Cambridge
  - 05 Chunhui Wang - Geely Automobile Research Institute
  - 06 Jian Zhao - Geely Automobile Research Institute
  - 07 Kai Yu - Shanghai Jiao Tong University
  - 08 Xie Chen - Shanghai Jiao Tong University - chenxie95@sjtu.edu.cn
- 链接:
  - [ArXiv](https://arxiv.org/abs/2410.06885)
  - [Publication]
  - [Github](https://github.com/SWivid/F5-TTS)
  - [Demo](https://swivid.github.io/F5-TTS)
- 文件:
  - [ArXiv](_PDF/2410.06885v1__F5-TTS__A_Fairytaler_that_Fakes_Fluent_and_Faithful_Speech_with_Flow_Matching.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

This paper introduces ***F5-TTS***, a fully non-autoregressive text-to-speech system based on flow matching with Diffusion Transformer (DiT).
Without requiring complex designs such as duration model, text encoder, and phoneme alignment, the text input is simply padded with filler tokens to the same length as input speech, and then the denoising is performed for speech generation, which was originally proved feasible by E2 TTS.
However, the original design of E2 TTS makes it hard to follow due to its slow convergence and low robustness.
To address these issues, we first model the input with ConvNeXt to refine the text representation, making it easy to align with the speech.
We further propose an inference-time Sway Sampling strategy, which significantly improves our model's performance and efficiency.
This sampling strategy for flow step can be easily applied to existing flow matching based models without retraining.
Our design allows faster training and achieves an inference RTF of 0.15, which is greatly improved compared to state-of-the-art diffusion-based TTS models.
Trained on a public 100K hours multilingual dataset, our ***Fairytaler Fakes Fluent and Faithful speech with Flow matching (F5-TTS)*** exhibits highly natural and expressive zero-shot ability, seamless code-switching capability, and speed control efficiency.
Demo samples can be found at [this https URL](https://swivid.github.io/F5-TTS).
We release all code and checkpoints to promote community development ([Github](https://github.com/SWivid/F5-TTS)).

</details>
<br>

本论文介绍了 ***F5-TTS***, 一个基于流匹配和 DiT 的完全非自回归文本到语音系统.
无需复杂的设计, 如时长模型, 文本编码器和音素对齐, 文本输入只需要通过补充填充符号使得长度与输入语音相同, 然后进行去噪以生成语音, 这一方式最初由 E2 TTS 证明是可行的.
然而, E2 TTS 的原始设计使其难以跟随研究, 这是因为其缓慢的收敛和低鲁棒性.
为了解决这些问题, 我们首先使用 ConvNeXt 模型对输入进行建模, 优化文本表示使其更容易与语音对齐.
我们进一步提出了一个推理时摇摆采样策略, 它显著提高了模型的性能和效率.
这个采样策略可以很容易地应用到现有的基于流匹配的模型上, 而不需要重新训练.
我们的设计允许更快的训练, 并实现了推理 RTF 为 0.15, 这比现有的基于流匹配的 TTS 模型的最新水平提高了很多.
在一个公开的 100K 小时多语言数据集上训练, 我们的 ***Fairytaler Fakes Fluent and Faithful speech with Flow matching (F5-TTS)*** 具有高度自然和富有表现力的零样本能力, 无缝切换能力, 以及速度控制的效率.
演示样本可以在[此处](https://swivid.github.io/F5-TTS)找到.
我们发布所有代码和检查点, 以促进社区开发 ([Github](https://github.com/SWivid/F5-TTS)).

## 1.Introduction: 引言


Recent research in Text-to-Speech (TTS) has experienced great advancement~\cite{tacotron2,transtts,fastspeech2,glowtts,vits,gradtts,viola,ns1}. With a few seconds of audio prompt, current TTS models are able to synthesize speech for any given text and mimic the speaker of audio prompt~\cite{valle,vallex}. The synthesized speech can achieve high fidelity and naturalness that they are almost indistinguishable from human speech~\cite{ns2,ns3,valle2,voicebox}.

While autoregressive (AR) based TTS models exhibit an intuitive way of consecutively predicting the next token(s) and have achieved promising zero-shot TTS capability, the inherent limitations of AR modeling require extra efforts addressing issues such as inference latency and exposure bias~\cite{ellav,vallt,valler,ralle,voicecraft}. Moreover, the quality of speech tokenizer is essential for AR models to achieve high-fidelity synthesis~\cite{soundstream,encodec,audiodec,hificodec,speechtokenizer,dmel,ndvq}. Thus, there have been studies exploring direct modeling in continuous space~\cite{ardittts,ar-wovq,melle} to enhance synthesized speech quality recently.

Although AR models demonstrate impressive zero-shot performance as they perform implicit duration modeling and can leverage diverse sampling strategies, non-autoregressive (NAR) models benefit from fast inference through parallel processing, and effectively balance synthesis quality and latency. Notably, diffusion models~\cite{ddpm,score} contribute most to the success of current NAR speech models~\cite{ns2,ns3}. In particular, Flow Matching with Optimal Transport path (FM-OT)~\cite{cfm-ot} is widely used in recent research fields not only text-to-speech~\cite{voicebox,voiceflow,matchatts,dittotts,e2tts} but also image generation~\cite{sd3} and music generation~\cite{fluxmusic}.

Unlike AR-based models, the alignment modeling between input text and synthesized speech is crucial and challenging for NAR-based models. While NaturalSpeech 3 \cite{ns3} and Voicebox \cite{voicebox} use frame-wise phoneme alignment; Matcha-TTS \cite{matchatts} adopts monotonic alignment search and relies on phoneme-level duration model; recent works find that introducing such rigid and inflexible alignment between text and speech hinders the model from generating results with higher naturalness~\cite{e2tts,seedtts}.

E3 TTS~\cite{e3tts} abandons phoneme-level duration and applies cross-attention on the input sequence but yields limited audio quality. DiTTo-TTS~\cite{dittotts} uses Diffusion Transformer (DiT)~\cite{dit} with cross-attention conditioned on encoded text from a pretrained language model. To further enhance alignment, it uses the pretrained language model to finetune the neural audio codec, infusing semantic information into the generated representations. In contrast, E2 TTS~\cite{e2tts}, based on Voicebox~\cite{voicebox}, adopts a simpler way, which removes the phoneme and duration predictor and directly uses characters padded with filler tokens to the length of mel spectrograms as input. This simple scheme also achieves very natural and realistic synthesized results. However, we found that robustness issues exist in E2 TTS for the text and speech alignment. Seed-TTS~\cite{seedtts} employs a similar strategy and achieves excellent results, though not elaborated in model details. In these ways of not explicitly modeling phoneme-level duration, models learn to assign the length of each word or phoneme according to the given total sequence length, resulting in improved prosody and rhythm.

In this paper, we propose \textbf{F5-TTS}, a \textbf{F}airytaler that \textbf{F}akes \textbf{F}luent and \textbf{F}aithful speech with \textbf{F}low matching. Maintaining the simplicity of pipeline without phoneme alignment, duration predictor, text encoder, and semantically infused codec model, F5-TTS leverages the Diffusion Transformer with ConvNeXt V2~\cite{convnextv2} to better tackle text-speech alignment during in-context learning. We stress the deep entanglement of semantic and acoustic features in the E2 TTS model design, which has inherent problems and will pose alignment failure issues that could not simply be solved with re-ranking. With in-depth ablation studies, our proposed F5-TTS demonstrates stronger robustness, in generating more faithful speech to the text prompt, while maintaining comparable speaker similarity. Additionally, we introduce an inference-time sampling strategy for flow steps substantially improving naturalness, intelligibility, and speaker similarity of generation. This approach can be seamlessly integrated into existing flow matching based models without retraining.

## 2.Preliminaries: 预备知识

### Flow Matching: 流匹配

The Flow Matching (FM) objective is to match a probability path $p_t$ from a simple distribution $\displaystyle p_0$, \textit{e.g.}, the standard normal distribution $p(x) = \mathcal{N}(x|0,I)$, to $\displaystyle p_1$ approximating the data distribution $q$. In short, the FM loss regresses the vector field $u_t$ with a neural network $v_t$ as

$$
\mathcal{L}_{FM}(\theta) = E_{t, p_t(x)} \left\| v_t(x) - u_t(x) \right\| ^2,
$$

where $\theta$ parameterizes the neural network, $t\sim\mathcal{U}[0,1]$ and $x\sim p_t(x)$. The model $v_t$ is trained over the entire flow step and data range, ensuring it learns to handle the entire transformation process from the initial distribution to the target distribution.

As we have no prior knowledge of how to approximate $p_t$ and $u_t$, a conditional probability path $p_t(x|x_1) = \mathcal{N}(x\ |\ \mu_t(x_1),\sigma_t(x_1)^2I)$ is considered in actual training, and the Conditional Flow Matching (CFM) loss is proved to have identical gradients \textit{w.r.t.} $\theta$~\cite{cfm-ot}. $x_1$ is the random variable corresponding to training data. $\mu$ and $\sigma$ is the time-dependent mean and scalar standard deviation of Gaussian distribution.

Remember that the goal is to construct target distribution (data samples) from initial simple distribution, \textit{e.g.}, Gaussian noise. With the conditional form, the flow map $\psi_t(x)=\sigma_t(x_1)x+\mu_t(x_1)$ with $\mu_0(x_1)=0$ and $\sigma_0(x_1)=1$, $\mu_1(x_1)=x_1$ and $\sigma_1(x_1)=0$ is made to have all conditional probability paths converging to $p_0$ and $p_1$ at the start and end. The flow thus provides a vector field $d\psi_t(x_0)/dt = u_t(\psi_t(x_0)|x_1)$. Reparameterize $p_t(x|x_1)$ with $x_0$, we have

$$
\mathcal{L}_{\text{CFM}}(\theta) = E_{t, q(x_{1}), p(x_0)} \| v_{t}(\psi_t(x_0)) - \frac{d}{dt}\psi_t(x_0) \| ^2.
$$

Further leveraging Optimal Transport form $\psi_t(x)=(1-t)x+tx_1$, we have the OT-CFM loss,

$$
\mathcal{L}_{\text{CFM}}(\theta) = E_{t, q(x_{1}), p(x_0)} \| v_{t}((1-t)x_0+tx_1) - (x_1-x_0) \| ^2.
$$

To view in a more general way~\cite{snrtheory}, if formulating the loss in terms of log signal-to-noise ratio (log-SNR) $\lambda$ instead of flow step $t$, and parameterizing to predict $x_0$ ($\epsilon$, commonly stated in diffusion model) instead of predict $x_1-x_0$, the CFM loss is equivalent to the $v$-prediction~\cite{vpredict} loss with cosine schedule.

For inference, given sampled noise $x_0$ from initial distribution $p_0$, flow step $t\in[0,1]$ and condition with respect to generation task, the ordinary differential equation (ODE) solver~\cite{torchdiffeq} is used to evaluate $\psi_1(x_0)$ the integration of $d\psi_t(x_0)/dt$ with $\psi_0(x_0)=x_0$. The number of function evaluations (NFE) is the times going through the neural network as we may provide multiple flow step values from 0 to 1 as input to approximate the integration. Higher NFE will produce more accurate results and certainly take more calculation time.

### Classifier-Free Guidance


Classifier Guidance (CG) is proposed by \cite{cg}, functions by adding the gradient of an additional classifier, while such an explicit way to condition the generation process may have several problems. Extra training of the classifier is required and the generation result is directly affected by the quality of the classifier. Adversarial attacks might also occur as the guidance is introduced through the way of updating the gradient. Thus deceptive images with imperceptible details to human eyes may be generated, which are not conditional.

Classifier-Free Guidance (CFG)~\cite{cfg} proposes to replace the explicit classifier with an implicit classifier without directly computing the explicit classifier and its gradient. The gradient of a classifier can be expressed as a combination of conditional generation probability and unconditional generation probability. By dropping the condition with a certain rate during training, and linear extrapolating the inference outputs with and without condition $c$, the final guided result is obtained. We could balance between fidelity and diversity of the generated samples with

$$
v_{t,CFG} = v_{t}(\psi_t(x_0),c) + \alpha (v_{t}(\psi_t(x_0),c)-v_{t}(\psi_t(x_0)))
$$

in CFM case, where $\alpha$ is the CFG strength.\footnote{Note that the inference time will be doubled if CFG. Model $v_t$ will execute the forward process twice, once with condition, and once without.}

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
