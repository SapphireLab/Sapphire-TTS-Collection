# AudioGen

<details>
<summary>基本信息</summary>

- 标题: "AudioGen: Textually Guided Audio Generation"
- 作者:
  - 01 Felix Kreuk
  - 02 Gabriel Synnaeve
  - 03 Adam Polyak
  - 04 Uriel Singer
  - 05 Alexandre Defossez
  - 06 Jade Copet
  - 07 Devi Parikh
  - 08 Yaniv Taigman
  - 09 Yossi Adi
- 链接:
  - [ArXiv](https://arxiv.org/abs/2209.15352)
  - [Publication](https://openreview.net/forum?id=CYK7RfcOzQ4) ICLR2023
  - [Github]
  - [Demo](https://felixkreuk.github.io/audiogen)
- 文件:
  - [ArXiv](_PDF/2209.15352v2__AudioGen__Textually_Guided_Audio_Generation.pdf)
  - [Publication](_PDF/2209.15352p0__AudioGen__ICLR2023.pdf)

</details>

## Abstract: 摘要

<table><tr><td width="50%">

We tackle the problem of generating audio samples conditioned on descriptive text captions.
In this work, we propose ***AudioGen***, an auto-regressive generative model that generates audio samples conditioned on text inputs.
***AudioGen*** operates on a learnt discrete audio representation.
The task of text-to-audio generation poses multiple challenges.
Due to the way audio travels through a medium, differentiating "objects" can be a difficult task (e.g., separating multiple people simultaneously speaking).
This is further complicated by real-world recording conditions (e.g., background noise, reverberation, etc.).
Scarce text annotations impose another constraint, limiting the ability to scale models.
Finally, modeling high-fidelity audio requires encoding audio at high sampling rate, leading to extremely long sequences.
To alleviate the aforementioned challenges we propose an augmentation technique that mixes different audio samples, driving the model to internally learn to separate multiple sources.
We curated 10 datasets containing different types of audio and text annotations to handle the scarcity of text-audio data points.
For faster inference, we explore the use of multi-stream modeling, allowing the use of shorter sequences while maintaining a similar bitrate and perceptual quality.
We apply **Classifier-Free Guidance** to improve adherence to text.
Comparing to the evaluated baselines, ***AudioGen*** outperforms over both objective and subjective metrics.
Finally, we explore the ability of the proposed method to generate audio continuation conditionally and unconditionally.
Samples: [this https URL](https://felixkreuk.github.io/audiogen)

</td><td>

我们尝试解决以描述性文本标题为条件的音频样本生成问题.
本项工作提出了 ***AudioGen***, 一种自回归生成模型, 它可以将文本输入作为条件生成音频样本.
***AudioGen*** 采用了预训练好的离散音频表示.
文本转音频生成的任务存在多个挑战.
由于音频在媒体中的传播方式, 区分不同对象是一项困难任务 (例如, 分离同时说话的多个人).
此外由于现实录音条件会进一步变得复杂 (例如, 背景噪声, 混响等等)
稀缺的文本注释施加了另外的约束, 限制了模型的扩展能力.
最后, 建模高保真音频需要以高采样率对音频进行编码, 这会导致极长的序列.

为了缓解上述挑战, 我们提出了一种增强技术, 将不同的音频样本混合在一起, 促使模型内部学习分离多个来源. 为了应对文本-音频数据点的稀缺, 我们整理了10个包含不同类型音频和文本注释的数据集.
为了加速推理, 我们探索了使用多流建模的方法, 使得在保持相似比特率和感知质量的同时, 可以使用较短的序列. 我们还应用了**无分类器引导 (Classifier-Free Guidance)** 来改善与文本的契合度.
与评估的基准模型相比, ***AudioGen*** 在目标和主观指标上均表现优异.

最后, 我们探索了所提方法在条件生成和无条件生成音频续接方面的能力.

样本：[此链接](https://felixkreuk.github.io/audiogen)

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td></tr></table>
