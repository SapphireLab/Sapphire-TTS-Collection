# AudioGen

<details>
<summary>基本信息</summary>

- 标题: "AudioGen: Textually Guided Audio Generation"
- 作者:
  - 01 Felix Kreuk
  - 02 Gabriel Synnaeve
  - 03 Adam Polyak
  - 04 Uriel Singer
  - 05 Alexandre Defossez
  - 06 Jade Copet
  - 07 Devi Parikh
  - 08 Yaniv Taigman
  - 09 Yossi Adi
- 链接:
  - [ArXiv](https://arxiv.org/abs/2209.15352)
  - [Publication](https://openreview.net/forum?id=CYK7RfcOzQ4) ICLR2023
  - [Github]
  - [Demo](https://felixkreuk.github.io/audiogen)
- 文件:
  - [ArXiv](_PDF/2209.15352v2__AudioGen__Textually_Guided_Audio_Generation.pdf)
  - [Publication](_PDF/2209.15352p0__AudioGen__ICLR2023.pdf)

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

We tackle the problem of generating audio samples conditioned on descriptive text captions.
In this work, we propose ***AudioGen***, an auto-regressive generative model that generates audio samples conditioned on text inputs.
***AudioGen*** operates on a learnt discrete audio representation.
The task of text-to-audio generation poses multiple challenges.
Due to the way audio travels through a medium, differentiating "objects" can be a difficult task (e.g., separating multiple people simultaneously speaking).
This is further complicated by real-world recording conditions (e.g., background noise, reverberation, etc.).
Scarce text annotations impose another constraint, limiting the ability to scale models.
Finally, modeling high-fidelity audio requires encoding audio at high sampling rate, leading to extremely long sequences.
To alleviate the aforementioned challenges we propose an augmentation technique that mixes different audio samples, driving the model to internally learn to separate multiple sources.
We curated 10 datasets containing different types of audio and text annotations to handle the scarcity of text-audio data points.
For faster inference, we explore the use of multi-stream modeling, allowing the use of shorter sequences while maintaining a similar bitrate and perceptual quality.
We apply classifier-free guidance to improve adherence to text.
Comparing to the evaluated baselines, ***AudioGen*** outperforms over both objective and subjective metrics.
Finally, we explore the ability of the proposed method to generate audio continuation conditionally and unconditionally.
Samples: [this https URL](https://felixkreuk.github.io/audiogen)

</details>
<br>

我们尝试解决以描述性文本标题为条件的音频样本生成问题.
本项工作提出了 ***AudioGen***, 一种自回归生成模型, 它可以将文本输入作为条件生成音频样本.
***AudioGen*** 采用了预训练好的离散音频表示.
文本转音频生成的任务存在多个挑战.
由于音频在媒体中的传播方式, 区分不同对象是一项困难任务 (例如, 分离同时说话的多个人).
此外由于现实录音条件会进一步变得复杂 (例如, 背景噪声, 混响等等)
稀缺的文本注释施加了另外的约束, 限制了模型的扩展能力.
最后, 建模高保真音频要求在高采样率下编码音频, 这导致了非常长的序列.

为了缓解上述挑战, 我们提出了一种增强技术, 混合不同的音频样本,

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论