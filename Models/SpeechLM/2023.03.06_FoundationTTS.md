# FoundationTTS

<details>
<summary>基本信息</summary>

- 标题: "FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model"
- 作者:
  - 01 Ruiqing Xue,
  - 02 Yanqing Liu,
  - 03 Lei He,
  - 04 Xu Tan,
  - 05 Linquan Liu,
  - 06 Edward Lin,
  - 07 Sheng Zhao
- 链接:
  - [ArXiv](https://arxiv.org/abs/2303.02939)
  - [Publication]
  - [Github]
  - [Demo]
- 文件:
  - [ArXiv](_PDF/2303.02939v3__FoundationTTS__Text-to-Speech_for_ASR_Customization_with_Generative_Language_Model.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

Neural text-to-speech (TTS) generally consists of cascaded architecture with separately optimized acoustic model and vocoder, or end-to-end architecture with continuous mel-spectrograms or self-extracted speech frames as the intermediate representations to bridge acoustic model and vocoder, which suffers from two limitations:
1) the continuous acoustic frames are hard to predict with phoneme only, and acoustic information like duration or pitch is also needed to solve the one-to-many problem, which is not easy to scale on large scale and noise datasets;
2) to achieve diverse speech output based on continuous speech features, complex VAE or flow-based models are usually required.

In this paper, we propose ***FoundationTTS***, a new speech synthesis system with a neural audio codec for discrete speech token extraction and waveform reconstruction and a large language model for discrete token generation from linguistic (phoneme) tokens.
Specifically, 1) we propose a hierarchical codec network based on vector-quantized auto-encoders with adversarial training (**VQ-GAN**), which first extracts continuous frame-level speech representations with fine-grained codec, and extracts a discrete token from each continuous speech frame with coarse-grained codec; 2) we jointly optimize speech token, linguistic tokens, speaker token together with a large language model and predict the discrete speech tokens autoregressively.
Experiments show that ***FoundationTTS*** achieves a MOS gain of +0.14 compared to the baseline system.
In ASR customization tasks, our method achieves 7.09% and 10.35% WERR respectively over two strong customized ASR baselines.

</td><td>

神经语音合成 (TTS) 通常由级联架构组成, 分别优化声学模型和声码器, 或者是端到端架构, 其中使用连续的梅尔频谱或自提取的语音帧作为中间表示, 以连接声学模型和声码器, 但这种方法存在两个局限性:
1) 连续的声学帧仅通过音素很难预测, 解决一对多问题还需要如持续时间或音高等声学信息, 这在大规模和噪声数据集上不容易扩展；
2) 为了基于连续语音特征实现多样化的语音输出, 通常需要复杂的变分自编码器 (VAE) 或基于流的方法.

本文提出了 **FoundationTTS**, 一种新的语音合成系统, 结合了用于离散语音标记提取和波形重建的神经音频编解码器, 以及用于从语言 (音素) 标记生成离散标记的大型语言模型.

具体来说：
1) 我们提出了一种基于矢量量化自编码器和对抗训练的分层编解码网络 (VQ-GAN) , 首先使用精细化的编解码器提取连续帧级的语音表示, 然后使用粗粒度的编解码器从每个连续语音帧中提取离散标记；
2) 我们联合优化语音标记、语言标记和说话人标记, 并通过大型语言模型对离散语音标记进行自回归预测.

实验表明, **FoundationTTS** 比基准系统提高了 +0.14 的 MOS.
在 ASR 定制化任务中, 我们的方法在两个强大的定制化 ASR 基准上分别取得了 7.09% 和 10.35% 的 WERR (词错误率改进) .

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td></tr></table>
