# AudioChatLLaMA

<details>
<summary>基本信息</summary>

- 标题: "AudioChatLlama: Towards General-Purpose Speech Abilities for LLMs"
- 作者:
  - 01 Yassir Fathullah,
  - 02 Chunyang Wu,
  - 03 Egor Lakomkin,
  - 04 Ke Li,
  - 05 Junteng Jia,
  - 06 Yuan Shangguan,
  - 07 Jay Mahadeokar,
  - 08 Ozlem Kalinli,
  - 09 Christian Fuegen,
  - 10 Mike Seltzer
- 链接:
  - [ArXiv](https://arxiv.org/abs/2311.06753)
  - [Publication](https://doi.org/10.18653/v1/2024.naacl-long.309)
  - [Github]
  - [Demo]
- 文件:
  - [ArXiv](_PDF/2311.06753v2__AudioChatLLaMA__Towards_General-Purpose_Speech_Abilities_for_LLMs.pdf)
  - [Publication](_PDF/2311.06753p0__AudioChatLLaMA__NAACL2024.pdf)

</details>

## Abstract: 摘要

In this work, we extend the instruction-tuned Llama-2 model with end-to-end general-purpose speech processing and reasoning abilities while maintaining the wide range of original LLM capabilities, without using any carefully curated paired data.
The resulting end-to-end model, named ***AudioChatLLaMA***, can utilize audio prompts as a replacement for text and sustain a conversation.
Such a model also has extended cross-modal capabilities such as being able to perform spoken question answering (QA), speech translation, and audio summarization amongst many other closed and open-domain tasks.
This is unlike prior approaches in speech, in which LLMs are extended to handle audio for a limited number of pre-designated tasks.
On both synthesized and recorded speech QA test sets, evaluations show that our end-to-end approach is on par with or outperforms cascaded systems (speech recognizer + LLM) in terms of modeling the response to a prompt.
Furthermore, unlike cascades, our approach can interchange text and audio modalities and intrinsically utilize prior context in a conversation to provide better results.

## 1.Introduction: 引言

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论