# OSUM (Open Speech Understanding Model)

<details>
<summary>基本信息</summary>

- 标题: "OSUM: Advancing Open Speech Understanding Models with Limited Resources in Academia"
- 作者:
  - 01 Xuelong Geng
  - 02 Kun Wei 魏坤
  - 03 Qijie Shao 邵琪杰
  - 04 Shuiyun Liu 刘水云
  - 05 Zhennan Lin
  - 06 Zhixian Zhao
  - 07 Guojian Li
  - 08 Wenjie Tian
  - 09 Peikun Chen 陈培坤
  - 10 Yangze Li 李泱泽
  - 11 Pengcheng Guo 郭鹏程
  - 12 Mingchen Shao
  - 13 Shuiyuan Wang
  - 14 Yuang Cao
  - 15 Chengyou Wang
  - 16 Tianyi Xu 徐天翼
  - 17 Yuhang Dai
  - 18 Xinfa Zhu 朱新发
  - 19 Yue Li 李越
  - 20 Li Zhang
  - 21 Lei Xie 谢磊
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.13306)
  - [Publication]()
  - [Github](https://github.com/ASLP-lab/OSUM)
  - [Demo](https://aslp-lab.github.io/OSUM.github.io/)
- 文件:
  - [ArXiv](_PDF/2501.13306v1__OSUM__Advancing_Open_Speech_Understanding_Models_with_Limited_Resources_in_Academia.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Large Language Models (LLMs) have made significant progress in various downstream tasks, inspiring the development of Speech Understanding Language Models (SULMs) to enable comprehensive speech-based interactions.
However, most advanced SULMs are developed by the industry, leveraging large-scale datasets and computational resources that are not readily available to the academic community.
Moreover, the lack of transparency in training details creates additional barriers to further innovation.
In this study, we present ***OSUM***, an ***Open Speech Understanding Model*** designed to explore the potential of training SLUMs under constrained academic resources.
The ***OSUM*** model combines a Whisper encoder with a Qwen2 LLM and supports a wide range of speech tasks, including speech recognition (ASR), speech recognition with timestamps (SRWT), vocal event detection (VED), speech emotion recognition (SER), speaking style recognition (SSR), speaker gender classification (SGC), speaker age prediction (SAP), and speech-to-text chat (STTC).
By employing an ASR+X training strategy, ***OSUM*** achieves efficient and stable multi-task training by simultaneously optimizing ASR alongside target tasks.
Beyond delivering strong performance, ***OSUM*** emphasizes transparency by providing openly available data preparation and training methodologies, offering valuable insights and practical guidance for the academic community.
By doing so, we aim to accelerate research and innovation in advanced SULM technologies.

</details>
<br>

大语言模型已经在各种下游任务中取得了显著进展, 激励了**语音理解语言模型 (Speech Understanding Language Models, SULMs)** 的发展, 以实现全面的基于语音的交互.
然而, 大多数现有的 SULMs 都由工业界开发, 使用大规模的数据集和计算资源, 这对于学术社区来说并不容易获得.
此外, 训练细节的透明度的缺乏为进一步创新带来了障碍.

在本研究中, 我们展示了 ***OSUM***, 一种 ***开放式语音理解模型 (Open Speech Understanding Model)***, 设计用于探索在受限学术资源约束下训练 SULMs 的潜力.
***OSUM*** 模型结合了 Whisper 编码器和 Qwen2 语言模型, 并支持广泛的语音任务, 包括
- 语音识别 (Automatic Speech Recognition, ASR),
- 带时间戳的语音识别 (Speech Recognition with Timestamps, SRWT),
- 声音事件检测 (Vocal Event Detection, VED),
- 语音情绪识别 (Speech Emotion Recognition, SER),
- 说话风格识别 (Speaking Style Recognition, SSR),
- 说话者性别分类 (Speaker Gender Classification, SGC),
- 说话者年龄预测 (Speaker Age Prediction, SAP),
- 语音到文本聊天 (Speech-to-Text Chat, STTC).

通过采用 ASR+X 训练策略, ***OSUM*** 实现了高效和稳定的多任务训练, 同时优化 ASR 与目标任务.
除了取得强大性能外, ***OSUM*** 还强调了透明度, 提供了开放式的数据准备和训练方法, 为学术界提供了宝贵的见解和实用指导.
通过这样做, 我们期望加速 SULM 技术的研究和创新.

## 1·Introduction: 引言

Large language models (LLMs) have shown tremendous progress towards Artificial General Intelligence (AGI) in recent years.
Given the inherent human preference for speech-based interaction, there has been growing interest in extending LLMs with speech capabilities to develop Speech LLMs.
To generate fluent and expressive text or speech responses, Speech LLMs must fully comprehend input speech, including both its semantic content and paralinguistic information, like emotion, speaking style, speaker gender, and age.
Moreover, this comprehension ability is also crucial for audio data labeling. Currently, the mainstream multi-label generation approach is to use multiple models to label each task separately, which consumes extremely high computational resources. A labeling model capable of accurately generating multiple labels simultaneously holds broad application prospects.

The area which focuses on Speech Understanding Language Models (SULMs), has seen notable advancements through projects such as Qwen-Audio\citep{chu2023qwen_audio}, Qwen2-Audio\citep{chu2024qwen2_audio}, PandGPT\citep{su2023pandagpt}, and SALMONN~\citep{tangsalmonn}.
Whisper~\citep{radford2023whisper} marks a pioneering exploration of speech understanding independent of LLMs, utilizing an encoder-decoder Transformer~\citep{vaswani2017attention} architecture to tackle a variety of speech tasks, such as automatic speech recognition (ASR), speech translation (ST), language identification (LID), and voice activity detection (VAD).
Building on Whisper’s design, SenseVoice~\citep{an2024funaudiollm} and TouchASP~\citep{song2024touchasp} expand more tasks like speech emotion recognition (SER) and audio event detection (AED), further enriching their ability to process and comprehend human speech.
Qwen-Audio integrates Whisper's encoder with the text-based Qwen LLM~\citep{bai2023qwen}, enabling the latter to understand speech. Compared to Whisper, Qwen-Audio leverages a more powerful LLM decoder and performs over 30 speech-related tasks, making it a representative model in the field of SULMs.
Its successor, Qwen2-Audio, further enhances these capabilities by supporting natural language prompts and achieving superior performance across various benchmarks~\citep{chu2024qwen2_audio}.

Although these advanced SULMs have achieved remarkable progress, most of them are developed by industry, leveraging millions of hours of training data and massive GPU resources.
For instance, TouchASP and SenseVoice utilized 1,000,000 and 400,000 hours of training data, respectively.
Such large-scale resources are typically beyond the reach of academia institutions.
Furthermore, while inference models are often open-sourced, essential details regarding data preparation, training strategies, codebases, and hyper-parameters configurations are rarely disclosed.
These limitations hinder academic community efforts to further optimize and expand SULM research.
Recently, a growing movement advocating for open science in Speech LLM research has emerged. This movement emphasizes the importance of releasing comprehensive training frameworks, datasets, and methodological details to promote research and innovation.
A notable example is the Open Whisper-style Speech Model (OWSM) series~\citep{peng2023reproducing}, which replicates Whisper-style training using open-sourced tools and publicly available data, significantly advancing public understanding and research on speech understanding models.

In this study, we aim to foster broader academic exploration of SULMs with limited resource demands, encouraging wider research community participation.
To this end, we introduce OSUM, an open SULM with its data processing pipeline and training details publicly available.
The OSUM model integrates a Whisper speech encoder, fine-tuned on a multi-task dataset, with a Qwen2 LLM.
It is capable of performing a wide range of speech tasks, including automatic speech recognition (ASR), speech recognition with timestamps (SRWT), vocal event detection (VED),
speech emotion recognition (SER), speaking style recognition (SSR), speaker gender classification (SGC), speaker age prediction (SAP), and speech-to-text chat (STTC).
Notably, SSR is a distinctive feature of our OSUM model and serves as a vital component of speech understanding. It enhances the model’s capability by improving contextual comprehension and boosting performance across various downstream speech tasks. Furthermore, it establishes a foundation for enabling more natural and context-aware speech-based interactions.
We adopt an ASR+X training strategy to enhance training stability and reduce resource consumption for our SLUM model, wherein an auxiliary ASR task is optimized alongside the primary target task (denoted as ``X'').
For instance, during the training of the SER task, we concurrently train the ASR task (ASR+SER) by predicting both transcription and emotion labels for each speech sample.
This multi-task training accelerates modality alignment, enabling the LLM to effectively utilize both textual and acoustic modalities.
Our OSUM model utilizes only 44,100 hours of training data and achieves comparable or superior performance to other SULMs.
The overall performance of OSUM is illustrated in Fig.~\ref{fig:radar}.
The model is trained on Nvidia A6000 GPUs and Huawei Ascend NPUs, supporting inference on both platforms.
The goal of this study is to foster transparency and accelerate progress in the field of SULMs by providing accessible tools and resources for the broader research community.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论