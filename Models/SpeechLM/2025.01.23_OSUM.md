# OSUM (Open Speech Understanding Model)

<details>
<summary>基本信息</summary>

- 标题: "OSUM: Advancing Open Speech Understanding Models with Limited Resources in Academia"
- 作者:
  - 01 Xuelong Geng
  - 02 Kun Wei 魏坤
  - 03 Qijie Shao 邵琪杰
  - 04 Shuiyun Liu 刘水云
  - 05 Zhennan Lin
  - 06 Zhixian Zhao
  - 07 Guojian Li
  - 08 Wenjie Tian
  - 09 Peikun Chen 陈培坤
  - 10 Yangze Li 李泱泽
  - 11 Pengcheng Guo 郭鹏程
  - 12 Mingchen Shao
  - 13 Shuiyuan Wang
  - 14 Yuang Cao
  - 15 Chengyou Wang
  - 16 Tianyi Xu 徐天翼
  - 17 Yuhang Dai
  - 18 Xinfa Zhu 朱新发
  - 19 Yue Li 李越
  - 20 Li Zhang
  - 21 Lei Xie 谢磊
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.13306)
  - [Publication]()
  - [Github](https://github.com/ASLP-lab/OSUM)
  - [Demo](https://aslp-lab.github.io/OSUM.github.io/)
- 文件:
  - [ArXiv](_PDF/2501.13306v1__OSUM__Advancing_Open_Speech_Understanding_Models_with_Limited_Resources_in_Academia.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Large Language Models (LLMs) have made significant progress in various downstream tasks, inspiring the development of Speech Understanding Language Models (SULMs) to enable comprehensive speech-based interactions.
However, most advanced SULMs are developed by the industry, leveraging large-scale datasets and computational resources that are not readily available to the academic community.
Moreover, the lack of transparency in training details creates additional barriers to further innovation.
In this study, we present ***OSUM***, an ***Open Speech Understanding Model*** designed to explore the potential of training SLUMs under constrained academic resources.
The ***OSUM*** model combines a Whisper encoder with a Qwen2 LLM and supports a wide range of speech tasks, including speech recognition (ASR), speech recognition with timestamps (SRWT), vocal event detection (VED), speech emotion recognition (SER), speaking style recognition (SSR), speaker gender classification (SGC), speaker age prediction (SAP), and speech-to-text chat (STTC).
By employing an ASR+X training strategy, ***OSUM*** achieves efficient and stable multi-task training by simultaneously optimizing ASR alongside target tasks.
Beyond delivering strong performance, ***OSUM*** emphasizes transparency by providing openly available data preparation and training methodologies, offering valuable insights and practical guidance for the academic community.
By doing so, we aim to accelerate research and innovation in advanced SULM technologies.

</details>
<br>

大语言模型已经在各种下游任务中取得了显著进展, 激励了**语音理解语言模型 (Speech Understanding Language Models, SULMs)** 的发展, 以实现全面的基于语音的交互.
然而, 大多数现有的 SULMs 都由工业界开发, 使用大规模的数据集和计算资源, 这对于学术社区来说并不容易获得.
此外, 训练细节的透明度的缺乏为进一步创新带来了障碍.

在本研究中, 我们展示了 ***OSUM***, 一种 ***开放式语音理解模型 (Open Speech Understanding Model)***, 设计用于探索在受限学术资源约束下训练 SULMs 的潜力.
***OSUM*** 模型结合了 Whisper 编码器和 Qwen2 语言模型, 并支持广泛的语音任务, 包括
- 语音识别 (Automatic Speech Recognition, ASR),
- 带时间戳的语音识别 (Speech Recognition with Timestamps, SRWT),
- 声音事件检测 (Vocal Event Detection, VED),
- 语音情绪识别 (Speech Emotion Recognition, SER),
- 说话风格识别 (Speaking Style Recognition, SSR),
- 说话者性别分类 (Speaker Gender Classification, SGC),
- 说话者年龄预测 (Speaker Age Prediction, SAP),
- 语音到文本聊天 (Speech-to-Text Chat, STTC).

通过采用 ASR+X 训练策略, ***OSUM*** 实现了高效和稳定的多任务训练, 同时优化 ASR 与目标任务.
除了取得强大性能外, ***OSUM*** 还强调了透明度, 提供了开放式的数据准备和训练方法, 为学术界提供了宝贵的见解和实用指导.
通过这样做, 我们期望加速 SULM 技术的研究和创新.

## 1·Introduction: 引言

Large language models (LLMs) have shown tremendous progress towards Artificial General Intelligence (AGI) in recent years.
Given the inherent human preference for speech-based interaction, there has been growing interest in extending LLMs with speech capabilities to develop Speech LLMs.
To generate fluent and expressive text or speech responses, Speech LLMs must fully comprehend input speech, including both its semantic content and paralinguistic information, like emotion, speaking style, speaker gender, and age.
Moreover, this comprehension ability is also crucial for audio data labeling. Currently, the mainstream multi-label generation approach is to use multiple models to label each task separately, which consumes extremely high computational resources. A labeling model capable of accurately generating multiple labels simultaneously holds broad application prospects.

The area which focuses on Speech Understanding Language Models (SULMs), has seen notable advancements through projects such as Qwen-Audio\citep{chu2023qwen_audio}, Qwen2-Audio\citep{chu2024qwen2_audio}, PandGPT\citep{su2023pandagpt}, and SALMONN~\citep{tangsalmonn}.
Whisper~\citep{radford2023whisper} marks a pioneering exploration of speech understanding independent of LLMs, utilizing an encoder-decoder Transformer~\citep{vaswani2017attention} architecture to tackle a variety of speech tasks, such as automatic speech recognition (ASR), speech translation (ST), language identification (LID), and voice activity detection (VAD).
Building on Whisper’s design, SenseVoice~\citep{an2024funaudiollm} and TouchASP~\citep{song2024touchasp} expand more tasks like speech emotion recognition (SER) and audio event detection (AED), further enriching their ability to process and comprehend human speech.
Qwen-Audio integrates Whisper's encoder with the text-based Qwen LLM~\citep{bai2023qwen}, enabling the latter to understand speech. Compared to Whisper, Qwen-Audio leverages a more powerful LLM decoder and performs over 30 speech-related tasks, making it a representative model in the field of SULMs.
Its successor, Qwen2-Audio, further enhances these capabilities by supporting natural language prompts and achieving superior performance across various benchmarks~\citep{chu2024qwen2_audio}.

Although these advanced SULMs have achieved remarkable progress, most of them are developed by industry, leveraging millions of hours of training data and massive GPU resources.
For instance, TouchASP and SenseVoice utilized 1,000,000 and 400,000 hours of training data, respectively.
Such large-scale resources are typically beyond the reach of academia institutions.
Furthermore, while inference models are often open-sourced, essential details regarding data preparation, training strategies, codebases, and hyper-parameters configurations are rarely disclosed.
These limitations hinder academic community efforts to further optimize and expand SULM research.
Recently, a growing movement advocating for open science in Speech LLM research has emerged. This movement emphasizes the importance of releasing comprehensive training frameworks, datasets, and methodological details to promote research and innovation.
A notable example is the Open Whisper-style Speech Model (OWSM) series~\citep{peng2023reproducing}, which replicates Whisper-style training using open-sourced tools and publicly available data, significantly advancing public understanding and research on speech understanding models.

In this study, we aim to foster broader academic exploration of SULMs with limited resource demands, encouraging wider research community participation.
To this end, we introduce OSUM, an open SULM with its data processing pipeline and training details publicly available.
The OSUM model integrates a Whisper speech encoder, fine-tuned on a multi-task dataset, with a Qwen2 LLM.
It is capable of performing a wide range of speech tasks, including automatic speech recognition (ASR), speech recognition with timestamps (SRWT), vocal event detection (VED),
speech emotion recognition (SER), speaking style recognition (SSR), speaker gender classification (SGC), speaker age prediction (SAP), and speech-to-text chat (STTC).
Notably, SSR is a distinctive feature of our OSUM model and serves as a vital component of speech understanding. It enhances the model’s capability by improving contextual comprehension and boosting performance across various downstream speech tasks. Furthermore, it establishes a foundation for enabling more natural and context-aware speech-based interactions.
We adopt an ASR+X training strategy to enhance training stability and reduce resource consumption for our SLUM model, wherein an auxiliary ASR task is optimized alongside the primary target task (denoted as ``X'').
For instance, during the training of the SER task, we concurrently train the ASR task (ASR+SER) by predicting both transcription and emotion labels for each speech sample.
This multi-task training accelerates modality alignment, enabling the LLM to effectively utilize both textual and acoustic modalities.
Our OSUM model utilizes only 44,100 hours of training data and achieves comparable or superior performance to other SULMs.
The overall performance of OSUM is illustrated in Fig.~\ref{fig:radar}.
The model is trained on Nvidia A6000 GPUs and Huawei Ascend NPUs, supporting inference on both platforms.
The goal of this study is to foster transparency and accelerate progress in the field of SULMs by providing accessible tools and resources for the broader research community.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

This section introduces our proposed OSUM, a model designed for comprehensive speech understanding.
Section~\ref{sec:model_architecture} presents its architecture;.Section~\ref{sec:multitask_supervised_training} details its multitask training process.  Section~\ref{sec:training_data} and Section~\ref{sec:data_proccessing} provide an overview of the training data and processing pipeline, respectively.

### Model Architecture: 模型架构

As shown in Figure~\ref{fig:framework}, our OSUM model comprises a speech encoder, an adaptor, and an LLM.
During the training, all of the parameters in the encoder and adaptor are updated, while the LLM is fine-tuned with LoRA~\citep{hulora}.
The input of our model consists of a speech and a natural language prompt.
Unlink Whisper~\citep{radford2023whisper} and Qwen-audio~\citep{bai2023qwen}, which rely on instruct tags, the OSUM employs descriptive text, converting all eight supported tasks as shown in Fig.~\ref{fig:framework}.
Currently, our model supports only text-based responses, but audio output capabilities are under active development.
The following sections describe each sub-module in detail.

#### Speech Encoder

Our OSUM utilizes the Whisper-Medium~\footnote{\url{https://huggingface.co/openai/whisper-medium}} model as its speech encoder, which consists of 2 one-dimensional convolutional layers with 2 times downsampling, and 24 Transformer layers with 1024 hidden state dimensions and 16-headed self-attention.
The encoder has approximately 300 million parameters, which makes it take into account both speech comprehension ability and inference efficiency.

#### Adaptor

The adaptor module features a hybrid architecture combining 3-layer 1D convolutional layers (Conv1D) and 4-layer Transformer.
The Conv1D layers use kernel widths of (3, 3, 3) and strides of (1, 2, 2), achieving an overall 4 times downsampling.
The Transformer layers have a model dimension of 1,280, an inner dimension of 2,560, and 4 attention heads.
This architecture bridges the output of the speech encoder with the input requirements of the LLM, enabling efficient modality alignment.

#### LLM with LoRA

The Qwen2-7B-Instruct is selected as our LLM. Qwen2-7B-Instruct~\footnote{\url{https://huggingface.co/Qwen/Qwen2-7B-Instruct}} is a general-purpose LLM with a parameter scale of 7 billion, specifically designed for multi-task instruction optimization. In our work, we fine-tune the Qwen2-7B-Instruct model using LoRA (Low-Rank Adaptation) technology. The LoRA hyperparameters-${\alpha}$, rank, and dropout ratio are set to 32, 8, and 0.1, respectively.

### Multitask Supervised Training

The training procedure includes two stages.
First, we perform multi-task supervised fine-tuning on the original Whisper model without an LLM.
Second, we integrate the fine-tuned Whisper encoder with the Qwen2 LLM to create the complete OSUM system, then conduct further supervised training using a larger dataset.

#### Whisper Fine-tuning

The original Whisper model supports a limited scope of speech-understanding tasks, which makes the direct integration of the Whisper with an LLM for multi-task training risky when data and computation resources are constrained.
Therefore, we first fine-tune the Whisper via multi-task data to ensure faster convergence of the OSUM model.
Furthermore, this stage allows us to verify the reliability of our multi-task data.
Specifically, we expand Whisper's instruction tag set to accommodate more tasks. Each forward pass executes only a single task.

#### OSUM Training

Training SULMs typically begins with pre-training on an ASR task, which serves as a foundation for incorporating additional speech tasks to enable LLMs to process semantic content from the speech encoder.
Given computational constraints, we introduce an ASR+X paradigm for OSUM's multi-task training.
It concurrently trains ASR and a secondary task ``X'', accelerating training while allowing the ``X'' task to utilize both textual and acoustic features, thereby potentially improving performance
The ASR+X paradigm follows a two-step process: first, transcribing speech to text (ASR); then, integrating this transcription with acoustic features to execute the target task (X).
This is achieved within the LLM's autoregressive framework by adjusting predicted labels, without modifications to the model architectures or loss functions.
We implemented the ASR+X paradigm by prompting the LLLM with natural language prompts.
ChatGPT\footnote{\url{https://openai.com/index/chatgpt/}} is used to generate 5 candidate prompts for each task, one of which is randomly selected during training.
Table~\ref{tab:prompt_label} shows examples of the prompts and ASR+X prediction labels.

### Training Data

Our OSUM is designed to perform multi-task training using diverse speech datasets, with the goal of building a unified model capable of comprehensively understanding input speech in conversational scenarios.
The multi-task training process enables tasks to benefit from shared learning, enhancing overall model performance.
Upon completion of training, OSUM can be utilized for speech data annotation or further extended into a conversational Speech LLM. Detailed information about the datasets used for training is provided in Table~\ref{tab:train_data}.

### Data Processing Pipeline

The data processing pipeline is crucial for training multi-task SULMs. In this section, we reveal the data processing schemes used for each task in the OSUM project, with the aim of providing a valuable reference for academic research.

#### ASR

The training data include publicly available resources like Wenetspeech~\citep{wenetspeech}, AISHELL-1~\citep{aishell1}, AISHELL-2~\citep{aishell2}, and LibriSpeech~\citep{librispeech}, along with our internal ASR dataset, resulting in a total of 24,000 hours.

#### SRWT

For the SRWT task, a Gaussian Mixture Model - Hidden Markov Model (GMM-HMM) based conventional ASR model, is used to conduct force alignment and obtain word-level timestampes.
This model is trained on the 54,000-hour proprietary ASR dataset.
To evaluate its performance, we establish an internal SRWT test set and assess alignment quality using the Average Alignment Score (AAS) metric~\citep{shi2022aas}.
The GMM-HMM model achieves an AAS of 7.55, demonstrating its efficacy in generating reliable word-level timestamps.

#### SSR

Given the absence of open-sourced tools for annotating style labels directly from audio data, we leverage two text-based LLMs-Qwen2.5-14B~\footnote{\url{https://huggingface.co/Qwen/Qwen2.5-14B-Instruct}} and GLM-4-9B-Chat~\footnote{\url{https://huggingface.co/THUDM/glm-4-9b-chat}}- to annotate speech transcriptions using carefully designed prompts.
To enhance annotation accuracy and reliability, we retain only the intersection of labeling results from both models. This intersection-based approach ensures high-quality annotations for training the SSR task.

#### VED

We have attempted to train a vocal event labeling tool; however, due to the limited availability of training data, its classification performance is suboptimal, especially when vocal events and speech occur within the same audio segment.
Therefore, we employ a Voice Conversion (VC) tool to modify the timbre of vocal event audio and insert it randomly into speech audio, creating a dataset of ASR+VED format. We find that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training data with the assistance of VC. The open-source vocal event datasets we use include Audioset~\citep{jort_audioset_2017}, ESC-50~\citep{piczak2015dataset}, Vocal Sound~\citep{gong2022vocalsound}, and Nonspeech7k~\citep{rashid2023nonspeech7k}, while the ASR data consists solely of AISHELL-2~\citep{aishell2}.

#### SER

Emotion2Vec~\citep{ma-etal-2024-emotion2vec} is the first universal speech emotion representation model. Without additional fine-tuning, we directly apply the pre-trained Emotion2Vec+ Large model~\footnote{\url{https://huggingface.co/emotion2vec/emotion2vec_plus_large}}, which is trained on 40,000 hours of emotional speech data, to annotate the audio with emotion labels.
Additionally, we leverage the GLM-4-9B-Chat model to generate emotion labels from the textual transcriptions of the speech.
By intersecting these annotations, we generate high-quality emotional labels for the entire dataset.

#### SGC

Efforts to train a speaker gender classification model to label web-sourced data yield unsatisfactory performance.
Consequently, we discard the pseudo-labeled data and relied solely exclusively on manually labeled datasets for training.
For the SGC task, we select KeSpeech~\citep{tang2021kespeech}, Datatang-Kid~\citep{datatang_page}, AISHELL-1~\citep{aishell1}, AISHELL-2~\citep{aishell2}, LibriSpeech~\citep{librispeech}, Kaggle-CommonVoice~\citep{kagglecv}, and Magicdata-Read~\cite{magicdata_read} as training dataset, as they include reliable speaker gender labels.

#### SAP

Similar to the SGC task, due to the poor performance of the automated labeling model, only manually labeled data is used for training.
We use KeSpeech~\citep{tang2021kespeech}, Datatang-Kid~\citep{datatang_page}, Magicdata-Read~\citep{magicdata_read}, Kaggle-CommonVoice~\citep{kagglecv}, AISHELL-ASR0060~\citep{aishell_page}, and AISHELL-ASR0018~\citep{aishell_page} as the training dataset for the SAP task, as these datasets provide reliable speaker age labels.

#### STTC

For the STTC task, we use three types of data. First, we utilize a human-recorded audio question-answer dataset Databacker-Conversation~\citep{databaker}. Then, we use a text-based dialogue dataset LCCC~\citep{2020lccc} and the ChatTTS~\footnote{\url{https://github.com/2noise/ChatTTS}} system with random speaker capabilities to generate the utterances of the questioner in the dialogue, thus obtaining the speech-text pairs for the dialogue task. Finally, we filter suitable response sentences from the Wenetspeech~\citep{wenetspeech} dataset using Qwen2.5-7B~\footnote{\url{https://huggingface.co/Qwen/Qwen2.5-7B}}, guiding the LLM to generate text answers.

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论