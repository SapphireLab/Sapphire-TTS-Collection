# OSUM (Open Speech Understanding Model)

<details>
<summary>基本信息</summary>

- 标题: "OSUM: Advancing Open Speech Understanding Models with Limited Resources in Academia"
- 作者:
  - 01 Xuelong Geng
  - 02 Kun Wei 魏坤
  - 03 Qijie Shao 邵琪杰
  - 04 Shuiyun Liu 刘水云
  - 05 Zhennan Lin
  - 06 Zhixian Zhao
  - 07 Guojian Li
  - 08 Wenjie Tian
  - 09 Peikun Chen 陈培坤
  - 10 Yangze Li 李泱泽
  - 11 Pengcheng Guo 郭鹏程
  - 12 Mingchen Shao
  - 13 Shuiyuan Wang
  - 14 Yuang Cao
  - 15 Chengyou Wang
  - 16 Tianyi Xu 徐天翼
  - 17 Yuhang Dai
  - 18 Xinfa Zhu 朱新发
  - 19 Yue Li 李越
  - 20 Li Zhang
  - 21 Lei Xie 谢磊
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.13306)
  - [Publication]()
  - [Github](https://github.com/ASLP-lab/OSUM)
  - [Demo](https://aslp-lab.github.io/OSUM.github.io/)
- 文件:
  - [ArXiv](_PDF/2501.13306v1__OSUM__Advancing_Open_Speech_Understanding_Models_with_Limited_Resources_in_Academia.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Large Language Models (LLMs) have made significant progress in various downstream tasks, inspiring the development of Speech Understanding Language Models (SULMs) to enable comprehensive speech-based interactions.
However, most advanced SULMs are developed by the industry, leveraging large-scale datasets and computational resources that are not readily available to the academic community.
Moreover, the lack of transparency in training details creates additional barriers to further innovation.
In this study, we present ***OSUM***, an ***Open Speech Understanding Model*** designed to explore the potential of training SLUMs under constrained academic resources.
The ***OSUM*** model combines a Whisper encoder with a Qwen2 LLM and supports a wide range of speech tasks, including speech recognition (ASR), speech recognition with timestamps (SRWT), vocal event detection (VED), speech emotion recognition (SER), speaking style recognition (SSR), speaker gender classification (SGC), speaker age prediction (SAP), and speech-to-text chat (STTC).
By employing an ASR+X training strategy, ***OSUM*** achieves efficient and stable multi-task training by simultaneously optimizing ASR alongside target tasks.
Beyond delivering strong performance, ***OSUM*** emphasizes transparency by providing openly available data preparation and training methodologies, offering valuable insights and practical guidance for the academic community.
By doing so, we aim to accelerate research and innovation in advanced SULM technologies.

</details>
<br>

大语言模型已经在各种下游任务中取得了显著进展, 激励了**语音理解语言模型 (Speech Understanding Language Models, SULMs)** 的发展, 以实现全面的基于语音的交互.
然而, 大多数现有的 SULMs 都由工业界开发, 使用大规模的数据集和计算资源, 这对于学术社区来说并不容易获得.
此外, 训练细节的透明度的缺乏为进一步创新带来了障碍.

在本研究中, 我们展示了 ***OSUM***, 一种 ***开放式语音理解模型 (Open Speech Understanding Model)***, 设计用于探索在受限学术资源约束下训练 SULMs 的潜力.
***OSUM*** 模型结合了 Whisper 编码器和 Qwen2 语言模型, 并支持广泛的语音任务, 包括
- 语音识别 (Automatic Speech Recognition, ASR),
- 带时间戳的语音识别 (Speech Recognition with Timestamps, SRWT),
- 声音事件检测 (Vocal Event Detection, VED),
- 语音情绪识别 (Speech Emotion Recognition, SER),
- 说话风格识别 (Speaking Style Recognition, SSR),
- 说话者性别分类 (Speaker Gender Classification, SGC),
- 说话者年龄预测 (Speaker Age Prediction, SAP),
- 语音到文本聊天 (Speech-to-Text Chat, STTC).

通过采用 ASR+X 训练策略, ***OSUM*** 实现了高效和稳定的多任务训练, 同时优化 ASR 与目标任务.
除了取得强大性能外, ***OSUM*** 还强调了透明度, 提供了开放式的数据准备和训练方法, 为学术界提供了宝贵的见解和实用指导.
通过这样做, 我们期望加速 SULM 技术的研究和创新.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论