# OSUM (Open Speech Understanding Model)

<details>
<summary>基本信息</summary>

- 标题: "OSUM: Advancing Open Speech Understanding Models with Limited Resources in Academia"
- 作者:
  - 01 Xuelong Geng
  - 02 Kun Wei 魏坤
  - 03 Qijie Shao 邵琪杰
  - 04 Shuiyun Liu 刘水云
  - 05 Zhennan Lin
  - 06 Zhixian Zhao
  - 07 Guojian Li
  - 08 Wenjie Tian
  - 09 Peikun Chen 陈培坤
  - 10 Yangze Li 李泱泽
  - 11 Pengcheng Guo 郭鹏程
  - 12 Mingchen Shao
  - 13 Shuiyuan Wang
  - 14 Yuang Cao
  - 15 Chengyou Wang
  - 16 Tianyi Xu 徐天翼
  - 17 Yuhang Dai
  - 18 Xinfa Zhu 朱新发
  - 19 Yue Li 李越
  - 20 Li Zhang
  - 21 Lei Xie 谢磊
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.13306)
  - [Publication]()
  - [Github](https://github.com/ASLP-lab/OSUM)
  - [Demo](https://aslp-lab.github.io/OSUM.github.io/)
- 文件:
  - [ArXiv](_PDF/2501.13306v1__OSUM__Advancing_Open_Speech_Understanding_Models_with_Limited_Resources_in_Academia.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Large Language Models (LLMs) have made significant progress in various downstream tasks, inspiring the development of Speech Understanding Language Models (SULMs) to enable comprehensive speech-based interactions.
However, most advanced SULMs are developed by the industry, leveraging large-scale datasets and computational resources that are not readily available to the academic community.
Moreover, the lack of transparency in training details creates additional barriers to further innovation.
In this study, we present ***OSUM***, an ***Open Speech Understanding Model*** designed to explore the potential of training SLUMs under constrained academic resources.
The ***OSUM*** model combines a Whisper encoder with a Qwen2 LLM and supports a wide range of speech tasks, including speech recognition (ASR), speech recognition with timestamps (SRWT), vocal event detection (VED), speech emotion recognition (SER), speaking style recognition (SSR), speaker gender classification (SGC), speaker age prediction (SAP), and speech-to-text chat (STTC).
By employing an ASR+X training strategy, ***OSUM*** achieves efficient and stable multi-task training by simultaneously optimizing ASR alongside target tasks.
Beyond delivering strong performance, ***OSUM*** emphasizes transparency by providing openly available data preparation and training methodologies, offering valuable insights and practical guidance for the academic community.
By doing so, we aim to accelerate research and innovation in advanced SULM technologies.

</details>
<br>

大语言模型已经在各种下游任务中取得了显著进展, 激励了**语音理解语言模型 (Speech Understanding Language Models, SULMs)** 的发展, 以实现全面的基于语音的交互.
然而, 大多数现有的 SULMs 都由工业界开发, 使用大规模的数据集和计算资源, 这对于学术社区来说并不容易获得.
此外, 训练细节的透明度的缺乏为进一步创新带来了障碍.

在本研究中, 我们展示了 ***OSUM***, 一种 ***开放式语音理解模型 (Open Speech Understanding Model)***, 设计用于探索在受限学术资源约束下训练 SULMs 的潜力.
***OSUM*** 模型结合了 Whisper 编码器和 Qwen2 语言模型, 并支持广泛的语音任务, 包括
- 语音识别 (Automatic Speech Recognition, ASR),
- 带时间戳的语音识别 (Speech Recognition with Timestamps, SRWT),
- 声音事件检测 (Vocal Event Detection, VED),
- 语音情绪识别 (Speech Emotion Recognition, SER),
- 说话风格识别 (Speaking Style Recognition, SSR),
- 说话者性别分类 (Speaker Gender Classification, SGC),
- 说话者年龄预测 (Speaker Age Prediction, SAP),
- 语音到文本聊天 (Speech-to-Text Chat, STTC).

通过采用 ASR+X 训练策略, ***OSUM*** 实现了高效和稳定的多任务训练, 同时优化 ASR 与目标任务.
除了取得强大性能外, ***OSUM*** 还强调了透明度, 提供了开放式的数据准备和训练方法, 为学术界提供了宝贵的见解和实用指导.
通过这样做, 我们期望加速 SULM 技术的研究和创新.

## 1·Introduction: 引言

Large language models (LLMs) have shown tremendous progress towards Artificial General Intelligence (AGI) in recent years.
Given the inherent human preference for speech-based interaction, there has been growing interest in extending LLMs with speech capabilities to develop Speech LLMs.
To generate fluent and expressive text or speech responses, Speech LLMs must fully comprehend input speech, including both its semantic content and paralinguistic information, like emotion, speaking style, speaker gender, and age.
Moreover, this comprehension ability is also crucial for audio data labeling. Currently, the mainstream multi-label generation approach is to use multiple models to label each task separately, which consumes extremely high computational resources. A labeling model capable of accurately generating multiple labels simultaneously holds broad application prospects.

The area which focuses on Speech Understanding Language Models (SULMs), has seen notable advancements through projects such as Qwen-Audio\citep{chu2023qwen_audio}, Qwen2-Audio\citep{chu2024qwen2_audio}, PandGPT\citep{su2023pandagpt}, and SALMONN~\citep{tangsalmonn}.
Whisper~\citep{radford2023whisper} marks a pioneering exploration of speech understanding independent of LLMs, utilizing an encoder-decoder Transformer~\citep{vaswani2017attention} architecture to tackle a variety of speech tasks, such as automatic speech recognition (ASR), speech translation (ST), language identification (LID), and voice activity detection (VAD).
Building on Whisper’s design, SenseVoice~\citep{an2024funaudiollm} and TouchASP~\citep{song2024touchasp} expand more tasks like speech emotion recognition (SER) and audio event detection (AED), further enriching their ability to process and comprehend human speech.
Qwen-Audio integrates Whisper's encoder with the text-based Qwen LLM~\citep{bai2023qwen}, enabling the latter to understand speech. Compared to Whisper, Qwen-Audio leverages a more powerful LLM decoder and performs over 30 speech-related tasks, making it a representative model in the field of SULMs.
Its successor, Qwen2-Audio, further enhances these capabilities by supporting natural language prompts and achieving superior performance across various benchmarks~\citep{chu2024qwen2_audio}.

Although these advanced SULMs have achieved remarkable progress, most of them are developed by industry, leveraging millions of hours of training data and massive GPU resources.
For instance, TouchASP and SenseVoice utilized 1,000,000 and 400,000 hours of training data, respectively.
Such large-scale resources are typically beyond the reach of academia institutions.
Furthermore, while inference models are often open-sourced, essential details regarding data preparation, training strategies, codebases, and hyper-parameters configurations are rarely disclosed.
These limitations hinder academic community efforts to further optimize and expand SULM research.
Recently, a growing movement advocating for open science in Speech LLM research has emerged. This movement emphasizes the importance of releasing comprehensive training frameworks, datasets, and methodological details to promote research and innovation.
A notable example is the Open Whisper-style Speech Model (OWSM) series~\citep{peng2023reproducing}, which replicates Whisper-style training using open-sourced tools and publicly available data, significantly advancing public understanding and research on speech understanding models.

In this study, we aim to foster broader academic exploration of SULMs with limited resource demands, encouraging wider research community participation.
To this end, we introduce OSUM, an open SULM with its data processing pipeline and training details publicly available.
The OSUM model integrates a Whisper speech encoder, fine-tuned on a multi-task dataset, with a Qwen2 LLM.
It is capable of performing a wide range of speech tasks, including automatic speech recognition (ASR), speech recognition with timestamps (SRWT), vocal event detection (VED),
speech emotion recognition (SER), speaking style recognition (SSR), speaker gender classification (SGC), speaker age prediction (SAP), and speech-to-text chat (STTC).
Notably, SSR is a distinctive feature of our OSUM model and serves as a vital component of speech understanding. It enhances the model’s capability by improving contextual comprehension and boosting performance across various downstream speech tasks. Furthermore, it establishes a foundation for enabling more natural and context-aware speech-based interactions.
We adopt an ASR+X training strategy to enhance training stability and reduce resource consumption for our SLUM model, wherein an auxiliary ASR task is optimized alongside the primary target task (denoted as ``X'').
For instance, during the training of the SER task, we concurrently train the ASR task (ASR+SER) by predicting both transcription and emotion labels for each speech sample.
This multi-task training accelerates modality alignment, enabling the LLM to effectively utilize both textual and acoustic modalities.
Our OSUM model utilizes only 44,100 hours of training data and achieves comparable or superior performance to other SULMs.
The overall performance of OSUM is illustrated in Fig.~\ref{fig:radar}.
The model is trained on Nvidia A6000 GPUs and Huawei Ascend NPUs, supporting inference on both platforms.
The goal of this study is to foster transparency and accelerate progress in the field of SULMs by providing accessible tools and resources for the broader research community.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

This section introduces our proposed OSUM, a model designed for comprehensive speech understanding.
Section~\ref{sec:model_architecture} presents its architecture;.Section~\ref{sec:multitask_supervised_training} details its multitask training process.  Section~\ref{sec:training_data} and Section~\ref{sec:data_proccessing} provide an overview of the training data and processing pipeline, respectively.

### Model Architecture: 模型架构

As shown in Figure~\ref{fig:framework}, our OSUM model comprises a speech encoder, an adaptor, and an LLM.
During the training, all of the parameters in the encoder and adaptor are updated, while the LLM is fine-tuned with LoRA~\citep{hulora}.
The input of our model consists of a speech and a natural language prompt.
Unlink Whisper~\citep{radford2023whisper} and Qwen-audio~\citep{bai2023qwen}, which rely on instruct tags, the OSUM employs descriptive text, converting all eight supported tasks as shown in Fig.~\ref{fig:framework}.
Currently, our model supports only text-based responses, but audio output capabilities are under active development.
The following sections describe each sub-module in detail.

#### Speech Encoder

Our OSUM utilizes the Whisper-Medium~\footnote{\url{https://huggingface.co/openai/whisper-medium}} model as its speech encoder, which consists of 2 one-dimensional convolutional layers with 2 times downsampling, and 24 Transformer layers with 1024 hidden state dimensions and 16-headed self-attention.
The encoder has approximately 300 million parameters, which makes it take into account both speech comprehension ability and inference efficiency.

#### Adaptor

The adaptor module features a hybrid architecture combining 3-layer 1D convolutional layers (Conv1D) and 4-layer Transformer.
The Conv1D layers use kernel widths of (3, 3, 3) and strides of (1, 2, 2), achieving an overall 4 times downsampling.
The Transformer layers have a model dimension of 1,280, an inner dimension of 2,560, and 4 attention heads.
This architecture bridges the output of the speech encoder with the input requirements of the LLM, enabling efficient modality alignment.

#### LLM with LoRA

The Qwen2-7B-Instruct is selected as our LLM. Qwen2-7B-Instruct~\footnote{\url{https://huggingface.co/Qwen/Qwen2-7B-Instruct}} is a general-purpose LLM with a parameter scale of 7 billion, specifically designed for multi-task instruction optimization. In our work, we fine-tune the Qwen2-7B-Instruct model using LoRA (Low-Rank Adaptation) technology. The LoRA hyperparameters-${\alpha}$, rank, and dropout ratio are set to 32, 8, and 0.1, respectively.

### Multitask Supervised Training

The training procedure includes two stages.
First, we perform multi-task supervised fine-tuning on the original Whisper model without an LLM.
Second, we integrate the fine-tuned Whisper encoder with the Qwen2 LLM to create the complete OSUM system, then conduct further supervised training using a larger dataset.

#### Whisper Fine-tuning

The original Whisper model supports a limited scope of speech-understanding tasks, which makes the direct integration of the Whisper with an LLM for multi-task training risky when data and computation resources are constrained.
Therefore, we first fine-tune the Whisper via multi-task data to ensure faster convergence of the OSUM model.
Furthermore, this stage allows us to verify the reliability of our multi-task data.
Specifically, we expand Whisper's instruction tag set to accommodate more tasks. Each forward pass executes only a single task.

#### OSUM Training

Training SULMs typically begins with pre-training on an ASR task, which serves as a foundation for incorporating additional speech tasks to enable LLMs to process semantic content from the speech encoder.
Given computational constraints, we introduce an ASR+X paradigm for OSUM's multi-task training.
It concurrently trains ASR and a secondary task ``X'', accelerating training while allowing the ``X'' task to utilize both textual and acoustic features, thereby potentially improving performance
The ASR+X paradigm follows a two-step process: first, transcribing speech to text (ASR); then, integrating this transcription with acoustic features to execute the target task (X).
This is achieved within the LLM's autoregressive framework by adjusting predicted labels, without modifications to the model architectures or loss functions.
We implemented the ASR+X paradigm by prompting the LLLM with natural language prompts.
ChatGPT\footnote{\url{https://openai.com/index/chatgpt/}} is used to generate 5 candidate prompts for each task, one of which is randomly selected during training.
Table~\ref{tab:prompt_label} shows examples of the prompts and ASR+X prediction labels.

### Training Data

Our OSUM is designed to perform multi-task training using diverse speech datasets, with the goal of building a unified model capable of comprehensively understanding input speech in conversational scenarios.
The multi-task training process enables tasks to benefit from shared learning, enhancing overall model performance.
Upon completion of training, OSUM can be utilized for speech data annotation or further extended into a conversational Speech LLM. Detailed information about the datasets used for training is provided in Table~\ref{tab:train_data}.

### Data Processing Pipeline

The data processing pipeline is crucial for training multi-task SULMs. In this section, we reveal the data processing schemes used for each task in the OSUM project, with the aim of providing a valuable reference for academic research.

#### ASR

The training data include publicly available resources like Wenetspeech~\citep{wenetspeech}, AISHELL-1~\citep{aishell1}, AISHELL-2~\citep{aishell2}, and LibriSpeech~\citep{librispeech}, along with our internal ASR dataset, resulting in a total of 24,000 hours.

#### SRWT

For the SRWT task, a Gaussian Mixture Model - Hidden Markov Model (GMM-HMM) based conventional ASR model, is used to conduct force alignment and obtain word-level timestampes.
This model is trained on the 54,000-hour proprietary ASR dataset.
To evaluate its performance, we establish an internal SRWT test set and assess alignment quality using the Average Alignment Score (AAS) metric~\citep{shi2022aas}.
The GMM-HMM model achieves an AAS of 7.55, demonstrating its efficacy in generating reliable word-level timestamps.

#### SSR

Given the absence of open-sourced tools for annotating style labels directly from audio data, we leverage two text-based LLMs-Qwen2.5-14B~\footnote{\url{https://huggingface.co/Qwen/Qwen2.5-14B-Instruct}} and GLM-4-9B-Chat~\footnote{\url{https://huggingface.co/THUDM/glm-4-9b-chat}}- to annotate speech transcriptions using carefully designed prompts.
To enhance annotation accuracy and reliability, we retain only the intersection of labeling results from both models. This intersection-based approach ensures high-quality annotations for training the SSR task.

#### VED

We have attempted to train a vocal event labeling tool; however, due to the limited availability of training data, its classification performance is suboptimal, especially when vocal events and speech occur within the same audio segment.
Therefore, we employ a Voice Conversion (VC) tool to modify the timbre of vocal event audio and insert it randomly into speech audio, creating a dataset of ASR+VED format. We find that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training data with the assistance of VC. The open-source vocal event datasets we use include Audioset~\citep{jort_audioset_2017}, ESC-50~\citep{piczak2015dataset}, Vocal Sound~\citep{gong2022vocalsound}, and Nonspeech7k~\citep{rashid2023nonspeech7k}, while the ASR data consists solely of AISHELL-2~\citep{aishell2}.

#### SER

Emotion2Vec~\citep{ma-etal-2024-emotion2vec} is the first universal speech emotion representation model. Without additional fine-tuning, we directly apply the pre-trained Emotion2Vec+ Large model~\footnote{\url{https://huggingface.co/emotion2vec/emotion2vec_plus_large}}, which is trained on 40,000 hours of emotional speech data, to annotate the audio with emotion labels.
Additionally, we leverage the GLM-4-9B-Chat model to generate emotion labels from the textual transcriptions of the speech.
By intersecting these annotations, we generate high-quality emotional labels for the entire dataset.

#### SGC

Efforts to train a speaker gender classification model to label web-sourced data yield unsatisfactory performance.
Consequently, we discard the pseudo-labeled data and relied solely exclusively on manually labeled datasets for training.
For the SGC task, we select KeSpeech~\citep{tang2021kespeech}, Datatang-Kid~\citep{datatang_page}, AISHELL-1~\citep{aishell1}, AISHELL-2~\citep{aishell2}, LibriSpeech~\citep{librispeech}, Kaggle-CommonVoice~\citep{kagglecv}, and Magicdata-Read~\cite{magicdata_read} as training dataset, as they include reliable speaker gender labels.

#### SAP

Similar to the SGC task, due to the poor performance of the automated labeling model, only manually labeled data is used for training.
We use KeSpeech~\citep{tang2021kespeech}, Datatang-Kid~\citep{datatang_page}, Magicdata-Read~\citep{magicdata_read}, Kaggle-CommonVoice~\citep{kagglecv}, AISHELL-ASR0060~\citep{aishell_page}, and AISHELL-ASR0018~\citep{aishell_page} as the training dataset for the SAP task, as these datasets provide reliable speaker age labels.

#### STTC

For the STTC task, we use three types of data. First, we utilize a human-recorded audio question-answer dataset Databacker-Conversation~\citep{databaker}. Then, we use a text-based dialogue dataset LCCC~\citep{2020lccc} and the ChatTTS~\footnote{\url{https://github.com/2noise/ChatTTS}} system with random speaker capabilities to generate the utterances of the questioner in the dialogue, thus obtaining the speech-text pairs for the dialogue task. Finally, we filter suitable response sentences from the Wenetspeech~\citep{wenetspeech} dataset using Qwen2.5-7B~\footnote{\url{https://huggingface.co/Qwen/Qwen2.5-7B}}, guiding the LLM to generate text answers.

## 4·Experiments: 实验

This section begins by presenting our training setup in Section~\ref{sec:exp_training_setup}. Subsequently, to conduct a more comprehensive evaluation of OSUM, we establish a series of complete internal evaluation sets, as detailed in Section~\ref{sec:exp_test_sets}. Finally, we report the performance of OSUM on both public and internal test sets, accompanied by an analysis in Section~\ref{sec:exp_main_res}.

### Training Setup: 训练设置

The two-stage training process for OSUM is detailed as follows:

#### Whisper Fine-tuning

In the first stage, we fine-tune the Whisper-Medium model on the multi-task datasets described in Table~\ref{tab:train_data}.
This stage is conducted on 8 Nvidia A6000 GPUs.
A warm-up scheduler is employed to adjust the learning rate, peaking at 5e-5.
The multitask Whisper is trained for 150,000 steps, which takes approximately 15.3 days.

#### OSUM Training

In the second stage, we conduct experiments on 24 Huawei Ascend NPUs, using a learning rate of 5e-5. The process completes a total of 528,000 training steps and consumes 7.5 days.

### Internal Test Sets

Currently, most SULMs evaluate multi-task performance using publicly available English datasets~\citep{chu2023qwen_audio,chu2024qwen2_audio,radford2023whisper,song2024touchasp}.
However, as OSUM training incorporates a substantial amount of Chinese data, we have developed a series of internal multi-task test sets tailored for Chinese~\footnote{We plan to make the internal test sets publicly available in the future}.
These complement the publicly available English test sets, creating a more comprehensive evaluation framework.
To support the ASR+X paradigm, we further enhance the test sets with speech transcripts.
However, ASR metrics are used solely for internal reference to assess model convergence and will not be publicly reported.
Table~\ref{tab:test_set} presents a description of our internal multi-task test sets.

### Main Results

Table~\ref{tab:res_asr} and Table~\ref{tab:res_multi} show the experimental results of our OSUM across various tasks. The results reveal that our approach achieves performance that is comparable to, and in many cases superior to, speech understanding models such as Qwen-audio, Qwen2-audio, Whisper, and SenseVoice. Furthermore, in this section, we will highlight the performance disparities between our model and other comparable approaches, while providing a detailed analysis of the challenges SULMs face in these tasks. We hope that these experiences can provide useful references for researchers.

#### ASR

As illustrated in Table~\ref{tab:res_asr}, our approach reveals obvious advantages in the ASR task on the Chinese test sets.
Notably, the proposed OSUM consistently outperforms other models on the WenetSpeech test-meeting set, three AISHELL-2 sub-test sets, and three internally used SpeechIO test sets.
While OSUM does not surpass the top-performing method on the English test set, it rivals performance comparable to SenseVoice-S. These results are achieved, remarkably, with substantially less training data.
This underscores that our ASR+X task paradigm effectively enhances model convergence in ASR tasks, significantly minimizing the data and computational resources required for SULMs training.

#### SRWT

Table~\ref{tab:res_multi} presents the SRWT evaluation results for our proposed OSUM model compared to Whisper-Large-v3, Qwen-Audio, and the GMM-HMM model used for generating annotated data in SRWT tasks.
Our OSUM model significantly outperforms Whisper-Large-v3 by relative ${31.38\%}$ and also surpasses Qwen-Audio.
Additionally, our OSUM's SRWT performance is very close to the GMM-HMM model, which is widely regarded for its high accuracy in timestamp prediction.
These results underscore the effectiveness of OSUM in the SRWT task.
While OSUM's performance is slightly below that of the GMM-HMM model, its primary advantage lies in its ability to predict timestamps in an end-to-end fashion, simplifying the integration with other tasks, such as speaker diarization.

#### VED

We first evaluate OSUM's performance on the public test sets ESC-50 and VocalSound. However, since the event categories in these two datasets do not completely align with those in OSUM, the comparison to other approaches should only serve as a rough assessment.
Specifically, the ESC-50 contains a substantial number of non-vocal audio events, we categorize them as "other." The experimental results on this test set demonstrate that our model successfully classifies these non-vocal audio events as "other." Additionally, on the VocalSound set, we select the categories supported by OSUM and calculate the average accuracy across these categories.
This result reveals that our OSUM exhibits a gap compared to Qwen2-audio, primarily due to our training data consisting of concatenated speech and vocal events. In contrast, the VocalSound test set includes only the latter, resulting in a significant mismatch. Nevertheless, our OSUM achieves a norm level, successfully identifying the most independent vocal events.
In our internal human-recoded ASR+VED test set, PANNs become unusable due to similar mismatches, particularly because their design treats speech as a standalone event, exacerbating accuracy degradation. Qwen2-audio performs relatively better but also experiences a performance decline in our test set, likely due to overfitting. In contrast, our model demonstrates balanced results in both the public and internal test sets, showcasing enhanced generalization. This indicates that using VC to augment data for vocal events can effectively mitigate overfitting in VED tasks.

#### SER

For the SER task, we extract the categories supported by OSUM from the public datasets MELD and MER2023 for testing, followed by a comprehensive evaluation on our internal test set.
In the experiments with the public datasets, OSUM demonstrates superior performance on the MER2023 test set, outperforming several recent public benchmark models. On the MELD dataset, OSUM's performance ranks just below the SenseVoice-L model, likely due to the latter's additional training on a larger-scale speech emotion dataset. In addition, while OSUM's result on the internal test set is comparable to that of the EmoBox model, it significantly surpasses other comparative approaches. Furthermore, we observe that among the eight emotions supported, disgust and fear are particularly challenging to recognize, a difficulty partly attributed to the scarcity of training data for these two emotions. In our future work, we plan to enhance the model's performance and generalization capability by utilizing OSUM for labeling, thereby obtaining a larger and more balanced emotion dataset.

#### SSR

The acoustic-text dual-modal style classification employed by our OSUM significantly outperforms the single-text modality of GLM-4-9B-Chat.
It demonstrates a strong ability to distinguish among eight styles: news and science reporting, horror stories, fairy tales, customer service, poetry and prose, audiobooks, spontaneous conversation, and others.
Notably, the classification performance for news science communication, audiobooks, fairy tales, and customer service styles is commendable; however, there remains room for improvement in the categorization of poetry and prose, horror stories, and other styles. Moving forward, we leverage OSUM to label additional data, aiming to enhance data quality and optimize the distribution across categories.

#### SGC

In the SGC task, we evaluate Qwen2-Audio and OSUM.
The results demonstrate that OSUM achieves an ${100\%}$ accuracy on the AISHELL-1 test set. While this result is commendable, we suspect it may indicate some degree of overfitting. Furthermore, on the Kaggle test set, our approach slightly outperforms Qwen2-Audio, yet it falls short on our internal test set.
This is likely due to the presence of background music in our internal test set, a characteristic less prevalent in OSUM's training data. Addressing this limitation is a focus of our future work.
Overall, OSUM exhibits its value in the SGC task.

#### SAP

We also compare our OSUM with Qwen2-Audio on the SAP task.
During our previous experiments, we found that the acoustic similarity between teenagers and adults is remarkably high, complicating effective differentiation.
Consequently, we categorize age into three groups: child, adult, and old.
Curiously, despite our efforts to debug the prompts, Qwen2-Audio demonstrates a lower age classification accuracy on both the Kaggle test set and our internal test set. This may stem from their overly detailed age categorization, which hinders the model's training accuracy. Our model significantly surpasses Qwen2-Audio on the Kaggle test set, achieving an accuracy of ${76.72\%}$. Although the classification accuracy slightly declines on our proprietary test set, it still outperforms Qwen2-Audio. This indicates that our model exhibits strong generalization capabilities on different data.

#### STTC

In the STTC task, we follow AirBench's evaluation protocol across all test sets. This involves providing the text of audio queries along with the text of two distinct answers, allowing a text-based LLM to assign subjective scores from 1 to 10. The two answers consist of a real response and the answer generated by SULMs. While AirBench employs GPT-4 as the scoring LLM, it is currently inaccessible, so we instead utilize GPT-3.5-Turbo.
The test results presented in Table~\ref{tab:res_multi} indicate that, on AirBench's official speech sub-test set, our score is lower than that of Qwen2-Audio, suggesting that our model's capabilities in English conversation and audio description lag behind those of Qwen2-Audio.
This is primarily because we did not use English conversational data for training; the current score relies entirely on the LLM's performance.  Furthermore, the gap narrows significantly on our internal Chinese conversational test set, suggesting that our strategy of performing ASR before chat is beneficial. Overall, our OSUM still lags behind Qwen2-Audio in conversational ability, which will be a key focus of our future improvements.

## 5·Results: 结果

## 6·Conclusions: 结论