# OSUM (Open Speech Understanding Model)

<details>
<summary>基本信息</summary>

- 标题: "OSUM: Advancing Open Speech Understanding Models with Limited Resources in Academia"
- 作者:
  - 01 Xuelong Geng
  - 02 Kun Wei
  - 03 Qijie Shao
  - 04 Shuiyun Liu
  - 05 Zhennan Lin
  - 06 Zhixian Zhao
  - 07 Guojian Li
  - 08 Wenjie Tian
  - 09 Peikun Chen
  - 10 Yangze Li
  - 11 Pengcheng Guo
  - 12 Mingchen Shao
  - 13 Shuiyuan Wang
  - 14 Yuang Cao
  - 15 Chengyou Wang
  - 16 Tianyi Xu
  - 17 Yuhang Dai
  - 18 Xinfa Zhu
  - 19 Yue Li
  - 20 Li Zhang
  - 21 Lei Xie
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.13306)
  - [Publication]()
  - [Github](https://github.com/ASLP-lab/OSUM)
  - [Demo](https://aslp-lab.github.io/OSUM.github.io/)
- 文件:
  - [ArXiv](_PDF/2501.13306v1__OSUM__Advancing_Open_Speech_Understanding_Models_with_Limited_Resources_in_Academia.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Large Language Models (LLMs) have made significant progress in various downstream tasks, inspiring the development of Speech Understanding Language Models (SULMs) to enable comprehensive speech-based interactions.
However, most advanced SULMs are developed by the industry, leveraging large-scale datasets and computational resources that are not readily available to the academic community.
Moreover, the lack of transparency in training details creates additional barriers to further innovation.
In this study, we present ***OSUM***, an ***Open Speech Understanding Model*** designed to explore the potential of training SLUMs under constrained academic resources.
The ***OSUM*** model combines a Whisper encoder with a Qwen2 LLM and supports a wide range of speech tasks, including speech recognition (ASR), speech recognition with timestamps (SRWT), vocal event detection (VED), speech emotion recognition (SER), speaking style recognition (SSR), speaker gender classification (SGC), speaker age prediction (SAP), and speech-to-text chat (STTC).
By employing an ASR+X training strategy, ***OSUM*** achieves efficient and stable multi-task training by simultaneously optimizing ASR alongside target tasks.
Beyond delivering strong performance, ***OSUM*** emphasizes transparency by providing openly available data preparation and training methodologies, offering valuable insights and practical guidance for the academic community.
By doing so, we aim to accelerate research and innovation in advanced SULM technologies.

</details>
<br>

大语言模型已经在各种下游任务中取得了显著进展, 激励了**语音理解语言模型 (Speech Understanding Language Models, SULMs)** 的发展, 以实现全面的基于语音的交互.
然而, 大多数现有的 SULMs 都由工业界开发, 使用大规模的数据集和计算资源, 这对于学术社区来说并不容易获得.
此外, 训练细节的透明度的缺乏为进一步创新带来了障碍.

在本研究中, 我们展示了 ***OSUM***, 一种 ***开放式语音理解模型 (Open Speech Understanding Model)***, 设计用于探索在受限学术资源约束下训练 SULMs 的潜力.
***OSUM*** 模型结合了 Whisper 编码器和 Qwen2 语言模型, 并支持广泛的语音任务, 包括
- 语音识别 (Automatic Speech Recognition, ASR),
- 带时间戳的语音识别 (Speech Recognition with Timestamps, SRWT),
- 声音事件检测 (Vocal Event Detection, VED),
- 语音情绪识别 (Speech Emotion Recognition, SER),
- 说话风格识别 (Speaking Style Recognition, SSR),
- 说话者性别分类 (Speaker Gender Classification, SGC),
- 说话者年龄预测 (Speaker Age Prediction, SAP),
- 语音到文本聊天 (Speech-to-Text Chat, STTC).

通过采用 ASR+X 训练策略, ***OSUM*** 实现了高效和稳定的多任务训练, 同时优化 ASR 与目标任务.
除了取得强大性能外, ***OSUM*** 还强调了透明度, 提供了开放式的数据准备和训练方法, 为学术界提供了宝贵的见解和实用指导.
通过这样做, 我们期望加速 SULM 技术的研究和创新.

## 1·Introduction: 引言

Large language models (LLMs) have shown tremendous progress towards Artificial General Intelligence (AGI) in recent years.
Given the inherent human preference for speech-based interaction, there has been growing interest in extending LLMs with speech capabilities to develop Speech LLMs.
To generate fluent and expressive text or speech responses, Speech LLMs must fully comprehend input speech, including both its semantic content and paralinguistic information, like emotion, speaking style, speaker gender, and age.
Moreover, this comprehension ability is also crucial for audio data labeling.
Currently, the mainstream multi-label generation approach is to use multiple models to label each task separately, which consumes extremely high computational resources.
A labeling model capable of accurately generating multiple labels simultaneously holds broad application prospects.

The area which focuses on Speech Understanding Language Models (SULMs), has seen notable advancements through projects such as [Qwen-Audio](../SpokenDialogue/2023.11.14_Qwen-Audio.md)[^1], [Qwen2-Audio](../SpokenDialogue/2024.07.15_Qwen2-Audio.md)[^2], [PandaGPT](../CV/2023.05.25_PandaGPT.md)[^3], and [SALMONN](../SpokenDialogue/2023.10.20_SALMONN.md)[^4].
[Whisper](2022.12.06_Whisper.md)[^5] marks a pioneering exploration of speech understanding independent of LLMs, utilizing an encoder-decoder [Transformer](../_Transformer/2017.06.12_Transformer.md)[^6] architecture to tackle a variety of speech tasks, such as automatic speech recognition (ASR), speech translation (ST), language identification (LID), and voice activity detection (VAD).
Building on Whisper’s design, [SenseVoice](2024.07.04_FunAudioLLM.md)[^7] and [TouchASP](../_Basis/TouchASP.md)[^8] expand more tasks like speech emotion recognition (SER) and audio event detection (AED), further enriching their ability to process and comprehend human speech.
Qwen-Audio integrates Whisper's encoder with the text-based [Qwen LLM](../TextLM/2023.09.28_Qwen.md)[^9], enabling the latter to understand speech.
Compared to Whisper, Qwen-Audio leverages a more powerful LLM decoder and performs over 30 speech-related tasks, making it a representative model in the field of SULMs.
Its successor, [Qwen2-Audio](../SpokenDialogue/2024.07.15_Qwen2-Audio.md)[^2], further enhances these capabilities by supporting natural language prompts and achieving superior performance across various benchmarks.

Although these advanced SULMs have achieved remarkable progress, most of them are developed by industry, leveraging millions of hours of training data and massive GPU resources.
For instance, TouchASP and SenseVoice utilized 1,000,000 and 400,000 hours of training data, respectively.
Such large-scale resources are typically beyond the reach of academia institutions.
Furthermore, while inference models are often open-sourced, essential details regarding data preparation, training strategies, codebases, and hyper-parameters configurations are rarely disclosed.
These limitations hinder academic community efforts to further optimize and expand SULM research.
Recently, a growing movement advocating for open science in Speech LLM research has emerged.
This movement emphasizes the importance of releasing comprehensive training frameworks, datasets, and methodological details to promote research and innovation.
A notable example is the [Open Whisper-style Speech Model (OWSM) series](2023.09.25_OWSM.md)[^10], which replicates Whisper-style training using open-sourced tools and publicly available data, significantly advancing public understanding and research on speech understanding models.

In this study, we aim to foster broader academic exploration of SULMs with limited resource demands, encouraging wider research community participation.
To this end, we introduce ***OSUM***, an open SULM with its data processing pipeline and training details publicly available.
The ***OSUM*** model integrates a Whisper speech encoder, fine-tuned on a multi-task dataset, with a Qwen2 LLM.
It is capable of performing a wide range of speech tasks, including automatic speech recognition (ASR), speech recognition with timestamps (SRWT), vocal event detection (VED),
speech emotion recognition (SER), speaking style recognition (SSR), speaker gender classification (SGC), speaker age prediction (SAP), and speech-to-text chat (STTC).
Notably, SSR is a distinctive feature of our ***OSUM*** model and serves as a vital component of speech understanding.
It enhances the model’s capability by improving contextual comprehension and boosting performance across various downstream speech tasks.
Furthermore, it establishes a foundation for enabling more natural and context-aware speech-based interactions.
We adopt an ASR+X training strategy to enhance training stability and reduce resource consumption for our SLUM model, wherein an auxiliary ASR task is optimized alongside the primary target task (denoted as ``X'').
For instance, during the training of the SER task, we concurrently train the ASR task (ASR+SER) by predicting both transcription and emotion labels for each speech sample.
This multi-task training accelerates modality alignment, enabling the LLM to effectively utilize both textual and acoustic modalities.
Our ***OSUM*** model utilizes only 44,100 hours of training data and achieves comparable or superior performance to other SULMs.
The overall performance of ***OSUM*** is illustrated in Figure.01.
The model is trained on Nvidia A6000 GPUs and Huawei Ascend NPUs, supporting inference on both platforms.
The goal of this study is to foster transparency and accelerate progress in the field of SULMs by providing accessible tools and resources for the broader research community.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

This section introduces our proposed ***OSUM***, a model designed for comprehensive speech understanding.
Section.3.1 presents its architecture.
Section.3.2 details its multitask training process.
Section.3.3 and Section.3.4 provide an overview of the training data and processing pipeline, respectively.

### Model Architecture: 模型架构

As shown in Figure.02, our ***OSUM*** model comprises a speech encoder, an adaptor, and an LLM.
During the training, all of the parameters in the encoder and adaptor are updated, while the LLM is fine-tuned with [LoRA](../../Modules/LoRA/2021.06.17_LoRA.md)[^11].
The input of our model consists of a speech and a natural language prompt.
Unlink [Whisper](2022.12.06_Whisper.md)[^5] and [Qwen](../TextLM/2023.09.28_Qwen.md)[^9], which rely on instruct tags, the ***OSUM*** employs descriptive text, converting all eight supported tasks as shown in Figure.02.
Currently, our model supports only text-based responses, but audio output capabilities are under active development.
The following sections describe each sub-module in detail.

#### Speech Encoder

Our ***OSUM*** utilizes the [Whisper-Medium [URL]](https://huggingface.co/openai/whisper-medium) model as its speech encoder, which consists of 2 one-dimensional convolutional layers with 2 times downsampling, and 24 Transformer layers with 1024 hidden state dimensions and 16-headed self-attention.
The encoder has approximately 300 million parameters, which makes it take into account both speech comprehension ability and inference efficiency.

#### Adaptor

The adaptor module features a hybrid architecture combining 3-layer 1D convolutional layers (Conv1D) and 4-layer Transformer.
The Conv1D layers use kernel widths of (3, 3, 3) and strides of (1, 2, 2), achieving an overall 4 times downsampling.
The Transformer layers have a model dimension of 1,280, an inner dimension of 2,560, and 4 attention heads.
This architecture bridges the output of the speech encoder with the input requirements of the LLM, enabling efficient modality alignment.

#### LLM with LoRA

The Qwen2-7B-Instruct is selected as our LLM.
[Qwen2-7B-Instruct [URL]](https://huggingface.co/Qwen/Qwen2-7B-Instruct) is a general-purpose LLM with a parameter scale of 7 billion, specifically designed for multi-task instruction optimization.
In our work, we fine-tune the Qwen2-7B-Instruct model using LoRA (Low-Rank Adaptation) technology.
The LoRA hyperparameters-${\alpha}$, rank, and dropout ratio are set to 32, 8, and 0.1, respectively.

### Multitask Supervised Training

The training procedure includes two stages.
First, we perform multi-task supervised fine-tuning on the original Whisper model without an LLM.
Second, we integrate the fine-tuned Whisper encoder with the Qwen2 LLM to create the complete ***OSUM*** system, then conduct further supervised training using a larger dataset.

#### Whisper Fine-tuning

The original Whisper model supports a limited scope of speech-understanding tasks, which makes the direct integration of the Whisper with an LLM for multi-task training risky when data and computation resources are constrained.
Therefore, we first fine-tune the Whisper via multi-task data to ensure faster convergence of the ***OSUM*** model.
Furthermore, this stage allows us to verify the reliability of our multi-task data.
Specifically, we expand Whisper's instruction tag set to accommodate more tasks.
Each forward pass executes only a single task.

#### OSUM Training

Training SULMs typically begins with pre-training on an ASR task, which serves as a foundation for incorporating additional speech tasks to enable LLMs to process semantic content from the speech encoder.
Given computational constraints, we introduce an ASR+X paradigm for ***OSUM***'s multi-task training.
It concurrently trains ASR and a secondary task ``X'', accelerating training while allowing the ``X'' task to utilize both textual and acoustic features, thereby potentially improving performance
The ASR+X paradigm follows a two-step process: first, transcribing speech to text (ASR); then, integrating this transcription with acoustic features to execute the target task (X).
This is achieved within the LLM's autoregressive framework by adjusting predicted labels, without modifications to the model architectures or loss functions.
We implemented the ASR+X paradigm by prompting the LLLM with natural language prompts.
[ChatGPT [URL]](https://openai.com/index/chatgpt/) is used to generate 5 candidate prompts for each task, one of which is randomly selected during training.
Table.01 shows examples of the prompts and ASR+X prediction labels.

### Training Data

Our ***OSUM*** is designed to perform multi-task training using diverse speech datasets, with the goal of building a unified model capable of comprehensively understanding input speech in conversational scenarios.
The multi-task training process enables tasks to benefit from shared learning, enhancing overall model performance.
Upon completion of training, ***OSUM*** can be utilized for speech data annotation or further extended into a conversational Speech LLM.
Detailed information about the datasets used for training is provided in Table.02.

### Data Processing Pipeline

The data processing pipeline is crucial for training multi-task SULMs.
In this section, we reveal the data processing schemes used for each task in the ***OSUM*** project, with the aim of providing a valuable reference for academic research.

#### ASR

The training data include publicly available resources like [WenetSpeech](../../Datasets/2021.10.07_WenetSpeech.md)[^12], [AISHELL-1](../../Datasets/2017.09.16_AISHELL-1.md)[^13], [AISHELL-2](../../Datasets/2018.08.31_AISHELL-2.md)[^14], and [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md)[^15], along with our internal ASR dataset, resulting in a total of 24,000 hours.

#### SRWT

For the SRWT task, a Gaussian Mixture Model - Hidden Markov Model (GMM-HMM) based conventional ASR model, is used to conduct force alignment and obtain word-level timestampes.
This model is trained on the 54,000-hour proprietary ASR dataset.
To evaluate its performance, we establish an internal SRWT test set and assess alignment quality using the [Average Alignment Score (AAS) metric](../../Evaluations/AAS.md)[^16].
The GMM-HMM model achieves an AAS of 7.55, demonstrating its efficacy in generating reliable word-level timestamps.

#### SSR

Given the absence of open-sourced tools for annotating style labels directly from audio data, we leverage two text-based LLMs - [Qwen2.5-14B [URL]](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct) and [GLM-4-9B-Chat [URL]](https://huggingface.co/THUDM/glm-4-9b-chat) - to annotate speech transcriptions using carefully designed prompts.
To enhance annotation accuracy and reliability, we retain only the intersection of labeling results from both models.
This intersection-based approach ensures high-quality annotations for training the SSR task.

PS: [MSP-Podcast](../../Datasets/MSP-Podcast.md)[^35], [BIIC-Podcast](../../Datasets/BIIC-Podcast.md)[^36]

#### VED

We have attempted to train a vocal event labeling tool; however, due to the limited availability of training data, its classification performance is suboptimal, especially when vocal events and speech occur within the same audio segment.
Therefore, we employ a Voice Conversion (VC) tool to modify the timbre of vocal event audio and insert it randomly into speech audio, creating a dataset of ASR+VED format.
We find that this approach effectively mitigates the overfitting problems caused by the scarcity of vocal event training data with the assistance of VC.
The open-source vocal event datasets we use include [Audioset](../../Datasets/Audioset.md)[^17], [ESC-50](../../Datasets/ESC-50.md)[^18], [Vocal Sound](../../Datasets/Vocal_Sound.md)[^19], and [Nonspeech7k](../../Datasets/Nonspeech7k.md)[^20], while the ASR data consists solely of [AISHELL-2](../../Datasets/2018.08.31_AISHELL-2.md)[^14].

#### SER

[Emotion2Vec](../SpeechRepresentation/2023.12.23_Emotion2Vec.md)[^21] is the first universal speech emotion representation model.
Without additional fine-tuning, we directly apply the [pre-trained Emotion2Vec+ Large model [URL]](https://huggingface.co/emotion2vec/emotion2vec_plus_large), which is trained on 40,000 hours of emotional speech data, to annotate the audio with emotion labels.
Additionally, we leverage the GLM-4-9B-Chat model to generate emotion labels from the textual transcriptions of the speech.
By intersecting these annotations, we generate high-quality emotional labels for the entire dataset.

#### SGC

Efforts to train a speaker gender classification model to label web-sourced data yield unsatisfactory performance.
Consequently, we discard the pseudo-labeled data and relied solely exclusively on manually labeled datasets for training.
For the SGC task, we select [KeSpeech](../../Datasets/KeSpeech.md)[^22], [Datatang-Kid](../../Datasets/Datatang-Kid.md)[^23], [AISHELL-1](../../Datasets/2017.09.16_AISHELL-1.md)[^13], [AISHELL-2](../../Datasets/2018.08.31_AISHELL-2.md)[^14], [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md)[^15], [Kaggle-CommonVoice](../../Datasets/Kaggle-CommonVoice.md)[^24], and [Magicdata-Read](../../Datasets/Magicdata-Read.md)[^25] as training dataset, as they include reliable speaker gender labels.

#### SAP

Similar to the SGC task, due to the poor performance of the automated labeling model, only manually labeled data is used for training.
We use [KeSpeech](../../Datasets/KeSpeech.md)[^22], [Datatang-Kid](../../Datasets/Datatang-Kid.md)[^23], [Magicdata-Read](../../Datasets/Magicdata-Read.md)[^25], [Kaggle-CommonVoice](../../Datasets/Kaggle-CommonVoice.md)[^24], [AISHELL-ASR0060](../../Datasets/AISHELL.md)[^26], and [AISHELL-ASR0018](../../Datasets/AISHELL.md)[^26] as the training dataset for the SAP task, as these datasets provide reliable speaker age labels.

#### STTC

For the STTC task, we use three types of data.
First, we utilize a human-recorded audio question-answer dataset [Databacker-Conversation](../../Datasets/Databacker-Conversation.md)[^27].
Then, we use a text-based dialogue dataset [LCCC](../../Datasets/LCCC.md)[^28] and the [ChatTTS [URL]](https://github.com/2noise/ChatTTS) system with random speaker capabilities to generate the utterances of the questioner in the dialogue, thus obtaining the speech-text pairs for the dialogue task.
Finally, we filter suitable response sentences from the [WenetSpeech](../../Datasets/2021.10.07_WenetSpeech.md)[^12] dataset using [Qwen2.5-7B [URL]](https://huggingface.co/Qwen/Qwen2.5-7B), guiding the LLM to generate text answers.

## 4·Experiments: 实验

This section begins by presenting our training setup in Section.4.1.
Subsequently, to conduct a more comprehensive evaluation of ***OSUM***, we establish a series of complete internal evaluation sets, as detailed in Section.4.2.
Finally, we report the performance of ***OSUM*** on both public and internal test sets, accompanied by an analysis in Section.4.3.

### Training Setup: 训练设置

The two-stage training process for ***OSUM*** is detailed as follows:

#### Whisper Fine-tuning

In the first stage, we fine-tune the Whisper-Medium model on the multi-task datasets described in Table.02.
This stage is conducted on 8 Nvidia A6000 GPUs.
A warm-up scheduler is employed to adjust the learning rate, peaking at 5e-5.
The multitask Whisper is trained for 150,000 steps, which takes approximately 15.3 days.

#### OSUM Training

In the second stage, we conduct experiments on 24 Huawei Ascend NPUs, using a learning rate of 5e-5.
The process completes a total of 528,000 training steps and consumes 7.5 days.

### Internal Test Sets

Currently, most SULMs evaluate multi-task performance using publicly available English datasets ([Qwen-Audio](../SpokenDialogue/2023.11.14_Qwen-Audio.md)[^1]; [Qwen2-Audio](../SpokenDialogue/2024.07.15_Qwen2-Audio.md)[^2]; [Whisper](2022.12.06_Whisper.md)[^5]; [TouchASP](../_Basis/TouchASP.md)[^8]).
However, as ***OSUM*** training incorporates a substantial amount of Chinese data, we have developed a series of internal multi-task test sets tailored for Chinese.
We plan to make the internal test sets publicly available in the future.
These complement the publicly available English test sets, creating a more comprehensive evaluation framework.
To support the ASR+X paradigm, we further enhance the test sets with speech transcripts.
However, ASR metrics are used solely for internal reference to assess model convergence and will not be publicly reported.
Table.03 presents a description of our internal multi-task test sets.

### Main Results

Table.04 and Table.05 show the experimental results of our ***OSUM*** across various tasks.
The results reveal that our approach achieves performance that is comparable to, and in many cases superior to, speech understanding models such as Qwen-audio, Qwen2-audio, Whisper, and SenseVoice.
Furthermore, in this section, we will highlight the performance disparities between our model and other comparable approaches, while providing a detailed analysis of the challenges SULMs face in these tasks.
We hope that these experiences can provide useful references for researchers.

#### ASR

As illustrated in Table.04, our approach reveals obvious advantages in the ASR task on the Chinese test sets.
Notably, the proposed ***OSUM*** consistently outperforms other models on the WenetSpeech test-meeting set, three AISHELL-2 sub-test sets, and three internally used SpeechIO test sets.
While ***OSUM*** does not surpass the top-performing method on the English test set, it rivals performance comparable to SenseVoice-S.
These results are achieved, remarkably, with substantially less training data.
This underscores that our ASR+X task paradigm effectively enhances model convergence in ASR tasks, significantly minimizing the data and computational resources required for SULMs training.

#### SRWT

Table.05 presents the SRWT evaluation results for our proposed ***OSUM*** model compared to Whisper-Large-v3, Qwen-Audio, and the GMM-HMM model used for generating annotated data in SRWT tasks.
Our ***OSUM*** model significantly outperforms Whisper-Large-v3 by relative ${31.38\%}$ and also surpasses Qwen-Audio.
Additionally, our ***OSUM***'s SRWT performance is very close to the GMM-HMM model, which is widely regarded for its high accuracy in timestamp prediction.
These results underscore the effectiveness of ***OSUM*** in the SRWT task.
While ***OSUM***'s performance is slightly below that of the GMM-HMM model, its primary advantage lies in its ability to predict timestamps in an end-to-end fashion, simplifying the integration with other tasks, such as speaker diarization.

#### VED

We first evaluate ***OSUM***'s performance on the public test sets ESC-50 and VocalSound.
However, since the event categories in these two datasets do not completely align with those in ***OSUM***, the comparison to other approaches should only serve as a rough assessment.
Specifically, the ESC-50 contains a substantial number of non-vocal audio events, we categorize them as "other." The experimental results on this test set demonstrate that our model successfully classifies these non-vocal audio events as "other." Additionally, on the VocalSound set, we select the categories supported by ***OSUM*** and calculate the average accuracy across these categories.
This result reveals that our ***OSUM*** exhibits a gap compared to Qwen2-audio, primarily due to our training data consisting of concatenated speech and vocal events.
In contrast, the VocalSound test set includes only the latter, resulting in a significant mismatch.
Nevertheless, our ***OSUM*** achieves a norm level, successfully identifying the most independent vocal events.
In our internal human-recoded ASR+VED test set, PANNs become unusable due to similar mismatches, particularly because their design treats speech as a standalone event, exacerbating accuracy degradation.
Qwen2-audio performs relatively better but also experiences a performance decline in our test set, likely due to overfitting.
In contrast, our model demonstrates balanced results in both the public and internal test sets, showcasing enhanced generalization.
This indicates that using VC to augment data for vocal events can effectively mitigate overfitting in VED tasks.

#### SER

For the SER task, we extract the categories supported by ***OSUM*** from the public datasets [MELD](../../Datasets/2018.10.05_MELD.md)[^29] and MER2023 for testing, followed by a comprehensive evaluation on our internal test set.
In the experiments with the public datasets, ***OSUM*** demonstrates superior performance on the MER2023 test set, outperforming several recent public benchmark models.
On the MELD dataset, ***OSUM***'s performance ranks just below the SenseVoice-L model, likely due to the latter's additional training on a larger-scale speech emotion dataset.
In addition, while ***OSUM***'s result on the internal test set is comparable to that of the EmoBox model, it significantly surpasses other comparative approaches.
Furthermore, we observe that among the eight emotions supported, disgust and fear are particularly challenging to recognize, a difficulty partly attributed to the scarcity of training data for these two emotions.
In our future work, we plan to enhance the model's performance and generalization capability by utilizing ***OSUM*** for labeling, thereby obtaining a larger and more balanced emotion dataset.

Test data is selected from five publicly available emotional evaluation sets:
[IEMOCAP](../../Datasets/IEMOCAP.md)[^30], [MER2023](../../Datasets/MER2023.md)[^31], [M3ED](../../Datasets/M3ED.md)[^32], [MSP-IMPROV](../../Datasets/Msp-improv.md)[^33], and [ESD](../../Datasets/2020.10.28_ESD.md)[^34], including both Chinese and English, encompassing eight types of emotions.

#### SSR

The acoustic-text dual-modal style classification employed by our ***OSUM*** significantly outperforms the single-text modality of GLM-4-9B-Chat.
It demonstrates a strong ability to distinguish among eight styles: news and science reporting, horror stories, fairy tales, customer service, poetry and prose, audiobooks, spontaneous conversation, and others.
Notably, the classification performance for news science communication, audiobooks, fairy tales, and customer service styles is commendable; however, there remains room for improvement in the categorization of poetry and prose, horror stories, and other styles.
Moving forward, we leverage ***OSUM*** to label additional data, aiming to enhance data quality and optimize the distribution across categories.

#### SGC

In the SGC task, we evaluate Qwen2-Audio and ***OSUM***.
The results demonstrate that ***OSUM*** achieves an ${100\%}$ accuracy on the AISHELL-1 test set.
While this result is commendable, we suspect it may indicate some degree of overfitting.
Furthermore, on the Kaggle test set, our approach slightly outperforms Qwen2-Audio, yet it falls short on our internal test set.
This is likely due to the presence of background music in our internal test set, a characteristic less prevalent in ***OSUM***'s training data.
Addressing this limitation is a focus of our future work.
Overall, ***OSUM*** exhibits its value in the SGC task.

#### SAP

We also compare our ***OSUM*** with Qwen2-Audio on the SAP task.
During our previous experiments, we found that the acoustic similarity between teenagers and adults is remarkably high, complicating effective differentiation.
Consequently, we categorize age into three groups: child, adult, and old.
Curiously, despite our efforts to debug the prompts, Qwen2-Audio demonstrates a lower age classification accuracy on both the Kaggle test set and our internal test set.
This may stem from their overly detailed age categorization, which hinders the model's training accuracy.
Our model significantly surpasses Qwen2-Audio on the Kaggle test set, achieving an accuracy of ${76.72\%}$.
Although the classification accuracy slightly declines on our proprietary test set, it still outperforms Qwen2-Audio.
This indicates that our model exhibits strong generalization capabilities on different data.

#### STTC

In the STTC task, we follow AirBench's evaluation protocol across all test sets.
This involves providing the text of audio queries along with the text of two distinct answers, allowing a text-based LLM to assign subjective scores from 1 to 10.
The two answers consist of a real response and the answer generated by SULMs.
While AirBench employs GPT-4 as the scoring LLM, it is currently inaccessible, so we instead utilize GPT-3.5-Turbo.
The test results presented in Table.05 indicate that, on AirBench's official speech sub-test set, our score is lower than that of Qwen2-Audio, suggesting that our model's capabilities in English conversation and audio description lag behind those of Qwen2-Audio.
This is primarily because we did not use English conversational data for training; the current score relies entirely on the LLM's performance.
Furthermore, the gap narrows significantly on our internal Chinese conversational test set, suggesting that our strategy of performing ASR before chat is beneficial.
Overall, our ***OSUM*** still lags behind Qwen2-Audio in conversational ability, which will be a key focus of our future improvements.

## 5·Future Works: 未来工作

While ***OSUM*** demonstrates commendable performance, our research remains an ongoing endeavor to push the boundaries of academic exploration in Speech LLMs.
In the coming months, we aim to address several key areas for improvement and innovation:

- **Expanding OSUM’s Functionalities.**
We plan to enhance ***OSUM*** with additional capabilities, such as language and accent identification, to broaden its applicability in multilingual and diverse speech scenarios.

- **Enabling Multi-Task Capability.**
We aim to activate ***OSUM***'s ability to perform multiple tasks simultaneously, such as identifying the emotion, age, gender, and speaking style in a single inference pass.
Leveraging this multi-task capability, we plan to develop a versatile data labeling tool to streamline audio data processing pipelines.

- **Incorporating Full-Duplex Voice Interaction.**
To improve naturalness and responsiveness, we plan to integrate full-duplex voice interaction capabilities into ***OSUM***.
This enhancement will allow ***OSUM*** to generate context-aware, natural responses, such as matching the questioner’s emotion or mimicking specific speaking styles, like that of a child.

- **Open Science Contributions.**
As part of our commitment to advancing academic research, we will continue to share detailed training methodologies, data pipelines, and model updates.
Our aim is to foster collaboration, provide valuable resources for researchers, and democratize access to cutting-edge Speech LLM technologies.

Through these efforts, we seek to extend ***OSUM***’s capabilities, establish new benchmarks for Speech LLMs, and contribute meaningfully to the academic study and practical applications of speech understanding.

## 6·Conclusions: 结论

In this study, we propose ***OSUM***, an open-source Speech understanding language model.
The ***OSUM*** model integrates a Whisper encoder with a Qwen2 LLM, supporting eight speech tasks.
By employing an ASR+X training strategy, ***OSUM*** achieves efficient and stable multi-task training, simultaneously optimizing ASR alongside target tasks.
Beyond delivering robust performance, ***OSUM*** prioritizes transparency by providing openly accessible data preparation and training methodologies, offering valuable insights and practical guidance for the academic community.

## Reference

[^26]: [AISHELL Tech Co Ltd. Data Products. [URL]](https://www.aishelltech.com/General_Datasets) 2024.

[^7]: [FunaudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction between Humans and LLMs.](2024.07.04_FunAudioLLM.md) ArXiv:2407.04051.

[^9]: [Qwen Technical Report.](../TextLM/2023.09.28_Qwen.md) ArXiv:2309.16609.

[^13]: [AISHELL-1: An Open-Source Mandarin Speech Corpus and a Speech Recognition Baseline.](../../Datasets/2017.09.16_AISHELL-1.md) O-COCOSDA2017.

[^30]: [IEMOCAP: Interactive Emotional Dyadic Motion Capture Database.](../../Datasets/IEMOCAP.md) LREC2008.

[^33]: [MSP-IMPROV: An Acted Corpus of Dyadic Interactions to Study Emotion Perception.](../../Datasets/Msp-improv.md) IEEE@TAC2017.

[^1]: [Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models.](../SpokenDialogue/2023.11.14_Qwen-Audio.md) ArXiv:2311.07919.

[^2]: [Qwen2-Audio Technical Report.](../SpokenDialogue/2024.07.15_Qwen2-Audio.md) ArXiv:2407.10759.

[^27]: [Databaker Tech Co Ltd. Data products [URL]](https://www.data-baker.com) 2024.

[^23]: [Datatang Tech Co Ltd. Data products [URL]](https://www.datatang.com/speechRecognition) 2024

[^14]: [AISHELL-2: Transforming Mandarin ASR Research into Industrial Scale.](../../Datasets/2018.08.31_AISHELL-2.md) ArXiv:1808.10583.

[^17]: [Audio Set: An Ontology and Human-Labeled Dataset for Audio Events.](../../Datasets/Audioset.md) IEEE@ICASSP2017.

[^19]: [Vocalsound: A Dataset for Improving Human Vocal Sounds Recognition.](../../Datasets/Vocal_Sound.md) IEEE@ICASSP2022.

[^11]: [LoRA: Low-Rank Adaptation of Large Language Models.](../../Modules/LoRA/2021.06.17_LoRA.md) ICLR2022.

[^25]: [Kaggle Community. Datasets. [URL]](https://www.kaggle.com/datasets/mozillaorg/common-voice) 2017.

[^31]: [MER 2023: Multi-Label Learning, Modality Robustness, and Semi-Supervised Learning.](../../Datasets/MER2023.md) ACM MM2023.

[^21]: [Emotion2Vec: Self-Supervised Pre-Training for Speech Emotion Representation.](../SpeechRepresentation/2023.12.23_Emotion2Vec.md) ACL2024.

[^35]: [The MSP-Conversation Corpus.](../../Datasets/MSP-Podcast.md) Interspeech2020.

[^24]: [OpenSLR. MAGICDATA mandarin Chinese read speech corpus. [URL]](https://openslr.org/68/) 2019.

[^15]: [LibriSpeech: An ASR Corpus Based on Public Domain Audio Books.](../../Datasets/2015.04.19_LibriSpeech.md) IEEE@ICASSP2015.

[^10]: [Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data.](2023.09.25_OWSM.md) IEEE@ASRU2023.

[^18]: [ESC: Dataset for Environmental Sound Classification.](../../Datasets/ESC-50.md) ACM MM2015.

[^29]: [MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations.](../../Datasets/2018.10.05_MELD.md) ACL2019.

[^5]: [Robust Speech Recognition via Large-Scale Weak Supervision.](2022.12.06_Whisper.md) ICML2023.

[^20]: [Nonspeech7k Dataset: Classification and Analysis of Human Non-Speech Sound.](../../Datasets/Nonspeech7k.md) IET Signal Processing2023.

[^16]: [Achieving Timestamp Prediction while Recognizing with Non-Autoregressive End-to-End ASR Model.](../../Evaluations/AAS.md) NCMMSC2022.

[^8]: [TouchASP: Elastic Automatic Speech Perception that Everyone Can Touch.](../_Basis/TouchASP.md) ArXiv:2412.15622.

[^3]: [PandaGPT: One Model to Instructionfollow Them All.](../CV/2023.05.25_PandaGPT.md) TLLM2023.

[^4]: [SALMONN: Towards generic hearing abilities for large language models.](../SpokenDialogue/2023.10.20_SALMONN.md) ICLR2024.

[^22]: [Kespeech: An Open Source Speech Dataset of Mandarin and Its Eight Subdialects.](../../Datasets/KeSpeech.md) NeurIPS2021.

[^36]: [An Intelligent Infrastructure toward Large Scale Naturalistic Affective Speech Corpora Collection.](../../Datasets/BIIC-Podcast.md) ACII2023.

[^6]: [Attention is All You Need.](../_Transformer/2017.06.12_Transformer.md) NeurIPS2017.

[^28]: [A Large-Scale Chinese Short-Text Conversation Dataset.](../../Datasets/LCCC.md) NLPCC2020.

[^12]: [WenetSpeech: A 10000+ Hours Multi-Domain Mandarin Corpus for Speech Recognition.](../../Datasets/2021.10.07_WenetSpeech.md) IEEE@ICASSP2022.

[^32]: [M3ED: Multi-Modal Multi-Scene Multi-Label Emotional Dialogue Database.](../../Datasets/M3ED.md) ACL2022.

[^34]: [Seen and Unseen Emotional Style Transfer for Voice Conversion with a New Emotional Speech Dataset.](../../Datasets/2020.10.28_ESD.md) IEEE@ICASSP2021.
