# SPEAR-TTS

<details>
<summary>基本信息</summary>

- 标题: "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision"
- 作者:
  - 01 Eugene Kharitonov - Google
  - 02 Damien Vincent - Google
  - 03 Zalan Borsos - Google
  - 04 Raphael Marinier - Google
  - 05 Sertan Girgin - Google
  - 06 Olivier Pietquin - Google
  - 07 Matt Sharifi - Google
  - 08 Marco Tagliasacchi - Google
  - 09 Neil Zeghidour - Google
- 链接:
  - [ArXiv](https://arxiv.org/abs/2302.03540)
  - [Publication](https://doi.org/10.1162/tacl_a_00618)
  - [Github]
  - [Demo](https://google-research.github.io/seanet/speartts/examples/)
- 文件:
  - [ArXiv](_PDF/2302.03540v1__SPEAR-TTS__Speak_Read_&_Prompt__High-Fidelity_TTS_with_Minimial_Supervision.pdf)
  - [Publication](_PDF/2302.03540p0__SPEAR-TTS__TACL2023.pdf)

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

We introduce ***SPEAR-TTS***, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision.
By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to "reading") and from semantic tokens to low-level acoustic tokens ("speaking").
Decoupling these two tasks enables training of the "speaking" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the "reading" component.
To control the speaker identity, we adopt example prompting, which allows ***SPEAR-TTS*** to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels.
Our experiments demonstrate that ***SPEAR-TTS*** achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.

</details>
<br>

我们介绍了 ***SPEAR-TTS***, 这是一个多说话人的文本转语音系统, 可以在最小监督下进行训练.
通过结合两种离散的语音表示, 我们将文本转语音视为两个序列到序列任务的组合:
- 从文本到高级语义 Token (类似于 "阅读")
- 从语义 Token 到低级声学 Token (类似于 "说话")

解耦这两个任务使得可以使用丰富的仅音频数据训练 "说话" 模块, 并解锁了预训练和回译的高效组合, 以减少训练 "阅读" 组件时对平行数据的需求.
为了控制说话人身份, 我们采用了示例提示, 这使得 ***SPEAR-TTS*** 能够仅使用 3 秒的短样本泛化到未见过的说话人, 而无需任何显式的说话人表示或说话人 ID 标签.
我们的实验表明 ***SPEAR-TTS*** 在使用仅 15 分钟的平行数据时, 字符错误率与最先进的方法相当, 同时在主观测试中, 其自然度和声学质量与真实语音相当.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论