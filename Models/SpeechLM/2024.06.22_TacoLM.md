# TacoLM

<details>
<summary>基本信息</summary>

- 标题: "TacoLM: GaTed Attention Equipped Codec Language Model are Efficient Zero-Shot Text to Speech Synthesizers"
- 作者:
  - 01 Yakun Song (宋雅昆)
  - 02 Zhuo Chen (陈卓)
  - 03 Xiaofei Wang (王晓飞)
  - 04 Ziyang Ma (马子阳)
  - 05 Guanrou Yang (杨冠柔)
  - 06 Xie Chen (陈谐)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.15752)
  - [Publication](https://doi.org/10.21437/Interspeech.2024-1531) InterSpeech 2024
  - [Github](https://github.com/Ereboas/TacoLM)
  - [Demo]
- 文件:
  - [ArXiv](_PDF/2406.15752v1__TacoLM__Gated_Attention_Equipped_Codec_Language_Model_are_Efficient_Zero-Shot_TTS_Synthesizers.pdf)
  - [Publication](_PDF/2406.15752p0__TacoLM__InterSpeech2024.pdf)

</details>

## Abstract: 摘要

Neural codec language model (LM) has demonstrated strong capability in zero-shot text-to-speech (TTS) synthesis.
However, the codec LM often suffers from limitations in inference speed and stability, due to its auto-regressive nature and implicit alignment between text and audio.
In this work, to handle these challenges, we introduce a new variant of neural codec LM, namely ***TacoLM***.
Specifically, ***TacoLM*** introduces a gated attention mechanism to improve the training and inference efficiency and reduce the model size.
Meanwhile, an additional gated cross-attention layer is included for each decoder layer, which improves the efficiency and content accuracy of the synthesized speech.
In the evaluation of the Librispeech corpus, the proposed ***TacoLM*** achieves a better word error rate, speaker similarity, and mean opinion score, with 90% fewer parameters and 5.2 times speed up, compared with [VALL-E](2023.01.05_VALL-E.md).
Demo and code is available at [this https URL](https://ereboas.github.io/TacoLM/).

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论