# LauraGPT

<details>
<summary>基本信息</summary>

- 标题: "LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT"
- 作者:
  - 01 Zhihao Du
  - 02 Jiaming Wang
  - 03 Qian Chen
  - 04 Yunfei Chu
  - 05 Zhifu Gao
  - 06 Zerui Li
  - 07 Kai Hu
  - 08 Xiaohuan Zhou
  - 09 Jin Xu
  - 10 Ziyang Ma
  - 11 Wen Wang
  - 12 Siqi Zheng
  - 13 Chang Zhou
  - 14 Zhijie Yan
  - 15 Shiliang Zhang
- 链接:
  - [ArXiv](https://arxiv.org/abs/2310.04673)
  - [Publication] ICLR 2024 Reject
  - [Github]
  - [Demo](https://lauragpt.github.io)
- 文件:
  - [ArXiv](_PDF/2310.04673v4__LauraGPT__Listen_Attend_Understand_and_Regenerate_Audio_with_GPT.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Generative Pre-trained Transformer~(GPT) models have achieved remarkable performance on various natural language processing tasks, and have shown great potential as backbones for audio-and-text large language models (LLMs).
Previous mainstream audio-and-text LLMs use discrete audio tokens to represent both input and output audio; however, they suffer from performance degradation on tasks such as automatic speech recognition, speech-to-text translation, and speech enhancement over models using continuous speech features.
In this paper, we propose ***LauraGPT***, a novel unified audio-and-text GPT-based LLM for audio recognition, understanding, and generation.
***LauraGPT*** is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities.
We propose a novel data representation that combines continuous and discrete features for audio: ***LauraGPT*** encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes.
We propose a one-step codec vocoder to overcome the prediction challenge caused by the multimodal distribution of codec tokens.
We fine-tune ***LauraGPT*** using supervised multi-task learning.
Extensive experiments show that ***LauraGPT*** consistently achieves comparable to superior performance compared to strong baselines on a wide range of audio tasks related to content, semantics, paralinguistics, and audio-signal analysis, such as automatic speech recognition, speech-to-text translation, text-to-speech synthesis, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding.

## 1.Introduction: 引言

Large language models (LLMs) are neural networks that generate natural language texts based on a given context.
LLMs can learn from massive amounts of text data and mimic human language to acquire human knowledge.
LLMs such as [GPT-4](../TextLM/2023.03.15_GPT-4.md), [PaLM2](../TextLM/2023.05.17_PaLM2.md), [LLaMA](../TextLM/2023.02.27_LLaMA.md) have demonstrated impressive capabilities across various domains, exhibiting zero-shot generalization without the need for task-specific fine-tuning.
However, these models are primarily limited to processing text data.

Recent research aims to seamlessly integrate text and audio since they are two important modalities for human communication.
These efforts include
- **Audio-to-Text LLMs** ([Whisper](../SpeechLM/2022.12.06_Whisper.md); [USM](../SpeechLM/2023.03.02_Google_USM.md); [Pengi](../SpeechLM/2023.05.19_Pengi.md); [UniverSLU](../SpeechLM/2023.10.04_UniverSLU.md); [SALMONN](2023.10.20_SALMONN.md); [Qwen-Audio](2023.11.14_Qwen-Audio.md)), which can convert audio input into text and perform tasks such as automatic speech recognition (ASR) and spoken language understanding (SLU);
- **Text-to-Audio LLMs** ([UniAudio](../SpeechLM/2023.10.01_UniAudio.md); [Audiobox](2023.12.25_Audiobox.md); [AudioGen](../SpeechLM/2022.09.30_AudioGen.md); [AudioLDM2](../Diffusion/2023.08.10_AudioLDM2.md); [Make-An-Audio](../Diffusion/2023.01.30_Make-An-Audio.md); [VALL-E](../SpeechLM/2023.01.05_VALL-E.md)), which can convert text input into audio and perform tasks such as text-to-speech synthesis (TTS) and text-to-music synthesis.
An emerging line of research focuses on develop more universal and comprehensive **Audio-and-Text LLMs**~\citep{DBLP:conf/acl/AoWZ0RW0KLZWQ0W22,DBLP:journals/corr/abs-2105-03070,DBLP:journals/corr/abs-2305-11000,DBLP:journals/corr/abs-2305-16107,DBLP:journals/corr/abs-2306-12925,DBLP:journals/corr/abs-2304-12995}, which can support audio-and-text tasks, that is, process and generate both audio and text and perform tasks such as speech enhancement (SE) and speech-to-speech translation (S2ST), in addition to tasks supported by audio-to-text and text-to-audio LLMs.
Audio-to-text and text-to-audio LLMs can be considered as subsets of audio-and-text LLMs.

Audio-and-Text LLMs can be categorized into two directions.
One direction builds a collaborative AI system using LLMs as controllers to interface specialized audio models, such as ASR and TTS models, to support various audio-and-text tasks~\citep{DBLP:journals/corr/abs-2303-17580,DBLP:journals/corr/abs-2304-12995}.
 These methods have serious drawbacks, including high complexity, significant resource consumption, and unavoidable error accumulation problems.
The other direction develops a unified Audio-and-Text LLM leveraging LLMs as the backbone to support audio-and-text tasks~\citep{DBLP:conf/acl/AoWZ0RW0KLZWQ0W22,DBLP:journals/corr/abs-2105-03070,DBLP:journals/corr/abs-2305-16107,DBLP:journals/corr/abs-2306-12925}.
Decoder-only audio-and-text LLMs~\citep{DBLP:journals/corr/abs-2305-11000,DBLP:journals/corr/abs-2305-16107,DBLP:journals/corr/abs-2306-12925} are the dominant technique under this category.
These models convert continuous audio into discrete tokens and integrate text and audio tokens into unified vocabulary.
These models suffer from information loss from quantization of speech signals into discrete tokens, which leads to notable performance degradation on ASR compared to models using continuous speech features~\citep{DBLP:journals/corr/abs-2311-04534,DBLP:journals/corr/abs-2305-18108,DBLP:journals/corr/abs-2309-07377,DBLP:journals/corr/abs-2309-10922}.
In this paper, we focus on improving the second category of unified Audio-and-Text LLMs.
Moreover, recent advances in audio generation from unified audio-and-text LLMs ([VALL-E](../SpeechLM/2023.01.05_VALL-E.md))\citep{DBLP:journals/corr/abs-2305-16107} discretize speech into codec codes, then use an autoregressive language model (LM) to predict output tokens from the first quantizer and use a non-autoregressive model to predict tokens from the other quantizers individually.
One limitation of this mechanism is that it needs many prediction steps (hence called multi-step audio synthesis scheme) to generate good quality speech.
Another limitation is that predicting the indices of the other codec groups is challenging due to the multi-modal distribution nature of codec tokens~\citep{DBLP:journals/icassp/lmcodec}.

To overcome the drawbacks of existing unified audio-and-text LLMs, we propose ***LauraGPT***, a novel **unified Audio-and-Text LLM** based on the GPT framework for audio recognition, understanding, and generation.
***LauraGPT*** is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities, with a single model.
We propose a novel data representation that combines continuous and discrete features for audio: ***LauraGPT*** encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes.
This data representation improves the performance of audio-input tasks and also facilitates joint autoregressive modeling of audio and text features for audio generation tasks.

We also propose a one-step codec vocoder in ***LauraGPT*** to address the two limitations of the popular multi-step audio synthesis scheme.
Our one-step codec vocoder uses a transformer-based predictor to estimate the sum of all codec token groups instead of the individual indices, by minimizing the reconstruction losses.
Our approach simplifies the audio generation process to a single feed-forward calculation and also overcomes the prediction challenge caused by the multi-modal distribution of codec tokens.

We fine-tune ***LauraGPT*** using supervised multi-task learning on diverse audio tasks, including tasks focusing on content, semantics, paralinguistics, and audio-signal analysis, such as ASR, speech-to-text translation (S2TT), TTS, SE, automated audio captioning (AAC), speech emotion recognition (SER), and SLU.
Comprehensive experiments show that, to the best of our knowledge, ***LauraGPT*** consistently achieves comparable to superior performance compared to strong baselines on the largest and the most diverse set of audio recognition, understanding, and generation tasks among existing decoder-only unified audio-and-text LLMs focusing on these tasks~\citep{DBLP:journals/corr/abs-2305-11000,DBLP:journals/corr/abs-2305-16107,DBLP:journals/corr/abs-2306-12925}.
Demos  are available at https://lauragpt.github.io.
The results are remarkable since existing general speech models either focus solely on speech recognition and understanding tasks but neglect speech generative tasks, or support speech generation but suffer from severe performance degradation on speech recognition and understanding tasks.

## 2.Related Works: 相关工作

### Audio-to-Text LLMs

Audio-to-Text LLMs can generate text from audio inputs.
[Whisper](../SpeechLM/2022.12.06_Whisper.md) and [USM](../SpeechLM/2023.03.02_Google_USM.md) can perform speech recognition and translation across multiple languages and domains.
[Pengi](../SpeechLM/2023.05.19_Pengi.md) is an audio LM that formulates audio tasks as text-generation tasks.
[UniverSLU](../SpeechLM/2023.10.04_UniverSLU.md) is a universal SLU model that supports various speech classification and sequence generation tasks.
[SALMONN](2023.10.20_SALMONN.md) and [Qwen-Audio](2023.11.14_Qwen-Audio.md) integrate pre-trained text LLMs with separate speech and audio encoders into a single multimodal model.

### Text-to-Audio LLMs

Text-to-Audio LLMs can convert text input into audio output and perform tasks such as TTS or text-to-music synthesis.
Recently, two prominent categories of approaches have emerged for generating audio from text prompts.
In the first category, continuous representations such as utterance-level embeddings (DBLP:journals/corr/abs-2206-04769, DBLP:conf/icml/LiuCYMLM0P23; [Make-An-Audio](../Diffusion/2023.01.30_Make-An-Audio.md)) and Mel-frequency spectrograms~\citep{DBLP:journals/corr/abs-2305-15255} are used as the targets.
However, continuous representations present a challenge for unified modeling of text and audio within a single LM.
In the second category, discrete codec tokens are employed as audio representations and generated by diffusion models~\citep{DBLP:journals/taslp/YangYWWWZY23} or autoregressive LMs ([AudioGen](../SpeechLM/2022.09.30_AudioGen.md); [AudioLM](../SpeechLM/2022.09.07_AudioLM.md); [MusicGen](2023.06.08_MusicGen.md); [VALL-E](../SpeechLM/2023.01.05_VALL-E.md)).
Among models in the second category, in models such as [AudioGen](../SpeechLM/2022.09.30_AudioGen.md), [AudioLM](../SpeechLM/2022.09.07_AudioLM.md), and [MusicGen](2023.06.08_MusicGen.md), multiple output heads are used after the LM to predict synchronized or delayed groups of codec tokens.
However, this mechanism is only suitable for audio generation and may not be applicable to diverse audio-and-text tasks.
Alternatively, in [VALL-E](../SpeechLM/2023.01.05_VALL-E.md), the LM predicts output tokens of the first quantizer, while tokens of the remaining quantizers are predicted by a non-autoregressive model one by one.
This mechanism requires numerous prediction procedures to generate acceptable speech quality.
Moreover, the indices of the remaining codec groups are challenging to predict due to the multi-modal distribution nature of codec tokens~\citep{DBLP:journals/icassp/lmcodec}.

### Audio-and-Text LLMs

Audio-and-Text LLMs can process and generate both audio and text, which can be categorized into two directions.
One direction uses LLMs as controllers to interface specialized audio models, such as ASR and TTS models, to enable direct audio interaction with LLMs and support various audio-and-text tasks, such as HuggingGPT~\citep{DBLP:journals/corr/abs-2303-17580} and AudioGPT~\citep{DBLP:journals/corr/abs-2304-12995}.
However, these models are complex, resource-intensive, and prone to error accumulation.
The second direction uses LLMs as the backbone for a unified model that handles audio-and-text tasks~\citep{DBLP:conf/acl/AoWZ0RW0KLZWQ0W22,DBLP:journals/corr/abs-2105-03070,DBLP:journals/corr/abs-2305-16107,DBLP:journals/corr/abs-2306-12925}.
SpeechT5~\citep{DBLP:conf/acl/AoWZ0RW0KLZWQ0W22} and SpeechNet~\citep{DBLP:journals/corr/abs-2105-03070} perform various speech tasks with an encoder-decoder model, but they require modal-specific pre-nets and post-nets to deal with different input\&output modalities.
VioLA~\citep{DBLP:journals/corr/abs-2305-16107}, AudioPaLM~\citep{DBLP:journals/corr/abs-2306-12925}, SpeechGPT~\citep{DBLP:journals/corr/abs-2305-11000}, and SpeechGen~\citep{DBLP:journals/corr/abs-2306-02207} use decoder-only Transformers to model discrete audio tokens and text tokens as a shared vocabulary, but they suffer from information loss from quantization of audio signals into discrete tokens~\citep{DBLP:journals/corr/abs-2311-04534,DBLP:journals/corr/abs-2305-18108,DBLP:journals/corr/abs-2309-07377,DBLP:journals/corr/abs-2309-10922}.

## 3.Methodology: 方法

Figure~\ref{fig:overall} depicts the architecture of the proposed ***LauraGPT***.
Section \ref{sec:gpt-backbone} describes the audio encoder, the text tokenizer, and the modified GPT LM for unified audio-and-text modeling.
Section~\ref{sec:audio-tokenizer} elaborates the audio tokenizer.
Section~\ref{sec:codec_vocoder} introduces an efficient one-step codec vocoder for converting audio tokens into high-quality raw waveforms.
Section~\ref{sec:task-details} describes the multi-task fine-tuning and shows that ***LauraGPT*** provides an extensible framework for supporting more complex tasks.

### 3.1.Modified Language Model for Unifying Audio-and-Text Modeling

For audio inputs, different from other audio-and-text LLMs using discrete tokens to represent audio inputs, we extract the log-compressed Mel spectrogram features and convert them into continuous representations using a Conformer-based audio encoder.
Text inputs and outputs are tokenized using the Qwen tokenizer \cite{qwen}, which inherits the tiktoken tokenizer~\citep{tiktoken} and incorporates additional augmentations for commonly used characters and words in different languages.
The tokenized input text undergoes embedding matrix transformation to generate dense vectors.
The audio representations and text embeddings have the same dimension $D$.
The Conformer-based encoder is initialized with weights from a pre-trained ASR model~\citep{gao2023funasr}.
Since batch normalization can lead to endless loop decoding, we replace it with layer normalization in the Conformer-based encoder (details are in Appendix~\ref{sec:normlization}).

To achieve audio generation capabilities, the audio outputs are discretized into tokens using an audio tokenizer (Section~\ref{sec:audio-tokenizer}) to obtain discrete representations and the softmax output layer is augmented with the audio tokens.
As a result, the weight matrix $\mathbf{W}$ in the output layer is of size $(N+M+L) \times D$ and is utilized to calculate the logits for audio and text tokens at each position, where $N$, $M$, and $L$ denote the vocabulary sizes of text, audio, and task tokens, respectively.
Task tokens are used to inform the model which task should be performed.
Note that in order to control the sequence length, we perform the low frame rate~(LFR) method~\citep{gao2020san-m} to downsample audio inputs to 60ms and only select the first codec group of the audio outputs.

Based on the aforementioned representations, the GPT backbone is trained to model various audio and text tasks by minimizing the cross-entropy loss:

$$
    \mathcal{L}_{LM}=-\frac{1}{T_v}\sum_{j=1}^{T_v}{
    \log p_\theta\left(
    \mathbf{v}_j | \mathbf{u}_{1:T_u}, \mathbf{u}_{task},\mathbf{v}_{1:j-1}
    \right)
    }
$$

where $\mathbf{u}$ denotes the input embeddings with a sequence length $T_u$ and $\mathbf{v}$ represents the sequence of target tokens with a length $T_v$.
To specify a task, a special task-related token $\mathbf{u}_{task}$ is inserted between the input embeddings and output tokens.
Note that only the losses of outputs are taken into account, while losses on inputs and task token embeddings are masked out.
After the final output layer, audio tokens are decoded to raw waveforms using a codec vocoder (Section \ref{sec:codec_vocoder}).
Since it is challenging to train an LLM from scratch with limited data and computational resources, we use the open-source GPT LLM, Qwen~\citep{qwen}, as the backbone.
Qwen is pre-trained on a diverse corpus covering various domains in English and Chinese and supports 8192 context length.
Compared with other open-source GPT models with similar model sizes, Qwen models demonstrate impressive competitiveness, achieving better performance on widely used benchmarks, especially on Chinese tasks~\citep{qwen}.
Within ***LauraGPT***, all parameters including the Qwen backbone are jointly optimized, except for the codec vocoder, which is trained independently and kept frozen during both training and inference stages of ***LauraGPT***.

### 3.2.Audio tokenizer

For audio generation, we utilize a codec model as the audio tokenizer to extract discrete representations.
Our codec model shares a similar architecture as EnCodec~\citep{defossez2022highfi}, which comprises convolutional recurrent encoder and decoder~\citep{DBLP:conf/interspeech/TagliasacchiLMR20} and a residual vector quantizer (RVQ)~\citep{vasuki2006review}.
We enhance the original EnCodec model with the following modifications:
1) Add reconstruction losses in the magnitude spectrum domain to improve the quality of middle- and high-frequency signals.
2) Stack five strided convolution blocks with strides of $[8, 5, 4, 2, 2]$ to address the challenge of long sequence lengths, resulting in a token rate of 25Hz for each token group.
3) Use 32 quantizers with structured dropout in the RVQ module, each with vocabulary size 1024.
This revision improves speech quality with more quantizers while preserving most information in the shallow quantizers.
The encoder and the first RVQ quantizer are used as the audio tokenizer, and the outputs of the first quantizer are used as the audio tokens.
The choice of the first $N$ RVQ quantizers to use is a tradeoff between performance and sequence length (hence efficiency).
The remaining quantizers and the decoder are only used when training the codec model.
Details of training and the pre-trained codec model are in ~\cite{du2023funcodec}.

### 3.3.One-step Codec Vocoder for Audio Generation

We propose a one-step codec vocoder in ***LauraGPT*** to generate waveforms from the audio tokens, which are extracted from the first quantizer as described in Section~\ref{sec:audio-tokenizer}.
Our vocoder comprises two components: a transformer-based predictor and a codec decoder.
The predictor is trained to estimate the summation of codec embeddings from the 32 RVQ quantizers by minimizing the L1 and L2 distances between the predicted embeddings $\hat{\mathbf{E}}$ and their corresponding ground truth $\mathbf{E}$:

$$
    \mathcal{L}_{pre}=\sum_{t,i}^{T,D_c}{|\mathbf{E}_{t,i}-\hat{\mathbf{E}}_{t,i}|_1 + |\mathbf{E}_{t,i}-\hat{\mathbf{E}}_{t,i}|_2}
$$

where $T$ denotes the total number of frames and $D_{c}$ denotes the dimension of the codec embeddings.
After obtaining the estimated embeddings, the decoder of an pre-trained codec model is utilized to reconstruct the raw audio waveforms.

**Alongside the predicted audio tokens from the LLM, text and audio inputs are used as conditions and fed to the predictor**.
For zero-shot TTS task, the text inputs serve as a condition as well as the prompt audio features.
For SE task, the input noisy speech features are employed as conditions.
Such text and audio conditionings allow the model to generate high-quality audio signals by leveraging the diverse information in prompt audios and noisy speeches, which is lacked in the discrete tokens (output from the first quantizer).
Therefore, different from existing Text-to-Audio LLMs, **our approach simplifies the audio generation process to a single feed-forward calculation and overcomes the prediction challenge caused by the multi-modal distribution of codec tokens**.

### 3.4.Multi-task Finetuning

#### Basic Tasks

We unify modeling of the following basic tasks in the single ***LauraGPT*** model and use these tasks for multi-task fine-tuning:
Automatic Speech Recognition (**ASR**), Spoken Language Understanding (**SLU**), Speech-to-Text Translation (**S2TT**), Speech Emotion Recognition (**SER**), Automated Audio Captioning (**AAC**), Speech Enhancement (**SE**), and Text-to-speech Synthesis (**TTS**).
Task definitions are in Appendix~\ref{sec:task-intro}.

#### Unified Task Expression

***LauraGPT*** operates based on a unified task expression: `[input embeddings, task ID, output tokens]`.
With the same inputs, the desired outputs can differ across tasks.
For instance, ASR and S2TT tasks require different outputs even for the same audio input.
Task tokens are included in both input embedding and output weight matrices.
The TTS task takes text embeddings as inputs, while the ASR, S2TT, SLU, SE, ACC, and SER tasks take audio encodings as inputs.
The TTS and SE tasks use audio tokens as the target outputs, while the remaining tasks use text tokens as the target outputs.

#### Support More Complex Tasks

With its modularized design, ***LauraGPT*** provides an extensible framework to support complex tasks.
By breaking a task into sub-tasks among the basic tasks and cascading the raw inputs and model outputs of sub-tasks, ***LauraGPT*** can perform more complex tasks.
For example, we demonstrate that ***LauraGPT*** is capable of performing the advanced speech-to-speech translation (S2ST) task by combining the S2TT and TTS tasks.
Initially, a sequence is constructed to translate the speech content into the target language text using the S2TT task token: `[audio encoding, <S2TT>]`.
Subsequently, the translated text is combined with the TTS task token to synthesize speech: `[text embedding, <TTS>]`.
If maintaining the speaker identity is desired, the original inputs and content can be incorporated to perform personalized TTS.
This can be achieved with an input sequence as `[ASR recognized text embedding, S2TT translated text embedding, <TTS>, audio token of input speech]`, where `ASR recognized text embedding` is obtained using the ASR task: `[audio encoding, <ASR>]`.
This approach treats the bilingual text as the complete input and allows the model to generate an output sequence of codec tokens while maintaining the same speaker identity.
Audio samples of S2ST can be found on the demo site.
More examples of complex tasks are in Appendix~\ref{appendix:more-complex-tasks}.

## 4.Experiments: 实验

### Model Architecture

The Conformer-based audio encoder consists of 32 conformer blocks.
Each block consists of a feed-forward module with 1536 units, an attention module with 16 heads and a dimension of 512, a convolutional module including the pointwise and depthwise convolution layers, and a second feed-forward module with 1536 units.
Sinusoidal positional encoding is applied on the audio inputs.
For a trade-off between performance and training efficiency, we use [Qwen-1.8B](https://github.com/QwenLM/Qwen) as the backbone and ***LauraGPT*** has 2B parameters.
Qwen-1.8B comprises 24 transformer layers with a hidden size 2048 and 16 attention heads.
Although Conformer and Qwen-1.8B are selected as the audio encoder and GPT backbone, they can be replaced by other encoders and GPT models.

### Training Setup

In all experiments, we initialize the Qwen backbone and audio encoder with the pre-trained checkpoints.
We then optimize the model parameters through multi-task fine-tuning.
The training\&test datasets and evaluation metrics are presented in Appendix~\ref{sec:training-datasets} and ~\ref{sec:evaldata-metrics}.
Appendix~\ref{sec:detail_training_setup} describes the three-stage training process to address the significant variation in data volume across different tasks, and details the inference process.

## 5.Results: 结果

Section~\ref{sec:experiments} presents the main results of performance comparison on the basic tasks from the state-of-the-art (SOTA) model, a **comparable** baseline, and our ***LauraGPT***.
Ablation studies in Section~\ref{sec:analysis} demonstrate the advantages of using continuous representations for audio inputs in ***LauraGPT*** by comparing to a counterpart with both discrete inputs and outputs (denoted **Discrete IO**), the superiority of our one-step codec vocoder, and effectiveness of multi-task finetuning.
Further analyses include comparison with related unified Audio-and-Text LLMs~(Appendix~\ref{appendix:comparison-other-text-audio-model}), more analysis of multi-task fine-tuning on SER task~(Appendix~\ref{ser-impact-of-multi-task-finetuning}), comparing batch normalization with layer normalization in the audio encoder~(Appendix~\ref{sec:normlization}), and studying impact of initialization from pre-trained models~(Appendix~\ref{sec:init-gpt}).

### 5.1.Results on All Tasks

Table~\ref{tab:overall_res} shows the results from the SOTA model, a comparable baseline, and our ***LauraGPT***, in that order, on a variety of speech recognition, understanding, and generation benchmarks.
Our results are from single runs due to the stability of the models and limited computational resources.
The SOTA model yields the best results on each test set based on our literature review.
The baseline for each task is chosen to facilitate fair comparison with ***LauraGPT***: they are comparable to ***LauraGPT*** in model architecture or training data and are also common competitive baselines in the literature.
We cite the SOTA results to validate that ***LauraGPT*** consistently performs competitively on all the speech recognition, understanding, and generation tasks and the baselines are competitive.
However, ***LauraGPT*** results cannot be fairly compared to the SOTA results.
Specifically, QwenAudio achieves SOTA performance on most speech understanding tasks, but compared to ***LauraGPT***, QwenAudio uses a much larger LLM ($\sim$7B VS.
our 1.8B LLM), and uses the Whisper audio encoder trained on a large amount of ASR data while we use a Conformer encoder trained on much less data.
Moreover, QwenAudio does not support speech generative tasks hence cannot handle SE and TTS tasks.
Paraformer-large and UniverSLU achieve SOTA results on AISHELL-2 test-ios for Chinese ASR and on SLURP test for SLU; however, they only support single tasks and also train on more data than ***LauraGPT*** on the corresponding task.
Appendix~\ref{appendix:comparison-other-text-audio-model} shows that ***LauraGPT*** greatly outperforms Whisper Large V2 on Chinese ASR test sets while the gap on English ASR test sets are primarily attributed to the much smaller English data used for training ***LauraGPT***.
For TTS, the SOTA VALL-E Phone outperforms baseline VALL-E Token, suggesting the importance of text representation for TTS.
(We re-implement two VALL-E models with 0.34B trainable parameters, both trained with the same data as ***LauraGPT***.
VALL-E Phone uses phonemes as the text input representation, while VALL-E Token uses WordPiece tokens from the text tokenizer.)
Compared to both VALL-E models, ***LauraGPT*** achieves comparable speaker similarity (SECS) and speech quality (MOSNet).
The degradation in content consistency (WER) from ***LauraGPT*** results from the generalization issue, since the training data is too limited for ***LauraGPT*** with 2B parameters.
Overall, the results show that ***LauraGPT*** consistently achieves comparable to superior performance than strong baselines on diverse speech tasks, demonstrating the general effectiveness of ***LauraGPT*** on speech recognition, understanding, and generative tasks**.

### 5.2.Analysis

#### Discrete VS.
Continuous Representations for Audio Inputs

Existing unified Audio-and-Text LLMs use discrete tokens to represent audio inputs.
We analyze the efficacy of using continuous representations for audio inputs in ***LauraGPT*** by comparing to its counterpart **Discrete IO** on ASR, S2TT, and SE tasks, representing audio-input recognition and understanding, and audio generation capacities.
In Discrete IO, both audio inputs and outputs are represented by flattened codec tokens from the first four quantizers \footnote{Using outputs of the first quantizer (as in ***LauraGPT***) for audio tokenizer renders very poor performance for audio-input tasks with the Discrete IO models.
Using more quantizers improves performance but reduces efficiency.}, resulting in a token rate of 100Hz.
In ***LauraGPT***, audio inputs are represented by continuous acoustic features, which are also fed into our one-step vocoder as a condition to achieve high-quality outputs.
Table~\ref{tab:diif_input_token} shows that *****LauraGPT*** consistently outperforms Discrete IO with remarkable gains on all tasks**.
**For ASR task**, the performance degrades drastically when replacing continuous features with discrete audio tokens.
Although the performance degradation can be reduced by using more quantizers (more codec groups), e.g., 32~\citep{DBLP:journals/corr/abs-2309-10922}, more codec groups always cause higher token rates and longer sequence and in turn higher computational demands.
**For S2TT task**, Discrete IO only yields BLEU scores of 5.1 and 5.0 on test sets,  basically suggesting lack of translation capability.
**For SE task**, using codec tokens as inputs cannot improve the quality and intelligibility of noisy speeches, suggesting lack of enhancement capability, probably because the distribution of noisy speech is too complicated to be accurately represented by four groups of discrete audio tokens.

#### Comparison on Audio Synthesis Schemes

[VALL-E](../SpeechLM/2023.01.05_VALL-E.md) introduces a commonly used scheme formulating audio synthesis as a classification problem: A neural network is shared to predict the codec tokens in the following group with the previous ones as inputs and synthesizing target audio requires multiple steps or iterations to achieve a reasonable speech quality.
In contrast, our one-step codec vocoder formulates audio synthesis as a regression problem.
As described in Section~\ref{sec:codec_vocoder}, our one-step codec vocoder simplifies audio synthesis into a single feed-forward calculation and overcomes the prediction challenge caused by the multimodal distribution of codec tokens.
Table~\ref{tab:comp-audio-syn-se} shows that our one-step codec vocoder greatly outperforms the multi-step scheme in terms of content consistency (CER, WER) and speech quality (PESQ), while obtaining the same intelligibility (STOI).

#### Effectiveness of Multi-task Finetuning

The multi-task fine-tuned ***LauraGPT*** (Section~\ref{sec:task-details}) could be advantageous over individual single-task models: (1) Multi-task learning could exploit synergy between tasks and reduce over-fitting, in turn yield high performance on diverse tasks and achieve better performance than single-task training.
(2) Multi-task learning could learn a single model capable of supporting a wide range of tasks, hence practical deployment is greatly simplified through unified model implementation and API.

We investigate whether the multi-task trained ***LauraGPT*** could achieve better performance than single-task training for tasks with limited training data.
Among the basic tasks (Table~\ref{tab:datasets}), AAC, SLU, and SER tasks all have limited training data.
We initialize the Qwen backbone and the audio encoder the same as ***LauraGPT*** before conducting multi-task training, then train the single-task model only using the task-specific training data.
The results are shown in Table~\ref{tab:single-multi-compare}.

**For the AAC task**, we find that the multi-task trained ***LauraGPT*** outperforms the single-task model on SPICE, CIDEr and SPIDEr on the Clotho evaluation set.
**For the SLU task**, on the SLURP test set, ***LauraGPT*** greatly outperforms the single-task model on intent accuracy by +2.9 absolute and on SLU-F1 by +22.5 absolute.
**For the SER task**, on the MELD test set, ***LauraGPT*** substantially outperforms the single-task model in terms of UA and the primary WF1 metrics, while the WA result is slightly worse.
More analyses in Appendix~\ref{ser-impact-of-multi-task-finetuning} show that multi-task learning dramatically improves accuracies of the minority classes.
**In summary, these results verify that multi-task learning for ***LauraGPT*** consistently achieves better performance than single-task training for tasks with limited training data.**

## 6.Conclusions: 结论

We propose ***LauraGPT*** that can handle both audio and text inputs and outputs and perform audio recognition, understanding, and generation.
We propose combining continuous and discrete features for audio and a one-step codec vocoder, and employ multi-task learning.
Experiments demonstrate that ***LauraGPT*** achieves comparable to superior performance compared to strong baselines on a wide range of speech tasks on content, semantics, paralinguistics, and audio-signal analysis.

## 7.Limitations: 局限性

In this work, in order to support a wide range of audio recognition, understanding, and generation tasks, we choose to train all parameters in ***LauraGPT*** during supervised multi-task finetuning, including the Qwen backbone, except for the codec vocoder.
This strategy results in substantial computations for training.
In future work, we plan to investigate parameter-efficient fine-tuning to reduce computation demands.
Also, due to the limited computation resources, our comparisons between the multi-task trained ***LauraGPT*** and single-task models are focused on the low-resource tasks, that is, AAC, SLU, and SER tasks.
We find that multi-task learning for ***LauraGPT*** consistently achieves better performance than single-task training for tasks with limited training data.
Next, we plan to complete comparisons of ***LauraGPT*** and single-task models on all tasks, including relatively rich-resource tasks such as ASR.
These studies will promote understandings on where tasks could benefit from each other, including tasks with even conflicting objectives.
We also plan to conduct deeper analysis on the potential risk of catastrophic forgetting of the original text capabilities of the pre-trained text LLM, due to multi-task learning of speech tasks.
Note that exploration of parameter-efficient fine-tuning may also help preserve the original text capabilities of the pre-trained text LLMs.

***LauraGPT*** relies on discrete audio tokens for speech generative tasks.
Our research shows that the performance of this paradigm strongly depends on the quality of the audio tokenizer.
We plan to systematically analyze the impact of various audio tokenizers on diverse audio generative tasks.
We plan to develop new audio tokenizers that are more suitable for unified Auio-and-Text LLMs and provide desirable representations for generative tasks.

There are great emerging interests in fundamental speech models that are similar to those in the field of NLP.
This is a tremendously valuable research direction.
Our work achieves  important milestone for this research question, as we explore and provide promising answers to the following question:
How to design more efficient and scalable unified GPT-style Audio-and-Text LLMs than existing approaches that can leverage large-scale labeled data and achieve highly competitive performance on a diverse set of speech tasks, including speech recognition, understanding and generation, using a single model?
Note that previous general speech models either focus solely on speech recognition and understanding tasks but neglect speech generative tasks, or support speech generation but suffer from severe performance degradation on speech recognition and understanding tasks.

Inspired by the recent advances of LLMs in NLP, we envision that the fundamental speech models should have the following capabilities:
- In-context learning ability like GPT-3, which can learn from few-shot examples and adapt to new tasks, such as predicting the age of the speaker from a speech sample.
- Instruction-following ability like InstructGPT and ChatGPT, which can perform the appropriate speech-related task given a natural language instruction, such as synthesizing a speech with a specific emotion or style.
- General audio modeling abilities, i.e., speech, non-speech audio, and music, such as music generation.

Our work demonstrates that the current ***LauraGPT*** has made solid progress and reached one important milestone toward a speech foundation model.
From ***LauraGPT*** to the next-generation speech foundation model we envision, most remaining efforts are in more task data collection and more self-supervised and/or supervised pre-training and supervised fine-tuning.
There is no need to modify the model architecture.
