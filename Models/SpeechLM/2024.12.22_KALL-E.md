# KALL-E

<details>
<summary>基本信息</summary>

- 标题: "Autoregressive Speech Synthesis with Next-Distribution Prediction"
- 作者:
  - 01 Xinfa Zhu,
  - 02 Wenjie Tian,
  - 03 Lei Xie
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.16846)
  - [Publication]
  - [Github]
  - [Demo](https://zxf-icpc.github.io/kalle/)
- 文件:
  - [ArXiv](_PDF/2412.16846v1__KALL-E__Autoregressive_Speech_Synthesis_with_Next-Distribution_Prediction.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

We introduce KALL-E, a novel autoregressive (AR) language modeling approach with next-distribution prediction for text-to-speech (TTS) synthesis.
Unlike existing methods, KALL-E directly models and predicts the continuous speech distribution conditioned on text without relying on VAE- or diffusion-based components.
Specifically, we use WaveVAE to extract continuous speech distributions from waveforms instead of using discrete speech tokens.
A single AR language model predicts these continuous speech distributions from text, with a Kullback-Leibler divergence loss as the constraint.
Experimental results show that KALL-E outperforms open-source implementations of YourTTS, VALL-E, NaturalSpeech 2, and CosyVoice in terms of naturalness and speaker similarity in zero-shot TTS scenarios.
Moreover, KALL-E demonstrates exceptional zero-shot capabilities in emotion and accent cloning.
Importantly, KALL-E presents a more straightforward and effective paradigm for using continuous speech representations in TTS.
Audio samples are available at: [this https URL](https://zxf-icpc.github.io/kalle/).

## 1·Introduction: 引言


The past decade has seen remarkable advancements in speech synthesis driven by the development of neural networks~\cite{survey, survey2}.
Early text-to-speech (TTS) systems employ cascaded pipelines, combining acoustic models and vocoders, with Mel spectrograms serving as intermediate representations~\cite{tacotron, fastspeech, durian, fs2, ptaco}.
Later innovations have shifted towards end-to-end TTS modeling, enabling high-quality speech synthesis~\cite{vits, glow-wavegan, clone, vits2}.
However, due to the inherent one-to-many mapping nature of TTS, these systems continue to suffer from over-smoothing issues~\cite{oversmoothing, generspeech}.
Powered by large language models (LLMs)~\cite{valle, speartts, uniaudio, unistyle, seedtts, cosyvoice, dkguo, touchTTS}, diffusion models~\cite{ns2,ns3,flashsspech,simplespeech,e2tts,e3tts, maskgct} and large-scale corpora~\cite{librilight,wenetspeech4tts,Emilia}, current state-of-the-art (SOTA) TTS systems have achieved unprecedented levels of naturalness and diversity, including capabilities for zero-shot voice cloning.

Typical LLM-based TTS frameworks~\cite{valle, uniaudio} rely on speech tokenizers~\cite{encodec, soundstream} to quantize continuous speech waveforms into discrete tokens, which are then modeled autoregressively.
While significant efforts have been made to improve speech tokenizers~\cite{speechtokenizer, singlecodec, xcodec, wavtokenizer}, a fundamental trade-off persists between bitrate and the preservation of speech components~\cite{ns2, melle}.
Some tokenizers~\cite{encodec,soundstream,speechtokenizer} employing multiple discrete tokens per speech frame capture richer acoustic information but significantly increase sequence length, making language modeling challenging.
Conversely, tokenizers~\cite{vectok,cosyvoice} producing low-bitrate sequences simplify language modeling but result in lossy representations lacking acoustic detail.
Unlike text, speech waveforms are continuous in nature, which inherently makes it hard to achieve an ideal speech tokenizer that retains all acoustic nuances at a limited bit rate.

Recent works~\cite{Spectron, melle, contokenizer} have explored continuous speech representations within AR language modeling frameworks to overcome the limitations of speech tokenization.
Continuous representations are considered nearly lossless carriers of speech information.
However, as highlighted in MELLE~\cite{melle}, the key challenges of using continuous speech representations in AR language models lie in the \textit{training objective} and \textit{sampling mechanism}.
MELLE addresses these challenges by introducing a VAE-like latent sampling module into an AR language model to predict Mel spectrograms, while other works~\cite{kaiminghe,ms_diff} leverage diffusion-based heads for continuous representation prediction in visual and multimodal generation tasks.

In this work, we propose KALL-E, a novel AR speech synthesis framework with \textit{next-distribution prediction}.
KALL-E first extracts continuous speech distributions via WaveVAE and predicts them directly through an AR language model, bypassing the need for VAE or diffusion heads and eliminating the inherent dilemma associated with speech tokenizers.
To tackle the challenge of the training objective, we replace the traditional cross-entropy loss with a Kullback-Leibler (KL) divergence loss for next-distribution prediction, supplemented by a binary cross-entropy (BCE) loss for stop prediction.
For the sampling mechanism, we employ a straightforward reparameterization technique to sample from the predicted speech distributions, effectively addressing the challenge of the sampling mechanism.

We evaluate KALL-E on the LibriTTS~\cite{libritts} corpus and compare it with open-source implementations of several popular zero-shot TTS systems, including YourTTS~\cite{yourtts}, VALL-E~\cite{valle}, NaturalSpeech 2~\cite{ns2}, and CosyVoice~\cite{cosyvoice}.
Following established benchmarks, we use the LibriTTS test-clean set for zero-shot TTS evaluation, the ESD~\cite{ESD} corpus for zero-shot emotion cloning, and the VCTK~\cite{vctk} corpus for accent cloning.
Experimental results demonstrate that KALL-E achieves competitive performance with these TTS systems on objective metrics while surpassing them on subjective metrics.
Moreover, KALL-E exhibits exceptional capabilities in zero-shot emotion and accent cloning despite training on a modest 500-hour dataset.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论