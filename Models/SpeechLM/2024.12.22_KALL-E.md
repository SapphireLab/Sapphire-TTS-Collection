# KALL-E

<details>
<summary>基本信息</summary>

- 标题: "Autoregressive Speech Synthesis with Next-Distribution Prediction"
- 作者:
  - 01 Xinfa Zhu,
  - 02 Wenjie Tian,
  - 03 Lei Xie
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.16846)
  - [Publication]
  - [Github]
  - [Demo](https://zxf-icpc.github.io/kalle/)
- 文件:
  - [ArXiv](_PDF/2412.16846v1__KALL-E__Autoregressive_Speech_Synthesis_with_Next-Distribution_Prediction.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

We introduce KALL-E, a novel autoregressive (AR) language modeling approach with next-distribution prediction for text-to-speech (TTS) synthesis.
Unlike existing methods, KALL-E directly models and predicts the continuous speech distribution conditioned on text without relying on VAE- or diffusion-based components.
Specifically, we use WaveVAE to extract continuous speech distributions from waveforms instead of using discrete speech tokens.
A single AR language model predicts these continuous speech distributions from text, with a Kullback-Leibler divergence loss as the constraint.
Experimental results show that KALL-E outperforms open-source implementations of YourTTS, VALL-E, NaturalSpeech 2, and CosyVoice in terms of naturalness and speaker similarity in zero-shot TTS scenarios.
Moreover, KALL-E demonstrates exceptional zero-shot capabilities in emotion and accent cloning.
Importantly, KALL-E presents a more straightforward and effective paradigm for using continuous speech representations in TTS.
Audio samples are available at: [this https URL](https://zxf-icpc.github.io/kalle/).

## 1·Introduction: 引言


The past decade has seen remarkable advancements in speech synthesis driven by the development of neural networks~\cite{survey, survey2}.
Early text-to-speech (TTS) systems employ cascaded pipelines, combining acoustic models and vocoders, with Mel spectrograms serving as intermediate representations~\cite{tacotron, fastspeech, durian, fs2, ptaco}.
Later innovations have shifted towards end-to-end TTS modeling, enabling high-quality speech synthesis~\cite{vits, glow-wavegan, clone, vits2}.
However, due to the inherent one-to-many mapping nature of TTS, these systems continue to suffer from over-smoothing issues~\cite{oversmoothing, generspeech}.
Powered by large language models (LLMs)~\cite{valle, speartts, uniaudio, unistyle, seedtts, cosyvoice, dkguo, touchTTS}, diffusion models~\cite{ns2,ns3,flashsspech,simplespeech,e2tts,e3tts, maskgct} and large-scale corpora~\cite{librilight,wenetspeech4tts,Emilia}, current state-of-the-art (SOTA) TTS systems have achieved unprecedented levels of naturalness and diversity, including capabilities for zero-shot voice cloning.

Typical LLM-based TTS frameworks~\cite{valle, uniaudio} rely on speech tokenizers~\cite{encodec, soundstream} to quantize continuous speech waveforms into discrete tokens, which are then modeled autoregressively.
While significant efforts have been made to improve speech tokenizers~\cite{speechtokenizer, singlecodec, xcodec, wavtokenizer}, a fundamental trade-off persists between bitrate and the preservation of speech components~\cite{ns2, melle}.
Some tokenizers~\cite{encodec,soundstream,speechtokenizer} employing multiple discrete tokens per speech frame capture richer acoustic information but significantly increase sequence length, making language modeling challenging.
Conversely, tokenizers~\cite{vectok,cosyvoice} producing low-bitrate sequences simplify language modeling but result in lossy representations lacking acoustic detail.
Unlike text, speech waveforms are continuous in nature, which inherently makes it hard to achieve an ideal speech tokenizer that retains all acoustic nuances at a limited bit rate.

Recent works~\cite{Spectron, melle, contokenizer} have explored continuous speech representations within AR language modeling frameworks to overcome the limitations of speech tokenization.
Continuous representations are considered nearly lossless carriers of speech information.
However, as highlighted in MELLE~\cite{melle}, the key challenges of using continuous speech representations in AR language models lie in the \textit{training objective} and \textit{sampling mechanism}.
MELLE addresses these challenges by introducing a VAE-like latent sampling module into an AR language model to predict Mel spectrograms, while other works~\cite{kaiminghe,ms_diff} leverage diffusion-based heads for continuous representation prediction in visual and multimodal generation tasks.

In this work, we propose KALL-E, a novel AR speech synthesis framework with \textit{next-distribution prediction}.
KALL-E first extracts continuous speech distributions via WaveVAE and predicts them directly through an AR language model, bypassing the need for VAE or diffusion heads and eliminating the inherent dilemma associated with speech tokenizers.
To tackle the challenge of the training objective, we replace the traditional cross-entropy loss with a Kullback-Leibler (KL) divergence loss for next-distribution prediction, supplemented by a binary cross-entropy (BCE) loss for stop prediction.
For the sampling mechanism, we employ a straightforward reparameterization technique to sample from the predicted speech distributions, effectively addressing the challenge of the sampling mechanism.

We evaluate KALL-E on the LibriTTS~\cite{libritts} corpus and compare it with open-source implementations of several popular zero-shot TTS systems, including YourTTS~\cite{yourtts}, VALL-E~\cite{valle}, NaturalSpeech 2~\cite{ns2}, and CosyVoice~\cite{cosyvoice}.
Following established benchmarks, we use the LibriTTS test-clean set for zero-shot TTS evaluation, the ESD~\cite{ESD} corpus for zero-shot emotion cloning, and the VCTK~\cite{vctk} corpus for accent cloning.
Experimental results demonstrate that KALL-E achieves competitive performance with these TTS systems on objective metrics while surpassing them on subjective metrics.
Moreover, KALL-E exhibits exceptional capabilities in zero-shot emotion and accent cloning despite training on a modest 500-hour dataset.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

### Problem Formulation: Next-Distribution Prediction

KALL-E regards zero-shot TTS as a conditional language modeling task achieved through next-distribution prediction.
As illustrated in Figure~\ref{fig:overview}, KALL-E consists of a WaveVAE model and an AR language model.
The WaveVAE model extracts continuous speech distributions from speech waveforms.
Using well-established continuous distributions as the training targets, KALL-E directly predicts these distributions via the AR language model.
Finally, the predicted speech distributions are converted back into waveforms by the WaveVAE decoder.

Given a <text, speech> pair, the text tokenizer encodes the input text into a sequence of text tokens, $x=\{x_0, x_1, ..., x_L\}$, where $L$ is the number of text tokens.
The WaveVAE encoder extracts continuous speech distributions $z=\{z_0, z_1, ..., z_T\}$ from the corresponding speech waveform, where $T$ denotes the number of frames in continuous speech distributions,  and $z_i$ is sampled from $\mathcal{N}(Z_\mu^i, Z_\sigma^i)$.
KALL-E is trained to predict $z$ from $x$ autoregressively.
Specifically, at each AR step $t$, KALL-E predicts the next speech distribution $z_t$ conditioned on the text prompt $x$ and the previously generated speech distributions $z_{<t}$.
This can be formulated as:

$$
    p(z|x;\theta) =\prod_{t=0}^{T}p(z_{t}|z_{<t},x;\theta),
$$

where $z_{<t}$ represents the previously predicted speech distributions $\{z_0, z_1, ..., z_{t-1}\}$ and $\theta$ represents the parameters of the AR language model.

### WaveVAE

To enable the AR language model to predict continuous speech distributions, we first employ WaveVAE to learn these distributions in an unsupervised manner.
Inspired by the Glow-WaveGAN series~\cite{glow-wavegan,glow-wavegan2,glow-wavegan3}, WaveVAE is a variational auto-encoder (VAE)~\cite{vae} and consists of an encoder and a decoder, where the encoder maps the inputs $w$ into latent representations $z$, and the decoder reconstructs $w$ from $z$.
The process is formulated as follows:

$$
    z = Enc(w) \sim q(z|w),
$$

$$
    \hat{w} = Dec(z) \sim p(w|z),
$$

where $w$ means the input waveform, and $q(z|w)$ represents the latent distribution of speech, which is used to reconstruct waveform $\hat{w}$ from $p(w|z)$ via the decoder.

As for the detailed architecture of the proposed WaveVAE, the encoder consists of a stack of down-sampling dilated convolution layers with residual blocks, which effectively capture abstract features in the speech waveform.
After encoding, the produced mean and variance are interpreted as the parameters of the learned latent distribution $q(z|w) = \mathcal{N}(Z_\mu, Z_\sigma)$.
The decoder mirrors the encoder's architecture but uses transposed convolution layers with residual blocks to up-sample the latent representation $z$ back into the waveform $\hat{w}$.
Additionally, we integrate advanced techniques, such as the Snake activation function from BigVGAN~\cite{bigvgan}, to enhance the WaveVAE decoder's performance.

To train WaveVAE, we use four loss functions: (1) a Kullback-Leibler divergence loss $\mathcal{L}_{KL}$ between latent speech distribution $z$ and prior Gaussian distribution $\mathcal{N}(0, 1)$; (2) a reconstruction loss $\mathcal{L}_{recon}$ between predicted mel-spectrogram and ground-truth mel-spectrogram; (3) a discriminator loss $\mathcal{L}_{disc}$ including the multi-period discriminator loss~\cite{mpd} and multi-resolution discriminator loss~\cite{mrd}; (4) a feature matching loss $\mathcal{L}_{fm}$ between the feature map extracted from intermediate layers in each discriminator~\cite{featuremap}.
These losses work collaboratively to optimize WaveVAE, and the overall loss raining objective is defined as:

$$
    \mathcal{L}_{waave} = \lambda_{kl} \mathcal{L}_{KL} + \lambda_{recon} \mathcal{L}_{recon} + \lambda_{disc} \mathcal{L}_{disc} + \lambda_{fm} \mathcal{L}_{fm},
$$

where $\lambda_{kl}$, $\lambda_{recon}$, $\lambda_{disc}$, $\lambda_{fm}$ are the respective weighting factors for each loss component.

### Autoregressive Language Model

With the assistance of WaveVAE, KALL-E employs a causal transformer decoder as the language model to autoregressively predict the continuous speech distribution.
Specifically, input text tokens $x$, augmented with an <EOS> token, are first converted into embeddings by the text embedding layer.
Simultaneously, a linear layer projects the sampled speech distribution $z$ into the dimension of the language model.
The language model, consisting of blocks of multi-head self-attention and feed-forward layers, takes the concatenation of text and speech embeddings as input to model the dependency between semantic and acoustic information.

At each time step $t$, the output of the language model, $o_t$, is subsequently processed by a linear layer to predict the mean $\hat{Z}_{\mu}$ and variance $\hat{Z}_{\sigma}$ of target speech distribution.
These parameters are then used to sample the predicted speech distribution for the subsequent AR step.

The training objective for the AR language model consists of two components: (1) a Kullback-Leibler divergence loss $\mathcal{L}_{KL}$ between predicted and ground-truth speech distributions; (2) a binary cross-entropy loss $\mathcal{L}_{stop}$ for stop prediction.
Following the design of Tacotron~\cite{tacotron} and other AR TTS models~\cite{transformertts,speecht5}, we employ a linear layer to project the output of the LM to logits, which are then used to calculate the BCE loss for stop prediction.
The overall training objective for the AR language model is formulated as follows:

$$
    \mathcal{L}_{LM} = \mathcal{L}_{KL} + \lambda_{stop} \mathcal{L}_{stop},
$$

where $\lambda_{stop}$ is a hyperparameter that balances the stop prediction loss with the KL divergence loss.

### Inference

During inference, KALL-E is capable of both unconditional and conditional text-to-speech synthesis.
For unconditional TTS, KALL-E autoregressively generates the target speech distribution directly from the provided input text.
Thanks to the reparameterization technique used in the sampling process, KALL-E can produce diverse speech outputs with varying speaker timbres and speaking styles for the same input text when performing batch inference.

For conditional TTS, such as in the case of zero-shot TTS, KALL-E achieves this through an in-context learning mechanism, similar to VALL-E and MELLE.
Specifically, given the text content $x$ for synthesis, along with the text transcription $\hat{x}$ and speech distribution $\hat{z}$ of acoustic prompt, KALL-E autoregressively generates the target speech distribution $z$ of the corresponding content while preserving the acoustic characteristics of the prompt speech.
This process is formulated by maximizing the likelihood probability: $p(z|\hat{x},x,\hat{z};\theta)$.

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论