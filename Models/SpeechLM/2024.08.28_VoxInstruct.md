# VoxInstruct

<details>
<summary>基本信息</summary>

- 标题: "VoxInstruct: Expressive Human Instruction-to-Speech Generation with Unified Multilingual Codec Language Modelling"
- 作者:
  - 01 Yixuan Zhou (周逸轩)
  - 02 Xiaoyu Qin,
  - 03 Zeyu Jin,
  - 04 Shuoyi Zhou,
  - 05 Shun Lei,
  - 06 Songtao Zhou
  - 07 Zhiyong Wu (吴志勇)
  - 08 Jia Jia (贾珈)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2408.15676)
  - [Publication](https://openreview.net/forum?id=hQp6qimhbb) ACM Multimedia 2024
  - [Github](https://github.com/thuhcsi/VoxInstruct)
  - [Demo](https://voxinstruct.github.io/VoxInstruct/)
- 文件:
  - [ArXiv](_PDF/2408.15676v1__VoxInstruct__Expressive_Human_Instruction-to-Speech_Generation_with_Unified_Multilingual_Codec_Language_Modelling.pdf)
  - [Publication](_PDF/2408.15676p0__VoxInstruct__ACMMultiMedia2024.pdf)

</details>

## Abstract: 摘要

Recent AIGC systems possess the capability to generate digital multimedia content based on human language instructions, such as text, image and video.
However, when it comes to speech, existing methods related to human instruction-to-speech generation exhibit two limitations.
Firstly, they require the division of inputs into content prompt (transcript) and description prompt (style and speaker), instead of directly supporting human instruction.
This division is less natural in form and does not align with other AIGC models.
Secondly, the practice of utilizing an independent description prompt to model speech style, without considering the transcript content, restricts the ability to control speech at a fine-grained level.
To address these limitations, we propose ***VoxInstruct***, a novel unified multilingual codec language modeling framework that extends traditional text-to-speech tasks into a general human instruction-to-speech task.
Our approach enhances the expressiveness of human instruction-guided speech generation and aligns the speech generation paradigm with other modalities.
To enable the model to automatically extract the content of synthesized speech from raw text instructions, we introduce speech semantic tokens as an intermediate representation for instruction-to-content guidance.
We also incorporate multiple Classifier-Free Guidance (CFG) strategies into our codec language model, which strengthens the generated speech following human instructions.
Furthermore, our model architecture and training strategies allow for the simultaneous support of combining speech prompt and descriptive human instruction for expressive speech synthesis, which is a first-of-its-kind attempt.
Codes, models and demos are at: [this https URL](https://github.com/thuhcsi/VoxInstruct).

## 1.Introduction: 引言

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
