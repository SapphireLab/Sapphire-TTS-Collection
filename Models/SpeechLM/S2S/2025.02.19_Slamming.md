# Slamming

<details>
<summary>基本信息</summary>

- 标题: "Slamming: Training a Speech Language Model on One GPU in a Day"
- 作者:
  - 01 Gallil Maimon,
  - 02 Avishai Elmakies,
  - 03 Yossi Adi
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.15814)
  - [Publication]()
  - [Github](https://github.com/slp-rl/slamkit)
  - [Demo](https://pages.cs.huji.ac.il/adiyoss-lab/slamming/)
- 文件:
  - [ArXiv](../_PDF/2502.15814v1__Slamming__Training_a_Speech_Language_Model_on_One_GPU_in_a_Day.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

We introduce ***Slam***, a recipe for training high-quality Speech Language Models (SLMs) on a single academic GPU in 24 hours.
We do so through empirical analysis of model initialization and architecture, synthetic training data, preference optimization with synthetic data and tweaking all other components.
We empirically demonstrate that this training recipe also scales well with more compute getting results on par with leading SLMs in a fraction of the compute cost.
We hope these insights will make SLM training and research more accessible.
In the context of SLM scaling laws, our results far outperform predicted compute optimal performance, giving an optimistic view to SLM feasibility.
See code, data, models, samples - https://pages.cs.huji.ac.il/adiyoss-lab/slamming.

</td><td>

我们介绍了 ***Slam***, 这是一种在单块学术级 GPU 上 24 小时内训练高质量语音语言模型的方案.
我们通过对模型初始化和架构, 合成训练数据的使用, 基于合成数据的偏好优化以及对所有其他组件的微调的实证分析实现了这一目标.
我们通过实验证明, 这种训练方案在计算资源增加时具有良好的扩展性, 能够以更低的计算成本获得与领先 SLMs 相当的结果.
我们希望这些见解能够使 SLM 的训练和研究更加普及.
在 SLM 扩展规律的背景下, 我们的结果远超计算最优性能的预测, 为 SLM 的可行性提供了乐观的前景.
代码, 数据, 模型和示例, 请访问：https://pages.cs.huji.ac.il/adiyoss-lab/slamming.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

**Speech Language Models (SLMs)** have gained significant interest from researchers (**Survey by Peng et al (2024)**[^01]; **Survey by Cui et al (2024)**[^02]; **WavChat**[^03]; **Survey by Latif et al (2023)**[^04]), demonstrating remarkable performance in traditional speech tasks (**VALL-E**[^05]; [^06]), diverse generative applications (**UniAudio**[^07]; **UniAudio 1.5**[^08]), and reasoning over speech and audio signals (**SALMONN**[^09]; **Qwen-Audio**[^10]).

</td><td>

语音语言模型获得了研究人员的广泛关注 (**Survey by Peng et al (2024)**[^01]; **Survey by Cui et al (2024)**[^02]; **WavChat**[^03]; **Survey by Latif et al (2023)**[^04]), 展示出了在传统语音任务中的卓越性能 (**VALL-E**[^05]; **无监督语音分割**[^06]), 广泛的生成应用 (**UniAudio**[^07]; **UniAudio 1.5**[^08]), 以及对语音和音频信号进行推理 (**SALMONN**[^09]; **Qwen-Audio**[^10]).

</td></tr>
<tr><td>

SLMs can generally be classified into two main categories: (i) generative speech LMs (which can also incorporate text) and (ii) speech-aware LMs.
The first category follows a similar pre-training approach to text-based LLMs, directly maximizing the likelihood of speech considering both input and output, typically by representing audio as a sequence of discrete tokens.
The second category consists of pre-trained text LMs adapted to process speech inputs.
In this work, we focus on the first.

</td><td>

语音语言模型通常可以分为两个主要类别: (i) 生成式语音语言模型 (可以包含文本) 和 (ii) 语音感知语言模型.
- 第一类遵循类似于基于文本的大语言模型的预训练方法, 直接最大化语音输入和输出的似然, 通常通过将音频表示为离散 Token 序列.
- 第二类由预训练文本语言模型进行处理语音输入的适配.

本项工作着重于第一类.

</td></tr>
<tr><td>

Training high-quality SLMs can be highly resource intensive (**TWIST**[^11]; [^12]; [^13]; **SpiRit-LM**[^14]; **Moshi**[^15]).
For example, **SpiRit-LM**[^14] trained their SLM on approximately $570k$ hours of speech data, while **Moshi**[^15] utilized around $7M$ hours.
Additionally, [^12] proposed SLM scaling laws, suggesting that training high-quality SLMs requires $\sim3X$ more data compared to text-based counterparts.
These computational demands restrict the required fundamental research aimed at enhancing SLMs, such as advancements in speech tokenization, efficient acoustic modelling, etc.

</td><td>

训练高质量语音语言模型可以是高度资源密集型的 (**TWIST**[^11]; **SpiRit-LM**[^14]; **Moshi**[^15]; **SpiRit-LM**[^14]; **Moshi**[^15]).
- **SpiRit-LM**[^14] 在约 570K 小时的语音数据上训练 SLM,
- **Moshi**[^15] 使用约 7M 小时的语音数据.
- [^12] 提出了语音语言模型的**尺度定律 (Scaling Laws)**, 提出训练高质量的语音语言模型相比文本语言模型需要约 3 倍的数据.

这些计算需求限制了增强 SLM 的基础研究, 例如语音 Tokenization 的进步, 高效的声学建模, 等等.

</td></tr>
<tr><td>

In the Natural Language Processing (NLP) community, numerous studies have investigated efficient model training techniques, including masked language models such as **Cramming**[^16] and **ModernBERT**[^17], along with next-token prediction LLMs such as **MobileLLM**[^18].
These methods include implementation efficiencies, architectural improvements, data selection strategies, and enhancements to the overall training pipeline.

</td><td>

在自然语音处理社区中, 许多研究探讨了高效的模型训练技术, 包括掩码语言模型 (**Cramming**[^16] 和 **ModernBERT**[^17]) 以及下一 Token 预测语言模型 (**MobileLLM**[^18]).
这些方法包含了实现效率, 架构改进, 数据选择策略, 以及对整体训练流程的增强.

</td></tr>
<tr><td>

Inspired by **Cramming**[^16] in text, we investigate compute-limited SLM training, which we term ***Slamming***.
We pose the question: **Is it possible to train high-quality SLMs using a single GPU within 24 hours?**
For that, we conduct an extensive empirical analysis exploring how different training components influence performance.
From this, we derive a training recipe that maximizes model performance within a fixed compute budget.
Specifically, we investigate the impact of model initialization and architecture, various optimizers and learning rate schedulers, data selection strategies - including the role of synthetic data, text-interleaving and preference optimization.

</td><td>

受到 **Cramming**[^16] 在文本领域中的启发, 我们研究了计算受限的 SLM 训练, 我们将其称为 ***Slamming***.
我们提出了一个问题: **能否在 24 小时内使用单块 GPU 训练出高质量的语音语言模型?**

为了回答这一问题, 我们构造了广泛的经验分析探索不同训练组件对性能的影响.
从这些分析中我们得到了一个在有限计算预算下最大化模型性能的训练配方

具体来说, 我们研究了模型初始化和架构, 各种优化器和学习率调度器, 数据选择策略 - 包括合成数据, 文本交错和偏好优化等影响.

</td></tr>
<tr><td>

We believe that developing these training strategies and proving their feasibility will empower the speech and audio research community to advance SLMs beyond the scope of large, well-funded academic and industrial labs.
[Figure.01](#Fig.01) illustrates the performance of various SLMs relative to their training compute budget, with circle sizes representing the size of the models.
Furthermore, we compare our results with the scaling performance predicted from [^12].
Although the authors present a somewhat pessimistic view of the computational resources needed to train high-quality SLMs, we empirically show that reality is more promising, demonstrating that it is possible to significantly exceed the predicted performance per unit of compute.
We encourage the community to refine and expand scaling laws specifically tailored for SLM training across various settings.

</td><td>

我们相信开发这些训练策略并证明它们的可行性将赋予语音和音频研究社区以超越大型, 经费充足的学术和工业实验室来发展语音语言模型的能力.
[图 1](#Fig.01) 展示了不同 SLM 相对于其训练计算预算的性能, 圆的大小代表模型的大小.
此外, 我们将获得的结果和 [^12] 预测的扩展性能进行了比较.
尽管作者展示了训练高质量 SLM 需要的计算资源的某种悲观预期, 我们通过实验表明现实情况更令人振奋, 证明了它是可能超过计算资源单位的预期性能的.
我们鼓励社区改进和扩展 SLM 训练在各种设置中的定制尺度定律.

</td></tr>
<tr><td>

Our main contributions are:
- We introduce ***Slam***, a training recipe for efficiently training high-quality SLMs using a single A5000 GPU within 24 hours.
- We carry out extensive experiments exploring model initialization and architecture, optimization, data collection and generation, and training objectives (i.e., preference optimization and text-speech interleaving), providing insights into the impact of each component on model performance.
- Building on these insights, we scale the compute budget to two A100 GPUs for 48 hours and demonstrate that our model achieves performance on par with state-of-the-art models that require substantially more compute.

We open-source all code, models, training recipes, and synthetic datasets.

</td><td>

我们的主要贡献如下:
- 我们提出了 ***Slam***, 一种训练方案, 用于使用单块 A5000 GPU 24 小时内高效训练高质量的语音语言模型.
- 我们进行了广泛的实验, 探索了模型初始化和架构, 优化, 数据收集和生成, 以及训练目标 (即偏好优化和文本-语音交错), 提供了每个组件对模型性能的影响的见解.
- 基于这些见解, 我们将计算预算扩展到两块 A100 GPU 48 小时, 并展示了我们模型具有与需要大量计算的 SoTA 模型相当的性能.

我们开放所有代码, 模型, 训练配方, 和合成数据集.

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

**Efficient Training**

Enhancing the efficiency of neural network training has been extensively studied (**Survey by Shen et al (2023)**[^19]).
- **SwiftLearn**[^20], **GREATS**[^21] examined the impact of data selection on LLM training and introduced efficient data selection methods.
- **GRASS**[^22] proposed using structured sparse gradients to enhance compute efficiency in LLM training, while **SALT**[^23] explored the potential of leveraging smaller language models to improve the training efficiency of larger LLMs.
- **LPA**[^24] investigated the use of low-dimensional projections for attention parameters to enhance training efficiency.
Meanwhile, **LDB**[^25] proposed applying LayerDrop as a technique to optimize neural network training.

More closely related to our work, **FLM-101B**[^26] propose a training strategy for developing LLMs within a $100k\$$ budget.
**ModernBERT**[^17], an efficient training pipeline for optimising BERT models, while [^27] outline a method for training a BERT model in 24 hours using 8 GPUs.
The most relevant work to ours is **Cramming**[^16], where the authors conduct an in-depth analysis of masked LM training on a single GPU in one day.

While these studies offer valuable insights, they primarily focus on training text models, such as LLMs and masked LMs.
In the speech domain, similar research has been conducted on self-supervised representation models[^28], but not on SLMs.
In this work, we address this gap by focusing on efficient SLM training.

</td><td>

**高效训练**

增强神经网络训练的效率已经得到了广泛研究 (**Survey by Shen et al (2023)**[^19]).
- **SwiftLearn**[^20], **GREATS**[^21] 研究了数据选择对 LLM 训练的影响, 并提出了高效的数据选择方法.
- **GRASS**[^22] 提出了使用结构稀疏梯度来提高 LLM 训练的计算效率, 而 **SALT**[^23] 探索了利用较小的语言模型来提高较大 LLM 的训练效率.
- **LPA**[^24] 研究了为注意力参数使用低维投影来增强训练效率.
与此同时, **LDB**[^25] 提出了将 LayerDrop 作为一种技术来优化神经网络训练.

和我们的工作更相关的研究是 **FLM-101B**[^26], 提出了一种训练策略, 用于在 $100k\$$ 预算内开发 LLM.
- **ModernBERT**[^17], 一种高效的 BERT 训练流程, 虽然 [^27] 提出了一种方法, 用于在 24 小时内使用 8 块 GPU 训练 BERT 模型.
- 和我们工作最相关的是 **Cramming**[^16], 作者对掩码语言模型训练进行了深入分析, 并在一天内使用单块 GPU 训练掩码语言模型.

虽然这些研究提供了宝贵的见解, 但它们主要关注于训练文本模型, 如 LLM 和掩码语言模型.
在语音领域, 与自监督表示模型相关的研究已经有了 [^28], 但没有 SLM.
在本项工作中, 我们着重于 SLM 训练的高效训练.

</td></tr>
<tr><td>

**Generative speech language models** were explored under various setups (**GSLM**[^29], **pGSLM**[^30]).
**GSLM**[^29] were the first to show how raw and uncurated speech data can be leveraged into building a GSLM system.
Next, **AudioLM**[^31] proposed a cascade version using both coarse and fine speech tokens.
Such a modelling framework opened up a new and promising research direction for processing and modelling spoken data, such as speech resynthesis[^32], speaking style conversion([^33], **DISSC**[^34]), dialogue modelling (**dGSLM**[^35]), speech-to-speech translation ([^36], **MSLM-S2ST**[^37]), etc.

**Spectron**[^38] proposed augmenting a text LM with continuous speech data to improve spoken question-answering tasks.
Recently, **SpeechSSM**[^39] proposed SLM based on state-space models **S4**[^40] to further push long context-efficient modelling, while **Align-SLM**[^41] proposed to fine-tune SLMs using **Direct Preference Optimization**[^42] obtained from text LLM rankings.

Similar to text LLMs, training SLMs often demands large-scale datasets.
For instance, **Moshi**[^15] was trained on 7 million hours of speech data, **SpiRit-LM**[^14] utilized 560K hours, and **TWIST**[^11] was trained on approximately 150K.
Recently, [^12] introduced the first scaling laws for SLMs, suggesting that achieving comparable performance to text LMs requires three times more tokens.
In this work, we focus on reducing the computational demands while maintaining performance comparable to leading SLMs.

</td><td>

**生成式语音语言模型** 已经探索了不同的设置 (**GSLM**[^29], **pGSLM**[^30]).
- **GSLM**[^29] 首次表明了如何利用原始和未经整理的语音数据来构建 GSLM 系统.
- **AudioLM**[^31] 提出了一种级联版本, 使用粗粒度和细粒度语音 Token.
这种建模框架为处理和建模口语数据打开了一个新的和有前途的研究方向, 如语音重合成[^32], 说话风格转换[^33], 对话建模 (**dGSLM**[^35]), 语音到语音翻译 ([^36], **MSLM-S2ST**[^37]), 等等.
- **Spectron**[^38] 提出了用连续语音数据增强文本 LM 来改善口语问答任务.
- 最近, **SpeechSSM**[^39] 提出了基于状态空间模型 **S4**[^40] 的 SLM, 进一步推动了长上下文有效建模,
- **Align-SLM**[^41] 提出了利用从文本 LLM 排序获得的**直接偏好优化**[^42] 来微调 SLM.

类似于文本大语言模型, 训练语音语言模型通常需要大规模数据集.
- **Moshi**[^15] 训练了 7 百万小时的语音数据, **SpiRit-LM**[^14] 使用了 560K 小时, 而 **TWIST**[^11] 训练了约 150K.
- 最近, [^12] 提出了第一个 SLM 尺度定律, 表明获得与文本 LM 相当的性能需要三倍的 Token.

在本工作中, 我们聚焦于减少计算需求同时保持与领先 SLM 相当的性能.

</td></tr></table>

## 3·Setup: 设置

<table><tr><td width="50%">

In this study, we explore decoder-only generative SLMs, which aim at maximizing the likelihood of speech samples represented as discrete tokens.
We examine both purely speech-based SLMs trained on speech tokens and joint speech-text SLMs using interleaving strategies (**SpiRit-LM**[^14]).
Similarly to **TWIST**[^11], **GSLM**[^29], we obtain speech tokens by quantizing continuous latent representations of a self-supervised speech representation model using the K-Means algorithm, often known as semantic tokens.
Specifically, we utilize a multilingual **HuBERT**[^43] model running at 25 Hz, as employed in **TWIST**[^11].
We then train SLMs by minimizing the negative log-likelihood of the input segments.

Unless mentioned otherwise, all SLMs are trained using a single A5000 GPU (24GB VRAM) along with 16 CPU cores for 24 hours.
We deliberately focus on this constrained compute budget, assuming that most academic labs can access similar resources, thereby ensuring the accessibility of our research.
The training data is pre-processed, i.e.
extracting **HuBERT** units and dividing data into chunks, and stored prior to model training.
As a result, this pre-processing time is excluded from the compute budget.
This approach, aligned with **Cramming**[^16], is practical since many research experiments utilize the same pre-processed data.
We additionally do not count the time for running validation and visualizations as they are not used as part of the optimization pipeline and only used for demonstration purposes.

</td><td>

在本研究中, 我们探索了仅解码器架构的生成式语音语言模型, 其目标是最大化表示为离散 Token 的语音样本的似然.
我们调查了纯在语音 Token 上训练的基于语音的 SLM 和使用交错策略的联合语音-文本 SLM (**SpiRit-LM**[^14]).
类似于 **TWIST**[^11], **GSLM**[^29], 我们使用 K-Means 算法对自监督语音表示模型的连续潜在表示进行量化, 通常称为语义 Token.

具体来说, 我们使用多语言 **HuBERT**[^43] 模型, 其采样率为 25 Hz, 如 **TWIST**[^11] 所采用的.
然后我们通过最小化输入片段的负对数似然来训练 SLM.

除非另有说明, 所有的 SLM 都使用单块 A5000 GPU (24GB VRAM) 配备 16 核 CPU, 训练 24 小时.
我们故意着重于这一受限计算预算, 假设大多数学术实验室都可以获得类似的资源, 从而确保我们的研究的可及性.
训练数据是预先处理的, 即提取 **HuBERT**单元, 并将数据分块并在模型训练之前存储.
这一方法和 **Cramming**[^16] 一致, 因为许多研究实验都使用相同的预处理数据.
我们另外不计入运行验证和可视化的时间, 因为它们不作为优化流程的一部分, 只用于演示目的.

</td></tr>
<tr><td>

**Evaluation Metrics**

We assess all SLMs using four distinct evaluation metrics.
The first three are based on likelihood evaluation, while the fourth is a generative metric.
For likelihood based modelling we consider **sBLIMP**[^44], **Spoken StoryCloze (SSC)**, and **Topic StoryCloze (TSC)** (**TWIST**[^11]).
For modelling-likelihood metrics, we evaluate the likelihood assigned by the SLMs to pairs of speech utterances, consisting of a positive example and a distractor.
We calculate the percent of pairs in which the SLM assigns higher likelihood to the positive sample.
**sBLIMP** focuses on grammatical abilities thus the negative is ungrammatical version of the positive.
SSC and TSC focus on semantic modelling abilities.
In SSC, the distractor suffix is taken from the original textual **StoryCloze dataset**[^45], allowing to assess fine-grained semantic speech understanding.
In TSC, however, the distractor suffix is drawn from a different topic, enabling us to evaluate the model’s ability to understand the overall semantic concept.

Finally, to assess the generative abilities of SLMs, we compute **generative perplexity (GenPPL)**.
Following the approach of **GSLM**[^29], **TWIST**[^11], we provide the SLM with a short speech prompt and generate speech tokens continuation.
We use unit-vocoder with duration prediction to convert the tokens into speech ([^32], **TWIST**[^11]).
The generated speech is then transcribed, and its PPL is evaluated using a pre-trained text LLM.
To minimize the impact of token repetition on PPL measurements, we ground the generated text using diversity metrics derived from the auto-BLEU score(**GSLM**[^29]).
Similarly to **Align-SLM**[^41] we use bigram auto-BLEU.
In other words, we ensure that all models achieve similar auto-BLEU scores, allowing for a fair comparison of PPL.
Specifically, we transcribe speech segments using **Whisper-Large-V3-Turbo** model[^46] and measure PPL using **LLaMA-3.2-1B model**[^47].

</td><td>

**评估指标**

我们使用四个不同的评估指标来评估所有 SLM.
前三个基于似然评估, 而第四个是生成性指标.
- 对于基于似然的建模, 我们考虑 **sBLIMP**[^44], **Spoken StoryCloze (SSC)**, 和 **Topic StoryCloze (TSC)** (**TWIST**[^11]).
对于建模似然的度量, 我们评估 SLM 为成对的语音发言分配的似然, 其中包含正样本和错误样本.
我们计算 SLM 给正样本分配高似然的成对数据所占的百分比.

- **sBLIMP** 关注语法能力, 因此负样本是正样本的不合语法版本.

**SSC** 和 **TSC** 关注语义建模能力.
- 在 **SSC** 中, 错误样本的后缀来自原始文本 **StoryCloze 数据集**[^45], 允许评估精细语音理解能力.
- 在 **TSC** 中, 错误样本的后缀来自不同主题, 允许评估模型对整体语义概念的理解能力.

最后, 为了评估 SLM 的生成能力, 我们计算 **生成困惑度 (GenPPL)**.
遵循 **GSLM**[^29] 和 **TWIST**[^11] 的方法, 我们提供了一个短语音提示, 由 SLM 进行语音 Token 续写生成.
我们使用带有时长预测的单元声码器来将 Token 转换为语音 ([^32], **TWIST**[^11]).
生成的语音随后被转写, 其困惑度由预训练文本大语言模型进行评估.
为了最小化困惑都度量中 Token 重复的影响, 我们使用由自动 BLEU 得分导出的多样性度量来提供生成的文本的基线 (**GSLM**[^29]).
类似于 **Align-SLM**[^41], 我们使用二元组 auto-BLEU.
换句话说, 我们确保所有模型都获得了相似的 auto-BLEU 得分, 使得 PPL 评估公平.

具体来说, 我们使用 **Whisper-Large-V3-Turbo** 模型[^46] 将语音片段转写, 并使用 **LLaMA-3.2-1B 模型**[^47] 评估 PPL.

</td></tr>
<tr><td>

**Software Efficiency**

To maximize performance within 24 hours of model training, we leverage multiple efficient implementations.
Through extensive performance testing, we found that using bfloat16[^48] alongside **FlashAttention2**[^49] and data packing provided the most efficient compute performance in our setup.
We also experimented with model compilation using **torch.compile**[^50], but it lacked native compatibility with **FlashAttention2** at the time of our study, and its performance without **FlashAttention2** was subpar.
Future work could investigate this further with more efficient attention implementations (**FlashAttention3**[^51], **FlexAttention**[^52]).

To enable rapid and scalable experimentation, we developed a specialized library for SLM training that supports various model architectures, training objectives, and evaluation metrics.
This library accommodates both **TWIST**-style training, text-speech interleaving, preference optimization, etc.
We will open-source this package along all models weights and training recipes, aiming to empower the community to further explore SLMs.

</td><td>

**软件效率**

为了最大化在 24 小时内模型训练的性能, 我们采用了多个高效实现.
通过广泛的性能测试, 我们发现使用 BFloat16[^48] 与 **FlashAttention2**[^49] 以及数据打包技术可以提供最高效的计算性能.

我们还使用 **torch.compile**[^50] 尝试模型编译, 但该技术在我们研究时并不兼容 **FlashAttention2**, 因此其性能不及不使用 **FlashAttention2**.
未来工作可以研究更高效的注意力实现 (**FlashAttention3**[^51], **FlexAttention**[^52]).

为了实现快速和可扩展的实验, 我们开发了一个专门用于 SLM 训练的库, 该库支持各种模型架构, 训练目标和评估指标.
该库可以容纳 **TWIST** 式训练, 文本-语音交错, 偏好优化等.
我们将开源该库, 并提供所有模型权重和训练配方, 旨在鼓励社区进一步探索 SLM.

</td></tr></table>

## 4·Investigations: 研究

<table><tr><td width="50%">

With this setup, we systematically analyse and ablate each component of the training pipeline, ultimately refining an optimised cook-book for training SLMs.
We specifically examine the influence of model family, initialisation, size, and architectural choices (e.g., dropout, positional embedding, etc.).
We analyse optimization parameters and data characteristics.
Lastly, we explore alternative training objectives beyond standard next-token prediction, including speech-text interleaving and direct preference optimization using synthetic data.

</td><td>

在上述设置下, 我们系统性地分析和消除训练流程中的每个组件, 最终精炼出训练 SLM 的优化方案.
我们具体检查了模型系列, 初始化, 尺寸, 架构选择 (如随机失活, 位置编码等) 的影响.
我们分析了优化参数和数据特征.
最后, 我们探索了除了标准的 Next-Token 预测之外的可选训练目标, 包括使用合成数据的括语音-文本交错和直接偏好优化.

</td></tr></table>

### Model & Optimization: 模型与优化

<table><tr><td width="50%">

**Hyper-Parameters**

Unless specified otherwise, we use a context length of 512 tokens and an effective batch size of 256, employing gradient accumulation when necessary, as preliminary results indicated this configuration yields the best overall performance.
We set the peak learning rate to 1e-3 to enhance training speed and use a warmup period of 1% of the total training steps, as this proved more effective than the fixed 100-step warmup used in the original **TWIST**.
To improve training stability, particularly with large learning rates, we apply gradient normalisation with a norm of 0.5 at no additional cost, following **Cramming**[^16].
Unless modified later in our investigation, we use an inverse-square root scheduler and the **AdamW optimiser**[^53].

</td><td>

**超参数**

除非另有说明, 我们使用上下文长度为 512 个 Token, 有效批量大小为 256, 在必要时采用梯度累积, 初步结果表明该配置可以获得最佳的整体性能.
我们设置学习率峰值为 1e-3 以增强训练速度并使用总训练步数的 1% 作为预热期, 这比原始 **TWIST** 中使用的固定 100 步预热更有效.
为了提升训练稳定性, 尤其是大学习率, 我们采用梯度标准化, 范数为 0.5 且无额外成本, 遵循 **Cramming**[^16].
除非在我们的研究中作出修改, 我们使用倒数平方调度器和 **AdamW 优化器**[^53].

</td></tr>
<tr><td>

**Initialisation**

**TWIST**[^11] empirically demonstrated that initialising SLMs with pre-trained text LMs can enhance convergence speed and improve model performance.
We examine the effect of this initialisation within our setup across different model types.
To do so, we train multiple models, both with and without **TWIST** initialisation, while staying within our compute budget.
As shown in [Figure.02](#Fig.02), **TWIST** initialisation benefits all evaluated models at the beginning of training, though its overall impact by the end varies.
Notice, the x-axis in [Figure.02](#Fig.02) represents theoretical FLOPs, calculated as $6 * N_{params} * D_{tokens}$ following[^54].
However, due to variations in model architecture and implementation, practical efficiency differs, leading to varying amounts of compute processed within 24 hours.

Results suggest that benefits of **TWIST** initialisation can be substantial, especially for top-performing models like **Qwen2.5**.
As a result, we prioritise investigations based on existing pre-trained text LMs.
Interestingly, the results in [Figure.02](#Fig.02) demonstrate that **Qwen2.5** outperforms other models even without **TWIST** initialisation, perhaps suggesting that their architectural design choices or size might also provide some benefit.

</td><td>

**初始化**

**TWIST**[^11] 经验表明, 使用预训练文本语言模型初始化 SLM 可以提升收敛速度并提升模型性能.
我们在我们的设置中研究了不同模型类型的初始化效果.
为了做到这一点, 我们训练了多个模型, 采用或不采用 **TWIST** 初始化, 同时保持了计算预算.
如[图 02](#Fig.02) 所示, **TWIST** 初始化在训练初期对所有评估模型都有益, 但其最终影响因模型架构和实现而异.
注意, [图 02](#Fig.02) 中的 x 轴表示理论 FLOPs, 计算方法为 $6 * N_{params} * D_{tokens}$ 遵循[^54].
然而, 由于模型架构和实现的差异, 实际效率可能有所不同, 导致 24 小时内处理的计算量会有所不同.

结果表明, **TWIST** 初始化的好处可能是巨大的, 特别是 **Qwen2.5** 这样的顶级模型.
因此, 我们优先考虑基于现有预训练文本语言模型的研究.
有趣的是, [图 02](#Fig.02) 展示了 **Qwen2.5** 即使不采用 **TWIST** 初始化, 也能胜过其他模型, 这可能意味着它们的架构设计或大小也能提供一些好处.

</td></tr>
<tr><td colspan="2">

Figure 2: Comparing PPL of different models of similar parameter count, with and without TWIST initialisation.

</td></tr>
<tr><td>

**Optimal Model Size & Family**

[^12] conducted a scaling analysis on GSLM-style SLMs, estimating the optimal model size and token count for a compute-efficient model.
However, using a text LM initialisation might impact these findings.
As we observe, **TWIST** initialisation greatly impact model performance, suggesting that prioritising larger models may be more effective than simply increasing the dataset size.
Additionally, various model families gain different advantages from **TWIST** initialisation; for example, **Qwen2.5** models show significantly better performance compared to **OPT** models.
In [Figure.03](#Fig.03), we compare the results under the pre-defined compute budget within model families.

We use the text LM original names for clarity, but note that the actual size will be notably smaller due to reduced vocabulary size, e.g **Qwen2.5-0.5B** has 358M parameters.
Full model sizes can be found in Appendix B.

We note that the best model sizes for both **MobileLLM**[^18], **SmolLM2**[^55] and **Pythia**[^56] are $\sim300M$ parameters, while for **OPT** the best is 125M.
According to [^12], the estimated optimal model size is approximately 66M parameters.
However, the best-performing model, **Qwen2.5**, is significantly larger.
Since there are no smaller models in this family, it is difficult to determine whether this deviation is due to the quality of the initialisation or other factors.
Moving forward, we proceed with both **OPT-125M** and **Qwen2.5-0.5B**.

</td><td>

**最优模型大小与系列**

[^12] 对 GSLM 式 SLM 进行了规模分析, 估计了计算效率模型的最优模型大小和 Token 数量.
然而, 使用文本语言模型初始化可能会影响这些发现.
正如我们所观察到的, **TWIST** 初始化大大影响模型性能, 这表明, 优先考虑更大的模型可能比简单增加数据集大小更有效.
此外, 各种模型系列从 **TWIST** 初始化中获得了不同的优势;
例如, **Qwen2.5** 模型相较于 **OPT** 模型, 显示出显著的性能优势.
在[图 03](#Fig.03) 中, 我们比较了在模型系列内预定义的计算预算下的结果.

我们为清晰起见, 使用原始文本语言模型名称, 但请注意, 实际大小会由于减少词汇量而显著减小, 例如 **Qwen2.5-0.5B** 有 358M 参数.
完整模型大小可以在附录 B 中找到.

我们注意到对于 **MobileLLM**[^18], **SmolLM2**[^55] 和 **Pythia**[^56], 最优模型大小为 $\sim300M$, 而对于 **OPT** 系列, 最优模型大小为 125M.
根据 [^12], 估计的最优模型大小约为 66M 参数.
然而, 最佳模型, **Qwen2.5**, 明显大得多.
因为该系列没有更小的模型, 很难确定这一差异是否是由于初始化质量或其他因素导致的.
因此, 我们继续采用 **OPT-125M** 和 **Qwen2.5-0.5B**.

</td></tr>
<tr><td>

**Dropout**

The original **OPT** models includes dropout to mitigate overfitting.
Although dropout is beneficial for regularisation, it effectively decreases the number of gradient updates per parameter without shortening the update-step wall time.
Hence, reduces the number of parameter updates per second.
Following **Cramming**[^16], we experiment with removing dropout and observed improved performance in our setup.

</td><td>

**随机失活**

原始的 **OPT** 模型包含随机失活以减轻过拟合.
虽然随机失活有助于正则化, 但它实际上会减少每个参数的梯度更新次数, 而不会缩短更新步长的持续时间.
因此, 减少每秒参数更新次数.
遵循 **Cramming**[^16], 我们尝试去掉随机失活, 并观察到在我们的设置中获得了改进性能.

</td></tr>
<tr><td>

**Positional Encoding**

**Transformers** rely on positional encoding to capture the order of input tokens.
Many modern LMs, including the **Qwen** models, use **Rotary Position Embedding**[^57].
This method uses a hyperparameter, $\theta$, to control the trade-off between granularity and the ability to handle long contexts.
$\theta$ is often tuned to accommodate longer context lengths (**Qwen2.5**[^58]; **Code LLaMA**[^59]).
Since our context length is significantly shorter than that of the original LLM, we explore reducing $\theta$ for potential performance gains.
Our findings show that setting $\theta=10,000$ with a context length of 1024 enhances performance, so we adopt this configuration moving forward.
We note that since we increase the context length, we need to reduce the batch size as well, to not run into memory problems when training.
We reduce the batch size by a half and keep the same amount of gradient accumulation steps, which gives us an effective batch size of 128.

</td><td>

**位置编码**

**Transformers** 依赖于位置编码来捕获输入 Token 的顺序.
许多现代语言模型, 包括 **Qwen** 模型, 使用**旋转位置嵌入**[^57].
这一方法使用一个超参数 $\theta$ 来控制粒度和处理长上下文能力之间的平衡.
$\theta$ 通常根据长上下文长度进行调优 (**Qwen2.5**[^58]; **Code LLaMA**[^59]).
由于我们的上下文长度远小于原始语言模型的上下文长度, 我们探索降低 $\theta$ 以获得潜在性能提升.
我们的发现表明, 设置 $\theta=10,000$ 且上下文长度为 1024 增强了性能, 因此我们采用这一配置作为后续实验的基础.
我们注意到, 由于我们增加了上下文长度, 我们需要减少批量大小, 以避免在训练时出现内存问题.
我们将批量大小减半, 保持相同的梯度累积步数, 因此获得了有效批量大小为 128.

</td></tr>
<tr><td>

**Optimizer and Scheduler**

Various optimizers and schedulers have been developed to enhance training efficiency, reduce memory usage (**Adafactor**[^60], [^61]), or accelerate convergence (**AdEMAMeix**[^62], **Lion**[^63]).
With limited compute, faster convergence and better memory efficiency can be especially important.
We first consider efficient optimizers, specifically **AdamW with fused kernels**, and **8-bit AdamW**, but observe no notable improvements in batch size or runtime compared to standard **AdamW**.
This could do with the relatively small model size, resulting in a minimal memory footprint of the optimizers.
We then compare **AdamW** with two state-of-the-art optimizers: **AdaLomo**[^64] and **AdEMAMeix**[^62].
Results, presented in [Figure.04](#Fig.04), suggest that with the original InverseSqrt scheduler used by **TWIST**[^11], using **AdEMAMeix** improves validation loss, compared to **AdamW**, with **AdaLomo** far behind.

Next, we analyse a cosine decay learning rate scheduler, in place of the original InverseSqrt as this was shown to improve convergence (**SGDR**[^65]).
We consider the previous optimizers, and provide the validation loss throughout training in [Figure.04](#Fig.04).
We see that this notably improved the loss for **AdamW**, and slightly harmed results for **AdEMAMeix**.
Overall, **AdamW** with a cosine schedule provide the best setup, far outperforming the original setup.

</td><td>

**优化器和调度器**

各种优化器和调度器被开发用于增强训练效率, 减少内存使用 (**Adafactor**[^60], [^61]), 或加速收敛 (**AdEMAMeix**[^62], **Lion**[^63]).
在有限的计算下, 更快的收敛和更好的内存效率尤为重要.

我们首先考虑高效的优化器, 特别是带有**融合内核的 AdamW**, 以及 **8-bit AdamW**, 但在批量大小和运行时间方面, 与标准 **AdamW** 相比, 观察不到显著的改进.
这可能与相对较小的模型大小有关, 导致优化器的内存占用最小.
然后, 我们比较了 **AdamW** 与两个最先进的优化器: **AdaLomo**[^64] 和 **AdEMAMeix**[^62].
结果, 如[图 04](#Fig.04) 所示, 表明, 在 **TWIST**[^11] 使用的原始 InverseSqrt 调度器下, 使用 **AdEMAMeix** 能够改善验证损失, 相比于 **AdamW**, **AdaLomo** 稍逊.

接下来, 我们分析了余弦衰减学习率调度器, 代替了原始的 InverseSqrt, 因为这一方法被证明可以提高收敛速度 (**SGDR**[^65]).
我们考虑之前的优化器, 并在训练过程中提供验证损失, 如[图 04](#Fig.04) 所示.
我们看到, 对于 **AdamW**, 这一方法显著提高了损失, 而对于 **AdEMAMeix**, 结果稍有下降.
总而言之, **AdamW** 与余弦调度器提供的最佳设置, 远远优于原始设置.

</td></tr></table>

### Data: 数据

<table><tr><td width="50%">

Next, we examine how the training data-mix influences performance in a compute-constrained setting.
Specifically, we explore whether diversity in accents, speaking styles, etc. is beneficial and assess whether synthetic data can enhance semantic modelling abilities.

</td><td>

接下来, 我们检查训练数据混合对计算受限设置中的性能影响.
具体来说, 我们探索在口音, 说话风格等方面引入多样性是否有益, 并评估合成数据是否可以增强语义建模能力.

</td></tr>
<tr><td>

**Diverse Data**

We begin by examining how dataset diversity impacts model performance.
Many leading speech datasets, such as those based on audiobooks (**LibriSpeech**[^66], **Libri-Light**[^67]), consist of relatively clean, single-speaker recordings within a specific content domain.
To introduce greater diversity in speaking styles and content, we curate additional datasets, including **VoxPopuli**[^68], **TED-LIUM 3**[^69], **PeopleSpeech**[^70], and **SWC**[^71].
For all mentioned datasets, we use the official data cleaning and preprocessing scripts when available.
Specifically, for **Libri-Light**, we apply the official Voice Activity Detection model to remove silences and generate smaller audio segments.
To evaluate the impact of dataset diversity, we compare the performance of SLMs trained using our best training recipes using a subset of **LibriSpeech** and **Libri-Light** against all curated datasets.
This comparison is conducted for both **OPT-125M**, which processes a large number of tokens during training, and **Qwen-0.5B**, which encounters significantly less data due to model size.
Results are summarised in Table.01.
We observe that dataset diversity has an overall negative effect on model performance.
We hypothesise this is due to the models struggling in modelling rich and complex audio under such low compute resources.

</td><td>

**多样性数据**

我们从检查数据集多样性对模型性能的影响开始.
现有的主流语音数据集, 例如基于有声书的语料库 (**LibriSpeech**[^66]; **Libri-Light**[^67]), 基于相对干净, 单说话人录音, 只包含特定内容领域.
为了引入说话风格和内容的更多多样性, 我们借助额外的数据集, 包括 **VoxPopuli**[^68]; **TED-LIUM 3**[^69]; **PeopleSpeech**[^70]; **SWC**[^71].
对于所有提到的数据集, 我们尽可能使用官方数据清理和预处理脚本.
其中, 对于 **Libri-Light**, 我们使用官方语音活动检测模型来去除静音, 生成较小的音频片段.

为了评估数据集多样性, 我们比较 SLM 在使用我们最佳训练配方, 即使用 **LibriSpeech** 和 **Libri-Light** 的子集, 与所有额外的数据集训练的性能.
这项比较针对 **OPT-125M** 和 **Qwen-0.5B** 进行, 前者处理大量 Token, 后者遇到模型大小限制显著更少的数据量.
结果总结于表 01.

我们观察到, 数据集多样性对模型性能有着整体负面影响.
我们假设, 这是由于模型在如此低计算资源下无法很好地处理丰富和复杂的音频.

</td></tr>
<tr><td>

**Synthetic Data**

Recent studies have highlighted the potential of synthetic data generated through TTS [^12] or direct text-to-unit conversion [^13].
Hence, we examine the impact of including synthetically generated speech within our constrained compute setup.
To do so, we synthesised the **TinyStories dataset**[^72] using a single-speaker TTS model (**fairseq S^2**[^73]), as it is computationally efficient.
Additionally, prior research has shown that **HuBERT** units largely remove speaker information (**DISSC**[^34]).
**TinyStories** has been demonstrated to enhance text LM performance and improve SLMs[^12].
Results are presented in Table.01.
Results indicate that incorporating such synthetic data into the training data-mix significantly boosts both modelling and generative performance metrics, across all evaluated setups.
We also consider adding the synthetic data to the original **TWIST** recipe, and the results in the bottom of Table.02 suggests that while this helps with semantic metrics, it is far from enough without other optimizations we introduced.

We see that across all datasets, and specifically with our best mixture **Libri-Light**, **LibriSpeech** and ***sTinyStories***, **Qwen-0.5B** outperforms **OPT-125M** so we continue with it to the final stages.

</td><td>

**合成数据**

近期的研究表明通过文本转语音[^12] 或直接文本到单元转换[^13] 生成合成数据的潜力.
因此, 我们检查在受限计算设置中包含合成语音的影响.
为此, 我们使用单说话人 TTS 模型 (**fairseq S^2**[^73]) 合成了 **TinyStories 数据集**[^72], 因为它计算效率高.
此外, 先前的研究表明 HuBERT 单元大部分消除了说话人信息 (**DISSC**[^34]).
TinyStories 被证明可以增强文本语言模型性能, 并改善 SLM 性能[^12].
结果如表 01 所示.

结果表明, 将这样的合成数据集纳入训练数据混合, 显著提高了建模和生成性能指标, 适用于所有评估设置.
我们还考虑将合成数据集添加到原始 **TWIST** 配方中, 结果如表 02 所示, 表明, 虽然这有助于语义度量, 但它并不足以无需我们引入的其他优化.

我们看到在所有数据集上, 特别是我们的最佳混合 **Libri-Light**, **LibriSpeech** 和 ***sTinyStories***, **Qwen-0.5B** 都优于 **OPT-125M**, 因此我们继续使用它到最终阶段.

</td></tr></table>

### Text Interleaving: 文本交错

<table><tr><td width="50%">

Several recent SLMs combine both speech and text modalities, either predicting both simultaneously (**Moshi**[^15], **LLaMA-Omni**[^74], **Mini-Omni**[^75]) or training on interleaved data (**SpiRit-LM**[^14], [^13]).
Beyond enhancing cross-modal abilities, this has been shown to improve the semantic capabilities of SLMs, even in speech-only evaluations.
Building on these studies, we investigate whether speech-text interleaving can enhance semantic ability in speech-only tasks, even under strict computational constraints.

For this we use **Whisper-Large-V3-Turbo** to get aligned transcriptions of our data, except ***sTinyStories*** for which we get alignment from the TTS.
We follow [^13] by selecting speech spans with length from a Poisson distribution with $\lambda=10$ totalling $30\%$ of the interleaved data.
Following **SpiRit-LM**[^14] we train with balanced batches regarding token count between text data, speech data and interleaved data.
We use a subset of **RedPajama**[^76] filtered by **Gopher**[^77] rules as our text data.

We find that under our setup performance is notably worse than speech-only ***Slam*** in all metrics.
We hypothesise that in the ***Slamming*** setup the larger vocabulary (leading to more parameters), and fewer speech tokens led to under-trained models.
We leave for future work to find the minimal compute budget to benefit from text-interleaving or to consider other efficient approaches.

</td><td>

数项近期 SLM 将语音和文本模态相结合, 要么可以同时预测 (**Moshi**[^15], **LLaMA-Omni**[^74], **Mini-Omni**[^75]), 要么在交错数据上训练 (**SpiRit-LM**[^14], [^13]).

除了增强跨模态能力之外, 还表明可以提升 SLM 的语义能力, 即使在仅语音的评估中.
基于这些研究, 我们调查了语音文本交错是否可以增强在仅语音任务中的语义能力, 即使在严格的计算约束下.

为此我们使用 **Whisper-Large-V3-Turbo** 来获得我们数据的对齐转录文本, 除了 ***sTinyStories*** 我们从 TTS 获得对齐.
我们遵循 [^13] 的方法, 选择长度服从泊松分布 $\lambda=10$ 的音频片段, 总计 $30\%$ 的交错数据.
遵循 **SpiRit-LM**[^14] 的方法, 我们使用具有平衡批次的文本, 语音和交错数据的 Token 数量进行训练.
我们使用由 **Gopher**[^77] 规则过滤的 **RedPajama**[^76] 的子集作为文本数据.

我们发现, 在我们的设置中, 在所有指标上性能明显明显低于仅语音的 ***Slam***.
我们假设在 ***Slamming*** 设置中, 较大的词汇量 (导致更多参数), 以及较少的语音 Token 导致训练不足的模型.
我们将此留给未来的工作来找到最低计算预算来受益于文本交错, 或考虑其他高效的方法.

</td></tr></table>

### Synthetic Data Preference Optimization: 合成数据偏好优化

<table><tr><td width="50%">

Preference optimization methods have been shown to enhance the performance of text LLMs [^78] and, more recently, SLMs (**Align-SLM**[^41]).
With preference optimization, we aim to train our model to generate outputs that better align with a specified reward function or preference set.

</td><td>

偏好优化方法被证明可以增强文本大语言模型的性能[^78], 最近的研究表明, 也适用于 SLM (**Align-SLM**[^41]).
结合偏好优化, 我们的目标是训练模型以生成更好的与指定奖励函数或偏好集相匹配的输出.

</td></tr>
<tr><td>

We evaluate how preference optimization affects SLM performance while considering our constrained computational budget.
Using an off-policy approach with pre-generated preference data, we apply **DPO** to enhance training efficiency.
Specifically, we synthetically generate the **SWAG**[^79] text corpus for evaluating semantic knowledge.
**SWAG** consists of text prefixes paired with multiple possible suffixes, where only one is semantically plausible.
For preference data, we use the first sentence as the prompt, the correct suffix as the positive continuation, and a randomly chosen incorrect suffix as the rejected continuation.
To ensure quality, we filter out samples with repetitive patterns, identified by an auto-BLEU score above $0.3$.
We generate all recordings using **Kokoro TTS**[^80], incorporating four speakers (two male and two female), evenly split between British and American accents.
This process results in a total of 47K **SWAG** preference pairs.

</td><td>

我们评估偏好优化如何影响 SLM 性能, 同时考虑到我们的受限计算预算.
在预先生成的偏好数据上使用 Off-Policy 方法, 我们应用 **DPO** 来增强训练效率.

具体来说, 我们合成 **SWAG**[^79] 文本语料库, 以评估语义知识.
**SWAG** 由文本前缀和多个可能的后缀组成, 其中只有一个是语义上合理的.
对于偏好数据, 我们使用第一句作为提示, 正确的后缀作为正向延续, 随机选择的错误后缀作为拒绝延续.
为了确保质量, 我们过滤掉重复模式, 其自动 BLEU 得分高于 $0.3$.
我们使用 **Kokoro TTS**[^80] 生成所有录音, 包含四位说话人 (两男两女), 平等地包含 British 和 American 口音.
这一过程产生了 47K **SWAG** 偏好对.

</td></tr>
<tr><td>

For **DPO** we use $\beta=0.1$ (see Appendix A for full hyperparameters).
In initial tests, we observe that after **DPO** training, the model shows increased likelihood at the cost of repeated patterns, a known issue with **DPO** (**DivPO**[^81]).
To address this, we apply a repetition penalty with a factor of 1.1, following the approach of **CTRL**[^82], and find that it helps mitigate the problem.
Future work could explore alternative solutions, such as proposed by **DivPO**[^81].

</td><td>

对于 **DPO** 我们使用 $\beta=0.1$ (参见附录 A 了解完整超参数).

在初始测试中, 我们观察到在 **DPO** 训练后, 模型出现了似然增加, 代价是重复模式, 这是 **DPO** 已知的问题 (**DivPO**[^81]).
为了解决这个问题, 我们应用重复惩罚因子 1.1, 遵循 **CTRL**[^82] 的方法, 并发现它有助于缓解问题.
未来工作可以探索其他解决方案, 例如 **DivPO**[^81].

</td></tr>
<tr><td>

We begin by examining how the allocation of budget for **DPO** impacts performance, particularly when it comes at the cost of a shorter pre-training phase.
Figure.05 depicts the results.
We observe significant improvements across all metrics when applying **DPO** for at least 30 minutes compared to not using **DPO** at all.
However, allocating a higher proportion of the budget to **DPO** does not yield further gains and can even degrade model performance.
Thus we stick to 30 minutes out of 24 hours for **DPO**, using the rest for pre-training.

</td><td>

我们从检查为 **DPO** 预算的分配对性能的影响开始, 特别是当它的代价是较短的预训练阶段时.
图 05 展示了结果.
当应用 **DPO** 至少 30 分钟时, 我们观察到所有指标相比不使用 **DPO** 都有显著的改进.
然而, 分配更多的预算给 **DPO** 并不能获得进一步的收益, 甚至会降低模型性能.
因此, 我们将 24 小时内留 30 分钟给 **DPO**, 其余时间用于预训练.

</td></tr></table>

## 5·Final Recipe: 最终配方

<table><tr><td width="50%">

Building on these empirical findings, we develop the final ***Slam*** recipe.
Using it, we train SLMs based on **Qwen2.5-0.5B**.
We then compare ***Slam*** to the **TWIST** model family across various sizes: 350M, 1.3B, 7B, and 13B.
We also present results for **TWIST-350M** using our computational constraints but following **TWIST**'s original training recipe, along with our synthetic data.
Finally, we report results for the top-performing model from [^12], including their predicted optimal performance under our compute budget based on SLM scaling laws.
Results are reported in Table.02.
The results indicate that ***Slam*** delivers performance that is either superior or on par with baseline models while requiring significantly fewer computational resources (e.g., a single A5000 for a day compared to 160 days on a V100).

</td><td>

建立在这些经验性发现之上, 我们开发了最终的 ***Slam*** 配方.
使用该方案, 我们基于 **Qwen2.5-0.5B** 训练 SLM.
然后, 我们将 ***Slam*** 与 **TWIST** 模型系列中的不同大小进行比较: 350M, 1.3B, 7B, 13B.
我们还展示了使用我们计算资源约束但遵循 **TWIST** 原始训练配方的 **TWIST-350M** 的结果, 以及我们合成数据的结果.
最后, 我们报告了来自 [^12] 的表现最佳的模型的结果, 包括基于 SLM 尺度定律在我们计算预算下预测的最佳性能.
结果在表 02 中呈现.

结果表明 ***Slam*** 的性能是优于或与基线模型相当的, 同时只需要相当少的计算资源 (例如, 一天的 A5000 和 160 天的 V100).

</td></tr></table>

## 6·Increasing Compute: 增加计算

<table><tr><td width="50%">

Similarly to **Cramming**[^16], we analyse whether the proposed approach holds well also in increased compute budget.
We opt for 48 hours on 2 A100 GPUs as a reasonable academic budget for larger scale tests, and represents $\sim10$ times more compute than the ***Slamming*** setting.
We use exactly the same ***Slam*** recipe for more steps, and increase the batch size times 2.
We provide the full results in Table.03.
We note that the performance continues to improve across all metrics, also outperforming methods which have far larger compute scales.
We note that **DPO** training on synthetic data for 2 epochs, notably boosts performance.

</td><td>

类似于 **Cramming**[^16], 我们分析提出的方法在增加计算预算下是否也能良好运行.
我们选择 2 块 A100 GPU 的 48 小时作为合理的学术预算, 这相当于 ***Slamming*** 设置的 10 倍计算量.
我们使用完全相同的 ***Slam*** 配方, 但增加批次大小 2.
我们提供完整的结果在表 03 中.

我们注意到, 性能在所有指标上都持续改善, 甚至超过了具有更大计算规模的方法.
我们注意到, 在合成数据上 **DPO** 训练 2 个 epoch, 显著提升了性能.

</td></tr></table>

## 7·Conclusions: 结论

<table><tr><td width="50%">

In this work we show that training high quality SLMs with a very modest compute budget, is feasible.
We give these main guidelines:
1. **Do not skimp on the model** - not all model families are born equal and the **TWIST** initialisation exaggerates this, thus it is worth selecting a stronger / bigger text-LM even if it means less tokens.
we found **Qwen2.5** to be a good choice;
2. **Utilise synthetic training data** - pre-training on data generated with TTS helps a lot;
3. **Go beyond next token prediction** - we found that **DPO** boosts performance notably even when using synthetic data, and as little as 30 minutes training massively improves results;
4. **Optimise hyper-parameters** - as researchers we often dis-regard this stage, yet we found that tuning learning rate schedulers and optimising code efficiency can improve results notably.

We hope that these insights, and open source resources will be of use to the research community in furthering research into remaining open questions in SLMs.

</td><td>

在本项工作中我们展示了使用适量的计算预算来训练高质量 SLM, 是可行的.
我们给出了以下主要指导原则:
1. **不要忽视模型** - 并非所有模型系列都一样, **TWIST** 初始值设定会放大这一点, 因此选择更强大的/更大的文本语言模型是值得的, 即使这意味着减少了 Token. 我们发现 **Qwen2.5** 是个不错的选择;
2. **利用合成训练数据** - 使用 TTS 生成的数据可以大大帮助预训练;
3. **超越 Next-Token 预测** - 我们发现 **DPO** 甚至在使用合成数据时也能显著提升性能, 30 分钟的训练可以大大提升结果;
4. **优化超参数** - 作为研究人员, 我们常常忽视这一阶段, 然而我们发现调整学习率调度器和优化代码效率可以显著提升结果.

我们希望这些见解, 以及开源资源对 SLM 研究社区的后续研究有所帮助.

</td></tr></table>

[^01]: [**Survey by Peng et al (2024)**: A Survey on Speech Large Language Models.](../../../Surveys/2024.10.24__Survey__A_Survey_on_Speech_Large_Language_Models_(17P).md) ArXiv:2410.18908.
[^02]: [**Survey by Cui et al (2024)**: Recent Advances in Speech Language Models: A Survey.](../../../Surveys/2024.10.01_Recent_Advances_in_Speech_Language_Models__A_Survey_20P/Main.md) ArXiv:2410.03751.
[^03]: [**Survey by Ji et al (2024)**: **WavChat**: A Survey of Spoken Dialogue Models.](../../../Surveys/2024.11.15_WavChat_60P/Main.md) ArXiv:2411.13577.
[^04]: [**Survey by Latif et al (2023)**: Sparks of Large Audio Models: A Survey and Outlook.](../../../Surveys/2023.08.24__Survey__Sparks_of_Large_Audio_Models_(32P).md) ArXiv:2308.12792.
[^05]: [**VALL-E**: Neural Codec Language Models Are Zero-Shot Text to Speech Synthesizers.](../ST2S/2023.01.05_VALL-E.md) ArXiv:2301.02111.
[^06]: [Unsupervised Speech Segmentation: A General Approach Using Speech Language Models.](../2025.01.07_Unsupervised_Speech_Segmentation.md) ArXiv:2501.03711.
[^07]: [**UniAudio**: Towards Universal Audio Generation with Large Language Models.](../2023.10.01_UniAudio.md) ICML2024.
[^08]: [**UniAudio 1.5**: Large Language Model-Driven Audio Codec Is a Few-Shot Audio Task Learner](../2024.06.14_UniAudio1.5.md) ArXiv:2406.10056/NeurIPS2024Poster.
[^09]: [**SALMONN**: Towards Generic Hearing Abilities for Large Language Models.](../Interaction/2023.10.20_SALMONN.md) ICLR2024.
[^10]: [**Qwen-Audio**: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models.](../ST2T/2023.11.14_Qwen-Audio.md) ArXiv:2311.07919.
[^11]: [**TWIST**: Textually Pretrained Speech Language Models.](2023.05.22_TWIST.md) ArXiv:2305.13009/NeurIPS2023Poster.
[^12]: [Scaling Properties of Speech Language Models.](../../../Modules/Scaling/2024.03.31_Scaling_Properties_of_Speech_Language_Models.md) ArXiv:2404.00685.
[^13]: [Scaling Speech-Text Pre-training with Synthetic Interleaved Data.](../../../Modules/2024.11.26_Scaling_Speech-Text_Pre-training_with_Synthetic_Interleaved_Data.md) ArXiv:2411.17607.
[^14]: [**SpiRit-LM**: Interleaved Spoken and Written Language Model.](../2024.02.08_SpiRit-LM.md) ArXiv:2402.05755/TACL2025.
[^15]: [**Moshi**: A Speech-Text Foundation Model for Real-Time Dialogue.](../Interaction/2024.09.17_Moshi.md) ArXiv:2410.00037.
[^16]: [**Cramming**: Training a Language Model on a Single GPU in One Day.](../../TextLM/2022.12.28_Cramming.md) ArXiv:2212.14034/ICML2023.
[^17]: [**ModernBERT**: Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference](../../TextLM/2024.12.18_ModernBERT.md) ArXiv:2412.13663.
[^18]: [**MobileLLM**: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases.](../../TextLM/2024.02.22_MobileLLM.md) ArXiv:2402.14905/ICML2024.
[^19]: [**Survey by Shen et al (2023)**: On Efficient Training of Large-Scale Deep Learning Models: A Literature Review](../../../Surveys/2023.04.07__Survey__On_Efficient_Training_of_Large-Scale_Deep_Learning_Models_(60P).md) ArXiv:2304.03589.
[^20]: **SwiftLearn**: A Data-Efficient Training Method of Deep Learning Models Using Importance Sampling. ArXiv:2311.15134.
[^21]: **GREATS**: Online Selection of High-Quality Data for LLM Training in Every Iteration.
[^22]: **GRASS**: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients. ArXiv:2406.17660.
[^23]: **SALT**: A Little Help Goes a Long Way: Efficient LLM Training by Leveraging Small LMs. ArXiv:2410.18779.
[^24]: **LPA**: Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention. ArXiv:2411.02063.
[^25]: **LDB**: LayerDropBack: A Universally Applicable Approach for Accelerating Training of Deep Networks. ArXiv:2412.18027.
[^26]: **FLM-101B**: An Open LLM and How to Train It with $100K Budget. ArXiv:2309.03852.
[^27]: How to Train BERT with an Academic Budget. ArXiv:2104.07705.
[^28]: Efficient Training of Self-Supervised Speech Foundation Models on a Compute Budget. ArXiv:2409.16295/SLT2024.
[^29]: [**GSLM**: On Generative Spoken Language Modeling from Raw Audio.](2021.02.01_GSLM.md) ArXiv:2102.01192/TACL2021.
[^30]: **pGSLM**: Text-Free Prosody-Aware Generative Spoken Language Modeling. ArXiv:2109.03264.
[^31]: [**AudioLM**: A Language Modeling Approach to Audio Generation.](../ST2S/2022.09.07_AudioLM.md) TASLP2023.
[^32]: [Speech Resynthesis from Discrete Disentangled Self-Supervised Representations.](../../SpeechCodec/2021.04.01_Speech_Resynthesis_from_Discrete_Disentangled_Self-Supervised_Representations.md) ArXiv:2104.00355.
[^33]: Textless Speech Emotion Conversion using Discrete and Decomposed Representations. ArXiv:2111.07402/EMNLP2022.
[^34]: **DISSC**: Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units. ArXiv:2212.09730/EMNLP2023.
[^35]: [**dGSLM**: Generative Spoken Dialogue Language Modeling.](../Interaction/2022.03.30_dGSLM.md) ArXiv:2203.16502.
[^36]: Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation. ArXiv:2204.02967.
[^37]: **MSLM-S2ST**: A Multitask Speech Language Model for Textless Speech-to-Speech Translation with Speaker Style Preservation. ArXiv:2403.12408.
[^38]: [**Spectron**: Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM.](../Interaction/2023.05.24_Spectron.md) ArXiv:2305.15255/ICLR2024.
[^39]: [**SpeechSSM**: Long-Form Speech Generation with Spoken Language Models.](../2024.12.24_SpeechSSM.md) ArXiv:2412.18603.
[^40]: **S4**: Efficiently Modeling Long Sequences with Structured State Spaces. ArXiv:2111.00396.
[^41]: [**Align-SLM**: Textless Spoken Language Models with Reinforcement Learning from AI Feedback.](../2024.11.04_Align-SLM.md) ArXiv:2411.01834.
[^42]: [**DPO**： Direct Preference Optimization: Your Language Model is Secretly a Reward Model.](../../../Modules/RLHF/2023.05.29_DPO.md) ArXiv:2305.18290/NeurIPS2024.
[^43]: [**HuBERT**: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units.](../../SpeechRepresentation/2021.06.14_HuBERT.md) TASLP2021.
[^44]: **sBLIMP**: The Zero Resource Speech Challenge 2021: Spoken Language Modelling. ArXiv:2104.14700.
[^45]: **StoryCloze**: A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories. ArXiv:1604.01696.
[^46]: [**Whisper**: Robust Speech Recognition via Large-Scale Weak Supervision.](../../ASR/2022.12.06_Whisper.md) ICML2023.
[^47]: [**LLaMA3**: The LLaMA3 Herd of Models.](../../TextLM/2024.07.31_LLaMA3.md) ArXiv:2407.21783.
[^48]: A Study of BFLOAT16 for Deep Learning Training. ArXiv:1905.12322.
[^49]: [**FlashAttention2**: Faster Attention with Better Parallelism and Work Partitioning.](../../../Modules/Attention/2023.07.17_FlashAttention2.md) ArXiv:2307.08691.
[^50]: **PyTorch 2**: Fast Machine Learning through Dynamic Python Bytecode Transformation and Graph Compilation. ACM ICASPLOS2024.
[^51]: [**FlashAttention-3**: Fast and Accurate Attention with Asynchrony and Low-precision.](../../../Modules/Attention/2024.07.11_FlashAttention3.md) ArXiv:2407.08608.
[^52]: **FlexAttention**: FlexAttention for Efficient High-Resolution Vision-Language Models. ArXiv:2407.20228.
[^53]: [**AdamW**: Decoupled Weight Decay Regularization.](../../../Modules/Optimization/2017.11.14_AdamW.md) ICLR2019.
[^54]: Training Compute-Optimal Large Language Models. ArXiv:2203.15556.
[^55]: [**SmolLM2**: When Smol Goes Big - Data-Centric Training of a Small Language Model.](../../TextLM/2025.02.04_SmolLM2.md) ArXiv:2502.02737.
[^56]: [**Pythia**: A Suite for Analyzing Large Language Models Across Training and Scaling.](../../TextLM/2023.04.03_Pythia.md) ArXiv:2304.01373/ICML2023.
[^57]: [**Roformer**: Enhanced Transformer with Rotary Position Embedding.](../../../Modules/PositionEmb/RoPE.md) Neurocomputing2024.
[^58]: [**Qwen2.5**: Qwen2.5 Technical Report.](../../TextLM/2024.12.19_Qwen2.5.md) ArXiv:2412.15115.
[^59]: **Code LLaMA**: Open Foundation Models for Code. ArXiv:2308.12950.
[^60]: [**Adafactor**: Adaptive Learning Rates with Sublinear Memory Cost.](../../../Modules/Optimization/2018.04.11_Adafactor.md) ArXiv:1804.04235.
[^61]: [8-Bit Optimizers via Block-Wise Quantization.](../../../Modules/Optimization/2021.10.06_8-Bit_Optimizers.md) ArXiv:2110.02861/ICLR2022.
[^62]: [**AdEMAMix**: The AdEMAMix Optimizer: Better, Faster, Older](../../../Modules/Optimization/2024.09.05_AdEMAMix.md) ArXiv:2409.03137.
[^63]: [**Lion**: Symbolic Discovery of Optimization Algorithms.](../../../Modules/Optimization/2023.02.13_Lion.md) ArXiv:2302.06675.
[^64]: [**AdaLomo**: Low-Memory Optimization with Adaptive Learning Rate.](../../../Modules/Optimization/2023.10.16_AdaLomo.md) ArXiv:2310.10195/ACL2024.
[^65]: [**SGDR**: Stochastic Gradient Descent with Warm Restarts.](../../../Modules/Optimization/2016.08.13_SGDR.md) ArXiv:1608.03983/ICLR2017.
[^66]: [**LibriSpeech**: An ASR Corpus Based on Public Domain Audio Books.](../../../Datasets/2015.04.19_LibriSpeech.md) ICASSP2015.
[^67]: [**Lirbi-Light**: A Benchmark for ASR with Limited or No Supervision.](../../../Datasets/2019.12.17_Libri-Light.md) ArXiv:1912.07875/ICASSP2020.
[^68]: [**VoxPopuli**: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation.](../../../Datasets/2021.01.02_VoxPopuli.md) ArXiv:2101.00390/ACL2021.
[^69]: **TED-LIUM 3**: Twice as Much Data and Corpus Repartition for Experiments on Speaker Adaptation. ArXiv:1805.04699/SPECOM2018.
[^70]: [**PeopleSpeech**: The People's Speech: A Large-Scale Diverse English Speech Recognition Dataset for Commercial Usage.](../../../Datasets/2021.11.17_The_People's_Speech.md) ArXiv:2111.09344/NeurIPS2021.
[^71]: [**SWC**: The Spoken Wikipedia Corpus Collection: Harvesting, Alignment and an Application to Hyperlistening.](../../../Datasets/SWC.md) Springer2019.
[^72]: [**TinyStories**: How Small Can Language Models Be and Still Speak Coherent English?](../../../Datasets/2023.05.12_TinyStories.md) ArXiv:2305.07759.
[^73]: [**fairseq S^2**: A Scalable and Integrable Speech Synthesis Toolkit.](../../Toolkits/2021.09.14_FairSeq_S^2.md) ArXiv:2109.06912/EMNLP2021.
[^74]: [**LLaMA-Omni**: Seamless Speech Interaction with Large Language Models.](../Interaction/2024.09.10_LLaMA-Omni.md) ArXiv:2409.06666.
[^75]: [**Mini-Omni**: Language Models Can Hear, Talk While Thinking in Streaming.](../Interaction/2024.08.27_Mini-Omni.md) ArXiv:2408.16725.
[^76]: [**RedPajama**: An Open Dataset for Training Large Language Models.](../../../Datasets/2024.11.19_RedPajama.md) ArXiv:2411.12372/NeurIPS2024.
[^77]: [**Gopher**: Scaling Language Models: Methods, Analysis & Insights from Training Gopher.](../../TextLM/2021.12.08_Gopher.md) ArXiv:2112.11446.
[^78]: [**InstructGPT**: Training Language Models to Follow Instructions with Human Feedback.](../../TextLM/InstructGPT.md) ArXiv:2203.02155/NeurIPS2022.
[^79]: [**SWAG**: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference.](../../../Datasets/2018.08.16_SWAG.md) ArXiv:1808.05326/EMNLP2018.
[^80]: **Kokoro TTS** [URL](https://doi.org/10.57967/hf/4329).
[^81]: [**DivPO**: Diverse Preference Optimization.](../../../Modules/RLHF/2025.01.30_DivPO.md) ArXiv:2501.18101.
[^82]: [**CTRL**: A Conditional Transformer Language Model for Controllable Generation.](../../_Basis/2019.09.11_CTRL.md) ArXiv:1909.05858.
