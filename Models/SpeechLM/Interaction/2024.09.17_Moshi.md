# Moshi

<details>
<summary>基本信息</summary>

- 标题: "Moshi: A Speech-Text Foundation Model for Real-Time Dialogue"
- 作者:
  - 01 Alexandre Defossez,
  - 02 Laurent Mazare,
  - 03 Manu Orsini,
  - 04 Amelie Royer,
  - 05 Patrick Perez,
  - 06 Herve Jegou,
  - 07 Edouard Grave,
  - 08 Neil Zeghidour
- 链接:
  - [ArXiv](https://arxiv.org/abs/2410.00037)
  - [Publication]
  - [Github](https://github.com/kyutai-labs/moshi)
  - [Demo]
- 文件:
  - [ArXiv](_PDF/2410.00037v2__Moshi__A_Speech-Text_Foundation_Model_for_Real-Time_Dialogue.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

We introduce ***Moshi***, a speech-text foundation model and full-duplex spoken dialogue framework.
Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech.
Such frameworks cannot emulate the experience of real conversations.
First, their complexity induces a latency of several seconds between interactions.
Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction.
Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections.
***Moshi*** solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation.
Starting from a text language model backbone, ***Moshi*** generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams.
This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics.
We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens.
Not only this `Inner Monologue` method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech.
Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at [this https URL](https://github.com/kyutai-labs/moshi).

</details>
<br>

我们介绍了 ***Moshi***, 这是一个语音-文本基础模型和全双工口语对话框架.
当前的口语对话系统依赖于一系列独立的组件, 即语音活动检测, 语音识别, 文本对话, 文本转语音.
这样的框架无法模拟真实对话的体验.

首先, 其复杂性导致交互之间存在数秒钟的延迟.
其次, 由于文本是对话的中间媒介, 修改对话含义的非语言信息 (如情感或非语音声音) 将在交互中丢失.
最后, 它们依赖于将对话分割为说话者轮次, 这没有考虑到重叠语音, 打断和插话.

***Moshi*** 通过将口语对话视为语音到语音生成, 一次性解决了这些独立的问题.
从文本语言模型主干开始, ***Moshi*** 以 Token 的形式从神经音频编解码器的残差量化器生成语音, 同时将自身语音和用户语音分别建模为并行流.
这使得可以去除显式的说话者轮次, 并建模任意的对话动态.

此外, 我们将先前工作的分层语义到声学 Token 生成过程扩展为首先预测时间对齐的文本 Token 作为音频 Token 的前缀.
这种 `内心独白` 方法不仅显著提高了生成语音的语言质量, 我们还展示了它如何提供流式语音识别和文本转语音.
我们得到的模型是第一个实时全双工口语大型语言模型, 理论延迟为 160 毫秒, 实际延迟为 200 毫秒, 并且可以在以下链接中获取：[this https URL](https://github.com/kyutai-labs/moshi).

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论
