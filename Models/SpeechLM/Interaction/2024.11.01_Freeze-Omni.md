# ***Freeze-Omni***

<details>
<summary>基本信息</summary>

- 标题: "Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model with Frozen LLM"
- 作者:
  - 01 Xiong Wang,
  - 02 Yangze Li,
  - 03 Chaoyou Fu,
  - 04 Lei Xie,
  - 05 Ke Li,
  - 06 Xing Sun,
  - 07 Long Ma
- 链接:
  - [ArXiv](https://arxiv.org/abs/2411.00774)
  - [Publication]()
  - [Github](https://github.com/VITA-MLLM/Freeze-Omni)
  - [Demo](https://freeze-omni.github.io)
- 文件:
  - [ArXiv](../PDF/2411.00774v1__Freeze-Omni__A_Smart_and_Low_Latency_Speech-to-Speech_Dialogue_Model_with_Frozen_LLM.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

The rapid development of large language models has brought many new smart applications, especially the excellent multimodal human-computer interaction in GPT-4o has brought impressive experience to users.
In this background, researchers have proposed many multimodal LLMs that can achieve speech-to-speech dialogue recently.
In this paper, we propose a speech-text multimodal LLM architecture called ***Freeze-Omni***.
Our main contribution is the speech input and output modalities can connected to the LLM while keeping the LLM frozen throughout the training process.
We designed 3-stage training strategies both for the modeling of speech input and output, enabling ***Freeze-Omni*** to obtain speech-to-speech dialogue ability using text-speech paired data (such as ASR and TTS data) and only 60,000 multi-round text Q&A data on 8 GPUs.
Moreover, we can effectively ensure that the intelligence of the ***Freeze-Omni*** in the speech modality is at the same level compared with that in the text modality of its backbone LLM, while the end-to-end latency of the spoken response achieves a low level.
In addition, we also designed a method to achieve duplex dialogue ability through multi-task training, making ***Freeze-Omni*** have a more natural style of dialogue ability between the users.
***Freeze-Omni*** mainly provides a possibility for researchers to conduct multimodal LLM under the condition of a frozen LLM, avoiding various impacts caused by the catastrophic forgetting of LLM caused by fewer data and training resources.

## 1·Introduction: 引言

In recent years, the development of large language models has been extremely rapid.
A series of large language models represented by the GPT series~\citep{floridi2020gpt,achiam2023gpt} of OpenAI has demonstrated extraordinary capabilities.
As speech interaction is one of the most natural forms of human-computer interaction, combining speech input and output with an LLM can bring an extraordinary experience to users.
The traditional method is to use a cascaded approach of ASR + LLM + TTS to achieve the interaction with LLM in speech modality.
However, this approach often leads to a relatively high engineering complexity and a considerable interaction latency.
Nevertheless, GPT-4o~\citep{NBS_2021} has changed this situation, it provides an end-to-end speech interaction mode which has significantly improved the user experience, triggering a research boom among researchers regarding multimodal LLMs for speech-to-speech interaction.

In the field of general LLMs, many public models such as Llama 3.2~\citep{dubey2024llama}, Qwen2.5~\citep{qwen2.5}, Mixtral~\citep{jiang2024mixtral}, etc. have provided very good opportunities for researchers to develop downstream tasks on them.
Therefore, in the research field of multimodal LLMs for speech-to-speech, works such as Mini-Omni2~\citep{xie2024mini2}, LLaMA-Omni~\citep{fang2024llama}, and Moshi~\citep{defossez2024moshi} have provided excellent references for researchers.
These works adopt different strategies to align the speech modality with the LLM and design some methods to achieve a duplex dialogue mode, demonstrating excellent performance.

In this research context, we found that in the process of aligning the LLM with the speech modality in existing public speech-text multimodal LLMs~\citep{chu2024qwen2,defossez2024moshi,fang2024llama,fu2024vita,zhang2023speechgptempoweringlargelanguage,xie2024mini}, the parameters of the LLM are more or less fine-tuned.
However, in most cases, it is very difficult for researchers to easily collect spoken Q\&A data at the million-hour level (the corresponding text content can be comparable to the amount of data for training text-modal LLMs).  This inevitably brings about the forgetting problem to the LLM, resulting in a negative impact on its intelligence.
In addition, only a few works have evaluated the accuracy of spoken question-answering tasks for speech-to-speech multimodal LLMs, and show an obvious gap in performance between spoken question-answering and text-modality question-answering.
Therefore, in this paper, we propose a speech-to-speech dialogue LLM called ***Freeze-Omni***, achieving speech modality alignment while the LLM is frozen throughout the training process, and obtaining low latency speech dialogue capabilities while keeping the intelligence of the backbone LLM.
***Freeze-Omni*** is mainly implemented in the following steps:

**Modeling of speech input**

We first use a large amount of ASR data to align the speech encoder and the LLM, enabling the LLM to understand the semantic information from the speech.
Then, with the LLM frozen, a training strategy of prompt embedding is used to let the model have the ability to possess speech input to text output, training on only a small amount of Q\&A data.

**Modeling of speech output**

Second, we use a mount of text-speech paired data to train the AR-based speech decoder which can generate speech tokens from text and a single-codebook based codec model is used to decode the speech token into waveform.
Then, we design a prefix kv-cache fine-tune strategy, using the hidden state vector output by the LLM to transfer the speech decoder into the output text space of LLM, achieving the ability of text input to speech output while keeping the LLM frozen.

**Design for duplex dialogue**

Finally, we simultaneously connect the speech encoder and speech decoder from the above parts to the backbone LLM.
Then, a task of chunk-wise state prediction is used to enable the LLM to interrupt or reject the user's input, achieving the duplex speech-to-speech dialogue ability.

In conclusion, the main contributions of the proposed ***Freeze-Omni*** are as follows:
- The parameters of the LLM are completely frozen throughout the training process, ensuring that the intelligence of the LLM will be kept.
At the same time, the ability of low latency speech-to-speech dialogue is still obtained.
- The data scale relied on during the training process is small and consumes fewer computing resources.
It requires text-speech paired data (such as ASR and TTS training data) and only a small amount of Q\&A data in text modality.
- ***Freeze-Omni*** can support any (multimodal) LLM that has a text modality and retains the abilities of the LLM such as prompt following and role-playing.
Moreover, if it is necessary to change the style of the LLM's response, it is only necessary to fine-tune the LLM with text data in the corresponding style.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

### 3.1·Overview

***Freeze-Omni*** is a speech-to-speech dialogue model and the architecture is shown in Fig.~\ref{fig:overview}, exhibiting the characteristic of being "smart" as it is constructed upon a "frozen" text-modality LLM.
This enables it to keep the original intelligence of the LLM backbone, without being affected by the forgetting problem induced by the fine-tuning process for integration of the speech modality.
Specifically, ***Freeze-Omni*** contains a speech encoder that supports streaming speech input and a speech decoder that generates streaming output speech.
During the training process, ***Freeze-Omni*** first achieves the alignment between speech input to text output, and then the text input to speech output.
Finally, by connecting these two components with the LLM, the ability of speech input to speech output is obtained.
This section will provide a detailed introduction to the architecture, training strategy, and duplex dialogue design of ***Freeze-Omni***.

### 3.2·Modeling of Speech Input

#### 3.2.1·Chunk-Wise Streaming Speech Encoder

To enable ***Freeze-Omni*** to support speech input and achieve a rapid and low-latency response to the input speech, it utilizes a chunk-wise streaming speech encoder to transform the input speech features into a high-dimensional representation.
Then, an adapter module maps the high-dimensional representation into the embedding space of the backbone LLM.
The speech encoder module here consists of several down-sampling convolution layers and several Transformer~\citep{DBLP:conf/nips/VaswaniSPUJGKP17} blocks, while the adapter only comprises several down-sampling convolution layers.
The reason for using down-sampling is to reduce the frame rate of the speech features, increase the speed of the LLM in the prefill stage, and decrease the latency.

#### 3.2.2·Training Strategy

A 3-stage training strategy shown in Fig.~\ref{fig:speech_input} is used for the speech encoder, enabling ***Freeze-Omni*** to acquire the ability to understand the streaming input speech while keeping the LLM frozen.
- The first stage shown in Fig.~\ref{fig:speech_input}(a) is the same as the training process of a common speech recognition model.
The input is speech features and the label is the transcript corresponding to the speech, CTC~\citep{DBLP:conf/icml/GravesFGS06} is used as the loss function.
- In the second stage shown in Fig.~\ref{fig:speech_input}(b), we use the speech encoder trained in the first stage as the initialization parameter and connect it with the LLM using an adapter.
The output of the LLM still uses the transcript corresponding to the input speech as the label.
Several trainable special tokens are added to the input part to guide the LLM in completing the training process at this stage.
In this stage, except for the frozen LLM, the parameters of other networks are all trainable.
-  In the last stage shown in Fig.~\ref{fig:speech_input}(c), we first construct a dataset of multi-round questions and use the LLM backbone relied on in the training to generate multi-round answers.
The dataset constructed in this way will be completely compatible with the LLM backbone.
Subsequently, we use a multi-speaker TTS system to generate data in the speech modality for the questions part and add trainable prompt embedding before each question in the multi-round to guide the LLM to achieve the ability of speech input to text output.
In this stage, the trainable special tokens in stage 2 will be dropped, only the prompt embedding part is trainable and they use the same parameters for each question, the speech encoder is frozen to maintain the acoustic robustness obtained from stage 2, and the LLM is also frozen to ensure that its intelligence is not affected.

### 3.3·Modeling of Speech Output

#### 3.3.1·Architecture

Inspired by VALL-E~\citep{chen2024vall}, ***Freeze-Omni*** uses a token-based speech decoder which contains NAR prefill and AR generate stage to achieve speech output capabilities.
The speech decoder mainly consists of the NAR decoder, the AR decoder, and the decoder of a codec model.
Both the NAR decoder and AR decoder are built upon transformer blocks.
The NAR decoder is used to model the semantic features from the output of LLM, and then the AR decoder generates speech tokens based on the output of the NAR decoder.
Finally, a decoder of the codec model converts the speech tokens into a speech stream.

#### 3.3.2·Training Strategy

For the modeling of speech output, we still use a 3-stage training method as shown in Fig.~\ref{fig:speech_output}, enabling ***Freeze-Omni*** to obtain the ability of generate speech from the output of LLM while keeping the LLM frozen.

- As shown in Fig.~\ref{fig:speech_output}(a), we first train a single-codebook based codec model using only speech data.
Since a single codebook is sufficient for extracting speech tokens from the speech signal of a limited number of speakers, using a single codebook here can reduce the complexity and latency of the system as much as possible.
- In the second stage shown in Fig.~\ref{fig:speech_output}(b), we first construct a large amount of text-speech paired data and pass the text through the tokenizer of the backbone LLM to convert the text into text tokens.
Then, we pass the text tokens through the embedding layer of the LLM to convert them into embedding vectors as semantic features and send them to the NAR speech decoder.
The AR speech decoder predicts the output speech tokens in the form of teacher force.
The labels here are extracted using the codec model trained in stage 1.
The NAR and AR speech decoders use the same parameters, and the embedding layer of the LLM is frozen.
- In the last stage, we use the same multi-round questions and answers data set in stage 3 of Sec.~\ref{sec:2.2.2} and use the text tokens and hidden state sequence generated by the backbone LLM.
As shown in Fig.~\ref{fig:speech_output}(c), an additional NAR prefix speech decoder is added to model the hidden state of the LLM and pass its output kv-cache to the NAR speech decoder.
Then the text token will be fed to the NAR speech decoder trained in stage 2.
The text token label for AR speech decoder is the speech data produced by the output text of LLM using a TTS system and converted into speech tokens by the codec model in stage 1.
In this stage, the NAR prefix speech decoder uses different parameters from the NAR and AR speech decoders, and only the parameters of the NAR prefix speech decoder are trainable while the parameters of other networks are frozen.
Because the style of the text tokens produced by the LLM is different from that of the text in the large amount of text-speech paired data obtainable in stage 2, the significance of the third stage lies in closely coupling the speech decoder with the output of the LLM to reduce the occurrence of bad cases.

### 3.4·Design for Duplex Dialogue

After the above training process, ***Freeze-Omni*** has the ability of speech input to speech output.
However, to better approximate the natural form of speech-to-speech dialogue, we use multi-task for chunk-level state prediction as shown in Fig~\ref{fig:state}.
We first use an acoustic VAD\footnote{\url{https://github.com/snakers4/silero-vad}} module to detect the starting point of the streaming speech.
When the VAD is triggered, the speech stream will sent into ***Freeze-Omni*** chunk by chunk, and an additional classification layer will be added after the last layer of the LLM to predict different states.
Three states are defined here, state 0 indicates that the current LLM can continue to receive speech, and state 1 or 2 indicates that the current chunk is the end of the speech.
State 1 means that the LLM can interrupt the user and perform the generate stage, and state 2 means that there is no need to interrupt the user.
Both of these states will stop sending speech streams to ***Freeze-Omni*** and reset the VAD module.
The training process of this part is completed in stage 3 of Sec.~\ref{sec:2.2.2}, using a multi-task method to optimize the cross-entropy loss of both the state classification layer and the LLM.
It should be noted that the state labels here are only valid on the last frame of each chunk.

Besides, we used a "model as a server" strategy to implement the speech-to-speech dialogue system.
First, we started several models simultaneously and regarded them as a server.
Then, when a user's VAD was triggered, the speech would be sent to the server in the form of chunks, and the server would be responsible for scheduling which idle model should respond to the current chunk.
Since we separated all the kv-cache and CNN cache of the speech encoder and LLM during the inference process, the server only needs to save the inference cache for each user.
In this way, any model in the server could respond to any chunk of any user, and there was no need to specify which model was used as a monitor or a generator.

## 4·Experiments: 实验

### 4.1·SetUps

#### 4.1.1·Datasets

In this paper, we only randomly selected 60,000 multi-round Q\&A data from \emph{moss-003-sft-data}
\footnote{\url{https://huggingface.co/datasets/fnlp/moss-003-sft-data}} and used backbone LLM to generate new answers to replace its original one.
We used a zero-shot TTS system to synthesize its text into speech.
For the modeling of speech input of ***Freeze-Omni***, we used 110,000h internal speech-text paired ASR data including both Chinese and English in stage 1 and stage 2.
In stage 3, we used the pairing of speech input and text output of the multi-round Q\&A data mentioned above.
For the modeling of the speech output of ***Freeze-Omni***, we used about 3,000h of text-speech paired data generated by a zero-shot TTS system in stage 1 and stage 2.
In stage 3, we used the pairing of text input and speech output of the multi-round Q\&A data mentioned above.

#### 4.1.2·Model Configuration

**LLM backend**

For experiments in this paper, we used Qwen2-7B-Instruct\footnote{\url{https://huggingface.co/Qwen/Qwen2-7B-Instruct}} as our backbone LLM.
As an outstanding 7B-level public LLM, it is beneficial for us to verify our method.
Besides, ***Freeze-Omni*** can use any LLM as a backbone in actuality because its training process does not update any parameters of the LLM.

**Speech Encoder**

We used a multi-layer convolution with 4-times downsampling and 24 layers of transformers with a hidden size of 1024.
The adapter consists of a multi-convolution layer with 2-times downsampling.
The number of parameters for the speech encoder is approximately 350M, with an output frame rate of 12.5Hz.
The input of the speech encoder is the mel-filter bank feature with a 25ms window size and 10ms shift.

**Speech Decoder**

We used TiCodec\footnote{\url{https://github.com/y-ren16/TiCodec}}~\citep{ren2023fewer} as the codec model, and we customized the configuration so that the size of the codebook is 1024 with a single-codebook and the frequency of the speech token 40Hz.
For the speech decoder part, both the NAR (Prefix) speech decoder and the AR speech decoder are 4-layer Llama decoder layers with a hidden size of 896.
The number of parameters for the speech decoder is approximately 120M and the output sample rate of codec model is 24000Hz.

#### 4.1.3·Training

In training processes we used the Adamw~\citep{Loshchilov2017FixingWD} optimizer with a warm-up learning rate scheduler, and different learning rates were used in different stages.
The learning rates used in the three stages of the modeling of speech input are 2e-4, 1e-4, and 6e-4 respectively.
The learning rates used in stage 2\&3 of the modeling of speech output are both 5e-5 and the training hyper-parameters used in stage 1 are the same as that in TiCodec.
All the experiments were completed on 8 NVIDIA A100 GPUs.

## 5·Results: 结果

### 5.1·Results on Speech Input

To measure the understanding ability of ***Freeze-Omni*** for input speech, as shown in Tab.~\ref{asr}, we verified the accuracy of ASR on different evaluation sets for the model in stage 2 of the modeling of speech input.
Since the parameters of the speech encoder and adapter used in stage 3 are unchanged compared to those in stage 2, it can be considered that these results can represent the input speech understanding ability of ***Freeze-Omni***.
In the training of stage 2, we used a dynamic chunk training method~\citep{yao2021wenetproductionorientedstreaming} to enhance the robustness of the model so that different chunk sizes can be used in stage 3.
From the results, it can be seen that in the case of dynamic chunk training, decoding with $chunk=\infty$ shows better performance compared to $chunk=4$.
If dynamic chunk training is not used but $chunk=4$ decoding is used, better results can be obtained, but this also means that the chunk size cannot be changed in stage 3.
In this paper, to pursue the best performance, all experiments are completed on the model with this configuration of the last row in Tab.~\ref{asr}.

### 5.2·Results on Speech Output

Because we investigated the speech-out performance of ***Freeze-Omni*** in a single-speaker case in this paper, we randomly selected 1,000 utterances of text tokens and hidden states output by the LLM as the input of the speech decoder and compared the ASR accuracy of the synthesized speech with the label text.
As shown in Tab~\ref{tts}, the performance of the model in stage 2 of the modeling of speech output (Speech Decoder w/o Prefix) and the model in stage 3 (Speech Decoder) under different AR decoding parameters $top\text{-}k$ are presented respectively, and CER~(\%) is evaluated using \emph{paraformer-zh}\footnote{\url{https://huggingface.co/funasr/paraformer-zh}}~\citep{gao2022paraformer}.
From the results, it can be concluded that after introducing the hidden state of the LLM as the input of the NAR prefix speech decoder, the speech decoder can be more completely aligned with the LLM, reducing the occurrence of bad cases and get a lower CER~(\%).
In addition, the increasing $top\text{-}k$ shows better robustness of the speech decoder with a prefix fine-tune.

### 5.3·Results on Spoken Question Answering

To demonstrate the intelligence of ***Freeze-Omni***, we verified the accuracy of spoken question answering on three sets: LlaMA-Questions\footnote{\url{https://github.com/google-research-datasets/LLAMA1-Test-Set}}~\citep{nachmani2023spoken}, Web Questions\footnote{\url{https://huggingface.co/datasets/Stanford/web\_questions}}~\citep{berant-etal-2013-semantic}, and Trivia QA\footnote{\url{https://nlp.cs.washington.edu/triviaqa/}}~\citep{JoshiTriviaQA2017}.
Since Web Questions and Trivia QA only have text, we used the \emph{edge-tts}\footnote{\url{https://github.com/rany2/edge-tts}} tool with voice at \emph{en-US-BrianNeural} to synthesize them into spoken modality.
Tab.~\ref{sqa} shows the accuracy of Freeze Omni and its used backbone LLM Qwen2-7B-Instruct on these three sets.
From the results, it can observed that ***Freeze-Omni*** exhibits excellent performance compared to other models because the accuracy gap between it and the backbone LLM is smaller than that of Moshi, which also verifies that ***Freeze-Omni*** has the same level of intelligence in text and speech modalities.

### 5.4·Analysis on End-to-End Latency

To verify the latency of ***Freeze-Omni*** for speech-to-speech dialogue, we defined two parts of latency, namely statistical latency and non-statistical latency.
The statistical latency refers to the time from the time the LLM interrupts the user to the first PCM chunk of speech generated.
Specifically, it can be divided into four parts as shown in Fig~\ref{latency}, these results are based on a speech token chunk size of 40 and the use of text token chunk segmentation based on the sentence-split strategy.
The non-statistical latency refers to the time from the real endpoint of speech to the LLM outputting the interrupt state.
This part needs to be measured manually and cannot be counted automatically.
According to our case analysis conclusion, the non-statistical latency is about one to two speech encoder chunk sizes.
According to the experiment configuration above, this time is about 160ms to 320ms.
In summary, if we consider the influence of network latency (about 200 to 300ms), the average latency of ***Freeze-Omni*** used in real scenarios will be controlled at about 1.2 seconds.

## 6·Conclusions: 结论

In this paper, we proposed ***Freeze-Omni***, a text-audio multimodal LLM capable of low-latency speech-to-speech dialogue, which does not need fine-tuning the LLM, showing excellent performance in various evaluation tasks.
In the future, to explore more speech dialogue capabilities, we plan to do the following work:

- We will upgrade the speech encoder to an audio encoder so that it can process and understand non-speech to complete tasks such as emotion understanding and audio captioning.
- We will add more multi-tasks to make the LLM output more task labels to complete more downstream tasks of speech dialogue, taking the state prediction ability as an example, under the condition of LLM freeze.
- We will explore how to support multi-speaker synthesis and instruct follow ability in the speech decoder part so that it can obtain more instruct information from the hidden state of the LLM and provide more abundant speech output styles.
