# AudioGPT

<details>
<summary>基本信息</summary>

- 标题: "AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head"
- 作者:
  - 01 Rongjie Huang,
  - 02 Mingze Li,
  - 03 Dongchao Yang,
  - 04 Jiatong Shi,
  - 05 Xuankai Chang,
  - 06 Zhenhui Ye,
  - 07 Yuning Wu,
  - 08 Zhiqing Hong,
  - 09 Jiawei Huang,
  - 10 Jinglin Liu,
  - 11 Yi Ren,
  - 12 Zhou Zhao,
  - 13 Shinji Watanabe
- 链接:
  - [ArXiv](https://arxiv.org/abs/2304.12995)
  - [Publication](https://doi.org/10.1609/aaai.v38i21.30570)
  - [Github](https://github.com/AIGC-Audio/AudioGPT)
  - [Demo](https://aigc-audio.github.io/AudioGPT.github.io/)
- 文件:
  - [ArXiv](../PDF/2304.12995v1__AudioGPT__Understanding_and_Generating_Speech_Music_Sound_and_Talking_Head.pdf)
  - [Publication](../PDF/2304.12995p0__AudioGPT__AAAI2024.pdf)

</details>

## Abstract: 摘要

<table><tr><td width="50%">

Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition.
Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa).
In this work, we propose a multi-modal AI system named ***AudioGPT***, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue.
With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test ***AudioGPT*** in terms of consistency, capability, and robustness.
Experimental results demonstrate the capabilities of ***AudioGPT*** in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease.
Our system is publicly available at [this https URL](https://github.com/AIGC-Audio/AudioGPT).

</td><td>

大语言模型在各种领域和任务中显示了显著的能力, 挑战了我们对学习和认知的理解.
尽管最近取得了不少成功, 现有的大语言模型并不能处理复杂的音频信息或进行口语对话 (如 Siri 或 Alexa).

在本文中, 我们提出了多模态人工智能系统, 名为 ***AudioGPT***, 它通过以下方式补充语言模型 (LLM) (如 ChatGPT) :
1) 基础模型用于处理复杂的音频信息以解决众多理解和生成任务;
2) 输入/输出接口 (ASR, TTS) 用于支持口语对话.

随着对评估多模态大语言模型在人类意图理解和与基础模型合作方面的需求不断增加, 我们概述了原则和流程, 并在一致性、能力和健壮性方面测试了 ***AudioGPT***.
实验结果表明了 ***AudioGPT*** 在解决多轮对话中与语音, 音乐, 声音和讲话头部理解和生成的能力, 为人类提供了创造丰富, 多样化的音频内容提供了前所未有的便利.
我们的系统已公开可用, 位于 [https://github.com/AIGC-Audio/AudioGPT](https://github.com/AIGC-Audio/AudioGPT).

</td></tr></table>

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论