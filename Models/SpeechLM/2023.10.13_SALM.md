# SALM

<details>
<summary>基本信息</summary>

- 标题: "SALM: Speech-Augmented Language Model with In-Context Learning for Speech Recognition and Translation"
- 作者:
  - 01 Zhehuai Chen,
  - 02 He Huang,
  - 03 Andrei Andrusenko,
  - 04 Oleksii Hrinchuk,
  - 05 Krishna C. Puvvada,
  - 06 Jason Li,
  - 07 Subhankar Ghosh,
  - 08 Jagadeesh Balam,
  - 09 Boris Ginsburg
- 链接:
  - [ArXiv](https://arxiv.org/abs/2310.09424)
  - [Publication](https://doi.org/10.1109/ICASSP48485.2024.10447553) ICASSP 2024
  - [Github](https://github.com/NVIDIA/NeMo/tree/main/examples/multimodal/speech_llm)
  - [Demo]
- 文件:
  - [ArXiv](_PDF/2110.10329v1__SLAM__A_Unified_Encoder_for_Speech_and_Language_Modeling_via_Speech-Text_Joint_Pre-Training.pdf)
  - [Publication](_PDF/2310.09424p0__SLAM__ICASSP2024.pdf)

</details>

## Abstract: 摘要

We present a novel Speech Augmented Language Model (SALM) with {\em multitask} and {\em in-context} learning capabilities.
SALM comprises a frozen text LLM, a audio encoder, a modality adapter module, and LoRA layers to accommodate speech input and associated task instructions.
The unified SALM not only achieves performance on par with task-specific Conformer baselines for Automatic Speech Recognition (ASR) and Speech Translation (AST), but also exhibits zero-shot in-context learning capabilities, demonstrated through keyword-boosting task for ASR and AST.
Moreover, {\em speech supervised in-context training} is proposed to bridge the gap between LLM training and downstream speech tasks, which further boosts the in-context learning ability of speech-to-text models.
Proposed model is open-sourced via NeMo toolkit.

## 1.Introduction: 引言

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论