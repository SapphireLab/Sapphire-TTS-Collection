# SpeechPrompt

<details>
<summary>基本信息</summary>

- 标题: "SpeechPrompt: An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks"
- 作者:
  - 01 Kai-Wei Chang,
  - 02 Wei-Cheng Tseng,
  - 03 Shang-Wen Li,
  - 04 Hung-yi Lee
- 链接:
  - [ArXiv](https://arxiv.org/abs/2203.16773)
  - [Publication](https://doi.org/10.21437/Interspeech.2022-10610)
  - [Github](https://github.com/ga642381/SpeechPrompt)
  - [Demo](https://kwchang.org/SpeechPrompt/speech-prompt-v1.html)
- 文件:
  - [ArXiv](_PDF/2203.16773v3__SpeechPrompt__An_Exploration_of_Prompt_Tuning_on_Generative_Spoken_Language_Model_for_Speech_Processing_Tasks.pdf)
  - [Publication](_PDF/2203.16773p0__SpeechPrompt__InterSpeech2022.pdf)

</details>

## Abstract: 摘要

<table><tr><td width="50%">

Speech representations learned from Self-supervised learning (SSL) models can benefit various speech processing tasks.
However, utilizing SSL representations usually requires fine-tuning the pre-trained models or designing task-specific downstream models and loss functions, causing much memory usage and human labor.
Recently, prompting in Natural Language Processing (NLP) has been found to be an efficient technique to leverage pre-trained language models (LMs).
Specifically, prompt tuning optimizes a limited number of task-specific parameters with a fixed pre-trained model; as a result, only a small set of parameters is needed to be stored for each task.
Prompt tuning improves computation and memory efficiency by leveraging the pre-trained LM's prediction ability.
Nevertheless, such a paradigm is little studied in the speech community.
We report in this paper the first exploration of the prompt tuning paradigm for speech processing tasks based on **Generative Spoken Language Model (GSLM)**.
Experiment results show that the prompt tuning technique achieves competitive performance in speech classification tasks with fewer trainable parameters than fine-tuning specialized downstream models.
We further study the technique in challenging sequence generation tasks.
Prompt tuning also demonstrates its potential, while the limitation and possible research directions are discussed in this paper.
The source code is available on [this https URL](https://github.com/ga642381/SpeechPrompt).

</td><td>

从自监督学习 (SSL) 模型中学到的语音表示可以为各种语音处理任务带来好处.
然而, 利用 SSL 表示通常需要对预训练模型进行微调, 或者设计特定任务的下游模型和损失函数, 这会导致大量的内存占用和人工成本.

最近, **提示调优 (Prompt Tuning)** 在自然语言处理 (NLP) 领域被发现是一种高效的技术, 可以充分利用预训练语言模型.
具体而言, 提示调优通过优化少量与任务相关的参数, 并保持预训练模型不变, 从而仅需要为每个任务存储少量的参数. 提示调优通过利用预训练语言模型的预测能力, 提高了计算和内存效率.

然而, 这种范式在语音领域的研究尚不多见. 本文报告了基于 **生成式语音语言模型 (Generative Spoken Language Model, GSLM)** 的提示调优范式在语音处理任务中的首次探索. 实验结果表明, 在语音分类任务中, 提示调优技术比微调专门的下游模型使用更少的可训练参数, 但仍能达到具有竞争力的性能.

我们进一步研究了该技术在具有挑战性的序列生成任务中的应用.
提示调优也展现了其潜力, 文中讨论了该方法的局限性及可能的研究方向.

源代码可以在 [此链接](https://github.com/ga642381/SpeechPrompt) 找到.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td></tr></table>
