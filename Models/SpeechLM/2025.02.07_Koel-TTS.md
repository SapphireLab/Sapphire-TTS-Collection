# Koel-TTS

<details>
<summary>基本信息</summary>

- 标题: "Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance"
- 作者:
  - 01 Shehzeen Hussain (NVIDIA Corporation, shehzeenh@nvidia.com)
  - 02 Paarth Neekhara (NVIDIA Corporation, pneekhara@nvidia.com)
  - 03 Xuesong Yang (NVIDIA Corporation)
  - 04 Edresson Casanova (NVIDIA Corporation)
  - 05 Subhankar Ghosh (NVIDIA Corporation)
  - 06 Mikyas T.Desta (NVIDIA Corporation)
  - 07 Roy Fejgin (NVIDIA Corporation)
  - 08 Rafael Valle (NVIDIA Corporation)
  - 09 Jason Li (NVIDIA Corporation)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.05236)
  - [Publication]()
  - [Github]()
  - [Demo](https://koeltts.github.io)
- 文件:
  - [ArXiv](_PDF/2502.05236v1__Koel-TTS__Enhancing_LLM_based_Speech_Generation_with_Preference_Alignment_and_Classifier_Free_Guidance.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs.
We introduce ***Koel-TTS***, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models.
Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio.
Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech.
Notably, ***Koel-TTS*** directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset.
Audio samples and demos are available on [our website](https://koeltts.github.io/).

</td><td>

尽管自回归语音 Token 生成模型能够生成具有显著多样性和自然性的语音, 而它们固有的不可控性往往导致诸如幻觉和不符合条件输入的不良发音等问题.

我们介绍 ***Koel-TTS***, 一套增强的编码器-解码器 Transformer TTS 模型, 通过结合由自动语音识别和说话人验证模型引导的偏好对齐技术来解决这些挑战.

此外, 我们引入了无分类器引导技术来进一步提升合成语音对转写文本和参考说话人音频的遵从性.

我们的实验表明, 这些优化显著提高了合成语音关于目标说话人的相似度, 可理解性和自然度.

值得注意的是, ***Koel-TTS*** 直接将文本和上下文音频映射到音频标记, 并且在上述指标上, 它直接胜过了 SoTA TTS 模型, 尽管它是用一个相对较小的数据集进行训练的.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

vThe advancement of large language models (LLMs) has brought transformative improvements to speech synthesis, enabling more natural and contextually adaptive speech generation.
In particular, there has been a recent surge in the use of LLMs for various applications such as text-to-speech (TTS) and speech-to-speech translation (**VALL-E** [^wang2023neural]; **VALL-E X** [^zhang2023speak]; **AudioLM** [^borsos2023audiolm]; **T5-TTS** [^t5tts]; **UniAudio** [^yanguniaudio]; **SpeechX** [^wang2024speechx]).
LLM-based TTS systems enable prompt-based customization, generating speech with human-like intonation while adapting to stylistic cues, contexts, and expressive nuances.
This allows for diverse applications, from conversational interfaces to expressive narration, without extensive retraining.
These advancements have been largely driven by the emergence of discrete neural audio codecs, which compress raw audio into token-based representations while preserving high fidelity (**EnCodec** [^encodec]; **DAC** [^dac_kumar2024high]; **SoundStream** [^zeghidour2021soundstream]; **Spectral Codec** [^langman2024spectral]; **LFSC** [^casanova2024low]).

[^wang2023neural]: [**VALL-E**: Neural Codec Language Models Are Zero-Shot Text to Speech Synthesizers.](2023.01.05_VALL-E.md) ArXiv2023.
[^zhang2023speak]: [**VALL-E X**: Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling.](2023.03.07_VALL-E_X.md) ArXiv2023.
[^borsos2023audiolm]: [**AudioLM**: A Language Modeling Approach to Audio Generation.](2022.09.07_AudioLM.md) TASLP2023.
[^t5tts]: [**T5-TTS**: Improving Robustness of LLM-Based Speech Synthesis by Learning Monotonic Alignment.](2024.06.25_T5-TTS.md) InterSpeech2024.
[^yanguniaudio]: [**UniAudio**: Towards Universal Audio Generation with Large Language Models.](2023.10.01_UniAudio.md) ICML2024.
[^wang2024speechx]: [**SpeechX**: Neural Codec Language Models as a Versatile Speech Transformer.](2023.08.14_SpeechX.md) TASLP2024.
[^encodec]: [**EnCodec**: High Fidelity Neural Audio Compression.](../SpeechCodec/2022.10.24_EnCodec.md) ArXiv2022.
[^dac_kumar2024high]: [**DAC**: High-Fidelity Audio Compression with Improved RVQGAN.](../SpeechCodec/2023.06.11_Descript-Audio-Codec.md) NeurIPS2024.
[^zeghidour2021soundstream]: [**SoundStream**: An End-to-End Neural Audio Codec.](../SpeechCodec/2021.07.07_SoundStream.md) TASLP2021.
[^langman2024spectral]: [**Spectral Codecs**: Spectrogram-Based Audio Codecs for High Quality Speech Synthesis.](../SpeechCodec/2024.06.07_Spectral_Codec.md) ArXiv2024.
[^casanova2024low]: [**LFSC**: Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference.](../SpeechCodec/2024.09.18_LFSC.md) ICASSP2025.

</td><td>

</td></tr>
<tr><td>

Despite these advances, LLM-based TTS systems face challenges, with hallucinations being a prominent issue (**Survey** [^sahoo2024comprehensive]; **ELLA-V** [^song2024ella]; **T5-TTS** [^t5tts]; **AudioLM** [^borsos2023audiolm]).
For example, when encountering text with repeated or redundant phrases, LLM-based TTS models may overemphasize these repetitions or fail to capture the intended flow and naturalness of the sentence.
Additionally, among the multiple outputs sampled for the same input, there can be significant variation in quality, with some outputs sounding more natural, accurate, and appealing than others.
This issue is akin to challenges faced in text-generation LLMs, where outputs may range from highly coherent to erroneous, depending on the model's response to complex prompts.

[^sahoo2024comprehensive]: **Survey**: A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models. ACL EMNLP 2024.
[^song2024ella]: [**ELLA-V**: Stable Neural Codec Language Modeling with Alignment-Guided Sequence Reordering.](2024.01.14_ELLA-V.md) ArXiv2024.

</td><td>

</td></tr>
<tr><td>

For text-generation, preference alignment techniques (**RLHF** [^christiano2017deep]; **InstructGPT**[^ouyang2022rlhf]; **DeepSeekMath** [^shao2024deepseekmath]; **DPO** [^rafailov2024direct]; **Nemotron-4** [^adler2024nemotron]) have been proposed to guide models to produce outputs that better match human preferences in coherence, relevance, and clarity.
This is achieved through training with human feedback or automated scoring, based on criteria such as factual correctness and fluency.
Driven by these advances, recent research employs preference alignment algorithms, including **RLHF** [^ouyang2022rlhf] and offline preference ranking methods (**DPO** [^rafailov2024direct]; **IPO** [^azar2024ipo]), to refine audio LLM outputs.
For instance, **SpeechAlign** [^zhang2024speechalign] proposes an iterative strategy to align speech language models with human preferences by addressing the distribution gap between golden AR tokens (from real speech) and synthetic AR tokens (generated during inference).
Although real speech from ground truth can be used to guide training, we will show that it introduces inconsistencies due to its fundamentally different distribution from model-generated tokens.
This issue makes preference-based optimization such as **DPO** [^rafailov2024direct] less effective.
Nonetheless, this approach has been applied in scenarios where obtaining high-quality positive examples is particularly challenging (**SPIN** [^chen2024self]; **SpeechAlign** [^zhang2024speechalign]).

[^christiano2017deep]: [**RLHF**: Deep Reinforcement Learning from Human Preferences.](../../Modules/RLHF/2017.06.12_RLHF.md) NeurIPS2017.
[^ouyang2022rlhf]: [**InstructGPT**: Training Language Models to Follow Instructions with Human Feedback.](../TextLM/InstructGPT.md) NeurIPS2022.
[^shao2024deepseekmath]: [**DeepSeekMath**: Pushing the Limits of Mathematical Reasoning in Open Language Models.](../TextLM/2024.02.05_DeepSeekMath.md) ArXiv2024.
[^rafailov2024direct]: [**DPO**: Direct Preference Optimization: Your Language Model Is Secretly A Reward Model.](../../Modules/RLHF/2023.05.29_DPO.md) NeurIPS2024.
[^adler2024nemotron]: [**Nemotron-4** 340B Technical Report.](../TextLM/2024.06.17_Nemotron-4.md) ArXiv2024.
[^azar2024ipo]: [**IPO**: A General Theoretical Paradigm to Understand Learning from Human Preferences.](../../Modules/RLHF/2023.10.18_IPO.md) AISTATS2024.
[^zhang2024speechalign]: [**SpeechAlign**: Aligning Speech Generation to Human Preferences.](2024.04.08_SpeechAlign.md) NeurIPS2024.
[^chen2024self]: [**SPIN**: Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models.](../TextLM/2024.01.02_SPIN.md) ArXiv2024.

</td><td>

</td></tr>
<tr><td>

Another research direction to amplify the influence of conditioning inputs in generative models is **Classifier-Free Guidance (CFG)**.
**CFG** [^ho2021classifier] was originally introduced to trade-off sample fidelity and diversity without relying on a separate classifier in diffusion models.
Recently, CFG has been successfully explored in LLM-based text-generation models ([^sanchez2023stay]; [^fonseca2024can]; [^smirnov2024classifier]).
In the context of text-to-speech synthesis, CFG has been extended to improve non-autoregressive flow-matching models (CFM) (**VoiceBox** [^le2024voicebox]; **CosyVoice2** [^du2024cosyvoice]; **F5-TTS** [^chen2024f5]; **E2 TTS** [^eskimez2024e2]).
However, the applicability of CFG for enhancing LLM-based speech token prediction models is underexplored, with only a few attempts at improving textual coherence (**Parakeet** [^darefsky2024parakeet]).

[^ho2021classifier]: [**CFG**: Classifier-Free Diffusion Guidance.](../Diffusion/2022.07.26_Classifier-Free_Guidance.md) NeurIPS2021.
[^sanchez2023stay]: Stay on Topic with Classifier-Free Guidance. ArXiv2023.
[^fonseca2024can]: Can Large Language Model Summarizers Adapt to Diverse Scientific Communication Goals? ArXiv2024.
[^smirnov2024classifier]: Classifier-Free Guidance in LLMs Safety. ArXiv2024.
[^le2024voicebox]: [**VoiceBox**: Text-Guided Multilingual Universal Speech Generation at Scale.](2023.06.23_VoiceBox.md) NeurIPS2024.
[^du2024cosyvoice]: [**Cosyvoice2**: Scalable Streaming Speech Synthesis with Large Language Models.](2024.12.13_CosyVoice2.md) ArXiv2024.
[^chen2024f5]: [**F5-TTS**: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching.](../Diffusion/2024.10.09_F5-TTS.md) ArXiv2024.
[^eskimez2024e2]: [**E2 TTS**: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS.](../Diffusion/2024.06.26_E2_TTS.md) SLT2024.
[^darefsky2024parakeet]: **Parakeet**, 2024. https://jordandarefsky.com/blog/2024/parakeet/.

</td><td>

</td></tr>
<tr><td>

Building upon the above insights, we propose preference alignment and CFG techniques to enhance contextual coherence of LLM-based TTS models.
We introduce ***Koel-TTS***, a transformer-based TTS model that leverages a low-frame-rate ($21.5$ FPS) audio codec (**LFSC** [^casanova2024low]) to enable low-latency autoregressive speech generation.

</td><td>

</td></tr>
<tr><td>

To perform preference alignment, we first identify key metrics that strongly correlate with human judgments of generated speech: transcription accuracy and target speaker similarity.
Each metric captures distinct aspects of the generated output and can be evaluated using **Automatic Speech Recognition (ASR)** and **Speaker Verification (SV)** models.
We integrate these metrics into a reward system that ranks the generated outputs.
With this foundation, we then explore preference alignment algorithms, focusing on pairwise ranking methods and scalar reward optimization.
Our findings show that fine-tuning the base model with preference alignment significantly improves speaker similarity, intelligibility, and generalization to unseen speakers.
Notably, our method also enhances naturalness, despite not explicitly optimizing for this metric.

</td><td>

</td></tr>
<tr><td>

To further enhance synthesis quality with CFG, we train the ***Koel-TTS*** model with both conditional and unconditional inputs, by randomly dropping out conditional information (text and context audio) during training.
During inference, the unconditional logits are combined with conditional logits using a CFG scale, to achieve significant improvement in intelligibility, speaker similarity, and naturalness of generated speech.
Furthermore CFG can be applied independently to the base model or the preference aligned model, yielding substantial improvements across all
evaluation metrics for both.
Combining preference alignment with CFG, we train a 1.1 billion parameter multilingual ***Koel-TTS*** model that
achieves state-of-the-art zero-shot TTS results across several human and automatic evaluation metrics.

</td><td>

</td></tr>
<tr><td>

The key contributions of this work are as follows:
- We introduce ***Koel-TTS***, a multilingual encoder-decoder transformer model that maps text and context audio directly to acoustic tokens using a low-frame-rate audio codec, enabling expressive, robust, and low-latency autoregressive speech synthesis.
- We propose a novel preference alignment framework for LLM-based TTS by leveraging ASR and SV models as reward signals, significantly improving speaker similarity, intelligibility, and generalization to seen and unseen speakers.
- We adapt classifier-free guidance, dropping both text and context conditioning, to enhance LLM-based speech synthesis, demonstrating its effectiveness in improving naturalness, speaker similarity, and intelligibility.
- Our ***Koel-TTS*** model, trained with preference alignment and CFG, achieves state-of-the-art zero-shot TTS performance while reducing hallucinations and improving intelligibility.
Our model implementation is publicly available in the ***Koel-TTS*** repository.

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

Traditionally, TTS synthesis has been handled as a cascaded process involving an intermediate mel-spectrogram representation~\cite{tacotron,lancucki2021fastpitch,onealign,neekhara2021expressive,hussain2023ace}.
Recently, discrete neural audio codecs have emerged as an effective alternative, compressing raw audio into token-based representations that preserve fidelity~\cite{encodec,dac_kumar2024high,zeghidour2021soundstream,langman2024spectral,casanova2024low}.
This shift has enabled autoregressive (AR)~\cite{wang2023neural,zhang2023speak,chen2024vall,borsos2023audiolm,yanguniaudio,wang2024speechx} and non-autoregressive (NAR)~\cite{langman2024spectral} Transformer-based models, trained on discretized audio tokens, to synthesize speech with improved naturalness.

However, autoregressive token-based TTS models often struggle with hallucinations, where the generated speech deviates from intended input.
While one way to partially address this issue is biasing text and speech alignment to follow a monotonic pattern~\cite{t5tts,kim2020glow},
it is non-trivial to construct such attention priors for inputs like reference speaker audio.
This underscores the need for further refinement to better align model outputs with human expectations and preferences.

Recent research employs preference alignment algorithms, including RLHF~\cite{ouyang2022rlhf} and offline preference ranking methods~\cite{rafailov2024direct,azar2024ipo}, to refine audio LLM outputs.

For example, SpeechAlign~\cite{zhang2024speechalign}, proposes an iterative strategy to align speech language models with human preferences by addressing the distribution gap between golden AR tokens (from real speech) and synthetic AR tokens (generated during inference).
Although real speech from ground truth can be used to guide training, it introduces inconsistencies due to its fundamentally different distribution from model-generated tokens, making preference-based optimization such as DPO less effective~\cite{rafailov2024direct}.

In our work, we propose a reward mechanism based on speaker verification and speech recognition models to guide the model to generate accurate speaker-informed speech.

CFG has also emerged as a promising method to amplify the influence of conditioning inputs on the generated output, particularly in diffusion-based audio generation models~\cite{chen2024f5,liu2023audioldm}.
In our work, we show that autoregressive LLMs also benefit from CFG when trained with both conditional and unconditional inputs. Additionally, our experiments demonstrate that combining CFG with preference alignment can further improve performance over either technique in isolation.

</td><td>

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

We introduce ***Koel-TTS***, a suite of encoder-decoder transformer models that map text and context audio to acoustic speech tokens.
By incorporating preference alignment driven by transcription accuracy and speaker similarity, and Classifier Free Guidance, we improve intelligibility, speaker similarity, and naturalness of generated speech, achieving state-of-the-art zero-shot TTS performance.
***Koel-TTS*** excels in multilingual TTS, delivering high-quality, low-latency speech with a simplified model design.
Finally, through audio examples on our webpage, we demonstrate that ***Koel-TTS*** can be effectively fine-tuned for long-form multi-turn dialogue generation, by adapting the model for contextually aware podcast synthesis.

</td><td>

我们介绍了 ***Koel-TTS***, 一套基于编码器-解码器 Transformer 模型, 将文本和上下文音频映射到音频语音 Token 的模型.
通过结合由转写文本准确性和说话人相似度驱动的偏好对齐, 以及分类器无关指导, 我们提升了生成语音的可理解性, 说话人相似度, 自然度, 取得了最先进的零样本 TTS 性能.
***Koel-TTS*** 在多语言 TTS 中表现卓越, 具有简化模型设计, 实现了高质量, 低延迟语音.
最终, 通过我们的网页上的音频示例, 我们通过对模型进行上下文感知的播客合成展示了 ***Koel-TTS*** 如何有效地适用于长多轮对话生成.

</td></tr></table>
