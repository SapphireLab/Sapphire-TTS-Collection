# MARS6

<details>
<summary>基本信息</summary>

- 标题: "MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model"
- 作者:
  - 01 Matthew Baas,
  - 02 Pieter Scholtz,
  - 03 Arnav Mehta,
  - 04 Elliott Dyson,
  - 05 Akshat Prakash,
  - 06 Herman Kamper
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.05787)
  - [Publication]() ICASSP 2025
  - [Github](https://github.com/Camb-ai/mars6-turbo/)
  - [Demo](https://camb-ai.github.io/mars6-turbo/)
- 文件:
  - [ArXiv](_PDF/2501.05787v1__MARS6__A_Small_and_Robust_Hierarchical-Codec_Text-to-Speech_Model.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities.
However, they often struggle with more expressive references or complex text inputs.
We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS.
MARS6 is built on recent improvements in spoken language modelling.
Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality.
We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality.
This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger.
We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability.

## 1·Introduction: 引言

Text-to-speech (TTS) systems have improved many-fold in recent years, showcasing new capabilities in speaker cloning capability and naturalness~\cite{casanova2024xtts_interspeech, chen2024valle2neuralcodec, li2024styletts}.
One promising area in TTS is spoken language models (SLMs)~\cite{wang2023neuralcodeclanguagemodels}, where a neural audio codec converts speech into a sequence of discrete tokens.
Like text language models, SLMs are trained to predict the  next discrete token autoregressively, typically using a transformer-based architecture.
But most prior SLM-based TTS systems exhibit a key limitation -- they are unstable~\cite{hu2024robust, han2024vallerrobustefficient}.
When the reference audio or text is complex or out-of-domain, SLMs often perform poorly compared other TTS methodologies.

While there have been several methods proposed to address such limitations, they are typically considered in isolation (e.g., repetition aware sampling~\cite{chen2024valle2neuralcodec}), or they drastically increase the runtime (e.g., multiple sampling~\cite{melle_meng2024autoregressive,chen2024valle2neuralcodec}).
To this end, we propose ***MARS6*** -- a 70M parameter SLM for robust, rapid and expressive TTS.
We combine several recent techniques, and propose some new techniques from outside the TTS domain (e.g., odds ratio preference optimization~\cite{hong2024orpo} and a new top-$p$ fallback sampling mechanism).
***MARS6*** consists of an encoder-decoder transformer, and combines a hierarchical speech codec with a hierarchical decoder architecture to process speech tokens at a rate of 12~Hz.
Together with the aforementioned inference techniques, this makes ***MARS6*** a highly robust and capable TTS model.
It is also a showcase for a `bag of tricks' that we introduce for SLM-based TTS.

For our experiments, we construct a difficult in-the-wild TTS evaluation set using the expressive EARS dataset~\cite{richter2024ears}.
We compare ***MARS6*** against prior diffusion- and autoregressive-based TTS models using objective and subjective evaluations.
***MARS6*** performs competitively, even against models many times its size.
When used with voice cloning based on a snippet of reference audio, ***MARS6*** captures the target speaker identity closely, surpassing prior models in subjective speaker similarity evaluations.
Our main contribution is to demonstrate that we can combine several recently proposed techniques with new techniques proposed herein during model design, training, and inference, to stabilize outputs and yield a more robust SLM-based TTS system.

Demo, samples, code, and checkpoints: https://camb-ai.github.io/mars6-turbo/.

## 2·Related Works: 相关工作

Within SLMs, there are broadly three ways to approach speech tokenization.
The first is to tokenize speech using acoustic tokens at a fixed sample rate, as done in EnCodec and DAC~\cite{defossez2022highfi,kumar2024high}.
The second is to mix acoustic and semantic tokens using two different quantizers~\cite{baade24_interspeech}, e.g., using clustered HuBERT features for semantic and EnCodec for acoustic tokens.
The third, which we explore here, is that of hierarchical acoustic codecs, such as SNAC~\cite{Siuzdak_SNAC_Multi-Scale_Neural_2024}.
These codecs quantize speech into acoustic tokens in different codebooks, each with its own sampling rate.
This makes lower codebooks more 'coarse', and higher sample-rate codebooks 'fine'.
The progenitor SLM TTS model, VALL-E, and its successors~\cite{wang2023neuralcodeclanguagemodels,chen2024valle2neuralcodec,han2024vallerrobustefficient}, uses an autoregressive transformer to predict the most coarse acoustic codebook, and a non-autoregressive model to predict the remaining codebook values.

Despite success, VALL-E and its descendants often suffer from stability issues.
Several studies have tried to address this~\cite{song2024ellavstableneuralcodec,dang2024livespeech}, e.g., by adding linguistic and phonemic constraints to improve coherence between the output speech  and the given input text~\cite{wang2024hamttshierarchicalacousticmodeling}.
But most of these improvements require phoneme alignments during training.
The `bag-of-tricks' we introduce in this paper does not require such resources.

## 3·Methodology: 方法

\figurename~\ref{fig:1_system_diagram} shows the ***MARS6*** model, which follows an encoder-decoder architecture.
For zero-shot speaker cloning, the encoder takes in reference speaker embeddings together with the target text.
The decoder is hierarchical and made of two components: a local and global decoder, similar to the proposal of~\cite{yu2024megabyte}.
The global decoder takes input acoustic features in patches, and its output is fed into the local decoder to autoregressively predict all acoustic tokens for the next patch. Details are given next.\footnote{Mars is the Roman god of war. It is also the name of a chocolate bar first produced in 1932. ***MARS6*** was our sixth internal model version.}

### Encoder and Input Representation

The encoder is a non-causal transformer encoder using Mish activations~\cite{misra2019mish} with sinusoidal positional embeddings, similar to~\cite{vaswani2017}.
Its input sequence consists of two parts.
First, to clone the target speaker, we compute a speaker embedding using a pretrained speaker verification model and a secondary embedding using CLAP~\cite{CLAP2023}.
The former, being trained mostly on non-emotive speech, gives a good base speaker representation.
But, for expressive references where the speaker verifier's embeddings are less meaningful, the more broadly trained (but less speaker-specific) CLAP embedding is useful.
These two vectors are mapped to the dimension of the transformer using a projection layer, and then joined along the sequence length ('speaker embeddings' in Fig.~\ref{fig:1_system_diagram}).
Second is the sequence of text embeddings corresponding to the desired text being spoken  ('text embeddings' in Fig.~\ref{fig:1_system_diagram}).
To reduce the token count and improve speed, the text is tokenized using byte-pair encoding (BPE)~\cite{gage1994new_bpe}.

To improve reference coherence and output stability, we adapt a lesson from~\cite{allenzhu2024physicslanguagemodels33}.
We give the encoder a way to learn when an output should be high fidelity (e.g., 48~kHz audio from VCTK~\cite{Yamagishi2019CSTRVC} downsampled to the 24~kHz codec sampling rate) or lower fidelity (e.g., upsampled 16~kHz audiobook data).
To indicate the target quality to the encoder, we prepend the original sample rate to the text, e.g. for 16~kHz, "Mister \ldots" becomes "[16000] Mister \ldots ".

### Global Decoder

***MARS6*** operates on hierarchical acoustic tokens from the SNAC acoustic model~\cite{Siuzdak_SNAC_Multi-Scale_Neural_2024}.
SNAC encodes speech into discrete sequences using residual vector quantization with codebooks at different sampling rates, representing different levels in a hierarchy, where earlier codebooks are sampled less frequently.
For ***MARS6*** we use the 3-codebook SNAC~\cite{Siuzdak_SNAC_Multi-Scale_Neural_2024}, with codebook sample rates of 12 ($L_0$), 24 ($L_1$), and 48~Hz ($L_2$).

Like the encoder, this decoder uses Mish activations and sinusoidal positional embeddings.
The global decoder takes patches of acoustic tokens from SNAC at
12~Hz, whereby all codebook tokens generated within  $\frac{1}{12}$s are flattened and fed through a patch embedding~\cite{yu2024megabyte} to yield a 12~Hz input vector sequence as shown in \figurename~\ref{fig:1_system_diagram}.
This corresponds to a patch size of seven, since for every $\frac{1}{12}$s, there is one token from the 12~Hz $L_0$ codebook, two from the 24~Hz $L_1$ codebook, and four from the 48~Hz $L_2$ codebook.

### Local Decoder

The global decoder's output must be converted to the full hierarchical codec tokens to vocode the output speech.
Each output vector from the global decoder is fed as the first input vector to the local decoder.
As shown in \figurename~\ref{fig:1_system_diagram}, the local decoder then autoregressively predicts each codec token for all codebooks for the current patch in a flattened way, predicting $L_0$, then two $L_1$ tokens, then the last four $L_2$ codebook tokens.

The local decoder is also a causal autoregressive transformer.
But unlike the encoder and global decoder, it always operates on a fixed sequence length of seven.
So we use fixed, learnt positional embeddings instead of sinusoidal embeddings.

### Training

The model is trained end-to-end with a standard cross-entropy loss to predict the next acoustic token.
Speaker embeddings are computed from the ground truth audio during training, while during inference they are computed from a desired reference speaker.
The local decoder is applied in parallel to the global decoder outputs during training and autoregressively during inference.
During training, an end-of-sequence token is appended to the acoustic tokens of the utterance, which the local encoder is trained to predict.

### Inference and Fine-Tuning Techniques

***MARS6*** is fast and small because most of its parameters operatore on only a 12~Hz sequence in the global decoder.
The shorter sequence can also improve stability.
But on its own, this new architecture does not solve the SLM-robustness problem.
Below we introduce and incorporate a `bag of tricks' for inference and fine-tuning to improve stability and performance.

#### Fine-tuning setup

We split model training into two parts: pretraining and fine-tuning.
Pretraining involves next-token prediction, as described earlier.
We then fine-tune the model using a curated high-quality subset of the training data.

For fine-tuning, we combine odds ratio preference optimization (ORPO)~\cite{hong2024orpo} and reverse inference optimization (RIO)~\cite{hu2024robust}.
First, we compute the pretraining model predictions on arbitrary text using reference waveforms from a high quality subset of the training data.
We then feed these outputs back to ***MARS6*** as references, with the transcript of the original reference, and predict the original reference audio in a cyclic way, as in~\cite{hu2024robust}.
We then rank the cyclic outputs based on character error rate and UTMOS~\cite{saeki2022utmos}, and select the worst performing outputs as 'rejected' samples, and the corresponding ground truth audio as 'chosen' samples for ORPO.
While not precisely the same as either the original ORPO (where both chosen and rejected samples come from model predictions) or RIO (where both the best and worst-performing cyclic outputs are used), we found this setup to yield the best results in preliminary experiments.

We also found that the model had a tendency to get stuck producing the same acoustic token -- this is why prior work incorporate semantic tokens in addition to acoustic tokens~\cite{baade24_interspeech}.
To remedy this, we incorporate a flux loss to penalize repetitive generations~\cite{meng2024autoregressive}.
We adapt the flux loss used for the continuous autoregressive TTS~\cite{meng2024autoregressive} to discrete units, defining it as:

$$
    \mathcal{L}_{\text{flux}} = \frac{\beta}{\epsilon + \text{CrossEntropy}(\hat{\mathbf{y}_t}, y_{t-1})}
$$

where $\beta$ is a scaling coefficient for the loss term, $\epsilon$ is a small offset added for numerical stability, $\hat{\mathbf{y}}_t$ is the decoder logit predictions at timestep $t$, and $y_{t-1}$ is the ground truth codebook index of the \textit{prior timestep}.
Intuitively, this penalizes the probability of the token in the prior timestep.
We apply this flux loss to $L_0$ codebook predictions, during both ORPO fine-tuning and pretraining, each with different weightings.

#### Inference algorithms


We combine three inference methods.

- **Repetition Aware Sampling (RAS)**
  This approach from~\cite{chen2024valle2neuralcodec} is used on the local decoder predictions for positions corresponding to the $L_0$.
Using the notation of the original paper, we found $K=10, t_r=0.09$ to yield best results.

- **Quality Prefixing**
  As mentioned in Sec.~\ref{subsec:encoder}, in training we prepend the original sample rate of the reference to the text to give the model am indication for output quality. In inference, we always set this to ``[48000]'' to maximize output quality.

- **Top-P Backoff Sampling**
  SLM outputs can be made more stable by sampling with a low top-$p$ value.
However, sometimes this can cause the model to still get stuck in a loop.
We alleviate this by using a backoff approach similar to the temperature backoff used by Whisper~\cite{whisper_radford2022robust}.
Concretely, we sample with a top-$p$ of 0.2, and check the output length before vocoding. If the predicted audio is unrealistically short, we increment the top-$p$ by 0.2 and sample again.

#### Shallow and Deep Cloning

***MARS6*** can clone from a reference in two ways -- shallow clone and deep clone.
The prior is where we compute the speaker embeddings from the reference audio and perform inference directly.
While simple, the speaker similarity is not optimal.
The latter is similar to the approach of VALL-E, where we assume knowledge of the reference transcript, and then assign a prefix to both the encoder and global decoder as the reference transcript and acoustic tokens, respectively.
This gives better prosody and speaker transfer from the reference, at the cost of inference time (longer sequence length).

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论