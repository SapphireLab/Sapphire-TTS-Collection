# Slamming

<details>
<summary>基本信息</summary>

- 标题: "Slamming: Training a Speech Language Model on One GPU in a Day"
- 作者:
  - 01 Gallil Maimon,
  - 02 Avishai Elmakies,
  - 03 Yossi Adi
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.15814)
  - [Publication]()
  - [Github](https://github.com/slp-rl/slamkit)
  - [Demo](https://pages.cs.huji.ac.il/adiyoss-lab/slamming/)
- 文件:
  - [ArXiv](_PDF/2502.15814v1__Slamming__Training_a_Speech_Language_Model_on_One_GPU_in_a_Day.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

We introduce ***Slam***, a recipe for training high-quality Speech Language Models (SLMs) on a single academic GPU in 24 hours.
We do so through empirical analysis of model initialization and architecture, synthetic training data, preference optimization with synthetic data and tweaking all other components.
We empirically demonstrate that this training recipe also scales well with more compute getting results on par with leading SLMs in a fraction of the compute cost.
We hope these insights will make SLM training and research more accessible.
In the context of SLM scaling laws, our results far outperform predicted compute optimal performance, giving an optimistic view to SLM feasibility.
See code, data, models, samples - https://pages.cs.huji.ac.il/adiyoss-lab/slamming.

</td><td>

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

Speech Language Models (SLMs) have gained significant interest from researchers~\cite{peng2024survey, cui2024recent, ji2024wavchat, latif2023sparks}, demonstrating remarkable performance in traditional speech tasks~\cite{valle, elmakies2025unsupervisedspeechsegmentationgeneral}, diverse generative applications~\cite{yang2023uniaudio, yang2024uniaudio}, and reasoning over speech and audio signals~\cite{salmonn, qwen_audio}.

SLMs can generally be classified into two main categories: (i) generative speech LMs (which can also incorporate text) and (ii) speech-aware LMs.
The first category follows a similar pre-training approach to text-based LLMs, directly maximizing the likelihood of speech considering both input and output, typically by representing audio as a sequence of discrete tokens.
The second category consists of pre-trained text LMs adapted to process speech inputs.
In this work, we focus on the first.

Training high-quality SLMs can be highly resource intensive~\cite{twist, cuervo2024scaling, scaling_interleaving, spiritlm, defossez2024moshi}.
For example, ~\citet{spiritlm} trained their SLM on approximately $570k$ hours of speech data, while ~\citet{defossez2024moshi} utilized around $7M$ hours.
Additionally, ~\citet{cuervo2024scaling} proposed SLM scaling laws, suggesting that training high-quality SLMs requires $\sim3X$ more data compared to text-based counterparts.
These computational demands restrict the required fundamental research aimed at enhancing SLMs, such as advancements in speech tokenization, efficient acoustic modelling, etc.

In the Natural Language Processing (NLP) community, numerous studies have investigated efficient model training techniques, including masked language models such as Cramming~\citep{geiping2023cramming} and ModernBERT~\citep{warner2024modernbert}, along with next-token prediction LLMs such as MobileLLM~\citep{mobilellm}.
These methods include implementation efficiencies, architectural improvements, data selection strategies, and enhancements to the overall training pipeline.

Inspired by Cramming~\cite{geiping2023cramming} in text, we investigate compute-limited SLM training, which we term ***Slamming***.
We pose the question: \emph{Is it possible to train high-quality SLMs using a single GPU within 24 hours?} For that, we conduct an extensive empirical analysis exploring how different training components influence performance.
From this, we derive a training recipe that maximizes model performance within a fixed compute budget.
Specifically, we investigate the impact of model initialization and architecture, various optimizers and learning rate schedulers, data selection strategies - including the role of synthetic data, text-interleaving and preference optimization.

We believe that developing these training strategies and proving their feasibility will empower the speech and audio research community to advance SLMs beyond the scope of large, well-funded academic and industrial labs.
Figure~\ref{fig:teaser} illustrates the performance of various SLMs relative to their training compute budget, with circle sizes representing the size of the models.
Furthermore, we compare our results with the scaling performance predicted from \citet{cuervo2024scaling}.
Although the authors present a somewhat pessimistic view of the computational resources needed to train high-quality SLMs, we empirically show that reality is more promising, demonstrating that it is possible to significantly exceed the predicted performance per unit of compute.
We encourage the community to refine and expand scaling laws specifically tailored for SLM training across various settings.

Our main contributions are:
- We introduce \method, a training recipe for efficiently training high-quality SLMs using a single $A5000$ GPU within $24$ hours.
- We carry out extensive experiments exploring model initialization and architecture, optimization, data collection and generation, and training objectives (i.e., preference optimization and text-speech interleaving), providing insights into the impact of each component on model performance.
- Building on these insights, we scale the compute budget to two $A100$ GPUs for $48$ hours and demonstrate that our model achieves performance on par with state-of-the-art models that require substantially more compute.

We open-source all code, models, training recipes, and synthetic datasets.

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

**Efficient training.**

Enhancing the efficiency of neural network training has been extensively studied~\citep{shen2023efficient}.
\citet{hajimolahoseini2023swiftlearn, wang2024greats} examined the impact of data selection on LLM training and introduced efficient data selection methods.
\citet{muhamed2024grass} proposed using structured sparse gradients to enhance compute efficiency in LLM training, while \citet{rawat2024little} explored the potential of leveraging smaller language models to improve the training efficiency of larger LLMs.
\citet{lv2024scalable} investigated the use of low-dimensional projections for attention parameters to enhance training efficiency.
Meanwhile, \citet{neiterman2024layerdropback} proposed applying LayerDrop as a technique to optimize neural network training.

More closely related to our work, \citet{li2023flm} propose a training strategy for developing LLMs within a $100k\$$ budget.
\citet{warner2024modernbert} introduce ModernBERT, an efficient training pipeline for optimising BERT models, while \citet{izsak2021train} outline a method for training a BERT model in $24$ hours using $8$ GPUs.
The most relevant work to ours is Cramming \cite{geiping2023cramming}, where the authors conduct an in-depth analysis of masked LM training on a single GPU in one day.

While these studies offer valuable insights, they primarily focus on training text models, such as LLMs and masked LMs.
In the speech domain, similar research has been conducted on self-supervised representation models~\citep{liu2024efficient}, but not on SLMs.
In this work, we address this gap by focusing on efficient SLM training.

</td><td>

</td></tr>
<tr><td>

**Generative speech language models** were explored under various setups~\citep{gslm, kharitonov2021text}.
\citet{gslm} were the first to show how raw and uncurated speech data can be leveraged into building a GSLM system.
Next,~\citet{borsos2023audiolm} proposed a cascade version using both coarse and fine speech tokens.
Such a modelling framework opened up a new and promising research direction for processing and modelling spoken data, such as speech resynthesis~\citep{polyak2021speech}, speaking style conversion~\citep{kreuk2021textless,maimon2023dissc}, dialogue modelling~\cite{nguyen2022generative}, speech-to-speech translation~\citep{popuri2022enhanced, peng2024mslm}, etc.

\citet{nachmani2023spoken} proposed augmenting a text LM with continuous speech data to improve spoken question-answering tasks.
Recently, \citet{park2024long} proposed SLM based on state-space models~\citep{gu2021efficiently} to further push long context-efficient modelling, while ~\citet{lin2024alignslm} proposed to fine-tune SLMs using direct preference optimisation~\citep{rafailov2024dpo} obtained from text LLM rankings.

Similar to text LLMs, training SLMs often demands large-scale datasets.
For instance, Moshi~\cite{defossez2024moshi} was trained on $7$ million hours of speech data, SpiritLM~\cite{spiritlm} utilized $560k$ hours, and TWIST~\cite{twist} was trained on approximately $150k$.
Recently, \citet{cuervo2024scaling} introduced the first scaling laws for SLMs, suggesting that achieving comparable performance to text LMs requires three times more tokens.
In this work, we focus on reducing the computational demands while maintaining performance comparable to leading SLMs.

</td><td>

</td></tr></table>

## 3·Setup: 设置

<table><tr><td width="50%">

In this study, we explore decoder-only generative SLMs, which aim at maximising the likelihood of speech samples represented as discrete tokens.
We examine both purely speech-based SLMs trained on speech tokens and joint speech-text SLMs using interleaving strategies~\citep{spiritlm}.
Similarly to ~\citet{twist, gslm}, we obtain speech tokens by quantising continuous latent representations of a self-supervised speech representation model using the k-means algorithm, often known as \emph{semantic tokens}.
Specifically, we utilize a multilingual HuBERT~\citep{hubert} model running at $25$ Hz, as employed in ~\citet{twist}.
We then train SLMs by minimising the negative log-likelihood of the input segments.

Unless mentioned otherwise, all SLMs are trained using \textbf{a single $A5000$ GPU ($24GB$ VRAM)} along with $16$ CPU cores for $24$ hours.
We deliberately focus on this constrained compute budget, assuming that most academic labs can access similar resources, thereby ensuring the accessibility of our research.
The training data is pre-processed, i.e.
extracting HuBERT units and dividing data into chunks, and stored prior to model training.
As a result, this pre-processing time is excluded from the compute budget.
This approach, aligned with \citet{geiping2023cramming}, is practical since many research experiments utilize the same pre-processed data.
We additionally do not count the time for running validation and visualisations as they are not used as part of the optimisation pipeline and only used for demonstration purposes.

</td><td>

</td></tr>
<tr><td>

**Evaluation metrics.**

We assess all SLMs using four distinct evaluation metrics.
The first three are based on likelihood evaluation, while the fourth is a generative metric.
For likelihood based modelling we consider sBLIMP~\cite{dunbar2021zero}, \emph{Spoken Story Cloze} (\ssc)), and \emph{Topic Story-Cloze} (\tsc)~\cite{twist}.
For modelling-likelihood metrics, we evaluate the likelihood assigned by the SLMs to pairs of speech utterances, consisting of a positive example and a distractor.
We calculate the percent of pairs in which the SLM assigns higher likelihood to the positive sample.
sBLIMP focuses on grammatical abilities thus the negative is ungrammatical version of the positive.
\ssc and \tsc focus on semantic modelling abilities.
In \ssc, the distractor suffix is taken from the original textual StoryCloze dataset~\citep{mostafazadeh2016corpus}, allowing to assess fine-grained semantic speech understanding.
In \tsc, however, the distractor suffix is drawn from a different topic, enabling us to evaluate the model’s ability to understand the overall semantic concept.

Finally, to assess the generative abilities of SLMs, we compute \emph{generative perplexity} (GenPPL).
Following the approach of ~\cite{gslm, twist}, we provide the SLM with a short speech prompt and generate speech tokens continuation.
We use unit-vocoder with duration prediction to convert the tokens into speech~\citep{polyak2021speech, twist}.
The generated speech is then transcribed, and its PPL is evaluated using a pre-trained text LLM.
To minimize the impact of token repetition on PPL measurements, we ground the generated text using diversity metrics derived from the auto-BLEU score~\citep{gslm}.
Similarly to ~\citet{lin2024alignslm} we use bigram auto-BLEU.
In other words, we ensure that all models achieve similar auto-BLEU scores, allowing for a fair comparison of PPL.
Specifically, we transcribe speech segments using Whisper-large-v$3$-turbo model~\citep{radford2023robust} and measure PPL using Llama-$3.2$-$1$B model~\citep{grattafiori2024llama3herdmodels}.

</td><td>

</td></tr>
<tr><td>

**Software efficiency.**

To maximize performance within $24$ hours of model training, we leverage multiple efficient implementations.
Through extensive performance testing, we found that using bfloat$16$ ~\cite{kalamkar2019study} alongside FlashAttention$2$ \citep{dao2023flashattention2fasterattentionbetter} and data packing provided the most efficient compute performance in our setup.
We also experimented with model compilation using \texttt{torch.compile} \citep{ansel2024pytorch}, but it lacked native compatibility with FlashAttention$2$ at the time of our study, and its performance without FlashAttention$2$ was subpar.
Future work could investigate this further with more efficient attention implementations~\cite{FA3, li2024flexattention}.

To enable rapid and scalable experimentation, we developed a specialized library for SLM training that supports various model architectures, training objectives, and evaluation metrics.
This library accommodates both TWIST-style training, text-speech interleaving, preference optimisation, etc.
We will open-source this package along all models weights and training recipes, aiming to empower the community to further explore SLMs.

</td><td>

</td></tr></table>

## 4·Investigations: 研究

<table><tr><td width="50%">

With this setup, we systematically analyse and ablate each component of the training pipeline, ultimately refining an optimised cook-book for training \slms. We specifically examine the influence of model family, initialisation, size, and architectural choices (e.g., dropout, positional embedding, etc.). We analyse optimisation parameters and data characteristics. Lastly, we explore alternative training objectives beyond standard next-token prediction, including speech-text interleaving and direct preference optimisation using synthetic data.

</td><td>

</td></tr></table>

### Model & Optimization

<table><tr><td width="50%">

**Hyper-parameters.**

Unless specified otherwise, we use a context length of $512$ tokens and an effective batch size of $256$, employing gradient accumulation when necessary, as preliminary results indicated this configuration yields the best overall performance. We set the peak learning rate to $1e-3$ to enhance training speed and use a warmup period of $1\%$ of the total training steps, as this proved more effective than the fixed $100$-step warmup used in the original TWIST. To improve training stability, particularly with large learning rates, we apply gradient normalisation with a norm of $0.5$ at no additional cost, following~\citet{geiping2023cramming}. Unless modified later in our investigation, we use an inverse-square root scheduler and the AdamW optimiser~\citep{loshchilov2017decoupled}.

</td><td>

</td></tr>
<tr><td>

**Initialisation.**

\citet{twist} empirically demonstrated that initialising \slms with pre-trained text \ac{LM}s can enhance convergence speed and improve model performance. We examine the effect of this initialisation within our setup across different model types. To do so, we train multiple models, both with and without TWIST initialisation, while staying within our compute budget. As shown in Figure~\ref{fig:initialisation}, TWIST initialisation benefits all evaluated models at the beginning of training, though its overall impact by the end varies. Notice, the x-axis in Figure~\ref{fig:initialisation} represents theoretical FLOPs, calculated as $6 * N_{params} * D_{tokens}$ following~\citet{hoffmann2022training}. However, due to variations in model architecture and implementation, practical efficiency differs, leading to varying amounts of compute processed within $24$ hours.

Results suggest that benefits of TWIST initialisation can be substantial, especially for top-performing models like Qwen$2.5$. As a result, we prioritise investigations based on existing pre-trained text \ac{LM}s. Interestingly, the results in Figure~\ref{fig:initialisation} demonstrate that Qwen$2.5$ outperforms other models even without TWIST initialisation, perhaps suggesting that their architectural design choices or size might also provide some benefit.

</td><td>

</td></tr>
<tr><td>

**Optimal model size \& family.**

\citet{cuervo2024scaling} conducted a scaling analysis on GSLM-style \slms, estimating the optimal model size and token count for a compute-efficient model. However, using a text LM initialisation might impact these findings. As we observe, TWIST initialisation greatly impact model performance, suggesting that prioritising larger models may be more effective than simply increasing the dataset size. Additionally, various model families gain different advantages from TWIST initialisation; for example, Qwen$2.5$ models show significantly better performance compared to OPT models. In Figure~\ref{fig:model_choice}, we compare the results under the pre-defined compute budget within model families\footnote{We use the text LM original names for clarity, but note that the actual size will be notably smaller due to reduced vocabulary size, e.g Qwen$2.5$-$0.5$B has $358$M parameters. Full model sizes can be found in Appendix \ref{sec:model_sizes}.}. We note that the best model sizes for both MobileLLM \cite{mobilellm}, SmolLM$2$ \cite{smollm2} and Pythia \cite{pythia} are $\sim300M$ parameters, while for OPT the best is $125$M. According to \citet{cuervo2024scaling}, the estimated optimal model size is approximately $66$M parameters. However, the best-performing model, Qwen$2.5$, is significantly larger. Since there are no smaller models in this family, it is difficult to determine whether this deviation is due to the quality of the initialisation or other factors. Moving forward, we proceed with both OPT-$125$M and Qwen$2.5$-$0.5$B.

</td><td>

</td></tr>
<tr><td>

**Dropout.**

The original OPT models includes dropout to mitigate overfitting. Although dropout is beneficial for regularisation, it effectively decreases the number of gradient updates per parameter without shortening the update-step wall time. Hence, reduces the number of parameter updates per second. Following \citet{geiping2023cramming}, we experiment with removing dropout and observed improved performance in our setup.

</td><td>

</td></tr>
<tr><td>

**Positional Encoding.**

Transformers rely on positional encoding to capture the order of input tokens. Many modern LMs, including the Qwen models, use Rotary Position Embedding~\citep{su2023roformerenhancedtransformerrotary}. This method uses a hyperparameter, $\theta$, to control the trade-off between granularity and the ability to handle long contexts. $\theta$ is often tuned to accommodate longer context lengths \citep{qwen2, roziere2023code}. Since our context length is significantly shorter than that of the original LLM, we explore reducing $\theta$ for potential performance gains. Our findings show that setting $\theta=10,000$ with a context length of $1024$ enhances performance, so we adopt this configuration moving forward. We note that since we increase the context length, we need to reduce the batch size as well, to not run into memory problems when training. We reduce the batch size by a half and keep the same amount of gradient accumulation steps, which gives us an effective batch size of $128$.

</td><td>

</td></tr>
<tr><td>

**Optimiser and Scheduler.**

Various optimisers and schedulers have been developed to enhance training efficiency, reduce memory usage \citep{shazeer2018adafactoradaptivelearningrates, dettmers20228bitoptimizersblockwisequantization}, or accelerate convergence \citep{pagliardini2024ademamix, chen2023lion}. With limited compute, faster convergence and better memory efficiency can be especially important. We first consider efficient optimisers, specifically AdamW with fused kernels, and $8$-bit AdamW, but observe no notable improvements in batch size or runtime compared to standard AdamW. This could do with the relatively small model size, resulting in a minimal memory footprint of the optimisers. We then compare AdamW with two state-of-the-art optimisers: AdaLomo \cite{lv2023adalomo} and AdEMAMeix \citep{pagliardini2024ademamix}. Results, presented in Figure~\ref{fig:optim}, suggest that with the original InverseSqrt scheduler used by \citet{twist}, using AdEMAMeix improves validation loss, compared to AdamW, with AdaLomo far behind.

Next, we analyse a cosine decay learning rate scheduler, in place of the original InverseSqrt as this was shown to improve convergence \cite{loshchilov2016sgdr}. We consider the previous optimisers, and provide the validation loss throughout training in Figure~\ref{fig:optim}. We see that this notably improved the loss for AdamW, and slightly harmed results for AdEMAMeix. Overall, AdamW with a cosine schedule provide the best setup, far outperforming the original setup.

</td><td>

</td></tr></table>

## 5·Final Recipe: 最终配方

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 6·Increasing Compute: 增加计算

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 7·Conclusions: 结论

<table><tr><td width="50%">

</td><td>

</td></tr></table>
