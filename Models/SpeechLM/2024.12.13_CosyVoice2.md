# CosyVoice2

<details>
<summary>基本信息</summary>

- 标题: "CosyVoice2: Scalable Streaming Speech Synthesis with Large Language Models"
- 作者:
  - 01 Zhihao Du
  - 02 Yuxuan Wang
  - 03 Qian Chen
  - 04 Xian Shi
  - 05 Xiang Lv
  - 06 Tianyu Zhao
  - 07 Zhifu Gao
  - 08 Yexin Yang
  - 09 Changfeng Gao
  - 10 Hui Wang
  - 11 Fan Yu
  - 12 Huadai Liu
  - 13 Zhengyan Sheng
  - 14 Yue Gu
  - 15 Chong Deng
  - 16 Wen Wang
  - 17 Shiliang Zhang
  - 18 Zhijie Yan
  - 19 Jingren Zhou
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.10117)
  - [Publication]
  - [Github](https://github.com/funaudiollm/cosyvoice)
  - [Demo](https://funaudiollm.github.io/cosyvoice2)
- 文件:
  - [ArXiv](_PDF/2412.10117v3__CosyVoice2__Scalable_Streaming_Speech_Synthesis_with_Large_Language_Models.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens.
By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning.
Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience.
Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice2, which incorporates comprehensive and systematic optimizations.
Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens.
For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone.
In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model.
By training on a large-scale multilingual dataset, CosyVoice2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode.
We invite readers to listen to the demos at [this https URL](https://funaudiollm.github.io/cosyvoice2).

</details>
<br>

在我们先前的工作中, 我们提出了 CosyVoice, 一种基于监督式离散语音 Token 的多语言语音合成模型.
通过采用两种流行的生成式模型 (语言模型, 流匹配) 实现的渐进式语义解码, CosyVoice 在语音上下文学习中展现出了高韵律自然度, 内容一致性和说话人相似度.

最近, 多模态大语言模型出现了显著进展, 语音合成的响应延迟和实时因子成为了交互体验的关键.
因此, 在本报告中, 我们展示了一个改进的流式语音合成模型, CosyVoice2, 继承了全面且系统的优化.

具体来说, 我们引入有限标量量化 (Finite-Scalar Quantization, FSQ) 来提升语音 Token 的码本利用率.
对于文本-语音语言模型, 我们将模型架构简化, 允许直接使用预训练的大语言模型作为骨干.
此外, 我们开发了一个基于分块感知的因果流匹配模型, 以支持各种合成场景, 使得单个模型能够支持流式和非流式合成.

通过在大规模多语言数据集上训练, CosyVoice2 在流式模式下实现了人类齐平的自然度, 最小的响应延迟, 几乎无损的合成质量.

## 1·Introduction: 引言

<details>
<summary>展开原文</summary>

In recent years, neural text-to-speech (TTS) synthesis models have garnered significant attention for surpassing traditional concatenative and statistical parametric methods \cite{DBLP:conf/interspeech/WangSSWWJYXCBLA17, DBLP:conf/icassp/ShenPWSJYCZWRSA18, DBLP:journals/corr/abs-1710-07654, DBLP:conf/iclr/PingPC19, DBLP:conf/nips/RenRTQZZL19, DBLP:conf/aaai/Li0LZL19, DBLP:conf/iclr/0006H0QZZL21}.
These models have achieved high fidelity and naturalness on pre-defined specific speakers.
Recent studies show that zero-shot TTS models are able to synthesize speech for any speaker by imitating the timbre, prosody and style of a reference speech \cite{DBLP:journals/corr/abs-2301-02111}.
Beyond their in-context learning (ICL) capability, zero-shot TTS models benefit from large-scale training data, achieving synthesis quality and naturalness nearly indistinguishable from human speech.

Recent zero-shot TTS models can be broadly divided into three categories: codec language models, feature diffusion models and their hybrid systems.
Codec language models utilize a speech codec model to extract discrete speech representation \cite{DBLP:journals/taslp/ZeghidourLOST22,DBLP:journals/tmlr/DefossezCSA23,DBLP:conf/icassp/DuZHZ24} and employ an autoregressive \cite{DBLP:journals/corr/abs-2301-02111,DBLP:journals/tacl/KharitonovVBMGP23,DBLP:journals/corr/abs-2401-07333,DBLP:journals/corr/abs-2401-14321,DBLP:journals/corr/abs-2404-03204,DBLP:journals/corr/abs-2406-05370,DBLP:journals/corr/abs-2406-07855} or masked \cite{DBLP:journals/corr/abs-2409-00750} language model to predict the speech tokens, which are then synthesized to waveforms via codec vocoders \cite{DBLP:conf/asru/OkamotoYOTK23,DBLP:conf/iclr/Siuzdak24}.
Continuous speech representations are also explored in \cite{DBLP:journals/corr/abs-2407-08551}.
Language model-based TTS can generate varied and prosody-consistent speech via autoregressive sampling.

Inspired by advances in image generation, denoising diffusion \cite{DBLP:conf/nips/HoJA20,DBLP:conf/iclr/0011SKKEP21} and flow matching models \cite{DBLP:conf/iclr/LipmanCBNL23} have been introduced into non-autoregressive (NAR) speech synthesis.
Early diffusion-based TTS models required duration prediction for each text (phone) to address the length disparity between text and speech features \cite{DBLP:conf/nips/LeVSKSMWMAMH23,DBLP:conf/icml/JuWS0XYLLST000024,DBLP:conf/icassp/GuoDM0024,DBLP:conf/icassp/MehtaTBSH24}.
However, this rigid alignment can affect naturalness, resulting in flat prosody.
To mitigate this issue, cross-attention and Diffusion Transformers (DiT) have been introduced into NAR TTS models \cite{DBLP:conf/asru/GaoMZC23,DBLP:journals/corr/abs-2406-11427}.
Recent research indicates simpler approaches for text-speech alignment in NAR TTS models, such as E2 TTS \cite{DBLP:journals/corr/abs-2406-18009}, F5-TTS \cite{DBLP:journals/corr/abs-2410-06885} and Seed-TTS \cite{DBLP:journals/corr/abs-2406-02430}.
In these models, input text is padded with special tokens to match the total speech length which is either automatically predicted by the utterance duration prediction module or specified by the user in advance.
Since NAR TTS models are not constrained by codec vocoders, they can achieve superior speech quality.

Hybrid systems combine the text-to-codec language model and codec-to-feature diffusion model \cite{DBLP:journals/corr/abs-2406-02430,cosyvoice,DBLP:journals/corr/abs-2409-03283}.
The language model addresses the alignment between text and speech as well as the utterance duration prediction, while the codec-to-feature diffusion model synthesizes speech features (Mel spectrum) based on the generated codec and other conditions.
By leveraging the strengths of both generative models, hybrid systems achieve high diversity, prosody consistency and speech quality.

Despite the success of recent zero-shot TTS models, they generally operate in non-streaming (offline) mode, which involves complete input text and requires synthesizing the entire utterance before returning the waveform.
This results in high latency, negatively impacting user experience in applications like voice chat \cite{hurst2024gpt,DBLP:conf/emnlp/ZhangLZZWZQ23}.
To address this issue, streaming synthesis has been explored for language model-based zero-shot TTS models \cite{DBLP:journals/corr/abs-2406-02897,DBLP:journals/corr/abs-2410-00767,DBLP:journals/corr/abs-2402-08093,DBLP:conf/icassp/DekelSFHKH24}, but diffusion-based TTS models and hybrid systems lack well-established streaming solutions.

Building on the success of CosyVoice \cite{cosyvoice}, we introduce CosyVoice2, a streaming zero-shot TTS model with improved prosody naturalness, content consistency, and speaker similarity.
Our contributions include:

- Unifying streaming and non-streaming synthesis in a single framework and proposing the unified text-speech language model and chunk-aware causal flow matching model, leading to lossless streaming synthesis compared to offline mode.
- Simplifying the LM architecture by removing the text encoder and speaker embedding, allowing pre-trained textual large language models (LLMs) to serve as the backbone, enhancing context understanding.
- Replacing vector quantization (VQ) in the speech tokenizer with finite scalar quantization (FSQ), improving codebook utilization and capturing more speech information.
- Upgrading the instructed TTS capacity to support more instructions, including emotion, accent, role style, and fine-grained control.
In CosyVoice2, the instruction and zero-shot capacity are integrated into a single model, enabling more versatile and vivid synthesis.

Through the above systemic modification and optimization, CosyVoice2 achieves human-parity synthesis quality and is nearly lossless in streaming mode.
The unified framework loosens deployment requirements, enabling a single model to support both streaming and non-streaming synthesis.
The upgraded instructed TTS capacity provides a more powerful and  easier approach for users to generate various speeches.
In addition, the chunk-aware flow matching design can also be applied to NAR TTS models, which suggests the potential for streaming NAR models.

</details>
<br>

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论