# Ichigo

<details>
<summary>基本信息</summary>

- 标题: "Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant"
- 作者:
  - 01 Alan Dao (Gia Tuan Dao),
  - 02 Dinh Bach Vu,
  - 03 Huy Hoang Ha
- 链接:
  - [ArXiv](https://arxiv.org/abs/2410.15316)
  - [Publication]
  - [Github](https://github.com/homebrewltd/ichigo)
  - [Demo](https://demo.homebrew.ltd/)
- 文件:
  - [ArXiv](_PDF/2410.15316v1__Ichigo__Mixed-Modal_Early-Fusion_Realtime_Voice_Assistant.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Large Language Models (LLMs) have revolutionized natural language processing, but their application to speech-based tasks remains challenging due to the complexities of integrating audio and text modalities.
This paper introduces ***Ichigo***, a mixed-modal model that seamlessly processes interleaved sequences of speech and text.
Utilizing a tokenized early-fusion approach, ***Ichigo*** quantizes speech into discrete tokens and employs a uniform transformer-based architecture for both speech and text modalities.
This method enables joint reasoning and generation across modalities without the need for separate adapters.
We present a comprehensive training methodology, including pre-training on multilingual speech recognition datasets and fine-tuning on a curated instruction dataset.
***Ichigo*** demonstrates state-of-the-art performance on speech question-answering benchmarks, outperforming existing open-source speech language models and achieving comparable results to cascaded systems.
Notably, ***Ichigo*** exhibits a latency of just 111 ms to first token generation, significantly lower than current models.
Our approach not only advances the field of multimodal AI but also provides a framework for smaller research teams to contribute effectively to open-source speech-language models.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论