# HALL-E

<details>
<summary>基本信息</summary>

- 标题: "HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis"
- 作者:
  - 01 Yuto Nishimura,
  - 02 Takumi Hirose,
  - 03 Masanari Ohi,
  - 04 Hideki Nakayama,
  - 05 Nakamasa Inoue
- 链接:
  - [ArXiv](https://arxiv.org/abs/2410.04380)
  - [Publication]
  - [Github]
  - [Demo](https://yutonishimura-v2.github.io/HALL-E_DEMO/)
- 文件:
  - [ArXiv](_PDF/2410.04380v1__HALL-E__Hierarchical_Neural_Codec_Language_Model_for_Minute-Long_Zero-Shot_Text-to-Speech_Synthesis.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Recently, Text-to-speech (TTS) models based on large language models (LLMs) that translate natural language text into sequences of discrete audio tokens have gained great research attention, with advances in neural audio codec (NAC) models using residual vector quantization (RVQ).
However, long-form speech synthesis remains a significant challenge due to the high frame rate, which increases the length of audio tokens and makes it difficult for autoregressive language models to generate audio tokens for even a minute of speech.
To address this challenge, this paper introduces two novel post-training approaches: 1) Multi-Resolution Requantization (MReQ) and 2) HALL-E.
MReQ is a framework to reduce the frame rate of pre-trained NAC models.
Specifically, it incorporates multi-resolution residual vector quantization (MRVQ) module that hierarchically reorganizes discrete audio tokens through teacher-student distillation.
HALL-E is an LLM-based TTS model designed to predict hierarchical tokens of MReQ.
Specifically, it incorporates the technique of using MRVQ sub-modules and continues training from a pre-trained LLM-based TTS model.
Furthermore, to promote TTS research, we create MinutesSpeech, a new benchmark dataset consisting of 40k hours of filtered speech data for training and evaluating speech synthesis ranging from 3s up to 180s.
In experiments, we demonstrated the effectiveness of our approaches by applying our post-training framework to VALL-E.
We achieved the frame rate down to as low as 8 Hz, enabling the stable minitue-long speech synthesis in a single inference step.
Audio samples, dataset, codes and pre-trained models are available at [this https URL](https://yutonishimura-v2.github.io/HALL-E_DEMO/).

</details>
<br>

近期, 随着使用残差向量量化的神经音频编解码器模型的进步, 将自然语言文本转换为离散音频标记序列的基于大语言模型的文本转语音模型引起了广泛的研究关注.
然而, 长语音合成仍然是一个重要的挑战, 因为高帧率会增加音频标记的长度, 使得自回归语言模型甚至很难为一分钟的语音生成音频标记.
为了应对这一挑战, 本文引入了两种新颖的后训练方法: 1. 多分辨率重量化 (MReQ) 和 2. HALL-E.

- MReQ 是用于减少预先训练的神经音频编解码器模型帧率的框架.
具体来说, 它采用多分辨率残差向量量化 (MRVQ) 模块, 通过教师-学生蒸馏的方式重新组织离散音频标记.
- HALL-E 是基于 LLM 的 TTS 模型, 旨在预测 MReQ 的层次化标记.
具体来说, 它采用 MRVQ 子模块, 并从预训练的 LLM-based TTS 模型继续训练.

为了促进 TTS 研究, 我们创建了 MinutesSpeech 新的数据集, 其中包含 40k 小时的过滤语音数据, 用于训练和评估语音合成, 范围从 3s 到 180s.
在实验中, 我们展示了我们的方法的有效性, 通过将我们的后训练框架应用于 VALL-E.
我们实现了帧率低至 8 Hz 的稳定分钟长语音合成, 使得单次推断可以实现分钟长度的语音合成.
音频样本、数据集、代码和预训练模型可在 [此 https URL](https://yutonishimura-v2.github.io/HALL-E_DEMO/) 获得.

## 1.Introduction: 引言

Recent advances in large language models (LLMs) have enabled us to model complex linguistic structures and patterns with unprecedented precision in natural language processing tasks~\citep{brown2020language, chowdhery2023palm, zeng2023glm130b}.
Motivated by these developments, LLM-based text-to-speech (TTS) models have gained significant research interest for their ability to model complex speech structures and patterns, enabling the synthesis of more natural-sounding speech in a zero-shot manner~\citep{wang2023valle, zhang2023vallex, song2024ella, han2024valler, chen2024valle2, xin2024ralle, meng2024melle}.
The core concept behind LLM-based TTS models is to translate natural language text into a sequence of audio tokens, typically using frozen natural audio codec (NAC) models that quantize audio signals into discrete tokens via vector quantization techniques~\citep{zeghidour2021soundstream, defossez2022encodec, kumar2024DAC, Zhang2024SpeechTokenizer, wu2023audiodec, huang2023repcodec, du2024funcodec, yang2023hificodec}.

Since LLMs are becoming capable of capturing long context in text~\citep{guo2022longt5, chen2024longlora, han2024hyperattention}, LLM-based TTS models are also expected to handle long context to synthesize speech over extended periods.
However, this presents significant challenges, mainly because the length of audio tokens per second is typically large.
More specifically, when using an autoregressive language model to predict audio tokens, as in the VALL-E architecture~\citep{wang2023valle}, the high frame rate at the first layer of residual vector quantization (RVQ)~\citep{zeghidour2021soundstream} in NAC models often becomes a major factor that hinders the synthesis of long speech.
We refer to the number of audio tokens per second as the frame rate (Hz).
As recently discussed and investigated by~\cite{han2024valler, chen2024valle2}, reducing the frame rate is essential, but not straightforward.
Simply reducing the frame rate in NAC models results in degraded audio quality and incorrect phoneme pronunciation.
Addressing this issue requires novel solutions that bridge NAC models and LLM-based TTS models.

In this paper, we propose a novel approach to tackle the challenge of minitue-long speech synthesis in LLM-based TTS models by introducing a hierarchical post-training framework that effectively manages the trade-off between reducing frame rate and producing high-quality speech.
Our contributions are summarized as follows:
1) We propose \textbf{Multi-Resolution Requantization (MReQ)}, a post-training framework for hierarchically reorganizing pre-trained RVQ module to reduce the frame rate at lower quantization layers.
MReQ incorporates a multi-resolution residual vector quantization (MRVQ) module into a pre-trained NAC model and continues training in a teacher-student distillation manner.
This results in reducing the frame rate at the first quantization layer to 8 Hz.
2) We propose \textbf{HALL-E}, a hierarchical LLM-based TTS model designed to predict hierarchical tokens of MReQ.
{The AR model is trained using 8Hz tokens, while the NAR model is trained by using sub-modules in MRVQ, and continues training from a pre-trained LLM-based TTS model.}
3) We introduce \textbf{MinutesSpeech}, a new benchmark dataset to promote TTS research, particularly for minute-long speech synthesis.
The training set consists of 40k hours of automatically filtered and balanced speech data.
The test set consists of 8 hours of speech data with transcriptions created by professional transcribers.
4) We thoroughly conducted experiments to provide best practices for managing the trade-off between reducing frame rate and producing high-quality speech, while demonstrating the effectiveness and efficiency of our approach.
We open-source dataset, codes and pre-trained models along with audio samples at [Demo Page](https://yutonishimura-v2.github.io/HALL-E_DEMO/).

## 2.Related Works: 相关工作

### Neural Audio Codec Models

NAC models produce discrete audio tokens by quantizing audio signals.
SoundStream~\citep{zeghidour2021soundstream} and Encodec~\citep{defossez2022encodec} are pioneering NAC models, which significantly improved compression efficiency over traditional audio codecs.
Recent studies have proposed NAC models with a focus on maintaining performance in speech processing tasks.
Examples include SpeechTokenizer~\citep{Zhang2024SpeechTokenizer}, Descript Audio Code~\citep{kumar2024DAC}, AcademiCodec~\citep{yang2023hificodec}, AudioDec~\citep{wu2023audiodec}, RepCodec~\citep{huang2023repcodec}, and FunCodec~\citep{du2024funcodec}.
Many studies have focused on reducing bps, while few have explored lowering the frame rate.
\cite{defossezmoshi} achieved the lowest frame rate for a NAC model, reaching 12.5Hz by incorporating Transformer models and SpeechTokenizer.
We report achieving an even lower frame rate of 8Hz, which is about 1.5 times shorter than it.

### Zero-Shot TTS

Zero-Shot TTS aims to synthesize speech from text in the voice of a target speaker using only a short reference audio segment from that speaker.
Early models relied on speaker embeddings or speaker adaptation~\citep{casanova22yourtts, arik2018neuralvoicecloning, chen2019sample},
while recent studies have focused on LLM-based models that use NAC models in conjunction with LLMs.
%~\citep{wang2023valle, zhang2023vallex, song2024ella, xin2024rall, chen2024valle2, meng2024melle}.
VALL-E~\citep{wang2023valle} was the first LLM-based model, demonstrating impressive capabilities in zero-shot TTS tasks.
Follow-up studies have explored various extensions such as VALL-E~X for cross-lingual TTS~\citep{zhang2023vallex}, ELLA-V using Montreal forced aligner~\citep{song2024ella},
RALL-E using prosody features~\citep{xin2024ralle}, VALL-E R using monotonic alignment~\citep{han2024valler}, VALL-E 2 using grouped code modeling~\citep{chen2024valle2}, and MELLE using mel-spectrogram features~\citep{meng2024melle}.
Prosody information can also be modeled by LLMs
latent language mode
Mega-TTS~\citep{jiang2023mega} and Mega-TTS 2~\citep{jiang2024megatts2} introduced prosody LLMs to generate more natural prosody.
Meanwhile, diffusion-based models ({\it e.g.,} NaturalSpeech2/3~\citep{shen2023naturalspeech2, ju2024naturalspeech3} and Voicebox~\citep{le2024voicebox}) and prompt-based models ({\it e.g.,} Prompt-TTS2~\citep{guo2023prompttts, leng2024prompttts2}) are also known to be effective to generate high-quality controllable speech.
Beyond speech synthesis, several studies proposed audio generation models such as
UniAudio~\citep{yang2023uniaudio},
Audiobox~\citep{vyas2023audiobox}.
In contrast, this work explores post-training methods to reduce the frame rate of LLM-based models, aiming at minute-long speech synthesis given a pre-trained NAC model such as Encodec.

### Preliminaries

#### Neural Audio Codec

A NAC model typically consists of three components: an encoder $\mathrm{Enc}(\cdot)$, a vector quantizer $\mathrm{VQ}(\cdot)$, and a decoder $\mathrm{Dec}(\cdot)$.
The quantizer is the core component that produces discrete audio tokens.
This work assumes that an RVQ module~\citep{zeghidour2021soundstream} is used as the quantizer, which is defined as follows:

$$
\bm{z}_{l} = \mathrm{VQ}_{l} (\bm{x}_{l - 1}),\quad \bm{x}_{l+1} = \bm{x}_{l} - \tilde{\bm{z}}_{l},\quad \bm{h} = \sum_{l=1}^{L} \tilde{\bm{z}}_{l},
$$

where $\bm{x}_{0} = \mathrm{Enc}(\bm{x}_{\mathrm{in}}) \in \mathbb{R}^{d \times n}$ is the encoder output for the input audio $\bm{x}_{\mathrm{in}}$,
$d$ is the latent dimension,
$n$ is the sequence length,
$\mathrm{VQ}_{l}$ is a vector quantizer,
$\bm{z}_{l} \in \mathbb{N}^{n}$ is a discrete token sequence,
$\tilde{\bm{z}}_{l} = \mathrm{Emb}_{l}(\bm{z}_{l}) \in \mathbb{R}^{d \times n}$ is a sequence of embeddings corresponding to $\bm{z}_{l}$ obtained through a learnable embedding layer $\mathrm{Emb}_{l}(\cdot)$\footnote{In this paper, the tilde symbol (~$\tilde{}$~) denotes the embeddings corresponding to a token sequence.},
$l \in \{1, 2, \cdots, L\}$ is the layer index, and $L$ is the number of layers.
The output $\bm{h} \in \mathbb{R}^{d \times n}$ is then fed into the decoder to reconstruct the input audio as $\bm{y} = \mathrm{Dec}(\bm{h})$.

#### LLM-based TTS.

An LLM-based TTS typically consists of two decoder-only language models: an autoregressive (AR) model $T_{\mathrm{ar}}$ and a non-autoregressive (NAR) model $T_{\mathrm{nar}}$ \citep{wang2023valle}.
The speech synthesis procedure is given by the following equations:

$$
\hat{\bm{z}}_{1} = T_{\mathrm{ar}}(\bm{t}, \bm{z}_{1}^{prompt}),\quad \hat{\bm{z}}_{l+1} = T_{\mathrm{nar}} (\bm{t}, \bm{h}^{prompt}_{L}, \hat{\bm{h}}_{l}, l),\\
\hat{\bm{h}}_{l} = \sum_{l^{\prime}=1}^{l} \hat{\tilde{\bm{z}}}_{l^{\prime}},\quad \hat{\bm{y}} = \mathrm{Dec} ([\bm{h}^{prompt}_{L}, \hat{\bm{h}}_{L}]),
$$

where $[\bm{h}^{prompt}_{L}, \hat{\bm{h}}_{L}]$ denotes the concatenation of these two matrices along the time axis.
In Eq.~(\ref{eq:valle1}), $T_{\mathrm{ar}}$ generates an audio token sequence $\hat{\bm{z}}_{1} \in \mathbb{N}^{n^{\prime}}$ corresponding to the first layer of RVQ given two inputs: a text prompt $\bm{t}$ and {an audio prompt} $\bm{z}^{prompt}_{1} = \mathrm{VQ}_{1} (\mathrm{Enc}(\bm{x}_{prompt}))$ {extracted from an audio input} $\bm{x}_{prompt}$.
In Eq.~(\ref{eq:valle2}), $T_{\mathrm{nar}}$ iteratively generates token sequences $\hat{\bm{z}}_{l+1} \in \mathbb{N}^{n^{\prime}}$ from the accumulated hidden features $\hat{\bm{h}}_{l}$ in Eq.~(\ref{eq:valle3}) and the audio prompt's hidden features $\bm{h}^{prompt}_{L}$.
Finally, in Eq.~(\ref{eq:valle4}), speech $\hat{\bm{y}}$ is generated.
Note that $\mathrm{Enc}, \mathrm{VQ}_{1}$, and $\mathrm{Dec}$ are from a frozen NAC model.

#### Preliminary Experiments

LLM-based TTS models have a predefined context window size and are typically trained with speech data ranging from several seconds to several tens of seconds.
To generate long speech segments, a straightforward approach is to reduce the frame rate in the NAC model.
However, reducing the frame rate below 48 Hz significantly
decreases speech reconstruction performance as shown in
Figure~\ref{fig:preliminary}, where we evaluated the performance of Encodec~\citep{defossez2022encodec} in terms of the Perceptual Evaluation of Speech Quality (PESQ) scores and word error rates (WERs) as functions of frame rates.
Specifically, it is confirmed that training becomes entirely difficult at 8Hz.
Therefore, in this study, we propose a NAC model that works even at an 8Hz, demonstrating a significant improvement over existing limitations.

## 3.Methodology: 方法

### 3.1.MReQ: Multi-Resolution Requantization

This section introduces MReQ, a post-training framework for hierarchically reorganizing a pre-trained RVQ module to reduce the frame rate.
Specifically, MReQ incorporates a multi-resolution residual vector quantization (MRVQ) module to a pre-trained NAC model as shown in Figure~\hyperref[fig:mreq]{\ref*{fig:mreq}}, and continues training the NAC model in a teacher-student distillation manner.
For a pre-trained 48Hz Encodec model, MReQ reduces the frame rate at the first quantization layer to 8 Hz.
This enables LLM-based TTS models to handle longer contexts.

#### Architecture

The MRVQ module is a nested structure of RVQ.
Specifically, it consists of a residual structure composed of multiple low frame-rate residual vector quantization (LRVQ) blocks, each of which is itself a residual structure operating at a different frame rate.
The definition is given as follows.

**Definition 1 (MRVQ module).**
Let $\bm{x}_{0} \in \mathbb{R}^{d \times n_{0}}$ be an encoder output, where $d$ is the latent dimension and $n_{0} = T s_{0}$ is the sequence length depending on the time length $T$ (sec) and the frame rate $s_{0}$ (Hz).
The MRVQ module is defined as follows:

$$
\begin{aligned}
  \bm{c}_{k} = \mathrm{LRVQ}^{(k)}_{\alpha-\beta-\gamma} (\bm{x}_{k - 1}),\\
  \bm{x}_{k+1} = \bm{x}_{k} - \tilde{\bm{c}}_{k},\\
  \bm{h} = \sum_{k=1}^{K} \tilde{\bm{c}}_{k},
\end{aligned}
$$

where $\mathrm{LRVQ}^{(k)}_{\alpha-\beta-\gamma}$ is an LRVQ block, $K$ is the number of blocks, $\alpha-\beta-\gamma$ is a triplet of hyperparameters to determine the block structure.

**Definition 2 (LRVQ block).**

Each LRVQ block $\mathrm{LRVQ}^{(k)}_{\alpha-\beta-\gamma}$ consists of five components:
a pre-quantizer $\mathrm{PreQ}^{(k)}_{\alpha}$,
a sub-encoder $E_{k}$ for down sampling,
a main quantizer $\mathrm{Quant}^{(k)}_{\beta}$,
a sub-decoder $D_{k}$ for upsampling, and a post-quantizer $\mathrm{PostQ}^{(k)}_{\gamma}$.

The quantization procedure is given by

$$
\begin{aligned}
  \bm{a}_{k} = \mathrm{PreQ}^{(k)}_{\alpha} (\bm{x}_{k-1}),\\
  \bm{b}_{k} = \mathrm{Quant}^{(k)}_{\beta}(E_{k} ( \tilde{\bm{a}}_{k} )),
  \bm{c}_{k} = \mathrm{PostQ}^{(k)}_{\gamma} (D_{k} (\tilde{\bm{b}}_{k})),
\end{aligned}
$$

where $\bm{a}_{k}, \bm{b}_{k}, \bm{c}_{k}$ are token sequences.
The three quantizers $\mathrm{PreQ}^{(k)}_{\alpha}, \mathrm{Quant}^{(k)}_{\beta}$ and $\mathrm{PostQ}^{(k)}_{\gamma}$ are implemented using RVQ with $\alpha$, $\beta$, and $\gamma$ layers, respectively.
Note that $\bm{b}_{k} \in \mathbb{N}^{\beta \times n_{k}}$ is the token sequence representing audio in a low frame rate.
Its length is given by $n_{k} = T s_{k}$, where $s_{k}$ is the frame rate satisfying $s_{1} < s_{2} < \cdots < s_{K}$ and $s_{K} = s_{0}$.
The other two sequences $\bm{a}_{k}$ and $\bm{c}_{k}$ are used only for facilitating training of NAR  models used in LLM-based TTS models.

**Implementation details.**

Figure~\hyperref[fig:mreq]{\ref*{fig:mreq}a} shows the MRVQ module applied to the Encodec model, where the frame rate is reduced from $s_{0} = 48$ Hz to $s_{1} = 8$ Hz using $4$ LRVQ blocks.
Figure~\hyperref[fig:mreq]{\ref*{fig:mreq}b} shows the LRVQ block.
Each sub-encoder $E_{k}$ consists of a convolution layer followed by two bi-LSTM layers, which reduces the frame rate from $s_{0}$ to $s_{k}$.
Each sub-decoder $D_{k}$ consists of two bi-LSTM layers followed by a transposed convolution layer, which is symmetric to $E_{k}$.
Table~\ref{tab:mrvq_arc} lists frame rates $s_{k}$, hyperparameter triplets $\alpha-\beta-\gamma$, and strides for the convolution and transposed convolution layers.
For $k=4$, only the pre-quantize is used,  and the other components are replaced with identical functions, reducing Eqs.~(\ref{eq:lrvq2}, \ref{eq:lrvq3}) to $\bm{b}_{4} = \bm{a}_{4}$ and $\bm{c}_{4} = \bm{b}_{4}$, respectively.

#### Post-Training with Teacher-Student Distillation

Training NAC models with the MRVQ module is challenging because quantization layers with lower frame rates are prone to be ignored.
To address this, we introduce a post-training technique based on teacher-student distillation, where a NAC model pre-trained with a high frame rate serves as the teacher model.
As shown in Figure~\hyperref[fig:mreq]{\ref*{fig:mreq}a}, teacher embeddings ({in green}) are extracted from the frozen RVQ module, while student embeddings ({in purple}) are extracted from the MRVQ module.
We then minimize the feature-level distillation (FLD) loss and the hidden-state reconstruction (HSR) loss.

**FLD loss.**
The FLD loss is a mean absolute error (MAE) loss between teacher and student embeddings, defined as follows:

$$
\begin{aligned}
  L_{\mathrm{FLD}} = \sum_{({s}, {t}) \in \mathcal{P}}\lambda^{\mathrm{FLD}}_{({s}, {t})}\|{\hat{\bm{h}}_{s}} - {\bm{h}_{t}}\|_{1},\\
  \hat{\bm{h}}_{s} = \sum_{k=1}^{{s}} \tilde{\bm{c}}_{k},\\
  \bm{h}_{t} = \sum_{l=1}^{{t}} \tilde{\bm{z}}_{l},
\end{aligned}
$$

where
${\bm{h}_{s}}$ is a student embedding,
${\bm{h}_{t}}$ is a teacher embedding,
$\mathcal{P}$ is a set of student-teacher index pairs,
and $\lambda^{\mathrm{FLD}}_{({s}, {t})}$ is a weight {coefficient}.
We use $\mathcal{P} = \{({1}, {1}), ({2}, {3}), ({3}, {5}), ({4}, {8})\}$ for RVQ with eight layers and MRVQ with four LRVQ blocks.
Note that $\tilde{\bm{c}}_{k}$ and $\tilde{\bm{z}}_{l}$ are obtained from Eqs.~(\ref{eq:mrvq1}) and Eqs.~(\ref{eq:rvq1}), respectively.
The student-teacher pairs in $\mathcal{P}$ are determined so that the cumulative number of student's post-quantization layers matches that of the teacher's quantization layers.

**HSR loss.**

The HSR loss is introduced to further facilitate training of each LRVQ block:

$$
L_{\mathrm{HSR}} =
\sum_{k=1}^{K}
\lambda^{\mathrm{HSR}}_{k}
\|
\tilde{\bm{a}}_{k} -
D_{k} (\tilde{\bm{b}}_{k})
\|_{1}
$$

where $\tilde{\bm{a}}_{k}, \tilde{\bm{b}}_{k}$ are from Eqs.~(\ref{eq:lrvq1}, \ref{eq:lrvq2}), and $\lambda^{\mathrm{HSR}}_{k}$ is a weight {coefficient}.

**Total loss.**
The total loss is given by
$L_{\mathrm{total}} = L_{\mathrm{NAC}} + L_{\mathrm{FLD}} + L_{\mathrm{HSR}}$, where $L_{\mathrm{NAC}}$ is the loss used to train the NAC model.
We continue to train the encoder and decoder with the MRVQ module using a copied weight from the NAC model, which is used as a teacher model.
Compared to training from scratch, this allows for more efficient and stable convergence.

**Discussion.**
Our post-training approach is designed to be independent of the encoder-decoder architecture of the original NAC model, as we only assumed the use of RVQ.
Consequently, by utilizing state-of-the-art NAC models such as SpeechTokenizer~\citep{Zhang2024SpeechTokenizer} instead of Encodec~\citep{defossez2022encodec}, it is possible to achieve higher performance at a lower frame rate.

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

We introduced two novel approaches for minute-long zero-shot text-to-speech synthesis: MReQ and HALL-E.
MReQ reduced the frame rate of the Encodec model to 8 Hz by reorganizing audio tokens via MRVQ.
HALL-E efficiently synthesized minute-long speech by using 8Hz tokens in the AR model and MRVQ sub-modules in the NAR model.
We demonstrated the effectiveness of these approaches on MinutesSpeech, the newly introduced dataset consisting of 40,000 hours of speech data.
Our work contributed to promote zero-shot TTS research.

### Limitations and Future Work

By reducing the frame rate to 8 Hz, our AR model can utilize longer contexts, enhancing the naturalness of the synthesized speech.
We believe that to handle extended context is particularly advantageous for larger AR models such as AudioLM~\citep{borsos2023audiolm}, SpeechGPT~\citep{zhang2023speechgpt}, and PSLM~\citep{mitsui2024pslm}.
Demonstrating the effectiveness of our approach not only in TTS but also with these models remains a future work.
Furthermore, as shown in Table~\ref{tab:dataset}, we have achieved shorter audio token length than  the corresponding text token length.
However, in our current AR model, we concatenate these tokens, which results in the text tokens becoming a bottleneck in terms of sequence length.
Small-E~\citep{lemerle2024small} propose methods to mitigate this issue by processing each token individually and fusing them using cross-attention.
Exploring such architectural enhancements is an important direction for future work.
Lastly, as shown in Table~\ref{tab:nacmodels}, our method brought significant improvements with SpeechTokenizer, even more so than when applied to Encodec.
It means that our approach can further enhance the objective of preserving linguistic information.
This indicates that our method could serve as a replacement for traditional SSL models~\citep{hsu2021hubert, chen2022wavlm} or ASR encoders~\citep{lakhotia2021generative, chu2023qwen}, marking an important direction for future research.
