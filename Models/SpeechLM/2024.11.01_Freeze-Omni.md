# Freeze-Omni

<details>
<summary>基本信息</summary>

- 标题: "Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model with Frozen LLM"
- 作者:
  - 01 Xiong Wang,
  - 02 Yangze Li,
  - 03 Chaoyou Fu,
  - 04 Lei Xie,
  - 05 Ke Li,
  - 06 Xing Sun,
  - 07 Long Ma
- 链接:
  - [ArXiv](https://arxiv.org/abs/2411.00774)
  - [Publication]()
  - [Github](https://github.com/VITA-MLLM/Freeze-Omni)
  - [Demo](https://freeze-omni.github.io)
- 文件:
  - [ArXiv](_PDF/2411.00774v1__Freeze-Omni__A_Smart_and_Low_Latency_Speech-to-Speech_Dialogue_Model_with_Frozen_LLM.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

The rapid development of large language models has brought many new smart applications, especially the excellent multimodal human-computer interaction in GPT-4o has brought impressive experience to users.
In this background, researchers have proposed many multimodal LLMs that can achieve speech-to-speech dialogue recently.
In this paper, we propose a speech-text multimodal LLM architecture called Freeze-Omni.
Our main contribution is the speech input and output modalities can connected to the LLM while keeping the LLM frozen throughout the training process.
We designed 3-stage training strategies both for the modeling of speech input and output, enabling Freeze-Omni to obtain speech-to-speech dialogue ability using text-speech paired data (such as ASR and TTS data) and only 60,000 multi-round text Q&A data on 8 GPUs.
Moreover, we can effectively ensure that the intelligence of the Freeze-Omni in the speech modality is at the same level compared with that in the text modality of its backbone LLM, while the end-to-end latency of the spoken response achieves a low level.
In addition, we also designed a method to achieve duplex dialogue ability through multi-task training, making Freeze-Omni have a more natural style of dialogue ability between the users.
Freeze-Omni mainly provides a possibility for researchers to conduct multimodal LLM under the condition of a frozen LLM, avoiding various impacts caused by the catastrophic forgetting of LLM caused by fewer data and training resources.

## 1.Introduction: 引言

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论