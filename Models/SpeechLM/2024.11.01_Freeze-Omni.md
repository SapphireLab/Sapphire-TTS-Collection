# Freeze-Omni

<details>
<summary>基本信息</summary>

- 标题: "Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model with Frozen LLM"
- 作者:
  - 01 Xiong Wang,
  - 02 Yangze Li,
  - 03 Chaoyou Fu,
  - 04 Lei Xie,
  - 05 Ke Li,
  - 06 Xing Sun,
  - 07 Long Ma
- 链接:
  - [ArXiv](https://arxiv.org/abs/2411.00774)
  - [Publication]()
  - [Github](https://github.com/VITA-MLLM/Freeze-Omni)
  - [Demo](https://freeze-omni.github.io)
- 文件:
  - [ArXiv](_PDF/2411.00774v1__Freeze-Omni__A_Smart_and_Low_Latency_Speech-to-Speech_Dialogue_Model_with_Frozen_LLM.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

The rapid development of large language models has brought many new smart applications, especially the excellent multimodal human-computer interaction in GPT-4o has brought impressive experience to users.
In this background, researchers have proposed many multimodal LLMs that can achieve speech-to-speech dialogue recently.
In this paper, we propose a speech-text multimodal LLM architecture called Freeze-Omni.
Our main contribution is the speech input and output modalities can connected to the LLM while keeping the LLM frozen throughout the training process.
We designed 3-stage training strategies both for the modeling of speech input and output, enabling Freeze-Omni to obtain speech-to-speech dialogue ability using text-speech paired data (such as ASR and TTS data) and only 60,000 multi-round text Q&A data on 8 GPUs.
Moreover, we can effectively ensure that the intelligence of the Freeze-Omni in the speech modality is at the same level compared with that in the text modality of its backbone LLM, while the end-to-end latency of the spoken response achieves a low level.
In addition, we also designed a method to achieve duplex dialogue ability through multi-task training, making Freeze-Omni have a more natural style of dialogue ability between the users.
Freeze-Omni mainly provides a possibility for researchers to conduct multimodal LLM under the condition of a frozen LLM, avoiding various impacts caused by the catastrophic forgetting of LLM caused by fewer data and training resources.

## 1.Introduction: 引言

In recent years, the development of large language models has been extremely rapid. A series of large language models represented by the GPT series~\citep{floridi2020gpt,achiam2023gpt} of OpenAI has demonstrated extraordinary capabilities. As speech interaction is one of the most natural forms of human-computer interaction, combining speech input and output with an LLM can bring an extraordinary experience to users. The traditional method is to use a cascaded approach of ASR + LLM + TTS to achieve the interaction with LLM in speech modality. However, this approach often leads to a relatively high engineering complexity and a considerable interaction latency. Nevertheless, GPT-4o~\citep{NBS_2021} has changed this situation, it provides an end-to-end speech interaction mode which has significantly improved the user experience, triggering a research boom among researchers regarding multimodal LLMs for speech-to-speech interaction.

In the field of general LLMs, many public models such as Llama 3.2~\citep{dubey2024llama}, Qwen2.5~\citep{qwen2.5}, Mixtral~\citep{jiang2024mixtral}, etc. have provided very good opportunities for researchers to develop downstream tasks on them. Therefore, in the research field of multimodal LLMs for speech-to-speech, works such as Mini-Omni2~\citep{xie2024mini2}, LLaMA-Omni~\citep{fang2024llama}, and Moshi~\citep{defossez2024moshi} have provided excellent references for researchers. These works adopt different strategies to align the speech modality with the LLM and design some methods to achieve a duplex dialogue mode, demonstrating excellent performance.

In this research context, we found that in the process of aligning the LLM with the speech modality in existing public speech-text multimodal LLMs~\citep{chu2024qwen2,defossez2024moshi,fang2024llama,fu2024vita,zhang2023speechgptempoweringlargelanguage,xie2024mini}, the parameters of the LLM are more or less fine-tuned. However, in most cases, it is very difficult for researchers to easily collect spoken Q\&A data at the million-hour level (the corresponding text content can be comparable to the amount of data for training text-modal LLMs).  This inevitably brings about the forgetting problem to the LLM, resulting in a negative impact on its intelligence. In addition, only a few works have evaluated the accuracy of spoken question-answering tasks for speech-to-speech multimodal LLMs, and show an obvious gap in performance between spoken question-answering and text-modality question-answering. Therefore, in this paper, we propose a speech-to-speech dialogue LLM called Freeze-Omni, achieving speech modality alignment while the LLM is frozen throughout the training process, and obtaining low latency speech dialogue capabilities while keeping the intelligence of the backbone LLM. Freeze-Omni is mainly implemented in the following steps:

\textbf{Modeling of speech input}
We first use a large amount of ASR data to align the speech encoder and the LLM, enabling the LLM to understand the semantic information from the speech. Then, with the LLM frozen, a training strategy of prompt embedding is used to let the model have the ability to possess speech input to text output, training on only a small amount of Q\&A data.

\textbf{Modeling of speech output}
Second, we use a mount of text-speech paired data to train the AR-based speech decoder which can generate speech tokens from text and a single-codebook based codec model is used to decode the speech token into waveform. Then, we design a prefix kv-cache fine-tune strategy, using the hidden state vector output by the LLM to transfer the speech decoder into the output text space of LLM, achieving the ability of text input to speech output while keeping the LLM frozen.

\textbf{Design for duplex dialogue}
Finally, we simultaneously connect the speech encoder and speech decoder from the above parts to the backbone LLM. Then, a task of chunk-wise state prediction is used to enable the LLM to interrupt or reject the user's input, achieving the duplex speech-to-speech dialogue ability.

In conclusion, the main contributions of the proposed Freeze-Omni are as follows:

- The parameters of the LLM are completely frozen throughout the training process, ensuring that the intelligence of the LLM will be kept. At the same time, the ability of low latency speech-to-speech dialogue is still obtained.
- The data scale relied on during the training process is small and consumes fewer computing resources. It requires text-speech paired data (such as ASR and TTS training data) and only a small amount of Q\&A data in text modality.
- Freeze-Omni can support any (multimodal) LLM that has a text modality and retains the abilities of the LLM such as prompt following and role-playing. Moreover, if it is necessary to change the style of the LLM's response, it is only necessary to fine-tune the LLM with text data in the corresponding style.

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论