# SyncSpeech

<details>
<summary>基本信息</summary>

- 标题: "SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based on Temporal Masked Transformer"
- 作者:
  - 01 Zhengyan Sheng,
  - 02 Zhihao Du,
  - 03 Shiliang Zhang,
  - 04 Zhijie Yan,
  - 05 Yexin Yang,
  - 06 Zhenhua Ling
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.11094)
  - [Publication]()
  - [Github]()
  - [Demo](https://syncspeech.github.io/)
- 文件:
  - [ArXiv](../_PDF/2502.11094v1__SyncSpeech__Low-Latency_and_Efficient_Dual-Stream_Text-to-Speech_based_on_Temporal_Masked_Transformer.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

This paper presents a dual-stream text-to-speech (TTS) model, ***SyncSpeech***, capable of receiving streaming text input from upstream models while simultaneously generating streaming speech, facilitating seamless interaction with large language models.
***SyncSpeech*** has the following advantages: Low latency, as it begins generating streaming speech upon receiving the second text token; High efficiency, as it decodes all speech tokens corresponding to the each arrived text token in one step.
To achieve this, we propose a temporal masked transformer as the backbone of ***SyncSpeech***, combined with token-level duration prediction to predict speech tokens and the duration for the next step.
Additionally, we design a two-stage training strategy to improve training efficiency and the quality of generated speech.
We evaluated the ***SyncSpeech*** on both English and Mandarin datasets.
Compared to the recent dual-stream TTS models, ***SyncSpeech*** significantly reduces the first packet delay of speech tokens and accelerates the real-time factor.
Moreover, with the same data scale, ***SyncSpeech*** achieves performance comparable to that of traditional autoregressive-based TTS models in terms of both speech quality and robustness.
Speech samples are available at [this https URL](https://syncspeech.github.io/).

</td><td>

本文展示了双流文本转语音模型 ***SyncSpeech***, 能够接收来自上游模型的流式文本输入, 同时生成流式语音, 实现和大型语言模型的无缝交互.
***SyncSpeech*** 有以下优点:
- 低延迟, 因为它在接收到第二个文本 Token 后立即开始生成流式语音;
- 高效率, 因为它在一步中解码所有与每个到达的文本 Token 对应的语音 Token.

为了实现这些特性, 我们提出了一个时序掩膜 Transformer 作为 ***SyncSpeech*** 的骨干, 并结合 Token 级别时长预测用于预测语音 Token 和下一步的时长.
此外, 我们设计了两阶段训练策略以提升训练效率和生成语音的质量.

我们在英语和中文数据集上评估了 ***SyncSpeech***.
与最近的双流 TTS 模型相比, ***SyncSpeech*** 显著降低了语音 Token 的首包延迟, 并加速了实时因子.
此外, 与传统的自回归 TTS 模型相比, 在相同数据尺度下, ***SyncSpeech*** 在语音质量和健壮性方面都达到了可比性.
语音样本可在 [此 https URL](https://syncspeech.github.io/) 获得.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

In recent years, with advancements in generative models and the expansion of training datasets, text-to-speech (TTS) models \cite{valle, voicebox, ns3} have made breakthrough progress in naturalness and quality, gradually approaching the level of real recordings.
However, low-latency and efficient dual-stream TTS, which involves processing streaming text inputs while simultaneously generating speech in real time, remains a challenging problem \cite{livespeech2}.
These models are ideal for integration with upstream tasks, such as large language models (LLMs) \cite{gpt4} and streaming translation models \cite{seamless}, which can generate text in a streaming manner.
Addressing these challenges can improve live human-computer interaction, paving the way for various applications, such as speech-to-speech translation and personal voice assistants.

Recently, inspired by advances in image generation, denoising diffusion \cite{diffusion, score}, flow matching \cite{fm}, and masked generative models \cite{maskgit} have been introduced into non-autoregressive (NAR) TTS \cite{seedtts, F5tts, pflow, maskgct}, demonstrating impressive performance in offline inference.
During this process, these offline TTS models first add noise or apply masking guided by the predicted duration.
Subsequently, context from the entire sentence is leveraged to perform temporally-unordered denoising or mask prediction for speech generation.
However, this temporally-unordered process hinders their application to streaming speech generation\footnote{
Here, “temporally” refers to the physical time of audio samples, not the iteration step $t \in [0, 1]$ of the above NAR TTS models.}.

When it comes to streaming speech generation, autoregressive (AR) TTS models \cite{valle, ellav} hold a distinct advantage because of their ability to deliver outputs in a temporally-ordered manner.
However, compared to recently proposed NAR TTS models, AR TTS models have a distinct disadvantage in terms of generation efficiency \cite{MEDUSA}.
Specifically, the autoregressive steps are tied to the frame rate of speech tokens, resulting in slower inference speeds.
While advancements like VALL-E 2 \cite{valle2} have boosted generation efficiency through group code modeling, the challenge remains that the manually set group size is typically small, suggesting room for further improvements.
In addition, most current AR TTS models \cite{dualsteam1} cannot handle stream text input and they only begin streaming speech generation after receiving the complete text, ignoring the latency caused by the streaming text input.
The most closely related works to ***SyncSpeech*** are CosyVoice2 \cite{cosyvoice2.0} and IST-LM \cite{yang2024interleaved}, both of which employ interleaved speech-text modeling to accommodate dual-stream scenarios.
However, their autoregressive process generates only one speech token per step, leading to low efficiency.

To seamlessly integrate with upstream LLMs and facilitate dual-stream speech synthesis, this paper introduces ***SyncSpeech***, designed to keep the generation of streaming speech in synchronization with the incoming streaming text.
***SyncSpeech*** has the following advantages:
(1) low latency, which means it begins generating speech in a streaming manner as soon as the second text token is received,
(2) high efficiency, which means for each arriving text token, only one decoding step is required to generate all the corresponding speech tokens.

***SyncSpeech*** is based on the proposed ***Temporal Masked generative Transformer (TMT)***.
During inference, ***SyncSpeech*** adopts the Byte Pair Encoding (BPE) token-level duration prediction, which can access the previously generated speech tokens and performs top-k sampling.
Subsequently, mask padding and greedy sampling are carried out based on the duration prediction from the previous step.

Moreover, sequence input is meticulously constructed to incorporate duration prediction and mask prediction into a single decoding step.
During the training process, we adopt a two-stage training strategy to improve training efficiency and model performance.
First, high-efficiency masked pretraining is employed to establish a rough alignment between text and speech tokens within the sequence, followed by fine-tuning the pre-trained model to align with the inference process.

Our experimental results demonstrate that, in terms of generation efficiency, ***SyncSpeech*** operates at 6.4 times the speed of the current dual-stream TTS model for English and at 8.5 times the speed for Mandarin.
When integrated with LLMs, ***SyncSpeech*** achieves latency reductions of 3.2 and 3.8 times, respectively, compared to the current dual-stream TTS model for both languages.
Moreover, with the same scale of training data, ***SyncSpeech*** performs comparably to traditional AR models in terms of the quality of generated English speech.
For Mandarin, ***SyncSpeech*** demonstrates superior quality and robustness compared to current dual-stream TTS models.
This showcases the potential of ***SyncSpeech*** as a foundational model to integrate with upstream LLMs.

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

**Text-to-Speech**

Text-to-Speech, the transformation of text into audible signals understandable by humans, is pivotal for human-computer interaction.
TTS systems can be mainly divided into AR-based and NAR-based categories.

For AR-based systems, VALL-E \cite{valle} predicts the first layer of acoustic tokens extracted by EnCodec \cite{encodec} using an AR codec language model, while a NAR model is used to predict the remaining layers.
CosyVoice \cite{cosyvoice} employs an AR model to predict supervised semantic representations and combines flow matching to predict acoustic representations.
AR-based TTS models, with their in-context learning capability, can generate natural, prosody-diverse speech in a streaming manner.
However, AR-based TTS models exhibit shortcomings in generation efficiency.
Besides the previously mentioned VALL-E 2 \cite{valle2}, MEDUSA \cite{MEDUSA} and VALL-E R \cite{Valler} introduce speculative decoding \cite{spdeco} and a codec-merging method, respectively, to accelerate autoregressive generation.
Nonetheless, the efficiency gains achieved by these approaches remain limited, unable to perform synchronized decoding steps with text tokens.

For NAR-based TTS models, most previous approaches require speech duration prediction conditioned on the input text, followed by upsampling the text representations to match the acoustic feature length before feeding them into the generation model.
Following FastSpeech \cite{fastspeech2}, VoiceBox \cite{voicebox} and NaturalSpeech 2 \cite{ns2} predict phone-level durations using a regression-based approach.
NaturalSpeech 3 \cite{ns3} adopts a discrete diffusion model, combining classification loss and duration prompts for duration prediction, which outperforms text-dependent regression-based duration prediction in terms of speech robustness and quality.
However, NaturalSpeech 3 requires an additional duration prediction model, which complicates the pipeline, whereas ***SyncSpeech*** integrates duration and speech token predictions into a unified framework.
The NAR TTS model most relevant to ***SyncSpeech*** is MaskGCT \cite{maskgct}, which predicts the total duration of the speech and then performs temporally-unordered multi-step mask prediction.
Unlike MaskGCT, ***SyncSpeech*** employs temporally-ordered mask prediction and BPE token-level duration prediction to achieve speech generation in a dual-stream scenario.

</td><td>

</td></tr>
<tr><td>

**Speech Large Language Models**

Speech Large Language Models (SLLMs) empower LLMs to interact with users through speech, responding to user’s instruction with low latency \cite{wavchat}.
A basic approach \cite{audiogpt} to achieve this speech interaction involves a cascade of automatic speech recognition (ASR), LLM and TTS models, where the ASR transcribes the users' speech instruction into text, and the TTS model converts the LLM's textual response into speech.
However, most current AR TTS models cannot process streaming text input, resulting in significant latency in the aforementioned cascaded systems.
In contrast, some end-to-end speech-language models have been proposed that can generate speech tokens directly, thereby achieving extremely low response latency.
LLaMA-Omni \cite{llamaomni} aligns the hidden states of LLMs with discrete HuBERT \cite{hubert} representations using CTC loss, but the generated speech exhibits less natural prosody.
Mini-Omni \cite{mini1} employs a parallel decoder approach to generate text and speech tokens simultaneously.
However, due to the significantly longer length of speech tokens compared to text tokens, its generation efficiency remains low.
The proposed ***SyncSpeech*** can process streaming text input and generates speech in synchronization, with the potential to unite with LLMs to become end-to-end SLLMs.

</td><td>

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

A dual-stream TTS model simultaneously processes streaming text input and generates speech in a streaming manner.
Upon receiving newly generated text tokens $\boldsymbol{y}_{\text{arr}}$ from the upstream LLMs, the objective of the dual-streaming TTS it to estimate $p(\boldsymbol{x}_{\text{arr}} | \boldsymbol{y}_{\text{arr}}, \boldsymbol{x}_{\text{pre}},\boldsymbol{y}_{\text{pre}})$.
In this context, $\boldsymbol{x}_{\text{arr}}$ represents the speech waveform segment corresponding to $\boldsymbol{y}_{\text{arr}}$, while $\boldsymbol{y}_{\text{pre}}$ and $ \boldsymbol{x}_{\text{pre}}$ denote the preceding text tokens and its corresponding speech waveform, respectively.

***SyncSpeech*** is a two-stage TTS system, consisting of the text-to-token and token-to-speech stages.
The estimation of $p(\boldsymbol{x}_{\text{arr}} | \boldsymbol{y}_{\text{arr}}, \boldsymbol{x}_{\text{pre}}, \boldsymbol{y}_{\text{pre}})$ is decomposed into a text-to-token model $p(\boldsymbol{s}_{\text{arr}}| \boldsymbol{y}_{\text{arr}}, \boldsymbol{x}_{\text{pre}}, \boldsymbol{y}_{\text{pre}})$ and a token-to-speech model $p(\boldsymbol{x}_{\text{arr}} | \boldsymbol{s}_{\text{arr}})$, where $\boldsymbol{s}_{\text{arr}}$ is the speech tokens corresponding to the speech waveform segment $\boldsymbol{x}_{\text{arr}}$.
Specifically, the proposed TMT is adopted as the backbone of text-to-token model.
Then, an off-the-shelf chunk-aware speech decoder \cite{cosyvoice2.0} is adopted as the token-to-speech model.

The proposed TMT module is based on a llama-style Transformer \cite{llama}.
We have specifically designed a novel attention mask to accommodate temporally-ordered mask generation.
Below, I will detail the two-stage training strategy of the TMT-based text-to-token model and its attention mask, cover the details of the other modules, and describe the inference process.

</td><td>

</td></tr></table>

### Training

<table><tr><td width="50%">

Given a dataset of transcribed speech ($\boldsymbol{\tilde{x}}$, $\boldsymbol{\tilde{y}}$), where $\boldsymbol{\tilde{x}}$ and $\boldsymbol{\tilde{y}}$ denote an audio sample and its transcript, respectively, the transcript $\boldsymbol{\tilde{y}}$ is tokenized into a BPE token sequence $\boldsymbol{y} = [y_1, y_2, y_3, ..., y_L]$, where $L$ is the number of BPE tokens.
An off-the-shelf speech tokenizer is used to encode the speech sample $\boldsymbol{\tilde{x}}$ into $T$ frame discrete speech tokens $\boldsymbol{s} = [s_1, s_2, s_3, ..., s_T]$.
We further define duration tokens $\boldsymbol{a} = [a_1, a_2, a_3, ..., a_L]$ as the positions indicating the end time of each corresponding BPE token within the speech token sequence, with $a_L = T.$ For a pair of ($\boldsymbol{\tilde{x}}$, $\boldsymbol{\tilde{y}}$), $\boldsymbol{a}$ can be obtained through an open-source alignment tool.

As shown in Figure \ref{fig1}, to maintain consistency with the inference process (see Section \ref{sec:inference}), the sequence input is then constructed as follows.
We select a random number $n \in [1, L]$, which indicates that when receiving streaming text input, ***SyncSpeech*** needs to generate the speech tokens corresponding to the $n$-th BPE token at this moment.
To avoid unnatural pauses, ***SyncSpeech*** allows look ahead $q$ text tokens, obtaining a truncated text token sequence $\boldsymbol{y}' = [y_1, y_2, y_3, ..., y_{L'}]$, where $L'=min(L, n+q)$.
Based on the duration tokens $\boldsymbol{a}$, the truncated speech token sequence $\boldsymbol{s}_{1:a_{n}}=[s_1, s_2, ...., s_{a_{n}}]$ is obtained.
Then, we define the masked speech token sequence $\boldsymbol{s}'$ and and corresponding binary mask
$\boldsymbol{m}$ as follows,
$$
\boldsymbol{s}' = \boldsymbol{s}_{1:a_{n}} \odot \boldsymbol{m},
$$

$$
\boldsymbol{m}=[m_{i}]_{i=1}^{a_{n}}, \boldsymbol{m}_{1:a_{n-1}}=0, \boldsymbol{m}_{a_{n-1}:a_{n}}=1.
$$

That is all speech tokens corresponding to $\boldsymbol{x}_{n}$ are replaced with the specific mask token, while the rest remain unchanged.
Then, the truncated text token sequence $\boldsymbol{y}'$, along with the masked speech token sequence $\boldsymbol{s}'$ and duration tokens $\boldsymbol{a}$, are used to construct the input sequence as follows,
$$
\boldsymbol{f} = [\boldsymbol{y}',E, D, \boldsymbol{s}'_{1:a_1},..., D, \boldsymbol{s}'_{a_{n-1}:a_n}, D],
$$

where $E$ is end-of-text token, $D$ is a placeholder for duration prediction.
Based on the duration tokens $\boldsymbol{a}$, $D$ is used to separate the masked speech token sequence corresponding to different BPE tokens.
In practice, $E$ is inserted only when $n=L$ .

The sequence $\boldsymbol{f}$ is used as input for the TMT with the mask prediction and duration prediction as training objectives.
Specifically, the sequence $\boldsymbol{f}$ is fed into the TMT forward to obtain the hidden states, which then pass through two different linear layers to predict the speech tokens corresponding to text token $y_{n}$ and the duration of the next text token $y_{n+1}$.
This enables us to integrate duration prediction and mask prediction into a single decoding step during inference, except for the first text token duration prediction (Details are provided in Section \ref{sec:inference}).
We minimize the following negative log-likelihood function for masked generative training and duration training,
$$
\mathcal{L}_{\text {mask}}^{1}=-\log p \left( \boldsymbol{s}_{a_{n-1}:{a_n}} \mid \boldsymbol{f}; \theta \right),
$$

$$
\mathcal{L}_{\text {duration}}^{1}=-\log p \left(l_{n+1}\mid \boldsymbol{f}; \theta \right),
$$

where $\theta$ represents the neural network parameters of TMT, $l_{n+1}=a_{n+1} - a_{n}$ and $a_0=0$.
In this way, we simulate the scenario of receiving streaming text input during the training process and are able to generate speech in sync.

We design a corresponding attention mask, as shown in Figure \ref{fig1}.
Specifically, a causal mask is used for the truncated text sequence $\boldsymbol{y}'$ and duration placeholder parts.
For the masked speech token sequence $\boldsymbol{s}'$, a dynamic chunk attention mask is applied based on the duration tokens $\boldsymbol{a}$, enabling it to attend all historical tokens, as well as all speech tokens and mask tokens corresponding to their own text BPE tokens.

</td><td>

</td></tr></table>

### Pretraining

<table><tr><td width="50%">

While the aforementioned method aligns with the prediction process, it suffers from low training efficiency.
This training inefficiency arises because, during each training step, only the gradients of speech tokens $\boldsymbol{s}_{a_{n-1}:{a_n}}$ and durations for $y_{n+1}$ are backpropagated.
To further improve the training efficiency, we first perform masked pre-training on the TMT.

Given speech tokens $\boldsymbol{s}$ of a speech sample, we obtain the masked speech tokens $\boldsymbol{\hat{s}} = \boldsymbol{s} \odot \boldsymbol{\hat{m}} $, where $\boldsymbol{\hat{m}}=[\hat{m}_{i}]_{i=1}^{a_{L}} $ is a binary mask of speech tokens.
We design the masking rules primarily from two perspectives, high masking probability and consistency with the prediction process as much as possible.
Specifically, the binary mask $\boldsymbol{\hat{m}}_\text{bpe}$ of text tokens is constructed first, where the first value is distributed according to a Bernoulli distribution ($p=0.5$) and the subsequent adjacent values cannot be the same.
Based on the duration tokens $\boldsymbol{a}$, the text token mask $\boldsymbol{\hat{m}}_\text{bpe}$ is converted into the corresponding speech token mask $\boldsymbol{\hat{m}}$.
Then, we build the following sequence as the input for TMT,
$$
\boldsymbol{\hat{f}} = [\boldsymbol{y},E, D, \boldsymbol{\hat{s}}_{1:a_1},..., D, \boldsymbol{\hat{s}}_{a_{L-1}:a_L}],
$$

and the TMT is optimized to minimize the negative log-likelihood for masked generative training and duration training as follows,
$$
\mathcal{L}_{\text {mask}}^{2} =- \sum_{\substack{j \in J}} \log p \left(\boldsymbol{s}_{a_{j-1}:a_{j}} \mid \boldsymbol{\hat{f}}_{\leq a_{j}}; \theta \right),
$$

$$
\mathcal{L}_{\text {duration}}^{2} =- \sum_{\substack{j \in J}} \log p \left(l_{j}\mid \boldsymbol{\hat{f}}_{\leq a_{j-1}}; \theta \right),
$$

where $J$ denote the sequence where each element satisfies the condition $\hat{m}_{j} = 1$,
$\boldsymbol{\hat{f}}_{\leq a_{j}}=[\boldsymbol{y},E, D,\boldsymbol{\hat{s}}_{1:a_1},..., D, \boldsymbol{\hat{s}}_{a_{j-1}:a_j}]$ and $l_{j} = a_{j} - a_{j-1}$.
Additionally, the attention mask mentioned above is also utilized for pretrain.

In summary, an efficient masked pretraining is initially performed with a high masking probability to facilitate the alignment between text and speech tokens.
Subsequently, we fine-tune the pretrained model using a training strategy consistent with the prediction process.
This approach enhances the efficiency of the training process, and the masked pretraining also contributes to the robustness of the generated speech.

</td><td>

</td></tr></table>

### Other Modules

<table><tr><td width="50%">

In this subsection, we introduce the other modules in ***SyncSpeech*** besides TMT.
1) Text BPE tokenizer: To facilitate interaction with upstream LLMs, we utilize the Qwen tokenizer \cite{qwen} directly.
2) Speech tokenizer: the open-source supervised speech semantic (S3) tokenizer \cite{cosyvoice2.0} is selected, which operates at 25 Hz.
The S3 tokenizer is developed by integrating finite scalar quantization (FSQ) \cite{fsq} into the intermediate representations of an ASR model trained on large-scale data, and then fine-tuning it for the ASR task.
2) The off-the-shelf speech decoder \cite{cosyvoice2.0} is based on the conditional flow matching (CFM) decoder and HiFi-GAN vocoder \cite{hifigan}.
The CFM decoder employs a chunk-aware training strategy, enabling the streaming generation of Mel-spectrograms from the chunk-size input speech tokens.
These Mel-spectrograms are then converted into speech using the vocoder, which operates in parallel with a fully convolutional network.

</td><td>

</td></tr></table>

### Inference

<table><tr><td width="50%">

During the inference process, ***SyncSpeech*** processes text in a streaming manner and synchronously generates speech, with the general algorithm flow shown in Algorithm \ref{alg1}.
Specifically, when the number of input text BPE tokens $\boldsymbol{y}$ exceeds the look-ahead number $q$, the input sequence $\boldsymbol{f} = [\boldsymbol{y}, D]$ is built, which is fed into TMT to predict the duration of speech tokens corresponding to $y_1$.
Then, based on the predicted duration, we perform sequence padding by inserting the mask tokens and a duration prediction placeholder.
Subsequently, the sequence is fed back into TMT for synchronous mask prediction of $y_1$ and the duration prediction of $y_2$, followed by the input sequence $\boldsymbol{s}$ update and padding.
For subsequent BPE token input, the above prediction step, update step, and padding step are repeated to generate speech tokens in a streaming manner.
In the process described above, once the number of generated speech tokens surpasses the chunk size of the off-the-shelf speech decoder, these tokens and the speaker prompt can be utilized to stream speech output.

Additionally, existing speech tokens can be accessed during duration prediction and speech token generation, which allows ***SyncSpeech*** to control the prosody of the generated speech with in-context learning.
Specifically, given a speech prompt, we construct the prompt sequence according to Equation \ref{eq1}, serving as the generated sequence for prosody control.
Figure \ref{fig2} in the Appendix shows detailed inference visualizations.

</td><td>

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

**Datasets**

We trained ***SyncSpeech*** on datasets in both English and Mandarin, including the 585-hour LibriTTS \cite{libritts} dataset and 600 hours of internal Mandarin datasets.
The internal Mandarin dataset was further expanded to approximately 2000 hours, employing techniques such as speed alteration and pitch shifting.
The Montreal Forced Aligner (MFA) \cite{mfa} aligned transcripts according to its phone set, after which the alignment was transformed into text BPE-level format.
We evaluated ***SyncSpeech*** using three benchmarks: (1) LibriSpeech text-clean \cite{librispeech}, a standard English TTS evaluation set; (2) SeedTTS test-zh \cite{seedtts}, with 2,000 samples from the out-of-domain Mandarin DiDiSpeech dataset \cite{didispeech}; and (3) SeedTTS test-hard, containing approximately 400 difficult cases to evaluate TTS model robustness with repeated text, tongue twisters, and other complex synthesis scenarios.

</td><td>

</td></tr>
<tr><td>

**Settings**

We set the number of text tokens to look ahead $q=1$.
The chunk size of speech decoder is 15.
TMT has 16 layers, 16 attention heads, 1024-dimensional embeddings, and 2048-dimensional feed-forward layers.
***SyncSpeech*** was trained on 4 NVIDIA A800 80G GPUs.
The pre-training stage lasts for 70K steps, and the second stage lasts for 20K steps.

</td><td>

</td></tr>
<tr><td>

**Baseline Models**

This paper focuses on low-latency and efficient TTS in dual-stream scenarios.
Under the same data scale, we reproduced the following baseline models for comparison: CosyVoice \cite{cosyvoice} and recently proposed CosyVoice2 \cite{cosyvoice2.0}.
CosyVoice requires complete text input before speech generation.
CosyVoice2 uses interleaved text-speech modeling to process streaming text input and simultaneously generate streaming speech.
We trained CosyVoice, CosyVoice2, and ***SyncSpeech*** using the same speech tokenizer and text tokenizer, and employed the same open-source streaming speech decoder.
We utilized the official code\footnote{https://github.com/FunAudioLLM/CosyVoice} to reproduce the model and adopted a Llama-style Transformer, matching the size of ***SyncSpeech***, as the backbone of the text-to-speech model.
Additionally, we compared the open-sourced TTS model MaskGCT \cite{maskgct}, F5-TTS \cite{F5tts}, and VALL-E \cite{valle}, which were trained on large-scale data.
More details about baseline models can be found in the Appendix \ref{baselines}.

</td><td>

</td></tr>
<tr><td>

**Evaluation Metrics**

For the three benchmarks, we evaluated speech quality, latency, and efficiency.
For speech robustness, we chose Whisper-V3 and Paraformer as the ASR models for English and Mandarin, respectively, to transcribe the generated speech.
Then, we calculated the WER compared to the original transcriptions to evaluate the spech robustness.
We adopted the ERes2Net-based \cite{eres2net} speaker verification model\footnote{https://github.com/modelscope/3D-Speaker} to evaluate speaker similarity (SS).
We selected 100 sentences from each system and invited 10 native listeners to conduct a subjective MOS evaluation for speech naturalness (MOS-N), scoring from 1 to 5.
In terms of latency and efficiency, we compared the performance of various models on a single A800 GPU.
Due to the off-the-shelf speech decoder, we evaluate the latency and efficiency of the text-to-token stage across all models, except for F5-TTS.
We calculated the time required for the number of speech tokens to reach the chunk size of the speech decoder as First-packet latency (FPL).
There are two scenarios: one assumes the text is already available (FPL-A), while the other involves receiving output from the upstream LLM model (FPL-L), accounting for the time required for text generation.
For the real-time factor (RTF), we measure the ratio of the total duration of generated speech to the total time taken by the model.
More details about FPL and RTF can be found in the Appendix \ref{evaluation metrics}.

</td><td>

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

The evaluation results for ***SyncSpeech*** and the baseline models are presented in Table \ref{table1}.

</td><td>

</td></tr>
<tr><td>

**Speech Robustness**

We found that ***SyncSpeech*** exhibits different performance compared to the baselines across the three benchmarks.
Specifically, on the LibriSpeech test-clean benchmark, the performance of ***SyncSpeech*** was very close to that of CosyVoice2 based on the WER metric, with only a minor difference of 0.07\%.
***SyncSpeech*** achieved a lower WER score on the Seed test-zh set compared to CosyVoice and CosyVoice2, with improvements of 0.65\% and 0.93\%, respectively.
A key difference between the English and Mandarin datasets is the higher compression rate of the LLM tokenizer for Mandarin.
In English, one word typically equals one token, while in Mandarin, a common phrase often corresponds to a single token.
This means that, compared to the baseline model, ***SyncSpeech*** is better suited to the high compression rate tokenizer of the upstream large model.
Furthermore, on the Seed test-hard set, the robustness advantage of ***SyncSpeech*** was even more pronounced, with the improvements 9.05\% and 4.40\%, respectively.
In handling complex text, the explicit duration modeling in ***SyncSpeech*** helped the model learn the alignment between text and speech.

</td><td>

</td></tr>
<tr><td>

**Speaker Similarity**

Due to the same speech decoder and the excellent voice disentanglement capability of the speech tokens, ***SyncSpeech***, CosyVoice, and CosyVoice2 exhibited similar performance in terms of speaker similarity.

</td><td>

</td></tr>
<tr><td>

**Speech Naturalness**

The MOS-N scores for ***SyncSpeech*** and CosyVoice2 were quite similar on the LibriSpeech text-clean, indicating that the naturalness of the generated speech was generally comparable.
On the Seed test-zh benchmark, ***SyncSpeech*** outperformed CosyVoice2 by 0.08.
In the Seed test-hard benchmark, high WER and uncommon text led to unnatural prosody and generally low MOS-N scores in the generated speech.

</td><td>

</td></tr>
<tr><td>

**Latency**

***SyncSpeech*** has made a breakthrough in terms of latency, as shown in Table \ref{table1}.
Specifically, on the LibriSpeech test-clean benchmark, ***SyncSpeech*** was approximately 4 times faster than traditional AR models and over 20 times faster than the SOTA offline models in terms of FPL-A.
On the Seed test-zh benchmark, ***SyncSpeech*** achieved speed improvements of over 5 times and 30 times, respectively.
When receiving streaming text from the upstream large model (FPL-L), ***SyncSpeech*** can begin generating speech with just two text tokens.
In contrast, CosyVoice2 requires five tokens, while CosyVoice and other baseline models need the entire text input.
This highlights the distinct advantage of ***SyncSpeech*** in practical applications.

</td><td>

</td></tr>
<tr><td>

**Efficiency**

In terms of RTF, ***SyncSpeech*** is about 6.4 times faster on the LibriSpeech test-clean benchmark and about 8.6 times faster on the Seed test-zh benchmark compared to previous AR models.
On the Seed test-hard set, due to the increased number of text tokens caused by the uncommon text, the efficiency of ***SyncSpeech*** is slightly reduced.
Theoretically, the time complexity of AR models is $O(T)$, while the time complexity of ***SyncSpeech*** is $O(L)$, where $T$ represents the number of speech tokens and $L$ denotes the number of text tokens, thereby significantly improving efficiency.

</td><td>

</td></tr></table>

### Analysis

<table><tr><td width="50%">

**Sampling Strategy**

In the LibriSpeech validation set, we provided the ground-truth durations and applied greedy search along with different Top-k thresholds for duration prediction, as shown in Table \ref{table3}.
We found that, in terms of speech robustness, both Top-k 3 and greedy search outperformed the use of ground-truth durations in terms of the WER metric.
This is because the model struggled to effectively generalize to anomalies in the ground-truth durations.
We employed UTMOSv2\footnote{https://github.com/sarulab-speech/UTMOS22} as a surrogate objective metric of MOS-N.
In terms of speech naturalness, the results of Top-k 3 sampling are slightly better than those with the given ground-truth durations.
Additionally, we applied different Top-k thresholds for speech token prediction.
***SyncSpeech*** exhibited superior performance during greedy search, which is different from the previous AR TTS models or offline models.
This is because the speech tokens obtained through single-step decoding have the temporal dependency, which cannot be compensated by subsequent generation.

</td><td>

</td></tr>
<tr><td>

**Number of Look-ahead Tokens**

We evaluated how varying the number of tokens to look ahead affects speech robustness and speech naturalness on two validation sets, with the results presented in Table \ref{table5}.
We discovered that the optimal number of look-ahead text tokens varies across different languages in terms of WER performance.
This is influenced by the difference in the compression rate of text tokens and the contextual dependency in different languages.
In terms of speech naturalness, when the look-ahead number $q$ is greater than $2$, the generated speech exhibits slightly more natural pauses and speed, but it results in increased latency.

</td><td>

</td></tr>
<tr><td>

**Ablation Study**

We conducted an ablation study on the pre-training strategy by directly training the randomly initialized model in a manner consistent with the prediction process.
The WER results on the two validation sets are shown in Table \ref{table6}.
We found that pre-training significantly improved the speech robustness of the model, improving the WER metric by 1.17\% and 1.06\% on the two languages, respectively.
This indicated that masked pre-training not only improved training efficiency but also enhanced the robustness of the synthesized speech.
Additionally, a standard causal attention mask was applied to replace the designed attention mask, as shown in Table \ref{table6}.
If the mask token sequence of the same text token cannot attend to each other during inference, the robustness of the generated speech significantly decreased.
This further demonstrated the effectiveness of the designed attention mask.

</td><td>

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

This paper presents ***SyncSpeech***, a dual-stream speech generation model built on a temporal masked transformer.
***SyncSpeech*** can efficiently generate low-latency streaming speech from the real-time text input, maintaining the high quality and robustness of the generated speech.
We conducted comprehensive performance evaluations and analysis experiments in both English and Mandarin, demonstrating its capability as a foundational model for integration with upstream LLMs.
In the future, ***SyncSpeech*** will be trained on larger datasets to further improve its performance.

</td><td>

</td></tr></table>

### Limitations

<table><tr><td width="50%">

In this section, we will analyze the limitations of ***SyncSpeech*** and discuss potential future work.
***SyncSpeech*** requires token-level alignment information, which is challenging to achieve for sentences with mixed languages, and preprocessing becomes time-consuming on large-scale datasets.
In the future, we will explore semi-supervised duration prediction, which only requires the duration of a complete sentence without strict token-level alignment information, and integrate ***SyncSpeech*** into SLLM as a speech generation module.
In addition, since the off-and-shelf streaming speech decoder relies on flow matching, it limits the off-the-shelf RTF and the FPL.
Moreover, current single-codebook acoustic tokens, such as WavTokenizer \cite{wavtokenizer}, do not support streaming decoding.
In the future, we will investigate efficient and low-latency streaming speech decoders.

</td><td>

</td></tr></table>

## Appendix

### A·Details of Baselines

<table><tr><td width="50%">

**CosyVoice** A two-stage large-scale TTS system.
The first stage is an autoregressive model similar to VALL-E \cite{valle}, and the second stage is a diffusion model.
We use the official code and the 25Hz version of the pre-trained checkpoint\footnote{https://www.modelscope.cn/iic/CosyVoice-300M-25Hz.git}.

**CosyVoice2** Compared to CosyVoice, improvements have been made in the following three areas:
(1) The quantizer speech tokenizer has been upgraded to FSQ, further improve the performance of the quantization encoder.
(2) Interleaved text-speech modeling is employed, allowing for streaming text input.
(3) A chunk-aware speech decoder is used for streaming speech generation.
We use the official code and the 25Hz version of the pre-trained checkpoint\footnote{https://github.com/FunAudioLLM/CosyVoice}.

**VALL-E** A large-scale TTS system employs both an autoregressive and an auxiliary non-autoregressive model to predict discrete tokens derived from the Encodec \cite{encodec}.
We used an open-source checkpoint for inference.
As there is currently no open-source streaming speech decoder for Encodec, we assumed 15 frames when calculating the FPL metric for a fair comparison.

**MaskGCT**\cite{maskgct}  This is a large-scale, two-stage trained model.
In the first stage, the model utilizes text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model.
In the second stage, it predicts acoustic tokens based on these semantic tokens.
During training, MaskGCT learns to predict masked semantic or acoustic tokens given specific conditions and prompts.
During inference, MaskGCT generates speech through multi-step temporally non-sequential masked prediction.
Here, we use the official code and pre-trained checkpoint\footnote{https://github.com/openmmlab/Amphion}.

**F5-TTS**\cite{F5tts} a fully non-autoregressive text-to-speech system based on flow matching with Diffusion Transformer (DiT).
The text input is simply padded with filler tokens to the same length as input speech, and then the denoising is performed for speech generation.
F5-TTS does not utilize speech tokens and directly maps text to acoustic features.
Here, we use the official code and pre-trained checkpoint\footnote{https://github.com/SWivid/F5-TTS}.

</td><td>

</td></tr></table>

### B·Details of Latency and Efficiency Evaluation Metrics

<table><tr><td width="50%">

The first-package latency (FPL) and real-time factor (RTF) are two import metrics for streaming TTS models.
We define $d_{\text{LLM}}$ as the average time required by the upstream LLM to generate one text token and $d_{\text{TTS}}$ as the the time for the corresponding AR TTS models to forward one step and for the NAR TTS models to perform one sampling.
The FPL-L of baseline models and ***SyncSpeech*** are as follows,
$$
\begin{aligned}
& L_{\text{FPL-L}}^{\text{CosyVoice}} =L \cdot d_{\text{LLM}} + 15 \cdot d_{\text{TTS}}, \\
& L_{\text{FPL-L}}^{\text{VALL-E}} =L \cdot d_{\text{LLM}} + 15 \cdot d_{\text{TTS}}, \\
&L_{\text{FPL-L}}^{\text{CosyVoice2}} =5 \cdot d_{\text{LLM}} + 15 \cdot d_{\text{TTS}}, \\
& L_{\text{FPL-L}}^{\text{MaskGCT}} =L \cdot d_{\text{LLM}} + b \cdot d_{\text{TTS}}, \\
& L_{\text{FPL-L}}^{\text{F5-TTS}} =L \cdot d_{\text{LLM}} + b \cdot d_{\text{TTS}}, \\
&L_{\text{FPL-L}}^{\text{SyncSpeech}} =(k+1) \cdot d_{\text{LLM}} + c \cdot d_{\text{TTS}},
\end{aligned}
$$

where $b$ represents the number of sampling iterations for the NAR model, and $c$ denotes the number of BPE text tokens when the generated speech tokens surpass the decoder's chunk size, typically ranging from 1 to 3.
Here, we assume the upstream LLM model is Qwen-7B, and when running on a single NVIDIA A800 GPU, we obtain an average token generation time $d_{LLM} = 25 ms$.
When the first term in FPL-L is omitted, it becomes FPL-A.
It is important to note that when calculating above metrics, we did not apply any engineering optimizations, such as KV cache.

We also conducted a brief theoretical analysis of RTF for ***SyncSpeech***.
The RTF for ***SyncSpeech*** is calculated as follows,

$$
L_{RTF} = \frac{ (L+1) \cdot d_{\text{TTS}}}{T\cdot F},
$$

where $L$ and $T$ represent the number of BPE tokens and speech tokens, respectively $F$ refers to the frame length of the speech tokens.
The time complexity for ***SyncSpeech*** to generate an entire sentence can be simplified to $O(L)$, whereas the time complexity for concurrent approaches, such as CosyVoice2 and IST-LM, is $O(T)$.
As a result, ***SyncSpeech*** can significantly expedite speech generation.

</td><td>

</td></tr></table>

### C·Duration Control

<table><tr><td width="50%">

Since we have implemented duration prediction and control, we can multiply the predicted durations by a modulation factor to adjust speech rate.
The results, shown in Table \ref{table7}, indicate that the robustness of synthesized speech is optimal when the modulation factor is 1.1.
However, when the modulation factor is too small or too large, the WER of the synthesized speech by ***SyncSpeech*** increases significantly.
This is because when we multiply the predicted duration of each text token by a fixed modulation factor of less than 1, ***SyncSpeech***'s contextual learning capability causes the subsequent tokens to be spoken increasingly faster, leading to a surge in WER.
When the modulation factor is set to 0.8, the average total duration of the synthesized speech is 0.68 times that when the modulation factor is 1.
Therefore, more reasonable duration control requires two inference processes: the duration obtained from the first inference is multiplied by a modulation factor during the second inference to control the speech rate.

</td><td>

</td></tr></table>

### D·Other Strategies for Sequence Construction

<table><tr><td width="50%">

We also experimented with other sequence construction strategies.
(1) One approach is to separate duration prediction and speech tokens prediction into two steps.
This method reduces efficiency by half but achieves better speech robustness, with a WER of around 2.75 on the LibriSpeech test-clean dataset.
(2) We also tried removing the duration placeholder and using the last speech token of the previous text token to predict the number of speech tokens corresponding to the current text token.
However, we found that this sequence construction made the corresponding pre-training less effective than it is now.
(3) We also attempted a method similar to ELLA-V \cite{ellav}, where the corresponding text token is placed before each placeholder.
However, we found that this sequence generated speech that was unnatural, with a noticeable disconnection between words.

</td><td>

</td></tr></table>
