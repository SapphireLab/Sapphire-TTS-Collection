# SyncSpeech

<details>
<summary>基本信息</summary>

- 标题: "SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based on Temporal Masked Transformer"
- 作者:
  - 01 Zhengyan Sheng,
  - 02 Zhihao Du,
  - 03 Shiliang Zhang,
  - 04 Zhijie Yan,
  - 05 Yexin Yang,
  - 06 Zhenhua Ling
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.11094)
  - [Publication]()
  - [Github]()
  - [Demo](https://syncspeech.github.io/)
- 文件:
  - [ArXiv](../_PDF/2502.11094v1__SyncSpeech__Low-Latency_and_Efficient_Dual-Stream_Text-to-Speech_based_on_Temporal_Masked_Transformer.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

This paper presents a dual-stream text-to-speech (TTS) model, ***SyncSpeech***, capable of receiving streaming text input from upstream models while simultaneously generating streaming speech, facilitating seamless interaction with large language models.
***SyncSpeech*** has the following advantages: Low latency, as it begins generating streaming speech upon receiving the second text token; High efficiency, as it decodes all speech tokens corresponding to the each arrived text token in one step.
To achieve this, we propose a temporal masked transformer as the backbone of ***SyncSpeech***, combined with token-level duration prediction to predict speech tokens and the duration for the next step.
Additionally, we design a two-stage training strategy to improve training efficiency and the quality of generated speech.
We evaluated the ***SyncSpeech*** on both English and Mandarin datasets.
Compared to the recent dual-stream TTS models, ***SyncSpeech*** significantly reduces the first packet delay of speech tokens and accelerates the real-time factor.
Moreover, with the same data scale, ***SyncSpeech*** achieves performance comparable to that of traditional autoregressive-based TTS models in terms of both speech quality and robustness.
Speech samples are available at [this https URL](https://syncspeech.github.io/).

</td><td>

本文展示了双流文本转语音模型 ***SyncSpeech***, 能够接收来自上游模型的流式文本输入, 同时生成流式语音, 实现和大型语言模型的无缝交互.
***SyncSpeech*** 有以下优点:
- 低延迟, 因为它在接收到第二个文本 Token 后立即开始生成流式语音;
- 高效率, 因为它在一步中解码所有与每个到达的文本 Token 对应的语音 Token.

为了实现这些特性, 我们提出了一个时序掩膜 Transformer 作为 ***SyncSpeech*** 的骨干, 并结合 Token 级别时长预测用于预测语音 Token 和下一步的时长.
此外, 我们设计了两阶段训练策略以提升训练效率和生成语音的质量.

我们在英语和中文数据集上评估了 ***SyncSpeech***.
与最近的双流 TTS 模型相比, ***SyncSpeech*** 显著降低了语音 Token 的首包延迟, 并加速了实时因子.
此外, 与传统的自回归 TTS 模型相比, 在相同数据尺度下, ***SyncSpeech*** 在语音质量和健壮性方面都达到了可比性.
语音样本可在 [此 https URL](https://syncspeech.github.io/) 获得.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td><td>

</td></tr></table>
