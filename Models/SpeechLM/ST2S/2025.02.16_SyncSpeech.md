# SyncSpeech

<details>
<summary>基本信息</summary>

- 标题: "SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based on Temporal Masked Transformer"
- 作者:
  - 01 Zhengyan Sheng,
  - 02 Zhihao Du,
  - 03 Shiliang Zhang,
  - 04 Zhijie Yan,
  - 05 Yexin Yang,
  - 06 Zhenhua Ling
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.11094)
  - [Publication]()
  - [Github]()
  - [Demo](https://syncspeech.github.io/)
- 文件:
  - [ArXiv](../_PDF/2502.11094v1__SyncSpeech__Low-Latency_and_Efficient_Dual-Stream_Text-to-Speech_based_on_Temporal_Masked_Transformer.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

This paper presents a dual-stream text-to-speech (TTS) model, ***SyncSpeech***, capable of receiving streaming text input from upstream models while simultaneously generating streaming speech, facilitating seamless interaction with large language models.
***SyncSpeech*** has the following advantages: Low latency, as it begins generating streaming speech upon receiving the second text token; High efficiency, as it decodes all speech tokens corresponding to the each arrived text token in one step.
To achieve this, we propose a temporal masked transformer as the backbone of ***SyncSpeech***, combined with token-level duration prediction to predict speech tokens and the duration for the next step.
Additionally, we design a two-stage training strategy to improve training efficiency and the quality of generated speech.
We evaluated the ***SyncSpeech*** on both English and Mandarin datasets.
Compared to the recent dual-stream TTS models, ***SyncSpeech*** significantly reduces the first packet delay of speech tokens and accelerates the real-time factor.
Moreover, with the same data scale, ***SyncSpeech*** achieves performance comparable to that of traditional autoregressive-based TTS models in terms of both speech quality and robustness.
Speech samples are available at [this https URL](https://syncspeech.github.io/).

</td><td>

本文展示了双流文本转语音模型 ***SyncSpeech***, 能够接收来自上游模型的流式文本输入, 同时生成流式语音, 实现和大型语言模型的无缝交互.
***SyncSpeech*** 有以下优点:
- 低延迟, 因为它在接收到第二个文本 Token 后立即开始生成流式语音;
- 高效率, 因为它在一步中解码所有与每个到达的文本 Token 对应的语音 Token.

为了实现这些特性, 我们提出了一个时序掩膜 Transformer 作为 ***SyncSpeech*** 的骨干, 并结合 Token 级别时长预测用于预测语音 Token 和下一步的时长.
此外, 我们设计了两阶段训练策略以提升训练效率和生成语音的质量.

我们在英语和中文数据集上评估了 ***SyncSpeech***.
与最近的双流 TTS 模型相比, ***SyncSpeech*** 显著降低了语音 Token 的首包延迟, 并加速了实时因子.
此外, 与传统的自回归 TTS 模型相比, 在相同数据尺度下, ***SyncSpeech*** 在语音质量和健壮性方面都达到了可比性.
语音样本可在 [此 https URL](https://syncspeech.github.io/) 获得.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

In recent years, with advancements in generative models and the expansion of training datasets, text-to-speech (TTS) models \cite{valle, voicebox, ns3} have made breakthrough progress in naturalness and quality, gradually approaching the level of real recordings.
However, low-latency and efficient dual-stream TTS, which involves processing streaming text inputs while simultaneously generating speech in real time, remains a challenging problem \cite{livespeech2}.
These models are ideal for integration with upstream tasks, such as large language models (LLMs) \cite{gpt4} and streaming translation models \cite{seamless}, which can generate text in a streaming manner.
Addressing these challenges can improve live human-computer interaction, paving the way for various applications, such as speech-to-speech translation and personal voice assistants.

Recently, inspired by advances in image generation, denoising diffusion \cite{diffusion, score}, flow matching \cite{fm}, and masked generative models \cite{maskgit} have been introduced into non-autoregressive (NAR) TTS \cite{seedtts, F5tts, pflow, maskgct}, demonstrating impressive performance in offline inference.
During this process, these offline TTS models first add noise or apply masking guided by the predicted duration.
Subsequently, context from the entire sentence is leveraged to perform temporally-unordered denoising or mask prediction for speech generation.
However, this temporally-unordered process hinders their application to streaming speech generation\footnote{
Here, “temporally” refers to the physical time of audio samples, not the iteration step $t \in [0, 1]$ of the above NAR TTS models.}.

When it comes to streaming speech generation, autoregressive (AR) TTS models \cite{valle, ellav} hold a distinct advantage because of their ability to deliver outputs in a temporally-ordered manner.
However, compared to recently proposed NAR TTS models, AR TTS models have a distinct disadvantage in terms of generation efficiency \cite{MEDUSA}.
Specifically, the autoregressive steps are tied to the frame rate of speech tokens, resulting in slower inference speeds.
While advancements like VALL-E 2 \cite{valle2} have boosted generation efficiency through group code modeling, the challenge remains that the manually set group size is typically small, suggesting room for further improvements.
In addition, most current AR TTS models \cite{dualsteam1} cannot handle stream text input and they only begin streaming speech generation after receiving the complete text, ignoring the latency caused by the streaming text input.
The most closely related works to SyncSpeech are CosyVoice2 \cite{cosyvoice2.0} and IST-LM \cite{yang2024interleaved}, both of which employ interleaved speech-text modeling to accommodate dual-stream scenarios.
However, their autoregressive process generates only one speech token per step, leading to low efficiency.

To seamlessly integrate with upstream LLMs and facilitate dual-stream speech synthesis, this paper introduces \textbf{SyncSpeech}, designed to keep the generation of streaming speech in synchronization with the incoming streaming text.
SyncSpeech has the following advantages:
(1) \textbf{low latency}, which means it begins generating speech in a streaming manner as soon as the second text token is received,
(2) \textbf{high efficiency}, which means for each arriving text token, only one decoding step is required to generate all the corresponding speech tokens.

SyncSpeech is based on the proposed \textbf{T}emporal \textbf{M}asked generative \textbf{T}ransformer (TMT).
During inference, SyncSpeech adopts the Byte Pair Encoding (BPE) token-level duration prediction, which can access the previously generated speech tokens and performs top-k sampling.
Subsequently, mask padding and greedy sampling are carried out based on the duration prediction from the previous step.

Moreover, sequence input is meticulously constructed to incorporate duration prediction and mask prediction into a single decoding step.
During the training process, we adopt a two-stage training strategy to improve training efficiency and model performance.
First, high-efficiency masked pretraining is employed to establish a rough alignment between text and speech tokens within the sequence, followed by fine-tuning the pre-trained model to align with the inference process.

Our experimental results demonstrate that, in terms of generation efficiency, SyncSpeech operates at 6.4 times the speed of the current dual-stream TTS model for English and at 8.5 times the speed for Mandarin.
When integrated with LLMs, SyncSpeech achieves latency reductions of 3.2 and 3.8 times, respectively, compared to the current dual-stream TTS model for both languages.
Moreover, with the same scale of training data, SyncSpeech performs comparably to traditional AR models in terms of the quality of generated English speech.
For Mandarin, SyncSpeech demonstrates superior quality and robustness compared to current dual-stream TTS models.
This showcases the potential of SyncSpeech as a foundational model to integrate with upstream LLMs.

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td><td>

</td></tr></table>
