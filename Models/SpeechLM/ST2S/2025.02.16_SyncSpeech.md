# SyncSpeech

<details>
<summary>基本信息</summary>

- 标题: "SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based on Temporal Masked Transformer"
- 作者:
  - 01 Zhengyan Sheng,
  - 02 Zhihao Du,
  - 03 Shiliang Zhang,
  - 04 Zhijie Yan,
  - 05 Yexin Yang,
  - 06 Zhenhua Ling
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.11094)
  - [Publication]()
  - [Github]()
  - [Demo](https://syncspeech.github.io/)
- 文件:
  - [ArXiv](../_PDF/2502.11094v1__SyncSpeech__Low-Latency_and_Efficient_Dual-Stream_Text-to-Speech_based_on_Temporal_Masked_Transformer.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

This paper presents a dual-stream text-to-speech (TTS) model, ***SyncSpeech***, capable of receiving streaming text input from upstream models while simultaneously generating streaming speech, facilitating seamless interaction with large language models.
***SyncSpeech*** has the following advantages: Low latency, as it begins generating streaming speech upon receiving the second text token; High efficiency, as it decodes all speech tokens corresponding to the each arrived text token in one step.
To achieve this, we propose a temporal masked transformer as the backbone of ***SyncSpeech***, combined with token-level duration prediction to predict speech tokens and the duration for the next step.
Additionally, we design a two-stage training strategy to improve training efficiency and the quality of generated speech.
We evaluated the ***SyncSpeech*** on both English and Mandarin datasets.
Compared to the recent dual-stream TTS models, ***SyncSpeech*** significantly reduces the first packet delay of speech tokens and accelerates the real-time factor.
Moreover, with the same data scale, ***SyncSpeech*** achieves performance comparable to that of traditional autoregressive-based TTS models in terms of both speech quality and robustness.
Speech samples are available at [this https URL](https://syncspeech.github.io/).

</td><td>

本文展示了双流文本转语音模型 ***SyncSpeech***, 能够接收来自上游模型的流式文本输入, 同时生成流式语音, 实现和大型语言模型的无缝交互.
***SyncSpeech*** 有以下优点:
- 低延迟, 因为它在接收到第二个文本 Token 后立即开始生成流式语音;
- 高效率, 因为它在一步中解码所有与每个到达的文本 Token 对应的语音 Token.

为了实现这些特性, 我们提出了一个时序掩膜 Transformer 作为 ***SyncSpeech*** 的骨干, 并结合 Token 级别时长预测用于预测语音 Token 和下一步的时长.
此外, 我们设计了两阶段训练策略以提升训练效率和生成语音的质量.

我们在英语和中文数据集上评估了 ***SyncSpeech***.
与最近的双流 TTS 模型相比, ***SyncSpeech*** 显著降低了语音 Token 的首包延迟, 并加速了实时因子.
此外, 与传统的自回归 TTS 模型相比, 在相同数据尺度下, ***SyncSpeech*** 在语音质量和健壮性方面都达到了可比性.
语音样本可在 [此 https URL](https://syncspeech.github.io/) 获得.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

In recent years, with advancements in generative models and the expansion of training datasets, text-to-speech (TTS) models \cite{valle, voicebox, ns3} have made breakthrough progress in naturalness and quality, gradually approaching the level of real recordings.
However, low-latency and efficient dual-stream TTS, which involves processing streaming text inputs while simultaneously generating speech in real time, remains a challenging problem \cite{livespeech2}.
These models are ideal for integration with upstream tasks, such as large language models (LLMs) \cite{gpt4} and streaming translation models \cite{seamless}, which can generate text in a streaming manner.
Addressing these challenges can improve live human-computer interaction, paving the way for various applications, such as speech-to-speech translation and personal voice assistants.

Recently, inspired by advances in image generation, denoising diffusion \cite{diffusion, score}, flow matching \cite{fm}, and masked generative models \cite{maskgit} have been introduced into non-autoregressive (NAR) TTS \cite{seedtts, F5tts, pflow, maskgct}, demonstrating impressive performance in offline inference.
During this process, these offline TTS models first add noise or apply masking guided by the predicted duration.
Subsequently, context from the entire sentence is leveraged to perform temporally-unordered denoising or mask prediction for speech generation.
However, this temporally-unordered process hinders their application to streaming speech generation\footnote{
Here, “temporally” refers to the physical time of audio samples, not the iteration step $t \in [0, 1]$ of the above NAR TTS models.}.

When it comes to streaming speech generation, autoregressive (AR) TTS models \cite{valle, ellav} hold a distinct advantage because of their ability to deliver outputs in a temporally-ordered manner.
However, compared to recently proposed NAR TTS models, AR TTS models have a distinct disadvantage in terms of generation efficiency \cite{MEDUSA}.
Specifically, the autoregressive steps are tied to the frame rate of speech tokens, resulting in slower inference speeds.
While advancements like VALL-E 2 \cite{valle2} have boosted generation efficiency through group code modeling, the challenge remains that the manually set group size is typically small, suggesting room for further improvements.
In addition, most current AR TTS models \cite{dualsteam1} cannot handle stream text input and they only begin streaming speech generation after receiving the complete text, ignoring the latency caused by the streaming text input.
The most closely related works to SyncSpeech are CosyVoice2 \cite{cosyvoice2.0} and IST-LM \cite{yang2024interleaved}, both of which employ interleaved speech-text modeling to accommodate dual-stream scenarios.
However, their autoregressive process generates only one speech token per step, leading to low efficiency.

To seamlessly integrate with upstream LLMs and facilitate dual-stream speech synthesis, this paper introduces \textbf{SyncSpeech}, designed to keep the generation of streaming speech in synchronization with the incoming streaming text.
SyncSpeech has the following advantages:
(1) \textbf{low latency}, which means it begins generating speech in a streaming manner as soon as the second text token is received,
(2) \textbf{high efficiency}, which means for each arriving text token, only one decoding step is required to generate all the corresponding speech tokens.

SyncSpeech is based on the proposed \textbf{T}emporal \textbf{M}asked generative \textbf{T}ransformer (TMT).
During inference, SyncSpeech adopts the Byte Pair Encoding (BPE) token-level duration prediction, which can access the previously generated speech tokens and performs top-k sampling.
Subsequently, mask padding and greedy sampling are carried out based on the duration prediction from the previous step.

Moreover, sequence input is meticulously constructed to incorporate duration prediction and mask prediction into a single decoding step.
During the training process, we adopt a two-stage training strategy to improve training efficiency and model performance.
First, high-efficiency masked pretraining is employed to establish a rough alignment between text and speech tokens within the sequence, followed by fine-tuning the pre-trained model to align with the inference process.

Our experimental results demonstrate that, in terms of generation efficiency, SyncSpeech operates at 6.4 times the speed of the current dual-stream TTS model for English and at 8.5 times the speed for Mandarin.
When integrated with LLMs, SyncSpeech achieves latency reductions of 3.2 and 3.8 times, respectively, compared to the current dual-stream TTS model for both languages.
Moreover, with the same scale of training data, SyncSpeech performs comparably to traditional AR models in terms of the quality of generated English speech.
For Mandarin, SyncSpeech demonstrates superior quality and robustness compared to current dual-stream TTS models.
This showcases the potential of SyncSpeech as a foundational model to integrate with upstream LLMs.

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

**Text-to-Speech**

Text-to-Speech, the transformation of text into audible signals understandable by humans, is pivotal for human-computer interaction.
TTS systems can be mainly divided into AR-based and NAR-based categories.

For AR-based systems, VALL-E \cite{valle} predicts the first layer of acoustic tokens extracted by EnCodec \cite{encodec} using an AR codec language model, while a NAR model is used to predict the remaining layers.
CosyVoice \cite{cosyvoice} employs an AR model to predict supervised semantic representations and combines flow matching to predict acoustic representations.
AR-based TTS models, with their in-context learning capability, can generate natural, prosody-diverse speech in a streaming manner.
However, AR-based TTS models exhibit shortcomings in generation efficiency.
Besides the previously mentioned VALL-E 2 \cite{valle2}, MEDUSA \cite{MEDUSA} and VALL-E R \cite{Valler} introduce speculative decoding \cite{spdeco} and a codec-merging method, respectively, to accelerate autoregressive generation.
Nonetheless, the efficiency gains achieved by these approaches remain limited, unable to perform synchronized decoding steps with text tokens.

For NAR-based TTS models, most previous approaches require speech duration prediction conditioned on the input text, followed by upsampling the text representations to match the acoustic feature length before feeding them into the generation model.
Following FastSpeech \cite{fastspeech2}, VoiceBox \cite{voicebox} and NaturalSpeech 2 \cite{ns2} predict phone-level durations using a regression-based approach.
NaturalSpeech 3 \cite{ns3} adopts a discrete diffusion model, combining classification loss and duration prompts for duration prediction, which outperforms text-dependent regression-based duration prediction in terms of speech robustness and quality.
However, NaturalSpeech 3 requires an additional duration prediction model, which complicates the pipeline, whereas SyncSpeech integrates duration and speech token predictions into a unified framework.
The NAR TTS model most relevant to SyncSpeech is MaskGCT \cite{maskgct}, which predicts the total duration of the speech and then performs temporally-unordered multi-step mask prediction.
Unlike MaskGCT, SyncSpeech employs temporally-ordered mask prediction and BPE token-level duration prediction to achieve speech generation in a dual-stream scenario.

</td><td>

</td></tr>
<tr><td>

**Speech Large Language Models**

Speech Large Language Models (SLLMs) empower LLMs to interact with users through speech, responding to user’s instruction with low latency \cite{wavchat}.
A basic approach \cite{audiogpt} to achieve this speech interaction involves a cascade of automatic speech recognition (ASR), LLM and TTS models, where the ASR transcribes the users' speech instruction into text, and the TTS model converts the LLM's textual response into speech.
However, most current AR TTS models cannot process streaming text input, resulting in significant latency in the aforementioned cascaded systems.
In contrast, some end-to-end speech-language models have been proposed that can generate speech tokens directly, thereby achieving extremely low response latency.
LLaMA-Omni \cite{llamaomni} aligns the hidden states of LLMs with discrete HuBERT \cite{hubert} representations using CTC loss, but the generated speech exhibits less natural prosody.
Mini-Omni \cite{mini1} employs a parallel decoder approach to generate text and speech tokens simultaneously.
However, due to the significantly longer length of speech tokens compared to text tokens, its generation efficiency remains low.
The proposed SyncSpeech can process streaming text input and generates speech in synchronization, with the potential to unite with LLMs to become end-to-end SLLMs.

</td><td>

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

A dual-stream TTS model simultaneously processes streaming text input and generates speech in a streaming manner.
Upon receiving newly generated text tokens $\boldsymbol{y}_{\text{arr}}$ from the upstream LLMs, the objective of the dual-streaming TTS it to estimate $p(\boldsymbol{x}_{\text{arr}} | \boldsymbol{y}_{\text{arr}}, \boldsymbol{x}_{\text{pre}},\boldsymbol{y}_{\text{pre}})$.
In this context, $\boldsymbol{x}_{\text{arr}}$ represents the speech waveform segment corresponding to $\boldsymbol{y}_{\text{arr}}$, while $\boldsymbol{y}_{\text{pre}}$ and $ \boldsymbol{x}_{\text{pre}}$ denote the preceding text tokens and its corresponding speech waveform, respectively.

SyncSpeech is a two-stage TTS system, consisting of the text-to-token and token-to-speech stages.
The estimation of $p(\boldsymbol{x}_{\text{arr}} | \boldsymbol{y}_{\text{arr}}, \boldsymbol{x}_{\text{pre}}, \boldsymbol{y}_{\text{pre}})$ is decomposed into a text-to-token model $p(\boldsymbol{s}_{\text{arr}}| \boldsymbol{y}_{\text{arr}}, \boldsymbol{x}_{\text{pre}}, \boldsymbol{y}_{\text{pre}})$ and a token-to-speech model $p(\boldsymbol{x}_{\text{arr}} | \boldsymbol{s}_{\text{arr}})$, where $\boldsymbol{s}_{\text{arr}}$ is the speech tokens corresponding to the speech waveform segment $\boldsymbol{x}_{\text{arr}}$.
Specifically, the proposed TMT is adopted as the backbone of text-to-token model.
Then, an off-the-shelf chunk-aware speech decoder \cite{cosyvoice2.0} is adopted as the token-to-speech model.

The proposed TMT module is based on a llama-style Transformer \cite{llama}.
We have specifically designed a novel attention mask to accommodate temporally-ordered mask generation.
Below, I will detail the two-stage training strategy of the TMT-based text-to-token model and its attention mask, cover the details of the other modules, and describe the inference process.

</td><td>

</td></tr></table>

### Training

<table><tr><td width="50%">

Given a dataset of transcribed speech ($\boldsymbol{\tilde{x}}$, $\boldsymbol{\tilde{y}}$), where $\boldsymbol{\tilde{x}}$ and $\boldsymbol{\tilde{y}}$ denote an audio sample and its transcript, respectively, the transcript $\boldsymbol{\tilde{y}}$ is tokenized into a BPE token sequence $\boldsymbol{y} = [y_1, y_2, y_3, ..., y_L]$, where $L$ is the number of BPE tokens.
An off-the-shelf speech tokenizer is used to encode the speech sample $\boldsymbol{\tilde{x}}$ into $T$ frame discrete speech tokens $\boldsymbol{s} = [s_1, s_2, s_3, ..., s_T]$.
We further define duration tokens $\boldsymbol{a} = [a_1, a_2, a_3, ..., a_L]$ as the positions indicating the end time of each corresponding BPE token within the speech token sequence, with $a_L = T.$ For a pair of ($\boldsymbol{\tilde{x}}$, $\boldsymbol{\tilde{y}}$), $\boldsymbol{a}$ can be obtained through an open-source alignment tool.

As shown in Figure \ref{fig1}, to maintain consistency with the inference process (see Section \ref{sec:inference}), the sequence input is then constructed as follows.
We select a random number $n \in [1, L]$, which indicates that when receiving streaming text input, SyncSpeech needs to generate the speech tokens corresponding to the $n$-th BPE token at this moment.
To avoid unnatural pauses, SyncSpeech allows look ahead $q$ text tokens, obtaining a truncated text token sequence $\boldsymbol{y}' = [y_1, y_2, y_3, ..., y_{L'}]$, where $L'=min(L, n+q)$.
Based on the duration tokens $\boldsymbol{a}$, the truncated speech token sequence $\boldsymbol{s}_{1:a_{n}}=[s_1, s_2, ...., s_{a_{n}}]$ is obtained.
Then, we define the masked speech token sequence $\boldsymbol{s}'$ and and corresponding binary mask
$\boldsymbol{m}$ as follows,
$$
\boldsymbol{s}' = \boldsymbol{s}_{1:a_{n}} \odot \boldsymbol{m},
$$

$$
\boldsymbol{m}=[m_{i}]_{i=1}^{a_{n}}, \boldsymbol{m}_{1:a_{n-1}}=0, \boldsymbol{m}_{a_{n-1}:a_{n}}=1.
$$

That is all speech tokens corresponding to $\boldsymbol{x}_{n}$ are replaced with the specific mask token, while the rest remain unchanged.
Then, the truncated text token sequence $\boldsymbol{y}'$, along with the masked speech token sequence $\boldsymbol{s}'$ and duration tokens $\boldsymbol{a}$, are used to construct the input sequence as follows,
$$
\boldsymbol{f} = [\boldsymbol{y}',E, D, \boldsymbol{s}'_{1:a_1},..., D, \boldsymbol{s}'_{a_{n-1}:a_n}, D],
$$

where $E$ is end-of-text token, $D$ is a placeholder for duration prediction.
Based on the duration tokens $\boldsymbol{a}$, $D$ is used to separate the masked speech token sequence corresponding to different BPE tokens.
In practice, $E$ is inserted only when $n=L$ .

The sequence $\boldsymbol{f}$ is used as input for the TMT with the mask prediction and duration prediction as training objectives.
Specifically, the sequence $\boldsymbol{f}$ is fed into the TMT forward to obtain the hidden states, which then pass through two different linear layers to predict the speech tokens corresponding to text token $y_{n}$ and the duration of the \textbf{next text token} $y_{n+1}$.
This enables us to integrate duration prediction and mask prediction into a single decoding step during inference, except for the first text token duration prediction (Details are provided in Section \ref{sec:inference}).
We minimize the following negative log-likelihood function for masked generative training and duration training,
$$
\mathcal{L}_{\text {mask}}^{1}=-\log p \left( \boldsymbol{s}_{a_{n-1}:{a_n}} \mid \boldsymbol{f}; \theta \right),
$$

$$
\mathcal{L}_{\text {duration}}^{1}=-\log p \left(l_{n+1}\mid \boldsymbol{f}; \theta \right),
$$

where $\theta$ represents the neural network parameters of TMT, $l_{n+1}=a_{n+1} - a_{n}$ and $a_0=0$.
In this way, we simulate the scenario of receiving streaming text input during the training process and are able to generate speech in sync.

We design a corresponding attention mask, as shown in Figure \ref{fig1}.
Specifically, a causal mask is used for the truncated text sequence $\boldsymbol{y}'$ and duration placeholder parts.
For the masked speech token sequence $\boldsymbol{s}'$, a dynamic chunk attention mask is applied based on the duration tokens $\boldsymbol{a}$, enabling it to attend all historical tokens, as well as all speech tokens and mask tokens corresponding to their own text BPE tokens.

</td><td>

</td></tr></table>

### Pretraining

<table><tr><td width="50%">

While the aforementioned method aligns with the prediction process, it suffers from low training efficiency.
This training inefficiency arises because, during each training step, only the gradients of speech tokens $\boldsymbol{s}_{a_{n-1}:{a_n}}$ and durations for $y_{n+1}$ are backpropagated.
To further improve the training efficiency, we first perform masked pre-training on the TMT.

Given speech tokens $\boldsymbol{s}$ of a speech sample, we obtain the masked speech tokens $\boldsymbol{\hat{s}} = \boldsymbol{s} \odot \boldsymbol{\hat{m}} $, where $\boldsymbol{\hat{m}}=[\hat{m}_{i}]_{i=1}^{a_{L}} $ is a binary mask of speech tokens.
We design the masking rules primarily from two perspectives, high masking probability and consistency with the prediction process as much as possible.
Specifically, the binary mask $\boldsymbol{\hat{m}}_\text{bpe}$ of text tokens is constructed first, where the first value is distributed according to a Bernoulli distribution ($p=0.5$) and the subsequent adjacent values cannot be the same.
Based on the duration tokens $\boldsymbol{a}$, the text token mask $\boldsymbol{\hat{m}}_\text{bpe}$ is converted into the corresponding speech token mask $\boldsymbol{\hat{m}}$.
Then, we build the following sequence as the input for TMT,
$$
\boldsymbol{\hat{f}} = [\boldsymbol{y},E, D, \boldsymbol{\hat{s}}_{1:a_1},..., D, \boldsymbol{\hat{s}}_{a_{L-1}:a_L}],
$$

and the TMT is optimized to minimize the negative log-likelihood for masked generative training and duration training as follows,
$$
\mathcal{L}_{\text {mask}}^{2} =- \sum_{\substack{j \in J}} \log p \left(\boldsymbol{s}_{a_{j-1}:a_{j}} \mid \boldsymbol{\hat{f}}_{\leq a_{j}}; \theta \right),
$$

$$
\mathcal{L}_{\text {duration}}^{2} =- \sum_{\substack{j \in J}} \log p \left(l_{j}\mid \boldsymbol{\hat{f}}_{\leq a_{j-1}}; \theta \right),
$$

where $J$ denote the sequence where each element satisfies the condition $\hat{m}_{j} = 1$,
$\boldsymbol{\hat{f}}_{\leq a_{j}}=[\boldsymbol{y},E, D,\boldsymbol{\hat{s}}_{1:a_1},..., D, \boldsymbol{\hat{s}}_{a_{j-1}:a_j}]$ and $l_{j} = a_{j} - a_{j-1}$.
Additionally, the attention mask mentioned above is also utilized for pretrain.

In summary, an efficient masked pretraining is initially performed with a high masking probability to facilitate the alignment between text and speech tokens.
Subsequently, we fine-tune the pretrained model using a training strategy consistent with the prediction process.
This approach enhances the efficiency of the training process, and the masked pretraining also contributes to the robustness of the generated speech.

</td><td>

</td></tr></table>

### Other Modules

<table><tr><td width="50%">

In this subsection, we introduce the other modules in SyncSpeech besides TMT.
1) Text BPE tokenizer: To facilitate interaction with upstream LLMs, we utilize the Qwen tokenizer \cite{qwen} directly.
2) Speech tokenizer: the open-source supervised speech semantic (S3) tokenizer \cite{cosyvoice2.0} is selected, which operates at 25 Hz.
The S3 tokenizer is developed by integrating finite scalar quantization (FSQ) \cite{fsq} into the intermediate representations of an ASR model trained on large-scale data, and then fine-tuning it for the ASR task.
2) The off-the-shelf speech decoder \cite{cosyvoice2.0} is based on the conditional flow matching (CFM) decoder and HiFi-GAN vocoder \cite{hifigan}.
The CFM decoder employs a chunk-aware training strategy, enabling the streaming generation of Mel-spectrograms from the chunk-size input speech tokens.
These Mel-spectrograms are then converted into speech using the vocoder, which operates in parallel with a fully convolutional network.

</td><td>

</td></tr></table>

### Inference

<table><tr><td width="50%">

During the inference process, SyncSpeech processes text in a streaming manner and synchronously generates speech, with the general algorithm flow shown in Algorithm \ref{alg1}.
Specifically, when the number of input text BPE tokens $\boldsymbol{y}$ exceeds the look-ahead number $q$, the input sequence $\boldsymbol{f} = [\boldsymbol{y}, D]$ is built, which is fed into TMT to predict the duration of speech tokens corresponding to $y_1$.
Then, based on the predicted duration, we perform sequence padding by inserting the mask tokens and a duration prediction placeholder.
Subsequently, the sequence is fed back into TMT for synchronous mask prediction of $y_1$ and the duration prediction of $y_2$, followed by the input sequence $\boldsymbol{s}$ update and padding.
For subsequent BPE token input, the above prediction step, update step, and padding step are repeated to generate speech tokens in a streaming manner.
In the process described above, once the number of generated speech tokens surpasses the chunk size of the off-the-shelf speech decoder, these tokens and the speaker prompt can be utilized to stream speech output.

Additionally, existing speech tokens can be accessed during duration prediction and speech token generation, which allows SyncSpeech to control the prosody of the generated speech with in-context learning.
Specifically, given a speech prompt, we construct the prompt sequence according to Equation \ref{eq1}, serving as the generated sequence for prosody control.
Figure \ref{fig2} in the Appendix shows detailed inference visualizations.

</td><td>

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

**Datasets**

We trained SyncSpeech on datasets in both English and Mandarin, including the 585-hour LibriTTS \cite{libritts} dataset and 600 hours of internal Mandarin datasets.
The internal Mandarin dataset was further expanded to approximately 2000 hours, employing techniques such as speed alteration and pitch shifting.
The Montreal Forced Aligner (MFA) \cite{mfa} aligned transcripts according to its phone set, after which the alignment was transformed into text BPE-level format.
We evaluated SyncSpeech using three benchmarks: (1) LibriSpeech \textit{text-clean} \cite{librispeech}, a standard English TTS evaluation set; (2) SeedTTS \textit{test-zh} \cite{seedtts}, with 2,000 samples from the out-of-domain Mandarin DiDiSpeech dataset \cite{didispeech}; and (3) SeedTTS \textit{test-hard}, containing approximately 400 difficult cases to evaluate TTS model robustness with repeated text, tongue twisters, and other complex synthesis scenarios.

</td><td>

</td></tr>
<tr><td>

**Settings**

We set the number of text tokens to look ahead $q=1$.
The chunk size of speech decoder is 15.
TMT has 16 layers, 16 attention heads, 1024-dimensional embeddings, and 2048-dimensional feed-forward layers.
SyncSpeech was trained on 4 NVIDIA A800 80G GPUs.
The pre-training stage lasts for 70K steps, and the second stage lasts for 20K steps.

</td><td>

</td></tr>
<tr><td>

**Baseline Models**

This paper focuses on low-latency and efficient TTS in dual-stream scenarios.
Under the same data scale, we reproduced the following baseline models for comparison: CosyVoice \cite{cosyvoice} and recently proposed CosyVoice2 \cite{cosyvoice2.0}.
CosyVoice requires complete text input before speech generation.
CosyVoice2 uses interleaved text-speech modeling to process streaming text input and simultaneously generate streaming speech.
We trained CosyVoice, CosyVoice2, and SyncSpeech using the same speech tokenizer and text tokenizer, and employed the same open-source streaming speech decoder.
We utilized the official code\footnote{https://github.com/FunAudioLLM/CosyVoice} to reproduce the model and adopted a Llama-style Transformer, matching the size of SyncSpeech, as the backbone of the text-to-speech model.
Additionally, we compared the open-sourced TTS model MaskGCT \cite{maskgct}, F5-TTS \cite{F5tts}, and VALL-E \cite{valle}, which were trained on large-scale data.
More details about baseline models can be found in the Appendix \ref{baselines}.

</td><td>

</td></tr>
<tr><td>

**Evaluation Metrics**

For the three benchmarks, we evaluated speech quality, latency, and efficiency.
For speech robustness, we chose Whisper-V3 and Paraformer as the ASR models for English and Mandarin, respectively, to transcribe the generated speech.
Then, we calculated the WER compared to the original transcriptions to evaluate the spech robustness.
We adopted the ERes2Net-based \cite{eres2net} speaker verification model\footnote{https://github.com/modelscope/3D-Speaker} to evaluate speaker similarity (SS).
We selected 100 sentences from each system and invited 10 native listeners to conduct a subjective MOS evaluation for speech naturalness (MOS-N), scoring from 1 to 5.
In terms of latency and efficiency, we compared the performance of various models on a single A800 GPU.
Due to the off-the-shelf speech decoder, we evaluate the latency and efficiency of the text-to-token stage across all models, except for F5-TTS.
We calculated the time required for the number of speech tokens to reach the chunk size of the speech decoder as First-packet latency (FPL).
There are two scenarios: one assumes the text is already available (FPL-A), while the other involves receiving output from the upstream LLM model (FPL-L), accounting for the time required for text generation.
For the real-time factor (RTF), we measure the ratio of the total duration of generated speech to the total time taken by the model.
More details about FPL and RTF can be found in the Appendix \ref{evaluation metrics}.

</td><td>

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

The evaluation results for SyncSpeech and the baseline models are presented in Table \ref{table1}.

</td><td>

</td></tr>
<tr><td>

**Speech Robustness**

We found that SyncSpeech exhibits different performance compared to the baselines across the three benchmarks.
Specifically, on the LibriSpeech \textit{test-clean} benchmark, the performance of SyncSpeech was very close to that of CosyVoice2 based on the WER metric, with only a minor difference of 0.07\%.
SyncSpeech achieved a lower WER score on the Seed \textit{test-zh} set compared to CosyVoice and CosyVoice2, with improvements of 0.65\% and 0.93\%, respectively.
A key difference between the English and Mandarin datasets is the higher compression rate of the LLM tokenizer for Mandarin.
In English, one word typically equals one token, while in Mandarin, a common phrase often corresponds to a single token.
This means that, compared to the baseline model, SyncSpeech is better suited to the high compression rate tokenizer of the upstream large model.
Furthermore, on the Seed \textit{test-hard} set, the robustness advantage of SyncSpeech was even more pronounced, with the improvements 9.05\% and 4.40\%, respectively.
In handling complex text, the explicit duration modeling in SyncSpeech helped the model learn the alignment between text and speech.

</td><td>

</td></tr>
<tr><td>

**Speaker Similarity**
Due to the same speech decoder and the excellent voice disentanglement capability of the speech tokens, SyncSpeech, CosyVoice, and CosyVoice2 exhibited similar performance in terms of speaker similarity.

</td><td>

</td></tr>
<tr><td>

**Speech Naturalness**

The MOS-N scores for SyncSpeech and CosyVoice2 were quite similar on the LibriSpeech \textit{text-clean}, indicating that the naturalness of the generated speech was generally comparable.
On the Seed \textit{test-zh} benchmark, SyncSpeech outperformed CosyVoice2 by 0.08.
In the Seed \textit{test-hard} benchmark, high WER and uncommon text led to unnatural prosody and generally low MOS-N scores in the generated speech.

</td><td>

</td></tr>
<tr><td>

**Latency**

SyncSpeech has made a breakthrough in terms of latency, as shown in Table \ref{table1}.
Specifically, on the LibriSpeech \textit{test-clean} benchmark, SyncSpeech was approximately 4 times faster than traditional AR models and over 20 times faster than the SOTA offline models in terms of FPL-A.
On the Seed \textit{test-zh} benchmark, SyncSpeech achieved speed improvements of over 5 times and 30 times, respectively.
When receiving streaming text from the upstream large model (FPL-L), SyncSpeech can begin generating speech with just two text tokens.
In contrast, CosyVoice2 requires five tokens, while CosyVoice and other baseline models need the entire text input.
This highlights the distinct advantage of SyncSpeech in practical applications.

</td><td>

</td></tr>
<tr><td>

**Efficiency**

In terms of RTF, SyncSpeech is about 6.4 times faster on the LibriSpeech \textit{test-clean} benchmark and about 8.6 times faster on the Seed \textit{test-zh} benchmark compared to previous AR models.
On the Seed \textit{test-hard} set, due to the increased number of text tokens caused by the uncommon text, the efficiency of SyncSpeech is slightly reduced.
Theoretically, the time complexity of AR models is $O(T)$, while the time complexity of SyncSpeech is $O(L)$, where $T$ represents the number of speech tokens and
$L$ denotes the number of text tokens, thereby significantly improving efficiency.

</td><td>

</td></tr></table>

### Analysis

<table><tr><td width="50%">

**Sampling Strategy**

In the LibriSpeech validation set, we provided the ground-truth durations and applied greedy search along with different Top-k thresholds for duration prediction, as shown in Table \ref{table3}.
We found that, in terms of speech robustness, both Top-k 3 and greedy search outperformed the use of ground-truth durations in terms of the WER metric.
This is because the model struggled to effectively generalize to anomalies in the ground-truth durations.
We employed
UTMOSv2\footnote{https://github.com/sarulab-speech/UTMOS22} as a surrogate objective metric of MOS-N.
In terms of speech naturalness, the results of Top-k 3 sampling are slightly better than those with the given ground-truth durations.
Additionally, we applied different Top-k thresholds for speech token prediction.
SyncSpeech exhibited superior performance during greedy search, which is different from the previous AR TTS models or offline models.
This is because the speech tokens obtained through single-step decoding have the temporal dependency, which cannot be compensated by subsequent generation.

</td><td>

</td></tr>
<tr><td>

**Number of Look-ahead Tokens**

We evaluated how varying the number of tokens to look ahead affects speech robustness and speech naturalness on two validation sets, with the results presented in Table \ref{table5}.
We discovered that the optimal number of look-ahead text tokens varies across different languages in terms of WER performance.
This is influenced by the difference in the compression rate of text tokens and the contextual dependency in different languages.
In terms of speech naturalness, when the look-ahead number $q$ is greater than $2$, the generated speech exhibits slightly more natural pauses and speed, but it results in increased latency.

</td><td>

</td></tr>
<tr><td>

**Ablation Study**

We conducted an ablation study on the pre-training strategy by directly training the randomly initialized model in a manner consistent with the prediction process.
The WER results on the two validation sets are shown in Table \ref{table6}.
We found that pre-training significantly improved the speech robustness of the model, improving the WER metric by 1.17\% and 1.06\% on the two languages, respectively.
This indicated that masked pre-training not only improved training efficiency but also enhanced the robustness of the synthesized speech.
Additionally, a standard causal attention mask was applied to replace the designed attention mask, as shown in Table \ref{table6}.
If the mask token sequence of the same text token cannot attend to each other during inference, the robustness of the generated speech significantly decreased.
This further demonstrated the effectiveness of the designed attention mask.

</td><td>

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td><td>

</td></tr></table>
