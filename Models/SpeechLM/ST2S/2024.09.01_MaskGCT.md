# MaskGCT

<details>
<summary>基本信息</summary>

- 标题: "MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer"
- 作者:
  - 01 Yuancheng Wang,
  - 02 Haoyue Zhan,
  - 03 Liwei Liu,
  - 04 Ruihong Zeng,
  - 05 Haotian Guo,
  - 06 Jiachen Zheng,
  - 07 Qiang Zhang,
  - 08 Xueyao Zhang,
  - 09 Shunsi Zhang,
  - 10 Zhizheng Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2409.00750)
  - [Publication]()
  - [Github](https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct)
  - [Demo](https://maskgct.github.io/)
- 文件:
  - [ArXiv](../_PDF/2409.00750v3__MaskGCT__Zero-Shot_Text-to-Speech_with_Masked_Generative_Codec_Transformer.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems.
The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability.
Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g., phone), which may compromise their naturalness.
In this paper, we introduce ***Masked Generative Codec Transformer (MaskGCT)***, a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction.
***MaskGCT*** is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens.
***MaskGCT*** follows the mask-and-predict learning paradigm.
During training, ***MaskGCT*** learns to predict masked semantic or acoustic tokens based on given conditions and prompts.
During inference, the model generates tokens of a specified length in a parallel manner.
Experiments with 100K hours of in-the-wild speech demonstrate that ***MaskGCT*** outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility.
Audio samples are available at [this https URL](https://maskgct.github.io/).
We release our code and model checkpoints at [this https URL](https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct).

</td><td>

近期的大规模文本转语音系统通常分为自回归和非自回归系统.
- 自回归系统隐式建模了模型时长, 但在鲁棒性和持续时长控制性方面存在一些缺陷.
- 非自回归系统要求在训练时显式对齐文本和语音之间的信息, 并预测语言单元 (如音素) 的持续实践, 这可能妨碍了它们的自然度.

本文介绍了 ***掩膜生成式编解码 Transformer (Masked Generative Codec Transformer, MaskGCT)***, 一种完全非自回归的文本转语音模型, 消除了显式对齐文本和语音监督之间的信息, 以及音素级时长预测的需求.
***MaskGCT*** 是一种两阶段模型: 在第一阶段, 模型使用文本预测语音自监督学习 (SSL) 模型提取的语义 Token, 而在第二阶段, 模型根据这些语义 Token 为条件预测声学 Token.
***MaskGCT*** 遵循掩膜-预测学习范式.
在训练时, ***MaskGCT*** 学习根据给定的条件和提示预测掩盖的语义或声学 Token.
在推理时, 模型以并行的方式生成指定长度的 Token.

在 100K 小时的真实语音数据集上的实验表明, ***MaskGCT*** 在质量, 语义相似度, 和流畅度方面优于当前最先进的零样本 TTS 系统.
音频样本可以在 [https URL](https://maskgct.github.io/) 获得.
我们在 [https URL](https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct) 发布了我们的代码和模型检查点.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

In recent years, large-scale zero-shot text-to-speech (TTS) systems (**SPEAR-TTS**[^01], **VALL-E**[^02], **BASE-TTS**[^03], **CLaM-TTS**[^04], **VoiceCraft**[^05], **Seed-TTS**[^06], **NaturalSpeech2**[^07], **NaturalSpeech3**[^08], **Voicebox**[^09], **MegaTTS**[^10]) have achieved significant improvements by scaling data and model sizes, including both autoregressive (AR) (**SPEAR-TTS**[^01], **VALL-E**[^02], **BASE-TTS**[^03], **CLaM-TTS**[^04], **VoiceCraft**[^05], **Seed-TTS**[^06]) and non-autoregressive (NAR) models (**NaturalSpeech2**[^07], **NaturalSpeech3**[^08], **Voicebox**[^09], **MegaTTS**[^10]).
However, both AR-based and NAR-based systems still exhibit some shortcomings.
In particular, AR-based TTS systems typically quantize speech into discrete tokens and then use decoder-only models to autoregressively generate these tokens, which offer diverse prosody but also suffer from problems such as poor robustness and slow inference speed.
NAR-based models, typically based on diffusion (**NaturalSpeech3**[^08], **NaturalSpeech2**[^07]), flow matching (**Voicebox**[^09]), or GAN (**MegaTTS**[^10]), require explicit text and speech alignment information as well as the prediction of phone-level duration, resulting in a complex pipeline and producing more standardized but less diverse speech.

Recently, masked generative transformers, a class of generative models, have achieved significant results in the fields of image ([^11], [^12], [^13]), video ([^14], [^15]), and audio ([^16], [^17], [^18]) generation, demonstrating potential comparable to or superior to autoregressive models or diffusion models.
These models employ a mask-and-predict training paradigm and utilize iterative parallel decoding during inference.
Some previous works have attempted to introduce masked generative models into the field of TTS.
**SoundStorm**[^19] was the first attempt to use a masked generative transformer to predict multi-layer acoustic tokens extracted from SoundStream, conditioned on speech semantic tokens; however, it needs to receive the semantic tokens of an AR model as input.
Thus, SoundStorm is more of an acoustic model that converts semantic tokens into acoustic tokens and does not fully utilize the powerful generative potential of masked generative models.
**NaturalSpeech3**[^08] decomposes speech into discrete token sequences representing different attributes through special designs and generates tokens representing different attributes through masked generative models.
However, it still needs speech-text alignment supervision and phone-level duration prediction.

In this work, we propose ***MaskGCT***, a fully non-autoregressive model for text-to-speech synthesis that uses masked generative transformers without requiring text-speech alignment supervision and phone-level duration prediction.
***MaskGCT*** is a two-stage system, both stages are trained using the mask-and-predict learning paradigm.
The first stage, the text-to-semantic (T2S) model, predicts masked semantic tokens with in-context learning, using text token sequences and prompt speech semantic token sequences as the prefix, without explicit duration prediction.
The second stage, the semantic-to-acoustic (S2A) model, utilizes semantic tokens to predict masked acoustic tokens extracted from an RVQ-based speech codec with prompt acoustic tokens.
During inference, ***MaskGCT*** can generate semantic tokens of various specified lengths with a few iteration steps given a sequence of text.
% , which is significantly fewer than AR-based models.
In addition, we train a **VQ-VAE**[^20] to quantize speech self-supervised learning embedding, rather than using k-means to extract semantic tokens that is common in previous work.
This approach minimizes the information loss of semantic features even with a single codebook.
We also explore the scalability of our methods beyond the zero-shot TTS task, such as speech translation (cross-lingual dubbing), speech content editing, voice conversion, and emotion control, demonstrating the potential of ***MaskGCT*** as a foundational model for speech generation.
Table~\ref{table:overview} shows a comparison between ***MaskGCT*** and some previous works.

Our experiments demonstrate that ***MaskGCT*** has achieved performance comparable to or superior to that of existing models in terms of speech quality, similarity, prosody, and intelligibility.
Specifically, (1) It achieves comparable or better quality and naturalness than the ground truth speech across three benchmarks (LibriSpeech, SeedTTS test-en, and SeedTTS test-zh) in terms of CMOS.
(2) It achieves human-level similarity between the generated speech and the prompt speech, with improvements of +0.017, -0.002, and +0.027 in SIM-O and +0.28, +0.32 and +0.25 in SMOS for LibriSpeech, SeedTTS test-en, and SeedTTS test-zh, respectively.
(3) It achieves comparable intelligibility in terms of WER across the three benchmarks and demonstrates stability within a reasonable range of speech duration, which also indicates the diversity and controllability of the generated speech.

In summary, we propose a non-autoregressive zero-shot TTS system based on masked generative transformers and introduce a speech discrete semantic representation by training a VQ-VAE on speech self-supervised representations.
Our system achieves human-level similarity, naturalness, and intelligibility by scaling data to 100K hours of in-the-wild speech, while also demonstrating high flexibility, diversity, and controllability.
We investigate the scalability of our system across various tasks, including cross-lingual dubbing, voice conversion, emotion control, and speech content editing, utilizing zero-shot learning or post-training methods.
This showcases the potential of our system as a foundational model for speech generation.

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

**Large-scale TTS**

Traditional TTS systems (**FastSpeech2**[^21], **FastSpeech**[^22], **NaturalSpeech**[^23], **Tacotron**[^24], **VITS**[^25]) are trained to generate speech from a single speaker or multiple speakers using hours of high-quality transcribed training data.
Modern large-scale TTS systems (**SPEAR-TTS**[^01], **VALL-E**[^02], **BASE-TTS**[^03], **CLaM-TTS**[^04], **VoiceCraft**[^05], **Seed-TTS**[^06]) aim to achieve zero-shot TTS (synthesizing speech for unseen speakers with speech prompts) by scaling both the model and data size.
These systems can be mainly divided into AR-based and NAR-based categories.
For AR-based systems:
**SPEAR-TTS**[^01] utilizes three AR models to predict semantic tokens from text, coarse-grained acoustic tokens from semantic tokens, and fine-grained acoustic tokens from coarse-grained tokens.
**VALL-E**[^02] predicts the first layer of acoustic tokens extracted from **EnCodec**[^26] using an AR codec language model, and the final layers with a NAR model.
**VoiceCraft**[^05] employs a single AR model to predict multi-layer acoustic tokens in a delayed pattern (**MusicGen**[^27]).
**BASE-TTS**[^03] predicts novel speech codes extracted from WavLM features and uses a GAN model for waveform reconstruction.
For NAR-based systems:
**NaturalSpeech2**[^07] employs latent diffusion to predict the latent representations from a codec model (**SoundStream**[^28]).
**Voicebox**[^09] uses flow matching and in-context learning to predict mel-spectrograms.
**MegaTTS**[^10] utilizes a GAN to predict mel-spectrograms, while an AR model predicts phone-level prosody codes.
**NaturalSpeech3**[^08] employs a unified framework based on discrete diffusion models to predict discrete representations of different speech attributes.
However, these NAR systems need to predict phoneme-level duration, leading to a complex pipeline and more standardized generative results.
**SimpleSpeech**[^29], **DiTTo-TTS**[^30], and **E2 TTS**[^31] are also NAR-based models that do not require precise alignment information between text and speech, nor do they predict phoneme-level duration.
We discuss these concurrent works in Appendix~\ref{appendix:concurrent_works}.

**Masked Generative Model**

Masked generative transformers, a class of generative models, achieve significant results and demonstrate potential comparable to or superior to that of autoregressive models or diffusion models in the fields of image[^11], [^12], [^32], [^13], video[^14], [^15], and audio (**SoundStorm**[^19], [^16], [^17], [^18]) generation.
MaskGIT[^11] is the first work to use masked generative models for both unconditional and conditional image generation.
Subsequently, Muse[^12] leverages rich text to achieve high-quality and diverse text-to-image generation within the same framework.
MAGVIT-v2[^15] employs masked generative models with novel lookup-free quantization, outperforming diffusion models in image and video generation.
Recently, some efforts have been made to adapt masked generative models to the field of audio.
**SoundStorm**[^19] takes in the semantic tokens from AudioLM and utilizes this generative paradigm to generate tokens for a neural audio codec (**SoundStream**[^28]).
VampNet[^16] and MAGNeT[^18] apply masked generative models for music and audio generation, while MaskSR[^17] extends these models for speech restoration.

**Discrete Speech Representation**

Speech representation is a crucial aspect of speech generation.
Early works (**FastSpeech**[^22], **Tacotron**[^24]) typically utilized mel-spectrograms as the modeling target.
Recently, some large-scale TTS systems (**VALL-E**[^02], **NaturalSpeech3**[^08]) have shifted to using discrete speech representations.
Discrete speech representation can be primarily divided into two types: semantic discrete representation and acoustic discrete representation.
We give a more detailed discussion about the definitions of "semantic" and "acoustic" in Appendix~\ref{appendix:definition}.

Semantic discrete representations are mainly extracted from various speech SSL models (**W2V-BERT**[^33], **HuBERT**[^34], **WavLM**[^35]) using quantization methods such as k-means.
Acoustic discrete representations, on the other hand, are usually obtained by training a VQ-GAN model (**VQ-VAE**[^20]) with the goal of waveform reconstruction, as seen in speech codecs (**EnCodec**[^26], **SoundStream**[^28], **DAC**[^36]).
Semantic discrete representation typically shows a stronger correlation with text, whereas acoustic discrete representation more effectively reconstructs audio.
Consequently, some two-stage TTS models predict both semantic and acoustic tokens.
**FACodec**[^08] is a novel speech codec that disentangles speech into subspaces of different attributes, including content, prosody, timbre, and acoustic details.

</td><td>

</td></tr></table>

## 3·Methodology: 方法

### Background: Non-Autoregressive Masked Generative Transformer

<table><tr><td width="50%">

Given a discrete representation sequence $\mathbf{X}$ of some data, we define $\mathbf{X}_t = \mathbf{X} \odot \mathbf{M}_t$ as the process of masking a subset of tokens in $\mathbf{X}$ with the corresponding binary mask $\mathbf{M}_t=[m_{t,i}]_{i=1}^{N}$.
Specifically, this involves replacing $x_i$ with a special $\text{[MASK]}$ token if $m_{t,i}=1$, and otherwise leaving $x_i$ unmasked if $m_{t,i}=0$.
Here, each $m_{t,i}$ is independently and identically distributed according to a Bernoulli distribution with parameter $\gamma(t)$, where $\gamma(t) \in (0,1]$ represents a mask schedule function (for example, $\gamma(t) = \sin(\frac{\pi t}{2T}), t \in (0,T]$).
We denote $\mathbf{X}_0 = \mathbf{X}$.
The non-autoregressive masked generative transformers are trained to predict the masked tokens based on the unmasked tokens and a condition $\mathbf{C}$.
This prediction is modeled as $p_{\theta}(\mathbf{X}_0|\mathbf{X}_t, \mathbf{C})$.
The parameters $\theta$ are optimized to minimize the negative log-likelihood of the masked tokens:
$$
\begin{aligned}
\mathcal{L}_{\text{mask}} &= \mathop{\mathbb{E}}\limits_{\mathbf{X} \in \mathcal{D}, t \in \left[0, T\right]}  -\sum_{i=1}^{N} m_{t,i} \cdot \log(p_{\theta}(x_i|\mathbf{X}_{t}, \mathbf{C})).
\end{aligned}
$$

At the inference stage, we decode the tokens in parallel through iterative decoding.
We start with a fully masked sequence $\mathbf{X}_T$.
Assuming the total number of decoding steps is $S$, for each step $i$ from 1 to $S$, we first sample $\mathbf{\hat X}_0$ from $p_{\theta}(\mathbf{X}_0|\mathbf{X}_{T - (i-1) \cdot \frac{T}{S}}, \mathbf{C})$.
Then, we sample $\lfloor N \cdot \gamma(T - i \cdot \frac{T}{S}) \rfloor$ tokens based on the confidence score to remask, resulting in $\mathbf{X}_{T - i \cdot \frac{T}{S}}$, where $N$ is the total number of tokens in $\mathbf{X}$.
The confidence score for $\hat{x}_i$ in $\mathbf{\hat X}_0$ is assigned to $p_{\theta}(\mathbf{X}_0|\mathbf{X}_{T - (i-1) \cdot \frac{T}{S}}, \mathbf{C})$ if $x_{T - (i-1) \cdot \frac{T}{S}, i}$ is a $\text{[MASK]}$ token; otherwise, we set the confidence score of $\hat{x}_i$ to $1$, indicating that tokens already unmasked in $\mathbf{X}_{T - (i-1) \cdot \frac{T}{S}}$ will not be remasked.
Particularly, we choose $\lfloor N \cdot \gamma(T - i \cdot \frac{T}{S}) \rfloor$ tokens with the lowest confidence scores to be masked.

The masked generative modeling paradigm was first introduced in[^11], and subsequent work such as[^32] has further explored it under the perspective of discrete diffusion.

</td><td>

</td></tr></table>

### Model Overview

<table><tr><td width="50%">

An overview of the ***MaskGCT*** framework is presented in Figure \ref{fig:overview}.
Following **TorToise-TTS**[^37], **SoundStorm**[^19], **VALL-E**[^02], ***MaskGCT*** is a two-stage TTS system.
The first stage uses text to predict speech semantic representation tokens, which contain most information of content and partial information of prosody.
The second stage model is trained to learn more acoustic information.
Unlike previous works (**TorToise-TTS**[^37], **VALL-E**[^02], **SoundStorm**[^19], **SPEAR-TTS**[^01]) use an autoregressive model for the first stage, ***MaskGCT*** utilizes the non-autoregressive masked generative modeling paradigm for both the two stages without text-speech alignment supervision and phone-level duration prediction: (1) For the first stage model, we trained a model to learn $p_{\theta_{\text{s1}}}(\mathbf{S}|\mathbf{S}_t, (\mathbf{S}^p, \mathbf{P}))$, where $\mathbf{S}$ is the speech semantic representation token sequence obtained from a speech semantic representation codec (we introduce in \ref{sub:repcodec}), $\mathbf{S}^p$ is the prompt semantic token sequence, and $\mathbf{P}$ is the text token sequence.
$\mathbf{S}^p$ and $\mathbf{P}$ are the condition for the first stage model.
(2) The second stage model is trained to learn $p_{\theta_{\text{s2}}}(\mathbf{A}|\mathbf{A}_t, (\mathbf{A}^p, \mathbf{S}))$, where $\mathbf{A}$ is the multi-layer acoustic token sequence from a speech acoustic codec like **SoundStream**[^28], **EnCodec**[^26].
Our second stage model is similar to **SoundStorm**[^19].
 We give more details about the four parts in the following sections.

</td><td>

</td></tr>
<tr><td>

#### Speech Semantic Representation Codec

Discrete speech representations can be divided into semantic tokens and acoustic tokens.
Generally, semantic tokens are obtained by discretizing features from speech self-supervised learning (SSL).
Previous two-stage, large-scale TTS systems (**TorToise-TTS**[^37], **SoundStorm**[^19], **SPEAR-TTS**[^01]) typically first use text to predict semantic tokens, and then employ another model to predict acoustic tokens or features.
This is because semantic tokens have a stronger correlation with text or phonemes, which makes predicting them more straightforward than directly predicting acoustic tokens.
Commonly, previous works have used k-means to discretize semantic features to obtain semantic tokens; however, this method can lead to a loss of information.
This loss may complicate the accurate reconstruction of high-quality speech or the precise prediction of acoustic tokens, especially for tonally rich languages.
For example, our early experiments demonstrate the challenges of accurately predicting acoustic tokens to achieve proper prosody for Chinese using semantic tokens obtained via k-means.
Therefore, we need to discretize semantic representation features while minimizing information loss.
Inspired by **RepCodec**[^38], we train a VQ-VAE model to learn a vector quantization codebook that reconstructs speech semantic representations from a speech SSL model.
For a speech semantic representation sequence $\mathbf{S} \in \mathbb{R}^{T \times d}$, the vector quantizer quantizes the output of the encoder $\mathcal{E}(\mathbf{S})$ to $\mathbf{E}$, and the decoder reconstructs $\mathbf{E}$ back to $\hat{\mathbf{S}}$.
We optimize the encoder and the decoder using a reconstruction loss between $\mathbf{S}$ and $\hat{\mathbf{S}}$, employ codebook loss to optimize the codebook and use commitment loss to optimize the encoder with the straight-through method (**VQ-VAE**[^20]).
The total loss for training the semantic representation codec can be written as:
$$
\begin{aligned}
\mathcal{L}_{\text{total}} &= \frac{1}{Td} (\lambda_{\text{rec}}\cdot||\mathbf{S} - \hat{\mathbf{S}}||_1 + \lambda_{\text{codebook}}\cdot||\text{sg}(\mathcal{E}(\mathbf{S})) - \mathbf{E}||_2 + \lambda_{\text{commit}}\cdot||\text{sg}(\mathbf{E})-\mathcal{E}(\mathbf{S})||_2).
\end{aligned}
$$

where $\text{sg}$ means stop-gradient.

In detail, we utilize the hidden states from the 17th layer of **W2V-BERT**[^33] as the semantic features for our speech encoder.
The encoder and decoder are composed of multiple **ConvNext**[^39] blocks.
Following the methods of improved **VQ-GAN**[^40] and **DAC**[^36], we use factorized codes to project the output of the encoder into a low-dimensional latent variable space.
The codebook contains 8,192 entries, each of dimension 8.
Further details about the model architecture are provided in Appendix~\ref{appendix:codec}.

</td><td>

</td></tr>
<tr><td>

#### Text-to-Semantic Model

Based on the previous discussion, we employ a non-autoregressive masked generative transformer to train a text-to-semantic (T2S) model, instead of using an autoregressive model or any text-to-speech alignment information.
During training, we randomly extract a portion of the prefix of the semantic token sequence as the prompt, denoted as $\mathbf{S}^p$.
We then concatenate the text token sequence $\mathbf{P}$ with $\mathbf{S}^p$ to form the condition.
We simply add $(\mathbf{P}, \mathbf{S}^p)$ as the prefix sequence to the input masked semantic token sequence $\mathbf{S}_t$ to leverage the in-context learning ability of language models.
We use a **LLaMA-style**[^41] transformer as the backbone of our model, incorporating gated linear units with **GELU**[^42] activation, rotation position encoding (**RoPE**[^43]), etc., but replacing causal attention with bidirectional attention.
We also use **adaptive RMSNorm**[^44], which accepts the time step $t$ as the condition.

During inference, we generate the target semantic token sequence of any specified length conditioned on the text and the prompt semantic token sequence.
In this paper, we also train a **Flow Matching**[^45] based duration prediction model to predict the total duration conditioned on the text and prompt speech duration, leveraging in-context learning.
More details can be found in Appendix~\ref{appendix:dur_pred}.

</td><td>

</td></tr>
<tr><td>

#### Semantic-to-Acoustic Model

We also train a semantic-to-acoustic (S2A) model using a masked generative codec transformer conditioned on the semantic tokens.
Our semantic-to-acoustic model is based on **SoundStorm**[^19], which generates multi-layer acoustic token sequences.
Given $N$ layers of the acoustic token sequence $\mathbf{A}^{1:N}$, during training, we select one layer $j$ from $1$ to $N$.
We denote the $j$th layer of the acoustic token sequence as $A^j$.
Following the previous discussion, we mask $A^j$ at the timestep $t$ to get $\mathbf{A}^j_t$.
The model is then trained to predict $\mathbf{A}^j$ conditioned on the prompt $\mathbf{A}^p$, the corresponding semantic token sequence $\mathbf{S}$, and all the layers smaller than $j$ of the acoustic tokens.
This can be formulated as $p_{\theta_{\text{s2a}}}(\mathbf{A}^j|\mathbf{A}^j_t, (\mathbf{A}^p, \mathbf{S}, \mathbf{A}^{1:j-1}))$.
We sample $j$ according to a linear schedule $p(j) = 1 - \frac{2j}{N(N+1)}$.
For the input of the S2A model, since the number of frames in the semantic token sequence is equal to the sum of the frames in the prompt acoustic sequence and the target acoustic sequence, we simply sum the embeddings of the semantic tokens and the embeddings of the acoustic tokens from layer $1$ to $j$.
During inference, we generate tokens for each layer from coarse to fine, using iterative parallel decoding within each layer.
Figure~\ref{fig:maskgct} shows a simplified training diagram of the T2S and S2A models.

</td><td>

</td></tr>
<tr><td>

#### Speech Acoustic Codec

Speech acoustic codec is trained to quantize speech waveform to multi-layer discrete tokens while aiming to preserve all the information of the speech as soon as possible.
We follow the residual vector quantization (RVQ) method to compress the 24K sampling rate speech waveform into discrete tokens of 12 layers.
The codebook size of each layer is 1,024 and the codebook dimension is 8.
The model architectures, discriminators, and training losses follow **DAC**[^36], except that we use the **Vocos**[^46] architecture as the decoder for more efficient training and inference.
Figure~\ref{fig:codec} shows the comparison between the semantic codec and acoustic codec.

</td><td>
</td></tr></table>

### Other Applications

<table><tr><td width="50%">

***MaskGCT*** can accomplish tasks beyond zero-shot TTS, such as duration-controllable speech translation (cross-lingual dubbing), emotion control, speech content editing, and voice conversion with simple modifications or the assistance of external tools, demonstrating the potential of ***MaskGCT*** as a foundational model for speech generation.
We provide more details in Appendix~\ref{appendix:s2s}, ~\ref{appendix:emo}, ~\ref{appendix:se}, ~\ref{appendix:vc}.

</td><td>

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

**Datasets**

We use the Emilia[^47] dataset to train our models.
Emilia is a multilingual and diverse in-the-wild speech dataset designed for large-scale speech generation.
In this work, we use English and Chinese data from Emilia, each with 50K hours of speech (totaling 100K hours).
We evaluate our zero-shot TTS models with three benchmarks:
(1) **LibriSpeech**[^48] test-clean, a widely used test set for English zero-shot TTS.
(2) SeedTTS test-en, a test set introduced in **Seed-TTS**[^06] of samples extracted from English public corpora, includes 1,000 samples from the **Common Voice dataset**[^49].
(3) SeedTTS test-zh, a test set introduced in Seed-TTS of samples extracted from Chinese
public corpora, includes 2,000 samples from the **DiDiSpeech dataset**[^50].
We also scale the training dataset to six languages to support multilingual zero-shot TTS.
We provide additional experimental details and evaluation results about multilingual zero-shot TTS in Appendix~\ref{appendix:mutli-zstts}.

</td><td>

</td></tr>
<tr><td>

**Evaluation Metrics**

We use both objective and subjective metrics to evaluate our models.
For the objective metrics, we evaluate speaker similarity (SIM-O), robustness (WER), and speech quality (FSD).
Specifically, for speaker similarity, we compute the cosine similarity between the [**WavLM** TDNN [URL]](https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification)[^35] speaker embedding of generated samples and the prompt.
For Word Error Rate (WER), we use a [HuBERT-based [HF]](https://huggingface.co/facebook/hubert-large-ls960-ft) ASR model for LibriSpeech test-clean, Whisper-large-v3 for Seed-TTS test-en, and Paraformer-zh for Seed-TTS test-zh, following previous works.
For speech quality, we use Fréchet Speech Distance (FSD) with self-supervised **wav2vec 2.0**[^51] features, following **Voicebox**[^09].
For the subjective metrics, comparative mean option score (CMOS) and similarity mean option score (SMOS) are used to evaluate naturalness and similarity, respectively.
CMOS is on a scale of -3 to 3, and SMOS is on a scale of 1 to 5.

</td><td>

</td></tr>
<tr><td>

**Baseline**

We compare our models with state-of-the-art zero-shot TTS systems, including **NaturalSpeech3**[^08], **VALL-E**[^02], **Voicebox**[^09], **VoiceCraft**[^05], **XTTS-v2**[^52], and **CosyVoice**[^53].
More details of each model can be found in Appendix~\ref{appendix:baseline}.
We also train an AR-based T2S model to replace the T2S part of ***MaskGCT***, we term it as AR + SoundStorm.

</td><td>

</td></tr>
<tr><td>

**Training**

We train all models on 8 NVIDIA A100 80GB GPUs.
We train two T2S models of different sizes (denoted as T2S-Base and T2S-large).
For more details about the model architecture, please refer to Appendix~\ref{appendix:architecture}.
We report the metrics of T2S-large by default, and you can find a comparison of model sizes in Section~\ref{ablation}.
We also compare two different methods of text tokenization:
**Grapheme-to-Phoneme (G2P)**[^54] and **Byte Pair Encoding (BPE)**[^55].
See more details of the two methods in Appendix~\ref{appendix:text_tokenizer}.
We report the metrics of G2P by default.
We optimize these models with the **AdamW**[^56] optimizer with a learning rate of 1e-4 and 32K warmup steps, following the inverse square root learning schedule.
We use the **classifier-free guidance**[^57], during training for both the T2S and S2A models, we drop the prompt with a probability of 0.15.
See more details about classifier-free guidance and classifier-free guidance rescale in Appendix~\ref{appendix:cfg}.

</td><td>

</td></tr>
<tr><td>

**Inference**

For the T2S model, we use 50 steps as the default total inference steps.
The classifier-free guidance scale and the classifier-free guidance rescale factor[^58] are set to 2.5 and 0.75, respectively.
For sampling, we use a top-k of 20, with the sampling temperature annealing from 1.5 to 0.
We add Gumbel noise to token confidences when determining the remasking process, following[^11].
For the S2A model, we use $[40, 16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]$ steps for acoustic RVQ layers by default, we find the S2A model can also perform well with fewer inference steps of $[10, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]$ (see Appendix~\ref{appendix:s2a_infer}).
We use the same sampling strategy as the T2S model, except that we use greedy sampling instead of top-k sampling if the inference step is 1.

</td><td>

</td></tr></table>

## 5·Results: 结果

### Zero-Shot TTS

<table><tr><td width="50%">

In this section, we show the main results of zero-shot TTS: we show comparison results with SOTA baselines in Section~\ref{section:compare_baseline}; we compare ***MaskGCT*** with replacing T2S model to an AR model in Section~\ref{section:ar_vs_maskgct}; We present the performance of ***MaskGCT*** across varying speech tempos in Section~\ref{section:dur_len_analysis}.
Additionally, we present the results of zero-shot TTS for speech style imitation in Section~\ref{style-imitation}, multilingual zero-shot TTS in Appendix~\ref{appendix:mutli-zstts}, and cross-lingual speech translation (dubbing) in Appendix~\ref{appendix:s2s}.

</td><td>

</td></tr>
<tr><td>

#### Comparison with Baselines

We compare ***MaskGCT*** with baselines in terms of similarity, robustness, and generation quality.
The main results are shown in Table \ref{table:zs-tts-result}.
***MaskGCT*** demonstrates excellent performance on all metrics and achieves human-level similarity, naturalness, and intelligibility.
In similarity, ***MaskGCT***'s SIM-O and SMOS both outperform the best baseline, whether assessed using the total length of ground truth or the predicted total duration (0.67$\rightarrow$0.687 in LibriSpeech, 0.643$\rightarrow$0.717 in SeedTTS test-en, 0.75$\rightarrow$0.774 in SeedTTS test-zh for SIM-O; +0.01 in LibriSpeech, +0.72 in SeedTTS test-en, +0.55 in SeedTTS test-zh for SMOS).
When compared with human recordings, ***MaskGCT*** achieves human-level similarity across all three test sets (+0.017, -0.002, and +0.027 for SIM-O respectively in the three test sets, and +0.28, +0.32, and +0.25 for SMOS respectively in the three test sets).
In robustness, ***MaskGCT*** likewise results nearly on par with ground truth (with 2.634, 2.623, 2.273 WER on LibriSpeech, SeedTTS test-en, and SeedTTS test-zh, respectively), exhibiting enhanced robustness compared to AR-based models and performing on par or better than NAR-based models such as VoiceBox and NaturalSpeech 3, without relying on phone-level duration predictions.
In generation quality, ***MaskGCT*** achieves +0.10, +0.03, and +0.05 CMOS across the three test sets when compared with human recordings, indicating that ***MaskGCT*** attains human-level naturalness on these test sets.
We also observe that ***MaskGCT*** exhibits excellent performance when using both ground truth total duration and predicted total duration, indicating the robustness of ***MaskGCT*** within a reasonable range of total speech duration and the capability of our total duration predictor to yield appropriate durations.

</td><td>

</td></tr>
<tr><td>

#### Autoregressive vs Masked Generative Models

We compare ***MaskGCT*** to replacing T2S ***MaskGCT*** with an AR T2S model (which we call AR + SoundStorm).
Table~\ref{table:res-ar} shows the performance of these two models on all three test sets.
***MaskGCT*** demonstrates improved similarity, robustness, and CMOS (+0.12 on LibriSpeech test-clean, +0.08 on SeedTTS test-en, and +0.37 on SeedTTS test-zh) across all three test sets.
We also conduct comparisons on more challenging hard cases (such as repeating words, and tongue twisters, which are often considered as samples where TTS systems are prone to hallucinations).
***MaskGCT*** exhibits a more pronounced robustness advantage in these scenarios.
See details in Appendix~\ref{appendix:hard_case}.
In addition, compared to AR-based models, ***MaskGCT*** offers the capability to control the total duration of the generated speech, along with fewer inference steps, requiring only 25 to 50 steps for T2S models to achieve optimal results for speeches of any length.
Conversely, the inference steps for AR-based models increase linearly with the length of the speech.

</td><td>

</td></tr>
<tr><td>

#### Duration Length Analysis

We analyze the robustness of the generated results of ***MaskGCT*** under different changes in total duration length (which can also be regarded as changes in speech tempo).
The results are shown in Figure~\ref{fig:wer_duration}.
We explore the results of multiplying the ground truth total duration by 0.7 to 1.3.
The results show that the lowest WER is achieved at a total duration multiplier of 1.0, indicating that the models perform best when the speech is played at its natural speed.
When the multiplier is 0.9 or 1.1, the model is still able to achieve a WER very close to the best.
When the multiplier is 0.7 or 1.3, the WER is slightly higher but still within a reasonable range.
This shows that our model can generate reasonable and accurate content at different speech tempos.

</td><td>

</td></tr></table>

### Speech Style Imitation

<table><tr><td width="50%">

Zero-shot TTS endeavors to learn how to speak, including voice timbre and style, from prompt speech.
Previous works utilized SIM-O to measure the similarity between generated speech and reference speech; however, SIM-O primarily assesses the similarity in voice timbre.
In addition to evaluating the model's zero-shot cloning ability through timbre similarity metrics, we also explored ***MaskGCT***'s capability to clone overall style from two more expressive and stylized dimensions: accent and emotion.
We randomly sampled a portion of data from the **L2-ARCTIC**[^59] accent corpus and the **ESD**[^60] emotion corpus to construct our accent and emotion evaluation datasets.
Additionally, we introduce supplementary metrics to assess the model's performance.
For accent imitation, we employ SIM-Accent, to measure the similarity in accent between the generated speech and reference speech.
The calculation process is analogous to SIM-O, but we utilize [CommonAccent [HF]](https://huggingface.co/Jzuluaga/accent-id-commonaccent_ecapa)[^61], **AutoVC**[^62] to derive the accent representation features of the speech.
We also incorporate a subjective evaluation metric, Accent SMOS, which is similar to SMOS but focuses on accent rather than timbre.
For emotion, we introduce Emotion SIM (with [emotion2vec [URL]](https://github.com/ddlBoJack/emotion2vec)[^63] to extract features) and Emotion SMOS.

Our experiments demonstrate that ***MaskGCT*** exhibits powerful style cloning capabilities.
For accent imitation, ***MaskGCT*** achieves the highest SIM-O of 0.717, close to the ground truth of 0.747.
It also maintains a competitive WER of 6.382 and the best Accent SIM of 0.645.
Additionally, ***MaskGCT*** leads in CMOS of 0.23, SMOS of 4.24, and Accent SMOS of 4.38.
For emotion imitation, ***MaskGCT*** achieves the highest SIM-O of 0.600.
It also attains a competitive WER of 12.502 and a strong Emotion SIM of 0.822.
Furthermore, ***MaskGCT*** leads in all subjective metrics with CMOS of -0.31, SMOS of 4.07, and Emotion SMOS of 3.76, indicating natural and pleasant emotion imitation.

</td><td>

</td></tr></table>

### Ablation Study

<table><tr><td width="50%">

**Inference Timesteps**

We explore the impact of inference steps of the T2S model on the results, ranging from 5 steps to 75 steps.
Initially, SIM increases significantly and stabilizes after 25 steps.
For test-zh, it rises from 0.761 at 5 steps to 0.771 at 75 steps, and for test-en, from 0.696 to 0.715.
SIM peaks around 25 steps.
WER improves more dramatically, especially up to 25 steps.
For test-zh, it drops from 10.19 at 5 steps to 2.507 at 25 steps, and for test-en, from 8.096 to 2.346.
Both SIM and WER show minimal changes beyond 25 steps.
These findings suggest that SIM can be optimized with around 10 steps, while achieving the lowest WER requires approximately 25 steps.
Beyond this, both metrics show minimal changes, indicating that further increases in steps do not yield substantial improvements.

Therefore, for practical applications, 25 inference steps may be considered optimal for balancing SIM and WER, ensuring efficient and effective performance.
See more details in Appendix~\ref{appendix:t2s_infer}.

</td><td>

</td></tr>
<tr><td>

**Model Size**

We compare the performance differences of T2S models with varying model sizes.
The result is shown in Table~\ref{table:t2s-model-size}.
We observe that the large model outperforms the base model across all metrics, albeit not significantly.
We suggest that our system can achieve good performance with just the setting of the base model when using 100K hours of data.
In the future, we will explore more comprehensive scaling laws for both model size and data scaling.

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

In this paper, we present ***MaskGCT***, a large-scale zero-shot TTS system that leverages fully non-autoregressive masked generative codec transformers while not requiring text-speech alignment supervision
and phone-level duration prediction.
***MaskGCT*** achieves high-quality text-to-speech synthesis using text to predict semantic tokens extracted from a
speech self-supervised learning (SSL) model, and then predicting acoustic tokens conditioned on these semantic tokens.
Our experiments demonstrate that ***MaskGCT*** outperforms the state-of-the-art TTS system on speech quality, similarity, and intelligibility with scaled model size and training data, and ***MaskGCT*** can control the total duration of generated speech.
We also explore the scalability of ***MaskGCT*** in tasks such as speech translation, voice conversion, emotion control, and speech content editing, demonstrating the potential of ***MaskGCT*** as a foundational model for speech generation.

</td><td>

</td></tr></table>

## References: 参考文献

[^01]: [SPEAR-TTS](2023.02.07_SPEAR-TTS.md)
[^02]: [VALL-E](2023.01.05_VALL-E.md)
[^03]: [BASE-TTS](2024.02.12_BASE-TTS.md)
[^04]: [CLaM-TTS](../2024.04.03_CLaM-TTS.md)
[^05]: [VoiceCraft](../2024.03.25_VoiceCraft.md)
[^06]: [Seed-TTS](../2024.06.04_Seed-TTS.md)
[^07]: [NaturalSpeech2](../../Diffusion/2023.04.18_NaturalSpeech2.md)
[^08]: [NaturalSpeech3/FACodec](../../Diffusion/2024.03.05_NaturalSpeech3.md)
[^09]: [Voicebox](../../Diffusion/2023.06.23_Voicebox.md)
[^10]: [Mega-TTS](2023.06.06_Mega-TTS.md)
[^11]: Maskgit: Masked generative image transformer.
[^12]: Muse: Text-to-image generation via masked generative transformers.
[^13]: Mage: Masked generative encoder to unify representation learning and image synthesis.
[^14]: Magvit: Masked generative video transformer.
[^15]: Language model beats diffusion–tokenizer is key to visual generation.
[^16]: Vampnet: Music generation via masked acoustic token modeling.
[^17]: Masksr: Masked language model for full-band speech restoration.
[^18]: Masked audio generation using a single non-autoregressive transformer.
[^19]: [SoundStorm](2023.05.16_SoundStorm.md)
[^20]: [VQ-VAE](../../../Modules/VQ/2017.11.02_VQ-VAE.md)
[^21]: [FastSpeech2](../../Acoustic/2020.06.08_FastSpeech2.md)
[^22]: [FastSpeech](../../Acoustic/2019.05.22_FastSpeech.md)
[^23]: [NaturalSpeech](../../E2E/2022.05.09_NaturalSpeech.md)
[^24]: [Tacotron](../../Acoustic/2017.03.29_Tacotron.md)
[^25]: [VITS](../../E2E/2021.06.11_VITS.md)
[^26]: [EnCodec](../../SpeechCodec/2022.10.24_EnCodec.md)
[^27]: [MusicGen](../../Music/2023.06.08_MusicGen.md)
[^28]: [SoundStream](../../SpeechCodec/2021.07.07_SoundStream.md)
[^29]: [SimpleSpeech](../../Diffusion/2024.06.04_SimpleSpeech.md)
[^30]: [DiTTo-TTS](../../Diffusion/2024.06.17_DiTTo-TTS.md)
[^31]: [E2 TTS](../../Diffusion/2024.06.26_E2_TTS.md)
[^32]: Improved masked image generation with token-critic.
[^33]: [W2V-BERT](../../SpeechRepresentation/2021.08.07_W2V-BERT.md)
[^34]: [HuBERT](../../SpeechRepresentation/2021.06.14_HuBERT.md)
[^35]: [WavLM](../../SpeechRepresentation/2021.10.26_WavLM.md)
[^36]: [DAC](../../SpeechCodec/2023.06.11_Descript-Audio-Codec.md)
[^37]: [TorToise-TTS](../../Diffusion/2023.05.12_TorToise-TTS.md)
[^38]: [RepCodec](../../SpeechCodec/2023.08.31_RepCodec.md)
[^39]: [ConvNeXt](../../_Basis/2022.01.10_ConvNeXt.md)
[^40]: [VQ-GAN](../../../Modules/VQ/2020.12.17_VQGAN.md)
[^41]: [LLaMA 2](../../TextLM/2023.07.18_LLaMA2.md)
[^42]: [GELU](../../../Modules/Activation/GELU.md)
[^43]: [RoPE](../../../Modules/PositionEmb/RoPE.md)
[^44]: [RMSNorm](../../../Modules/Normalization/2019.10.16_RMSNorm.md)
[^45]: [Flow Matching](../../Diffusion/2022.10.06_Flow_Matching.md)
[^46]: [Vocos](../../Vocoder/2023.03.01_Vocos.md)
[^47]: [Emilia](../../../Datasets/2024.07.07_Emilia.md)
[^48]: [LibriSpeech](../../../Datasets/2015.04.19_LibriSpeech.md)
[^49]: [Common Voice](../../../Datasets/CommonVoice.md)
[^50]: [DiDiSpeech](../../../Datasets/2020.10.19_DiDiSpeech.md)
[^51]: [Wav2Vec2.0](../../SpeechRepresentation/2020.06.20_Wav2Vec2.0.md)
[^52]: [XTTS](../2024.06.07_XTTS.md)
[^53]: [CosyVoice](2024.07.07_CosyVoice.md)
[^54]: [Phonemizer](../../_Basis/Phonemizer.md)
[^55]: [BPE](../../_Basis/2015.08.31_BPE.md) A New Algorithm for Data Compression.
[^56]: [AdamW](../../../Modules/Optimization/2017.11.14_AdamW.md)
[^57]: [CFG](../../Diffusion/2022.07.26_Classifier-Free_Guidance.md)
[^58]: Common diffusion noise schedules and sample steps are flawed.
[^59]: L2-ARCTIC: A non-native english speech corpus.
[^60]: [ESD](../../../Datasets/2020.10.28_ESD.md)
[^61]: xue2024convert
[^62]: [AutoVC](../../Voice_Conversion/2019.05.14_AutoVC.md)
[^63]: [Emotion2Vec](../../SpeechRepresentation/2023.12.23_Emotion2Vec.md)
