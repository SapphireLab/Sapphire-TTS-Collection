# MaskGCT

<details>
<summary>基本信息</summary>

- 标题: "MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer"
- 作者:
  - 01 Yuancheng Wang,
  - 02 Haoyue Zhan,
  - 03 Liwei Liu,
  - 04 Ruihong Zeng,
  - 05 Haotian Guo,
  - 06 Jiachen Zheng,
  - 07 Qiang Zhang,
  - 08 Xueyao Zhang,
  - 09 Shunsi Zhang,
  - 10 Zhizheng Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2409.00750)
  - [Publication]()
  - [Github](https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct)
  - [Demo](https://maskgct.github.io/)
- 文件:
  - [ArXiv](../_PDF/2409.00750v3__MaskGCT__Zero-Shot_Text-to-Speech_with_Masked_Generative_Codec_Transformer.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems.
The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability.
Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g., phone), which may compromise their naturalness.
In this paper, we introduce ***Masked Generative Codec Transformer (MaskGCT)***, a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction.
***MaskGCT*** is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens.
***MaskGCT*** follows the mask-and-predict learning paradigm.
During training, ***MaskGCT*** learns to predict masked semantic or acoustic tokens based on given conditions and prompts.
During inference, the model generates tokens of a specified length in a parallel manner.
Experiments with 100K hours of in-the-wild speech demonstrate that ***MaskGCT*** outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility.
Audio samples are available at [this https URL](https://maskgct.github.io/).
We release our code and model checkpoints at [this https URL](https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct).

</td><td>

近期的大规模文本转语音系统通常分为自回归和非自回归系统.
- 自回归系统隐式建模了模型时长, 但在鲁棒性和持续时长控制性方面存在一些缺陷.
- 非自回归系统要求在训练时显式对齐文本和语音之间的信息, 并预测语言单元 (如音素) 的持续实践, 这可能妨碍了它们的自然度.

本文介绍了 ***掩膜生成式编解码 Transformer (Masked Generative Codec Transformer, MaskGCT)***, 一种完全非自回归的文本转语音模型, 消除了显式对齐文本和语音监督之间的信息, 以及音素级时长预测的需求.
***MaskGCT*** 是一种两阶段模型: 在第一阶段, 模型使用文本预测语音自监督学习 (SSL) 模型提取的语义 Token, 而在第二阶段, 模型根据这些语义 Token 为条件预测声学 Token.
***MaskGCT*** 遵循掩膜-预测学习范式.
在训练时, ***MaskGCT*** 学习根据给定的条件和提示预测掩盖的语义或声学 Token.
在推理时, 模型以并行的方式生成指定长度的 Token.

在 100K 小时的真实语音数据集上的实验表明, ***MaskGCT*** 在质量, 语义相似度, 和流畅度方面优于当前最先进的零样本 TTS 系统.
音频样本可以在 [https URL](https://maskgct.github.io/) 获得.
我们在 [https URL](https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct) 发布了我们的代码和模型检查点.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

In recent years, large-scale zero-shot text-to-speech (TTS) systems~\cite{kharitonov2023speak, wang2023neural, lajszczak2024base, kim2024clam, peng2024voicecraft, anastassiou2024seed, shen2023naturalspeech, ju2024naturalspeech, le2024voicebox, jiang2023mega} have achieved significant improvements by scaling data and model sizes, including both autoregressive (AR)~\cite{kharitonov2023speak, wang2023neural, lajszczak2024base, kim2024clam, peng2024voicecraft, anastassiou2024seed} and non-autoregressive (NAR) models~\cite{shen2023naturalspeech, ju2024naturalspeech, le2024voicebox, jiang2023mega}.
However, both AR-based and NAR-based systems still exhibit some shortcomings.
In particular, AR-based TTS systems typically quantize speech into discrete tokens and then use decoder-only models to autoregressively generate these tokens, which offer diverse prosody but also suffer from problems such as poor robustness and slow inference speed.
NAR-based models, typically based on diffusion~\cite{ju2024naturalspeech, shen2023naturalspeech}, flow matching~\cite{le2024voicebox}, or GAN~\cite{jiang2023mega}, require explicit text and speech alignment information as well as the prediction of phone-level duration, resulting in a complex pipeline and producing more standardized but less diverse speech.

Recently, masked generative transformers, a class of generative models, have achieved significant results in the fields of image~\cite{chang2022maskgit, chang2023muse, li2023mage}, video~\cite{yu2023magvit, yu2023language}, and audio~\cite{garcia2023vampnet, li2024masksr, ziv2024masked} generation, demonstrating potential comparable to or superior to autoregressive models or diffusion models.
These models employ a mask-and-predict training paradigm and utilize iterative parallel decoding during inference.
Some previous works have attempted to introduce masked generative models into the field of TTS.
SoundStorm~\cite{borsos2023soundstorm} was the first attempt to use a masked generative transformer to predict multi-layer acoustic tokens extracted from SoundStream, conditioned on speech semantic tokens; however, it needs to receive the semantic tokens of an AR model as input.
Thus, SoundStorm is more of an acoustic model that converts semantic tokens into acoustic tokens and does not fully utilize the powerful generative potential of masked generative models.
NaturalSpeech 3~\cite{ju2024naturalspeech} decomposes speech into discrete token sequences representing different attributes through special designs and generates tokens representing different attributes through masked generative models.
However, it still needs speech-text alignment supervision and phone-level duration prediction.

In this work, we propose MaskGCT, \textbf{\textit{a fully non-autoregressive model for text-to-speech synthesis that uses masked generative transformers without requiring text-speech alignment supervision and phone-level duration prediction}}.
MaskGCT is a two-stage system, both stages are trained using the \textit{mask-and-predict learning} paradigm.
The first stage, the text-to-semantic (T2S) model, predicts masked semantic tokens with in-context learning, using text token sequences and prompt speech semantic token sequences as the prefix, without explicit duration prediction.
The second stage, the semantic-to-acoustic (S2A) model, utilizes semantic tokens to predict masked acoustic tokens extracted from an RVQ-based speech codec with prompt acoustic tokens.
During inference, MaskGCT can generate semantic tokens of various specified lengths with a few iteration steps given a sequence of text.
% , which is significantly fewer than AR-based models.
In addition, we train a VQ-VAE~\cite{van2017neural} to quantize speech self-supervised learning embedding, rather than using k-means to extract semantic tokens that is common in previous work.
This approach minimizes the information loss of semantic features even with a single codebook.
We also explore the scalability of our methods beyond the zero-shot TTS task, such as speech translation (cross-lingual dubbing), speech content editing, voice conversion, and emotion control, demonstrating the potential of MaskGCT as a foundational model for speech generation.
Table~\ref{table:overview} shows a comparison between MaskGCT and some previous works.

Our experiments demonstrate that MaskGCT has achieved performance comparable to or superior to that of existing models in terms of speech quality, similarity, prosody, and intelligibility.
Specifically, (1) It achieves comparable or better quality and naturalness than the ground truth speech across three benchmarks (LibriSpeech, SeedTTS \textit{test-en}, and SeedTTS \textit{test-zh}) in terms of CMOS.
(2) It achieves human-level similarity between the generated speech and the prompt speech, with improvements of +0.017, -0.002, and +0.027 in SIM-O and +0.28, +0.32 and +0.25 in SMOS for LibriSpeech, SeedTTS \textit{test-en}, and SeedTTS \textit{test-zh}, respectively.
(3) It achieves comparable intelligibility in terms of WER across the three benchmarks and demonstrates stability within a reasonable range of speech duration, which also indicates the diversity and controllability of the generated speech.

In summary, we propose a non-autoregressive zero-shot TTS system based on masked generative transformers and introduce a speech discrete semantic representation by training a VQ-VAE on speech self-supervised representations.
Our system achieves human-level similarity, naturalness, and intelligibility by scaling data to 100K hours of in-the-wild speech, while also demonstrating high flexibility, diversity, and controllability.
We investigate the scalability of our system across various tasks, including cross-lingual dubbing, voice conversion, emotion control, and speech content editing, utilizing zero-shot learning or post-training methods.
This showcases the potential of our system as a foundational model for speech generation.

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

**Large-scale TTS**

Traditional TTS systems~\cite{ren2020fastspeech, ren2019fastspeech, tan2024naturalspeech, wang2017tacotron, kim2021conditional} are trained to generate speech from a single speaker or multiple speakers using hours of high-quality transcribed training data.
Modern large-scale TTS systems~\cite{kharitonov2023speak, wang2023neural, lajszczak2024base, kim2024clam, peng2024voicecraft, anastassiou2024seed} aim to achieve zero-shot TTS (synthesizing speech for unseen speakers with speech prompts) by scaling both the model and data size.
These systems can be mainly divided into AR-based and NAR-based categories.
For AR-based systems:
SpearTTS~\cite{kharitonov2023speak} utilizes three AR models to predict semantic tokens from text, coarse-grained acoustic tokens from semantic tokens, and fine-grained acoustic tokens from coarse-grained tokens.
VALL-E~\cite{wang2023neural} predicts the first layer of acoustic tokens extracted from EnCodec~\cite{defossez2022high} using an AR codec language model, and the final layers with a NAR model.
VoiceCraft~\cite{peng2024voicecraft} employs a single AR model to predict multi-layer acoustic tokens in a delayed pattern~\cite{copet2024simple}.
BASETTS~\cite{lajszczak2024base} predicts novel speech codes extracted from WavLM features and uses a GAN model for waveform reconstruction.
For NAR-based systems:
NaturalSpeech 2~\cite{shen2023naturalspeech} employs latent diffusion to predict the latent representations from a codec model~\cite{zeghidour2021soundstream}.
VoiceBox~\cite{le2024voicebox} uses flow matching and in-context learning to predict mel-spectrograms.
MegaTTS~\cite{jiang2023mega} utilizes a GAN to predict mel-spectrograms, while an AR model predicts phone-level prosody codes.
NaturalSpeech 3~\cite{ju2024naturalspeech} employs a unified framework based on discrete diffusion models to predict discrete representations of different speech attributes.
However, these NAR systems need to predict phoneme-level duration, leading to a complex pipeline and more standardized generative results.
SimpleSpeech~\cite{yang2024simplespeech}, DiTTo-TTS~\cite{lee2024ditto}, and E2 TTS~\cite{eskimez2024e2} are also NAR-based models that do not require precise alignment information between text and speech, nor do they predict phoneme-level duration.
We discuss these concurrent works in Appendix~\ref{appendix:concurrent_works}.

**Masked Generative Model**

Masked generative transformers, a class of generative models, achieve significant results and demonstrate potential comparable to or superior to that of autoregressive models or diffusion models in the fields of image~\cite{chang2022maskgit, chang2023muse, lezama2022improved, li2023mage}, video~\cite{yu2023magvit, yu2023language}, and audio~\cite{borsos2023soundstorm, garcia2023vampnet, li2024masksr, ziv2024masked} generation.
MaskGIT~\cite{chang2022maskgit} is the first work to use masked generative models for both unconditional and conditional image generation.
Subsequently, Muse~\cite{chang2023muse} leverages rich text to achieve high-quality and diverse text-to-image generation within the same framework.
MAGVIT-v2~\cite{yu2023language} employs masked generative models with novel lookup-free quantization, outperforming diffusion models in image and video generation.
Recently, some efforts have been made to adapt masked generative models to the field of audio.
SoundStorm~\cite{borsos2023soundstorm} takes in the semantic tokens from AudioLM and utilizes this generative paradigm to generate tokens for a neural audio codec~\cite{zeghidour2021soundstream}.
VampNet~\cite{garcia2023vampnet} and MAGNeT~\cite{ziv2024masked} apply masked generative models for music and audio generation, while MaskSR~\cite{li2024masksr} extends these models for speech restoration.

**Discrete Speech Representation**

Speech representation is a crucial aspect of speech generation.
Early works~\cite{ren2019fastspeech, wang2017tacotron} typically utilized mel-spectrograms as the modeling target.
Recently, some large-scale TTS systems~\cite{wang2023neural, ju2024naturalspeech} have shifted to using discrete speech representations.
Discrete speech representation can be primarily divided into two types: semantic discrete representation and acoustic discrete representation\footnote{We give a more detailed discussion about the definitions of "semantic" and "acoustic" in Appendix~\ref{appendix:definition}.}.
Semantic discrete representations are mainly extracted from various speech SSL models~\cite{chung2021w2v, hsu2021hubert, chen2022wavlm} using quantization methods such as k-means.
Acoustic discrete representations, on the other hand, are usually obtained by training a VQ-GAN model~\cite{van2017neural} with the goal of waveform reconstruction, as seen in speech codecs~\cite{defossez2022high, zeghidour2021soundstream, kumar2024high}.
Semantic discrete representation typically shows a stronger correlation with text, whereas acoustic discrete representation more effectively reconstructs audio.
Consequently, some two-stage TTS models predict both semantic and acoustic tokens.
FACodec~\cite{ju2024naturalspeech} is a novel speech codec that disentangles speech into subspaces of different attributes, including content, prosody, timbre, and acoustic details.

</td><td>

</td></tr></table>

## 3·Methodology: 方法

### Background: Non-Autoregressive Masked Generative Transformer

<table><tr><td width="50%">

Given a discrete representation sequence $\mathbf{X}$ of some data, we define $\mathbf{X}_t = \mathbf{X} \odot \mathbf{M}_t$ as the process of masking a subset of tokens in $\mathbf{X}$ with the corresponding binary mask $\mathbf{M}_t=[m_{t,i}]_{i=1}^{N}$.
Specifically, this involves replacing $x_i$ with a special $\text{[MASK]}$ token if $m_{t,i}=1$, and otherwise leaving $x_i$ unmasked if $m_{t,i}=0$.
Here, each $m_{t,i}$ is independently and identically distributed according to a Bernoulli distribution with parameter $\gamma(t)$, where $\gamma(t) \in (0,1]$ represents a mask schedule function (for example, $\gamma(t) = \sin(\frac{\pi t}{2T}), t \in (0,T]$).
We denote $\mathbf{X}_0 = \mathbf{X}$.
The non-autoregressive masked generative transformers are trained to predict the masked tokens based on the unmasked tokens and a condition $\mathbf{C}$.
This prediction is modeled as $p_{\theta}(\mathbf{X}_0|\mathbf{X}_t, \mathbf{C})$.
The parameters $\theta$ are optimized to minimize the negative log-likelihood of the masked tokens:
\begin{equation*}
    \begin{aligned}
        \mathcal{L}_{\text{mask}} &= \mathop{\mathbb{E}}\limits_{\mathbf{X} \in \mathcal{D}, t \in \left[0, T\right]}  -\sum_{i=1}^{N} m_{t,i} \cdot \log(p_{\theta}(x_i|\mathbf{X}_{t}, \mathbf{C})).
    \end{aligned}
\end{equation*}
At the inference stage, we decode the tokens in parallel through iterative decoding.
We start with a fully masked sequence $\mathbf{X}_T$.
Assuming the total number of decoding steps is $S$, for each step $i$ from 1 to $S$, we first sample $\mathbf{\hat X}_0$ from $p_{\theta}(\mathbf{X}_0|\mathbf{X}_{T - (i-1) \cdot \frac{T}{S}}, \mathbf{C})$.
Then, we sample $\lfloor N \cdot \gamma(T - i \cdot \frac{T}{S}) \rfloor$ tokens based on the confidence score to remask, resulting in $\mathbf{X}_{T - i \cdot \frac{T}{S}}$, where $N$ is the total number of tokens in $\mathbf{X}$.
The confidence score for $\hat{x}_i$ in $\mathbf{\hat X}_0$ is assigned to $p_{\theta}(\mathbf{X}_0|\mathbf{X}_{T - (i-1) \cdot \frac{T}{S}}, \mathbf{C})$ if $x_{T - (i-1) \cdot \frac{T}{S}, i}$ is a $\text{[MASK]}$ token; otherwise, we set the confidence score of $\hat{x}_i$ to $1$, indicating that tokens already unmasked in $\mathbf{X}_{T - (i-1) \cdot \frac{T}{S}}$ will not be remasked.
Particularly, we choose $\lfloor N \cdot \gamma(T - i \cdot \frac{T}{S}) \rfloor$ tokens with the lowest confidence scores to be masked.

The masked generative modeling paradigm was first introduced in~\cite{chang2022maskgit}, and subsequent work such as~\cite{lezama2022improved} has further explored it under the perspective of discrete diffusion.

</td><td>

</td></tr></table>

### Model Overview

<table><tr><td width="50%">

An overview of the MaskGCT framework is presented in Figure \ref{fig:overview}.
Following~\cite{betker2023better, borsos2023soundstorm, wang2023neural}, MaskGCT is a two-stage TTS system.
The first stage uses text to predict speech semantic representation tokens, which contain most information of content and partial information of prosody.
The second stage model is trained to learn more acoustic information.
Unlike previous works~\cite{betker2023better, wang2023neural, borsos2023soundstorm, kharitonov2023speak} use an autoregressive model for the first stage, MaskGCT utilizes the non-autoregressive masked generative modeling paradigm for both the two stages without text-speech alignment supervision and phone-level duration prediction: (1) For the first stage model, we trained a model to learn $p_{\theta_{\text{s1}}}(\mathbf{S}|\mathbf{S}_t, (\mathbf{S}^p, \mathbf{P}))$, where $\mathbf{S}$ is the speech semantic representation token sequence obtained from a speech semantic representation codec (we introduce in \ref{sub:repcodec}), $\mathbf{S}^p$ is the prompt semantic token sequence, and $\mathbf{P}$ is the text token sequence.
$\mathbf{S}^p$ and $\mathbf{P}$ are the condition for the first stage model.
(2) The second stage model is trained to learn $p_{\theta_{\text{s2}}}(\mathbf{A}|\mathbf{A}_t, (\mathbf{A}^p, \mathbf{S}))$, where $\mathbf{A}$ is the multi-layer acoustic token sequence from a speech acoustic codec like~\cite{zeghidour2021soundstream, defossez2022high}.
Our second stage model is similar to SoundStorm \cite{borsos2023soundstorm}.
 We give more details about the four parts in the following sections.

</td><td>

</td></tr>
<tr><td>

#### Speech Semantic Representation Codec

Discrete speech representations can be divided into semantic tokens and acoustic tokens.
Generally, semantic tokens are obtained by discretizing features from speech self-supervised learning (SSL).
Previous two-stage, large-scale TTS systems~\cite{betker2023better, borsos2023soundstorm, kharitonov2023speak} typically first use text to predict semantic tokens, and then employ another model to predict acoustic tokens or features.
This is because semantic tokens have a stronger correlation with text or phonemes, which makes predicting them more straightforward than directly predicting acoustic tokens.
Commonly, previous works have used k-means to discretize semantic features to obtain semantic tokens; however, this method can lead to a loss of information.
This loss may complicate the accurate reconstruction of high-quality speech or the precise prediction of acoustic tokens, especially for tonally rich languages.
For example, our early experiments demonstrate the challenges of accurately predicting acoustic tokens to achieve proper prosody for Chinese using semantic tokens obtained via k-means.
Therefore, we need to discretize semantic representation features while minimizing information loss.
Inspired by~\cite{huang2023repcodec}, we train a VQ-VAE model to learn a vector quantization codebook that reconstructs speech semantic representations from a speech SSL model.
For a speech semantic representation sequence $\mathbf{S} \in \mathbb{R}^{T \times d}$, the vector quantizer quantizes the output of the encoder $\mathcal{E}(\mathbf{S})$ to $\mathbf{E}$, and the decoder reconstructs $\mathbf{E}$ back to $\hat{\mathbf{S}}$.
We optimize the encoder and the decoder using a reconstruction loss between $\mathbf{S}$ and $\hat{\mathbf{S}}$, employ codebook loss to optimize the codebook and use commitment loss to optimize the encoder with the straight-through method~\cite{van2017neural}.
The total loss for training the semantic representation codec can be written as:
\begin{equation*}
    \begin{aligned}
        \mathcal{L}_{\text{total}} &= \frac{1}{Td} (\lambda_{\text{rec}}\cdot||\mathbf{S} - \hat{\mathbf{S}}||_1 + \lambda_{\text{codebook}}\cdot||\text{sg}(\mathcal{E}(\mathbf{S})) - \mathbf{E}||_2 + \lambda_{\text{commit}}\cdot||\text{sg}(\mathbf{E})-\mathcal{E}(\mathbf{S})||_2).
    \end{aligned}
\end{equation*}
where $\text{sg}$ means stop-gradient.

In detail, we utilize the hidden states from the 17th layer of W2v-BERT 2.0~\cite{chung2021w2v} as the semantic features for our speech encoder.
The encoder and decoder are composed of multiple ConvNext~\cite{liu2022convnet} blocks.
Following the methods of improved VQ-GAN~\cite{yu2021vector} and DAC~\cite{kumar2024high}, we use factorized codes to project the output of the encoder into a low-dimensional latent variable space.
The codebook contains 8,192 entries, each of dimension 8.
Further details about the model architecture are provided in Appendix~\ref{appendix:codec}.

</td><td>

</td></tr>
<tr><td>

#### Text-to-Semantic Model

Based on the previous discussion, we employ a non-autoregressive masked generative transformer to train a text-to-semantic (T2S) model, instead of using an autoregressive model or any text-to-speech alignment information.
During training, we randomly extract a portion of the prefix of the semantic token sequence as the prompt, denoted as $\mathbf{S}^p$.
We then concatenate the text token sequence $\mathbf{P}$ with $\mathbf{S}^p$ to form the condition.
We simply add $(\mathbf{P}, \mathbf{S}^p)$ as the prefix sequence to the input masked semantic token sequence $\mathbf{S}_t$ to leverage the in-context learning ability of language models.
We use a Llama-style~\cite{touvron2023llama} transformer as the backbone of our model, incorporating gated linear units with GELU~\cite{hendrycks2016gaussian} activation, rotation position encoding~\cite{su2024roformer}, etc., but replacing causal attention with bidirectional attention.
We also use adaptive RMSNorm~\cite{zhang2019root}, which accepts the time step $t$ as the condition.

During inference, we generate the target semantic token sequence of any specified length conditioned on the text and the prompt semantic token sequence.
In this paper, we also train a flow matching~\cite{lipman2022flow} based duration prediction model to predict the total duration conditioned on the text and prompt speech duration, leveraging in-context learning.
More details can be found in Appendix~\ref{appendix:dur_pred}.

</td><td>

</td></tr>
<tr><td>

#### Semantic-to-Acoustic Model

We also train a semantic-to-acoustic (S2A) model using a masked generative codec transformer conditioned on the semantic tokens.
Our semantic-to-acoustic model is based on SoundStorm ~\cite{borsos2023soundstorm}, which generates multi-layer acoustic token sequences.
Given $N$ layers of the acoustic token sequence $\mathbf{A}^{1:N}$, during training, we select one layer $j$ from $1$ to $N$.
We denote the $j$th layer of the acoustic token sequence as $A^j$.
Following the previous discussion, we mask $A^j$ at the timestep $t$ to get $\mathbf{A}^j_t$.
The model is then trained to predict $\mathbf{A}^j$ conditioned on the prompt $\mathbf{A}^p$, the corresponding semantic token sequence $\mathbf{S}$, and all the layers smaller than $j$ of the acoustic tokens.
This can be formulated as $p_{\theta_{\text{s2a}}}(\mathbf{A}^j|\mathbf{A}^j_t, (\mathbf{A}^p, \mathbf{S}, \mathbf{A}^{1:j-1}))$.
We sample $j$ according to a linear schedule $p(j) = 1 - \frac{2j}{N(N+1)}$.
For the input of the S2A model, since the number of frames in the semantic token sequence is equal to the sum of the frames in the prompt acoustic sequence and the target acoustic sequence, we simply sum the embeddings of the semantic tokens and the embeddings of the acoustic tokens from layer $1$ to $j$.
During inference, we generate tokens for each layer from coarse to fine, using iterative parallel decoding within each layer.
Figure~\ref{fig:maskgct} shows a simplified training diagram of the T2S and S2A models.

</td><td>

</td></tr>
<tr><td>

#### Speech Acoustic Codec

Speech acoustic codec is trained to quantize speech waveform to multi-layer discrete tokens while aiming to preserve all the information of the speech as soon as possible.
We follow the residual vector quantization (RVQ) method to compress the 24K sampling rate speech waveform into discrete tokens of 12 layers.
The codebook size of each layer is 1,024 and the codebook dimension is 8.
The model architectures, discriminators, and training losses follow DAC~\cite{kumar2024high}, except that we use the Vocos~\cite{siuzdak2023vocos} architecture as the decoder for more efficient training and inference.
Figure~\ref{fig:codec} shows the comparison between the semantic codec and acoustic codec.

</td><td>
</td></tr></table>

### Other Applications

<table><tr><td width="50%">

MaskGCT can accomplish tasks beyond zero-shot TTS, such as duration-controllable speech translation (cross-lingual dubbing), emotion control, speech content editing, and voice conversion with simple modifications or the assistance of external tools, demonstrating the potential of MaskGCT as a foundational model for speech generation.
We provide more details in Appendix~\ref{appendix:s2s}, ~\ref{appendix:emo}, ~\ref{appendix:se}, ~\ref{appendix:vc}.

</td><td>

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td><td>

</td></tr></table>
