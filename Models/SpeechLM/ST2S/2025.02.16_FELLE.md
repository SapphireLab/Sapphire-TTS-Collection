# FELLE

<details>
<summary>基本信息</summary>

- 标题: "FELLE: Autoregressive Speech Synthesis with Token-Wise Coarse-to-Fine Flow Matching"
- 作者:
  - 01 Hui Wang,
  - 02 Shujie Liu,
  - 03 Lingwei Meng,
  - 04 Jinyu Li,
  - 05 Yifan Yang,
  - 06 Shiwan Zhao,
  - 07 Haiyang Sun,
  - 08 Yanqing Liu,
  - 09 Haoqin Sun,
  - 10 Jiaming Zhou,
  - 11 Yan Lu,
  - 12 Yong Qin
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.11128)
  - [Publication]()
  - [Github]()
  - [Demo](https://aka.ms/felle)
- 文件:
  - [ArXiv](../_PDF/2502.11128v1__FELLE__Autoregressive_Speech_Synthesis_with_Token-Wise_Coarse-to-Fine_Flow_Matching.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

To advance continuous-valued token modeling and temporal-coherence enforcement, we propose ***FELLE***, an autoregressive model that integrates language modeling with token-wise flow matching.
By leveraging the autoregressive nature of language models and the generative efficacy of flow matching, ***FELLE*** effectively predicts continuous-valued tokens (mel-spectrograms).
For each continuous-valued token, ***FELLE*** modifies the general prior distribution in flow matching by incorporating information from the previous step, improving coherence and stability.
Furthermore, to enhance synthesis quality, ***FELLE*** introduces a coarse-to-fine flow-matching mechanism, generating continuous-valued tokens hierarchically, conditioned on the language model’s output.
Experimental results demonstrate the potential of incorporating flow-matching techniques in autoregressive mel-spectrogram modeling, leading to significant improvements in TTS generation quality, as shown in https://aka.ms/felle.

</td><td>

为了推进连续值 Token 建模和时序一致性施加, 我们提出了 ***FELLE***, 一种将语言建模和 Token 级流匹配 ( Flow-Matching) 相结合的自回归模型.
通过利用语言模型的自回归特征和流匹配的生成效率, ***FELLE*** 可以有效地预测连续值 Token (梅尔频谱).
对于每个连续值 Token, ***FELLE*** 通过将前一步的信息融入流匹配的一般先验分布中, 增强了一致性和稳定性.
此外, 为了增强合成质量, ***FELLE*** 引入了从粗到细的流匹配机制, 以语言模型的输出为条件, 层次化地生成连续值 Token.

实验结果表明了将流匹配技术整合到自回归梅尔频谱建模中的潜力, 能够带来 TTS 生成质量的显著提升.
音频示例可见 https://aka.ms/felle.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

The remarkable success of large language models (LLMs) (**GPT-3**[^brown2020language], **GPT-4**[^achiam2023gpt], **Gemini 1.5**[^team2024gemini]) has prompted a paradigm shift in speech synthesis, redefining it as a language modeling task.
This shift has driven notable progress in zero-shot speech synthesis (**VALL-E**[^wang2023valle], **VALL-E2**[^chen2024valle2]).
Consistent with the standard LLM training methodology, researchers have naturally adopted discrete-valued tokens as the foundational modeling units.
However, unlike textual data, which is inherently discrete, speech signals require complex quantization techniques to transform continuous waveforms into discrete-valued tokens.
These essential quantization processes impose fundamental constraints compared to continuous representations, particularly in terms of fidelity preservation and training complexity ([^puvvada2024discrete], **MELLE**[^meng2024autoregressive]).
Consequently, discrete token-based text-to-speech (TTS) systems often face challenges such as intricate modeling workflows and reduced output quality.
In response to these limitations, recent research has increasingly explored autoregressive (AR) modeling frameworks that leverage continuous representations (**MELLE**[^meng2024autoregressive], **SALAD**[^turetzky2024continuous], **KALL-E**[^zhu2024autoregressive]), showing notable improvements in model performance and simplifying training processes.

</td><td>

</td></tr>
<tr><td>

However, modeling continuous representations introduces its own set of challenges.
Due to the rich information contained in continuous representations, modeling them demands more advanced capabilities from models.
Conventional regression-based loss functions used in **MELLE**[^meng2024autoregressive], including mean absolute error (MAE) and mean squared error (MSE), adopt oversimplified distributional assumptions.
These assumptions may not fully capture the multimodal structures and complex features of the distribution, leading to blurred, oversimplified, or averaged predictions [^vasquez2019melnet], [^ren-etal-2022-revisiting].
Similarly, **KALL-E** relies on WaveVAE-derived distributions, but the restrictive Gaussian prior assumption in **Variational Autoencoder (VAE)**[^kingma2013auto] limits their ability to model complex speech patterns, leading to low-diversity and blurry samples ([^tomczak2018vae], [^bredell2023explicitly]).

A further limitation of existing approaches lies in the inadequate modeling of temporal dependencies.
Current methodologies primarily use autoregressive architecture to implicitly capture temporal dependencies, yet they lack explicit mechanisms to model temporal relationships.
This structural characteristic may limit their effectiveness in handling complex temporal dependencies (**VALL-E R**[^han2024valler]).
For instance, **SALAD**[^turetzky2024continuous], which is based on diffusion processes, denoises tokens independently without explicit temporal modeling.
**MELLE**[^meng2024autoregressive] applies a flux loss focused solely on increasing frame-level variability, oversimplifying the modeling of temporal relationships.
Notably, continuous-valued tokens like mel-spectrograms inherently exhibit strong correlations across temporal and frequency dimensions [^ren-etal-2022-revisiting].
Insufficient consideration of these correlations could compromise the model's ability to preserve speech's sequential characteristics, potentially affecting output naturalness and requiring additional computational resources.

</td><td>

</td></tr>
<tr><td>

In this work, we introduce ***FELLE***, an autoregressive speech synthesis framework that utilizes token-wise coarse-to-fine flow matching for continuous-valued token modeling.
Unlike regression-based or VAE approaches (commonly used in other methods) constrained with preset distribution assumptions, **Flow Matching**[^lipman2022flow] enables flexible density estimation without restrictive prior assumptions, thereby preserving the multimodal characteristics of speech.
Meanwhile, by integrating the autoregressive properties of language models with flow-matching techniques, we develop a temporal modeling mechanism that dynamically adjusts the prior distribution of each frame through the integration of preceding contextual information.
This architecture effectively preserves temporal dependencies and ensures spectral continuity.
Moreover, we propose a coarse-to-fine flow-matching (C2F-FM) module to improve generation quality by capturing inter-frequency correlations.
It synthesizes mel-spectrogram features in multiple stages, inspired by the effectiveness of coarse-to-fine methods in discrete token modeling (**AudioLM**[^borsos2023audiolm], **Moshi**[^defossez2024moshi]), which capture structural dependencies in sequential tasks.
Evaluations on the **LibriSpeech corpus**[^panayotov2015librispeech] demonstrate the framework's competitiveness: compared to MELLE, our method achieves comparable Word Error Rates (WER) while delivering superior similarity scores in modeling complex mel-spectrogram patterns.
Our contributions can be summarized as:
- We propose an AR speech synthesis framework leveraging token-wise flow matching for continuous speech modeling, eliminating restrictive distribution assumptions while preserving speech signals' multimodal characteristics.
- We design a dynamic prior mechanism that modifies the vanilla prior distribution in flow matching by incorporating information from the previous step, improving coherence and stability.
- We introduce a coarse-to-fine flow matching architecture that explicitly captures inter-frequency correlations through multi-stage spectral refinement, achieving significant improvements in mel-spectrogram generation.

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

Zero-shot text-to-speech approaches are commonly categorized into autoregressive and non-autoregressive paradigms based on their output generation mechanisms.
Autoregressive systems typically rely on language model architectures (**VALL-E**[^wang2023valle], **SPEAR-TTS**[^kharitonov2023speak], **IST-LM**[^yang2024interleaved]), whereas non-autoregressive implementations commonly employ diffusion models and analogous methodologies (**NaturalSpeech3**[^junaturalspeech], **F5-TTS**[^chen2024f5]).
The subsequent discussion concentrates on research efforts investigating diverse representations under the framework of autoregressive language modeling architectures.

</td><td>

</td></tr>
<tr><td>

**Discrete-Valued Token-Based TTS**

TTS systems based on discrete representations utilize tokenized acoustic units derived from unsupervised or semi-supervised learning frameworks.
These discrete tokens serve as compact and efficient representations of speech, capturing phonetic and prosodic attributes while reducing redundancy in data storage and computation.
**VALL-E**[^wang2023valle] is a neural codec language model for text-to-speech synthesis that firstly redefines TTS as a conditional language modeling task, enabling high-quality, personalized speech generation from just a 3-second acoustic prompt, significantly advancing naturalness and speaker similarity.
Recent studies further enhance **VALL-E**’s capabilities across multilingual generalization (**VALL-E X**[^zhang2023vallex]), decoding efficiency (**VALL-E 2**[^chen2024valle2]), and robustness (**ELLA-V**[^song2024ellav], **RALL-E**[^xin2024ralle], **VALL-E R**[^han2024valler]), collectively advancing zero-shot speech synthesis in scalability, quality, and linguistic flexibility.
In contrast to the unified language modeling approach of **VALL-E** and its variants, **CosyVoice**[^du2024cosyvoice] leverages an LLM for text-to-token conversion followed by a conditional flow-matching model for token-to-spectrogram synthesis, enhancing zero-shot voice cloning through end-to-end supervised speech token learning.

</td></tr>
<tr><td>

**Continuous-Valued Token-Based TTS**

Recent advances in continuous representation-based TTS systems eliminate the need for cumbersome codec training while achieving promising performance.
Notably, **MELLE**[^meng2024autoregressive] proposes a single-pass language model architecture leveraging rich continuous acoustic representations, enabling precise control over prosodic features including pitch, rhythm, and timbre for high-fidelity speech synthesis.
In contrast, **SALAD**[^turetzky2024continuous] is a zero-shot text-to-speech system that employs a per-token latent diffusion model on continuous representations, enabling variable-length audio generation through semantic tokens for contextual guidance and stopping control.
While this method achieves superior intelligibility scores, it may face challenges related to time costs.
Alternatively, **KALL-E**[^zhu2024autoregressive] adopts an autoregressive approach with WaveVAE to directly model speech distributions, bypassing both VAE and diffusion paradigms, demonstrating enhanced naturalness and speaker similarity through probabilistic waveform prediction.

</td><td>

</td></tr>
<tr><td>

**Flow Matching**[^lipman2022flow] is a technique for learning a transformation that maps a prior distribution $p_0$ to a target distribution $q(x)$.
The core idea of flow matching is to define a flow $\phi_t(x)$ that evolves over time, transforming the prior distribution $p_0$ into the target distribution $q(x)$.
This flow $\phi_t(x)$ is governed by a vector field $v_t(x)$ and satisfies the following ordinary differential equation (ODE):

$$
\begin{aligned}
\frac{d}{dt} \phi_t(x) = v_t(\phi_t(x)), \quad \phi_0(x) = x.
\end{aligned}
$$

Here, $\phi_0(x) = x$ indicates that at time $t = 0$, the flow $\phi_t(x)$ is an identity mapping.

While flow matching provides a principled framework for learning such transformations, it can be computationally expensive due to the difficulty of directly accessing the true vector field $u_t(x)$ and the target distribution $q(x)$.
To address this, Conditional Flow Matching (CFM) is introduced.
In CFM, the flow and the vector field are conditioned on the data $x_1$, making the optimization process more efficient.
The objective of CFM is to minimize the discrepancy between the conditional true vector field $u_t$ and the learned conditional vector field $v_t(x; \theta)$.
This discrepancy is measured by the following loss function:

$$
\begin{aligned}
L_{\text{CFM}} = \mathbb{E}_{t, x_1, x} \left\| u_t - v_t(x; \theta) \right\|^2,
\end{aligned}
$$

where time $t$ is uniformly sampled from $\mathcal{U}[0,1]$, data points $x_1$ are drawn from the target distribution $q(x_1)$, samples $x$ are generated through the conditional probability path $p_t(x|x_1)$, and the conditional vector field $u_t \equiv u_t(x|x_1)$.

</td><td>

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td></tr></table>

## References: 参考文献

[^achiam2023gpt]: [**GPT-4**: GPT-4 Technical Report.](../../TextLM/2023.03.15_GPT-4.md) ArXiv:2303.08774.
[^borsos2023audiolm]: [**AudioLM**: A Language Modeling Approach to Audio Generation.](2022.09.07_AudioLM.md) TASLP2023.
[^bredell2023explicitly]: Explicitly Minimizing the Blur Error of Variational Autoencoders.
ICLR2023.
[^brown2020language]: [**GPT-3**: Language Models Are Few-Shot Learners.](../../TextLM/2020.05.28_GPT-3.md) NeurIPS2020.
[^chen2024f5]: [**F5-TTS**: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching.](../../Diffusion/2024.10.09_F5-TTS.md) ArXiv:2410.06885.
[^chen2024valle2]: [**VALL-E 2**: Neural Codec Language Models Are Human Parity Zero-Shot Text to Speech Synthesizers.](2024.06.08_VALL-E_2.md) ArXiv:2406.05370.
[^defossez2024moshi]: [**Moshi**: A Speech-Text Foundation Model for Real-Time Dialogue.](../../SpokenDialogue/2024.09.17_Moshi.md) ArXiv:2410.00037.
[^du2024cosyvoice]: [**Cosyvoice**: A Scalable Multilingual Zero-Shot Text-to-Speech Synthesizer Based on Supervised Semantic Tokens.](../2024.07.07_CosyVoice.md) ArXiv:2407.05407.
[^han2024valler]: [**VALL-E R**: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment.](2024.06.12_VALL-E_R.md) ArXiv:2406.07855.
[^junaturalspeech]: [**NaturalSpeech3**: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models.](../../Diffusion/2024.03.05_NaturalSpeech3.md) ICML2024.
[^kharitonov2023speak]: [**SPEAR-TTS**: Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision.](2023.02.07_SPEAR-TTS.md) TACL2023.
[^kingma2013auto]: [**VAE**: Auto-Encoding Variational Bayes.](../../_Basis/VAE.md) ICLR2014.
[^lipman2022flow]: [**Flow Matching**: Flow Matching for Generative Modeling](../../Diffusion/2022.10.06_Flow_Matching.md) ArXiv:2210.02747.
[^meng2024autoregressive]: [**MELLE**: Autoregressive Speech Synthesis without Vector Quantization.](2024.07.11_MELLE.md) ArXiv:2407.08551.
[^panayotov2015librispeech]: [**LibriSpeech**: An ASR Corpus Based on Public Domain Audio Books.](../../../Datasets/2015.04.19_LibriSpeech.md) ICASSP2015.
[^puvvada2024discrete]: Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition.
ICASSP2024.
[^ren-etal-2022-revisiting]: Revisiting Over-Smoothness in Text to Speech.
ACL2022.
[^song2024ellav]: [**ELLA-V**: Stable Neural Codec Language Modeling with Alignment-Guided Sequence Reordering.](2024.01.14_ELLA-V.md) ArXiv2024.
[^team2024gemini]: [**Gemini 1.5**: Unlocking Multimodal Understanding Across Millions of Tokens of Context.](../../TextLM/2024.03.08_Gemini_1.5.md) ArXiv:2403.05530.
[^tomczak2018vae]: VAE with a VampPrior.
AISTATS2018.
[^turetzky2024continuous]: [**SALAD**: Continuous Speech Synthesis Using Per-Token Latent Diffusion.](../../Diffusion/2024.10.21_SALAD.md) ArXiv:2410.16048.
[^vasquez2019melnet]: [**MelNet**: A Generative Model for Audio in the Frequency Domain.](../../Acoustic/2019.06.04_MelNet.md) ArXiv:1906.01083.
[^wang2023valle]: [**VALL-E**: Neural Codec Language Models Are Zero-Shot Text to Speech Synthesizers.](2023.01.05_VALL-E.md) ArXiv:2301.02111.
[^xin2024ralle]: [**RALL-E**: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis.](2024.04.04_RALL-E.md) ArXiv:2404.03204.
[^yang2024interleaved]: [**IST-LM**: Interleaved Speech-Text Language Models Are Simple Streaming Text to Speech Synthesizers.](2024.12.20_IST-LM.md) ArXiv:2412.16102.
[^zhang2023vallex]: [**VALL-E X**: Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling.](2023.03.07_VALL-E_X.md) ArXiv:2303.03926.
[^zhu2024autoregressive]: [**KALL-E**: Autoregressive Speech Synthesis with Next-Distribution Prediction.](2024.12.22_KALL-E.md) ArXiv:2412.16846.
