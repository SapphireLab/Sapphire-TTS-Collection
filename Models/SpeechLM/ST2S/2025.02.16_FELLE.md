# FELLE

<details>
<summary>基本信息</summary>

- 标题: "FELLE: Autoregressive Speech Synthesis with Token-Wise Coarse-to-Fine Flow Matching"
- 作者:
  - 01 Hui Wang,
  - 02 Shujie Liu,
  - 03 Lingwei Meng,
  - 04 Jinyu Li,
  - 05 Yifan Yang,
  - 06 Shiwan Zhao,
  - 07 Haiyang Sun,
  - 08 Yanqing Liu,
  - 09 Haoqin Sun,
  - 10 Jiaming Zhou,
  - 11 Yan Lu,
  - 12 Yong Qin
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.11128)
  - [Publication]()
  - [Github]()
  - [Demo](https://aka.ms/felle)
- 文件:
  - [ArXiv](../_PDF/2502.11128v1__FELLE__Autoregressive_Speech_Synthesis_with_Token-Wise_Coarse-to-Fine_Flow_Matching.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

To advance continuous-valued token modeling and temporal-coherence enforcement, we propose ***FELLE***, an autoregressive model that integrates language modeling with token-wise flow matching.
By leveraging the autoregressive nature of language models and the generative efficacy of flow matching, ***FELLE*** effectively predicts continuous-valued tokens (mel-spectrograms).
For each continuous-valued token, ***FELLE*** modifies the general prior distribution in flow matching by incorporating information from the previous step, improving coherence and stability.
Furthermore, to enhance synthesis quality, ***FELLE*** introduces a coarse-to-fine flow-matching mechanism, generating continuous-valued tokens hierarchically, conditioned on the language model’s output.
Experimental results demonstrate the potential of incorporating flow-matching techniques in autoregressive mel-spectrogram modeling, leading to significant improvements in TTS generation quality, as shown in https://aka.ms/felle.

</td><td>

为了推进连续值 Token 建模和时序一致性施加, 我们提出了 ***FELLE***, 一种将语言建模和 Token 级流匹配 ( Flow-Matching) 相结合的自回归模型.
通过利用语言模型的自回归特征和流匹配的生成效率, ***FELLE*** 可以有效地预测连续值 Token (梅尔频谱).
对于每个连续值 Token, ***FELLE*** 通过将前一步的信息融入流匹配的一般先验分布中, 增强了一致性和稳定性.
此外, 为了增强合成质量, ***FELLE*** 引入了从粗到细的流匹配机制, 以语言模型的输出为条件, 层次化地生成连续值 Token.

实验结果表明了将流匹配技术整合到自回归梅尔频谱建模中的潜力, 能够带来 TTS 生成质量的显著提升.
音频示例可见 https://aka.ms/felle.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

The remarkable success of large language models (LLMs) (**GPT-3**[^brown2020language], **GPT-4**[^achiam2023gpt], **Gemini 1.5**[^team2024gemini]) has prompted a paradigm shift in speech synthesis, redefining it as a language modeling task.
This shift has driven notable progress in zero-shot speech synthesis (**VALL-E**[^wang2023valle], **VALL-E2**[^chen2024valle2]).
Consistent with the standard LLM training methodology, researchers have naturally adopted discrete-valued tokens as the foundational modeling units.
However, unlike textual data, which is inherently discrete, speech signals require complex quantization techniques to transform continuous waveforms into discrete-valued tokens.
These essential quantization processes impose fundamental constraints compared to continuous representations, particularly in terms of fidelity preservation and training complexity ([^puvvada2024discrete], **MELLE**[^meng2024autoregressive]).
Consequently, discrete token-based text-to-speech (TTS) systems often face challenges such as intricate modeling workflows and reduced output quality.
In response to these limitations, recent research has increasingly explored autoregressive (AR) modeling frameworks that leverage continuous representations (**MELLE**[^meng2024autoregressive], **SALAD**[^turetzky2024continuous], **KALL-E**[^zhu2024autoregressive]), showing notable improvements in model performance and simplifying training processes.

</td><td>

</td></tr>
<tr><td>

However, modeling continuous representations introduces its own set of challenges.
Due to the rich information contained in continuous representations, modeling them demands more advanced capabilities from models.
Conventional regression-based loss functions used in **MELLE**[^meng2024autoregressive], including mean absolute error (MAE) and mean squared error (MSE), adopt oversimplified distributional assumptions.
These assumptions may not fully capture the multimodal structures and complex features of the distribution, leading to blurred, oversimplified, or averaged predictions [^vasquez2019melnet], [^ren-etal-2022-revisiting].
Similarly, **KALL-E** relies on WaveVAE-derived distributions, but the restrictive Gaussian prior assumption in **Variational Autoencoder (VAE)**[^kingma2013auto] limits their ability to model complex speech patterns, leading to low-diversity and blurry samples ([^tomczak2018vae], [^bredell2023explicitly]).

A further limitation of existing approaches lies in the inadequate modeling of temporal dependencies.
Current methodologies primarily use autoregressive architecture to implicitly capture temporal dependencies, yet they lack explicit mechanisms to model temporal relationships.
This structural characteristic may limit their effectiveness in handling complex temporal dependencies (**VALL-E R**[^han2024valler]).
For instance, **SALAD**[^turetzky2024continuous], which is based on diffusion processes, denoises tokens independently without explicit temporal modeling.
**MELLE**[^meng2024autoregressive] applies a flux loss focused solely on increasing frame-level variability, oversimplifying the modeling of temporal relationships.
Notably, continuous-valued tokens like mel-spectrograms inherently exhibit strong correlations across temporal and frequency dimensions [^ren-etal-2022-revisiting].
Insufficient consideration of these correlations could compromise the model's ability to preserve speech's sequential characteristics, potentially affecting output naturalness and requiring additional computational resources.

</td><td>

</td></tr>
<tr><td>

In this work, we introduce ***FELLE***, an autoregressive speech synthesis framework that utilizes token-wise coarse-to-fine flow matching for continuous-valued token modeling.
Unlike regression-based or VAE approaches (commonly used in other methods) constrained with preset distribution assumptions, **Flow Matching**[^lipman2022flow] enables flexible density estimation without restrictive prior assumptions, thereby preserving the multimodal characteristics of speech.
Meanwhile, by integrating the autoregressive properties of language models with flow-matching techniques, we develop a temporal modeling mechanism that dynamically adjusts the prior distribution of each frame through the integration of preceding contextual information.
This architecture effectively preserves temporal dependencies and ensures spectral continuity.
Moreover, we propose a coarse-to-fine flow-matching (C2F-FM) module to improve generation quality by capturing inter-frequency correlations.
It synthesizes mel-spectrogram features in multiple stages, inspired by the effectiveness of coarse-to-fine methods in discrete token modeling (**AudioLM**[^borsos2023audiolm], **Moshi**[^defossez2024moshi]), which capture structural dependencies in sequential tasks.
Evaluations on the **LibriSpeech corpus**[^panayotov2015librispeech] demonstrate the framework's competitiveness: compared to MELLE, our method achieves comparable Word Error Rates (WER) while delivering superior similarity scores in modeling complex mel-spectrogram patterns.
Our contributions can be summarized as:
- We propose an AR speech synthesis framework leveraging token-wise flow matching for continuous speech modeling, eliminating restrictive distribution assumptions while preserving speech signals' multimodal characteristics.
- We design a dynamic prior mechanism that modifies the vanilla prior distribution in flow matching by incorporating information from the previous step, improving coherence and stability.
- We introduce a coarse-to-fine flow matching architecture that explicitly captures inter-frequency correlations through multi-stage spectral refinement, achieving significant improvements in mel-spectrogram generation.

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td></tr></table>
