# FELLE

<details>
<summary>基本信息</summary>

- 标题: "FELLE: Autoregressive Speech Synthesis with Token-Wise Coarse-to-Fine Flow Matching"
- 作者:
  - 01 Hui Wang,
  - 02 Shujie Liu,
  - 03 Lingwei Meng,
  - 04 Jinyu Li,
  - 05 Yifan Yang,
  - 06 Shiwan Zhao,
  - 07 Haiyang Sun,
  - 08 Yanqing Liu,
  - 09 Haoqin Sun,
  - 10 Jiaming Zhou,
  - 11 Yan Lu,
  - 12 Yong Qin
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.11128)
  - [Publication]()
  - [Github]()
  - [Demo](https://aka.ms/felle)
- 文件:
  - [ArXiv](../_PDF/2502.11128v1__FELLE__Autoregressive_Speech_Synthesis_with_Token-Wise_Coarse-to-Fine_Flow_Matching.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

To advance continuous-valued token modeling and temporal-coherence enforcement, we propose ***FELLE***, an autoregressive model that integrates language modeling with token-wise flow matching.
By leveraging the autoregressive nature of language models and the generative efficacy of flow matching, ***FELLE*** effectively predicts continuous-valued tokens (mel-spectrograms).
For each continuous-valued token, ***FELLE*** modifies the general prior distribution in flow matching by incorporating information from the previous step, improving coherence and stability.
Furthermore, to enhance synthesis quality, ***FELLE*** introduces a coarse-to-fine flow-matching mechanism, generating continuous-valued tokens hierarchically, conditioned on the language model’s output.
Experimental results demonstrate the potential of incorporating flow-matching techniques in autoregressive mel-spectrogram modeling, leading to significant improvements in TTS generation quality, as shown in https://aka.ms/felle.

</td><td>

为了推进连续值 Token 建模和时序一致性施加, 我们提出了 ***FELLE***, 一种将语言建模和 Token 级流匹配 ( Flow-Matching) 相结合的自回归模型.
通过利用语言模型的自回归特征和流匹配的生成效率, ***FELLE*** 可以有效地预测连续值 Token (梅尔频谱).
对于每个连续值 Token, ***FELLE*** 通过将前一步的信息融入流匹配的一般先验分布中, 增强了一致性和稳定性.
此外, 为了增强合成质量, ***FELLE*** 引入了从粗到细的流匹配机制, 以语言模型的输出为条件, 层次化地生成连续值 Token.

实验结果表明了将流匹配技术整合到自回归梅尔频谱建模中的潜力, 能够带来 TTS 生成质量的显著提升.
音频示例可见 https://aka.ms/felle.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

The remarkable success of large language models (LLMs) (**GPT-3**[^brown2020language], **GPT-4**[^achiam2023gpt], **Gemini 1.5**[^team2024gemini]) has prompted a paradigm shift in speech synthesis, redefining it as a language modeling task.
This shift has driven notable progress in zero-shot speech synthesis (**VALL-E**[^wang2023valle], **VALL-E2**[^chen2024valle2]).
Consistent with the standard LLM training methodology, researchers have naturally adopted discrete-valued tokens as the foundational modeling units.
However, unlike textual data, which is inherently discrete, speech signals require complex quantization techniques to transform continuous waveforms into discrete-valued tokens.
These essential quantization processes impose fundamental constraints compared to continuous representations, particularly in terms of fidelity preservation and training complexity ([^puvvada2024discrete], **MELLE**[^meng2024autoregressive]).
Consequently, discrete token-based text-to-speech (TTS) systems often face challenges such as intricate modeling workflows and reduced output quality.
In response to these limitations, recent research has increasingly explored autoregressive (AR) modeling frameworks that leverage continuous representations (**MELLE**[^meng2024autoregressive], **SALAD**[^turetzky2024continuous], **KALL-E**[^zhu2024autoregressive]), showing notable improvements in model performance and simplifying training processes.

</td><td>

</td></tr>
<tr><td>

However, modeling continuous representations introduces its own set of challenges.
Due to the rich information contained in continuous representations, modeling them demands more advanced capabilities from models.
Conventional regression-based loss functions used in **MELLE**[^meng2024autoregressive], including mean absolute error (MAE) and mean squared error (MSE), adopt oversimplified distributional assumptions.
These assumptions may not fully capture the multimodal structures and complex features of the distribution, leading to blurred, oversimplified, or averaged predictions [^vasquez2019melnet], [^ren-etal-2022-revisiting].
Similarly, **KALL-E** relies on WaveVAE-derived distributions, but the restrictive Gaussian prior assumption in **Variational Autoencoder (VAE)**[^kingma2013auto] limits their ability to model complex speech patterns, leading to low-diversity and blurry samples ([^tomczak2018vae], [^bredell2023explicitly]).

A further limitation of existing approaches lies in the inadequate modeling of temporal dependencies.
Current methodologies primarily use autoregressive architecture to implicitly capture temporal dependencies, yet they lack explicit mechanisms to model temporal relationships.
This structural characteristic may limit their effectiveness in handling complex temporal dependencies (**VALL-E R**[^han2024valler]).
For instance, **SALAD**[^turetzky2024continuous], which is based on diffusion processes, denoises tokens independently without explicit temporal modeling.
**MELLE**[^meng2024autoregressive] applies a flux loss focused solely on increasing frame-level variability, oversimplifying the modeling of temporal relationships.
Notably, continuous-valued tokens like mel-spectrograms inherently exhibit strong correlations across temporal and frequency dimensions [^ren-etal-2022-revisiting].
Insufficient consideration of these correlations could compromise the model's ability to preserve speech's sequential characteristics, potentially affecting output naturalness and requiring additional computational resources.

</td><td>

</td></tr>
<tr><td>

In this work, we introduce ***FELLE***, an autoregressive speech synthesis framework that utilizes token-wise coarse-to-fine flow matching for continuous-valued token modeling.
Unlike regression-based or VAE approaches (commonly used in other methods) constrained with preset distribution assumptions, **Flow Matching**[^lipman2022flow] enables flexible density estimation without restrictive prior assumptions, thereby preserving the multimodal characteristics of speech.
Meanwhile, by integrating the autoregressive properties of language models with flow-matching techniques, we develop a temporal modeling mechanism that dynamically adjusts the prior distribution of each frame through the integration of preceding contextual information.
This architecture effectively preserves temporal dependencies and ensures spectral continuity.
Moreover, we propose a coarse-to-fine flow-matching (C2F-FM) module to improve generation quality by capturing inter-frequency correlations.
It synthesizes mel-spectrogram features in multiple stages, inspired by the effectiveness of coarse-to-fine methods in discrete token modeling (**AudioLM**[^borsos2023audiolm], **Moshi**[^defossez2024moshi]), which capture structural dependencies in sequential tasks.
Evaluations on the **LibriSpeech corpus**[^panayotov2015librispeech] demonstrate the framework's competitiveness: compared to **MELLE**, our method achieves comparable Word Error Rates (WER) while delivering superior similarity scores in modeling complex mel-spectrogram patterns.
Our contributions can be summarized as:
- We propose an AR speech synthesis framework leveraging token-wise flow matching for continuous speech modeling, eliminating restrictive distribution assumptions while preserving speech signals' multimodal characteristics.
- We design a dynamic prior mechanism that modifies the vanilla prior distribution in flow matching by incorporating information from the previous step, improving coherence and stability.
- We introduce a coarse-to-fine flow matching architecture that explicitly captures inter-frequency correlations through multi-stage spectral refinement, achieving significant improvements in mel-spectrogram generation.

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

Zero-shot text-to-speech approaches are commonly categorized into autoregressive and non-autoregressive paradigms based on their output generation mechanisms.
Autoregressive systems typically rely on language model architectures (**VALL-E**[^wang2023valle], **SPEAR-TTS**[^kharitonov2023speak], **IST-LM**[^yang2024interleaved]), whereas non-autoregressive implementations commonly employ diffusion models and analogous methodologies (**NaturalSpeech3**[^junaturalspeech], **F5-TTS**[^chen2024f5]).
The subsequent discussion concentrates on research efforts investigating diverse representations under the framework of autoregressive language modeling architectures.

</td><td>

</td></tr>
<tr><td>

**Discrete-Valued Token-Based TTS**

TTS systems based on discrete representations utilize tokenized acoustic units derived from unsupervised or semi-supervised learning frameworks.
These discrete tokens serve as compact and efficient representations of speech, capturing phonetic and prosodic attributes while reducing redundancy in data storage and computation.
**VALL-E**[^wang2023valle] is a neural codec language model for text-to-speech synthesis that firstly redefines TTS as a conditional language modeling task, enabling high-quality, personalized speech generation from just a 3-second acoustic prompt, significantly advancing naturalness and speaker similarity.
Recent studies further enhance **VALL-E**’s capabilities across multilingual generalization (**VALL-E X**[^zhang2023vallex]), decoding efficiency (**VALL-E 2**[^chen2024valle2]), and robustness (**ELLA-V**[^song2024ellav], **RALL-E**[^xin2024ralle], **VALL-E R**[^han2024valler]), collectively advancing zero-shot speech synthesis in scalability, quality, and linguistic flexibility.
In contrast to the unified language modeling approach of **VALL-E** and its variants, **CosyVoice**[^du2024cosyvoice] leverages an LLM for text-to-token conversion followed by a conditional flow-matching model for token-to-spectrogram synthesis, enhancing zero-shot voice cloning through end-to-end supervised speech token learning.

</td></tr>
<tr><td>

**Continuous-Valued Token-Based TTS**

Recent advances in continuous representation-based TTS systems eliminate the need for cumbersome codec training while achieving promising performance.
Notably, **MELLE**[^meng2024autoregressive] proposes a single-pass language model architecture leveraging rich continuous acoustic representations, enabling precise control over prosodic features including pitch, rhythm, and timbre for high-fidelity speech synthesis.
In contrast, **SALAD**[^turetzky2024continuous] is a zero-shot text-to-speech system that employs a per-token latent diffusion model on continuous representations, enabling variable-length audio generation through semantic tokens for contextual guidance and stopping control.
While this method achieves superior intelligibility scores, it may face challenges related to time costs.
Alternatively, **KALL-E**[^zhu2024autoregressive] adopts an autoregressive approach with WaveVAE to directly model speech distributions, bypassing both VAE and diffusion paradigms, demonstrating enhanced naturalness and speaker similarity through probabilistic waveform prediction.

</td><td>

</td></tr>
<tr><td>

**Flow Matching**[^lipman2022flow] is a technique for learning a transformation that maps a prior distribution $p_0$ to a target distribution $q(x)$.
The core idea of flow matching is to define a flow $\phi_t(x)$ that evolves over time, transforming the prior distribution $p_0$ into the target distribution $q(x)$.
This flow $\phi_t(x)$ is governed by a vector field $v_t(x)$ and satisfies the following ordinary differential equation (ODE):

$$
\begin{aligned}
\frac{d}{dt} \phi_t(x) = v_t(\phi_t(x)), \quad \phi_0(x) = x.
\end{aligned}
$$

Here, $\phi_0(x) = x$ indicates that at time $t = 0$, the flow $\phi_t(x)$ is an identity mapping.

While flow matching provides a principled framework for learning such transformations, it can be computationally expensive due to the difficulty of directly accessing the true vector field $u_t(x)$ and the target distribution $q(x)$.
To address this, Conditional Flow Matching (CFM) is introduced.
In CFM, the flow and the vector field are conditioned on the data $x_1$, making the optimization process more efficient.
The objective of CFM is to minimize the discrepancy between the conditional true vector field $u_t$ and the learned conditional vector field $v_t(x; \theta)$.
This discrepancy is measured by the following loss function:

$$
\begin{aligned}
L_{\text{CFM}} = \mathbb{E}_{t, x_1, x} \left\| u_t - v_t(x; \theta) \right\|^2,
\end{aligned}
$$

where time $t$ is uniformly sampled from $\mathcal{U}[0,1]$, data points $x_1$ are drawn from the target distribution $q(x_1)$, samples $x$ are generated through the conditional probability path $p_t(x|x_1)$, and the conditional vector field $u_t \equiv u_t(x|x_1)$.

</td><td>

</td></tr></table>

## 3·Methodology: 方法

### Problem Formulation: Token-wise Flow Matching for AR Model

<table><tr><td width="50%">

Following **MELLE**'s autoregressive language modeling framework for mel-spectrogram prediction, we reformulate zero-shot TTS through a hierarchical flow-matching mechanism at each prediction step.
Each mel-spectrogram frame $\bm{x}^i \in \mathbb{R}^D$ (where $D$ denotes the mel-band dimension) is treated as a continuous token, generated sequentially through an autoregressive process.
Given an input text sequence $\bm{y} = [y^0, \ldots, y^{N-1}]$, speech prompt $\bm{\widehat{x}}$, and previously generated tokens $\bm{x}^{<i} = [\bm{x}^0, \ldots, \bm{x}^{i-1}]$, the model predicts the current token $\bm{x}^i$ by integrating language model guidance into the flow-matching paradigm.
The joint distribution is decomposed autoregressively as:
$$
\begin{aligned}
    p(\bm{X} \! \mid\!\bm{y})\!
   &= \prod_{i=0}^{L-1} p(\bm{x}^i \mid \bm{x}^{<i}, \bm{y}, \bm{\widehat{x}})  \\
   &=\! \prod_{i=0}^{L-1} p_{\theta_\text{FM}}(\bm{x}^i \mid \bm{z}^i), \bm{z}^i\!=\!f_{\theta_\text{LM}}(\bm{x}^{<i}, \bm{y},\bm{\widehat{x}} \notag) .
\end{aligned}
$$

$\bm{X} = [\bm{x}^0, \ldots, \bm{x}^{L-1}] \in \mathbb{R}^{L \times D}$ denotes full mel-spectrogram sequence, $L$ represents the total number of mel-spectrogram frames.
The language model $f_{\theta_\text{LM}}(\cdot)$ generates hidden state $\bm{z}^i$ that captures both linguistic content and acoustic context, while $p_{\theta_\text{FM}}(\cdot \mid \bm{z}^i)$ denotes the flow-matching module that transforms prior distributions into target distributions conditioned on $\bm{z}^i$.

</td><td>

</td></tr></table>

### Architecture

<table><tr><td width="50%">

The proposed framework combines an autoregressive language model with a flow-matching mechanism, which facilitates the progressive generation of high-fidelity speech.
As shown in [Figure.01](#Fig.01), the autoregressive model $f_{\theta_\text{LM}}$ extracts features from the text prompt $\bm{y}$ and speech prompt  $\bm{\widehat{x}}$, generating latent representations $\bm{z}^i$ (where $i$ denotes the generation step) that serve as conditional inputs for the flow-matching mechanism.
The flow-matching mechanism applies a coarse-to-fine strategy to generate high-quality mel-spectrogram frames $\bm{x}^i$.
The main components of the approach are described in detail below.

</td></tr>
<tr><td colspan="2">

![](../Images/2025.02.16_FELLE_Fig.01.png)

<a id="#Fig.02">Figure.01</a>: Overview of ***FELLE***, an autoregressive mel-spectrograms model that generates personalized speech from text and acoustic prompts.
At each timestep, the framework relies on the previous mel-spectrogram distribution as a prior, conditioned on the output of the language model, applying a coarse-to-fine flow-matching module to produce refined spectral features.

</td></tr></table>

### Autoregressive Language Model

<table><tr><td width="50%">

The language model, designed as a unidirectional Transformer decoder, generates acoustic features autoregressively by utilizing both text sequences and mel-spectrogram prompts.
In the initial step, the text tokens are embedded, while a pre-net maps the mel-spectrogram into the dimensional space of the LM.
By processing the combined text $\bm{y}$, speech prompt $\bm{\widehat{x}}$, and acoustic embeddings $\bm{x}^{<i}$, the language model $f_{\theta_\text{LM}}$ processes multi-head attention and feed-forward layers to capture the intricate relationship between linguistic and acoustic information.
The output at each time step subsequently serves as a conditioning input for the coarse-to-fine flow-matching module to synthesize the next-frame acoustic features.

</td><td>

</td></tr></table>

### Coarse-to-Fine Flow Matching

<table><tr><td width="50%">

For high-quality mel-spectrogram generation, we introduce a coarse-to-fine flow-matching approach.
As illustrated in [Figure.02](#Fig.02), the method generates each mel-spectrogram frame based on its preceding frame, maintaining temporal consistency throughout the sequence.
The generation process is divided into two phases: a coarse generation phase followed by a fine refinement phase.
A detailed introduction will be given below.

</td><td>

</td></tr>
<tr><td colspan="2">

![](../Images/2025.02.16_FELLE_Fig.02.png)

<a id="#Fig.02">Figure.02</a>: The coarse-to-fine flow-matching module of ***FELLE***.
(a) The training process along with the detailed data flow within the coarse-to-fine module.
The gray dashed lines merely indicate the relationships between components in the model structure and are not activated during training.
(b) The inference process.

</td></tr>
<tr><td>

**Prior Distribution**

Flow-matching-based methods in speech synthesis commonly adopt a simple prior distribution **VoiceBox**[^le2024voicebox], **Matcha-TTS**[^mehta2024matcha], as prior knowledge is often challenging to define precisely (**F5-TTS**[^chen2024f5]).
However, utilizing a prior distribution that closely aligns with the target distribution can significantly enhance computational efficiency and synthesis quality (**SpeechGPT**[^zhang2024speechgpt]).
Given the autoregressive nature of token generation and the sequential structure of speech,***FELLE*** employs the preceding token as an informative prior to guide the flow matching process for generating the current token.
Specifically, the prior distribution $p_0$ for the initial state $x_0^i$ of the current frame $x^i$ is derived from the mel-spectrogram of the previous frame $x^{i-1}$:

$$
\begin{aligned}
p_0(x_0^i | x^{i-1}) = \mathcal{N}(x_0^i | x^{i-1}, \sigma^2 I),
\end{aligned}
$$

where $\sigma^2 I$ represents the covariance matrix of the Gaussian noise.
For $i = 0$, where no prior frame exists, the initial state is drawn from a standard Gaussian distribution.

</td><td>

</td></tr>
<tr><td>

**Coarse-to-Fine Generation**

Our method combines autoregressive language modeling with hierarchical flow matching.
Each step $i$ follows a two-stage process, as illustrated in [Figure.02(a)](#Fig.02): a coarse flow-matching phase that produces an initial low-resolution mel-spectrogram representation, followed by a fine flow-matching phase that enhances the output by incorporating both the coarse representation and language model outputs.

The coarse generation stage is designed to produce the low-resolution component $x^{i,c}$ of the $i$-th frame through a downsampling operation $x^{i,c} = \mathrm{Downsample}(x^i)$.
In this framework, the coarse flow-matching model predicts a vector field $v_t^c(x^{i,c}, z^i; \theta_{\text{FM}}^c)$ by conditioning on linguistic features $z^i$ extracted from the language model.

In the fine stage, the model refines this approximation by recovering details $x^{i,f}$, represented as the residual between the original frame $x^i$ and the upsampled coarse component $\text{\textit{Upsample}}(x^{i,c})$.
A secondary flow-matching model predicts the vector field $v_t^f(x^{i,f}, z^i, x^{i,c}; \theta_\text{FM}^f)$, governing this process by leveraging both the features $z^i$ and the coarse component (with ground-truth coarse features $x^{i,c}$ during training and predicted values $\tilde{x}^{i,c}$ during inference) as conditional inputs.
This hierarchical conditioning allows the fine model to focus on local details while preserving global coherence from the coarse stage.

For step $i$, the training objective combines losses from both stages:
$$
\begin{aligned}
\mathcal{L}_{\text{C2F-FM}} = \underbrace{\mathbb{E}_{t, x_1^{i,c}, x^{i,c}} \left\| u_t^c - v_t^c(x^{i,c}, z^i; \theta_\text{FM}^c) \right\|^2}_{\text{Coarse Stage}} + \underbrace{\mathbb{E}_{t, x_1^{i,f}, x^{i,f}} \left\| u_t^f - v_t^f(x^{i,f}, z^i, x_1^{i,c}; \theta_\text{FM}^f) \right\|^2}_{\text{Fine Stage}},
\end{aligned}
$$
where $u_t^c$ and $u_t^f$ represent the true conditional vector fields for the coarse and fine components, respectively, and $t \sim \mathcal{U}[0,1] $.
The initial states $x_0^{i,c}$ and $x_0^{i,f}$ are similarly initialized using the prior from Equation~\ref{eq:prior}, applying the corresponding sampling operations.
By decoupling low-resolution structure learning from high-detail refinement, this coarse-to-fine approach generates high-fidelity mel-spectrograms while maintaining temporal consistency through autoregressive dependencies.

</td><td>

</td></tr>
<tr><td>

**Classifier-Free Guidance (CFG)** is a powerful technique to enhance the quality and controllability of generated outputs in flow matching and diffusion models (**CFG**[^ho2022classifier], **iDDPM**[^nichol2021improved]).
In ***FELLE***, we implement CFG through joint training of coarse and fine flow matching models using both conditional and unconditional objectives.
During training, we randomly mask the speech prompt with probability $p_{\text{drop}}$ for unconditional learning, which enables each model to learn dual vector fields.

At inference, guided vector fields are computed through linear blending:

$$
\begin{aligned}
\hat{v}_t^\ast(x^\ast; \cdot) &= w v_t^\ast(x^\ast, c; \theta_\text{FM}^\ast) + (1-w) v_t^\ast(x^\ast, \bar{c}; \theta_\text{FM}^\ast),
\end{aligned}
$$

where $\ast \in \{c,f\}$ denotes the model stage, $c$ represents the full conditions, $\bar{c}$ indicates the reduced conditioning state where the speaker prompt is masked, and $w$ represents the guidance scale.

</td><td>

</td></tr></table>

### Training Objective

<table><tr><td width="50%">

In ***FELLE***, we integrate the condition loss $\mathcal{L}_\text{cond}$ in addition to coarse-to-fine loss $\mathcal{L}_{\text{C2F-FM}}$.
$\mathcal{L}_\text{cond}$ is a hybrid loss function that combines L1 and L2 norms, defined as $\mathcal{L}_{\text{cond}} = \|z_i - x_i\|_1 + \|z_i - x_i\|_2^2$, for step $i$ to regularize the conditional input for flow matching.
Additionally, we introduce a stop prediction module to the autoregressive language model.
This module, during each step of generation, transforms the hidden state output by the language model into the probability of a stop signal through a linear layer and calculates the Binary Cross-Entropy loss $\mathcal{L}_\text{stop}$ for training.
The model can automatically determine when to stop during the generation process without the need to preset length rules.
The overall training objective is:

$$
\begin{aligned}
\mathcal{L} = \mathcal{L}_{\text{C2F-FM}} + \lambda \mathcal{L}_{\text{cond}} + \alpha \mathcal{L}_{\text{stop}}
\end{aligned}
$$

where $\lambda$ and $\alpha$ control the respective contributions of $\mathcal{L}_{\text{cond}}$ and $\mathcal{L}_{\text{stop}}$.

</td><td>

</td></tr></table>

### Inference

<table><tr><td width="50%">

As illustrated in [Figure.02(b)](#Fig.02), the inference process employs an autoregressive language model that progressively generates hidden representations based on textual and speaker prompts.
At each step $i$, the computed latent state $z_i$ serves two key purposes.
First, it provides conditional guidance for the coarse flow-matching module, facilitating the gradual transformation from the previous mel-spectrogram approximation $\tilde{x}^{i-1,c}$ to the current coarse structural estimate $\tilde{x}^{i,c}$.
Following this coarse estimation phase, the integrated information of $\tilde{x}^{i,c}$ and $z_i$ drives the fine flow-matching module to produce the fined mel-spectrogram frame $\tilde{x}^{i,f}$.
The final output frame $\tilde{x}^i$ emerges through the integration of these complementary coarse and refined predictions.
Secondly, the latent state $z_i$ processed by the stop prediction module to compute the stop probability, which is compared against a predefined threshold to decide whether to terminate the process.
The iterative generation continues until the stop criterion is satisfied, after which a neural vocoder converts the accumulated mel-spectrogram into the final speech waveform.

</td><td>

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

In this paper, we propose a novel autoregressive speech synthesis framework based on continuous representations, which overcomes the limitations of temporal consistency and model capacity in existing systems.
By leveraging the sequential nature of language models and the temporal dynamics of speech signals,***FELLE*** utilizes pervious tokens to assist in the flow-matching generation process.
A coarse-to-fine flow-matching architecture is then developed, capturing both temporal and spectral correlations present in mel-spectrograms, allowing for precise modeling of each continuous token.
Experimental results show that our model consistently outperforms several baseline systems across various evaluation metrics, producing clear and natural speech with significantly improved similarity.

</td><td>

</td></tr></table>

## References: 参考文献

[^achiam2023gpt]: [**GPT-4**: GPT-4 Technical Report.](../../TextLM/2023.03.15_GPT-4.md) ArXiv:2303.08774.
[^borsos2023audiolm]: [**AudioLM**: A Language Modeling Approach to Audio Generation.](2022.09.07_AudioLM.md) TASLP2023.
[^bredell2023explicitly]: Explicitly Minimizing the Blur Error of Variational Autoencoders.
ICLR2023.
[^brown2020language]: [**GPT-3**: Language Models Are Few-Shot Learners.](../../TextLM/2020.05.28_GPT-3.md) NeurIPS2020.
[^chen2024f5]: [**F5-TTS**: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching.](../../Diffusion/2024.10.09_F5-TTS.md) ArXiv:2410.06885.
[^chen2024valle2]: [**VALL-E 2**: Neural Codec Language Models Are Human Parity Zero-Shot Text to Speech Synthesizers.](2024.06.08_VALL-E_2.md) ArXiv:2406.05370.
[^defossez2024moshi]: [**Moshi**: A Speech-Text Foundation Model for Real-Time Dialogue.](../../SpokenDialogue/2024.09.17_Moshi.md) ArXiv:2410.00037.
[^du2024cosyvoice]: [**Cosyvoice**: A Scalable Multilingual Zero-Shot Text-to-Speech Synthesizer Based on Supervised Semantic Tokens.](../2024.07.07_CosyVoice.md) ArXiv:2407.05407.
[^han2024valler]: [**VALL-E R**: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment.](2024.06.12_VALL-E_R.md) ArXiv:2406.07855.
[^ho2022classifier]: [**CFG**: Classifier-Free Diffusion Guidance.](../../Diffusion/2022.07.26_Classifier-Free_Guidance.md) NeurIPS2021.
[^junaturalspeech]: [**NaturalSpeech3**: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models.](../../Diffusion/2024.03.05_NaturalSpeech3.md) ICML2024.
[^kharitonov2023speak]: [**SPEAR-TTS**: Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision.](2023.02.07_SPEAR-TTS.md) TACL2023.
[^kingma2013auto]: [**VAE**: Auto-Encoding Variational Bayes.](../../_Basis/VAE.md) ICLR2014.
[^le2024voicebox]: [**VoiceBox**: Text-Guided Multilingual Universal Speech Generation at Scale.](../2023.06.23_VoiceBox.md) NeurIPS2024.
[^lipman2022flow]: [**Flow Matching**: Flow Matching for Generative Modeling](../../Diffusion/2022.10.06_Flow_Matching.md) ArXiv:2210.02747.
[^mehta2024matcha]: [**Matcha-TTS**: A Fast TTS Architecture with Conditional Flow Matching.](../../Diffusion/2023.09.06_Matcha-TTS.md) ICASSP2024.
[^meng2024autoregressive]: [**MELLE**: Autoregressive Speech Synthesis without Vector Quantization.](2024.07.11_MELLE.md) ArXiv:2407.08551.
[^nichol2021improved]: [**iDDPM**: Improved Denoising Diffusion Probabilistic Models.](../../Diffusion/2021.02.18_iDDPM.md) ICML2021.
[^panayotov2015librispeech]: [**LibriSpeech**: An ASR Corpus Based on Public Domain Audio Books.](../../../Datasets/2015.04.19_LibriSpeech.md) ICASSP2015.
[^puvvada2024discrete]: Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition.
ICASSP2024.
[^ren-etal-2022-revisiting]: Revisiting Over-Smoothness in Text to Speech.
ACL2022.
[^song2024ellav]: [**ELLA-V**: Stable Neural Codec Language Modeling with Alignment-Guided Sequence Reordering.](2024.01.14_ELLA-V.md) ArXiv2024.
[^team2024gemini]: [**Gemini 1.5**: Unlocking Multimodal Understanding Across Millions of Tokens of Context.](../../TextLM/2024.03.08_Gemini_1.5.md) ArXiv:2403.05530.
[^tomczak2018vae]: VAE with a VampPrior.
AISTATS2018.
[^turetzky2024continuous]: [**SALAD**: Continuous Speech Synthesis Using Per-Token Latent Diffusion.](../../Diffusion/2024.10.21_SALAD.md) ArXiv:2410.16048.
[^vasquez2019melnet]: [**MelNet**: A Generative Model for Audio in the Frequency Domain.](../../Acoustic/2019.06.04_MelNet.md) ArXiv:1906.01083.
[^wang2023valle]: [**VALL-E**: Neural Codec Language Models Are Zero-Shot Text to Speech Synthesizers.](2023.01.05_VALL-E.md) ArXiv:2301.02111.
[^xin2024ralle]: [**RALL-E**: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis.](2024.04.04_RALL-E.md) ArXiv:2404.03204.
[^yang2024interleaved]: [**IST-LM**: Interleaved Speech-Text Language Models Are Simple Streaming Text to Speech Synthesizers.](2024.12.20_IST-LM.md) ArXiv:2412.16102.
[^zhang2023vallex]: [**VALL-E X**: Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling.](2023.03.07_VALL-E_X.md) ArXiv:2303.03926.
[^zhang2024speechgpt]: [**SpeechGPT**: Empowering Large Language Models with Intrinsic Crossmodal Conversational Abilities.](../../SpokenDialogue/2023.05.18_SpeechGPT.md) EMNLP2023.
[^zhu2024autoregressive]: [**KALL-E**: Autoregressive Speech Synthesis with Next-Distribution Prediction.](2024.12.22_KALL-E.md) ArXiv:2412.16846.
