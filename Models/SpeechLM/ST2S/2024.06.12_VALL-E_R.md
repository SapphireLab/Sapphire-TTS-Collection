# VALL-E R

<details>
<summary>基本信息</summary>

- 标题: "VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment"
- 作者:
  - 01 Bing Han
  - 02 Long Zhou (周龙)
  - 03 Shujie Liu (刘树杰)
  - 04 Sanyuan Chen (陈三元)
  - 05 Lingwei Meng
  - 06 Yanming Qian
  - 07 Yanqing Liu
  - 08 Sheng Zhao (赵胜)
  - 09 Jinyu Li (李劲宇)
  - 10 Furu Wei (韦福如)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.07855)
  - [Publication]()
  - [Github]()
  - [Demo](https://aka.ms/valler)
- 文件:
  - [ArXiv](../_PDF/2406.07855v1__VALL-E_R__Robust_and_Efficient_Zero-Shot_Text-to-Speech_Synthesis_via_Monotonic_Alignment.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis.
However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition.
In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression.
To address these issues, we propose ***VALL-E R***, a robust and efficient zero-shot TTS system, building upon the foundation of VALL-E [Wang et al., 2023a].
Specifically, we introduce a phoneme monotonic alignment strategy to strengthen the connection between phonemes and acoustic sequence, ensuring a more precise alignment by constraining the acoustic tokens to match their associated phonemes.
Furthermore, we employ a codec-merging approach to downsample the discrete codes in shallow quantization layer, thereby accelerating the decoding speed while preserving the high quality of speech output.
Benefiting from these strategies, ***VALL-E R*** obtains controllability over phonemes and demonstrates its strong robustness by approaching the WER of ground truth.
In addition, it requires fewer autoregressive steps, with over 60% time reduction during inference.
This research has the potential to be applied to meaningful projects, including the creation of speech for those affected by aphasia.
Audio samples will be available at: https://aka.ms/valler.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

In this study, we propose a robust and efficient zero-shot TTS system named ***VALL-E R***.
We first introduce the codec-merging approach in Section 3.1 which can improve inference speed without retraining the codec, and then illustrate the monotonic alignment strategy in decoder-only neural codec LM in Section 3.2.

### 3.1·Codec-Merging Approach

Building upon the foundational work of [EnCodec](../../SpeechCodec/2022.10.24_EnCodec.md), we introduce the concept of a merged codec.
This innovative approach enables the downsampling of discrete codes across various layers through alterations in the inference forward process.
Remarkably, this is achieved without necessitating any retraining or fine-tuning of the model, presenting a significant advancement in the efficient manipulation of audio data representations.

The visual description of the proposed codec can be seen in Figure.02.
The overall architecture of the model remains consistent with Encodec, which comprises of three components:
(1) a convolution-based encoder that maps waveform data $x^{1\times L}$ into a sequence of latent representations $z^{F\times T}$, where $F$ is channel and $T$ is length of extracted codes;
(2) a decoder that reconstructs the data $\hat{x}^{1\times L}$ from a sequence of the quantized latent representations $\hat{z}^{F\times T}$;
(3) an 8-layer residual vector quantizer (RVQ) module, which can convert the continual latent vector $z^{F\times T}$ into the discrete code representation $C^{8\times T}$ iteratively.
The main difference is that our merged codec inserts a codec-merging module before the vector quantizer module to downsample the representation $z^{F\times T}$.

Assuming the merged rate of layer $d$ is $m_d$, $r_{d}^{F\times T}$ represents the residual input of layer $d$.
Then the codec-merging module consists of two steps: the first one is downsampling the residual input $r_{d}^{F\times T}$ to $r_{md}^{F\times (T/m_{d})}$ by average pooling, and then upsampling $r_{md}$ to its original length through repeat operation.
Next, the residual representation processed by the Merge module will be feed into the following VQ layer to quantized into discrete code $C^{1\times T}_{d}$ through nearest-neighbor lookup over the codebook embeddings.
Through the merge module, we reduced the resolution of $C^{1\times T}_{d}$ by ensuring consistent code of consecutive $m_d$ frames.

### 3.2·Neural Codec LM with Monotonic Alignment

Previously, monotonic strategies were only applicable to encoder-decoder structures.
To address the robustness issue in the decoder-only transformer based TTS, we integrated phonemes prediction into the training of neural codec LM and design the monotonic alignment strategy during the inference process.
The overview is illustrated in Figure.03 and details of training and inference are discussed in the subsequent sections.

#### 3.2.1·Training with Phoneme Prediction

To achieve a good trade-off between speech quality and inference speed, our ***VALL-E R*** includes two models: autoregressive (AR) and non-autoregressive (NAR), which is following **VALL-E**.
Specifically, given a training data pair $\{\mathbf{s}, \mathbf{p}\}$, where $s$ is speech sample, and $\mathbf{p} = \{p_1, p_2, \cdots, p_L\}$ is its corresponding phoneme transcription.
Then, the codec-merging model introduced in Sec.3.1 is utilized to compress speech waveform $s$ into discrete acoustic tokens $\mathbf{A}$ with 8 quantizers, formulated as: $\text{MergeCodec}(x) = \mathbf{A}^{8×T}= \{\mathbf{a}^1, \cdots, \mathbf{a}^8\}$, where $T$ is length of discrete codes and $\mathbf{a}^i= \{a_1, \cdots, a_T\}$ represent the tokens in the $i$-th layer.
Because the training of ***VALL-E R*** requires the aligned phonemes and acoustic tokens, aligner tool is adopted here to align $p$ with acoustic tokens $\mathbf{A}$, denoted as $\hat{\mathbf{p}}_{1:T}= \{\hat{p}_1, \hat{p}_2, \cdots, \hat{p}_L\}$ where $\hat{p}_i$ contains $N_i$ repetitions of $p_i$ and $\sum_{i=1}^{L}N_i = T$.

For AR stage, to enhance the connection between phoneme and acoustic sequence, we build a neural codec LM $\theta_{AR}$ to model the discrete acoustic tokens $\mathbf{a}^{1}_{1:T}$ from the first quantizer of codec-merging model with phoneme prediction.
As shown in Figure.03, it’s conditioned on the phoneme sequence $\mathbf{p}$ to generate both the acoustic token $\mathbf{a}^{1}_{1:T}$ and aligned phonemes $\hat{p}_{1:T}$ simultaneously, formulated as maximizing the following probability:

$$
  p(\mathbf{a}^{1}_{1:T}, \hat{\mathbf{p}}_{1:T}|\mathbf{p};\theta_{AR}) = \prod_{t=1}^{T} p(a_t,p_t|a_{1:t-1},\hat{\mathbf{p}}_{1:t-1}, \mathbf{p}_{1:L};\theta_{AR})
$$

In the second stage, we train a NAR LM $\theta_{NAR}$ to generate the acoustic tokens from $2$nd to $8$-th layer quantizers iteratively.
It’s conditioned on phoneme sequences $\mathbf{p}$, the previous few layers of generated acoustic tokens $\mathbf{a}^{1:n}$ and phonemes alignment $l_{1:T}$ to predict next layer of acoustic token $\mathbf{a}^{n+1}$, formulated as maximizing:

$$
  p(\mathbf{a}^{2:8}_{1:T}|\hat{\mathbf{p}}_{1:T}, \mathbf{p}_{1:L};\theta_{NAR}) = \prod_{n=2}^{8} p(\mathbf{a}_{1:T}^n|\hat{\mathbf{p}}_{1:T}, \mathbf{a}_{1:T}^{1:n-1}, \mathbf{p}_{1:L};\theta_{NAR})
$$

We also share the parameters of the acoustic embedding layer and the output prediction layer, which means the weights of the $j$-th prediction layer are the same as the $(j + 1)$-th acoustic embedding layer.

#### 3.2.2·Inference with Monotonic Alignment

After training with teacher forcing, the neural codec LM we obtained is surprising in context learning ability.
With just a 3 seconds acoustic prompt, we can replicate the timbre and prosody of unseen speakers without any fine-tuning or adaptation methods (**VALL-E**).
Take Figure.03 as an example of autoregressive inference, we convert the text input into phonemes sequence $\mathbf{p}^t= \{p^{t}_1, \cdots, p^{t}_3\}$ and concatenate it with transcription phoneme of acoustic prompt $\mathbf{p}^p= \{p^{p}_1, p^{p}_2\}$ to form phoneme tokens for model.
In the following, we will use acoustic tokens $\mathbf{a} = \{a^p_1, \cdots, a^t_n\}$ and corresponding aligned phoneme tokens $\mathbf{p}^a= \{p^p_1, \cdots, p^t_n\}$ as condition to predict the audio and phoneme of next step autoregressively.
In order to improve the robustness of the model, we adopted the Monotonic Alignment (MA) strategy during the inference process, which means that the predicted phone needs to be aligned with the text input $\mathbf{p}^t$ and can only maintain the current or jump to the next phoneme.
Specifically, at each step $i$, the current input phoneme token is $p^t_j$.
The output representation is $\mathbf{e}_i$, and corresponding probability of phoneme $p^t_j$ and $p^t_{j+1}$ is denoted as $e_{i,j}$ and $e_{i,j+1}=1 − e_{i,j}$ respectively.
Then, the phoneme pointer would decide to keep $p^{t}_{j}$ unmoved or jump to $p^{t}_{j+1}$ by sampling:

$$
  z_{i,j} \sim \text{Bernoulli}(\dfrac{1}{1+\exp(e_{i,j})})
$$

The sampled phoneme will be used as input for the $i + 1$ step and this AR process will repeat until all phonemes have been covered.
To increase the diversity of synthesis process, sampling based decoding is used for acoustic tokens prediction.
And for NAR model, we adopt greedy search method to generate the $(j + 1)$-th layer acoustic tokens based on the self-aligned phoneme sequence and previous few layers of generated acoustic tokens.
Finally, decoder of neural codec is adopted here to convert generated discrete codes into waveform.
In summary, our inference process has the following three characteristics ([A Survey on Neural Speech Synthesis](../../Surveys/2021.06.29__Survey__A_Survey_on_Neural_Speech_Synthesis_(63P).md)) by using MA strategy:

- Locality: Each phoneme token can correspond to one or several consecutive acoustic tokens, ensuring a flexible and accurate mapping.
Conversely, each acoustic token is uniquely aligned to a single phoneme token.
This one-to-one alignment strategy effectively prevents issues such as misreading, enhancing the clarity and stability of the model.
- Monotonicity: If phoneme $p_a$ follows $p_b$ in the phonemes sequence, the corresponding acoustic tokens for $p_a$ are also positioned sequentially after those for $p_b$.
This sequential alignment is critical as it inherently prevents the repetition of words.
- Completeness: It is mandated that each phoneme token is represented by at least one corresponding acoustic token.
This coverage requirement is essential for preventing the word skipping.

#### 3.2.3·Inference for Controlling Prosody

In the inference process, benefiting from the powerful in context learning ability of the LM, our proposed ***VALL-E R*** can automatically clone both timbre and prosody of the speaker in the prompt by predicting the acoustic and phoneme autoregressively.
Because ***VALL-E R*** explicitly models phoneme, it has strong control over prosody: when we use preset phoneme sequences to replace the self-predicted phoneme sequences in the inference process, we can use the preset prosody to generate speech, thereby achieving the effect of controlling prosody and timbre separately.
It can also be regarded as a voice conversion task, whose goal is to make the timbre of target speech sound like that of prompt speech without changing the linguistic information and prosody of source speech.

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusion: 结论

In this paper, we propose ***VALL-E R***, a robust and efficient zero-shot text-to-speech model based on neural codec language models.
We first employ codec-merging method to downsample the discrete codes to improve inference speed without affecting the sound quality.
More importantly, we incorporate phoneme prediction into the training process and utilize monotonic alignment strategy during the inference.
Experimental results demonstrate that ***VALL-E R*** can effectively model the correlation between phonemes and audio, significantly improve the robustness of speech synthesis, and endow the ability to flexibly control prosody.