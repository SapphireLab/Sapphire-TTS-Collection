# VALL-T

<details>
<summary>基本信息</summary>

- 标题: "VALL-T: Decoder-Only Generative Transducer for Robust and Decoding-Controllable Text-to-Speech"
- 作者:
  - 01 Chenpeng Du (杜晨鹏)
  - 02 Yiwei Guo (郭奕玮)
  - 03 Hankun Wang (王翰坤)
  - 04 Yifan Yang (杨亦凡)
  - 05 Zhikang Niu (牛志康)
  - 06 Shuai Wang (王帅)
  - 07 Hui Zhang
  - 08 Xie Chen (陈谐)
  - 09 Kai Yu (俞凯)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2401.14321)
  - [Publication](https://doi.org/10.1109/ICASSP49660.2025.10890943) ICASSP2025
  - [Github](https://github.com/cpdu/vallt)
  - [Demo](http://cpdu.github.io/vallt)
- 文件:
  - [ArXiv](../_PDF/2401.14321v5__VALL-T__Decoder-Only_Generative_Transducer_for_Robust_and_Decoding-Controllable_Text-to-Speech.pdf)
  - [Publication] #TODO

</details>

## Abstract·摘要

<table><tr><td width="50%">

Recent TTS models with decoder-only Transformer architecture, such as **SPEAR-TTS** and **VALL-E**, achieve impressive naturalness and demonstrate the ability for zero-shot adaptation given a speech prompt.
However, such decoder-only TTS models lack monotonic alignment constraints, sometimes leading to hallucination issues such as mispronunciation, word skipping and repeating.
To address this limitation, we propose ***VALL-T***, a generative Transducer model that introduces shifting relative position embeddings for input phoneme sequence, explicitly indicating the monotonic generation process while maintaining the architecture of decoder-only Transformer.
Consequently, ***VALL-T*** retains the capability of prompt-based zero-shot adaptation and demonstrates better robustness against hallucinations with a relative reduction of 28.3% in the word error rate.
The audio samples are available at http://cpdu.github.io/vallt.

</td><td>

最近, 采用仅解码器 Transformer 架构的文本转语音模型, 如 **SPEAR-TTS** 和 **VALL-E**, 在自然性方面取得了显著进展, 并且展示了在给定语音提示的情况下进行零样本适应的能力.
然而, 这些仅解码器的TTS模型缺乏单调对齐约束, 有时会导致幻觉问题, 如发音错误、单词跳过或重复等.

为了解决这一限制, 我们提出了 ***VALL-T***, 一种生成型Transducer模型, 它为输入的音素序列引入了相对位置嵌入的位移, 从而明确表示单调生成过程, 同时保持仅解码器Transformer的架构.
因此, ***VALL-T*** 保留了基于提示的零样本适应能力, 并且在抗幻觉方面表现得更为稳健, 单词错误率相对减少了28.3%.

此外, ***VALL-T*** 在解码过程中对齐的可控性使其能够使用未转录的语音提示, 甚至是在未知语言中.
它还通过利用对齐的上下文窗口, 能够合成较长的语音.

音频样本可以通过访问 [http://cpdu.github.io/vallt](http://cpdu.github.io/vallt) 获取.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

Text-to-speech (TTS) synthesis is a monotonic sequence-to-sequence task, maintaining a strict order between the input phoneme sequence and the output speech sequence.
Moreover, the output speech sequence is at frame-level and one phoneme may correspond to multiple frames of speech, so the output sequence is significantly longer than its corresponding input phoneme sequence.
Typical non-autoregressive neural text-to-speech models, such as FastSpeech 2 \cite{fastspeech2}, GradTTS \cite{gradtts}, UniCATS \cite{unicats} and NaturalSpeech 2 \cite{naturalspeech2}, integrate an explicit duration prediction module.
Prior to training, the target duration is conventionally derived using the Viterbi forced alignment algorithm.

Over the past two years, utilizing discrete speech tokens for speech generation is proposed in GSLM \cite{textlessnlp} and VQTTS \cite{vqtts}, paving the way for integrating cutting-edge language modeling techniques into TTS systems.
Inspired by exceptional strides in natural language processing driven by decoder-only large Transformer models like GPT 3 \cite{gpt3} and the LLAMA 2 \cite{llama2}, Tortoise-TTS \cite{tortoise}, SPEAR-TTS \cite{speartts} and VALL-E \cite{valle} adopted the decoder-only architecture for TTS, achieving remarkable naturalness.
SPEAR-TTS and VALL-E also have the ability to perform zero-shot speaker adaptation through in-context learning and auto-regressive (AR) continuation from a given speech prompt.
Furthermore, these decoder-only TTS models, unlike the previous non-autoregressive neural TTS models, circumvent explicit duration modeling and the requirement for phoneme durations obtained before hand.
This characteristic offers convenience and simplifies training process, especially when training on large scale datasets.
However, the implicit duration modeling within these systems lacks the monotonic alignment constraints, often leading to hallucination issues like mispronunciation, word skipping and repeating.

In this work, we introduce VALL-T, a decoder-only generative Transducer model that combines the best of both worlds.
It eliminates the need for obtaining phoneme duration before hand while still maintains the monotonic alignment constraint.

Transducer \cite{transducer}, also known as RNN-T, is an existing training scheme designed specifically for monotonic sequence-to-sequence task.
Typical Transducer model adopts a modularized architecture, composed of an encoder, a prediction network and a joint network, and has demonstrated success in automatic speech recognition (ASR) \cite{transducer_asr} as a classification task.

During inference, VALL-T allows us to explicitly guide the monotonic generation process by shifting the relative positions from left to right and dramatically alleviate the hallucination issue in decoder-only TTS.

To the best of our knowledge, this is the first work that implements Transducer with a decoder-only Transformer architecture.
VALL-T presents several advantages compared to previous TTS models:

- VALL-T introduces monotonic alignment constraints without altering the decoder-only architecture, leading to a better robustness against hallucination.
- VALL-T is capable of forced alignment given paired phoneme and speech sequences.
- The alignment between phoneme and speech sequences is aware during inference by tracking the timing of shifting the relative positions.

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

The Transducer model \cite{transducer}, also known as RNN-T, is designed for monotonic sequence-to-sequence tasks and comprises three components: an encoder, a prediction network, and a joint network.
Here, the prediction network is an auto-regressive network, such as RNN and LSTM.
Transducer model also introduces a special output token called blank, denoted as $\varnothing$, which signifies the alignment boundary between output and input sequence.
We define $\mathcal{Y}$ as the vocabulary of output tokens and $\bar{\mathcal{Y}}=\mathcal{Y}\cup \{\varnothing\}$ as the extended vocabulary.
Also, we denote the lengths of the input sequence $\bm{x}$ and output sequence $\bm{y}$ as $T$ and $U$ and the size of the extended vocabulary $\bar{\mathcal{Y}}$ as $\bar{V}$.

In the training phase, the encoder and prediction network encode the two sequences $\bm{x}$ and $\bm{y}$ respectively, yielding encoded hidden sequences $\bm{f}$ and $\bm{g}$.
Then the joint network combines $\bm{f}$ and $\bm{g}$ at all possible positions to predict the corresponding next output token, constructing an alignment grid.
Each path $\bar{\bm{y}}$ from the bottom left corner to the top right corner of the grid represents an alignment between $\bm{x}$ and $\bm{y}$, with a length of $T+U$.
The training criterion of Transducer model is to maximize the probability of $\mathbf{Pr}(\bm{y}|\bm{x})$, which is the summation of the probabilities of all possible alignment paths $\bar{\bm{y}}$.

In the inference phase, the prediction network auto-regressively predicts the next token, conditioning on the sliced input hidden vectors that slide from $\bm{f}_0$ to $\bm{f}_{T-1}$ whenever the blank token $\varnothing$ emerges.

</td><td>

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td><td>

</td></tr></table>
