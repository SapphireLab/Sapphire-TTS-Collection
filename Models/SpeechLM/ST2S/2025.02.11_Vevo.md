# Vevo

<details>
<summary>基本信息</summary>

- 标题: "Vevo: Controllable Zero-Shot Voice Imitation with Self-Supervised Disentanglement"
- 作者:
  - 01 Xueyao Zhang
  - 02 Xiaohui Zhang
  - 03 Kainan Peng
  - 04 Zhenyu Tang
  - 05 Vimal Manohar
  - 06 Yingru Liu
  - 07 Jeff Hwang
  - 08 Dangna Li
  - 09 Yuhao Wang
  - 10 Julian Chan
  - 11 Yuan Huang
  - 12 Zhizheng Wu
  - 13 Mingbo Ma
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.07243)
  - [Publication]() ICLR2025
  - [Github](https://github.com/open-mmlab/Amphion/tree/main/models/vc/vevo) VC
  - [Demo](https://versavoice.github.io/)
- 文件:
  - [ArXiv](../_PDF/2502.07243v1__Vevo__Controllable_Zero-Shot_Voice_Imitation_with_Self-Supervised_Disentanglement.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

The imitation of voice, targeted on specific speech attributes such as timbre and speaking style, is crucial in speech generation.
However, existing methods rely heavily on annotated data, and struggle with effectively disentangling timbre and style, leading to challenges in achieving controllable generation, especially in zero-shot scenarios.
To address these issues, we propose ***Vevo***, a versatile zero-shot voice imitation framework with controllable timbre and style.
***Vevo*** operates in two core stages: (1) Content-Style Modeling: Given either text or speech's content tokens as input, we utilize an autoregressive transformer to generate the content-style tokens, which is prompted by a style reference; (2) Acoustic Modeling: Given the content-style tokens as input, we employ a flow-matching transformer to produce acoustic representations, which is prompted by a timbre reference.
To obtain the content and content-style tokens of speech, we design a fully self-supervised approach that progressively decouples the timbre, style, and linguistic content of speech.
Specifically, we adopt VQ-VAE as the tokenizer for the continuous hidden features of HuBERT.
We treat the vocabulary size of the VQ-VAE codebook as the information bottleneck, and adjust it carefully to obtain the disentangled speech representations.
Solely self-supervised trained on 60K hours of audiobook speech data, without any fine-tuning on style-specific corpora, ***Vevo*** matches or surpasses existing methods in accent and emotion conversion tasks.
Additionally, ***Vevo***'s effectiveness in zero-shot voice conversion and text-to-speech tasks further demonstrates its strong generalization and versatility.
Audio samples are available at [this https URL](https://versavoice.github.io/).

</td><td>

语音模仿, 特别是针对特定语音属性如音色和说话风格的模仿, 在语音生成中至关重要.
然而, 现有方法过度依赖标注数据, 并且在有效解耦音色和风格方面存在困难, 导致在实现可控生成时面临挑战, 尤其是在零样本场景中.

为了解决这些问题, 我们提出了 ***Vevo***, 一种多功能的零样本语音模仿框架, 具备可控的音色和风格.

***Vevo***的核心包含两个阶段：
(1) 内容-风格建模：给定文本或语音的内容标记作为输入, 我们利用自回归变换器生成内容-风格标记, 风格由风格参考引导；
(2) 声学建模：给定内容-风格标记作为输入, 我们采用流匹配变换器生成声学表示, 由音色参考进行引导.

为了获得语音的内容和内容-风格标记, 我们设计了一种完全自监督的方法, 逐步解耦语音的音色、风格和语言内容.

具体来说, 我们采用VQ-VAE作为HuBERT连续隐藏特征的标记器.
我们将VQ-VAE词汇表的大小视为信息瓶颈, 并精心调整它以获得解耦的语音表示.

***Vevo*** 仅通过在60K小时的有声书语音数据上自监督训练, 无需针对特定风格的语料库进行微调, 便在口音和情感转换任务中与现有方法相匹配或超越它们.
此外, ***Vevo*** 在零样本语音转换和文本到语音任务中的有效性进一步证明了其强大的泛化能力和多功能性.

音频样本可以通过访问[这个链接](https://versavoice.github.io/)获取.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

The imitation of voice has long been an important issue in the field of speech generation.
This includes the imitation of speaker identity~\cite{parallel-vc-survey-2017,vc-survey-taslp}, the imitation of speaking style such as accent~\cite{accent-conversion-2009,l2arctic} or emotion~\cite{esd}, and a broader concept of voice cloning such as in zero-shot text-to-speech (TTS) task~\cite{tts-book-tanxu}.
These techniques have a wide range of applications, including spoken language learning~\cite{accent-conversion-2009,l2arctic,parallel-ac-zhaoguanlong21}, voice anonymization~\cite{vc-as-anonymization}, voice assistants~\cite{seedtts,fireredtts}, and video dubbing~\cite{seedtts,maskgct,fireredtts}.

To achieve targeted and controllable imitation over various speech attributes, many studies focuses on factorizing speech into multiple sub-spaces~\cite{speechsplit,speech-resynthesis-interspeech21,megatts,ns3}.
In this work, we follow this idea and decompose speech into three key attributes: linguistic content (\textit{what to speak}), style (\textit{how to speak}), and timbre (\textit{who speaks}).
Based on this, we define three zero-shot speech generation tasks (Table~\ref{tab:task}): (1) \textbf{Timbre Imitation}: Given a speech as source, imitate only the timbre of the reference speech while preserving the linguistic content and speaking style.
It can be adopted in voice conversion that only spectral aspects of speech are converted~\cite{parallel-vc-survey-2017}.
(2) \textbf{Style Imitation}: Given a speech as source, imitate only the speaking style of the reference speech while preserving the content and the timbre.
It can be adopted in accent conversion~\cite{accent-conversion-2009} and emotion conversion~\cite{esd}.
(3) \textbf{Voice Imitation}: Given either a speech (i.e., \textit{conversion task}) or text (i.e., \textit{synthesis task}) as source, imitate both the timbre and style of the reference speech while preserving the content.
It can be adopted in voice conversion that both spectral and prosodic aspects of speech are converted~\cite{parallel-vc-survey-2017,vc-survey-taslp} and zero-shot TTS~\cite{tts-book-tanxu}.

To address these imitation tasks, existing work has explored approaches including learning the conversion between parallel corpus~\cite{parallel-vc-2015,parallel-ec-2016,parallel-ac-zhaoguanlong21,voiceshop,convertandspeak}, disentangled representation learning~\cite{autovc,speechsplit,HuBERT,basetts,ns3,cosyvoice}, and large-scale in-context learning~\cite{tortoise-tts,valle,voicebox,uniaudio,seedtts}.
However, these approaches still suffer from the following limitations.
Firstly, for the style imitation, existing methods rely heavily on supervision with annotated data, which is hard to collect and scale up.
This reliance includes the use of parallel corpus~\cite{parallel-ac-zhaoguanlong21,voiceshop,convertandspeak}, style labels (such as categories of accent~\cite{asr-ac,voiceshop,convertandspeak} or emotion~\cite{emovox,pavits}), and textual transcriptions~\cite{asr-ac,chenxi-tts-ac,emovox,pavits}.
Moreover, achieving \textit{zero-shot} style imitation—where a system can imitate an accent, emotion, or other speaking styles from just a few seconds of speech—remains a significant challenge.
Secondly, the decoupling of timbre and style in existing methods is still insufficient, making it challenging to control them independently, unless mitigated by some timbre (or style) perturbations or additional fine-tuning stages ~\cite{seedtts,maskgct,u-style}.

Motivated by the above, this paper proposes Vevo, a \underline{ve}rsatile zero-shot \underline{vo}ice imitation framework with controllable timbre and style (Figure~\ref{fig:Vevo-pipeline}).
It can serve as a unified framework for a wide range of zero-shot speech generation tasks.
Vevo consists of two core stages: (1) \textbf{Content-Style Modeling} (\textit{Content to Content-Style}): Given a speech prompt as style reference, we generate \textit{content-style} tokens from the input \textit{content} tokens (or the input text).
We employ the decoder-only autoregressive transformer~\cite{transformer,llama}, leveraging its powerful capability of continued generation to model style.
(2) \textbf{Acoustic Modeling} (\textit{Content-Style to Acoustic}): Given a speech prompt as timbre reference, we generate acoustic representations (such as Mel spectrograms) from the input of \textit{content-style} tokens.
We use a flow-matching transformer~\cite{flow-matching,dit}, which has been verified to excel in in-context learning and reconstructing high-quality audio~\cite{voicebox,audiobox,cosyvoice,fireredtts}, to achieve timbre-controllable generation.

To obtain the \textit{content} and \textit{content-style} tokens of speech, we design a self-supervised method to decouple the timbre, style, and linguistic content gradually, which is similar to a progressive information filtering: (1) We firstly investigate the commonly used self-supervised speech pre-trained model, HuBERT~\cite{HuBERT}.
We find that its \textbf{continuous} hidden features contain rich information about timbre, style, and linguistic content (Section~\ref{sec:results-effect-of-codebook-size}), making it a suitable initial stage for information filtering.
(2) Inspired by existing works for disentangling speaker-agnostic representations~\cite{vq-vae,vqvc,vq-content-style,ns3}, we employ VQ-VAE~\cite{vq-vae} as a tokenizer for HuBERT to filter out timbre, resulting in \textbf{content-style tokens}.
(3) Furthermore, we propose that the vocabulary size of the VQ-VAE codebook can function as the ``width" of the information bottleneck~\cite{autovc}.
By reducing the vocabulary size, we can narrow the bottleneck and filter out not only timbre but also significant style information, thereby obtaining \textbf{content tokens}.
Besides, we propose to reduce the consecutive duplicate units~\cite{mhubert-duration-reduction} of the content tokens, called \textit{duration reduction}, to further remove some style patterns such as unit-level duration.

The contributions of this paper are summarized as follows:

- We introduce a fully self-supervised approach that progressively decouple timbre, style, and linguistic content of speech.
The resulting content-style tokens and content tokens enhance controllability in downstream speech generation tasks, particularly for timbre and style.
- We propose Vevo, a unified framework that enables versatile, controllable zero-shot voice imitation tasks.
It significantly reduces the reliance on annotated corpora, facilitating self-supervised training and in-context learning that can easily be scaled up.
- Pre-trained on 60K hours of audiobook speech data without any fine-tuning on style-specific corpora, Vevo matches or even surpasses existing methods in accent and emotion conversion tasks -- notably, through zero-shot imitation.
Additionally, Vevo's effectiveness in voice conversion and text-to-speech tasks further demonstrates its strong generalization and versatility.

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

**Controllable Voice Imitation**

We focus primarily on how existing works approach the imitation of two key speech attributes: timbre and style.
(1) \textbf{Imitation of Timbre}: As a crucial aspect of speaker identity, timbre imitation has been extensively explored within the voice conversion (VC) field.
Most studies aim to utilize the speaker-agnostic representations such as PPG features~\cite{ppg-vc,voiceshop} or some self-supervised representations~\cite{self-supervised-vc,amphion-svc}, and use models including GAN~\cite{cyclegan-vc,stargan-vc}, auto-encoder~\cite{autovc,speechsplit}, and diffusion models~\cite{diffvc,diff-hiervc} to achieve timbre imitation.
(2) \textbf{Imitation of Style}: In terms of style imitation, accent and emotion are two widely studied attributes.
For conversion tasks (with speech as input), classic approaches often involve learning the conversion between parallel corpus~\cite{parallel-ac-zhaoguanlong21,parallel-ec-2016,voiceshop,convertandspeak}.
Additionally, many studies aim to obtain the style-agnostic features, such as pushing them to be close to textual transcriptions~\cite{zhouyi-ac,chenxi-tts-ac,emovox,pavits}.
Besides, leveraging automatic speech recognition (ASR) models can transform conversion tasks into synthesis tasks, allowing the injection of style label's embeddings into TTS models to achieve style imitation~\cite{asr-ac,liusongxiang-ac}.
In conclusion, these existing approaches often rely on annotated data and struggle to achieve \textit{zero-shot} style imitation.
(3) \textbf{Imitation of both Timbre and Style}: In VC, some works suggest adopting a sequence-to-sequence formulation~\cite{non-parallel-seq2seq-vc,lmvc} or introducing an additional modeling for prosody features~\cite{diff-hiervc,hierspeech++} to achieve both timbre and style imitation.
However, these models still have significant room for improvement in both quality and style imitation.
Recent advances in zero-shot TTS have greatly improved voice imitation and cloning.
They leverage large-scale in-context learning to mimic all speech attributes of a reference prompt, including timbre and style, with high quality and speaker similarity~\cite{valle,megatts,ns3,seedtts,maskgct,u-style}.
Nonetheless, it is challenging to obtain the speech representations disentangled timbre and style effectively~\cite{basetts,u-style}, leading to inadequate targeted control of these attributes.
For instance, using the existing representations directly for VC tasks will lead to timbre leakage, unless mitigated by timbre perturbation or an additional fine-tuning stage~\cite{seedtts,maskgct}.

**Disentangled Speech Representation**

There are many studies aim to decouple linguistic content, timbre, and style.
Existing work on obtaining disentangled speech representations can generally be categorized into several approaches: (1) Knowledge distillation using auxiliary tasks such as ASR, F0 prediction, and speaker verification~\cite{speech-resynthesis-interspeech21,ns3,basetts}, (2) Model architecture design based on information bottlenecks, including careful adjustments to hidden layer dimensions~\cite{autovc,speechsplit} or vector quantization methods like K-means~\cite{HuBERT,softvc,sef-vc-kmeans} or VQ-VAE~\cite{vq-vae,vqvc,vq-content-style,speech-resynthesis-interspeech21,ns3}, and (3) Perturbation of acoustic signals~\cite{nancy,nancypp,speechsplit2}.
Besides, existing works also leverage additional learning strategies including adversarial learning~\cite{ns3,basetts}, comparative learning~\cite{contentvec,basetts}, and mutual information minimization~\cite{vq-content-style,mutual-information-zhuxinfa,mutual-information} to enhance disentanglement effectiveness.
However, existing work still has two main weaknesses.
On one hand, as mentioned earlier, finding suitable representations for downstream generation tasks that can effectively decouple timbre and style remains quite challenging.
On the other hand, how to design voice imitation models that can control specific attributes based on these disentangled speech representations has been scarcely explored.

</td><td>

</td></tr></table>

## 3·Methodology: 方法

### VQ-VAE tokenizer for HuBERT

<table><tr><td width="50%">

**Motivation**

To disentangle representations of different speech attributes, we adopt a VQ-VAE tokenizer~\cite{vq-vae} due to its demonstrated potential in disentangling high-level information within speech such as speaker-invariant features~\cite{vq-vae,vqvc,ns3}.
In speech domain, it is common practice to apply VQ-VAE either directly on the raw waveform~\cite{vq-vae,vqvc,ns3} or on the self-supervised learning (SSL) based speech representations~\cite{speech-resynthesis-interspeech21,repcodec,fireredtts}.
In this work, we choose to apply VQ-VAE based on SSL representations -- specifically, HuBERT~\cite{HuBERT}.
The reasons are two fold: (1) HuBERT's continuous hidden features already contain rich information about timbre, style, and linguistic content, making them well-suited for reconstructing acoustic representations such as Mel spectrograms (Section~\ref{sec:results-effect-of-codebook-size}); (2) Self-supervised learning on speech could be also treated as a high-level knowledge distillation.
VQ-VAE enables us to further information filtering and disentangling for the SSL features.

**Architecture**

The VQ-VAE consists of three components: Encoder, Vector Quantization (VQ), and Decoder.
Formally, given the codebook $\bm{E} = [\bm{e}_1, \bm{e}_2, \dots, \bm{e}_K]
$ whose vocabulary size is $K$, taking HuBERT hidden features $\bm{x}$ as input, we get the reconstructed $\hat{\bm{x}}$ after the three modules:

$$
    \bm{z}_e(\bm{x}) &= \text{Encoder}(\bm{x}), \\
    \bm{z}_q(\bm{x}) &= \bm{e}_k,~\text{where}~k = \arg\min_j \|\bm{z}_e(\bm{x}) - \bm{e}_j\|_2, \\
    \hat{\bm{x}} &= \text{Decoder}(\bm{z}_q(\bm{x})),
$$

where $\bm{z}_q(\bm{x})$ is the quantized representation (i.e., token) of $\bm{z}_e(\bm{x})$ after VQ.
The loss function consists of the reconstruction loss (whose weight is $\lambda$) and quantization loss (whose weight is $\beta$):

$$
    \mathcal{L} = \lambda \|\bm{x} - \hat{\bm{x}}\|^2_2 + \beta \|\bm{z}_e(\bm{x}) - \bm{z}_q(\bm{x})\|^2_2.
$$

Note that there is no real gradient defined for $\bm{z}_q(\bm{x})$.
We could utilize the straight-through gradient estimator or exponential moving average (EMA) as the optimization algorithm~\cite{vq-vae}.
In this paper, we follow the design in~\cite{soundstream,repcodec} and use the EMA algorithm.
We describe the specific module design of VQ-VAE in Appendix~\ref{sec:appendix-vq-vae-tokenizer}.
Notably, the VQ-VAE model does not contain any downsampling or upsampling operations, thus preserving the sequence length of the input $\bm{x}$.
In other words, for the 50 Hz frame-level HuBERT features~\cite{HuBERT}, we can also get 50 Hz frame-level tokens after VQ.

**Analysis of the Vocabulary Size of Codebook**

The quantization of HuBERT hidden features by VQ-VAE can be viewed as a form of \textit{lossy compression}.
Inspired by AutoVC~\cite{autovc}, we propose that the vocabulary size of the VQ codebook acts as an information bottleneck.
If the input $\bm{x}$ possesses sufficient speech information, reducing the vocabulary size $K$ from infinity to zero:
(1) \textbf{When $K \rightarrow \infty$}, we consider the bottleneck to be extremely wide, capable of accommodating all information without any loss.
(2) \textbf{As $K$ decreases}, more low-level acoustic information begins to be lost, such as spectral features related to timbre or prosodic features related to style.
At a certain reduced $K$, only the highest-level, most abstract information like linguistic content is preserved within $\bm{x}$.
(3) \textbf{When $K \rightarrow 0$}, the bottleneck becomes exceedingly narrow, filtering out even high-level information like linguistic content.
We validate the above hypothesis through experiments on the zero-shot timbre imitation task (Section~\ref{sec:results-effect-of-codebook-size}).
Interestingly, as we progressively reduce $K$, we observe that timbre information is the first to be filtered out (assuming when $K=K_s$), from which we derive the \textit{content-style} tokens.
Subsequently, most style information is filtered, and ultimately, almost only the highest-level linguistic content information is retained (assuming when $K=K_c$), from which we derive the \textit{content} tokens.
We refer to the VQ-VAE model whose $K=K_s$ as the content-style tokenizer $\bm{Q}_s$, and the model whose $K=K_c$ as the content tokenizer $\bm{Q}_c$.

### Content-Style Modeling (Content to Content-Style)


During the content-style modeling stage, our goal is to transform the content token of speech (or text) into content-style tokens, which is prompted by a style reference.
This can be formulated as a sequence-to-sequence generation task.
For this stage, we employ a decoder-only autoregressive (AR) transformer, known for its powerful capability in such tasks~\cite{transformer,llama,seedtts}.
In this section, we will focus only on cases where speech's content tokens are used as input (Figure~\ref{fig:model-ar}).
The scenarios where text serves as input will be discussed in Appendix~\ref{sec:appendix-content-style-modeling-text}.

\textbf{Duration Reduction}\quad Given a speech input ${u}$, we denote the content and content-style tokens as $\bm{Q}_c ({u})$ and $\bm{Q}_s ({u})$.
Both of them are 50 Hz frame-level representations of equal length.
In the content-style modeling stage, $\bm{Q}_s ({u})$ is used as the output.
However, instead of using $\bm{Q}_c ({u})$, we apply a \textit{Duration Reduction} strategy to it, yielding the reduced $\bm{Q}_c^{'} ({u})$ as the input.
Specifically, we merge the consecutive duplicate units of $\bm{Q}_c ({u})$ into one.
For instance, if $\bm{Q}_c ({u}) = [\bm{e}_1, \bm{e}_1, \bm{e}_1, \bm{e}_2, \bm{e}_3, \bm{e}_3]$, it will be condensed to $\bm{Q}_c^{'} ({u}) = [\bm{e}_1, \bm{e}_2, \bm{e}_3]$.
This strategy offers significant benefits: (1) It further filters out style-specific information within $\bm{Q}_c ({u})$ such as the unit-level duration.
Some studies also point out that such a reduction could aid in reducing accents and other style elements~\cite{mhubert-duration-reduction}; (2) It resolves the model's challenge with learning changes in sequence length before and after style modeling when $\bm{Q}_c ({u})$ and $\bm{Q}_s ({u})$ are always equal in length; (3) It shortens the overall sequence length, which is beneficial to model context for transformer.

\textbf{Global Style Encoder}\quad We design a global style encoder to capture the global style guidance from the speech input ${u}$, producing a style embedding (denoted as $\bm{g} ({u})$).
Its advantage comes from the flexibility during inference: if we aim to optimize inference speed and reduce memory usage, we can rely solely on this style embedding for style guidance, named as \textit{reference-global-guided} continuation (Figure~\ref{fig:model-ar-inference-global}).
However, to maximize the performance of style imitation, in addition to using $\bm{g} ({u})$, we can also append the style reference's content-style tokens into the input sequence to enhance its effect, named as \textit{reference-style-enhanced} continuation (Figure~\ref{fig:model-ar-inference-enhanced}).
The global style encoder consists of WavLM-based representation layers and TDNN-based feature extraction layers~\cite{wavlm,ecapa-tdnn}.
We describe the detailed module design in Appendix~\ref{sec:appendix-content-sylte-modeling}.

\textbf{Training and Inference}\quad During training, we conduct self-supervised learning on speech data.
The input sequence of transformer is $[\langle \text{SOS} \rangle, \bm{Q}_{c}^{'} ({u}), \langle \text{SEP} \rangle, \bm{g} ({u}), \langle \text{SEP} \rangle, \bm{Q}_{s} ({u})]$.
We only perform the next token prediction on the last $[\langle \text{SEP} \rangle, \bm{Q}_{s} ({u})]$, with the ground truth being $[\bm{Q}_{s} ({u}), \langle \text{EOS} \rangle]$.
Here, $\langle \text{SOS} \rangle$, $\langle \text{SEP} \rangle$, and $\langle \text{EOS} \rangle$ are treated as three special tokens in language model~\cite{bert}.
During inference, for a source speech ${u}_i$ and a style reference ${u}_{sr}$, we can conduct the reference-style-enhanced continuation (Figure~\ref{fig:model-ar-inference-enhanced}) by feeding the input sequence $[\langle \text{SOS} \rangle, \bm{Q}_{c}^{'} ({u}_{sr} \oplus {u}_i), \bm{g} ({u}_{sr}), \bm{Q}_s ({u}_{sr})]$ for autoregressive generation, where $\oplus$ means the concatenation.
For reference-global-guided continuation (Figure~\ref{fig:model-ar-inference-global}), the input sequence becomes $[\langle \text{SOS} \rangle, \bm{Q}_{c}^{'} ({u}_i), \bm{g} ({u}_{sr})]$.

### Acoustic Modeling (Content-Style to Acoustic)

During the acoustic modeling stage, prompted by a timbre reference, we aim to transform the content-style tokens to Mel spectrograms.
We adopt a flow matching transformer~\cite{flow-matching,transformer,llama} (Figure~\ref{fig:model-diffusion}), which has been verified to be effective in in-context learning and reconstructing high-quality acoustic representations~\cite{voicebox,audiobox,cosyvoice,fireredtts}.

During training, given a speech ${u}$ and its Mel spectrogram $\bm{y}_1$, we randomly select a part of $\bm{y}_1$ as the timbre reference (denoted as $\bm{y}_1^{ctx}$), and aim to reconstruct the other part (denoted as $\bm{y}_1^{mis}$) conditioned on $\bm{y}_1^{ctx}$ and the content-style tokens $\bm{Q}_s({u})$.
In other words, we aim to model the conditional probability $p(\bm{y}_1^{mis} | \bm{y}_1^{ctx}, \bm{Q}_s({u}))$.
Specifically, we follow Voicebox~\cite{voicebox} and use a temporal span masking strategy: $\bm{y}_1^{mis} = \bm{m} \odot \bm{y}_1$, and $\bm{y}_1^{ctx} = (\bm{1} - \bm{m}) \odot \bm{y}_1$, where $\bm{m}$ is a binary temporal mask that is of the same length as $\bm{y}_1$, and $\odot$ means the element-wise multiplying operation.
During inference, given a source speech ${u}_i$ and a timbre reference ${u}_{tr}$, all the source's Mel spectrogram will be masked (i.e., $\bm{y}_1^{mis}$).
The input conditions become the timbre reference's Mel spectrogram (i.e., $\bm{y}_1^{ctx}$) and the concatenated content-style tokens $\bm{Q}_s ({u}_i \oplus {u}_{tr})$.
This enables the generated target to preserve the linguistic content and style of ${u}_i$, and the timbre of ${u}_{tr}$ (Figure~\ref{fig:model-diffusion-inference}).

We use the conditional flow matching algorithms based on optimal transport path, which is widely adopted in related works~\cite{voicebox,cosyvoice,fireredtts}.
The loss function is defined as:
\begin{equation}
\begin{split}
    \mathcal{L}_{cfm} &= \mathbb{E}_{t,\bm{m},\bm{y}_0,\bm{y}_1}  \left\| \frac{d \bm{y}_t}{d t} - f_t(\bm{y}_t, t, \bm{y}_1^{ctx}, \bm{Q}_s({u})) \right\|_2^2, \\
    \text{where}~\bm{y}_t &= (1 - (1 - \sigma)t) \cdot \bm{y}_0 + t \cdot \bm{y}_1,
\end{split}
\end{equation}
where $t$ is the time step that is sampled from the uniform distribution $\mathcal{U}(0, 1)$, $\bm{y}_0$ is a noise sampled from standard Gaussian distribution, $f_t(\cdot)$ is the vector filed (which is estimated by transformer).
and $\sigma$ is a small constant of the optimal transport (OT) path.
% We follow the implementation of Voicebox~\cite{voicebox} and only compute the loss for masked frames.
Notably, the frame rates of the content-style tokens $\bm{Q}_s({u})$ and the Mel spectrogram $\bm{y}_1$ could be different.
We follow~\cite{amphion-svc} and use a simple signal resampling operation to align them.
Then we use the adding operation to fuse their frame-level features.
We describe the detailed module design in Appendix~\ref{sec:appendix-acoustic-modeling}.
After obtaining the Mel spectrogram, we utilize a BigVGAN~\cite{bigvgan} vocoder to produce the waveform (Appendix~\ref{sec:appendix-vocoder}).

### Vevo for Various Zero-Shot Imitation Tasks

Assume that during the content-style modeling and acoustic modeling stages, we have obtained pre-trained models $\mathcal{M}_{style}$ and $\mathcal{M}_{acoustic}$ respectively.
We can then adjust only the inference pipeline to apply Vevo to various zero-shot imitation tasks.
Given the source speech ${u}_{\textcolor{blue}{\bm{i}}}$ (or text $\mathcal{T}_{\textcolor{blue}{\bm{i}}}$) and the reference ${u}_{\textcolor{red}{\bm{r}}}$, we can utilize the following variants of Vevo to achieve zero-shot timbre, style, and voice imitation tasks (`` $\xrightarrow{u} \mathcal{M}$ " means that the model $\mathcal{M}$ is prompted by $u$ to generate):

- \textbf{Vevo-Timbre} for timbre imitation: $\bm{Q}_s ({u}_{\textcolor{blue}{\bm{i}}}) \xrightarrow{\displaystyle {{u}_{\textcolor{red}{\bm{r}}}}} \mathcal{M}_{acoustic}$
- \textbf{Vevo-Style} for style Imitation: $\bm{Q}_c^{'} ({u}_{\textcolor{blue}{\bm{i}}}) \xrightarrow{\displaystyle {u}_{\textcolor{red}{\bm{r}}}} \mathcal{M}_{style} \xrightarrow{\displaystyle {u}_{\textcolor{blue}{\bm{i}}}} \mathcal{M}_{acoustic}$
- \textbf{Vevo-Voice} for voice imitation (conversion task): $\bm{Q}_c^{'} ({u}_{\textcolor{blue}{\bm{i}}}) \xrightarrow{\displaystyle {u}_{\textcolor{red}{\bm{r}}}} \mathcal{M}_{style} \xrightarrow{\displaystyle {u}_{\textcolor{red}{\bm{r}}}} \mathcal{M}_{acoustic}$
- \textbf{Vevo-TTS} for voice imitation (synthesis task): $\widetilde{\bm{Q}_c} (\mathcal{T}_{\textcolor{blue}{\bm{i}}}) \xrightarrow{\displaystyle {u}_{\textcolor{red}{\bm{r}}}} \widetilde{\mathcal{M}}_{style} \xrightarrow{\displaystyle {u}_{\textcolor{red}{\bm{r}}}} \mathcal{M}_{acoustic}$

For Vevo-TTS, $\widetilde{\bm{Q}_c} (\mathcal{T}_{\textcolor{blue}{\bm{i}}})$ means the tokenization for $\mathcal{T}_{\textcolor{blue}{\bm{i}}}$, and $\widetilde{\mathcal{M}}_{style}$ means the pre-trained model for content-style modeling that takes text as input.
We describe its detailed design in Appendix~\ref{sec:appendix-content-style-modeling-text}.

</td><td>

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

We introduce Vevo, a versatile zero-shot voice imitation framework featuring controllable timbre and style.
Vevo contains of two primary stages: content-style modeling via an autoregressive transformer, and acoustic modeling via a flow matching transformer.
Both stages are trainable through self-supervised and in-context learning, friendly to scale up.
Vevo operates based on our newly proposed content and content-style tokens, generated by VQ-VAE tokenizers of HuBERT with carefully adjusted vocabulary sizes.
Pre-trained only on 60K hours of audiobook speech data without fine-tuning on style-specific corpus, Vevo outperforms state-of-the-art models of accent and emotion conversion fields, particularly achieving these conversions in a zero-shot manner.
Furthermore, Vevo's robust performance in zero-shot voice conversion and text-to-speech tasks underscores its versatility and also highlights the broad potential of our proposed disentangled speech tokens.

</td><td>

</td></tr></table>
