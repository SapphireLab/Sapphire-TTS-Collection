# Vevo

<details>
<summary>基本信息</summary>

- 标题: "Vevo: Controllable Zero-Shot Voice Imitation with Self-Supervised Disentanglement"
- 作者:
  - 01 Xueyao Zhang
  - 02 Xiaohui Zhang
  - 03 Kainan Peng
  - 04 Zhenyu Tang
  - 05 Vimal Manohar
  - 06 Yingru Liu
  - 07 Jeff Hwang
  - 08 Dangna Li
  - 09 Yuhao Wang
  - 10 Julian Chan
  - 11 Yuan Huang
  - 12 Zhizheng Wu
  - 13 Mingbo Ma
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.07243)
  - [Publication]() ICLR2025
  - [Github](https://github.com/open-mmlab/Amphion/tree/main/models/vc/vevo) VC
  - [Demo](https://versavoice.github.io/)
- 文件:
  - [ArXiv](../_PDF/2502.07243v1__Vevo__Controllable_Zero-Shot_Voice_Imitation_with_Self-Supervised_Disentanglement.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

The imitation of voice, targeted on specific speech attributes such as timbre and speaking style, is crucial in speech generation.
However, existing methods rely heavily on annotated data, and struggle with effectively disentangling timbre and style, leading to challenges in achieving controllable generation, especially in zero-shot scenarios.
To address these issues, we propose ***Vevo***, a versatile zero-shot voice imitation framework with controllable timbre and style.
***Vevo*** operates in two core stages: (1) Content-Style Modeling: Given either text or speech's content tokens as input, we utilize an autoregressive transformer to generate the content-style tokens, which is prompted by a style reference; (2) Acoustic Modeling: Given the content-style tokens as input, we employ a flow-matching transformer to produce acoustic representations, which is prompted by a timbre reference.
To obtain the content and content-style tokens of speech, we design a fully self-supervised approach that progressively decouples the timbre, style, and linguistic content of speech.
Specifically, we adopt VQ-VAE as the tokenizer for the continuous hidden features of HuBERT.
We treat the vocabulary size of the VQ-VAE codebook as the information bottleneck, and adjust it carefully to obtain the disentangled speech representations.
Solely self-supervised trained on 60K hours of audiobook speech data, without any fine-tuning on style-specific corpora, ***Vevo*** matches or surpasses existing methods in accent and emotion conversion tasks.
Additionally, ***Vevo***'s effectiveness in zero-shot voice conversion and text-to-speech tasks further demonstrates its strong generalization and versatility.
Audio samples are available at [this https URL](https://versavoice.github.io/).

</td><td>

语音模仿, 特别是针对特定语音属性如音色和说话风格的模仿, 在语音生成中至关重要.
然而, 现有方法过度依赖标注数据, 并且在有效解耦音色和风格方面存在困难, 导致在实现可控生成时面临挑战, 尤其是在零样本场景中.

为了解决这些问题, 我们提出了 ***Vevo***, 一种多功能的零样本语音模仿框架, 具备可控的音色和风格.

***Vevo***的核心包含两个阶段：
(1) 内容-风格建模：给定文本或语音的内容标记作为输入, 我们利用自回归变换器生成内容-风格标记, 风格由风格参考引导；
(2) 声学建模：给定内容-风格标记作为输入, 我们采用流匹配变换器生成声学表示, 由音色参考进行引导.

为了获得语音的内容和内容-风格标记, 我们设计了一种完全自监督的方法, 逐步解耦语音的音色、风格和语言内容.

具体来说, 我们采用VQ-VAE作为HuBERT连续隐藏特征的标记器.
我们将VQ-VAE词汇表的大小视为信息瓶颈, 并精心调整它以获得解耦的语音表示.

***Vevo*** 仅通过在60K小时的有声书语音数据上自监督训练, 无需针对特定风格的语料库进行微调, 便在口音和情感转换任务中与现有方法相匹配或超越它们.
此外, ***Vevo*** 在零样本语音转换和文本到语音任务中的有效性进一步证明了其强大的泛化能力和多功能性.

音频样本可以通过访问[这个链接](https://versavoice.github.io/)获取.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

The imitation of voice has long been an important issue in the field of speech generation. This includes the imitation of speaker identity~\cite{parallel-vc-survey-2017,vc-survey-taslp}, the imitation of speaking style such as accent~\cite{accent-conversion-2009,l2arctic} or emotion~\cite{esd}, and a broader concept of voice cloning such as in zero-shot text-to-speech (TTS) task~\cite{tts-book-tanxu}.
These techniques have a wide range of applications, including spoken language learning~\cite{accent-conversion-2009,l2arctic,parallel-ac-zhaoguanlong21}, voice anonymization~\cite{vc-as-anonymization}, voice assistants~\cite{seedtts,fireredtts}, and video dubbing~\cite{seedtts,maskgct,fireredtts}.

To achieve targeted and controllable imitation over various speech attributes, many studies focuses on factorizing speech into multiple sub-spaces~\cite{speechsplit,speech-resynthesis-interspeech21,megatts,ns3}. In this work, we follow this idea and decompose speech into three key attributes: linguistic content (\textit{what to speak}), style (\textit{how to speak}), and timbre (\textit{who speaks}). Based on this, we define three zero-shot speech generation tasks (Table~\ref{tab:task}): (1) \textbf{Timbre Imitation}: Given a speech as source, imitate only the timbre of the reference speech while preserving the linguistic content and speaking style. It can be adopted in voice conversion that only spectral aspects of speech are converted~\cite{parallel-vc-survey-2017}. (2) \textbf{Style Imitation}: Given a speech as source, imitate only the speaking style of the reference speech while preserving the content and the timbre. It can be adopted in accent conversion~\cite{accent-conversion-2009} and emotion conversion~\cite{esd}. (3) \textbf{Voice Imitation}: Given either a speech (i.e., \textit{conversion task}) or text (i.e., \textit{synthesis task}) as source, imitate both the timbre and style of the reference speech while preserving the content. It can be adopted in voice conversion that both spectral and prosodic aspects of speech are converted~\cite{parallel-vc-survey-2017,vc-survey-taslp} and zero-shot TTS~\cite{tts-book-tanxu}.

To address these imitation tasks, existing work has explored approaches including learning the conversion between parallel corpus~\cite{parallel-vc-2015,parallel-ec-2016,parallel-ac-zhaoguanlong21,voiceshop,convertandspeak}, disentangled representation learning~\cite{autovc,speechsplit,HuBERT,basetts,ns3,cosyvoice}, and large-scale in-context learning~\cite{tortoise-tts,valle,voicebox,uniaudio,seedtts}. However, these approaches still suffer from the following limitations.
Firstly, for the style imitation, existing methods rely heavily on supervision with annotated data, which is hard to collect and scale up. This reliance includes the use of parallel corpus~\cite{parallel-ac-zhaoguanlong21,voiceshop,convertandspeak}, style labels (such as categories of accent~\cite{asr-ac,voiceshop,convertandspeak} or emotion~\cite{emovox,pavits}), and textual transcriptions~\cite{asr-ac,chenxi-tts-ac,emovox,pavits}. Moreover, achieving \textit{zero-shot} style imitation—where a system can imitate an accent, emotion, or other speaking styles from just a few seconds of speech—remains a significant challenge.
Secondly, the decoupling of timbre and style in existing methods is still insufficient, making it challenging to control them independently, unless mitigated by some timbre (or style) perturbations or additional fine-tuning stages ~\cite{seedtts,maskgct,u-style}.

Motivated by the above, this paper proposes Vevo, a \underline{ve}rsatile zero-shot \underline{vo}ice imitation framework with controllable timbre and style (Figure~\ref{fig:Vevo-pipeline}). It can serve as a unified framework for a wide range of zero-shot speech generation tasks. Vevo consists of two core stages: (1) \textbf{Content-Style Modeling} (\textit{Content to Content-Style}): Given a speech prompt as style reference, we generate \textit{content-style} tokens from the input \textit{content} tokens (or the input text). We employ the decoder-only autoregressive transformer~\cite{transformer,llama}, leveraging its powerful capability of continued generation to model style. (2) \textbf{Acoustic Modeling} (\textit{Content-Style to Acoustic}): Given a speech prompt as timbre reference, we generate acoustic representations (such as Mel spectrograms) from the input of \textit{content-style} tokens. We use a flow-matching transformer~\cite{flow-matching,dit}, which has been verified to excel in in-context learning and reconstructing high-quality audio~\cite{voicebox,audiobox,cosyvoice,fireredtts}, to achieve timbre-controllable generation.

To obtain the \textit{content} and \textit{content-style} tokens of speech, we design a self-supervised method to decouple the timbre, style, and linguistic content gradually, which is similar to a progressive information filtering: (1) We firstly investigate the commonly used self-supervised speech pre-trained model, HuBERT~\cite{HuBERT}. We find that its \textbf{continuous} hidden features contain rich information about timbre, style, and linguistic content (Section~\ref{sec:results-effect-of-codebook-size}), making it a suitable initial stage for information filtering. (2) Inspired by existing works for disentangling speaker-agnostic representations~\cite{vq-vae,vqvc,vq-content-style,ns3}, we employ VQ-VAE~\cite{vq-vae} as a tokenizer for HuBERT to filter out timbre, resulting in \textbf{content-style tokens}. (3) Furthermore, we propose that the vocabulary size of the VQ-VAE codebook can function as the ``width" of the information bottleneck~\cite{autovc}. By reducing the vocabulary size, we can narrow the bottleneck and filter out not only timbre but also significant style information, thereby obtaining \textbf{content tokens}. Besides, we propose to reduce the consecutive duplicate units~\cite{mhubert-duration-reduction} of the content tokens, called \textit{duration reduction}, to further remove some style patterns such as unit-level duration.

The contributions of this paper are summarized as follows:

- We introduce a fully self-supervised approach that progressively decouple timbre, style, and linguistic content of speech. The resulting content-style tokens and content tokens enhance controllability in downstream speech generation tasks, particularly for timbre and style.
- We propose Vevo, a unified framework that enables versatile, controllable zero-shot voice imitation tasks. It significantly reduces the reliance on annotated corpora, facilitating self-supervised training and in-context learning that can easily be scaled up.
- Pre-trained on 60K hours of audiobook speech data without any fine-tuning on style-specific corpora, Vevo matches or even surpasses existing methods in accent and emotion conversion tasks -- notably, through zero-shot imitation. Additionally, Vevo's effectiveness in voice conversion and text-to-speech tasks further demonstrates its strong generalization and versatility.

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

**Controllable Voice Imitation**

We focus primarily on how existing works approach the imitation of two key speech attributes: timbre and style. (1) \textbf{Imitation of Timbre}: As a crucial aspect of speaker identity, timbre imitation has been extensively explored within the voice conversion (VC) field. Most studies aim to utilize the speaker-agnostic representations such as PPG features~\cite{ppg-vc,voiceshop} or some self-supervised representations~\cite{self-supervised-vc,amphion-svc}, and use models including GAN~\cite{cyclegan-vc,stargan-vc}, auto-encoder~\cite{autovc,speechsplit}, and diffusion models~\cite{diffvc,diff-hiervc} to achieve timbre imitation. (2) \textbf{Imitation of Style}: In terms of style imitation, accent and emotion are two widely studied attributes. For conversion tasks (with speech as input), classic approaches often involve learning the conversion between parallel corpus~\cite{parallel-ac-zhaoguanlong21,parallel-ec-2016,voiceshop,convertandspeak}. Additionally, many studies aim to obtain the style-agnostic features, such as pushing them to be close to textual transcriptions~\cite{zhouyi-ac,chenxi-tts-ac,emovox,pavits}. Besides, leveraging automatic speech recognition (ASR) models can transform conversion tasks into synthesis tasks, allowing the injection of style label's embeddings into TTS models to achieve style imitation~\cite{asr-ac,liusongxiang-ac}. In conclusion, these existing approaches often rely on annotated data and struggle to achieve \textit{zero-shot} style imitation. (3) \textbf{Imitation of both Timbre and Style}: In VC, some works suggest adopting a sequence-to-sequence formulation~\cite{non-parallel-seq2seq-vc,lmvc} or introducing an additional modeling for prosody features~\cite{diff-hiervc,hierspeech++} to achieve both timbre and style imitation. However, these models still have significant room for improvement in both quality and style imitation. Recent advances in zero-shot TTS have greatly improved voice imitation and cloning. They leverage large-scale in-context learning to mimic all speech attributes of a reference prompt, including timbre and style, with high quality and speaker similarity~\cite{valle,megatts,ns3,seedtts,maskgct,u-style}. Nonetheless, it is challenging to obtain the speech representations disentangled timbre and style effectively~\cite{basetts,u-style}, leading to inadequate targeted control of these attributes. For instance, using the existing representations directly for VC tasks will lead to timbre leakage, unless mitigated by timbre perturbation or an additional fine-tuning stage~\cite{seedtts,maskgct}.

**Disentangled Speech Representation**

There are many studies aim to decouple linguistic content, timbre, and style. Existing work on obtaining disentangled speech representations can generally be categorized into several approaches: (1) Knowledge distillation using auxiliary tasks such as ASR, F0 prediction, and speaker verification~\cite{speech-resynthesis-interspeech21,ns3,basetts}, (2) Model architecture design based on information bottlenecks, including careful adjustments to hidden layer dimensions~\cite{autovc,speechsplit} or vector quantization methods like K-means~\cite{HuBERT,softvc,sef-vc-kmeans} or VQ-VAE~\cite{vq-vae,vqvc,vq-content-style,speech-resynthesis-interspeech21,ns3}, and (3) Perturbation of acoustic signals~\cite{nancy,nancypp,speechsplit2}. Besides, existing works also leverage additional learning strategies including adversarial learning~\cite{ns3,basetts}, comparative learning~\cite{contentvec,basetts}, and mutual information minimization~\cite{vq-content-style,mutual-information-zhuxinfa,mutual-information} to enhance disentanglement effectiveness. However, existing work still has two main weaknesses. On one hand, as mentioned earlier, finding suitable representations for downstream generation tasks that can effectively decouple timbre and style remains quite challenging. On the other hand, how to design voice imitation models that can control specific attributes based on these disentangled speech representations has been scarcely explored.

</td><td>

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td></tr></table>
