# Vevo

<details>
<summary>基本信息</summary>

- 标题: "Vevo: Controllable Zero-Shot Voice Imitation with Self-Supervised Disentanglement"
- 作者:
  - 01 Xueyao Zhang
  - 02 Xiaohui Zhang
  - 03 Kainan Peng
  - 04 Zhenyu Tang
  - 05 Vimal Manohar
  - 06 Yingru Liu
  - 07 Jeff Hwang
  - 08 Dangna Li
  - 09 Yuhao Wang
  - 10 Julian Chan
  - 11 Yuan Huang
  - 12 Zhizheng Wu
  - 13 Mingbo Ma
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.07243)
  - [Publication]() ICLR2025
  - [Github](https://github.com/open-mmlab/Amphion/tree/main/models/vc/vevo) VC
  - [Demo](https://versavoice.github.io/)
- 文件:
  - [ArXiv](../_PDF/2502.07243v1__Vevo__Controllable_Zero-Shot_Voice_Imitation_with_Self-Supervised_Disentanglement.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

The imitation of voice, targeted on specific speech attributes such as timbre and speaking style, is crucial in speech generation.
However, existing methods rely heavily on annotated data, and struggle with effectively disentangling timbre and style, leading to challenges in achieving controllable generation, especially in zero-shot scenarios.
To address these issues, we propose ***Vevo***, a versatile zero-shot voice imitation framework with controllable timbre and style.
***Vevo*** operates in two core stages: (1) Content-Style Modeling: Given either text or speech's content tokens as input, we utilize an autoregressive transformer to generate the content-style tokens, which is prompted by a style reference; (2) Acoustic Modeling: Given the content-style tokens as input, we employ a flow-matching transformer to produce acoustic representations, which is prompted by a timbre reference.
To obtain the content and content-style tokens of speech, we design a fully self-supervised approach that progressively decouples the timbre, style, and linguistic content of speech.
Specifically, we adopt VQ-VAE as the tokenizer for the continuous hidden features of HuBERT.
We treat the vocabulary size of the VQ-VAE codebook as the information bottleneck, and adjust it carefully to obtain the disentangled speech representations.
Solely self-supervised trained on 60K hours of audiobook speech data, without any fine-tuning on style-specific corpora, ***Vevo*** matches or surpasses existing methods in accent and emotion conversion tasks.
Additionally, ***Vevo***'s effectiveness in zero-shot voice conversion and text-to-speech tasks further demonstrates its strong generalization and versatility.
Audio samples are available at [this https URL](https://versavoice.github.io/).

</td><td>

语音模仿, 特别是针对特定语音属性如音色和说话风格的模仿, 在语音生成中至关重要.
然而, 现有方法过度依赖标注数据, 并且在有效解耦音色和风格方面存在困难, 导致在实现可控生成时面临挑战, 尤其是在零样本场景中.

为了解决这些问题, 我们提出了 ***Vevo***, 一种多功能的零样本语音模仿框架, 具备可控的音色和风格.

***Vevo***的核心包含两个阶段：
(1) 内容-风格建模：给定文本或语音的内容标记作为输入, 我们利用自回归变换器生成内容-风格标记, 风格由风格参考引导；
(2) 声学建模：给定内容-风格标记作为输入, 我们采用流匹配变换器生成声学表示, 由音色参考进行引导.

为了获得语音的内容和内容-风格标记, 我们设计了一种完全自监督的方法, 逐步解耦语音的音色、风格和语言内容.

具体来说, 我们采用VQ-VAE作为HuBERT连续隐藏特征的标记器.
我们将VQ-VAE词汇表的大小视为信息瓶颈, 并精心调整它以获得解耦的语音表示.

***Vevo*** 仅通过在60K小时的有声书语音数据上自监督训练, 无需针对特定风格的语料库进行微调, 便在口音和情感转换任务中与现有方法相匹配或超越它们.
此外, ***Vevo*** 在零样本语音转换和文本到语音任务中的有效性进一步证明了其强大的泛化能力和多功能性.

音频样本可以通过访问[这个链接](https://versavoice.github.io/)获取.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td></tr></table>
