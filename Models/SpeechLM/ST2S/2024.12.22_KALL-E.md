# KALL-E

<details>
<summary>基本信息</summary>

- 标题: "Autoregressive Speech Synthesis with Next-Distribution Prediction"
- 作者:
  - 01 Xinfa Zhu (NPU@ASLP)
  - 02 Wenjie Tian (NPU@ASLP)
  - 03 Lei Xie (NPU@ASLP)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.16846)
  - [Publication]
  - [Github]
  - [Demo](https://zxf-icpc.github.io/kalle/)
- 文件:
  - [ArXiv](../_PDF/2412.16846v1__KALL-E__Autoregressive_Speech_Synthesis_with_Next-Distribution_Prediction.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

We introduce ***KALL-E***, a novel autoregressive (AR) language modeling approach with next-distribution prediction for text-to-speech (TTS) synthesis.
Unlike existing methods, ***KALL-E*** directly models and predicts the continuous speech distribution conditioned on text without relying on VAE- or diffusion-based components.
Specifically, we use WaveVAE to extract continuous speech distributions from waveforms instead of using discrete speech tokens.
A single AR language model predicts these continuous speech distributions from text, with a Kullback-Leibler divergence loss as the constraint.
Experimental results show that ***KALL-E*** outperforms open-source implementations of YourTTS, VALL-E, NaturalSpeech 2, and CosyVoice in terms of naturalness and speaker similarity in zero-shot TTS scenarios.
Moreover, ***KALL-E*** demonstrates exceptional zero-shot capabilities in emotion and accent cloning.
Importantly, ***KALL-E*** presents a more straightforward and effective paradigm for using continuous speech representations in TTS.
Audio samples are available at: [this https URL](https://zxf-icpc.github.io/kalle/).

</td><td>

我们介绍 ***KALL-E***, 一种新颖的自回归语言建模方法, 通过下一分布预测来实现文本转语音合成.
和现有方法不同, ***KALL-E*** 直接建模并预测基于文本条件化的连续语音分布, 而不依赖于变分自编码器或扩散模型组件.
具体来说, 我们使用 WaveVAE 来从波形中提取连续语音分布而不是使用离散语音 Token.
单个自回归语言模型从文本预测这些连续语音分布, 以 Kullback-Leibler 散度损失作为约束.

实验结果表明 ***KALL-E*** 在零样本文本转语音场景下的自然度和说话人相似性优于 YourTTS, VALL-E, NaturalSpeech2, CosyVoice 的开源实现.
此外, ***KALL-E*** 在情感和口音克隆方面也展现出了卓越的零样本能力.
重要的是, ***KALL-E*** 提供了一个更加直接和有效的在文本转语音中使用连续语音表示的范式.
音频示例可在[此链接](https://zxf-icpc.github.io/kalle/) 获得.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

The past decade has seen remarkable advancements in speech synthesis driven by the development of neural networks ([^survey], [^survey2]).
Early text-to-speech (TTS) systems employ cascaded pipelines, combining acoustic models and vocoders, with Mel spectrograms serving as intermediate representations (**Tacotron**[^tacotron], **FastSpeech**[^fastspeech], **DurIAN**[^durian], **FastSpeech2**[^fs2], **Parallel Tacotron**[^ptaco]).
Later innovations have shifted towards end-to-end TTS modeling, enabling high-quality speech synthesis (**VITS**[^vits], **Glow-WaveGAN**[^glow-wavegan], [^clone], **VITS2**[^vits2]).
However, due to the inherent one-to-many mapping nature of TTS, these systems continue to suffer from over-smoothing issues ([^oversmoothing], **GenerSpeech**[^generspeech]).
Powered by large language models (LLMs) (**VALL-E**[^valle], **SPEAR-TTS**[^speartts], **UniAudio**[^uniaudio], [^unistyle], **Seed-TTS**[^seedtts], **CosyVoice**[^cosyvoice], [^dkguo], [^touchTTS]), diffusion models (**NaturalSpeech2**[^ns2], **NaturalSpeech3**[^ns3], **FlashSpeech**[^flashsspech], **SimpleSpeech**[^simplespeech], **E2 TTS**[^e2tts], **E3 TTS**[^e3tts], **MaskGCT**[^maskgct]) and large-scale corpora (**LibriLight**[^librilight], **wenetSpeech4TTS**[^wenetspeech4tts], **Emilia**[^Emilia]), current state-of-the-art (SOTA) TTS systems have achieved unprecedented levels of naturalness and diversity, including capabilities for zero-shot voice cloning.

</td><td>

</td></tr>
<tr><td>

Typical LLM-based TTS frameworks (**VALL-E**[^valle], **UniAudio**[^uniaudio]) rely on speech tokenizers (**EnCodec**[^encodec], **SoundStream**[^soundstream]) to quantize continuous speech waveforms into discrete tokens, which are then modeled autoregressively.
While significant efforts have been made to improve speech tokenizers (**SpeechTokenizer**[^speechtokenizer], **Single-Codec**[^singlecodec], **XCodec**[^xcodec], **WavTokenizer**[^wavtokenizer]), a fundamental trade-off persists between bitrate and the preservation of speech components (**NaturalSpeech2**[^ns2], **MELLE**[^melle]).
Some tokenizers (**EnCodec**[^encodec], **SoundStream**[^soundstream], **SpeechTokenizer**[^speechtokenizer]) employing multiple discrete tokens per speech frame capture richer acoustic information but significantly increase sequence length, making language modeling challenging.
Conversely, tokenizers ([^vectok], **CosyVoice**[^cosyvoice]) producing low-bitrate sequences simplify language modeling but result in lossy representations lacking acoustic detail.
Unlike text, speech waveforms are continuous in nature, which inherently makes it hard to achieve an ideal speech tokenizer that retains all acoustic nuances at a limited bit rate.

</td><td>

</td></tr>
<tr><td>

Recent works (**Spectron**[^Spectron], **MELLE**[^melle], [^contokenizer]) have explored continuous speech representations within AR language modeling frameworks to overcome the limitations of speech tokenization.
Continuous representations are considered nearly lossless carriers of speech information.
However, as highlighted in **MELLE**[^melle], the key challenges of using continuous speech representations in AR language models lie in the training objective and sampling mechanism.
**MELLE** addresses these challenges by introducing a VAE-like latent sampling module into an AR language model to predict Mel spectrograms, while other works ([^kaiminghe], [^ms_diff]) leverage diffusion-based heads for continuous representation prediction in visual and multimodal generation tasks.

</td><td>

</td></tr>
<tr><td>

In this work, we propose ***KALL-E***, a novel AR speech synthesis framework with next-distribution prediction.
***KALL-E*** first extracts continuous speech distributions via WaveVAE and predicts them directly through an AR language model, bypassing the need for VAE or diffusion heads and eliminating the inherent dilemma associated with speech tokenizers.
To tackle the challenge of the training objective, we replace the traditional cross-entropy loss with a Kullback-Leibler (KL) divergence loss for next-distribution prediction, supplemented by a binary cross-entropy (BCE) loss for stop prediction.
For the sampling mechanism, we employ a straightforward reparameterization technique to sample from the predicted speech distributions, effectively addressing the challenge of the sampling mechanism.

</td><td>

</td></tr>
<tr><td>

We evaluate ***KALL-E*** on the **LibriTTS**[^libritts] corpus and compare it with open-source implementations of several popular zero-shot TTS systems, including **YourTTS**[^yourtts], **VALL-E**[^valle], **NaturalSpeech2**[^ns2], and **CosyVoice**[^cosyvoice].
Following established benchmarks, we use the **LibriTTS test-clean set** for zero-shot TTS evaluation, the **ESD**[^ESD] corpus for zero-shot emotion cloning, and the **VCTK**[^vctk] corpus for accent cloning.
Experimental results demonstrate that ***KALL-E*** achieves competitive performance with these TTS systems on objective metrics while surpassing them on subjective metrics.
Moreover, ***KALL-E*** exhibits exceptional capabilities in zero-shot emotion and accent cloning despite training on a modest 500-hour dataset.

</td><td>

</td></tr></table>

## ~~2·Related Works: 相关工作~~

## 3·Methodology: 方法

### Problem Formulation: Next-Distribution Prediction<br>问题形式化: 下一分布预测

<table><tr><td width="50%">

***KALL-E*** regards zero-shot TTS as a conditional language modeling task achieved through next-distribution prediction.
As illustrated in [Figure.01](#Fig.01), ***KALL-E*** consists of a WaveVAE model and an AR language model.
The WaveVAE model extracts continuous speech distributions from speech waveforms.
Using well-established continuous distributions as the training targets, ***KALL-E*** directly predicts these distributions via the AR language model.
Finally, the predicted speech distributions are converted back into waveforms by the WaveVAE decoder.

</td><td>

</td></tr>
<tr><td>

Given a <text, speech> pair, the text tokenizer encodes the input text into a sequence of text tokens, $x=\{x_0, x_1, ..., x_L\}$, where $L$ is the number of text tokens.
The WaveVAE encoder extracts continuous speech distributions $z=\{z_0, z_1, ..., z_T\}$ from the corresponding speech waveform, where $T$ denotes the number of frames in continuous speech distributions,  and $z_i$ is sampled from $\mathcal{N}(Z_\mu^i, Z_\sigma^i)$.
***KALL-E*** is trained to predict $z$ from $x$ autoregressively.
Specifically, at each AR step $t$, ***KALL-E*** predicts the next speech distribution $z_t$ conditioned on the text prompt $x$ and the previously generated speech distributions $z_{<t}$.
This can be formulated as:

$$
    p(z|x;\theta) =\prod_{t=0}^{T}p(z_{t}|z_{<t},x;\theta),
$$

where $z_{<t}$ represents the previously predicted speech distributions $\{z_0, z_1, ..., z_{t-1}\}$ and $\theta$ represents the parameters of the AR language model.

</td><td>

</td></tr></table>

### WaveVAE

<table><tr><td width="50%">

To enable the AR language model to predict continuous speech distributions, we first employ WaveVAE to learn these distributions in an unsupervised manner.
Inspired by the Glow-WaveGAN series(**Glow-WaveGAN**[^glow-wavegan], [^glow-wavegan2], [^glow-wavegan3]), WaveVAE is a variational auto-encoder (VAE)[^vae] and consists of an encoder and a decoder, where the encoder maps the inputs $w$ into latent representations $z$, and the decoder reconstructs $w$ from $z$.
The process is formulated as follows:

$$
    z = Enc(w) \sim q(z|w),
$$

$$
    \hat{w} = Dec(z) \sim p(w|z),
$$

where $w$ means the input waveform, and $q(z|w)$ represents the latent distribution of speech, which is used to reconstruct waveform $\hat{w}$ from $p(w|z)$ via the decoder.

</td><td>

</td></tr>
<tr><td>

As for the detailed architecture of the proposed WaveVAE, the encoder consists of a stack of down-sampling dilated convolution layers with residual blocks, which effectively capture abstract features in the speech waveform.
After encoding, the produced mean and variance are interpreted as the parameters of the learned latent distribution $q(z|w) = \mathcal{N}(Z_\mu, Z_\sigma)$.
The decoder mirrors the encoder's architecture but uses transposed convolution layers with residual blocks to up-sample the latent representation $z$ back into the waveform $\hat{w}$.
Additionally, we integrate advanced techniques, such as the Snake activation function from BigVGAN[^bigvgan], to enhance the WaveVAE decoder's performance.

</td><td>

</td></tr>
<tr><td>

To train WaveVAE, we use four loss functions: (1) a Kullback-Leibler divergence loss $\mathcal{L}_{KL}$ between latent speech distribution $z$ and prior Gaussian distribution $\mathcal{N}(0, 1)$; (2) a reconstruction loss $\mathcal{L}_{recon}$ between predicted mel-spectrogram and ground-truth mel-spectrogram; (3) a discriminator loss $\mathcal{L}_{disc}$ including the multi-period discriminator loss[^mpd] and multi-resolution discriminator loss[^mrd]; (4) a feature matching loss $\mathcal{L}_{fm}$ between the feature map extracted from intermediate layers in each discriminator[^featuremap].
These losses work collaboratively to optimize WaveVAE, and the overall loss raining objective is defined as:

$$
    \mathcal{L}_{waave} = \lambda_{kl} \mathcal{L}_{KL} + \lambda_{recon} \mathcal{L}_{recon} + \lambda_{disc} \mathcal{L}_{disc} + \lambda_{fm} \mathcal{L}_{fm},
$$

where $\lambda_{kl}$, $\lambda_{recon}$, $\lambda_{disc}$, $\lambda_{fm}$ are the respective weighting factors for each loss component.

</td><td>

</td></tr></table>

### Autoregressive Language Model

<table><tr><td width="50%">

With the assistance of WaveVAE, ***KALL-E*** employs a causal transformer decoder as the language model to autoregressively predict the continuous speech distribution.
Specifically, input text tokens $x$, augmented with an <EOS> token, are first converted into embeddings by the text embedding layer.
Simultaneously, a linear layer projects the sampled speech distribution $z$ into the dimension of the language model.
The language model, consisting of blocks of multi-head self-attention and feed-forward layers, takes the concatenation of text and speech embeddings as input to model the dependency between semantic and acoustic information.

</td><td>

</td></tr>
<tr><td>

At each time step $t$, the output of the language model, $o_t$, is subsequently processed by a linear layer to predict the mean $\hat{Z}_{\mu}$ and variance $\hat{Z}_{\sigma}$ of target speech distribution.
These parameters are then used to sample the predicted speech distribution for the subsequent AR step.

</td><td>

</td></tr>
<tr><td>

The training objective for the AR language model consists of two components: (1) a Kullback-Leibler divergence loss $\mathcal{L}_{KL}$ between predicted and ground-truth speech distributions; (2) a binary cross-entropy loss $\mathcal{L}_{stop}$ for stop prediction.
Following the design of Tacotron[^tacotron] and other AR TTS models[^transformertts], [^speecht5], we employ a linear layer to project the output of the LM to logits, which are then used to calculate the BCE loss for stop prediction.
The overall training objective for the AR language model is formulated as follows:

$$
    \mathcal{L}_{LM} = \mathcal{L}_{KL} + \lambda_{stop} \mathcal{L}_{stop},
$$

where $\lambda_{stop}$ is a hyperparameter that balances the stop prediction loss with the KL divergence loss.

</td><td>

</td></tr></table>

### Inference

<table><tr><td width="50%">

During inference, ***KALL-E*** is capable of both unconditional and conditional text-to-speech synthesis.
For unconditional TTS, ***KALL-E*** autoregressively generates the target speech distribution directly from the provided input text.
Thanks to the reparameterization technique used in the sampling process, ***KALL-E*** can produce diverse speech outputs with varying speaker timbres and speaking styles for the same input text when performing batch inference.

</td><td>

</td></tr>
<tr><td>

For conditional TTS, such as in the case of zero-shot TTS, ***KALL-E*** achieves this through an in-context learning mechanism, similar to VALL-E and MELLE.
Specifically, given the text content $x$ for synthesis, along with the text transcription $\hat{x}$ and speech distribution $\hat{z}$ of acoustic prompt, ***KALL-E*** autoregressively generates the target speech distribution $z$ of the corresponding content while preserving the acoustic characteristics of the prompt speech.
This process is formulated by maximizing the likelihood probability: $p(z|\hat{x},x,\hat{z};\theta)$.

</td><td>

</td></tr></table>

## 4·Experiments: 实验

### Dataset

<table><tr><td width="50%">

We conduct experiments on the open-source LibriTTS corpus[^libritts], a high-quality speech dataset derived from the LibriSpeech corpus[^Librispeech].
LibriTTS contains approximately 585 hours of 24 kHz speech data from over 2,400 speakers, making it suitable for high-fidelity speech synthesis.
For evaluation, we use the LibriTTS test-clean set for zero-shot TTS performance evaluation.
Additionally, we utilize the ESD corpus[^ESD] for zero-shot emotion cloning evaluation and the VCTK corpus[^vctk] for zero-shot accent cloning evaluation.

</td><td>

</td></tr></table>

### Implement Details

<table><tr><td width="50%">

For WaveVAE, the encoder follows the architecture of **Glow-WaveGAN**[^glow-wavegan], while the decoder refers to the settings of **BigVGAN**[^bigvgan].
The latent dimension of the speech distribution $z$ is set to 64, with a frame rate of 100 Hz.
The AR language model comprises approximately 445M parameters and 24 **LLaMA**[^llama] layers, each with 16 attention heads.
The input embedding dimension for the language model is set to 1024.
The WaveVAE model is trained with a total batch size of 64 for 2,000k steps, while the AR language model is trained with a total batch size of 256 for 32k steps.

</td><td>

</td></tr></table>

### Comparison Systems

<table><tr><td width="50%">

To assess the performance of ***KALL-E***, we compare it with the open-source implementation of several popular zero-shot TTS models.
Note that the training data used for these models might be different.

- YourTTS[^yourtts]: A non-autoregressive TTS model based on VITS, enhanced with a pre-trained speaker encoder to improve zero-shot TTS capabilities.
We use the released checkpoint [Github](https://github.com/Edresson/YourTTS).
- VALL-E[^valle]: A codec language model approach for zero-shot TTS synthesis that uses audio codec codes as intermediate representations.
We use the open-source checkpoint [Github](https://github.com/open-mmlab/Amphion/tree/main/egs/tts/VALLE) from Amphion [^amphion].
- NaturalSpeech 2[^ns2]: A zero-shot TTS system that incorporates a neural audio codec with continuous latent vectors and a latent diffusion model for non-autoregressive speech generation.
We use the open-source checkpoint [Github](https://github.com/open-mmlab/Amphion/tree/main/egs/tts/NaturalSpeech2) from Amphion [^amphion].
- CosyVoice[^cosyvoice]: a scalable zero-shot speech synthesis system that integrates language and flow-matching models.
We use their released checkpoint [Github](https://github.com/FunAudioLLM/CosyVoice).

</td><td>

</td></tr></table>

### Evaluation Metrics

<table><tr><td width="50%">

We employ subjective and objective evaluations to assess the performance of synthetic speech.
For subjective evaluation, we use the Mean Opinion Score (MOS) to evaluate the naturalness of the synthetic speech, Similarity Mean Opinion Score (SMOS) to assess speaker similarity between the synthetic and prompt speech, Comparative Mean Opinion Scores (CMOS) for emotion or accent preference according to reference speech.
Each evaluation has at least 10 participants.
The rating scale is as follows: bad = 1, poor = 2, fair = 3, good = 4, great = 5, with half-point increments.
For objective evaluation, we follow Seed-TTS [Github](https://github.com/BytedanceSpeech/seed-tts-eval/tree/main) [^seedtts] and measure word error rate (WER) and speaker similarity (SIM).
Specifically, we use Whisper-large-v3[^whisper] to compute the WER of synthetic speech.
For SIM, we employ WavLM-large[^wavlm] fine-tuned on a speaker verification task to extract speaker embeddings and calculate the cosine similarity between the synthetic speech and reference clips.

</td><td>

</td></tr></table>

## 5·Results: 结果

### Zero-shot TTS

<table><tr><td width="50%">

As shown in Table~\ref{tab:zero-shot_tts}, ***KALL-E*** outperforms YourTTS, VALL-E, NaturalSpeech 2, and CosyVoice in MOS, demonstrating its ability to synthesize highly natural speech in zero-shot scenarios.
Moreover, ***KALL-E*** surpasses these systems by a notable margin in SMOS, highlighting its superior capability in cloning speaker timbre and other speech characteristics.
This advantage can be attributed to the use of continuous speech distributions, which capture finer-grained acoustic details compared to discrete representations used in comparison systems.

In objective evaluations, CosyVoice achieves the best WER, with ***KALL-E*** obtaining a higher WER compared to CosyVoice and NaturalSpeech 2.
This discrepancy is likely due to occasional issues in ***KALL-E***’s generated speech, such as missed or duplicated words, pointing to areas where synthesis robustness can be further improved.
Despite this, ***KALL-E*** achieves the highest SIM among all systems, corroborating its strength in zero-shot voice cloning and aligning with subjective evaluation results.

</td><td>

</td></tr></table>

### Zero-shot Emotion and Accent Cloning

<table><tr><td width="50%">

To further assess the zero-shot capabilities of ***KALL-E***, we conduct experiments on emotion cloning and accent cloning.
As shown in Table~\ref{tab:esd_tts}, all comparison models exhibit performance degradation in zero-shot emotional voice cloning, underscoring the inherent challenge of this task.
However, ***KALL-E*** demonstrates superior performance compared to its results on the LibriTTS test-clean set.
This observation highlights the richness of continuous speech distributions in capturing acoustic details.
Notably, the CMOS results indicate that ***KALL-E*** achieves the best performance in zero-shot emotion cloning among all models.

In zero-shot accent cloning, as depicted in Table~\ref{tab:vctk_tts}, NaturalSpeech 2 and CosyVoice achieve low WER, whereas YourTTS, VALL-E, and ***KALL-E*** exhibit relatively higher WER.
We find that accented pronunciation adversely affects the clarity of synthetic speech produced by YourTTS, VALL-E, and ***KALL-E***.
Despite this, ***KALL-E*** achieves the best results in accent preference as reflected in CMOS evaluations.
Furthermore, ***KALL-E*** records the highest SIM score, affirming its effectiveness in preserving speaker timbre during zero-shot accent cloning.

Additionally, ***KALL-E*** can generate diverse speech with various speaker timbres and speaking styles in an unconditional TTS manner.
We show this ability on our online demo page.

</td><td>

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

In this study, we propose ***KALL-E***, an AR language model approach for TTS with next-distribution prediction.
By leveraging WaveVAE to extract continuous speech distributions, ***KALL-E*** predicts these distributions conditioned on text input through an AR language model.
We demonstrate the superior in-context learning capability of ***KALL-E*** in zero-shot scenarios.
Additionally, ***KALL-E*** excels in cloning the emotion and accent during synthesis, offering diverse outputs in sampling-based inference.

Despite these advancements, ***KALL-E*** has several limitations.
First, like other AR TTS systems, it still faces challenges in synthesis robustness.
Specifically, some words may be unclear, missed, or duplicated in the generated speech.
Second, the inference efficiency can be improved.
Although the current system operates at a frame rate of 100 Hz, reducing the frame rate to a lower frame rate, such as 12.5 Hz, could improve computational efficiency.
Lastly, the LibriTTS corpus used for training is relatively small, and the performance of ***KALL-E*** could benefit from leveraging larger datasets, such as LibriLight[^librilight], WenetSpeech4TTS[^wenetspeech4tts] or Emilia[^Emilia].

</td><td>

</td></tr></table>

### Future Work

#### Scaling law

<table><tr><td width="50%">

We observe that when ***KALL-E*** is trained on a 10-hour corpus, the generated speech lacks intelligibility.
However, when scaled to a 500-hour corpus, such as LibriTTS, the intelligibility and naturalness of the synthesized speech improve significantly.
We believe that by scaling up to tens of thousands of hours of speech data, the performance of ***KALL-E*** could be further enhanced, enabling more realistic and expressive speech synthesis.

</td><td>

</td></tr></table>

#### Sampling mechanism

<table><tr><td width="50%">

In discrete token-based language models, the sampling mechanism significantly affects the quality, diversity, and coherence of the generated token sequence.
Standard techniques, such as top-k sampling, top-p sampling, and additional parameters like temperature and repetition penalty, can be adjusted to optimize performance.
However, there is limited research on effective sampling from continuous speech representations.
Investigating and developing new sampling strategies for continuous latent distributions will be key to improving ***KALL-E***'s generation capabilities.

</td><td>

</td></tr></table>

#### Spoken dialogue system

<table><tr><td width="50%">

According to **WavChat**[^wavchat], most speech comprehension systems rely on continuous speech representations for capturing fine-grained speech details, whereas most speech generation systems typically use discrete speech representations due to next-token prediction.
However, in AR language models, the output is intrinsically equal to the input, meaning the output and input representations should be the same type.
***KALL-E***'s success in using continuous speech distributions for speech generation suggests a potential pathway for integrating continuous representations into spoken dialogue systems, addressing the current divide between comprehension and generation models.

</td><td>

</td></tr></table>

## References: 参考文献

[^amphion]:
[^bigvgan]:
[^clone]:
[^contokenizer]:
[^cosyvoice]:
[^dkguo]:
[^durian]:
[^e2tts]:
[^e3tts]:
[^Emilia]:
[^encodec]: [**EnCodec**: High Fidelity Neural Audio Compression.](../../SpeechCodec/2022.10.24_EnCodec.md) ArXiv:2210.13438.
[^ESD]:
[^fastspeech]:
[^featuremap]:
[^flashsspech]:
[^fs2]:
[^generspeech]:
[^glow-wavegan]:
[^glow-wavegan2]:
[^glow-wavegan3]:
[^kaiminghe]:
[^librilight]:
[^Librispeech]:
[^libritts]:
[^llama]:
[^maskgct]:
[^melle]:
[^mpd]:
[^mrd]:
[^ms_diff]:
[^ns2]:
[^ns3]:
[^oversmoothing]:
[^ptaco]:
[^seedtts]:
[^simplespeech]:
[^singlecodec]:
[^soundstream]:
[^speartts]:
[^Spectron]:
[^speecht5]:
[^speechtokenizer]:
[^survey]: [**Survey**: Neural Text-to-Speech Synthesis.]() Springer2023.
[^survey2]: [**Survey**: Towrads](../../../Surveys/2024.12.09_Towards_Controllable_Speech_Synthesis_in_the_Era_of_LLM_23P/Main.md) ArXiv:2412.06602.
[^tacotron]: [**Tacotron**: A Fully End-to-End Text-to-Speech Synthesis Model.](../../Acoustic/2017.03.29_Tacotron.md) InterSpeech2017.
[^touchTTS]:
[^transformertts]:
[^uniaudio]: [**UniAudio**: Towards Universal Audio Generation with Large Language Models.](../2023.10.01_UniAudio.md) ICML2024.
[^unistyle]:
[^vae]:
[^valle]: [**VALL-E**: Neural Codec Language Models Are Zero-Shot Text to Speech Synthesizers.](2023.01.05_VALL-E.md) ArXiv:2301.02111.
[^vctk]:
[^vectok]:
[^vits]:
[^vits2]:
[^wavchat]:
[^wavlm]:
[^wavtokenizer]:
[^wenetspeech4tts]:
[^whisper]: [**Whisper**: Robust Speech Recognition via Large-Scale Weak Supervision.](../../ASR/2022.12.06_Whisper.md) ICML2023.
[^xcodec]:
[^yourtts]:
