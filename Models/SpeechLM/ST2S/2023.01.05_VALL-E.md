# VALL-E

<details>
<summary>基本信息</summary>

- 标题: "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers"
- 作者:
  - 01 Chengyi Wang (王程一) - Microsoft
  - 02 Sanyuan Chen (陈三元) - Microsoft
  - 03 Yu Wu (吴俣) - Microsoft
  - 04 Ziqiang Zhang (张自强) - Microsoft
  - 05 Long Zhou (周龙) - Microsoft
  - 06 Shujie Liu (刘树杰) - Microsoft
  - 07 Zhuo Chen (陈卓) - Microsoft
  - 08 Yanqing Liu - Microsoft
  - 09 Huaming Wang - Microsoft
  - 10 Jinyu Li (李劲宇) - Microsoft
  - 11 Lei He (何磊) - Microsoft
  - 12 Sheng Zhao (赵胜) - Microsoft
  - 13 Furu Wei (韦福如) - Microsoft
- 链接:
  - [ArXiv](https://arxiv.org/abs/2301.02111)
  - [Publication]
  - [Github]
    - 2023.01.27 [lifeiteng/vall-e](https://github.com/lifeiteng/vall-e)
    - 2023.12.02 [open-mmlab/Amphion](https://github.com/open-mmlab/Amphion/tree/main/models/tts/valle)
    - 2024.06.10 [dukGuo/valle-audiodec](https://github.com/dukGuo/valle-audiodec) 仅推理
  - [Demo](https://aka.ms/valle)
- 文件:
  - [ArXiv](../_PDF/2301.02111v1__VALL-E__Neural_Codec_Language_Models_are_Zero-Shot_TTS.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

We introduce a language modeling approach for *text-to-speech synthesis (TTS)*.
Specifically, we train a neural codec language model (called ***VALL-E***) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work.
During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems.
***VALL-E*** emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt.
Experiment results show that ***VALL-E*** significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity.
In addition, we find that ***VALL-E*** could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis.
See https://aka.ms/valle for demos of our work.

</td><td>

我们介绍了一种用于语音合成的语言建模方法.
具体来说, 我们使用从现成的神经音频编解码器模型中导出的离散编码训练了一个神经编解码器语言模型 (称为 ***VALL-E***), 并将 TTS 视为一个条件语言建模任务, 而不是像以前的工作那样视为连续信号回归任务.
在预训练阶段, 我们将 TTS 训练数据扩展到 60K 小时的英语语音, 这比现有系统大数百倍.
***VALL-E*** 展现出上下文学习能力, 并可用于使用仅 3 秒的未见说话者的输入录音作为声学提示来合成高质量的个性化语音.
实验结果表明 ***VALL-E*** 在语音自然度和说话者相似度方面显著优于最先进的零次语言合成系统.
此外, 我们发现 ***VALL-E*** 可以在合成中保留声学提示中的说话者的情感和声学环境
请访问 https://aka.ms/valle 查看本项工作的示例.

</td></tr></table>

## 1·Introduction: 引言

![Images/2023.01.05_VALL-E_Fig.01.png](../Images/2023.01.05_VALL-E_Fig.01.png)

<table><tr><td width="50%">

The last decade has yielded dramatic breakthroughs in speech synthesis through the development of neural networks and end-to-end modeling.
Currently, cascaded text to speech (TTS) systems (**Tacotron2**[^01], **FastSpeech**[^02], **Transformer TTS**[^03]) usually leverage a pipeline with an acoustic model and a vocoder using mel spectrograms as the intermediate representations.
While advanced TTS systems can synthesize high-quality speech from single or multiple speakers (**DelightfulTTS2**[^04], **VITS**[^05]), it still requires high-quality clean data from the recording studio.
Large-scale data crawled from the Internet cannot meet the requirement, and always lead to performance degradation.
Because the training data is relatively small, current TTS systems still suffer from poor generalization.
Speaker similarity and speech naturalness decline dramatically for unseen speakers in the zero-shot scenario.
To tackle the zero-shot TTS problem, existing work leverages speaker adaptation (**SEA**[^06], [^07]) and speaker encoding ([^08], **YourTTS**[^09]) methods, requiring additional fine-tuning, complex pre-designed features, or heavy structure engineering.

</td><td>

</td></tr>
<tr><td>

Instead of designing a complex and specific network for this problem, the ultimate solution is to train a model with large and diverse data as much as possible, motivated by success in the field of text synthesis (**GPT-3**[^10], **PaLM**[^11]).
Recent years have witnessed notable performance improvement for data increase in the text language model, from 16GB of uncompressed text (**BERT**[^12]), to 160GB (**RoBERTa**[^13]), to 570GB (**GPT-3**[^10]), and finally, around 1TB (**PaLM**[^11]).
Transferring this success to the field of speech synthesis, we introduce ***VALL-E***, the first language model based TTS framework leveraging the large, diverse, and multi-speaker speech data.
As shown in [Figure.01](#Fig.01), to synthesize personalized speech (e.g., zero-shot TTS), ***VALL-E*** generates the corresponding acoustic tokens conditioned on the acoustic tokens of the 3-second enrolled recording and the phoneme prompt, which constrain the speaker and content information respectively.
Finally, the generated acoustic tokens are used to synthesize the final waveform with the corresponding neural codec decoder (**EnCodec**[^14]).
The discrete acoustic tokens derived from an audio codec model enable us to treat TTS as conditional codec language modeling, and advanced prompting-based large-model techniques (as in **GPTs**[^10]) can be leveraged for the TTS tasks.
The acoustic tokens also allow us to generate diverse synthesized results in TTS by using different sampling strategies during inference.

</td><td>

</td></tr>
<tr><td>

We train ***VALL-E*** with **Libri-Light**[^15], a corpus consisting of 60K hours of English speech with over 7000 unique speakers.
The original data is audio-only, so we employ a speech recognition model to generate the transcriptions.
 Compared to previous TTS training datasets, such as **LibriTTS**[^16], our data contain more noisy speech and inaccurate transcriptions but provide diverse speakers and prosodies.
We believe the proposed approach is robust to the noise and generalize well by leveraging large data.
It is worth noting that existing TTS systems are always trained with dozens of hours of single-speaker data or hundreds of hours of multi-speaker data, which is over hundreds of times smaller than ***VALL-E***.
[Table.01](#Tab.01) summarizes the innovation of ***VALL-E***, a language model approach for TTS, using audio codec codes as intermediate representations, leveraging large and diverse data, leading to strong in-context learning capabilities.

</td><td>

</td></tr>
<tr><td colspan="2">

|Table 1|Current Systems|VALL-E|
|:-:|:-:|:-:|
|Intermediate Representation|Mel Spectrogram|Audio Codec Code|
|Objective Function|Continuous Signal Regression|Language Model|
|Training Data|≤600 Hours| 60K Hours|
|In-Context Language|×|√|

</td></tr>
<tr><td>

We evaluate ***VALL-E*** on **LibriSpeech**[^17] and **VCTK**[^18] datasets, where all test speakers are unseen in the training corpus.
***VALL-E*** significantly outperforms the state-of-the-art zero-shot TTS system (**YourTTS**[^09]) in terms of speech naturalness and speaker similarity, with +0.12 comparative mean option score (CMOS) and +0.93 similarity mean option score (SMOS) improvement on **LibriSpeech**.
***VALL-E*** also beats the baseline on **VCTK** with +0.11 SMOS and +0.23 CMOS improvements.
It even achieves a +0.04 CMOS score against ground truth, showing the synthesized speech of unseen speakers is as natural as human recordings on **VCTK**.
Moreover, the qualitative analysis shows that ***VALL-E*** is able to synthesize diverse outputs with the same text and target speaker, which could benefit pseudo-data creation for the speech recognition task.
We also find that ***VALL-E*** could keep the acoustic environment (e.g., reverberation) and emotion (e.g., anger) of the acoustic prompt.

</td><td>

</td></tr>
<tr><td>

In summary, we make the following contributions.
- We propose ***VALL-E***, the first TTS framework with strong in-context learning capabilities as GPT-3, which treats TTS as a language model task with audio codec codes as an intermediate representation to replace the traditional mel spectrogram.
It has in-context learning capability and enables prompt-based approaches for zero-shot TTS, which does not require additional structure engineering, pre-designed acoustic features, and fine-tuning as in previous work.
- We build a generalized TTS system in the speaker dimension by leveraging a huge amount of semi-supervised data, suggesting that simple scaling up semi-supervised data has been underestimated for TTS.
- ***VALL-E*** is able to provide diverse outputs with the same input text and keep the acoustic environment and speaker’s emotion of the acoustic prompt.
- We verify that ***VALL-E*** synthesizes natural speech with high speaker similarity by prompt-ing in the zero-shot scenario.
Evaluation results show that ***VALL-E*** significantly outperforms the state-of-the-art zero-shot TTS system on **LibriSpeech** and **VCTK**.

We encourage the reader to listen to our samples on the [demo page](https://aka.ms/valle).

</td><td>

</td></tr></table>

## 2·Related Work: 相关工作

### Zero-Shot TTS: 零样本文本转语音

<table><tr><td width="50%">

Current TTS methods can be categorized into cascaded and end-to-end methods.
Cascaded TTS systems (**Tacotron2**[^01], **FastSpeech**[^02], **Transformer TTS**[^03]) usually leverage a pipeline with an acoustic model and a vocoder using mel spectrograms as the intermediate representations.
To tackle the drawbacks of the vocoder, end-to-end TTS models (**VITS**[^05], **DelightfulTTS2**[^04]) are proposed to jointly optimize the acoustic model and vocoder.
In real scenarios, it is highly desirable to customize a TTS system to an arbitrary voice with rare enrolled recordings.
Therefore, there is growing interest in the zero-shot multi-speaker TTS techniques, and most of work is done in the context of cascaded TTS systems.
As the pioneers, [^08] proposes speaker adaptation and speaker encoding approaches.
In the line of speaker adaptation, the following work (**SEA**[^06], [^07], **AdaSpeech**[^19]) tries to improve the adaptation efficiency with less target speaker data and speaker-specific parameters.
**Meta-TTS**[^20] applies meta-learning on speaker adaptation, which only requires 5-shot to build a well-performed system.
In parallel, speaker encoding-based methods achieved great progress in recent years.
A speaker encoding based system contains a speaker encoder and a TTS component, where the speaker encoder could be pre-trained on the speaker verification task [^21].
In [^21] and [^08], the experiments show that the model is able to generate high-quality outputs with 3 seconds enrolled recordings for in-domain speakers.
To improve the quality of unseen speakers, advanced speaker embedding models [^22] can be employed, but it is still undesirable according to **Survey by Tan et al.(2021)**[^23].
Another way is to design advanced but complex speaker encoder (**AdaSpeech4**[^24]).
Diffusion model based TTS (**Grad-TTS**[^25], **Guided-TTS**[^26]) is also extended to zero-shot TTS (**Grad-StyleSpeech**[^27]) and achieved good results.
Compared to previous work (**FastSpeech**[^02], **VQTTS**[^28]), our work follows the line of cascaded TTS but first uses audio codec code as intermediate representations.
It is the first one that has strong in-context learning capabilities as GPT-3, which does not require fine-tuning, pre-designed features, or a complex speaker encoder.

</td><td>

</td></tr></table>

### Spoken Generative Pre-Trained Models: 口语生成式预训练模型

<table><tr><td width="50%">

Self-supervised learning is widely investigated in the field of speech understanding (**Wav2Vec2.0**[^29], **HuBERT**[^30], **WavLM**[^31]) and speech-to-speech generation (**GSLM**[^32], **AudioLM**[^33]).
In the context of speech-to-speech generation, a hot topic is how to synthesize speech in a textless setting.
**GSLM**[^32] proposes to synthesize speech based on **HuBERT** codes [^30], and [^34] improves the performance by combining HuBERT codes with codes of VQVAE and a speaker encoder.
**AudioLM**[^33] follows a similar way but use audio codecs (**SoundStream**[^35]) to synthesize speech, together with semantic codes.
It should be noted that AudioLM is able to synthesize speech based on audio codecs without training an additional vocoder such as **HiFi-GAN**[^36].
AudioLM is a speech-to-speech model, whereas ***VALL-E*** is a TTS model, so we can explicitly control the content in speech synthesis.
Another direction is to apply pre-training to the neural TTS.
[^37] pre-trains speech decoder in TTS through autoregressive mel-spectrogram prediction.
In **SpeechT5**[^38], the authors propose a unified-modal encoder-decoder framework SpeechT5, which can leverage unlabeled speech and text data to pre-train all components of TTS model.
[^39] quantizes unlabeled speech into discrete tokens by a **VQ-VAE** model[^40], and train a model with the token-to-speech sequence.
They demonstrate that the pre-trained model only requires a small amount of real data for fine-tuning.
**A^3T**[^41] proposes mask and reconstruction on mel spectrogram and showing better performance on speech editing and synthesis.
Previous TTS pre-training work leverages less than 1K hours of data, whereas ***VALL-E*** is pre-trained with 60K hours of data.
Furthermore, ***VALL-E*** is the first to use audio codec codes as intermediate representations, and emerge in-context learning capability in zero-shot TTS.

</td><td>

</td></tr></table>

## 3·Background·Speech Quantization: 语音量化背景

<table><tr><td width="50%">

Since audio is typically stored as a sequence of 16-bit integer values, a generative model is required to output $2^{16}=65,536$ probabilities per timestep to synthesize the raw audio.
In addition, the audio sample rate exceeding ten thousand leads to an extraordinarily long sequence length, making it more intractable for raw audio synthesis.
To this end, speech quantization is required to compress integer values and sequence length.
$\mu$-law transformation can quantize each timestep to 256 values and reconstruct high-quality raw audio.
It is widely used in speech generative models, such as **WaveNet**[^42], but the inference speed is still slow since the sequence length is not reduced.
Recently, vector quantization is widely applied in self-supervised speech models for feature extraction, such as **VQ-Wav2Vec**[^43] and **HuBERT**[^30].
The following work (**GSLM**[^32], **VQTTS**[^28]) shows the codes from self-supervised models can also reconstruct content, and the inference speed is faster than **WaveNet**.
However, the speaker identity has been discarded and the reconstruction quality is low (**AudioLM**[^33]).
**AudioLM**[^33] trains speech-to-speech language models on both k-means tokens from a self-supervised model and acoustic tokens from a neural codec model, leading to high-quality speech-to-speech generation.

In this paper, we follow **AudioLM**[^33] to leverage neural codec models to represent speech in discrete tokens.
To compress audio for network transmission, codec models are able to encode waveform into discrete acoustic codes and reconstruct high-quality waveform even if the speaker is unseen in training.
Compared to traditional audio codec approaches, the neural-based codec is significantly better at low bitrates, and we believe the quantized tokens contain sufficient information about the speaker and recording conditions.
Compared to other quantization methods, the audio codec shows the following advantages:
1) It contains abundant speaker information and acoustic information, which could maintain speaker identity in reconstruction compared to **HuBERT** codes[^30].
2) There is an off-the-shelf codec decoder to convert discrete tokens into a waveform, without the additional efforts on vocoder training like VQ-based methods that operated on spectrum (**VQTTS**[^28]).
3) It could reduce the length of time steps for efficiency to address the problem in $\mu$-law transformation (**WaveNet**[^42]).

We adopt a pre-trained neural audio codec model, **EnCodec**[^14], as our tokenizer.
EnCodec is a convolutional encoder-decoder model, whose input and output are both 24 kHz audio across variable bitrates.
The encoder produces embeddings at 75 Hz for input waveforms at 24 kHz, which is a 320-fold reduction in the sampling rate.
Each embedding is modeled by a residual vector quantization (RVQ), in which we choose eight hierarchy quantizers with 1024 entries each as shown in [Figure.02](#Fig.02).

This configuration corresponds to EnCodec at 6K bitrates for 24 kHz audio reconstruction.
In this setting, given a 10-second waveform, the discrete representation is a matrix with750 × 8entries, where 750 =24,000×10/320 is the downsampled time step and 8 is the number of quantizers.
It is fine to choose other bitrate settings.
A larger bitrate corresponds to more quantizers and better reconstruction quality.
For example, if we choose EnCodec at 12K bitrates, there are 16 quantizers are needed and the 10-second waveform corresponds to a matrix with 750×16 entries.
With the discrete codes from all quantizers, the convolutional decoder of EnCodec generates real-valued embeddings and reconstructs the waveform at 24 kHz.

</td><td>

</td></tr></table>

![](../Images/2023.01.05_VALL-E_Fig.02.png)

## 4·Methodology: 方法

### 4.1·Problem Formulation: Regarding TTS as Conditional Codec Language Modeling

<table><tr><td width="50%">

Given a dataset $\mathcal{D}=\{\mathbf{x}_i, \mathbf{y}_i\}$, where $\mathbf{y}$ is an audio sample and $\mathbf{x} = \{x_0,x_1, \cdots x_L\}$ is its corresponding phoneme transcription, we use a pre-trained neural codec model to encode each audio sample into discrete acoustic codes, denoted as $\text{Encodec}(\mathbf{y}) = C^{T\times 8}$, where $C$ represents the two-dimensional acoustic code matrix, and $T$ is the downsampled utterance length.
The row vector of each acoustic code matrix $c_{t,:}$ represents the eight codes for frame $t$ and the column vector of each acoustic code matrix $c_{:,j}$ represents the code sequence from the $j$-th codebook, where $j \in \{1,\cdots 8\}$.
After quantization, the neural codec decoder is able to reconstruct the waveform, denoted as $\text{Decodec}(C)\approx\hat{\mathbf{y}}$.

</td><td>

给定一个数据集 $\mathcal{D}=\{\mathbf{x}_i, \mathbf{y}_i\}$, 其中 $\mathbf{y}$ 是音频样本，$\mathbf{x} = \{x_0,x_1, \cdots x_L\}$ 是其对应的音素序列.
我们使用预训练的神经编解码模型将每个音频样本编码为离散声学编码，记为
$$
\text{Encodec}(\mathbf{y}) = C^{T\times 8},
$$

- $C$: 二维音频码矩阵;
- $T$: 降采样后的语音长度.

每个声学编码矩阵的行向量 $c_{t,:}$ 表示第 $t$ 帧的八个编码, 列向量 $c_{:,j}$ 表示第 $j$ 个码本的编码序列, 其中 $j \in \{1,\cdots 8\}$.
在量化之后, 神经编解码器的解码器能够重构波形, 记为

$$
\text{Decodec}(C)\approx\hat{\mathbf{y}}.
$$

</td></tr>
<tr><td>

Zero-shot TTS requires the model to synthesize high-quality speech for unseen speakers.
In this work, we **regard zero-shot TTS as a conditional codec language modeling task**.
We train a neural language model to generate an acoustic code matrix $C$ conditioned on a phoneme sequence $\mathbf{x}$ and an acoustic prompt matrix $\tilde{C}^{T'\times 8}$ with the optimization objective of $\max p(C|\mathbf{x},\tilde{C})$.
Here, $\tilde{C}$ is obtained by the same neural codec with an enrolled recording as the input.
We expect the neural language model learns to extract the content and speaker information from the phoneme sequence and the acoustic prompt, respectively.
During inference, given a phoneme sequence and a 3-second enrolled recording of the unseen speaker, the acoustic code matrix with corresponding content and speaker’s voice is firstly estimated by the trained language model.
Then the neural codec decoder synthesizes the high-quality speech.

</td><td>

零样本 TTS 要求模型能够为未见过的说话人生成高质量语音.
在本文中, 我们**将零样本 TTS 视为条件编解码语言建模任务**.
我们训练了一个神经语言模型以音素序列 $\mathbf{x}$ 和声学提示矩阵 $\tilde{C}^{T'\times 8}$ 为条件, 生成声学编码矩阵 $C$, 优化目标为最大化 $p(C|\mathbf{x},\tilde{C})$.
这里, $\tilde{C}$ 是通过相同神经编解码器从输入录音中获得的.

我们期望神经语言模型能够从音素序列和声学提示中分别提取内容和说话人信息.
在推理时, 给定一个音素序列和一个未见过的说话人的三秒录音, 首先通过训练好的语言模型估计出声学编码矩阵, 及其内容和说话人声音.
然后, 神经编解码器的解码器进行高质量语音的合成.

</td></tr></table>

### 4.2·Training: Conditional Codec Language Modeling

<table><tr><td width="50%">

The neural speech codec model allows us to operate on discrete audio representations.
Due to residual quantization in the neural codec model, the tokens have a hierarchical structure: tokens from previous quantizers recover acoustic properties like speaker identity, while the consecutive quantizers learn fine acoustic details.
Each quantizer is trained to model the residual from the previous quantizers.
Motivated by this, we design two conditional language models in a hierarchical manner.


</td><td>

神经语音编解码器模型允许我们在离散的音频表示上进行操作.
由于神经编解码器模型中的残差量化, Token 具有层次结构: 来自前面量化器的 Token 可以恢复声学性质 (如说话人身份), 而后续量化器学习精细的声学细节.
每个量化器都被训练为建模先前量化器的残差.
受此启发, 我们以层次化的方式设计了两个条件语言模型.

</td></tr>
<tr><td>

For the discrete tokens from the first quantizer $c_{:,1}$, we train an *AutoRegressive (AR)* decoder-only language model.
It is conditioned on the phoneme sequence $x$ and the acoustic prompt $\tilde{C}_{:,1}$, formulated as <a id="Eq.01"></a>

$$
  p(c_{:,1}|\mathbf{x}, \tilde{C}_{:,1}; \theta_{AR}) =\prod_{t=0}^T p(c_{t,1}|c_{<t,1},\tilde{c}_{:,1}, \mathbf{x}; \theta_{AR}) \tag{1}
$$

Since ***VALL-E*** is a decoder-only LM, the concatenation of $\tilde{c}_{:,1}$ and $c_{:,1}$ is a whole sequence, and we do not distinguish them or insert a specific token in training.
Only $c_{:,1}$ is predicted while the prefix $\tilde{c}_{:,1}$ is given during inference.

</td><td>

对于来自第一个量化器的离散 Token $c_{:,1}$, 我们训练一个自回归的仅解码器架构的语言模型.
它以音素序列 $x$ 和声学提示 $\tilde{C}_{:,1}$ 为条件, 形式化为

$$
  p(c_{:,1}|\mathbf{x}, \tilde{C}_{:,1}; \theta_{AR}) =\prod_{t=0}^T p(c_{t,1}|c_{<t,1},\tilde{c}_{:,1}, \mathbf{x}; \theta_{AR}) \tag{1}
$$

由于 ***VALL-E*** 是仅解码器架构的语言模型, 因此将 $\tilde{c}_{:,1}$ 和 $c_{:,1}$ 连接起来作为一个整体序列, 我们并不区分它们或在训练中插入特定的标记.
在推理时当给定前缀 $\tilde{c}_{:,1}$ 时, 只需预测 $c_{:,1}$.

</td></tr>
<tr><td>

For the discrete tokens from the second to the last quantizers, $c_{:,j}\in[2,8]$, we train a *non-autoregressive (NAR)* language model.
Since the tokens can not access each other in a NAR manner, to constrain the speaker identity, the acoustic prompt matrix $\tilde{C}$ is used as an acoustic prompt.
Thus, the model is conditioned on the phoneme sequence $x$, the acoustic prompt $\tilde{C}$ and the predicted acoustic tokens belong to the previous codebooks $C_{:,<j}$: <a id="Eq.02"></a>

$$
  p(C_{:,2:8}|\mathbf{x},\tilde{C};\theta_{NAR})=\prod_{j=2}^{8}p(c_{:,j}|C_{:,<j},\mathbf{x},\tilde{C};\theta_{NAR}) \tag{2}
$$

</td><td>

对于来自第二个到最后一个量化器的离散 Token $c_{:,j}\in[2,8]$, 我们训练了一个非自回归的语言模型.
因为在非自回归方式下, Token 无法访问彼此, 为了约束说话人身份, 声学提示矩阵 $\tilde{C}$ 被用作声学提示.
因此, 模型以音素序列 $x$, 声学提示 $\tilde{C}$ 和属于前面的码本 $C_{:,<j}$ 的已预测声学 Token 为条件:

$$
  p(C_{:,2:8}|\mathbf{x},\tilde{C};\theta_{NAR})=\prod_{j=2}^{8}p(c_{:,j}|C_{:,<j},\mathbf{x},\tilde{C};\theta_{NAR}) \tag{2}
$$

</td></tr>
<tr><td>

The combination of the AR model and the NAR model provides a good trade-off between speech quality and inference speed.
On the one hand, the rate of the generated speech should be consistent with the enrolled recording, and it is hard to train a length predictor for different speakers since their speaking speed may be very diverse.
In this case, the AR model is a more natural choice with its flexibility for acoustic sequence length prediction.
On the other hand, for the consecutive stages, as the number of output slots follows the sequence length of the first stage, NAR can reduce the time complexity from $\mathcal{O}(T)$ to $\mathcal{O}(1)$.
Overall, the prediction of $C$ can be modeled as: <a id="Eq.03"></a>

$$
  p(C|\mathbf{x},\tilde{C};\theta)=p(c_{:,1}|\tilde{C}_{:,1}, \mathbf{X}; \theta_{AR}) \prod_{j=2}^{8}p(c_{:,j}|c_{:,<j},\mathbf{x},\tilde{C};\theta_{NAR}) \tag{3}
$$

</td><td>

自回归模型和非自回归模型的组合提供了语音质量和推理速度之间的良好权衡.
一方面, 生成语音的速率应该和输入录音一致, 但由于不同说话人的说话速度可能非常不同, 要训练用于不同说话人的长度预测器是困难的.
在这种情况下, 自回归模型是一种更自然的选择, 它在声学序列长度预测方面具有灵活性.

另一方面, 对于后续阶段, 由于输出槽的数量与第一个阶段的序列长度一致, 非自回归可以将时间复杂度从 $\mathcal{O}(T)$ 降低到 $\mathcal{O}(1)$.

综上, $C$ 的预测可以表示为:

$$
  p(C|\mathbf{x},\tilde{C};\theta)=p(c_{:,1}|\tilde{C}_{:,1}, \mathbf{X}; \theta_{AR}) \prod_{j=2}^{8}p(c_{:,j}|c_{:,<j},\mathbf{x},\tilde{C};\theta_{NAR}) \tag{3}
$$

</td></tr></table>

#### Autoregressive Codec Language Modeling

<table><tr><td width="50%">

The autoregressive language model generates the tokens from the first quantizer.
It comprises a phoneme embedding $W_x$, an acoustic embedding $W_a$, a Transformer decoder, and a prediction layer.
In order to generate speech with specific content, we use the phoneme sequence as the phoneme prompt of the language model.
Thus, the model input is the concatenation of $\mathbf{x}$ and $\mathbf{c}_{:,1}$, and two special `<EOS>` tokens are appended after each of them.
We compute sinuous position embedding separately for prompt and input tokens.
For the causal Transformer model, each token $c_{t,1}$ can attend to $(\mathbf{x}, \mathbf{c}_{\leq t,1})$ as illustrated in the left part of [Figure.03](#Fig.03).
The model is optimized to maximize the probability of the next token in the first codebook.
We share the parameters of the output projection layer with the parameters of the acoustic embedding $W_a$.

</td><td>

自回归语言模型生成来自第一个量化器的 Token.
它由音素嵌入 $W_x$, 声学嵌入 $W_a$, Transformer 解码器, 预测层组成.
为了生成具有指定内容的语音, 我们使用音素序列作为语言模型的音素提示.
因此, 模型输入是音素序列 $\mathbf{x}$ 和 $\mathbf{c}_{:,1}$ 的拼接, 并分别在每个后面添加 `<EOS>` 特殊标记.
我们为提示和输入 Token 分别计算正弦位置嵌入.
对于因果 Transformer 模型, 每个 Token $c_{t,1}$ 可以注意到 $(\mathbf{x}, \mathbf{c}_{\leq t,1})$ (如[图 03](#Fig.03) 左侧所示).
模型被优化以最大化第一个码本中 Next-Token 的概率.
我们共享输出映射层与声学嵌入层的参数.

</td></tr>
<tr><td colspan="2">

![](../Images/2023.01.05_VALL-E_Fig.03.png)

<a id="Fig.03">Figure.03</a>: The structure of the conditional codec language modeling, which is built in a hierarchical
manner.
In practice, the NAR decoder will be called seven times to generate codes in seven quantizers.

</td><td>
</td></tr>
<tr><td>

In the AR model, we do not explicitly extract an audio clip as the prompt in training.
The training process is pure casual language model training.
In this way, any prefix sequence $c_{<t,1}$ is treated as a prompt for the latter part of the sequence $c_{\geq t,1}$.
During inference, given an enrolled recording, we should concatenate the phoneme sequence of the enrolled recording and the phoneme sequence for synthesis together.
Meanwhile, the acoustic token sequence of the enrolled recording is used as the prefix in AR decoding, as formulated in [Equation.01](#Eq.01).
We will study the superiority of this setting in the experiment.

</td><td>

在自回归模型中, 我们不显式提取声学提示作为训练中的提示.
训练过程是纯因果语言模型训练.
通过这种方式, 任意的前缀序列 $c_{<t,1}$ 都被视为后续序列 $c_{\geq t,1}$ 的提示.
在推理时, 给定输入录音, 我们应将输入录音的音素序列和待合成的音素序列拼接起来.
同时, 输入录音的声学 Token 序列在自回归解码中作为前缀, 正如[公式 (01)](#Eq.01).
我们将在实验中研究这种设置的优越性.

</td></tr></table>

#### Non-Autoregressive Codec Language Modeling

<table><tr><td width="50%">

When we obtain the first quantizer codes by the AR model, we employ a non-autoregressive (NAR) model to generate codes of the other seven quantizers.
The NAR model has a similar architecture to the AR model, except that it contains eight separate acoustic embedding layers.
In each training step, we randomly sample a training stage $i\in [2, 8]$.
The model is trained to maximize the acoustic tokens from the $i$-th quantizer codebook.
The acoustic tokens from stage $1$ to stage $i−1$ are embedded and summed up as model input:

$$
\begin{align}
  e_{c_{t,j}}&=W_a^j\odot c_{t,j}\tag{4}\\\mathbf{e_{c_t}}&=\sum_{j=1}^{i-1}e_{c_t,j}\tag{5}
\end{align}
$$

where $\odot$ indicates index selection.

</td><td>

当我们通过自回归模型获得第一个量化器编码, 我们采用非自回归模型来生成其他七个量化器的编码.
非自回归模型具有和自回归模型类似的架构, 除了它包含八个单独的声学嵌入层.
在每个训练步中, 我们随机采样一个训练阶段 $i\in [2, 8]$.
模型被训练为最大化来自第 $i$ 个量化器码本的声学 Token.
来自阶段 $1$ 到 $i-1$ 的声学 Token 被嵌入并相加作为模型输入:

$$
\begin{align}
  e_{c_{t,j}}&=W_a^j\odot c_{t,j}\tag{4}\\\mathbf{e_{c_t}}&=\sum_{j=1}^{i-1}e_{c_t,j}\tag{5}
\end{align}
$$

- 其中 $\odot$ 表示索引选择.

</td></tr>
<tr><td>

The phoneme sequence is also regarded as the prompt of the language model.
Besides, to clone the unique voice of the given speaker, we also use the acoustic tokens from the enrolled speech as the acoustic prompt.
Specifically, we first tokenize the enrolled speech with the neural codec model as $\tilde{C}^{T\times 8}$.
The embedded representations from all of the eight codebooks are summed up as the acoustic prompt $e_{\tilde{c}_t}=\sum_{j=1}^8 e_{\tilde{c}_{t,j}}$.
To predict the acoustic tokens from the i-th codebook, the Transformer input is the concatenation of $(\mathbf{e}_{\mathbf{x}}, \mathbf{e}_{\tilde{c}}, \mathbf{e}_{c_{:,<i}})$.
The positional embeddings are also computed separately for prompts and the acoustic sequence.
The current stage $i$ is injected into the network with **Adaptive Layer Normalization**[^44] operator, i.e., $\text{AdaLN}(h, i) = a_i\text{LayerNorm}(h) + b_i$, where $h$ is the intermediate activations, $a_i$ and $b_i$ are obtained from a linear projection of the stage embedding.
Unlike AR, the NAR model allows each token to attend to all the input tokens in the self-attention layer.
We also share the parameters of the acoustic embedding layer and the output prediction layer, which means the weights of the $j$-th prediction layer are the same as the $(j + 1)$-th acoustic embedding layer.

</td><td>

音素序列也被视为语言模型的提示.
除此之外, 为了克隆给定说话人的独特声音, 我们还使用来自输入语音的声学 Token 作为声学提示.
具体来说, 我们首先将输入语音用神经编解码器模型离散化为 $\tilde{C}^{T\times 8}$.
来自所有八个码本的嵌入表示被相加后作为声学提示 $e_{\tilde{c}_t}=\sum_{j=1}^8 e_{\tilde{c}_{t,j}}$.
为了预测来自第 $i$ 个码本的声学 Token, Transformer 输入是 $(\mathbf{e}_{\mathbf{x}}, \mathbf{e}_{\tilde{c}}, \mathbf{e}_{c_{:,<i}})$ 的拼接.
位置嵌入也为提示和声学序列分别计算.
当前阶段 $i$ 采用 **自适应层归一化**[^44] 操作插入网络中, 即 $\text{AdaLN}(h, i) = a_i\text{LayerNorm}(h) + b_i$, 其中 $h$ 是中间激活, $a_i$ 和 $b_i$ 是由阶段嵌入的线性投影获得.
和自回归不同, 非自回归模型允许自注意力层中每个 Token 注意到所有输入 Token.
我们也共享声学嵌入层和输出预测层的参数, 这意味着第 $j$ 个预测层的参数与 $(j+1)$ 个声学嵌入层的权重相同.

</td></tr></table>

### 4.3·Inference: In-Context Learning via Prompting

<table><tr><td width="50%">

In-context learning is a surprising ability of the text-based language model, which is able to predict labels for unseen inputs without additional parameter updates.
For TTS, if the model can synthesize high-quality speech for unseen speakers without fine-tuning, the model is believed to have in-context learning capability.
However, the in-context learning capability of existing TTS systems is not strong, because they either require additional fine-tuning or degrade dramatically for unseen speakers.

For language models, prompting is necessary to enable in-context learning in the zero-shot scenario.
We design prompts and inference as follows.
We first convert the text into a phoneme sequence and encode the enrolled recording into an acoustic matrix, forming the phoneme prompt and acoustic prompt.
Both prompts are used in the AR and NAR models.
For the AR model, we use sampling-based decoding conditioned on the prompts since we observe that beam search may lead the LM into an infinity loop.
Furthermore, the sampling-based method could significantly increase the diversity of the output.
For the NAR model, we use greedy decoding to choose the token with the highest probability.
Finally, we use the neural codec decoder to generate the waveform conditioned on the eight code sequences.

The acoustic prompt may or may not semantically relate to the speech to be synthesized, resulting in two cases:
- ***VALL-E***:
Our main interest is to generate given content for unseen speakers.
The model is given a text sentence, a segment of enrolled speech, and its corresponding transcription.
We prepend the transcription phoneme of the enrolled speech to the phoneme sequence of the given sentence as the phoneme prompt, and use the first layer acoustic token of the enrolled speech $\tilde{c}_{:,1}$ as an acoustic prefix.
With the phoneme prompt and the acoustic prefix, ***VALL-E*** generates the acoustic tokens for the given text cloning the voice of this speaker.
- **VALL-E-continual**:
In this setting, we use the whole transcription and the first 3 seconds of the utterance as the phoneme and acoustic prompts respectively, and ask the model to generate the continuations.
The inference process is the same as setting ***VALL-E***, except that the enrolled speech and the generated speech are semantically continuous.

</td><td>

上下文学习是基于文本的语言模型的一个令人惊讶的能力, 它能够对未见过的输入无需额外的参数更新就能预测标签.
对于 TTS, 如果模型能够为未见过的说话人生成高质量的语音而无需微调, 则模型被认为具有上下文学习能力.
然而, 现有的 TTS 系统的上下文学习能力并不强, 因为它们对于未见过的说话人要么需要额外的微调, 要么严重退化.

对于语言模型, 提示是零样本场景下实现上下文学习的必要条件.

我们设计提示和推理如下:
我们首先将文本转换为音素序列, 并将输入录音编码为声学矩阵, 形成音素提示和声学提示.
两个提示都被用于自回归和非自回归模型.

对于自回归模型, 我们使用基于采样的解码, 以提示为条件, 因为我们观察到束搜索可能导致语言模型陷入无限循环.
此外, 基于采样的方法可以显著增加输出的多样性.

对于非自回归模型, 我们使用贪婪解码来选择概率最高的 Token.
最后, 我们使用神经编解码器以八个码本序列为条件生成声学信号.

根据声学提示是否在语义上和合成语音相关, 分为两种情况:
- ***VALL-E***:
我们的主要兴趣是为未见过的说话人生成给定的内容.
模型被给定一个文本句子, 一个输入录音片段, 以及其对应的转写文本.
我们将输入语音的转写音素序列前置到给定句子的音素序列作为音素提示, 并使用输入语音的第一层声学 Token 作为声学前缀.
通过音素提示和声学前缀, ***VALL-E*** 生成给定文本的声学 Token, 克隆了此说话人的声音.
- **VALL-E-continual**:
在这种设置中, 我们使用整个转写文本和输入语音的前 3 秒作为音素和声学提示, 并要求模型生成续写.
推理过程与 ***VALL-E*** 设置相同, 除了输入语音和生成语音是语义连贯的.

</td></tr></table>

## 5·Experiment

### 5.1·Experiment Setup

#### Dataset

<table><tr><td width="50%">

We use **Libri-Light**[^15] as the training data which contains 60K hours of unlabelled speech from audiobooks in English.
The number of distinct speakers is around 7000 in LibriLight.
 We train a hybrid DNN-HMM ASR model on 960 hours labeled **LibriSpeech** following **Kaldi** recipe[^45].
Once the hybrid model is trained, unlabeled speech data is decoded and transduced to the best phoneme-level alignment paths where the frameshift is 30ms.
The **EnCodec** model [^14] is used to generate the acoustic code matrix for the 60K hours of data.

</td><td>

</td></tr></table>

#### Model

<table><tr><td width="50%">

Both the AR model and the NAR model have the same Transformer architecture with 12 layers, 16 attention heads, an embedding dimension of 1024, a feed-forward layer dimension of 4096, and a dropout of 0.1.
The average length of the waveform in LibriLight is 60 seconds.
During training, we randomly crop the waveform to a random length between 10 seconds and 20 seconds.
Its corresponding phoneme alignments are used as the phoneme prompt.
We remove the consecutive repetitions in the force-aligned phoneme sequence.
For the NAR acoustic prompt tokens, we select a random segment waveform of 3 seconds from the same utterance.

The models are trained using 16 NVIDIA TESLA V100 32GB GPUs with a batch size of 6k acoustic tokens per GPU for 800k steps.
We optimize the models with the AdamW optimizer, warm up the learning rate for the first 32k updates to a peak of 5 × 10−4, and then linear decay it.

</td><td>

</td></tr></table>

#### Baseline

<table><tr><td width="50%">

We choose the SOTA zero-shot TTS model **YourTTS**[^09] as the baseline, which is trained on a combined dataset of **VCTK**[^18], **LibriTTS**[^16], and **TTS-Portuguese**[^46].
We use their released checkpoint in [Github](https://github.com/Edresson/YourTTS).

</td><td>

</td></tr></table>

#### Automatic metrics

<table><tr><td width="50%">

We employ the SOTA speaker verification model, **WavLM-TDNN**[^31], to evaluate the speaker similarity between prompt (the decompressed enrolled speech) and synthesized speech.
WavLM-TDNN achieved the top rank at the VoxSRC Challenge 2021 and 2022 leaderboards.
It reached an average Equal Error Rate (EER) of 0.383, 0.480, and 0.986 on Vox1-O, Vox1-E, and Vox1-H respectively.
The similarity score predicted by WavLM-TDNN is in the range of [−1, 1], where a larger value indicates a higher similarity of input samples.

We also evaluate the synthesis robustness of our model.
Neural TTS systems suffer from the robustness issue, which sometimes has deletion, insertion, and replacement errors due to wrong attention alignments.
We perform ASR on the generated audio and calculate the word error rate (WER) with respect to the original transcriptions.
In this experiment, we employ the **HuBERT-Large**[^30] model fine-tuned on **LibriSpeech** 960h as the ASR model, which is a CTC-based model without language model fusion.

</td><td>

</td></tr></table>

#### Human evaluation

<table><tr><td width="50%">

We calculate the **Comparative Mean Option Score (CMOS)** and **Similarity Mean Option Score (SMOS)** by crowdsourcing, where 12 and 6 native speakers are invited as CMOS and SMOS contributors.
The scale of SMOS is from 1 to 5 with 0.5-point increments.
CMOS ranges from -3 (the new system is much worse than baseline) to 3 (the new system is much better than baseline) with intervals of 1.
CMOS is an indicator of speech naturalness, and SMOS measures whether the speech is similar to the original speaker’s voice.

</td><td>

</td></tr></table>

### 5.2·LibriSpeech Evaluation

<table><tr><td width="50%">

We first use **LibriSpeech**[^17] for zero-shot TTS evaluation, since there is no speaker overlap between LibriLight training data and **LibriSpeech** test-clean data.
Following **AudioLM**[^33], we use the samples from **LibriSpeech** test-clean with lengths between 4 and 10 seconds, resulting in a 2.2 hours subset.
For each sample synthesis, ***VALL-E*** randomly choose another utterance of the same speaker and crop a 3-seconds speech segment as the enrolled speech.
Each experiment runs three times and the average score is reported.
VALL-E-continual uses the first 3 seconds of the ground-truth speech as enrolled speech.

Table 2 shows the objective evaluation results.
We first compute the WER score and the speaker similarity score of the ground truth speech as the upper bound.
To compare the speaker similarity, we use speech pairs from the same speaker in the test set.
Compared with the YourTTS baseline, our model is significantly better in both robustness and speaker similarity, showing that our generated speech is highly faithful to the given text and the given enrolled speech.
Furthermore, the word error rate can be further reduced in ***VALL-E***-continual setting, because the acoustic tokens for the first 3 seconds are extracted from the ground truth.
We also compare the robustness with other speech-to-speech LM-based generation models, GSLM and AudioLM, which use audio latent codes as input.
GSLM uses HuBERT code as input and reconstructs the waveform with the **Tacotron2**[^01] model and the **WaveGlow**[^47] vocoder.
We run their open-sourced code using the released model and evaluate the results.
Since the HuBERT codes discard the speaker identity, it achieves a poor speaker score.
For the AudioLM, we list their WER score reported in their paper, which is obtained by a Conformer Transducer model.
The experiment results show that ***VALL-E*** is better than other speech-to-speech LM-based generative systems in terms of robustness.
One major reason is ***VALL-E*** trained with pseudo-phoneme instead of HuBERT/w2v-BERT codes, which enjoys better alignment quality with the input text.

We randomly sample one utterance for each speaker in **LibriSpeech** test-clean for the human evaluation, resulting in 40 test cases.
Table 3 shows the human evaluation results.
***VALL-E*** is very closed to ground truth in terms of SMOS, indicating the synthesized speech is similar to the given unseen speaker in testing.
It significantly outperforms the baseline with +0.93 SMOS, demonstrating the effectiveness of ***VALL-E*** in zero-shot scenarios.
Regarding naturalness, ***VALL-E*** beats the baseline with +0.12 CMOS, indicating the proposed method could synthesize more natural and realistic speech against baselines

</td><td>

</td></tr></table>

#### Ablation Study

<table><tr><td width="50%">

In this section, we perform detailed ablation experiments.
We first study the NAR model.
We train three NAR models with different numbers of prompts.
The settingNAR-no prompt is trained without any prompts.
The settingNAR-phn prompt is trained with only phoneme sequence as prompt and the setting NAR-2 prompts uses both phoneme prompt and acoustic token prompt as conditions.
In evaluation, we use the ground-truth first-level acoustic tokens as the model input and compute the WER and speaker similarity scores.
The results are listed in Table 4.
Results show that the model without any prompts performs poorly on both ASR and speaker similarity evaluations, even though the acoustic input token is ground truth.
When adding the phoneme prompt, the WER is reduced by a large margin from 19.6 to 3.0.
It shows the phoneme prompt mainly contributes to the content of the generation.
In the NAR-2 prompts, the model can learn speaker information from the acoustic token prompt and thus improve the speaker evaluation quality.

We further conduct the ablation experiments on the AR model.
In these experiments, we always use the NAR-2 prompts setting as the NAR model.
In Table 5, we can see that when we remove the acoustic prompt (w/o acoustic prompt), it can only obtain a speaker similarity score of 0.236, showing the prompt is extremely crucial for speaker identity.
Even if the NAR model could see the prompt, the prompt for the AR model also contributes a lot to speaker similarity.

</td><td>

</td></tr></table>

### 5.3·VCTK Evaluation

<table><tr><td width="50%">

We evaluate our model on **VCTK** consisting of 108 speakers, where none of the speakers are observed during training.
Since YourTTS has seen 97 speakers in **VCTK** as training, we evaluate YourTTS performance on the full 107 speakers and 11 unseen speakers, respectively.
For each speaker, we randomly selected three utterances of 3s/5s/10s as the prompts and the text of another utterance as the text prompt.

We first evaluate two models with the speaker verification metric as described before.
From Table 6, we can see that ***VALL-E*** outperforms the baseline even if the baseline has seen 97 speakers in training, indicating our model is able to synthesize speech with higher speaker similarity.
When we compare with the baseline in a fair setting (11 speakers), the performance gap becomes larger, especially when only 3s prompts are available.
By comparing different lengths of the prompt, we can see our model is able to generate more similar speech when the prompt becomes longer, which is consistent with our intuition.

We sample 60 speakers for human evaluation, one utterance for each, where 11 are unseen speakers, and 49 speakers have been seen for YourTTS.
***VALL-E*** do not see any of the 60 speakers.
During model synthesis, each speaker has a 3-second enrolled recording.
Table 7 shows a comparison of our method against baseline and ground truth.
The comparison of SMOS shows that ***VALL-E*** has better speaker similarity than the baseline, even if the baseline has seen some of the speakers in training.
The side-by-side CMOS evaluation shows that ***VALL-E*** is +0.23 over YourTTS, indicating a significantly better performance on speaking of naturalness.
Furthermore, ***VALL-E*** achieves +0.04 CMOS over ground-truth, demonstrating no statistically significant difference from human recordings on this dataset.
Compared to the evaluation results on **LibriSpeech**, ***VALL-E*** shows a better CMOS score in the comparison with ground truth, which is mainly because the average sentence length is shorter and some of the ground truth utterances also have noisy environments in **VCTK**.
In terms of speaker similarity, **VCTK** is more challenging as it contains speakers with various accents while the training data and **LibriSpeech** test data do not contain various accent speakers.

</td><td>

</td></tr></table>

### 5.4·Qualitative Analysis

#### Diversity

<table><tr><td width="50%">

Previous TTS systems have a strong one-one mapping between input text and output waveform, because mel spectrum generation is based on reconstruction for each step without randomness.
Since ***VALL-E*** uses the sampling-based method to generate discrete tokens, its output is diverse for the same input text due to the randomness in inference.
Given a sentence and an enrolled recording, we run the inference process twice and visualize its waveform in Figure 4.
In Figure 4(a), we observe the two samples have different lengths and phrase durations, where the first has a faster speech rate.
In Figure 4(b), we observe that the accents of the two samples are different.
The second output emphasizes the word “must" with a larger amplitude whereas the first output does not.
We leave more samples on our demo page.

The diversity is important for some downstream scenarios.
For example, speech recognition always benefits from diverse inputs with different speakers and acoustic environments, which cannot be met by the previous TTS system.
Considering the diversity feature of ***VALL-E***, it is an ideal candidate to generate pseudo-data for speech recognition.

</td><td>

</td></tr></table>

#### Acoustic environment maintenance:

<table><tr><td width="50%">

Another interesting finding is the acoustic environment consistency between the acoustic prompt and the generation.
When the acoustic prompt has reverberation, ***VALL-E*** could synthesize speech with reverberation as well, whereas the baseline outputs clean speech.
Our explanation is that ***VALL-E*** is trained on a large-scale dataset consisting of more acoustic conditions than the data used by the baseline, so ***VALL-E*** could learn the acoustic consistency instead of a clean environment only during training.
We show consistency on our demo page.

</td><td>

</td></tr></table>

#### Speaker’s emotion maintenance

<table><tr><td width="50%">

Emotional TTS is a classic subtopic of speech synthesis, which synthesizes speech with a required emotion.
Traditional methods[^48] always train a model on a supervised emotional TTS dataset, where the speech corresponds to a transcription and an emotion label.
We find that ***VALL-E*** can preserve the emotion in the prompt at a zero-shot setting.
We select acoustic prompts from **EmoV-DB**[^49], a dataset containing speech with five emotions, ***VALL-E*** is able to keep the same emotion of the prompt in speech synthesis, even if the model is not fine-tuned on an emotional TTS dataset.
We put audio samples on our demo page.

</td><td>

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

We introduced ***VALL-E***, a language model approach for TTS with audio codec codes as intermediate representations.
We pre-train ***VALL-E*** with 60K hours of speech data, and show the in-context learning capability in zero-shot scenarios.
We achieve new state-of-the-art zero-shot TTS results on **LibriSpeech** and **VCTK**.
Furthermore, ***VALL-E*** could keep the acoustic environment and speaker’s emotion in synthesis, and provide diverse outputs in different sampling-based decoding processes.

Despite making significant progress, ***VALL-E*** still suffers from several issues.

</td><td>

</td></tr>
<tr><td>

**Synthesis Robustness**

We observe that some words may be unclear, missed, or duplicated in speech synthesis.
It is mainly because the phoneme-to-acoustic language part is an autoregressive model, in which disordered attention alignments exist and no constraints to solving the issue.
The phenomenon is also observed in vanilla Transformer-based TTS, which was addressed by applying non-autoregressive models or modifying the attention mechanism in modeling.
In the future, we would like to leverage these techniques to solve the issue.

</td><td>

</td></tr>
<tr><td>

**Data Coverage**

Even if we use 60K hours of data for training, it still cannot cover everyone’s voice,especially accent speakers.
The worse result on **VCTK** than **LibriSpeech** also implies insufficient coverage of accent speakers.
Moreover, the diversity of speaking styles is not enough, as LibriLight is an audiobook dataset, in which most utterances are in reading style.
In the future, we will further scale up the training data to improve the model performance across prosody, speaking style, and speaker similarity perspectives.
We believe the zero-shot TTS task could be almost solved through our approach with model and data scale-up.

</td><td>

</td></tr>
<tr><td>

**Model Structure**

Now, we use two models to predict codes of different quantizers.
A promising direction is to predict them with a large universal model.
Another interesting direction is using full NAR models to speed up model inference in the framework.

</td><td>

</td></tr>
<tr><td>

**Broader impacts**

Since ***VALL-E*** could synthesize speech that maintains speaker identity, it may carry potential risks in misuse of the model, such as spoofing voice identification or impersonating a specific speaker.
To mitigate such risks, it is possible to build a detection model to discriminate whether an audio clip was synthesized by ***VALL-E***.
We will also put Microsoft AI Principles∗into practice when further developing the models.

</td><td>

</td></tr></table>

## References: 参考文献

[^01]: [Tacotron2](../../Acoustic/2017.12.16_Tacotron2.md)
[^02]: [FastSpeech](../../Acoustic/2019.05.22_FastSpeech.md)
[^03]: [Transformer TTS](../../Acoustic/2018.09.19_TransformerTTS.md)
[^04]: [DelightfulTTS2](../../TTS2_Acoustic/2022.07.11_DelightfulTTS2.md)
[^05]: [VITS](../../E2E/2021.06.11_VITS.md)
[^06]: [SEA](../../TTS2_Acoustic/2018.09.27_SEA.md)
[^07]: [Spoken Content and Voice Factorization for Few-Shot Speaker Adaptation.](../../_Full/Spoken_Content_and_Voice_Factorization_for_Few-Shot_Speaker_Adaptation.md)
[^08]: [Neural Voice Cloning with a Few Samples.](../../_Full/2018.02.14_Neural_Voice_Cloning_with_a_Few_Samples.md)
[^09]: [YourTTS](../../E2E/2021.12.04_YourTTS.md)
[^10]: [GPT-3](../../TextLM/2020.05.28_GPT-3.md)
[^11]: [PaLM](../../TextLM/2022.04.05_PaLM.md)
[^12]: [BERT](../../TextLM/2018.10.11_BERT.md)
[^13]: [RoBERTa (2019)](../../TextLM/2019.07.26_RoBERTa.md)
[^14]: [EnCodec (2022)](../../SpeechCodec/2022.10.24_EnCodec.md)
[^15]: [Libri-Light (2019)](../../../Datasets/2019.12.17_Libri-Light.md)
[^16]: [LibriTTS (2019)](../../../Datasets/2019.04.05_LibriTTS.md)
[^17]: [LibriSpeech (2015)](../../../Datasets/2015.04.19_LibriSpeech.md)
[^18]: [VCTK (2016)](../../../Datasets/2012.08.00_VCTK.md)
[^19]: [AdaSpeech (2021)](../../Acoustic/2021.03.01_AdaSpeech.md)
[^20]: [Meta-TTS (2021)](../../TTS2_Acoustic/2021.11.07_Meta-TTS.md)
[^21]: [Transfer Learning from Speaker Verification to Multispeaker Text-to-Speech Synthesis.](../../_Full/2018.06.12_Transfer_Learning_from_Speaker_Verification_to_Multispeaker_Text-To-Speech_Synthesis.md)
[^22]: [Exploring the encoding layer and loss function in end-to-end speaker and language recognition system.](../../_Full/2018.04.14_Exploring_the_Encoding_Layer_and_Loss_Function_in_End-to-End_Speaker_and_Language_Recognition_System.md)
[^23]: [**Survey by Tan et al.(2021)**](../../../Surveys/2021.06.29__Survey__A_Survey_on_Neural_Speech_Synthesis_(63P).md)
[^24]: [AdaSpeech4 (2022)](../../Acoustic/2022.04.01_AdaSpeech4.md)
[^25]: [Grad-TTS (2021)](../../Acoustic/2021.05.13_Grad-TTS.md)
[^26]: [Guided-TTS (2021)](../../TTS2_Acoustic/2021.11.23_Guided-TTS.md)
[^27]: [Grad-StyleSpeech (2022)](../../TTS2_Acoustic/2022.11.17_Grad-StyleSpeech.md)
[^28]: [VQTTS (2022)](../../E2E/2022.04.02_VQTTS.md)
[^29]: [Wav2Vec2.0 (2020)](../../SpeechRepresentation/2020.06.20_Wav2Vec2.0.md)
[^30]: [HuBERT (2021)](../../SpeechRepresentation/2021.06.14_HuBERT.md)
[^31]: [WavLM (2021)](../../SpeechRepresentation/2021.10.26_WavLM.md)
[^32]: [GSLM (2021)](../S2S/2021.02.01_GSLM.md)
[^33]: [AudioLM](../ST2S/2022.09.07_AudioLM.md)
[^34]: [Speech Resynthesis from Discrete Disentangled Self-Supervised Representations.](../../SpeechCodec/2021.04.01_Speech_Resynthesis_from_Discrete_Disentangled_Self-Supervised_Representations.md)
[^35]: [SoundStream (2022)](../../SpeechCodec/2021.07.07_SoundStream.md)
[^36]: [HiFi-GAN (2020)](../../Vocoder/2020.10.12_HiFi-GAN.md)
[^37]: [Semi-Supervised Training for Improving Data Efficiency in End-to-End Speech Synthesis.](../../_Full/2018.08.30_Semi-Supervised_Training_for_Improving_Data_Efficiency_in_End-to-End_Speech_Synthesis.md)
[^38]: [SpeechT5 (2021)](../ST2ST/2021.10.14_SpeechT5.md)
[^39]: [VQVAE Unsupervised Unit Discovery and Multi-scale Code2Spec Inverter for Zerospeech Challenge 2019.](../../_Full/2019.05.27_VQVAE_Unsupervised_Unit_Discovery_and_Multi-scale_Code2Spec_Inverter_for_Zerospeech_Challenge_2019.md)
[^40]: [VQ-VAE (2017)](../../../Modules/VQ/2017.11.02_VQ-VAE.md)
[^41]: [A^3T (2022)](../../SpeechRepresentation/2022.03.18_A^3T.md)
[^42]: [WaveNet](../../Vocoder/2016.09.12_WaveNet.md)
[^43]: [VQ-Wav2Vec](../../SpeechRepresentation/2019.10.12_VQ-Wav2Vec.md)
[^44]: [Adaptive Layer Normalization](../../../Modules/Normalization/2019.11.16_AdaNorm.md)
[^45]: [Kaldi](../../Toolkits/2011.00.00_Kaidi.md)
[^46]: [TTS-Portuguese](../../../Datasets/2020.05.11_TTS-Portuguese.md)
[^47]: [WaveGlow](../../Vocoder/2018.10.31_WaveGlow.md)
[^48]: [Fine-Grained Emotion Strength Transfer, Control and Prediction for Emotional Speech Synthesis.](../../_Full/2020.11.17_Fine-grained_Emotion_Strength_Transfer_Control_and_Prediction_for_Emotional_Speech_Synthesis.md)
[^49]: [Emov-DB](../../../Datasets/2018.06.25_EmoV-DB.md)
