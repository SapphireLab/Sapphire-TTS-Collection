# Google USM

<details>
<summary>基本信息</summary>

- 标题: "Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages"
- 作者:
  - 01 Yu Zhang
  - 02 Wei Han
  - 03 James Qin
  - 04 Yongqiang Wang
  - 05 Ankur Bapna
  - 06 Zhehuai Chen
  - 07 Nanxin Chen
  - 08 Bo Li
  - 09 Vera Axelrod
  - 10 Gary Wang
  - 11 Zhong Meng
  - 12 Ke Hu
  - 13 Andrew Rosenberg
  - 14 Rohit Prabhavalkar
  - 15 Daniel S. Park
  - 16 Parisa Haghani
  - 17 Jason Riesa
  - 18 Ginger Perng
  - 19 Hagen Soltau
  - 20 Trevor Strohman
  - 21 Bhuvana Ramabhadran
  - 22 Tara Sainath
  - 23 Pedro Moreno
  - 24 Chung-Cheng Chiu
  - 25 Johan Schalkwyk
  - 26 Françoise Beaufays
  - 27 Yonghui Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2303.01037)
  - [Publication]
  - [Github]
  - [Demo]
- 文件:
  - [ArXiv](_pdf/2303.01037v3__Google_USM__Scaling_Automatic_Speech_Recognition_Beyond_100_Languages.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

We introduce the ***Universal Speech Model (USM)***, a single large model that performs automatic speech recognition (ASR) across 100+ languages.
This is achieved by pre-training the encoder of the model on a large unlabeled multilingual dataset of 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller labeled dataset.
We use multilingual pre-training with random-projection quantization and speech-text modality matching to achieve state-of-the-art performance on downstream multilingual ASR and speech-to-text translation tasks.
We also demonstrate that despite using a labeled training set 1/7-th the size of that used for the Whisper model, our model exhibits comparable or better performance on both in-domain and out-of-domain speech recognition tasks across many languages.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论