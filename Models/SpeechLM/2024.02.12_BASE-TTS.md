# BASE TTS

<details>
<summary>基本信息</summary>

- 标题: "BASE TTS: Lessons from Building a Billion-Parameter Text-to-Speech Model on 100K Hours of Data"
- 作者:
  - 01 Mateusz Łajszczak,
  - 02 Guillermo Cámbara,
  - 03 Yang Li,
  - 04 Fatih Beyhan,
  - 05 Arent van Korlaar,
  - 06 Fan Yang,
  - 07 Arnaud Joly,
  - 08 Álvaro Martín-Cortinas,
  - 09 Ammar Abbas,
  - 10 Adam Michalski,
  - 11 Alexis Moinet,
  - 12 Sri Karlapati,
  - 13 Ewa Muszyńska,
  - 14 Haohan Guo,
  - 15 Bartosz Putrycz,
  - 16 Soledad López Gambino,
  - 17 Kayeon Yoo,
  - 18 Elena Sokolova,
  - 19 Thomas Drugman
- 链接:
  - [ArXiv](https://arxiv.org/abs/2402.08093)
  - [Publication]()
  - [Github]()
  - [Demo](https://amazon-ltts-paper.com/)
- 文件:
  - [ArXiv](_PDF/2402.08093v2__BASE_TTS__Lessons_from_Building_a_Billion-Parameter_Text-to-Speech_Model_on_100K_Hours_of_Data.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

We introduce a text-to-speech (TTS) model called ***BASE TTS***, which stands for ***Big Adaptive Streamable TTS with Emergent abilities***.
***BASE TTS*** is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness.
It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes ("speechcodes") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner.
Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding.
Echoing the widely-reported "emergent abilities" of large language models when trained on increasing volume of data, we show that ***BASE TTS*** variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences.
We design and share a specialized dataset to measure these emergent abilities for text-to-speech.
We showcase state-of-the-art naturalness of ***BASE TTS*** by evaluating against baselines that include publicly available large-scale text-to-speech systems: YourTTS, Bark and TortoiseTTS.
Audio samples generated by the model can be heard at https://amazon-ltts-paper.com/.

## 1.Introduction: 引言

Generative deep learning models are progressing at a rapid pace.
Natural Language Processing (NLP) and Computer Vision (CV) are undergoing a fundamental shift from specialized models with supervised training, to generalized models that can achieve miscellaneous tasks with limited explicit instruction \cite{sparks}.
In NLP, tasks such as question answering, sentiment analysis and text summarization can now be performed by a large language model (LLM) which was not specifically targeted for these tasks \cite{brown2020language,openai2023gpt4,scao2022bloom,touvron2023llama,hoffmann2022training}.
In CV, pre-trained models that learn from hundreds of millions of image-caption pairs have achieved top performance on image-to-text benchmarks \cite{clip1,tejankar2021fistful,shen2022k}, while delivering remarkably photo-realistic results in text-to-image tasks \cite{rombach2022high, ramesh2022hierarchical, nichol2022glide, saharia2022photorealistic, yu2022scaling}.
This progress has been enabled by Transformer-based architectures \cite{originaltransformerpaper} that drive improvements using many orders of magnitude more data than previous models.
Similar advances are now occurring in Speech Processing and Text-to-Speech (TTS), with models leveraging thousands of hours of data that push synthesis ever closer towards human-like speech.
Some of these models, described in depth in Section \ref{sec:related_work}, rely on causal language modeling tasks, like [AudioLM [16]](../SpeechLM/2022.09.07_AudioLM.md) or [VALL-E [17]](../SpeechLM/2023.01.05_VALL-E.md), whereas others use non-causal modules, such as [SoundStrom [18]](../SpeechLM/2023.05.16_SoundStorm.md) or [SpeechX [19]](../SpeechLM/2023.08.14_SpeechX.md), or diffusion decoders ([NaturalSpeech2 [20]](../../Models/Diffusion/2023.04.18_NaturalSpeech2.md); [TortoiseTTS [21]](../Diffusion/2023.05.12_TorToise-TTS.md)).

Until 2022, leading Neural TTS models were almost exclusively trained on a few hundreds of hours of recorded audio ([Tacotron2 [22]](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md); ecat, [VITS [24]](../../Models/E2E/2021.06.11_VITS.md); DBLP:conf/iclr/0006H0QZZL21; [Grad-TTS [26]](../../Models/TTS2_Acoustic/2021.05.13_Grad-TTS.md)).
Such systems can create well-enunciated speech that is occasionally expressive for the target speakers, but typically cannot generalize beyond the small amount of training data to render ambiguous and complex texts with truly expressive spoken performance \cite{triantafyllopoulos2023overview,tahon2018can,schnell2022controllability}.
To achieve such higher levels of expressiveness, TTS systems historically had to rely on labeled datasets for specific speech phenomena; and even so, achieving human-like prosody for certain types of textual inputs has remained elusive \cite{tan2021survey}.
For example, in English, compound nouns and questions are notoriously hard to render correctly without accurate syntactic parsing and semantic understanding \cite{kenter2020improvingprosodywithbert}.

In this paper, we introduce ***BASE TTS***: Big Adaptive Streamable TTS with Emergent abilities.
It is a multi-lingual and multi-speaker Large TTS (LTTS) system trained on around 100K (doubling previous high in [VALL-E [17]](../SpeechLM/2023.01.05_VALL-E.md)) hours of public domain speech data.
***BASE TTS*** follows the approach of casting TTS as a next-token-prediction problem ([AudioLM [16]](../SpeechLM/2022.09.07_AudioLM.md); [VALL-E [17]](../SpeechLM/2023.01.05_VALL-E.md); [TortoiseTTS [21]](../Diffusion/2023.05.12_TorToise-TTS.md)), inspired by the success of LLMs.
This approach is usually applied in combination with large amount of training data to achieve strong multi-lingual and multi-speaker capabilities (e.g.
one-shot voice cloning).
Our goal is to improve general TTS quality and study how scaling affects the model's ability to produce appropriate prosody and expression for challenging text inputs, similar to how LLMs acquire new abilities through data and parameter scaling, a phenomenon known as "emergence" or "emergent abilities" in the LLM literature \cite{wei2022emergent, webb2023emergent}. \cite{wei2022emergent} defines \textit{emergent abilities of large language models} as "abilities that are not present in smaller-scale models but are present in large-scale models;" for example, they show that on a range of few-shot tasks, model capability stays at a low level from 10\textsuperscript{18} to 10\textsuperscript{22} training FLOPs, but makes a drastic jump from 10\textsuperscript{22} to 10\textsuperscript{24}.
To test the hypothesis that this also holds for LTTS, we propose an evaluation scheme to assess potential emergent abilities in TTS, identifying seven categories that are challenging from the literature \cite{triantafyllopoulos2023overview,tahon2018can,schnell2022controllability,tan2021survey,kenter2020improvingprosodywithbert}: compound nouns, emotions, foreign words, paralinguistics, punctuations, questions, and syntactic complexities.
See more details in Section \ref{sec:evaluation}.

We design ***BASE TTS*** to model a joint distribution of text tokens followed by discrete speech representations, which we refer to as speechcodes.
Discretization of speech through audio codecs ([VQ-VAE [34]](../../Modules/VQ/2017.11.02_VQ-VAE.md); [SoundStream [35]](../../Models/Speech_Neural_Codec/2021.07.07_SoundStream.md); [EnCodec [36]](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md)) is central to our design, as it enables the direct application of methods developed for LLMs, which underlie recent works on LTTS ([AudioLM [16]](../SpeechLM/2022.09.07_AudioLM.md); [VALL-E [17]](../SpeechLM/2023.01.05_VALL-E.md); [SoundStrom [18]](../SpeechLM/2023.05.16_SoundStorm.md); [SpeechX [19]](../SpeechLM/2023.08.14_SpeechX.md); [NaturalSpeech2 [20]](../../Models/Diffusion/2023.04.18_NaturalSpeech2.md); [TortoiseTTS [21]](../Diffusion/2023.05.12_TorToise-TTS.md); kharitonov2023speak).
Specifically, we model speechcodes using a decoder-only autoregressive Transformer with a cross-entropy training objective.
Despite its simplicity, this objective can capture complex probability distributions of expressive speech, thus alleviating the oversmoothing problem seen in early neural TTS systems \cite{ren2022revisiting}.
As an implicit language model, ***BASE TTS*** is also observed to make a qualitative jump in prosody rendering once a large enough variant is trained on sufficient data.

Further, we propose speaker-disentangled speechcodes that are built on top of a [WavLM [39]](../../Models/Speech_Representaion/2021.10.26_WavLM.md) Self-Supervised Learning (SSL) speech model.
We follow [AudioLM [16]](../SpeechLM/2022.09.07_AudioLM.md) which introduces semantic tokens constructed by discretizing activations of an SSL model.
We extend this approach to better control information captured by the speechcodes.
Our strategy is to limit the responsibilities of the autoregressive speechcode predictor ("speechGPT") to segmental contents, prosody, and duration, while designating a separate, speechcode-to-waveform decoder (called "speechcode decoder") with the reconstruction of speaker identity and recording conditions.
We show that this convolution-based speechcode decoder is compute-efficient and reduces the whole-system synthesis time by over 70\% compared to the baseline diffusion-based decoder.

Our main contributions are summarized as follows:

- We introduce ***BASE TTS***, which to our knowledge is the largest TTS model to date, featuring 1B parameters and trained on a dataset consisting of 100K hours of public domain speech data.
In subjective evaluations, ***BASE TTS*** performs better than publicly available LTTS baseline models.
- We demonstrate how scaling ***BASE TTS*** to larger dataset and model sizes improves its capability to render appropriate prosody for complex texts.
To this end, we develop and make available an "emergent abilities" testset that can serve as a subjective evaluation benchmark for text understanding and rendering of large-scale TTS models.
We report performance on different variants of ***BASE TTS*** over this benchmark, showing monotonic improvement in quality with increased dataset size and parameter count.
- We introduce novel discrete speech representations that are built on top of a WavLM SSL model, intended to capture only phonemic and prosodic information of the speech signal.
We demonstrate that these representations outperform the baseline quantization method.
We also show that they can be decoded to high quality waveforms with a simple, fast, and streamable decoder, despite high level of compression (only 400 bits/s).

## 2.Related Works: 相关工作

### Text-to-Speech as Language Modeling

Casting TTS problem as next token prediction has gained popularity in recent years due to how easy it is to scale language models to large data and model sizes.

The first model using this approach was [TortoiseTTS [21]](../Diffusion/2023.05.12_TorToise-TTS.md) released in 2022.
It combines a GPT2-style speechcode language model with a diffusion decoder and an off-the-shelf vocoder, achieving remarkable few-shot capability.
[VALL-E [17]](../SpeechLM/2023.01.05_VALL-E.md) followed a similar approach: the model is scaled to 60k hours of speech data and uses a pre-trained audio encoder EnCodec for speechcode extraction ([EnCodec [36]](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md)).
[VALL-E X [61]](../SpeechLM/2023.03.07_VALL-E_X.md) replicated VALL-E with 70k hours, with an additional cross-lingual speech-to-speech translation task in English and Chinese by using target language token as prompt.
VioLa \cite{wang2023viola} extended VALL-EX with both text-to-text translation and speech-to-text recognition tasks.
[SpeechX [19]](../SpeechLM/2023.08.14_SpeechX.md) extended VALL-E by adding noise suppression, target speaker extraction, clean and noisy speech editing, and speech removal tasks.

While VALL-E X, VioLA and SpeechX are versatile models reporting improved performance on word error rate and speaker similarity, no effects on core TTS aspects such as naturalness or prosody improvement were reported.
VALL-E reported significant improvements over industry baselines, but we are unable to compare as the model is not publicly available.
Here, we propose a multi-lingual and multi-speaker TTS leveraging 100K hours of data, with strong proven naturalness results in English and Spanish.
Evaluations show improved speech naturalness compared to Tortoise-TTS.
Qualitative linguistic analysis shows "emergent abilities" \cite{wei2022emergent} - ***BASE TTS*** can render complex prosody patterns, taking cues from texts without explicit labels for emotions.

### Discrete Speech Representations

In audio generative AI, the first acoustic encoders used in GPT-TTS are [VQ-VAE [34]](../../Modules/VQ/2017.11.02_VQ-VAE.md), [SoundStream [35]](../../Models/Speech_Neural_Codec/2021.07.07_SoundStream.md) and [EnCodec [36]](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md), which are deep learning audio compression techniques producing discrete acoustic token based on vector quantization.
These acoustic encoders are shown to be superior to Mel-spectrograms \cite{DBLP:journals/corr/abs-2309-10922, yang2023towards}, but ignore semantic information and are unnecessarily complex due to lack of disentanglement ([RepCodec [65]](../Speech_Neural_Codec/2023.08.31_RepCodec.md)).
[AudioLM [16]](../SpeechLM/2022.09.07_AudioLM.md) combines SoundStream tokens with semantic BERT tokens \cite{chung2021w2v}.
[SpeechTokenizer [67]](../../Models/Speech_Neural_Codec/2023.08.31_SpeechTokenizer.md) and [RepCodec [65]](../Speech_Neural_Codec/2023.08.31_RepCodec.md) aim to capture semantics by adding self-supervised embedding prediction-based losses \cite{mohamed2022self}.
***BASE TTS*** builds acoustic tokens by leveraging semantic information from self-supervised embeddings and disentangling speaker information from the acoustic tokens.
Then, it applies byte-pair encoding on these tokens to reduce memory requirement, allowing the model to train on longer sequences.

### LTTS Simplifies Modeling

Since [Tacotron2 [22]](../../Models/TTS2_Acoustic/2017.12.16_Tacotron2.md), a successful TTS paradigm has been to separate speech creation into three sub-systems: (1) a Frontend responsible for text normalization, grapheme-to-phoneme conversion and SSML tag handling e.g.
to mark emphasis \cite{DBLP:journals/corr/abs-2307-07062}, (2) a Mel-spectrogram generator, inferring the amplitudes of the speech signal and responsible for the model expressivity, and (3) a dedicated Vocoder \cite{DBLP:conf/ssw/OordDZSVGKSK16, jiao2021universal}, generating the waveform by inferring the phase information.
Expert knowledge can be induced to improve its performance such as using pre-computed features \cite{DBLP:conf/iclr/0006H0QZZL21}, coarse or fine-grained prosody information (\cite{Copycat}, and style control \cite{DBLP:conf/naacl/PrateekLBDLMRW19}.
It has also proven successful with architectures such as Transformers \cite{li2019neural, DBLP:conf/iclr/0006H0QZZL21}, Flows （[VITS [24]](../../Models/E2E/2021.06.11_VITS.md)), and Diffusion ([Grad-TTS [26]](../../Models/TTS2_Acoustic/2021.05.13_Grad-TTS.md)).

These systems require a complex pipeline.
Any error made during an earlier stage is propagated to the next.
***BASE TTS*** and other LTTS models are breaking these limitations.
We simplify the data preparation by requiring only a large amount of audio, where texts can be obtained from a speech-to-text system, and require no phoneme extraction.
GPT-based architectures \cite{radford2019language} have flourished by enabling versatile prompt-based task formulation, integration of expert feedback \cite{ouyang2022training}, and use as a foundational model with quick fine-tuning \cite{hu2022lora, fu2023effectiveness} e.g.
on a specific task, a set of speakers or a new locale.
***BASE TTS*** shows that an end-to-end approach can achieve high expressivity on a few audio examples and in a multilingual setting, marking a high bar of quality in Spanish.

### Contextual and Emotional TTS

This usually requires separate, dedicated TTS sub-systems.
Multiple approaches rely on predicting prosody representations using contextualized word embeddings \cite{copycat2, xin2023improving, wu2022self, DBLP:journals/corr/abs-2309-01576}.
While prosody predictors of these models usually generate appropriate prosody, they often ignore strong text cues that would force a dramatic change e.g.
in emotions or speech cues like shouting or whispering.
Context-aware emotional TTS systems \cite{pan2021chapter, liu2023emotion, emotts, mukherjee2022text, schnell2021improving} are even more limited.
They usually favor a text-based emotion predictor coupled with an emotion controlable TTS system.
These systems require high-quality recordings with a forced speaking style and annotated audio and text data, limiting their usefulness due to the sheer number of emotions expressible in human speech \cite{cambria2012hourglass}.
***BASE TTS*** benefits from being a language model which is both acoustic and semantically/syntactically aware.
In this paper, we systemize an approach to produce and evaluate emergent contextual understanding of the text with a wide range of styles, without requiring supervised training or annotation.

### ***BASE TTS*** has Data Efficiency Built-In

Low-resource TTS has focused on reducing the required amount of high quality training data through e.g. voice conversion [huybrechts2021low [88]](), data generation \cite{DBLP:conf/icassp/LajszczakPKBBJN22}, modeling techniques \cite{DBLP:journals/corr/abs-2307-07062}, or recording script optimizations \cite{shamsi2020tts}.
This stems from the early difficulty of ingesting a high volume of data from multiple speakers in potentially different styles or languages.
A step up in that direction is zero-shot inference such as in [Bark](https://github.com/suno-ai/bark) and [YourTTS [60]](../../Models/E2E/2021.12.04_YourTTS.md), [AdaSpeech4 [91]](../../Models/TTS2_Acoustic/2022.04.01_AdaSpeech4.md), which aim to clone a voice with only a few seconds of recordings.
Due to leveraging a substantially larger dataset, a bigger model, and a dedicated speaker encoder, ***BASE TTS*** and the recent LTTS models set a new standard in data efficiency.

## 3.Methodology: 方法

### 3.1.Overview: 概览

Similar to recent works in speech modeling, we adopt an LLM-based method for the TTS task (\cref{fig:overview}).
We consider a dataset $\mathcal{D} = \{ \mathbf{x}_i, \mathbf{y}_i \}_{i=0}^N$, where $\mathbf{y}$ is an audio sample and $\mathbf{x} = \{x_1, \cdots, x_T \}$ is the corresponding text transcription.
The audio $\mathbf{y} = \{y_1, \cdots, y_S \}$ is represented by a sequence of $S$ discrete tokens (speechcodes), learnt using a separately trained speech tokenizer.
We use a Transformer-based autoregressive model with parameters $\phi$ in order to learn the joint probability of the text and audio sequences:

$$
    p(\mathbf{y}, \mathbf{x}) = p(\mathbf{y} | \mathbf{x}) p(\mathbf{x}) = \prod_{s=1}^S p(\mathbf{y}_s | \mathbf{y}_{<s}, \mathbf{x}; \phi) \prod_{t=1}^T p(\mathbf{x}_t | \mathbf{x}_{<t}; \phi).
$$

The predicted speech tokens are concatenated with speaker embeddings and decoded into waveforms using a separately trained speechcode decoder consisting of linear and convolutional layers.

### 3.2.Discrete Speech Representations: 离散语音表示

Discrete representation is foundational to the success of LLM, but identifying a compact and informative representation is less obvious in speech than in text, and less explored.
For ***BASE TTS***, we first experiment with a [Vector Quantized Variational Autoencoder (VQ-VAE) [34]](../../Modules/VQ/2017.11.02_VQ-VAE.md) baseline, where an auto-encoder based architecture reconstructs mel-spectrograms through a discrete bottleneck, as described in Section \ref{sec:vqvae}.
VQ-VAE has been a successful paradigm in speech and image representation, and especially as a unit of modeling for TTS ([VQ-VAE [34]](../../Modules/VQ/2017.11.02_VQ-VAE.md); [TortoiseTTS [21]](../Diffusion/2023.05.12_TorToise-TTS.md)).

In Section \ref{sec:wavlm}, however, we introduce a novel method of learning speech representations through WavLM-based speechcodes.
In this method, we discretize features extracted from a WavLM SSL model to reconstruct a mel-spectrogram.
We apply additional loss functions to promote speaker disentanglement, and compress the resulting speechcodes using [Byte-Pair Encoding (BPE)](../_Basis/BPE.md) to reduce sequence length, enabling us to model longer audio with Transformers.

Both representations are compressed (325 bits/s and 400 bits/s respectively) to allow more efficient autoregressive modeling compared to popular audio codecs (e.g. 6k bits/s in [VALL-E [17]](../SpeechLM/2023.01.05_VALL-E.md)).
With this level of compression, we aim to remove information from speechcodes that can be reconstructed during decoding (speaker, audio noise, etc.) to ensure that the capacity in speechcodes is primarily dedicated to encoding phonetic and prosodic information.

#### 3.2.1.Autoencoder-Based Speech Tokens

Our baseline discretization method is a VQ-VAE trained to reconstruct mel-spectrograms.
The encoder and decoder are convolutional neural networks with residual connections, which downsample the speech representation to a frequency of 25Hz.
To (partially) disentangle speaker information from the speech representations, we introduce a global reference encoder \cite{skerry2018towards}.
This encoder learns a fixed-size utterance-level representation, which is concatenated to the speechcodes before reconstructing with the VQ-VAE decoder.

From informal listening, we find that speechcodes produced by the autoencoder-based speech tokenizer still contain speaker information.
This motivates us to develop representations with improved speaker disentanglement.

#### 3.2.2.WavLM-Based Speechcodes

We aim to develop speechcodes that contain phonetic and prosodic information, but which are disentangled from speaker identity, recording conditions, and other spurious features in the audio signal.
To this end, we introduce a speech tokenizer based on features extracted from a pretrained [WavLM [39]](../../Models/Speech_Representaion/2021.10.26_WavLM.md) model, further trained with losses that encourage disentangling the speaker identity.
Our approach similar to the one introduced in \cite{martíncortinas2024enhancing} with modifications that reduce bitrate of the codes.
The overall architecture of the speech tokenizer is shown in \cref{fig:wavlm-based}.

We first pass the waveform through the WavLM model and extract the hidden states.
These hidden states are then passed through separate content and speaker linear regressors.
The output of these regressors is then fed into a convolutional residual encoder \cite{he2016deep}.
The content encodings are passed through a vector quantization module that outputs one speechcode per one WavLM frame (i.e. 20ms of speech).

The speaker encodings are passed through a Transformer-based speaker extractor \cite{originaltransformerpaper} to obtain the speaker embeddings.
The model only extracts, and we only use non-specific features that cannot be used for identification.

The speaker embeddings are concatenated with the speechcodes, and decoded into a spectrogram using a convolutional decoder.
We then compute L1 distance between decoded and target spectrograms and use it as the reconstruction loss.
While L1 is not the optimal reconstruction objective, we prioritize representations that are conducive for autoregressive modeling \cite{de2019hierarchical}, and demonstrate accordingly that the final audio quality can be kept high when this learned representation is decoded with our speechcode decoder, in Section \ref{sec:decoder}.
The speaker embeddings are used in a contrastive loss, maximizing the similarity between samples from the same speaker and minimizing it for those from different speakers \cite{oord2018representation}.
Furthermore, we maximize the cosine distance between the speaker embeddings and embeddings obtained by passing the output of the content regressor through the frozen speaker extractor and applying gradient reversal \cite{ganin2015unsupervised}.
We hypothesize that this encourages disentanglement between content and speaker information.

In addition to better disentanglement of speaker information, we also believe that using features from a pretrained WavLM model as input (as opposed to a jointly learnt audio encoding) keeps speechcodes more robust to recording conditions.
Our intuition is that WavLM was trained with data augmentation to encourage disentanglement from background noise.
The total loss is given by a weighted combination of these losses, in addition to the commitment loss for the vector quantizer:

$$
    L = L_\text{recon} + \alpha L_\text{commitment} + \beta L_\text{contrastive} + \gamma L_\text{cosine}
$$

#### 3.2.3.Byte-Pair Encoding on Speechcodes

The WavLM-based speech representations are learned at a frequency of 50 Hz to ensure that they offer enough resolution in time to be able to discriminate between single phonemes even in fast-paced speech (the average length of English phonemes tends not to dip below 20ms \cite{kuwabara1996acoustic}).
We apply [Byte-Pair Encoding ](../_Basis/BPE.md) to reduce the average sequence length of speechcodes by around 40\%, in order to mitigate the quadratic memory increases in Transformers sequence length and minimize complexity for the autoregressive model.
Similar to the BPE of texts, we iteratively join the most frequent pairs of speechcodes into new tokens and add them to the vocabulary until a pre-determined vocabulary size is reached.

### 3.3.Autoregressive Speech Modeling (SpeechGPT)

We train a GPT2-architecture autoregressive model \cite{radford2019language} that we call "SpeechGPT" to predict the speechcodes conditioned on text and reference speech.
The reference speech conditioning consists of a randomly selected utterance from the same speaker, which is encoded to a fixed-size embedding.
The reference speech embedding, text, and speechcodes are concatenated into a single sequence that is modeled by a Transformer-based autoregressive model.
We use separate positional embeddings and separate prediction heads for text and speech.

We train the autoregressive model from scratch, without pretraining on text (e.g.
as done in \cite{twist}).
In order to retain textual information to guide prosody, we also train SpeechGPT with an objective to predict the next token in the text portion of the input sequence, so that speechGPT is partially a text-only LM.
We apply a lower weight to this text loss compared to the speech loss.

### 3.4.Waveform Generation

Our baseline uses a diffusion-based spectrogram decoder and a separately trained UnivNet \cite{univnet} vocoder, as proposed in [TortoiseTTS [21]](../Diffusion/2023.05.12_TorToise-TTS.md).
Diffusion-based TTS decoding can generate high-quality speech, but it suffers from slow inference and cannot generate samples incrementally - This lack of streamabilility forces us to obtain the audio output for the entire sequence in one go.
Furthermore, the diffusion model in [TortoiseTTS [21]](../Diffusion/2023.05.12_TorToise-TTS.md) predicts spectrograms, and requires a separately trained vocoder to generate audio, complicating the training and inference pipeline.

Our proposed decoder, inspired by \cite{ecat}, is trained in an end-to-end fashion to predict waveforms.
Our variant uses speechcodes as an input to the model instead of phoneme encodings and prosody latents.
Additionally, to make the model more scalable, we replace LSTM layers \cite{hochreiter1997long} with convolutional ones to decode an intermediate representation.
The output of a HiFi-GAN based decoder block \cite{hifigan} is fed into BigVGAN vocoder \cite{bigvgan} to predict the waveform.
In training, we use the same set of adversarial and non-adversarial losses as \cite{ecat}.
We call our proposed system a "speechcode decoder" and depict it in \cref{fig:scv}.
In addition to simplifying the overall system, we hypothesize that training the decoder and vocoder end-to-end yields higher-quality speech.

In practice, instead of speechcodes, the speechcode decoder takes as an input the last hidden state of the autoregressive Transformer.
We do so because the dense latent representation provides much richer information than a single speechcode, following [TortoiseTTS [21]](../Diffusion/2023.05.12_TorToise-TTS.md).
During training, we feed the text and target codes to the trained SpeechGPT (with parameters frozen) and then condition the decoder on the last hidden state.
Feeding the last hidden states of SpeechGPT helps improving the segmental and acoustic quality of speech, but also couples the decoder to a specific version of SpeechGPT.
This complicates experimentation because it forces the two components to be always built sequentially.
This limitation needs to be addressed in future work.

## 4.Experiments: 实验

We design and conduct experiments to validate the architecture of ***BASE TTS*** and its quality, capabilities and compute performance.
First, we compare model quality achieved by autoencoder-based and WavLM-based speechcodes.
We then evaluate the two methods of acoustically decoding the speechcodes: a diffusion-based decoder and a speechcode decoder.
Having completed these architectural ablations, we assess the emergent abilities of ***BASE TTS*** in 3 variants of dataset sizes and model parameters, assessed by an expert linguist.
Further, we run subjective MUSHRA tests to measure naturalness, as well as automated intelligibility and speaker similarity measurements.
We also report the quality of speech comparison against other open-source text-to-speech models.

### 4.1.Dataset: 数据集

In order to test our hypothesis that abilities emerge with the scale of data, we set out to train with the largest speech dataset for our largest LTTS model.
We create a dataset consisting of 100K hours of unlabeled, public domain speech data.
The overall dataset is dominated by English data (over 90\%), followed by German, Dutch, Spanish.
We first download mp3 files from the Web, and resample them into 24 kHz mono LPCM files, with 16 bits per sample encoded as signed-integers.
The vast majority of the dataset is recorded in non-studio conditions and contain noises, and we avoid additional signal processing or denoising in order to test our model's ability to generate clean speech from noisy background data.

Next, we use an ASR model combined with Voice Activity Detection (VAD) to perform speech segmentation and Automatic Speech Recognition.
The ASR model splits the entire speech into 30-second or shorter fragments.
We then perform VAD to split sentences longer than 20 seconds, and reassemble them into longer sentences during training as long as their total length does not exceed 40 seconds.
We thus expose ***BASE TTS*** to speech segments which are between 0 and 40 seconds long, containing one or more sentences, so that the model becomes robust to both very short inputs, and learns from longer context \cite{9054106}.

To generate the transcript for the dataset, we initially relied on the ASR transcription.
We found that ASR tends to generate a smaller set of punctuations than their natural occurrences.
For example, it over-generates commas instead of colons and semicolons, and rarely generates parenthesis.
We then perform partial text restoration (similar to [Libriheavy [56]](../../Datasets/2023.09.15_Libriheavy.md)) by:
1) searching for source texts from the Internet and tying them to each recording;
2) matching the ASR transcript with the source text sentence by sentence, replacing the former with latter if their difference is small enough (length no longer/shorter by a factor of 3 and within a certain edit distance).
We restore around 1/2 of total texts to the `original`, resulting in an 300\% increase in quotation marks of all kinds and 20000\% increase in parentheses in the transcript.
This led to our models no longer having frequent acoustic artefacts on parentheses and other `rare' punctuations.

### 4.2.Training & Hyperparameters: 训练和超参数

We train the models in three steps.
Due to the use of discrete speech representations, we rely on the cross-entropy validation loss on $\sim$50 hours held-out from training data to guide our training, restarting, and stopping decisions.
All models are trained on clusters of AWS P3dn instances with NVIDIA® V100 GPUs.

First, we train two speech tokenizer variants: a VQ-VAE tokenizer as in Section \ref{sec:vqvae} and a WavLM tokenizer as in Section \ref{sec:wavlm}.
Due to the overwhelming presence of English data, we train both variants on a subset of the data in Section \ref{dataset} by keeping all data from non-English languages, but capping the amount of speech from individual English speakers at 200 hours - excess data from the speaker are randomly discarded.
We use a codebook size of 256 for the WavLM speechcodes and 8196 for the VQ-VAE ones.
The WavLM speechcodes are subsequently compressed with the BPE algorithm and a vocabulary of size 8192.
We first optimize the reconstruction loss on ground truth spectrograms, sanity-checking the reconstructed audio on "minority" languages such as Polish (0.2\% of total data) to ensure sufficient speechcode coverage.

Second, we train SpeechGPT on the entire training dataset with random initial weights.
The inputs to the model are: random reference speech segment from a speaker, text and corresponding target speech segments.
We convert the reference speech segment to a mel-spectrogram and feed it to the reference encoder.
The target speech segment is tokenized by either VQ-VAE or WavLM, forming the target speechcodes.
We concatenate the encoded reference with input text embeddings and speechcodes embedding, and feed them to the autoregressive Transformer.
The model is optimized with cross-entropy loss on both text tokens (weight 0.01) and speechcodes (weight 1.0).

Our autoregressive Transformer structure is largely identical to the GPT-2 language model \cite{radford2019language}.
For most of the experiments reported below ("BASE-large"), we train a 32-layer decoder-only transformer with masked self-attention heads (1536-dimension states and 24 attention heads).
For the feed-forward networks, we use 6144-dimension inner states.
We use Adam \cite{kingma2017adam} optimization with a max learning rate of 3.0e-4 and weight decay of 0.03.
The learning rate was increased linearly from zero over the first 10000 updates and annealed to 1.5e-4 using a cosine schedule.

To understand the importance of data and model size, we build two other versions (BASE-small \& BASE-medium) of SpeechGPT with WavLM speechcodes as described in Section \ref{sec:wavlm}, with increasing number of parameters and speech data in Table \ref{table:ablation-exp-table}.

Finally, we train the speechcodes decoder described in Section \ref{sec:decoder} with a modified log-likelihood-maximization GAN training scheme borrowed from \cite{ecat}.
Total parameter size for this module is around 150 million.

### 4.3.Evaluation Methodology: 评估方法

Three types of tests are considered to assess the model quality:

- **MUSHRA (Multiple Stimuli with Hidden Reference and Anchor)** among competing systems by native listeners in US English and Spanish.
We use a MUSHRA scale ranging from 0 to 100.
We do not place an Upper/Lower anchor or ask listeners to rate those at 0 or 100.
This way, we are able to get more ratings on target systems on a fixed evaluation budget.
For each MUSHRA test reported, 25-50 testers provide ratings on 50-100 text snippets that range between 5-40 seconds long, such that we achieve about 17-20 hours of total effective listening time per system per test.
- **Linguistic expert evaluation of "emergent abilities".**
To gauge the ability of ***BASE TTS*** to achieve finer understanding of the text, we hand-created an "emergent abilities testset" in English with 7 categories of texts: Questions, Emotions, Compound Nouns, Syntactic Complexities, Foreign Words, Paralinguistics, and Punctuations.
In Table \ref{table:emergent-abilities}, we present an example from each category, and how a linguistic expert rates the TTS output on a discrete 3-point scale.
These sentences are designed to contain challenging tasks - parsing garden-path sentences \cite{garden-path}, placing phrasal stress on long-winded compound nouns \cite{syntax-prosody}, producing emotional or whispered speech, or producing the correct phonemes for foreign words like "qi" or punctuations like "@" - none of which ***BASE TTS*** is explicitly trained to perform.
Our hypothesis is that as ***BASE TTS*** increases in model capacity and trains over more data, the model will start to acquire these abilities, following evidence that scaling in these dimensions begets qualitative ability jumps \cite{sparks, webb2023emergent, wei2022emergent}.
We share the full testset in Appendix \ref{appendix:emergent}.
- **Automated objective evaluations** to test TTS robustness, especially issues with missed, duplicated or hallucinated synthesis ([NaturalSpeech2 [20]](../../Models/Diffusion/2023.04.18_NaturalSpeech2.md)), and speaker similarity.
We use an ASR model to compute the Word Error Rate (WER) by comparing the testset text (ground truth) against the ASR output from the synthetic speech.
In addition, we employ a [speaker verification model](https://huggingface.co/microsoft/wavlm-base-plus-sv) fine-tuned on [WavLM [39]](../../Models/Speech_Representaion/2021.10.26_WavLM.md) features to obtain speaker embeddings from the original recordings and synthetic speech, and then we compute the cosine distance between them to get a speaker similarity metric (SIM).

For subjective evaluations, we report average MUSHRA scores with $95\%$ confidence intervals.
Furthermore, to determine the significance of differences between two systems, we conduct a t-test; if the p-value is $< 0.05$, we consider the difference significant.
To aid visualization we mark statistically significantly better systems with bold.

## 5.Results: 结果

### 5.1.VQ-VAE speechcode vs. WavLM speechcode

We conduct MUSHRA evaluations on 6 US English and 4 Spanish (Castilian and US) speakers in order to test comprehensively the quality and generalisability of the two speech tokenization approaches, using both held-out test data for speakers seen in the training ("seen" condition) and unseen speakers ("one-shot" condition).
In terms of average MUSHRA scores for English voices, VQ-VAE and WavLM based system are on par (VQ-VAE: 74.8 vs WavLM: 74.7, the difference is statistically non-significant).
However, for Spanish voices, WavLM based model outperforms the VQ-VAE one (VQ-VAE: 73.3 vs WavLM: 74.7) in a statistically significant way.
Note that English data comprises around 90\% of our dataset, while Spanish data only 2\%.
We hypothesize that speechcode improvements are more critical for low-resource languages while sheer data volume can make up for imperfect representations.
Further verification of this hypothesis needs to be addressed in future work.
Since WavLM-based system performs at least as well or better as the VQ-VAE baseline, we use it to represent ***BASE TTS*** in further experiments.
In Table \ref{VQVAE-WAVLM-TABLE}, we show the results categorized by speaker to provide additional details on per-speaker performance.

### 5.2.Diffusion-Based Decoder vs. Speechcode Decoder

***BASE TTS*** simplifies over the baseline diffusion-based decoder by proposing an end-to-end speechcode decoder, as described in Section \ref{sec:decoder}.
Our approach offers streamability and 3X improvement in inference speed.
In order to ensure that this approach does not degrade quality, we run an evaluation of the proposed speechcode decoder against the baseline.
Table \ref{DECODER-TABLE} presents the results of MUSHRA evaluation we conducted for 4 US English and 2 Spanish speakers.
For 4 voices out of 6, the ***BASE TTS*** variant with speechcode decoder outperforms the diffusion-based baseline in terms of average MUSHRA score.
For the remaining speakers the difference is not statistically significant.
We conclude that the speechcode decoder is the preferred approach, as it does not degrade quality and for most of the voices it brings quality improvements, while offering faster inference.
Our results suggest that combining two powerful generative models for speech modeling is redundant and can be simplified by dropping the diffusion decoder.

#### 5.3.Emergent abilities - Data and model size ablation

In this section, we report on our verification of the hypothesis that data and parameter scaling in LTTS brings qualitatively different results, analogous to training LLMs from 10\textsuperscript{22} to 10\textsuperscript{24} tokens, when LLMs "suddenly" start to master few-digit addition, recognise words in context at above chance level, and transcribe speech with expert phonetic alphabet.

We perform both MUSHRA and Linguistic expert judgement of "Emergent abilities," as described in \ref{sec:evaluation}, on 2 American English voices.
We report all scores by BASE-small, BASE-medium and BASE-large systems, as outlined in Table \ref{table:ablation-exp-table}.
In the MUSHRA results in Table \ref{ablation-table}, we observe that speech naturalness improves significantly from BASE-small to BASE-medium, but less so from BASE-medium to BASE-large - the difference is statistically significant only in Male speaker A.

We report the results of the linguistic expert judgement for the three systems, with their average score for each category, in Figure \ref{fig:jobInformationDialog}.
First, we see a universal jump from BASE-small to BASE-medium across categories.
BASE-small appears fundamentally unable to interpret emotions, paralinguistics, and foreign words (average score < 1.25, score lower bounded at 1), and never reaches an average score of 1.75 in any category.
By contrast, at BASE-medium, the model has mastered compound nouns and makes a significant jump in all categories; BASE-medium never performs below an average score of 1.75 in any category.
From BASE-medium to BASE-large, we observe continued but diminishing improvement in all categories except compound nouns, where the performance has saturated.
Combined with the findings from naturalness MUSHRA, we believe that scaling GPT-based TTS model from 1000+ to 10000+ hours and model size from 100 million to 500 million is the point at which "emergent abilities" \cite{wei2022emergent} start to occur for our TTS.

Examining results category by category, we observe that Emotions and Paralinguistics remain the most challenging tasks for all model variants, and even BASE-large performs at around an average score of 2.0, which indicates that model can recognise and react to relevant keywords, but the prosody quality remains imperfect to expert judgement.
By contrast, BASE-medium can perform close to ceiling on English Compound Nouns, while trailing BASE-large slightly in most of the other categories (Foreign Words, Punctuations, Questions, Syntactic Complexity).
We remain hopeful that further scaling and injection of textual knowledge from text-only LLM can help us close remaining performance gaps.

### 5.4.***BASE TTS*** vs. industry baselines

We select three industry baselines with publicly available pre-trained models: [YourTTS [60]](../../Models/E2E/2021.12.04_YourTTS.md), Bark\footnote{\url{https://github.com/suno-ai/bark}}, and [TortoiseTTS [21]](../Diffusion/2023.05.12_TorToise-TTS.md).
We conduct the comparison exclusively in one-shot setting, using 10-second reference clip for two US English speakers which ***BASE TTS*** was not previously trained or evaluated on.

As seen in Table \ref{competitor-mushra}, ***BASE TTS*** obtains significantly higher naturalness scores in MUSHRA than baseline systems, among two US English speakers tested.

We further conduct objective evaluation on WER and speaker similarity on 10 speakers and 307 samples for each, reported in Table \ref{table:objective_eval}.
Overall, ***BASE TTS*** produces the most natural speech, the least amount of misalignment with input text, and the most similar speech to the reference speaker, followed relatively closely by Tortoise on naturalness and WER.
Bark and YourTTS perform far worse on naturalness and WER, although Bark is relatively close to ***BASE TTS*** in speaker similarity.

### 5.5.Synthesis efficiency gain due to speechcode decoder

The speechcode decoder is capable of streaming, i.e.
generating speech incrementally.
Combining this feature with the autoregressive SpeechGPT gives our system a first-byte latency as low as 100ms - only a few decoded speechcodes are sufficient to produce intelligible speech.
This minimal latency contrasts with the diffusion-based decoder that requires the entire speech sequence (one or multiple sentences) to be generated in one go, where first-byte latency equals total generation time.
Further, we observe that the speechcode decoder makes the overall system 3X times more compute efficient compared to the diffusion baseline.
We run a benchmark where we generate 1000 utterances of around 20 seconds in duration on a NVIDIA® V100 GPU with batch size of 1.
On average, it takes 69.1 seconds for 1-billion-parameter SpeechGPT with the diffusion decoder to complete synthesis, but only 17.8s for the same SpeechGPT with the speechcode decoder.

## 6.Conclusions: 结论

We introduced ***BASE TTS***, a GPT-style TTS system using novel SSL-based speechcodes as an intermediate representation and a speechcode decoder that offers an efficient, streamable alternative to diffusion.
This is the largest model of its kind known to us, both in terms of parameters and training data.
We demonstrated new state-of-the-art TTS results against baselines including Tortoise, Bark and YourTTS.
We proposed a new way to measure textual understanding of TTS models, and showed that LTTS models built with 10K hours of data and 400 million parameters start to exhibit an advanced grasp of text that enables contextually appropriate prosody.
From ***BASE TTS***'s strong performance on English and Spanish, we caught a first glimpse of a multilingual TTS approach that achieves high expressiveness, adapts to textual clues, is data efficient, uses only public domain data, and works for streaming TTS usecases such as voicing LLM outputs.
Our approach points towards potential Scaling Laws \cite{scaling-law} of LTTS models, where an even larger amount of speech and other (text, image) data are needed to support multimodal objectives \cite{facebook-multimodal} and to break new grounds in TTS.

Our approach still contains some limitations:
a) ***BASE TTS*** occasionally produces hallucinations and cutoffs, where we produce either extra or incomplete audio than intended by the text.
This is an inherent problem with the autoregressive LM approach, made worse by the misalignment between audio data and the ASR-generated text;
b) Selecting the right discrete representation for GPT-style TTS is crucial.
More research is needed to establish how different properties of speechcodes translate into end-to-end system quality.
We only report results for one speechcode configuration and leave more comprehensive study for future work.
