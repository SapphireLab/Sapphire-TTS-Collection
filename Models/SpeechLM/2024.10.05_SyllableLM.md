# SyllableLM

<details>
<summary>基本信息</summary>

- 标题: "SyllableLM: Learning Coarse Semantic Units for Speech Language Models"
- 作者:
  - 01 Alan Baade,
  - 02 Puyuan Peng,
  - 03 David Harwath
- 链接:
  - [ArXiv](https://arxiv.org/abs/2410.04029)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2410.04029v1__SyllableLM__Learning_Coarse_Semantic_Units_for_Speech_Language_Models.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

## 1·Introduction: 引言

Language models require tokenized inputs.
However, tokenization strategies for continuous data like audio and vision are often based on simple heuristics such as fixed sized convolutions or discrete clustering, which do not necessarily align with the semantic structure of the data.
For speech in particular, the high resolution of waveforms (16,000 samples/second or more) presents a significant challenge as speech-based language models have had to use several times more tokens per word than text-based language models.
In this work, we introduce a controllable self-supervised technique to merge speech representations into coarser syllable-like units while still preserving semantic information.
We do this by 1) extracting noisy boundaries through analyzing correlations in pretrained encoder losses and 2) iteratively improving model representations with a novel distillation technique.
Our method produces controllable-rate semantic units at as low as 5Hz and 60bps and achieves SotA in syllabic segmentation and clustering.
Using these coarse tokens, we successfully train SyllableLM, a Speech Language Model (SpeechLM) that matches or outperforms current SotA SpeechLMs on a range of spoken language modeling tasks.
SyllableLM also achieves significant improvements in efficiency with a 30x reduction in training compute and a 4x wall-clock inference speedup.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论
