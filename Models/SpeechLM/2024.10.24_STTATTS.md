# STTATTS

<details>
<summary>基本信息</summary>

- 标题: "STTATTS: Unified Speech-To-Text And Text-To-Speech Model"
- 作者:
  - 01 Hawau Olamide Toyin
  - 02 Hao Li
  - 03 Hanan Aldarmaki
- 链接:
  - [ArXiv](https://arxiv.org/abs/2410.18607)
  - [Publication]() EMNLP 2024 Findings
  - [Github](https://github.com/mbzuai-nlp/sttatts)
  - [Demo]
- 文件:
  - [ArXiv](_PDF/2410.18607v1__STTATTS__Unified_Speech-To-Text_And_Text-To-Speech_Model.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Speech recognition and speech synthesis models are typically trained separately, each with its own set of learning objectives, training data, and model parameters, resulting in two distinct large networks.
We propose a parameter-efficient approach to learning ASR and TTS jointly via a multi-task learning objective and shared parameters.
Our evaluation demonstrates that the performance of our multi-task model is comparable to that of individually trained models while significantly saving computational and memory costs (∼50\% reduction in the total number of parameters required for the two tasks combined).
We experiment with English as a resource-rich language, and Arabic as a relatively low-resource language due to shortage of TTS data.
Our models are trained with publicly available data, and both the training code and model checkpoints are openly available for further research.

</details>
<br>

语音识别和语音合成模型通常是分别进行训练的, 各自拥有自己的学习目标, 训练数据和模型参数, 最终形成两个独立的大型网络.
我们提出了一种参数高效的方法, 通过多任务学习目标和共享参数来联合学习 ASR (自动语音识别) 和 TTS (文本到语音).
我们的评估显示, 我们的多任务模型性能与单独训练的模型相当, 同时显著节省了计算和内存成本 (大约减少了合并两个任务所需总参数数量的 50%).
我们分别使用英语作为资源丰富的语言以及阿拉伯语作为相对低资源的语言进行了实验, 后者由于缺少 TTS 数据而具有代表性.
我们的模型训练采用公开可用的数据, 并且训练代码和模型检查点都开放可用以供进一步研究.

## 1.Introduction: 引言

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论