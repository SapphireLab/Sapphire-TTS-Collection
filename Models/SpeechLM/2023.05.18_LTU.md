# LTU (Listen, Think, and Understand)

<details>
<summary>基本信息</summary>

- 标题: "Listen, Think, and Understand"
- 作者:
  - 01 Yuan Gong
  - 02 Hongyin Luo
  - 03 Alexander H. Liu
  - 04 Leonid Karlinsky,
  - 05 James Glass
- 链接:
  - [ArXiv](https://arxiv.org/abs/2305.10790)
  - [Publication](https://openreview.net/forum?id=nBZBPXdJlC)
  - [Github](https://github.com/YuanGongND/ltu)
  - [Demo](https://huggingface.co/spaces/yuangongfdu/ltu)
- 文件:
  - [ArXiv](_PDF/2305.10790v3__LTU__Listen_Think_and_Understand.pdf)
  - [Publication](_PDF/2305.10790p0__LTU__ICLR2024.pdf)

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications.
Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets.
In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any.
Such capabilities beyond perception are not yet present in existing audio models.
On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities.
Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability?
In this paper, we propose a new audio foundation model, called ***LTU (Listen, Think, and Understand)***.
To train ***LTU***, we created a new ***OpenAQA-5M*** dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum.
***LTU*** demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning.
More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models.
To the best of our knowledge, ***LTU*** is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.

</details>
<br>

人工智能系统在感知和理解音频信号的能力对于许多应用来说至关重要.
尽管这一领域自 AudioSet 发展以来已经取得了显著进展, 但现有的模型大多是将音频输入映射到预定义的离散的声音标签集合.
相比之下, 人类不仅能够将声音分类到一般类别中, 还能够聆听声音的更细微的细节, 解释预测的原因, 思考声音所按时的内容, 并理解场景和需要采取的动作, 如果有的话.
这些超越感知的能力在现有的音频模型中尚未出现.

另一方面, 现代大语言模型展示了新兴的推理能力, 但它们缺乏音频感知能力.
因此我们提出了一个问题: 能否构建一个同时具有音频感知和推理能力的模型?

在本文中, 我们提出了一个新的音频基础模型, 称为 ***LTU (Listen, Think, and Understand)***.
为了训练 ***LTU***, 我们创建了一个新的 ***OpenAQA-5M*** 数据集, 其中包含 1.9 百万个闭合问题和 3.7 百万个开放问题, 包含了不同的 (音频, 问题, 答案) 元组, 并使用了一个自回归训练框架, 并使用了从感知到理解的课程学习.
***LTU*** 在典型的音频任务, 如分类和字幕生成方面都表现出了强大的性能和泛化能力.
更重要的是, 它展示了突出的音频推理和理解能力, 这些能力在现有的音频模型中都缺乏.
至于我们所知, ***LTU*** 是首批专注于通用音频 (而非仅仅是语音) 理解的多模态大语言模型之一.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论