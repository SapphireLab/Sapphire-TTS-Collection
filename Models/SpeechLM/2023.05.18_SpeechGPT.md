# SpeechGPT

<details>
<summary>基本信息</summary>

- 标题: "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities"
- 作者:
  - 01 Dong Zhang
  - 02 Shimin Li
  - 03 Xin Zhang
  - 04 Jun Zhan
  - 05 Pengyu Wang
  - 06 Yaqian Zhou
  - 07 Xipeng Qiu (邱锡鹏)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2305.11000)
  - [Publication](https://doi.org/10.18653/v1/2023.findings-emnlp.1055)
  - [Github](https://github.com/0nutation/SpeechGPT)
  - [Demo](https://0nutation.github.io/SpeechGPT.github.io/)
- 文件:
  - [ArXiv](_PDF/2305.11000v2__SpeechGPT__Empowering_Large_Language_Models_with_Intrinsic_Cross-Modal_Conversational_Abilities.pdf)
  - [Publication](_PDF/2305.11000p0__SpeechGPT__EMNLP2023.pdf)

</details>

## Abstract: 摘要

Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT.
However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer.
In this paper, we propose ***SpeechGPT***, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-model content.
With discrete speech representations, we first construct SpeechInstruct, a large-scale cross-modal speech instruction dataset.
Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning.
The experimental results demonstrate that ***SpeechGPT*** has an impressive capacity to follow multi-modal human instructions and highlight the potential of handling multiple modalities with one model.
Demos are shown in [this https URL](https://0nutation.github.io/SpeechGPT.github.io/).

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论