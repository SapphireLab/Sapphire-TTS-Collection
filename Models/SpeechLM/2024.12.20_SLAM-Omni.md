# SLAM-Omni

<details>
<summary>基本信息</summary>

- 标题: "SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training"
- 作者:
  - 01 Wenxi Chen
  - 02 Ziyang Ma
  - 03 Ruiqi Yan
  - 04 Yuzhe Liang
  - 05 Xiquan Li
  - 06 Ruiyang Xu
  - 07 Zhikang Niu
  - 08 Yanqiao Zhu
  - 09 Yifan Yang
  - 10 Zhanxun Liu
  - 11 Kai Yu
  - 12 Yuxuan Hu
  - 13 Jinyu Li
  - 14 Yan Lu
  - 15 Shujie Liu
  - 16 Xie Chen
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.15649)
  - [Publication]()
  - [Github]()
  - [Demo](https://slam-omni.github.io)
- 文件:
  - [ArXiv](_PDF/2412.15649v1__SLAM-Omni__Timbre-Controllable_Voice_Interaction_System_with_Single-Stage_Training.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality.
In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training.
SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder.
By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference.
Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions.
Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data.
Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks.
Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.

## 1·Introduction: 引言

With the advent of large language models (LLMs), recent developments \citep{achiam2023gpt,dubey2024llama,yang2024qwen2} have showcased their powerful capabilities in textual conversation.
In spoken dialogue systems, however, traditional methods rely on a cascaded pipeline involving automatic speech recognition (ASR) to transcribe user input, LLMs to generate textual responses, and text-to-speech (TTS) models to produce audio outputs.
This design faces two major issues: (1) significantly increased interaction latency, and (2) reliance on text-based interaction, which overlooks rich non-verbal information in speech dialogue, such as emotions and prosody.
The release of GPT-4o~\cite {openai2024gpt4o} has underscored the potential of real-time spoken dialogue systems in delivering seamless interaction.
In response, several open-source frameworks, including Moshi \citep{defossez2024moshi}, Mini-Omni \citep{xie2024mini,xie2024mini2}, and LLaMA-Omni \citep{fang2024llama},
have been developed for effective end-to-end voice-based interaction.

Existing spoken dialogue models (SDMs) primarily model speech with discretized audio tokens.
Some approaches \citep{fang2024llama,wang2024freeze} rely on text embeddings to guide audio token generation, which limits their ability to generate critical audio paralinguistic attributes such as emotion and prosody.
Others \citep{zeng2024scaling,zhang2024omniflatten,nguyen2024spirit} adopt interleaved arrangements of audio and text tokens to restructure language modeling, while increasing training costs.
A third category \citep{xie2024mini,xie2024mini2,mitsui2024pslm} employs a parallel speech-text generation method, which aligns closely with ours, balancing the delivery of intrinsic audio attributes and consuming of computational burden.

A notable limitation of current SDMs is their disability to generate responses with diverse speaker timbres.
This restriction primarily stems from the uniform timbre of responses in most training datasets and the lack of explicit speaker modeling in existing frameworks.
To address this gap, we propose the first zero-shot timbre control solution for dialogue systems.
Drawing inspiration from zero-shot TTS \citep{wang2023neural}, our approach allows users to specify the desired output timbre by providing an audio prompt, paving the way for interactive applications such as personalized virtual assistants and customizable game character voices.

In this paper, we propose SLAM-Omni, a timbre-controllable, end-to-end spoken dialogue system with single-stage training.
For user speech input, the Whisper \citep{radford2023robust} encoder is employed to extract audio representations, which are then aligned with text embeddings via a projector and fed into the LLM.
On the output side, semantic audio tokens \citep{du2024cosyvoice} and text tokens are autoregressively predicted in parallel. These audio tokens naturally decouple speaker information into a separate vocoder, enabling zero-shot timbre control.
Inspired by VALL-E 2 \citep{chen2024vall}, SLAM-Omni predicts single-layer semantic tokens in grouped units per audio frame, reducing audio sequence length and accelerating training and inference.
For multi-round spoken dialogue modeling, we introduce historical text prompting, which leverages text-only history rather than alternating audio-text streams.
This strategy significantly compresses the dialogue history, improves data utilization, enables the model to handle more dialogue turns and enhances its instruction-following ability.
During inference, instruction text is extracted from encoded audio embeddings with a Whisper decoder and response text is directly obtained from the generated text stream, both of which provide low-cost speech transcription that enables efficient multi-round voice interactions.
Comprehensive evaluations demonstrate that ASR or TTS pre-training is not necessary, while our SLAM-Omni, with only 15 hours of single-stage training on 4 GPUs, greatly outperforms prior models of similar scale in both speech content, quality and speech-text alignment.

Our contributions are summarized below:
- We propose the first zero-shot \textit{timbre control solution} for voice interaction systems with speaker-decoupled semantic tokens.
- \textit{Semantic Group Modeling} approach is proposed for accelerating single-layer semantic speech token generation and model training.
- \textit{Historical Text Prompting} is proposed for efficient multi-round history modeling in \hspace{-1mm} SDMs.
- SLAM-Omni is the first voice assistant to achieve \textit{single-stage training}, requiring minimal data and computational resources.
-  Experiments show that SLAM-Omni outperforms prior models of similar scale on text-related tasks, and shows superior performance on acoustic quality and speech-text alignment among all existing SDMs.
Results on a larger dataset demonstrates its multilingual and multi-round dialogue capabilities.

## 2·Related Works: 相关工作

### 2.1·End-to-End Spoken Dialogue Modeling

Existing end-to-end SDMs primarily model voice interaction by treating text as either an intermediate output or a hidden state to leverage the pre-trained knowledge of LLMs.
As illustrated in Figure~\ref{fig:SDM-modeling}, these methods can be categorized into text-driven modeling and joint audio-text modeling.
For text-driven modeling, as shown in Figure~\ref{fig:SDM-modeling}a, existing methods \citep{fang2024llama,wang2024freeze} keep the original architecture of LLMs to retain textual abilities, using their hidden states as input to a speech decoder for audio generation.
This approach effectively preserves LLMs knowledge but struggles to capture rich audio paralinguistic attributes such as emotion and prosody, since only text tokens are used for autoregressive modeling.
Joint audio-text modeling, illustrated in Figure~\ref{fig:SDM-modeling}b and c, is further divided into interleaved and parallel paradigms. Both paradigms incorporate audio tokens into the autoregressive modeling, theoretically enhancing the ability to model non-verbal information.
In the interleaved paradigm, models \citep{zhang2024omniflatten,zeng2024scaling,nguyen2024spirit} alternate between text and audio tokens during generation.
This method typically requires extensive interleaved speech-text data and pre-training for re-modeling LLMs.
In contrast, the parallel paradigm, adopted by models like PSLM \citep{mitsui2024pslm}, Mini-Omni \citep{xie2024mini,xie2024mini2}, and our proposed SLAM-Omni, employs autoregressive modeling of text and audio tokens in parallel.
However, unlike PSLM and Mini-Omni, SLAM-Omni predicts single-layer grouped semantic tokens to accelerate audio generation process.
Combining semantic group modeling with single-stage training, we achieve an end-to-end SDM built on a pre-trained LLM that requires significantly less training costs compared to previous solutions.

### 2.2·Speech Tokenization

Speech tokenization is a foundational technique in speech language models (SLMs), typically categorized into acoustic tokens and semantic tokens \citep{zhang2023speechgpt,borsos2023audiolm}.
Acoustic tokens, derived from neural audio codecs \citep{defossez2022high,zeghidour2021soundstream} and optimized for reconstructing high-quality audio, have been widely adopted in SLMs for speech synthesis and editing \citep{wang2023neural,peng2024voicecraft}, as well as in SDMs for voice interaction \citep{xie2024mini,xie2024mini2,wang2024freeze}.
In contrast, semantic tokens are obtained by discretizing speech representations extracted from self-supervised speech pre-trained models \citep{hsu2021hubert,chung2021w2v}, focusing on capturing semantic content rather than acoustic detail.
These tokens are also extensively used in SLMs \citep{an2024funaudiollm,ma2024language} and SDMs \citep{zeng2024glm,fang2024llama}.
Among these approaches, CosyVoice \citep{du2024cosyvoice} leverages supervised semantic tokens to enable zero-shot TTS, demonstrating the potential of semantic tokens for timbre control. This insight inspires our work, which seeks to extend such functionality to SDMs—a promising yet underexplored direction in the field.

## 3·Methodology: 方法

### Overview

As shown in Figure~\ref{fig:SLAM-Omni}, SLAM-Omni processes input speech using continuous features and adopts parallel audio-text modeling with discrete semantic audio tokens for speech output.
This section details its modeling strategies, covering speech input, speech output, timbre control, and multi-round spoken dialogue, along with its training methodology.

### Speech Input Modeling

SLAM-Omni employs the Whisper encoder \citep{radford2023robust} to extract audio features $\mathbf{A} = [a_1, a_2, \cdots, a_N]$ from user speech instructions at a frequency of 50 Hz.
Whisper, a speech recognition model trained on large-scale supervised cross-lingual speech data, provides precise transcription and robust multilingual support, serving as a foundational component for SLAM-Omni's multi-turn and multilingual dialogue capabilities.
Following \citet{ma2024embarrassingly}, we downsample $\mathbf{A}$ by concatenating every $k$ consecutive frames along the feature dimension, yielding intermediate features $\mathbf{A}^I = [a^I_1, a^I_2, \dots, a^I_{N'}]$, where $a^I_i = a_{(i-1)*k+1} \oplus a_{(i-1)*k+2} \oplus \cdots \oplus a_{i*k-1}$ and $N' = N // k$.
A linear encoder projector then transforms $\mathbf{A}^I$ into $\mathbf{A}^P$ to ensure alignment with LLM’s embedding dimension, defined as $\mathbf{A^P} = \text{MLP}(\mathbf{A}^I)$. These reduced speech features are concatenated with the prompt embeddings $\mathbf{P}$ and serve as input to the LLM.

### Semantic Group Modeling

For speech output, we adopt parallel audio-text modeling, predicting single-layer semantic tokens \citep{du2024cosyvoice} alongside text tokens autoregressively.
To achieve this, the original LLM vocabulary \( V_t \) and embedding space are extended with a new codebook \( V_a \) for audio tokens, resulting in an expanded vocabulary \( V_j = V_t \cup V_a \). The original word embedding matrix is preserved, while the embeddings for audio tokens are randomly initialized.

At each generation step, the LLM outputs logits \( L_j \in \mathbb{R}^{|V_j|} \), which are partitioned into \( L_t \in \mathbb{R}^{|V_t|} \) and \( L_a \in \mathbb{R}^{|V_a|} \), representing predicted distributions for text and audio tokens, respectively. However, generating text and audio tokens at the same rate introduces a key challenge:
there is a substantial frequency mismatch between text tokens (\textasciitilde3Hz) and semantic tokens (50Hz).
The high frequency of audio tokens results in considerably longer sequences, significantly increasing both training and inference costs, as well as leading to higher latency in real-time speech generation.

To mitigate these issues, we propose \textit{semantic group modeling}, which allows the model to predict multiple audio tokens simultaneously at each step, as illustrated in Figure \ref{fig:group-prediction}.
This approach projects the audio logits \( L_a \) into group-sized logits \( L_g \) with a linear layer, where \( L_g \in \mathbb{R}^{|V_a| \times G} \), and \( G \) denotes the group size.
During training, the original semantic token sequence \( \mathbf{S}^T = [s_0, s_1, \dots, s_{T-1}] \) is grouped as \( \mathbf{G}^T = [g_0, g_1, \dots, g_{T'-1}] \), where:

\[
g_i = [s_{i \cdot G}, s_{i \cdot G + 1}, \dots, s_{(i+1) \cdot G - 1}], \quad T' = T // G . \tag{1}
\]

Given prompt embeddings \( \mathbf{P} \), audio features \( \mathbf{A}^P \)
and text token sequence \( \mathbf{T}^L = [t_0, t_1, \dots, t_{L-1}] \), the training objective is defined as a weighted cross-entropy loss:

\[
\mathcal{L} = \lambda_{\text{text}} \mathcal{L}_{\text{text}} + \lambda_{\text{audio}} \mathcal{L}_{\text{audio}} \tag{2}
\]

where:

\[
\mathcal{L}_{\text{text}} = -\frac{1}{L} \sum_{i=1}^{L} \log p(t_i \mid \mathbf{P}, \mathbf{A}^P, \mathbf{G}^T_{<i}, \mathbf{T}^L_{<i})  \tag{3}
\]

\[
\mathcal{L}_{\text{audio}} = -\frac{1}{T'G}  \sum_{i=1}^{T'} \sum_{j=1}^{G} \log p(s_{i \cdot G + j} \mid \mathbf{P}, \mathbf{A}^P, \mathbf{G}^T_{<i}, \mathbf{T}^L_{<i})  \tag{4}
\]

Here, \( \mathcal{L}_{\text{text}} \) and \( \mathcal{L}_{\text{audio}} \) represent the losses for text and audio token predictions, respectively, while \( \lambda_{\text{text}} \) and \( \lambda_{\text{audio}} \) are corresponding weights.

### Controllable Timbre Modeling

Previous approaches disentangle speech by modeling distinct subspaces for different attributes \citep{ju2024naturalspeech} or predicting supervised semantic tokens that separate content and speaker information \citep{du2024cosyvoice}.
These methods enable timbre disentanglement from semantic content, achieving zero-shot TTS where users can freely adjust the system’s vocal timbre by providing audio prompts.

Building on these insights from TTS modeling, we extend zero-shot timbre control to SDMs.
By modeling speech content as semantic tokens, SLAM-Omni inherently disentangles timbre from linguistic information.
Following techniques demonstrated in zero-shot TTS (e.g., CosyVoice), we employ a conditional flow matching model to convert semantic tokens and speaker prompts into mel spectrograms, which are then synthesized into waveforms via HiFi-GAN \citep{kong2020hifi}.
For real-time speech generation, same as common practice like~\citet{zeng2024scaling}, block causal attention is adopted in the Transformer of flow matching.

### Historical Text Prompting

Previous multi-turn spoken dialogue modeling often interleave text and audio tokens as the LLM history \citep{wang2024freeze,zeng2024glm}.
However, the lengthy audio token sequences pose challenges for model training, especially in joint audio-text modeling requiring full fine-tuning, significantly increasing computational costs and limiting the number of dialogue turns.
Moreover, longer histories hinder in-context learning and raise the risk of forgetting earlier dialogue content.

To address these issues, we introduce \textit{Historical Text Prompting}, which exclusively utilizes text modality to represent dialogue history.
As shown in Figure~\ref{fig:SLAM-Omni}, SLAM-Omni structures multi-turn interactions using the template: <System> <History> <Input> <Answer>.
Here, the system prompt specifies the model's role and the dialogue task, while the history prompt stores past dialogue content in text form.
This approach aligns naturally with the training paradigm of LLMs, inheriting their robust text-based in-context learning capabilities.
Moreover, it eliminates the burden of modeling long audio sequences as history, enabling the model to handle more dialogue turns within a constrained context window.

During inference, speech features \( \mathbf{A} \) extracted by Whisper can be decoded into the transcription of the input speech, represented as \(\text{Decoder}(\mathbf{A})\).
On the output side, the generated text tokens are converted back into text using the tokenizer.
Both the textual question and answer are appended to the dialogue history for subsequent turns.
As illustrated in Figure \ref{fig:kv-cache}, the transcription of the first-round spoken dialogue is incorporated into the historical prompt.
During the second round of inference, the corresponding key-value cache is generated and can be reused in the third and subsequent rounds of dialogue, facilitating efficient multi-round inference.

### Single-Stage Training

Current spoken dialogue models typically depend on multi-stage training, including modality adaptation, modality alignment, and supervised fine-tuning \citep{ji2024wavchat}.
These designs demand intricate training strategies, such as coordinating module training across stages and tuning numerous hyperparameters, leading to substantial time and computational overhead.

Aligned with the goal of making SDMs training accessible to everyone, SLAM-Omni achieves outstanding performance through one-stage training with minimal data.
In our experiments, both TTS and ASR training exhibit rapid loss convergence (see Appendix \ref{app:pre-train_details}), underscoring that extensive modality alignment pre-training is unnecessary in our modeling method.
Moreover, further experiments reveal that pre-training negatively impacts model's ability to follow instructions and retain general knowledge, as detailed in Section \ref{sec:training_strategy}.

## 4·Experiments: 实验

### Datasets

As most publicly available dialogue datasets are text-based, we synthesize spoken dialogue corpora using zero-shot TTS systems.
Specifically, we utilize discrete speech tokens from \citet{du2024cosyvoice} and employ CosyVoice\footnote{\scriptsize\url{https://github.com/FunAudioLLM/CosyVoice}} to generate dialogue utterances.
For user inputs, the CosyVoice-300M model is employed to produce corresponding speech.
Vocal timbre is controlled by randomly sampling speaker prompts from a timbre library, which contains 1007 English and 1010 Chinese human audio prompts sourced from seed-tts-eval\footnote{\scriptsize\url{https://github.com/BytedanceSpeech/seed-tts-eval}} \citep{anastassiou2024seed}.
For assistant responses, we use the text-to-token LLM from CosyVoice-300M-SFT to generate semantic tokens, which are used as target audio tokens during SLAM-Omni training.

Table~\ref{tab:training-dataset} summarizes the datasets used to synthesize spoken dialogue corpora.
The training data include VoiceAssistant-400K\footnote{\scriptsize\url{https://huggingface.co/datasets/gpt-omni/VoiceAssistant-400K}} from Mini-Omni \citep{xie2024mini}, the English multi-turn dataset UltraChat\footnote{\scriptsize\url{https://huggingface.co/datasets/stingning/ultrachat}} \citep{ding2023enhancing}, and the Chinese dialogue dataset Belle\_train\_3.5M\_CN\footnote{\scriptsize\url{https://huggingface.co/datasets/BelleGroup/train_3.5M_CN}} \citep{ji2023exploring}.
We clean the synthesized data by removing written artifacts (e.g., emojis, URLs), and we limit the duration of instructions and responses to a maximum of 30 and 60 seconds, respectively, to better align with natural conversational scenarios.
For the primary experiments with SLAM-Omni, only VoiceAssistant-400K is used, while the remaining datasets are incorporated in supplementary experiments to evaluate the model’s performance in multi-turn and multilingual dialogue tasks.

### Training and Inference Details

To ensure a fair comparison in low-resource settings, particularly with Mini-Omni \citep{xie2024mini,xie2024mini2}, another parallel audio-text modeling approach, we utilize Qwen2-0.5B\footnote{\scriptsize\url{https://huggingface.co/Qwen/Qwen2-0.5B}}
\citep{yang2024qwen2} as the LLM backbone and Whisper-small\footnote{\scriptsize\url{https://huggingface.co/openai/whisper-small}} \citep{radford2023robust} as the speech encoder and decoder.
Following \citet{ma2024embarrassingly}, user speech instructions are zero-padded to 30 seconds before being processed by the Whisper encoder, with the resulting speech features downsampled using \( k = 5 \).
In the main experiments, SLAM-Omni adopts a semantic group size of \( G = 3 \).
For ablation studies on group size, models with \( G > 1 \) include an additional linear layer for predicting grouped tokens.

During single-stage training, SLAM-Omni undergoes full fine-tuning, with the Whisper encoder kept frozen.
The weights for \( \mathcal{L}_{\text{text}} \) and \( \mathcal{L}_{\text{audio}} \) are set to 1.
We use the AdamW optimizer \citep{loshchilov2017decoupled} with a peak learning rate of \( 1 \times 10^{-4} \) and a batch size of 24.
Training spans 100,000 steps, with the first 1,000 steps used for warmup, followed by a linear decay schedule.
A validation set comprising 1\% of the training data is used, and validation is performed every 3,000 updates, saving checkpoints based on the lowest validation loss.
For a direct comparison with Mini-Omni, our primary experiments are \textbf{only conducted on VoiceAssistant-400K}, a subset of Mini-Omni's training data. Details on multilingual and multi-turn training are provided in Appendix \ref{app:multi-round} and Appendix \ref{app:chinese}.
The entire training process takes approximately 15 hours on 4 NVIDIA A100 GPUs.

For inference, we use greedy search decoding with a repetition penalty of 1.2 applied to both audio and text layers.
Consistent with \citep{fang2024llama}, models are evaluated using non-streaming decoding for speech response generation.

### Evaluation for Spoken Dialogue Models

Previous SDMs lacked a thorough evaluation of voice interaction capabilities.
VoiceBench \citep{chen2024voicebench} is the first benchmark for voice assistants, but it only assesses the model's text output.
To bridge this gap, we propose a comprehensive evaluation framework that directly measures the speech-to-speech capabilities of SDMs. Voice interaction in SDMs can be broken down into three key stages: understanding, reasoning, and oral conversation.
We have designed eight distinct test sets that assess SDMs across these three dimensions:

\paragraph{Understanding} To evaluate the model's ability of comprehending and following user instructions, we build two datasets to require the model to repeat the user's words or summarize a story.

\paragraph{Reasoning}
We adapt samples from TruthfulQA \citep{lin2021truthfulqa} and STORAL \citep{guan2022corpus}, and design additional questions on math, logic, and common sense (MLC) to assess the model's general knowledge and reasoning ability.

\paragraph{Oral Conversation }
We use AlpacaEval \citep{li2023alpacaeval} and CommonEval \citep{ardila2019common} from VoiceBench, along with real-life questions from WildChat \citep{zhao2024wildchat}, to test the model's conversational ability in open-ended scenarios.

The model's inference results on these tasks are evaluated using the following metrics:

\paragraph{ChatGPT Score}
To assess the \textbf{content quality} of the model's responses, we use Whisper-large-v3\footnote{\scriptsize\url{https://huggingface.co/openai/whisper-large-v3}} to transcribe the speech output into text, followed by evaluation using GPT-4o mini \citep{openai2024gpt4omini}.
The model is prompted to score the transcription based on predefined criteria, including accuracy, relevance, clarity, and completeness, with detailed prompts provided in Appendix \ref{app:scoring-criteria}.

\paragraph{UTMOS Score}
To measure the overall \textbf{speech quality}, we use the UTMOS \citep{saeki2022utmos} model to predict mean opinion scores (MOS).

\paragraph{WER Score}
To evaluate the \textbf{speech-text alignment}, we calculate the word error rate (WER) between the speech transcription and the corresponding text response, referred to as ASR-WER.

The overall scores for UTMOS and ASR-WER are calculated as the average of their respective scores across these eight evaluation datasets.

Table~\ref{tab:evaluation-dataset} summarizes the evaluation datasets, with details and scoring criteria in Appendices \ref{app:evaluation} and \ref{app:scoring-criteria}. Descriptions for the multi-turn and Chinese evaluation datasets are in Appendices \ref{app:multi-round} and \ref{app:chinese}.
We assess SLAM-Omni alongside the Mini-Omni  \citep{xie2024mini,xie2024mini2}, both using a 0.5B LLM backbone, and compare against larger SDMs including Freeze-Omni \citep{wang2024freeze}, Llama-Omni \citep{fang2024llama}, and GLM-4-Voice \citep{zeng2024glm}, as well as LLMs such as Qwen2-0.5B-instruct and Qwen2-7B-instruct \citep{yang2024qwen2}.

## 5·Results: 结果

## 6·Conclusions: 结论