# SLAM-Omni

<details>
<summary>基本信息</summary>

- 标题: "SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training"
- 作者:
  - 01 Wenxi Chen
  - 02 Ziyang Ma
  - 03 Ruiqi Yan
  - 04 Yuzhe Liang
  - 05 Xiquan Li
  - 06 Ruiyang Xu
  - 07 Zhikang Niu
  - 08 Yanqiao Zhu
  - 09 Yifan Yang
  - 10 Zhanxun Liu
  - 11 Kai Yu
  - 12 Yuxuan Hu
  - 13 Jinyu Li
  - 14 Yan Lu
  - 15 Shujie Liu
  - 16 Xie Chen
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.15649)
  - [Publication]()
  - [Github]()
  - [Demo](https://slam-omni.github.io)
- 文件:
  - [ArXiv](_PDF/2412.15649v1__SLAM-Omni__Timbre-Controllable_Voice_Interaction_System_with_Single-Stage_Training.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality.
In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training.
SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder.
By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference.
Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions.
Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data.
Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks.
Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.

## 1·Introduction: 引言

With the advent of large language models (LLMs), recent developments \citep{achiam2023gpt,dubey2024llama,yang2024qwen2} have showcased their powerful capabilities in textual conversation.
In spoken dialogue systems, however, traditional methods rely on a cascaded pipeline involving automatic speech recognition (ASR) to transcribe user input, LLMs to generate textual responses, and text-to-speech (TTS) models to produce audio outputs.
This design faces two major issues: (1) significantly increased interaction latency, and (2) reliance on text-based interaction, which overlooks rich non-verbal information in speech dialogue, such as emotions and prosody.
The release of GPT-4o~\cite {openai2024gpt4o} has underscored the potential of real-time spoken dialogue systems in delivering seamless interaction.
In response, several open-source frameworks, including Moshi \citep{defossez2024moshi}, Mini-Omni \citep{xie2024mini,xie2024mini2}, and LLaMA-Omni \citep{fang2024llama},
have been developed for effective end-to-end voice-based interaction.

Existing spoken dialogue models (SDMs) primarily model speech with discretized audio tokens.
Some approaches \citep{fang2024llama,wang2024freeze} rely on text embeddings to guide audio token generation, which limits their ability to generate critical audio paralinguistic attributes such as emotion and prosody.
Others \citep{zeng2024scaling,zhang2024omniflatten,nguyen2024spirit} adopt interleaved arrangements of audio and text tokens to restructure language modeling, while increasing training costs.
A third category \citep{xie2024mini,xie2024mini2,mitsui2024pslm} employs a parallel speech-text generation method, which aligns closely with ours, balancing the delivery of intrinsic audio attributes and consuming of computational burden.

A notable limitation of current SDMs is their disability to generate responses with diverse speaker timbres.
This restriction primarily stems from the uniform timbre of responses in most training datasets and the lack of explicit speaker modeling in existing frameworks.
To address this gap, we propose the first zero-shot timbre control solution for dialogue systems.
Drawing inspiration from zero-shot TTS \citep{wang2023neural}, our approach allows users to specify the desired output timbre by providing an audio prompt, paving the way for interactive applications such as personalized virtual assistants and customizable game character voices.

In this paper, we propose SLAM-Omni, a timbre-controllable, end-to-end spoken dialogue system with single-stage training.
For user speech input, the Whisper \citep{radford2023robust} encoder is employed to extract audio representations, which are then aligned with text embeddings via a projector and fed into the LLM.
On the output side, semantic audio tokens \citep{du2024cosyvoice} and text tokens are autoregressively predicted in parallel. These audio tokens naturally decouple speaker information into a separate vocoder, enabling zero-shot timbre control.
Inspired by VALL-E 2 \citep{chen2024vall}, SLAM-Omni predicts single-layer semantic tokens in grouped units per audio frame, reducing audio sequence length and accelerating training and inference.
For multi-round spoken dialogue modeling, we introduce historical text prompting, which leverages text-only history rather than alternating audio-text streams.
This strategy significantly compresses the dialogue history, improves data utilization, enables the model to handle more dialogue turns and enhances its instruction-following ability.
During inference, instruction text is extracted from encoded audio embeddings with a Whisper decoder and response text is directly obtained from the generated text stream, both of which provide low-cost speech transcription that enables efficient multi-round voice interactions.
Comprehensive evaluations demonstrate that ASR or TTS pre-training is not necessary, while our SLAM-Omni, with only 15 hours of single-stage training on 4 GPUs, greatly outperforms prior models of similar scale in both speech content, quality and speech-text alignment.

Our contributions are summarized below:
- We propose the first zero-shot \textit{timbre control solution} for voice interaction systems with speaker-decoupled semantic tokens.
- \textit{Semantic Group Modeling} approach is proposed for accelerating single-layer semantic speech token generation and model training.
- \textit{Historical Text Prompting} is proposed for efficient multi-round history modeling in \hspace{-1mm} SDMs.
- SLAM-Omni is the first voice assistant to achieve \textit{single-stage training}, requiring minimal data and computational resources.
-  Experiments show that SLAM-Omni outperforms prior models of similar scale on text-related tasks, and shows superior performance on acoustic quality and speech-text alignment among all existing SDMs.
Results on a larger dataset demonstrates its multilingual and multi-round dialogue capabilities.

## 2·Related Works: 相关工作

### 2.1·End-to-End Spoken Dialogue Modeling

Existing end-to-end SDMs primarily model voice interaction by treating text as either an intermediate output or a hidden state to leverage the pre-trained knowledge of LLMs.
As illustrated in Figure~\ref{fig:SDM-modeling}, these methods can be categorized into text-driven modeling and joint audio-text modeling.
For text-driven modeling, as shown in Figure~\ref{fig:SDM-modeling}a, existing methods \citep{fang2024llama,wang2024freeze} keep the original architecture of LLMs to retain textual abilities, using their hidden states as input to a speech decoder for audio generation.
This approach effectively preserves LLMs knowledge but struggles to capture rich audio paralinguistic attributes such as emotion and prosody, since only text tokens are used for autoregressive modeling.
Joint audio-text modeling, illustrated in Figure~\ref{fig:SDM-modeling}b and c, is further divided into interleaved and parallel paradigms. Both paradigms incorporate audio tokens into the autoregressive modeling, theoretically enhancing the ability to model non-verbal information.
In the interleaved paradigm, models \citep{zhang2024omniflatten,zeng2024scaling,nguyen2024spirit} alternate between text and audio tokens during generation.
This method typically requires extensive interleaved speech-text data and pre-training for re-modeling LLMs.
In contrast, the parallel paradigm, adopted by models like PSLM \citep{mitsui2024pslm}, Mini-Omni \citep{xie2024mini,xie2024mini2}, and our proposed SLAM-Omni, employs autoregressive modeling of text and audio tokens in parallel.
However, unlike PSLM and Mini-Omni, SLAM-Omni predicts single-layer grouped semantic tokens to accelerate audio generation process.
Combining semantic group modeling with single-stage training, we achieve an end-to-end SDM built on a pre-trained LLM that requires significantly less training costs compared to previous solutions.

### 2.2·Speech Tokenization

Speech tokenization is a foundational technique in speech language models (SLMs), typically categorized into acoustic tokens and semantic tokens \citep{zhang2023speechgpt,borsos2023audiolm}.
Acoustic tokens, derived from neural audio codecs \citep{defossez2022high,zeghidour2021soundstream} and optimized for reconstructing high-quality audio, have been widely adopted in SLMs for speech synthesis and editing \citep{wang2023neural,peng2024voicecraft}, as well as in SDMs for voice interaction \citep{xie2024mini,xie2024mini2,wang2024freeze}.
In contrast, semantic tokens are obtained by discretizing speech representations extracted from self-supervised speech pre-trained models \citep{hsu2021hubert,chung2021w2v}, focusing on capturing semantic content rather than acoustic detail.
These tokens are also extensively used in SLMs \citep{an2024funaudiollm,ma2024language} and SDMs \citep{zeng2024glm,fang2024llama}.
Among these approaches, CosyVoice \citep{du2024cosyvoice} leverages supervised semantic tokens to enable zero-shot TTS, demonstrating the potential of semantic tokens for timbre control. This insight inspires our work, which seeks to extend such functionality to SDMs—a promising yet underexplored direction in the field.

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论