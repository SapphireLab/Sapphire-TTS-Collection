# SLAM-Omni

<details>
<summary>基本信息</summary>

- 标题: "SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training"
- 作者:
  - 01 Wenxi Chen
  - 02 Ziyang Ma
  - 03 Ruiqi Yan
  - 04 Yuzhe Liang
  - 05 Xiquan Li
  - 06 Ruiyang Xu
  - 07 Zhikang Niu
  - 08 Yanqiao Zhu
  - 09 Yifan Yang
  - 10 Zhanxun Liu
  - 11 Kai Yu
  - 12 Yuxuan Hu
  - 13 Jinyu Li
  - 14 Yan Lu
  - 15 Shujie Liu
  - 16 Xie Chen
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.15649)
  - [Publication]()
  - [Github]()
  - [Demo](https://slam-omni.github.io)
- 文件:
  - [ArXiv](_PDF/2412.15649v1__SLAM-Omni__Timbre-Controllable_Voice_Interaction_System_with_Single-Stage_Training.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality.
In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training.
SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder.
By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference.
Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions.
Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data.
Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks.
Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论