# SLAM-Omni

<details>
<summary>基本信息</summary>

- 标题: "SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training"
- 作者:
  - 01 Wenxi Chen
  - 02 Ziyang Ma
  - 03 Ruiqi Yan
  - 04 Yuzhe Liang
  - 05 Xiquan Li
  - 06 Ruiyang Xu
  - 07 Zhikang Niu
  - 08 Yanqiao Zhu
  - 09 Yifan Yang
  - 10 Zhanxun Liu
  - 11 Kai Yu
  - 12 Yuxuan Hu
  - 13 Jinyu Li
  - 14 Yan Lu
  - 15 Shujie Liu
  - 16 Xie Chen
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.15649)
  - [Publication]()
  - [Github]()
  - [Demo](https://slam-omni.github.io)
- 文件:
  - [ArXiv](_PDF/2412.15649v1__SLAM-Omni__Timbre-Controllable_Voice_Interaction_System_with_Single-Stage_Training.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality.
In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training.
SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder.
By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference.
Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions.
Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data.
Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks.
Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.

## 1·Introduction: 引言

With the advent of large language models (LLMs), recent developments \citep{achiam2023gpt,dubey2024llama,yang2024qwen2} have showcased their powerful capabilities in textual conversation.
In spoken dialogue systems, however, traditional methods rely on a cascaded pipeline involving automatic speech recognition (ASR) to transcribe user input, LLMs to generate textual responses, and text-to-speech (TTS) models to produce audio outputs.
This design faces two major issues: (1) significantly increased interaction latency, and (2) reliance on text-based interaction, which overlooks rich non-verbal information in speech dialogue, such as emotions and prosody.
The release of GPT-4o~\cite {openai2024gpt4o} has underscored the potential of real-time spoken dialogue systems in delivering seamless interaction.
In response, several open-source frameworks, including Moshi \citep{defossez2024moshi}, Mini-Omni \citep{xie2024mini,xie2024mini2}, and LLaMA-Omni \citep{fang2024llama},
have been developed for effective end-to-end voice-based interaction.

Existing spoken dialogue models (SDMs) primarily model speech with discretized audio tokens.
Some approaches \citep{fang2024llama,wang2024freeze} rely on text embeddings to guide audio token generation, which limits their ability to generate critical audio paralinguistic attributes such as emotion and prosody.
Others \citep{zeng2024scaling,zhang2024omniflatten,nguyen2024spirit} adopt interleaved arrangements of audio and text tokens to restructure language modeling, while increasing training costs.
A third category \citep{xie2024mini,xie2024mini2,mitsui2024pslm} employs a parallel speech-text generation method, which aligns closely with ours, balancing the delivery of intrinsic audio attributes and consuming of computational burden.

A notable limitation of current SDMs is their disability to generate responses with diverse speaker timbres.
This restriction primarily stems from the uniform timbre of responses in most training datasets and the lack of explicit speaker modeling in existing frameworks.
To address this gap, we propose the first zero-shot timbre control solution for dialogue systems.
Drawing inspiration from zero-shot TTS \citep{wang2023neural}, our approach allows users to specify the desired output timbre by providing an audio prompt, paving the way for interactive applications such as personalized virtual assistants and customizable game character voices.

In this paper, we propose SLAM-Omni, a timbre-controllable, end-to-end spoken dialogue system with single-stage training.
For user speech input, the Whisper \citep{radford2023robust} encoder is employed to extract audio representations, which are then aligned with text embeddings via a projector and fed into the LLM.
On the output side, semantic audio tokens \citep{du2024cosyvoice} and text tokens are autoregressively predicted in parallel. These audio tokens naturally decouple speaker information into a separate vocoder, enabling zero-shot timbre control.
Inspired by VALL-E 2 \citep{chen2024vall}, SLAM-Omni predicts single-layer semantic tokens in grouped units per audio frame, reducing audio sequence length and accelerating training and inference.
For multi-round spoken dialogue modeling, we introduce historical text prompting, which leverages text-only history rather than alternating audio-text streams.
This strategy significantly compresses the dialogue history, improves data utilization, enables the model to handle more dialogue turns and enhances its instruction-following ability.
During inference, instruction text is extracted from encoded audio embeddings with a Whisper decoder and response text is directly obtained from the generated text stream, both of which provide low-cost speech transcription that enables efficient multi-round voice interactions.
Comprehensive evaluations demonstrate that ASR or TTS pre-training is not necessary, while our SLAM-Omni, with only 15 hours of single-stage training on 4 GPUs, greatly outperforms prior models of similar scale in both speech content, quality and speech-text alignment.

Our contributions are summarized below:
- We propose the first zero-shot \textit{timbre control solution} for voice interaction systems with speaker-decoupled semantic tokens.
- \textit{Semantic Group Modeling} approach is proposed for accelerating single-layer semantic speech token generation and model training.
- \textit{Historical Text Prompting} is proposed for efficient multi-round history modeling in \hspace{-1mm} SDMs.
- SLAM-Omni is the first voice assistant to achieve \textit{single-stage training}, requiring minimal data and computational resources.
-  Experiments show that SLAM-Omni outperforms prior models of similar scale on text-related tasks, and shows superior performance on acoustic quality and speech-text alignment among all existing SDMs.
Results on a larger dataset demonstrates its multilingual and multi-round dialogue capabilities.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论