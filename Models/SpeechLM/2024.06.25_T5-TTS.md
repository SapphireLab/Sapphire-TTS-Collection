# T5-TTS

<details>
<summary>基本信息</summary>

- 标题: "Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment"
- 作者:
  - 01 Paarth Neekhara,
  - 02 Shehzeen Hussain,
  - 03 Subhankar Ghosh,
  - 04 Jason Li,
  - 05 Rafael Valle,
  - 06 Rohan Badlani,
  - 07 Boris Ginsburg
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.17957)
  - [Publication](https://doi.org/10.21437/Interspeech.2024-335) InterSpeech 2024
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2406.17957v1__T5-TTS__Improving_Robustness_of_LLM-based_Speech_Synthesis_by_Learning_Monotonic_Alignment.pdf)
  - [Publication](_PDF/2406.17957p0__T5-TTS__InterSpeech2024.pdf)

</details>

## Abstract: 摘要

Large Language Model (LLM) based text-to-speech (TTS) systems have demonstrated remarkable capabilities in handling large speech datasets and generating natural speech for new speakers.
However, LLM-based TTS models are not robust as the generated output can contain repeating words, missing words and mis-aligned speech (referred to as hallucinations or attention errors), especially when the text contains multiple occurrences of the same token.
We examine these challenges in an encoder-decoder transformer model and find that certain cross-attention heads in such models implicitly learn the text and speech alignment when trained for predicting speech tokens for a given text.
To make the alignment more robust, we propose techniques utilizing CTC loss and attention priors that encourage monotonic cross-attention over the text tokens.
Our guided attention training technique does not introduce any new learnable parameters and significantly improves robustness of LLM-based TTS models.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论