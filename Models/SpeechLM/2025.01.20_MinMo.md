# MinMo

<details>
<summary>基本信息</summary>

- 标题: "MinMo: A Multimodal Large Language Model for Seamless Voice Interaction"
- 作者:
  - 01 Qian Chen,
  - 02 Yafeng Chen,
  - 03 Yanni Chen,
  - 04 Mengzhe Chen,
  - 05 Yingda Chen,
  - 06 Chong Deng,
  - 07 Zhihao Du,
  - 08 Ruize Gao,
  - 09 Changfeng Gao,
  - 10 Zhifu Gao,
  - 11 Yabin Li,
  - 12 Xiang Lv,
  - 13 Jiaqing Liu,
  - 14 Haoneng Luo,
  - 15 Bin Ma,
  - 16 Chongjia Ni,
  - 17 Xian Shi,
  - 18 Jialong Tang,
  - 19 Hui Wang,
  - 20 Hao Wang,
  - 21 Wen Wang,
  - 22 Yuxuan Wang,
  - 23 Yunlan Xu,
  - 24 Fan Yu,
  - 25 Zhijie Yan,
  - 26 Yexin Yang,
  - 27 Baosong Yang,
  - 28 Xian Yang,
  - 29 Guanrou Yang,
  - 30 Tianyu Zhao,
  - 31 Qinglin Zhang,
  - 32 Shiliang Zhang,
  - 33 Nan Zhao,
  - 34 Pei Zhang,
  - 35 Chong Zhang,
  - 36 Jinren Zhou
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.06282)
  - [Publication]()
  - [Github]()
  - [Demo](https://funaudiollm.github.io/minmo/)
- 文件:
  - [ArXiv](_PDF/2501.06282v1__MinMo__A_Multimodal_Large_Language_Model_for_Seamless_Voice_Interaction.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Recent advancements in large language models (LLMs) and multimodal speech-text models have laid the groundwork for seamless voice interactions, enabling real-time, natural, and human-like conversations.
Previous models for voice interactions are categorized as native and aligned.
Native models integrate speech and text processing in one framework but struggle with issues like differing sequence lengths and insufficient pre-training.
Aligned models maintain text LLM capabilities but are often limited by small datasets and a narrow focus on speech tasks.
In this work, we introduce ***MinMo***, a Multimodal Large Language Model with approximately 8B parameters for seamless voice interaction.
We address the main limitations of prior aligned multimodal models.
We train ***MinMo*** through multiple stages of speech-to-text alignment, text-to-speech alignment, speech-to-speech alignment, and duplex interaction alignment, on 1.4 million hours of diverse speech data and a broad range of speech tasks.
After the multi-stage training, ***MinMo*** achieves state-of-the-art performance across various benchmarks for voice comprehension and generation while maintaining the capabilities of text LLMs, and also facilitates full-duplex conversation, that is, simultaneous two-way communication between the user and the system.
Moreover, we propose a novel and simple voice decoder that outperforms prior models in voice generation.
The enhanced instruction-following capabilities of ***MinMo*** supports controlling speech generation based on user instructions, with various nuances including emotions, dialects, and speaking rates, and mimicking specific voices.
For ***MinMo***, the speech-to-text latency is approximately 100ms, full-duplex latency is approximately 600ms in theory and 800ms in practice.
The ***MinMo*** project web page is [this https URL](https://funaudiollm.github.io/minmo), and the code and models will be released soon.

</details>
<br>

在大语言模型和多模态语音-文本模型的最新进展为无缝声音交互奠定了基础, 实现了实时, 自然, 人类般的对话.
以前的声音交互模型可以分为**原生模型 (Native Models)** 和**对齐模型 (Aligned Models)**.
- **原生模型 (Native Models)** 在一个框架中集成了语音和文本处理, 但存在如序列长度差异和预训练不足的问题.
- **对齐模型 (Aligned Models)** 保持了文本大语言模型的能力, 通常但受限于小数据集和语音任务的狭窄关注.

本文介绍了 ***MinMo***, 一个多模态大语言模型, 具有约 8B 参数, 用于无缝声音交互.
我们解决了先前多模态**对齐模型**的主要局限.
我们在 1.4 百万小时 (1400 K Hours) 的多样语音数据和广泛的语音任务上对 ***MinMo*** 进行多阶段训练: 语音到文本对齐, 文本到语音对齐, 语音到语音对齐和双工交互对齐.

经过多阶段训练后, ***MinMo*** 实现了声音理解和生成的 SoTA 性能, 同时保持了文本大语言模型的能力, 并支持全双工对话, 即用户和系统之间可以同时进行双向通信.

此外, 我们提出了一个新式且简单的声音解码器, 在声音生成方面超过了先前模型.

***MinMo*** 的增强指令跟随能力支持根据用户指令控制语音生成, 包括情感, 方言, 以及语速等多种细微差别, 还可以模仿特定的声音.

***MinMo*** 的语音到文本延迟大约为 100ms, 全双工延迟大约为 600ms (理论值) 和 800ms (实际值).
***MinMo*** 项目网页为 [https://funaudiollm.github.io/minmo](https://funaudiollm.github.io/minmo), 代码和模型将很快发布.

## 1·Introduction: 引言

Seamless voice interaction indicates that \textit{a user experiences real-time, natural, relevant, and human-like spoken conversation with the system}.
Facilitating seamless voice interaction poses great challenges:
(1) the system needs to understand audio accurately and comprehensively, including comprehending the content and also paralinguistic cues in speech (e.g., emotion, prosody) as well as audio events;
(2) the system is expected to produce natural and expressive speech response;
(3) the system should provide relevant and reasonable response to the user, as an intelligent chatbot;
(4) the system is expected to support full-duplex conversation (simultaneous two-way communication), that is, the system listens while speaking and the user is free to interrupt when the system is speaking, then the system either continues the speech, or concedes it, listens to the user, and provides response to the new user query.

In recent years, seamless voice interaction systems have gained significant momentum, especially with the advancements in multimodal large language models, such as GPT-4o~\citep{hurst2024gpt} and Moshi~\citep{DBLP:journals/corr/abs-2410-00037}.
These systems not only produce natural and expressive speech but also understand cues beyond words, including emotional tones and audio events.
Current multimodal language models for voice interaction can be categorized into two main categories.
The first category includes \textit{native multimodal models}, such as Moshi~\citep{DBLP:journals/corr/abs-2410-00037} and GLM-4-Voice~\citep{zeng2024glm}.  These models typically use a decoder-only Transformer as the backbone to simultaneously model understanding and generation of both speech and text modalities within a single framework; they usually require pre-training with both speech and text data.
These models suffer from two major limitations.  Firstly, after speech discretization, speech token sequences are often more than twice the length of text (e.g., 12.5 tokens per second in Moshi).
This discrepancy in sequence length poses challenges as model sizes grow, such as the 175B GPT-3~\citep{DBLP:conf/nips/BrownMRSKDNSSAA20}.
Secondly, the scarcity of speech data compared to text leads to highly imbalanced speech-text training data and in turn causes catastrophic forgetting~\citep{wang2024freeze}.

The second category includes \textit{aligned multimodal models}, integrating voice capabilities while aiming to maintain the capabilities of the existing pre-trained text LLM.
This results in intermediate outputs that still contain text, as seen in models such as Llama-Omni~\citep{DBLP:journals/corr/abs-2409-06666} and Freeze-Omni~\citep{wang2024freeze}.
However, these alignment-based models are typically trained on limited speech data (200K samples for LLaMA-Omni and 120K hours for Freeze-Omni), leading to questions on the impact of larger speech datasets on model capabilities and whether the chat capabilities of the original text-LLM might be compromised.
Furthermore, investigation of extensive speech tasks has not been conducted on these models, such as speech translation, emotion recognition, speaker analysis, language identification, and audio event detection.
Moreover, these models lack systematic evaluations of instruction-following capabilities for rich and nuanced speaking styles, as well as lacking development and evaluation of full-duplex conversation capabilities, for achieving seamless voice interaction.

In this work, we introduce a new multimodal large language model ***MinMo***, to address these limitations of existing aligned multimodal models.
***MinMo*** is trained on over 1.4 million hours of speech data, encompassing various tasks such as Speech-to-Text, Text-to-Speech, and Speech-to-Speech, as detailed in Table~\ref{tab:MinMo_data}.
This extensive training enables ***MinMo*** to achieve state-of-the-art (SOTA) performance across various benchmarks, as shown in Figure~\ref{fig:benchmark_radar}.
We also apply methods that effectively mitigate catastrophic forgetting of the chat capabilities of the original text-LLM while enhancing voice comprehension and generation after training on such large-scale datasets.

We also propose a novel voice decoder that balances structural simplicity and competitive voice generation performance.
LLaMA-Omni uses a non-autoregressive (NAR) streaming Transformer, which takes the output hidden states of the LLM as input and employs connectionist temporal classification (CTC) to predict the discrete speech token sequence of the response.
This approach suffers from inferior performance compared to autoregressive speech decoder.
Freeze-Omni uses three speech decoders, including NAR prefix speech decoder, NAR speech decoder, and AR speech decoder, which complicates the model structure.
Different from both of these strategies, we design an AR streaming Transformer for ***MinMo***, which mixes the output hidden states of the LLM with speech tokens, based on a fixed ratio, as shown in Figure~\ref{fig:MinMo}.

Our contributions can be summarized as follows:

- We propose ***MinMo***, an end-to-end aligned multimodal large model that gains audio understanding, audio generation, and end-to-end duplex speech interaction capabilities by adapting a pre-trained text large language model (LLM) through a multi-stage alignment strategy over 1.4 million hours of audio data covering a wide range of speech tasks.
***MinMo*** achieves state-of-the-art (SOTA) performance on multiple open-source benchmarks, including spoken dialogue, multilingual speech recognition, speech translation, emotion recognition, and speaker analysis.
Different from previous multimodal models that often suffer from notable catastrophic forgetting of capabilities of the text LLM and significant performance degradation on text tasks, ***MinMo*** has minimal loss in the original capabilities of the text LLM.

- We propose a novel alignment method for streaming end-to-end audio generation, by exploring the use of the hidden layer representations of the text model as inputs to the Voice Decoder for aligning the audio output modality.
Experimental results demonstrate that our streaming voice decoder effectively balances structural simplicity, low latency, and high voice generation performance, and outperforms previous models.
Additionally, while most existing voice interaction systems only support controlling the content of the response, ***MinMo*** enhances instruction-following capabilities and enables the generation of speech corresponding to user-specified emotions, dialects, and speaking rates, as well as mimicking specific voices with a 98.4\% instruction-following accuracy.

- We develop a mechanism that effectively facilitates full-duplex interactions with ***MinMo***.
Specifically, we implement a full-duplex prediction module that harnesses the text LLM's semantic understanding capabilities to decide whether to continue system response, or concede, listen, and respond to new user query.
For ***MinMo***, the speech-to-text latency is approximately 100ms; the full-duplex latency is approximately 600ms in theory and 800ms in practice.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论

This research introduces ***MinMo***, an advanced multimodal large language model designed to overcome the limitations of existing aligned multimodal models in seamless voice interaction.
Trained on an extensive dataset of over 1.4 million hours of speech, ***MinMo*** showcases state-of-the-art performance across diverse benchmarks, including spoken dialogue, multilingual speech recognition, and emotion recognition.
By leveraging a multi-stage alignment strategy, ***MinMo*** adeptly balances audio understanding and generation while minimizing the catastrophic forgetting often observed in text-based LLMs.
A key innovation is ***MinMo***'s novel alignment method for streaming end-to-end audio generation.
By utilizing hidden layer representations of the text model, ***MinMo***’s voice decoder achieves structural simplicity and competitive performance with low latency.
This approach significantly enhances the model’s instruction-following capabilities, enabling nuanced speech generation that accurately reflects user-specified emotions, dialects, and speaking styles.
Furthermore, ***MinMo*** supports full-duplex interactions, facilitating a seamless conversational experience with a latency of approximately 600ms.
In conclusion, ***MinMo*** represents a substantial advancement in the field of voice interaction systems.
It not only addresses the inherent challenges of sequence length discrepancies and data imbalance but also sets a new standard for natural and expressive voice interactions, paving the way for future developments in multimodal language models.

### Limitations: 局限性

***MinMo*** has certain limitations that need to be addressed.
Firstly, ***MinMo*** integrates audio understanding and audio generation capabilities based on a pre-trained text large model by using alignment.
The text large model only participates in LoRA updates, and its ability to follow diverse instructions, such as language and task following, needs improvement.
Further exploration is needed to determine whether using more high-quality text data for more comprehensive updates of the text large model can enhance its instruction-following ability.
Secondly, there are some long-tail pronunciation error issues in ***MinMo***'s end-to-end audio generation.
This problem partly arises from retaining some one-to-many tokens of the LLM, and partly because some special symbols in the end-to-end modeled output text cannot be effectively converted into speech.
Data scaling can be explored to address these long-tail issues.
Additionally, the overall efficiency of audio generation controlled by instructions in ***MinMo*** needs to be improved.
This is partly due to the overall small size of the current instruction data and the limitation of only using hidden embeddings for end-to-end alignment, which restricts the transmission of historical information.
Finally, while ***MinMo*** implements a duplex module based on semantics, it still requires separate AEC and VAD modules.
In the future, a fully end-to-end duplex model will be explored.