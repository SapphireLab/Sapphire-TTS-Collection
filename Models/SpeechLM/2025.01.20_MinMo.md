# MinMo

<details>
<summary>基本信息</summary>

- 标题: "MinMo: A Multimodal Large Language Model for Seamless Voice Interaction"
- 作者:
  - 01 Qian Chen,
  - 02 Yafeng Chen,
  - 03 Yanni Chen,
  - 04 Mengzhe Chen,
  - 05 Yingda Chen,
  - 06 Chong Deng,
  - 07 Zhihao Du,
  - 08 Ruize Gao,
  - 09 Changfeng Gao,
  - 10 Zhifu Gao,
  - 11 Yabin Li,
  - 12 Xiang Lv,
  - 13 Jiaqing Liu,
  - 14 Haoneng Luo,
  - 15 Bin Ma,
  - 16 Chongjia Ni,
  - 17 Xian Shi,
  - 18 Jialong Tang,
  - 19 Hui Wang,
  - 20 Hao Wang,
  - 21 Wen Wang,
  - 22 Yuxuan Wang,
  - 23 Yunlan Xu,
  - 24 Fan Yu,
  - 25 Zhijie Yan,
  - 26 Yexin Yang,
  - 27 Baosong Yang,
  - 28 Xian Yang,
  - 29 Guanrou Yang,
  - 30 Tianyu Zhao,
  - 31 Qinglin Zhang,
  - 32 Shiliang Zhang,
  - 33 Nan Zhao,
  - 34 Pei Zhang,
  - 35 Chong Zhang,
  - 36 Jinren Zhou
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.06282)
  - [Publication]()
  - [Github]()
  - [Demo](https://funaudiollm.github.io/minmo/)
- 文件:
  - [ArXiv](_PDF/2501.06282v1__MinMo__A_Multimodal_Large_Language_Model_for_Seamless_Voice_Interaction.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Recent advancements in large language models (LLMs) and multimodal speech-text models have laid the groundwork for seamless voice interactions, enabling real-time, natural, and human-like conversations.
Previous models for voice interactions are categorized as native and aligned.
Native models integrate speech and text processing in one framework but struggle with issues like differing sequence lengths and insufficient pre-training.
Aligned models maintain text LLM capabilities but are often limited by small datasets and a narrow focus on speech tasks.
In this work, we introduce ***MinMo***, a Multimodal Large Language Model with approximately 8B parameters for seamless voice interaction.
We address the main limitations of prior aligned multimodal models.
We train ***MinMo*** through multiple stages of speech-to-text alignment, text-to-speech alignment, speech-to-speech alignment, and duplex interaction alignment, on 1.4 million hours of diverse speech data and a broad range of speech tasks.
After the multi-stage training, ***MinMo*** achieves state-of-the-art performance across various benchmarks for voice comprehension and generation while maintaining the capabilities of text LLMs, and also facilitates full-duplex conversation, that is, simultaneous two-way communication between the user and the system.
Moreover, we propose a novel and simple voice decoder that outperforms prior models in voice generation.
The enhanced instruction-following capabilities of ***MinMo*** supports controlling speech generation based on user instructions, with various nuances including emotions, dialects, and speaking rates, and mimicking specific voices.
For ***MinMo***, the speech-to-text latency is approximately 100ms, full-duplex latency is approximately 600ms in theory and 800ms in practice.
The ***MinMo*** project web page is [this https URL](https://funaudiollm.github.io/minmo), and the code and models will be released soon.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论

This research introduces ***MinMo***, an advanced multimodal large language model designed to overcome the limitations of existing aligned multimodal models in seamless voice interaction.
Trained on an extensive dataset of over 1.4 million hours of speech, ***MinMo*** showcases state-of-the-art performance across diverse benchmarks, including spoken dialogue, multilingual speech recognition, and emotion recognition.
By leveraging a multi-stage alignment strategy, ***MinMo*** adeptly balances audio understanding and generation while minimizing the catastrophic forgetting often observed in text-based LLMs.
A key innovation is ***MinMo***'s novel alignment method for streaming end-to-end audio generation.
By utilizing hidden layer representations of the text model, ***MinMo***’s voice decoder achieves structural simplicity and competitive performance with low latency.
This approach significantly enhances the model’s instruction-following capabilities, enabling nuanced speech generation that accurately reflects user-specified emotions, dialects, and speaking styles.
Furthermore, ***MinMo*** supports full-duplex interactions, facilitating a seamless conversational experience with a latency of approximately 600ms.
In conclusion, ***MinMo*** represents a substantial advancement in the field of voice interaction systems.
It not only addresses the inherent challenges of sequence length discrepancies and data imbalance but also sets a new standard for natural and expressive voice interactions, paving the way for future developments in multimodal language models.

### Limitations: 局限性

***MinMo*** has certain limitations that need to be addressed.
Firstly, ***MinMo*** integrates audio understanding and audio generation capabilities based on a pre-trained text large model by using alignment.
The text large model only participates in LoRA updates, and its ability to follow diverse instructions, such as language and task following, needs improvement.
Further exploration is needed to determine whether using more high-quality text data for more comprehensive updates of the text large model can enhance its instruction-following ability.
Secondly, there are some long-tail pronunciation error issues in ***MinMo***'s end-to-end audio generation.
This problem partly arises from retaining some one-to-many tokens of the LLM, and partly because some special symbols in the end-to-end modeled output text cannot be effectively converted into speech.
Data scaling can be explored to address these long-tail issues.
Additionally, the overall efficiency of audio generation controlled by instructions in ***MinMo*** needs to be improved.
This is partly due to the overall small size of the current instruction data and the limitation of only using hidden embeddings for end-to-end alignment, which restricts the transmission of historical information.
Finally, while ***MinMo*** implements a duplex module based on semantics, it still requires separate AEC and VAD modules.
In the future, a fully end-to-end duplex model will be explored.