# MinMo

<details>
<summary>基本信息</summary>

- 标题: "MinMo: A Multimodal Large Language Model for Seamless Voice Interaction"
- 作者:
  - 01 Qian Chen,
  - 02 Yafeng Chen,
  - 03 Yanni Chen,
  - 04 Mengzhe Chen,
  - 05 Yingda Chen,
  - 06 Chong Deng,
  - 07 Zhihao Du,
  - 08 Ruize Gao,
  - 09 Changfeng Gao,
  - 10 Zhifu Gao,
  - 11 Yabin Li,
  - 12 Xiang Lv,
  - 13 Jiaqing Liu,
  - 14 Haoneng Luo,
  - 15 Bin Ma,
  - 16 Chongjia Ni,
  - 17 Xian Shi,
  - 18 Jialong Tang,
  - 19 Hui Wang,
  - 20 Hao Wang,
  - 21 Wen Wang,
  - 22 Yuxuan Wang,
  - 23 Yunlan Xu,
  - 24 Fan Yu,
  - 25 Zhijie Yan,
  - 26 Yexin Yang,
  - 27 Baosong Yang,
  - 28 Xian Yang,
  - 29 Guanrou Yang,
  - 30 Tianyu Zhao,
  - 31 Qinglin Zhang,
  - 32 Shiliang Zhang,
  - 33 Nan Zhao,
  - 34 Pei Zhang,
  - 35 Chong Zhang,
  - 36 Jinren Zhou
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.06282)
  - [Publication]()
  - [Github]()
  - [Demo](https://funaudiollm.github.io/minmo/)
- 文件:
  - [ArXiv](_PDF/2501.06282v1__MinMo__A_Multimodal_Large_Language_Model_for_Seamless_Voice_Interaction.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Recent advancements in large language models (LLMs) and multimodal speech-text models have laid the groundwork for seamless voice interactions, enabling real-time, natural, and human-like conversations.
Previous models for voice interactions are categorized as native and aligned.
Native models integrate speech and text processing in one framework but struggle with issues like differing sequence lengths and insufficient pre-training.
Aligned models maintain text LLM capabilities but are often limited by small datasets and a narrow focus on speech tasks.
In this work, we introduce ***MinMo***, a Multimodal Large Language Model with approximately 8B parameters for seamless voice interaction.
We address the main limitations of prior aligned multimodal models.
We train ***MinMo*** through multiple stages of speech-to-text alignment, text-to-speech alignment, speech-to-speech alignment, and duplex interaction alignment, on 1.4 million hours of diverse speech data and a broad range of speech tasks.
After the multi-stage training, ***MinMo*** achieves state-of-the-art performance across various benchmarks for voice comprehension and generation while maintaining the capabilities of text LLMs, and also facilitates full-duplex conversation, that is, simultaneous two-way communication between the user and the system.
Moreover, we propose a novel and simple voice decoder that outperforms prior models in voice generation.
The enhanced instruction-following capabilities of ***MinMo*** supports controlling speech generation based on user instructions, with various nuances including emotions, dialects, and speaking rates, and mimicking specific voices.
For ***MinMo***, the speech-to-text latency is approximately 100ms, full-duplex latency is approximately 600ms in theory and 800ms in practice.
The ***MinMo*** project web page is [this https URL](https://funaudiollm.github.io/minmo), and the code and models will be released soon.

</details>
<br>

在大语言模型和多模态语音-文本模型的最新进展为无缝声音交互奠定了基础, 实现了实时, 自然, 人类般的对话.
以前的声音交互模型可以分为**原生模型 (Native Models)** 和**对齐模型 (Aligned Models)**.
- **原生模型 (Native Models)** 在一个框架中集成了语音和文本处理, 但存在如序列长度差异和预训练不足的问题.
- **对齐模型 (Aligned Models)** 保持了文本大语言模型的能力, 通常但受限于小数据集和语音任务的狭窄关注.

本文介绍了 ***MinMo***, 一个多模态大语言模型, 具有约 8B 参数, 用于无缝声音交互.
我们解决了先前多模态**对齐模型**的主要局限.
我们在 1.4 百万小时 (1400 K Hours) 的多样语音数据和广泛的语音任务上对 ***MinMo*** 进行多阶段训练: 语音到文本对齐, 文本到语音对齐, 语音到语音对齐和双工交互对齐.

经过多阶段训练后, ***MinMo*** 实现了声音理解和生成的 SoTA 性能, 同时保持了文本大语言模型的能力, 并支持全双工对话, 即用户和系统之间可以同时进行双向通信.

此外, 我们提出了一个新式且简单的声音解码器, 在声音生成方面超过了先前模型.

***MinMo*** 的增强指令跟随能力支持根据用户指令控制语音生成, 包括情感, 方言, 以及语速等多种细微差别, 还可以模仿特定的声音.

***MinMo*** 的语音到文本延迟大约为 100ms, 全双工延迟大约为 600ms (理论值) 和 800ms (实际值).
***MinMo*** 项目网页为 [https://funaudiollm.github.io/minmo](https://funaudiollm.github.io/minmo), 代码和模型将很快发布.

## 1·Introduction: 引言

Seamless voice interaction indicates that \textit{a user experiences real-time, natural, relevant, and human-like spoken conversation with the system}.
Facilitating seamless voice interaction poses great challenges:
(1) the system needs to understand audio accurately and comprehensively, including comprehending the content and also paralinguistic cues in speech (e.g., emotion, prosody) as well as audio events;
(2) the system is expected to produce natural and expressive speech response;
(3) the system should provide relevant and reasonable response to the user, as an intelligent chatbot;
(4) the system is expected to support full-duplex conversation (simultaneous two-way communication), that is, the system listens while speaking and the user is free to interrupt when the system is speaking, then the system either continues the speech, or concedes it, listens to the user, and provides response to the new user query.

In recent years, seamless voice interaction systems have gained significant momentum, especially with the advancements in multimodal large language models, such as GPT-4o~\citep{hurst2024gpt} and Moshi~\citep{DBLP:journals/corr/abs-2410-00037}.
These systems not only produce natural and expressive speech but also understand cues beyond words, including emotional tones and audio events.
Current multimodal language models for voice interaction can be categorized into two main categories.
The first category includes \textit{native multimodal models}, such as Moshi~\citep{DBLP:journals/corr/abs-2410-00037} and GLM-4-Voice~\citep{zeng2024glm}.
These models typically use a decoder-only Transformer as the backbone to simultaneously model understanding and generation of both speech and text modalities within a single framework; they usually require pre-training with both speech and text data.
These models suffer from two major limitations.
Firstly, after speech discretization, speech token sequences are often more than twice the length of text (e.g., 12.5 tokens per second in Moshi).
This discrepancy in sequence length poses challenges as model sizes grow, such as the 175B GPT-3~\citep{DBLP:conf/nips/BrownMRSKDNSSAA20}.
Secondly, the scarcity of speech data compared to text leads to highly imbalanced speech-text training data and in turn causes catastrophic forgetting~\citep{wang2024freeze}.

The second category includes \textit{aligned multimodal models}, integrating voice capabilities while aiming to maintain the capabilities of the existing pre-trained text LLM.
This results in intermediate outputs that still contain text, as seen in models such as Llama-Omni~\citep{DBLP:journals/corr/abs-2409-06666} and Freeze-Omni~\citep{wang2024freeze}.
However, these alignment-based models are typically trained on limited speech data (200K samples for LLaMA-Omni and 120K hours for Freeze-Omni), leading to questions on the impact of larger speech datasets on model capabilities and whether the chat capabilities of the original text-LLM might be compromised.
Furthermore, investigation of extensive speech tasks has not been conducted on these models, such as speech translation, emotion recognition, speaker analysis, language identification, and audio event detection.
Moreover, these models lack systematic evaluations of instruction-following capabilities for rich and nuanced speaking styles, as well as lacking development and evaluation of full-duplex conversation capabilities, for achieving seamless voice interaction.

In this work, we introduce a new multimodal large language model ***MinMo***, to address these limitations of existing aligned multimodal models.
***MinMo*** is trained on over 1.4 million hours of speech data, encompassing various tasks such as Speech-to-Text, Text-to-Speech, and Speech-to-Speech, as detailed in Table~\ref{tab:MinMo_data}.
This extensive training enables ***MinMo*** to achieve state-of-the-art (SOTA) performance across various benchmarks, as shown in Figure~\ref{fig:benchmark_radar}.
We also apply methods that effectively mitigate catastrophic forgetting of the chat capabilities of the original text-LLM while enhancing voice comprehension and generation after training on such large-scale datasets.

We also propose a novel voice decoder that balances structural simplicity and competitive voice generation performance.
LLaMA-Omni uses a non-autoregressive (NAR) streaming Transformer, which takes the output hidden states of the LLM as input and employs connectionist temporal classification (CTC) to predict the discrete speech token sequence of the response.
This approach suffers from inferior performance compared to autoregressive speech decoder.
Freeze-Omni uses three speech decoders, including NAR prefix speech decoder, NAR speech decoder, and AR speech decoder, which complicates the model structure.
Different from both of these strategies, we design an AR streaming Transformer for ***MinMo***, which mixes the output hidden states of the LLM with speech tokens, based on a fixed ratio, as shown in Figure~\ref{fig:MinMo}.

Our contributions can be summarized as follows:

- We propose ***MinMo***, an end-to-end aligned multimodal large model that gains audio understanding, audio generation, and end-to-end duplex speech interaction capabilities by adapting a pre-trained text large language model (LLM) through a multi-stage alignment strategy over 1.4 million hours of audio data covering a wide range of speech tasks.
***MinMo*** achieves state-of-the-art (SOTA) performance on multiple open-source benchmarks, including spoken dialogue, multilingual speech recognition, speech translation, emotion recognition, and speaker analysis.
Different from previous multimodal models that often suffer from notable catastrophic forgetting of capabilities of the text LLM and significant performance degradation on text tasks, ***MinMo*** has minimal loss in the original capabilities of the text LLM.

- We propose a novel alignment method for streaming end-to-end audio generation, by exploring the use of the hidden layer representations of the text model as inputs to the Voice Decoder for aligning the audio output modality.
Experimental results demonstrate that our streaming voice decoder effectively balances structural simplicity, low latency, and high voice generation performance, and outperforms previous models.
Additionally, while most existing voice interaction systems only support controlling the content of the response, ***MinMo*** enhances instruction-following capabilities and enables the generation of speech corresponding to user-specified emotions, dialects, and speaking rates, as well as mimicking specific voices with a 98.4\% instruction-following accuracy.

- We develop a mechanism that effectively facilitates full-duplex interactions with ***MinMo***.
Specifically, we implement a full-duplex prediction module that harnesses the text LLM's semantic understanding capabilities to decide whether to continue system response, or concede, listen, and respond to new user query.
For ***MinMo***, the speech-to-text latency is approximately 100ms; the full-duplex latency is approximately 600ms in theory and 800ms in practice.

## 2·Related Works: 相关工作

### Multimodal Spoken Dialogue Models: 多模态口语对话模型

A variety of speech foundation models have been developed for generic audio understanding, but not systematically explored for voice interaction.
For example, Qwen2-Audio \citep{chu2024qwen2} integrates Whisper speech encoder with a pre-trained text LLM and adapts the LLM for speech understanding capabilities through multi-task pre-training and instruction-based supervised fine-tuning.
SALMONN~\citep{salmonn} is another speech-text LLM for generic audio understanding, by integrating separate speech and audio encoders with a pre-trained text LLM through Q-Former and adopting LoRA for modality alignment.

Since this work aims to develop an end-to-end multimodal model for seamless voice interaction, we focus on comparing ***MinMo*** to speech-text models for voice interaction (or called multimodal spoken dialogue models).
Contemporaneously or inspired by GPT-4o, there have been active developments of multimodal spoken dialogue models managing to achieve real-time voice conversations with user.
\citep{DBLP:journals/corr/abs-2411-13577} provides an in-depth overview of recent spoken dialogue models.
Some works support traditional turn-based voice chat (i.e., half-duplex communication), but cannot handle full-duplex voice interaction (i.e., simultaneous two-way communication).
These models include collaborative systems and end-to-end frameworks.
PSLM~\citep{DBLP:conf/emnlp/MitsuiMWHS24} is a collaborative system since it replies on ASR to process audio input, which discards paralinguistic information and causes error propagation.
PSLM generates speech and text tokens in parallel hence it reduces the speech generation latency; however, it suffers from reduced response quality.
Different from the collaborative systems such as PSLM, end-to-end frameworks directly accept audio input and generate audio output.
Llama-Omni~\citep{DBLP:journals/corr/abs-2409-06666} and Mini-Omni~\citep{xie2024mini} are two recent end-to-end frameworks that have not been trained for full-duplex communication.
Llama-Omni integrates Whisper speech encoder, speech adapter, streaming speech decoder, and vocoder with a pre-trained text LLM backbone.
The speech decoder generates discrete units corresponding to generated text prefix in an NAR manner.
The model is trained with a two-stage strategy: in the first stage, the speech encoder is frozen, and the speech adapter and LLM are trained autoregressively; in the second stage, the speech encoder, speech adapter, and LLM are frozen and only the speech decoder is trained using the CTC loss.
Llama-Omni is evaluated on speech-to-text instruction-following and speech-to-speech instruction-following tasks.
Mini-Omni also adopts Whisper encoder and uses adapter for minimal training in order to reserve LLM's capabilities.
The model is trained through three stages of modality alignment, adapter training, and multi-modal fine-tuning.
Mini-Omni simultaneously generates text and audio tokens, while padding N tokens to ensure that the corresponding text tokens are produced first to guide audio token generation.

Our ***MinMo*** facilitates full-duplex spoken dialogues.
Existing full-duplex voice chat systems can also be categorized into collaborative systems and end-to-end models.
Among collaborative systems, VITA~\citep{DBLP:journals/corr/abs-2408-05211} runs two models at the same time, namely, the generation model and the monitoring model, to support full-duplex communication.
When the generation model is generating system response, the monitoring model monitors the environment and once it detects effective user interruption, it combines context and provides response to the new user query, while the generation model pauses and switches to the monitoring role.
Notably, VITA still relies on an external TTS module to generate speech output.
Alternatively, another collaborative system~\citep{DBLP:journals/corr/abs-2405-19487} operates with LLM interfacing with an ASR module and a streaming TTS module.
The system does not require modality alignment; instead, supervised fine-tuning is conducted on a pre-trained text LLM with the following paradigm: At each time step, the LLM either processes an input token, or generates a text token, or outputs a special control token for state transitions between SPEAK and LISTEN.
All these tasks are defined as next token prediction on a serialized, single-stream view of dialogues.
Full-duplex dialogue learning is conducted on data synthesized by GPT-4 to generate dialogues with different types of user interruptions.
Notably, with its cascaded architecture, this system suffers from high latency up to 680ms.

Among end-to-end full-duplex models, the early work of dGSLM~\citep{DBLP:journals/corr/abs-2203-16502} proposes a Siamese architecture to jointly process both audio token streams of user speech and system speech.
However, it suffers from several weaknesses: it relies on speech-only training, hence does not leverage capabilities of a pre-trained text LLM; it only uses semantic tokens, hence does not sufficiently model acoustic information; it does not support online mode.
LSLM~\citep{DBLP:journals/corr/abs-2408-02622} uses a decoder-only Transformer to generate speaking tokens and a streaming SSL encoder to process listening tokens.
It introduces an interruption token to stop speaking when detecting a turn-taking attempt from the user.
However, the model is insufficient in generating reasonable responses.
Among the more recent end-to-end full-duplex models, Moshi~\citep{DBLP:journals/corr/abs-2410-00037}, GLM-4-Voice~\citep{zeng2024glm}, SyncLM~\citep{DBLP:conf/emnlp/VeluriPYGG24}, IntrinsicVoice~\citep{DBLP:journals/corr/abs-2410-08035}, and Omni-Flatten~\citep{DBLP:journals/corr/abs-2410-17799} are \textit{native multimodal models}.
They simultaneously model understanding and generation of both speech and text modalities within a single framework, based on a GPT backbone, and require self-supervised autoregressive pre-training using both speech and text data.
As discussed in Section~\ref{sec:intro}, these native multimodal models need to tackle the challenges due to significant discrepancy between sequence lengths of speech tokens and text tokens, and also highly imbalanced speech-text training data and the resulting catastrophic forgetting.
IntrinsicVoice employs a GroupFormer to generate HuBERT tokens from the LLM’s hidden states, effectively shortening speech sequences to lengths comparable to text sequences.
OmniFlatten utilizes a multi-stage progressive post-training strategy that incorporates a chunk-based flattened single stream of speech tokens and text tokens to learn full-duplex and text-free speech-to-speech interaction.
Different from these native multimodal models, our ***MinMo*** is in the category of \textit{aligned multimodal models}, which also include Llama-Omni, Mini-Omni2 \citep{xie2024mini}, and Freeze-Omni~\citep{wang2024freeze}.
Aligned multimodal models integrate voice capabilities while aiming to maintain the capabilities of the existing pre-trained text LLM.
Mini-Omni2 introduces a command-based interruption mechanism for supporting full-duplex conversation; however, it is only evaluated on the ASR task and compared to Whisper, VITA, and Mini-Omni.
Freeze-Omni~\citep{wang2024freeze} is a speech-to-speech model that freezes the pre-trained text LLM to reserve the LLM's capabilities.
It supports streaming input speech and generates streaming output speech, uses multi-task training, and conducts chunk-level state prediction for modeling full-duplex voice interaction.
Our ***MinMo*** differs from these aligned multimodal models in the following ways.
We explore training ***MinMo*** on much larger speech datasets (1.4 million hours of diverse speech data in contrast to 200K samples for LLaMA-Omni and 120K hours for Freeze-Omni) and on much more extensive speech tasks.
***MinMo*** also differs from existing aligned multimodal models with a novel speech decoder, enhanced instruction following capabilities, and systematic training and evaluation of full-duplex spoken conversation capabilities.

### Text Style-Controllable Speech Synthesis: 文本风格可控语音合成

A distinctive feature of multimodal spoken dialogue models, compared to text-based dialogue models, is their ability to comprehend and generate acoustic information beyond mere textual content.
The speech modality not only contains the content but also acoustic information such as emotion, dialect, and speaking rate.
An intelligent multimodal spoken dialogue model should be able to comprehensively understand the acoustic information in input speech (e.g., emotion) and also ideally generate responses with specified emotions, dialects, and speaking rate, as well as mimicking specific voices, so that the system can achieve a deeper level of understanding and response in communication.
Collaborative systems, such as ParalinGPT~\citep{DBLP:conf/icassp/LinSGYGGSLB24}, E-Chat~\citep{DBLP:journals/corr/abs-2401-00475}, and Spoken-LLM~\citep{DBLP:conf/acl/LinCL24}, incorporate paralinguistic features to enhance the understanding of acoustic information such as emotions.
These systems can be cascaded with a style-controllable Text-to-Speech (TTS) system to generate responses with specific emotion, speaking rate, and volume.
Significant progresses have been made in text-style controllable TTS, such as TextrolSpeech~\citep{DBLP:conf/icassp/JiZ00CDHZ24}, PromptTTS~\citep{DBLP:conf/icassp/ShimizuYKSDKT24}, PromptTTS2~\citep{DBLP:conf/iclr/LengGSJ0LLYZS0024}, InstructTTS~\citep{DBLP:journals/taslp/YangLHWM24}, and ControlSpeech~\citep{DBLP:journals/corr/abs-2406-01205}.
In contrast to these collaborative systems, Moshi~\citep{DBLP:journals/corr/abs-2410-00037} uses a TTS engine with a single actor's voice and recorded monologues in over 70 speaking styles to synthesize training data to support understanding and generation of acoustic information in an end-to-end model.
GLM-4-Voice~\citep{zeng2024glm} employs high-quality, multi-turn spoken dialogues tailored to specific speech style requirements, such as speaking rate, emotion, or dialect, to support style-controllable spoken dialogues.
However, to the best of our knowledge, no previous work has demonstrated that aligned multimodal models can support style-controllable voice generation.
Contrary to previous claims that aligned multimodal models such as Llama-Omni and Freeze-Omni only allow language models to control the content of speech but not the style and prosody~\citep{zeng2024glm}, in this work, we propose a novel streaming voice decoder for the aligned multimodal model .
***MinMo*** and find that this decoder enhances instruction-following capabilities and enables .
***MinMo*** to generate speech corresponding to user-specified emotions, dialects, speaking rates, as well as mimicking specific voices.

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论

This research introduces ***MinMo***, an advanced multimodal large language model designed to overcome the limitations of existing aligned multimodal models in seamless voice interaction.
Trained on an extensive dataset of over 1.4 million hours of speech, ***MinMo*** showcases state-of-the-art performance across diverse benchmarks, including spoken dialogue, multilingual speech recognition, and emotion recognition.
By leveraging a multi-stage alignment strategy, ***MinMo*** adeptly balances audio understanding and generation while minimizing the catastrophic forgetting often observed in text-based LLMs.
A key innovation is ***MinMo***'s novel alignment method for streaming end-to-end audio generation.
By utilizing hidden layer representations of the text model, ***MinMo***’s voice decoder achieves structural simplicity and competitive performance with low latency.
This approach significantly enhances the model’s instruction-following capabilities, enabling nuanced speech generation that accurately reflects user-specified emotions, dialects, and speaking styles.
Furthermore, ***MinMo*** supports full-duplex interactions, facilitating a seamless conversational experience with a latency of approximately 600ms.
In conclusion, ***MinMo*** represents a substantial advancement in the field of voice interaction systems.
It not only addresses the inherent challenges of sequence length discrepancies and data imbalance but also sets a new standard for natural and expressive voice interactions, paving the way for future developments in multimodal language models.

### Limitations: 局限性

***MinMo*** has certain limitations that need to be addressed.
Firstly, ***MinMo*** integrates audio understanding and audio generation capabilities based on a pre-trained text large model by using alignment.
The text large model only participates in LoRA updates, and its ability to follow diverse instructions, such as language and task following, needs improvement.
Further exploration is needed to determine whether using more high-quality text data for more comprehensive updates of the text large model can enhance its instruction-following ability.
Secondly, there are some long-tail pronunciation error issues in ***MinMo***'s end-to-end audio generation.
This problem partly arises from retaining some one-to-many tokens of the LLM, and partly because some special symbols in the end-to-end modeled output text cannot be effectively converted into speech.
Data scaling can be explored to address these long-tail issues.
Additionally, the overall efficiency of audio generation controlled by instructions in ***MinMo*** needs to be improved.
This is partly due to the overall small size of the current instruction data and the limitation of only using hidden embeddings for end-to-end alignment, which restricts the transmission of historical information.
Finally, while ***MinMo*** implements a duplex module based on semantics, it still requires separate AEC and VAD modules.
In the future, a fully end-to-end duplex model will be explored.