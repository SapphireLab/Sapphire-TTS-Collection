# Whisper

<details>
<summary>基本信息</summary>

- 标题: "Robust Speech Recognition via Large-Scale Weak Supervision"
- 作者:
  - 01 Alec Radford,
  - 02 Jong Wook Kim,
  - 03 Tao Xu,
  - 04 Greg Brockman,
  - 05 Christine McLeavey,
  - 06 Ilya Sutskever
- 链接:
  - [ArXiv](https://arxiv.org/abs/2212.04356)
  - [Publication](https://proceedings.mlr.press/v202/radford23a.html)
  - [Github](https://github.com/openai/whisper)
  - [Demo]
- 文件:
  - [ArXiv](_PDF/2212.04356v1__Whisper__Robust_Speech_Recognition_via_Large-Scale_Weak_Supervision.pdf)
  - [Publication](_PDF/2212.04356p0__Whisper__ICML2023.pdf)

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet.
When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning.
When compared to humans, the models approach their accuracy and robustness.
We are releasing models and inference code to serve as a foundation for further work on robust speech processing.

</details>
<br>

我们研究了语音处理系统的能力, 该系统仅仅训练用于预测互联网上大量音频的转录文本.
当扩展到 680k 小时的多语种多任务监督时, 所得的模型在标准基准上表现得很好, 并且通常与完全监督的先前结果相当, 而无需任何微调进行零样本迁移.
与人类相比, 模型的准确性和鲁棒性都有所提高.
我们正在发布模型和推理代码, 以作为未来工作的基础.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论