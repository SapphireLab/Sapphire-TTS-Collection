# MobileSpeech

<details>
<summary>基本信息</summary>

- 标题:
- 作者:
  - 01 [Shengpeng_Ji](../../Authors/Shengpeng_Ji.md)
  - 02 [Ziyue_Jiang](../../Authors/Ziyue_Jiang.md)
  - 03 [Hanting_Wang](../../Authors/Hanting_Wang.md)
  - 04 [Jialong_Zuo](../../Authors/Jialong_Zuo.md)
  - 05 [Zhou_Zhao_(赵洲)](../../Authors/Zhou_Zhao_(赵洲).md)
- 机构:
  - 机构
- 时间:
  - 预印时间: 2024.02.14 ArXiv v1
  - 更新笔记: 2024.06.06
- 发表:
  - 期刊/会议
- 链接:
  - [ArXiv](https://arxiv.org/abs/2402.09378)
  - [DOI]()
  - [Github]()
  - [Demo](
- 标签:
  - [SpeechSynthesis](../../Tags/SpeechSynthesis.md)
  - [Zero-Shot](../../Tags/Zero-Shot.md)
  - [MobileDevice](../../Tags/MobileDevice.md)
- 页数: 13
- 引用: ?
- 被引: 1

</details>


# 标题

<details>
<summary>基本信息</summary>

- 标题: "MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech"
- 作者:
  - 01 Shengpeng Ji
  - 02 Ziyue Jiang
  - 03 Hanting Wang
  - 04 Jialong Zuo
  - 05 Zhou Zhao (赵洲)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2402.09378)
  - [Publication](https://doi.org/10.18653/v1/2024.acl-long.733)
  - [Github]()
  - [Demo](https://mobilespeech.github.io)
- 文件:
  - [ArXiv](_PDF/2402.09378v2__MobileSpeech__A_Fast_&_High-Fidelity_Framework_for_Mobile_Zero-Shot_TTS.pdf)
  - [Publication](_PDF/2402.09738p0__MobileSpeech__ACL2024.pdf)

</details>

## Abstract: 摘要

Zero-shot text-to-speech (TTS) has gained significant attention due to its powerful voice cloning capabilities, requiring only a few seconds of unseen speaker voice prompts.
However, all previous work has been developed for cloud-based systems.Taking autoregressive models as an example, although these approaches achieve high-fidelity voice cloning, they fall short in terms of inference speed, model size, and robustness.There-fore, we propose ***MobileSpeech***, which is a fast, lightweight, and robust zero-shot text-to-speech system based on mobile devices for the first time.
Specifically:
1) leveraging discrete codec, we design a parallel speech mask decoder module called SMD, which incorpo-rates hierarchical information from the speech codec and weight mechanisms across differ-ent codec layers during the generation process.
Moreover, to bridge the gap between text and speech, we introduce a high-level probabilistic mask that simulates the progression of information flow from less to more during speech generation.
2) For speaker prompts, we extract fine-grained prompt duration from the prompt speech and incorporate text, prompt speech by cross attention in SMD.
We demonstrate the effectiveness of ***MobileSpeech*** on multilingual datasets at different levels, achieving state-of-the-art results in terms of generating speed and speech quality.

***MobileSpeech*** achieves RTF of 0.09 on a single A100 GPU and we have successfully deployed ***MobileSpeech*** on mobile devices.
Audio samples are available at https://mobilespeech.github.io/ .

## 1.Introduction: 引言

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论