# LLaST

<details>
<summary>基本信息</summary>

- 标题: "LLaST: Improved End-to-end Speech Translation System Leveraged by Large Language Models"
- 作者:
  - 01 Xi Chen,
  - 02 Songyang Zhang,
  - 03 Qibing Bai,
  - 04 Kai Chen,
  - 05 Satoshi Nakamura
- 链接:
  - [ArXiv](https://arxiv.org/abs/2407.15415)
  - [Publication]()
  - [Github](https://github.com/openaudiolab/LLaST)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2407.15415v1__LLaST__Improved_End-to-End_Speech_Translation_System_Leveraged_by_Large_Language_Models.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

We introduces LLaST, a framework for building high-performance Large Language model based Speech-to-text Translation systems.
We address the limitations of end-to-end speech translation(E2E ST) models by exploring model architecture design and optimization techniques tailored for LLMs.
Our approach includes LLM-based speech translation architecture design, ASR-augmented training, multilingual data augmentation, and dual-LoRA optimization.
Our approach demonstrates superior performance on the CoVoST-2 benchmark and showcases exceptional scaling capabilities powered by LLMs.
We believe this effective method will serve as a strong baseline for speech translation and provide insights for future improvements of the LLM-based speech translation framework.
We release the data, code and models in [this https URL](https://github.com/openaudiolab/LLaST).

## 1.Introduction: 引言

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
