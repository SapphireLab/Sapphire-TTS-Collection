# SpeechSSM

<details>
<summary>基本信息</summary>

- 标题: "Long-Form Speech Generation with Spoken Language Models"
- 作者:
  - 01 Se Jin Park,
  - 02 Julian Salazar,
  - 03 Aren Jansen,
  - 04 Keisuke Kinoshita,
  - 05 Yong Man Ro,
  - 06 RJ Skerry-Ryan
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.18603)
  - [Publication]()
  - [Github]()
  - [Demo](https://google.github.io/tacotron/publications/speechssm/)
- 文件:
  - [ArXiv](_PDF/2412.18603v1__SpeechSSM__Long-Form_Speech_Generation_with_Spoken_Language_Models.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

We consider the generative modeling of speech over multiple minutes, a requirement for long-form multimedia generation and audio-native voice assistants.
However, current spoken language models struggle to generate plausible speech past tens of seconds, from high temporal resolution of speech tokens causing loss of coherence, to architectural issues with long-sequence training or extrapolation, to memory costs at inference time.
With these considerations we propose SpeechSSM, the first speech language model to learn from and sample long-form spoken audio (e.g., 16 minutes of read or extemporaneous speech) in a single decoding session without text intermediates, based on recent advances in linear-time sequence modeling.
Furthermore, to address growing challenges in spoken language evaluation, especially in this new long-form setting, we propose: new embedding-based and LLM-judged metrics; quality measurements over length and time; and a new benchmark for long-form speech processing and generation, LibriSpeech-Long.
Speech samples and the dataset are released at [this https URL](https://google.github.io/tacotron/publications/speechssm/).

## 1·Introduction: 引言

Generative spoken language models \citep{lakhotia2021generative, dieleman2021-variable, oord2017-vqvae} are autoregressive (AR) models of invertible audio representations, enabling the direct learning and generation of intelligible speech and its paralinguistic aspects, such as prosody \citep{kharitonov2022-pgslm} and turn-taking \citep{nguyen2023generative}.
These capabilities make speech LMs promising for applications like media understanding and co-creation, audio-native voice assistants, and textless NLP.
However, real-world use-cases of spoken language models require the ability to both understand and generate long-form audio.
For example, voice interactions can last many minutes, requiring a model to maintain a growing conversational history in real time, and expressive media like audiobooks and podcasts can require semantic, paralinguistic, and speaker coherence over a chapter or episode.

This presents significant challenges for existing spoken language models; spoken audio is inherently complex, as its textual semantic content is entangled with paralinguistic content and acoustic properties that may detract from learning higher-level speech features.
Furthermore, the audio representations used have high temporal rates; it can require 10+ speech tokens to cover the duration of a spoken word \citep{hassid2023textually}.
Hence, models must retain and aggregate semantics over a longer time horizon, as well as generate content that is coherent over the same horizon.
This is difficult for the vanilla Transformer \citep{vaswani2017-transformer} in implementation, as its initial cost grows quadratically with prompt length, and its per-step cost grows linearly with decoding length; and difficult in modeling, as suggested by lackluster performance on long-range dependency tasks \citep{tay2021-lra}.
Though a few works have improved speech coherence via joint modeling with text (\Cref{sec:related-work}), the challenges of directly modeling long-form speech, particularly generation, remain unstudied by existing work on spoken language models (\Cref{fig:prior_work}).
Finally, the novelty of generating long-form speech means that analyses and evaluations of such generations have not been studied to date.
Our work proposes and make initial progress on the topic of generative long-form speech:

**Modeling**.

We discuss the design choices required to enable the practical training, generation, and extrapolation to tens of minutes of audio, from tokenization to speaker conditioning to complexity with respect to sequence length.
The result is \textbf{SpeechSSM}, a new (textless) spoken language model designed for long-form generation.
To our knowledge, this is first to model and generate unbounded long-form speech in bounded memory, as well as being the first state-space spoken LM.
As a baseline, we also train spoken Transformer LMs to perform multi-minute generations.
Finally, we also describe \textbf{SpeechSSM-X}, an extemporaneous variant that produces naturalistic spontaneous speech.

**Evaluation**.

We observe that existing metrics in speech generation evaluation are noisy and poorly discriminative, and propose the use of reference-based semantic metrics, side-by-side LLM-as-judge, and time-stratified evaluations for speech generation.
To scale these to long-form evaluation, we introduce the \textbf{LibriSpeech-Long} benchmark, derived from raw LibriSpeech \cite{panayotov2015librispeech} dev and test sets.
Since original LibriSpeech dev and test sets are segmented into 10s, we reprocess the chapter-level raw audio files into longer 4-minute utterances.
LibriSpeech-Long enables extended prompts and provides 4-minute reference ground truth, facilitating reference-based evaluation for long-form speech continuation as well as future long-form speech tasks.

We find that SpeechSSM matches existing speech LMs in the short-time horizon, while greatly outperforming their sliding window-based extensions on long-form spoken audio generation (e.g.,  \Cref{fig:transcript}).
Furthermore, we find that our proposed metrics and benchmark help quantify the semantic quality gaps between past work, our work, and human-level speech generation, enabling future model improvements.
SpeechSSM matches a comparable Transformer while having constant memory consumption and per-token compute.
We release examples at https://google.github.io/tacotron/publications/speechssm/ of read- and extemporaneous-style generations of up to 16 minutes in length and the [LibriSpeech-Long [Github]](https://github.com/google-deepmind/librispeech-long/) evaluation dataset.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论