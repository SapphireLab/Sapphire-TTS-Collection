# SpeechSSM

<details>
<summary>基本信息</summary>

- 标题: "Long-Form Speech Generation with Spoken Language Models"
- 作者:
  - 01 Se Jin Park,
  - 02 Julian Salazar,
  - 03 Aren Jansen,
  - 04 Keisuke Kinoshita,
  - 05 Yong Man Ro,
  - 06 RJ Skerry-Ryan
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.18603)
  - [Publication]()
  - [Github]()
  - [Demo](https://google.github.io/tacotron/publications/speechssm/)
- 文件:
  - [ArXiv](_PDF/2412.18603v1__SpeechSSM__Long-Form_Speech_Generation_with_Spoken_Language_Models.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

We consider the generative modeling of speech over multiple minutes, a requirement for long-form multimedia generation and audio-native voice assistants.
However, current spoken language models struggle to generate plausible speech past tens of seconds, from high temporal resolution of speech tokens causing loss of coherence, to architectural issues with long-sequence training or extrapolation, to memory costs at inference time.
With these considerations we propose ***SpeechSSM***, the first speech language model to learn from and sample long-form spoken audio (e.g., 16 minutes of read or extemporaneous speech) in a single decoding session without text intermediates, based on recent advances in linear-time sequence modeling.
Furthermore, to address growing challenges in spoken language evaluation, especially in this new long-form setting, we propose: new embedding-based and LLM-judged metrics; quality measurements over length and time; and a new benchmark for long-form speech processing and generation, LibriSpeech-Long.
Speech samples and the dataset are released at [this https URL](https://google.github.io/tacotron/publications/speechssm/).

## 1·Introduction: 引言

Generative spoken language models \citep{lakhotia2021generative, dieleman2021-variable, oord2017-vqvae} are autoregressive (AR) models of invertible audio representations, enabling the direct learning and generation of intelligible speech and its paralinguistic aspects, such as prosody \citep{kharitonov2022-pgslm} and turn-taking \citep{nguyen2023generative}.
These capabilities make speech LMs promising for applications like media understanding and co-creation, audio-native voice assistants, and textless NLP.
However, real-world use-cases of spoken language models require the ability to both understand and generate long-form audio.
For example, voice interactions can last many minutes, requiring a model to maintain a growing conversational history in real time, and expressive media like audiobooks and podcasts can require semantic, paralinguistic, and speaker coherence over a chapter or episode.

This presents significant challenges for existing spoken language models; spoken audio is inherently complex, as its textual semantic content is entangled with paralinguistic content and acoustic properties that may detract from learning higher-level speech features.
Furthermore, the audio representations used have high temporal rates; it can require 10+ speech tokens to cover the duration of a spoken word \citep{hassid2023textually}.
Hence, models must retain and aggregate semantics over a longer time horizon, as well as generate content that is coherent over the same horizon.
This is difficult for the vanilla Transformer \citep{vaswani2017-transformer} in implementation, as its initial cost grows quadratically with prompt length, and its per-step cost grows linearly with decoding length; and difficult in modeling, as suggested by lackluster performance on long-range dependency tasks \citep{tay2021-lra}.
Though a few works have improved speech coherence via joint modeling with text (\Cref{sec:related-work}), the challenges of directly modeling long-form speech, particularly generation, remain unstudied by existing work on spoken language models (\Cref{fig:prior_work}).
Finally, the novelty of generating long-form speech means that analyses and evaluations of such generations have not been studied to date.
Our work proposes and make initial progress on the topic of generative long-form speech:

**Modeling**.

We discuss the design choices required to enable the practical training, generation, and extrapolation to tens of minutes of audio, from tokenization to speaker conditioning to complexity with respect to sequence length.
The result is \textbf{SpeechSSM}, a new (textless) spoken language model designed for long-form generation.
To our knowledge, this is first to model and generate unbounded long-form speech in bounded memory, as well as being the first state-space spoken LM.
As a baseline, we also train spoken Transformer LMs to perform multi-minute generations.
Finally, we also describe \textbf{SpeechSSM-X}, an extemporaneous variant that produces naturalistic spontaneous speech.

**Evaluation**.

We observe that existing metrics in speech generation evaluation are noisy and poorly discriminative, and propose the use of reference-based semantic metrics, side-by-side LLM-as-judge, and time-stratified evaluations for speech generation.
To scale these to long-form evaluation, we introduce the \textbf{LibriSpeech-Long} benchmark, derived from raw LibriSpeech \cite{panayotov2015librispeech} dev and test sets.
Since original LibriSpeech dev and test sets are segmented into 10s, we reprocess the chapter-level raw audio files into longer 4-minute utterances.
LibriSpeech-Long enables extended prompts and provides 4-minute reference ground truth, facilitating reference-based evaluation for long-form speech continuation as well as future long-form speech tasks.

We find that ***SpeechSSM*** matches existing speech LMs in the short-time horizon, while greatly outperforming their sliding window-based extensions on long-form spoken audio generation (e.g.,  \Cref{fig:transcript}).
Furthermore, we find that our proposed metrics and benchmark help quantify the semantic quality gaps between past work, our work, and human-level speech generation, enabling future model improvements.
***SpeechSSM*** matches a comparable Transformer while having constant memory consumption and per-token compute.
We release examples at https://google.github.io/tacotron/publications/speechssm/ of read- and extemporaneous-style generations of up to 16 minutes in length and the [LibriSpeech-Long [Github]](https://github.com/google-deepmind/librispeech-long/) evaluation dataset.

## 2·Related Works: 相关工作

### Generating with Spoken LMs

The family of GSLM models \cite{lakhotia2021generative} is a Transformer decoder LM trained on discrete units obtained from $k$-means clustering of HuBERT \cite{hsu2021hubert} features.
Reconstruction involved a unit-to-spectrogram model whose outputs are then synthesized with a pre-trained neural spectrogram vocoder, though this was later simplified to a unit-to-waveform GAN \cite{kharitonov2022-pgslm}.
Finding this approach gave promising temporal coherence but poor audio quality, AudioLM \cite{borsos2023audiolm} proposed learning separate LMs, one for semantic tokens as before, and two for modeling coarse-to-fine acoustic tokens conditional on a prefix of fine acoustic and semantic tokens, where acoustic tokens are the hierarchical discrete codes of a neural audio codec (SoundStream; \citealp{zeghidour2021soundstream}).
The acoustic stages were later simplified and made non-autoregressive by SoundStorm \cite{borsos2023soundstorm}.

TWIST \cite{hassid2023textually} found that initializing with a text LM improved content-level semantic coherence, atop which VoxtLM \citep{maiti2023voxtlm} and Spirit LM \citep{nguyen2024spirit} found that joint and interleaved training with text gave further improvements.
However, all these works evaluated 10s generations, with only Spirit LM releasing (but not assessing) a generation above 30s.
Though models with two speech channels and models that sample text intermediates are out of scope, we note that dGSLM \cite{nguyen2023generative} generated 90s dialogues given 30s prompts, though its semantic and human evaluations were restricted to 50-word or 20s continuations, and Moshi \citep{defossez2024moshi} extends two-channel models with hierarchical sampling via time-aligned text tokens to give 5-minute dialogue interactions.

### State-Space Models for Long-Form Audio

State-space models (SSMs; \citealp{gu2021combining}) have become popular among efficient (sub-quadratic) replacements for Transformer-based architectures, giving the first model (S4; \citealp{gu2021efficiently}) to perform all tasks in the Long-Range Arena \citep{tay2021-lra}, outperforming the vanilla Transformer.
They utilize constant computation and memory requirements to generate tokens during inference and can be efficiently trained.
Recent focus has shifted to hybrid models \cite{glorioso2024zamba, lieber2024jamba, de2024griffin} which integrate state-space layers and variants like linear recurrent units (LRU; \citealp{orvieto2023-lru}) with finite-context self-attention layers.

Recent works have considered SSMs in audio, primarily to support long speech \textit{inputs} for text-out tasks like automatic speech recognition (ASR) and summarization.
None are spoken LMs for speech continuation, with only one considering (acoustic-level) tokens \citep{gao2024speech}; most works involve spectrogram encoders or outputs \cite{shams2024ssamba, erol2024audio, lin2024audio, miyazaki2024exploring}.
Closest in spirit is SaShiMi \citep{goel2022s}, a multi-scale S4 operating directly on waveform samples; though they generate only 1s of speech, this corresponds to a sequence of 16k scalars.
For music, they also study generation-time extrapolation from 8s to 16s.
One can view our work as a difference in scale, with tokens instead of samples as the unit (our 16-min generation corresponds to a sequence of 24k token IDs).

### Evaluating Spoken LM Generations

\citet{lakhotia2021generative} was first to evaluate the generations of spoken LMs, proposing ASR as a path to automated text metrics like text perplexity (PPL) and proportion of repeated $k$-grams (auto-BLEU), along with human evaluations of intelligibility and meaningfulness with mean opinion scores (MOS and MMOS respectively).
For their spoken LMs, zero-shot (non-generative) metrics based on logprobs of contrastive pairs (sWUGGY and sBLIMP; \citealp{nguyen2020zero}) were predictive of generation performance, though they note scores vary with token vocabulary size.

However, these initial metrics seem to lack robustness or are saturating with respect to newer spoken LMs.
\citet{hassid2023textually} found transcript PPL and auto-BLEU to be noisy, favoring MMOS and expanding zero-shot metrics (sStoryCloze and tStoryCloze).
In turn, \citet{defossez2024moshi} found that performance on zero-shot sWUGGY and sBLIMP metrics degraded despite continued experiential improvement from noisy-condition training and instruction-tuning, instead favoring spoken question answering (SQA) tasks \citep{nachmani2023spoken} evaluated via ASR.
Closest to our work was the use of LLMs to assign absolute, reference-free scores to assess the instruction following of turn-based text-and-speech LMs in \citet{zhang2023speechgpt, zhang2024speechagents}.

As for saturation, \citet{borsos2023audiolm} found that humans could not distinguish between a synthetic 7s continuation versus the real 7s continuation of a 3s prompt on a holistic side-by-side evaluation, suggesting the need for more targeted and longer-form evaluations.

## 3·Methodology: 方法

### Unbounded Speech Generation

We begin by proposing a set of requirements for a general, unbounded, speech generation system:

- \textbf{Constant memory during decoding}, to enable indefinite AR sampling without running out of memory.
- \textbf{Infinite context}, so that arbitrarily distant dependencies can be (theoretically) expressed.
To be concurrent with the above, necessary past context must fit in a fixed-size state.
- \textbf{Generative length extrapolation}, so that speech quality remains consistent over time, in particular beyond audio durations seen during training.

The first leads us to linear-complexity sequence modeling with a fixed-size state.
The second leads us to models with aggregation mechanisms such as recurrences or compressive memories.
We show that with some care, one can also achieve the third requirement of generative extrapolation.
Finally, beyond these requirements, there is also a practical desire for efficient training (e.g., train-time dependence on sequence length that is subquadratic, to enable longer sequences and reduce reliance on extrapolation), requiring a parallel scheme for weight learning.
This leads us to state-space models, broadly defined (i.e., including linear recurrence models and certain hybrid variants; \citealp{patro2024mamba360, dao2024transformers}) and thus ***SpeechSSM***, a hybrid state-space speech language model for efficient long-form speech generation that fulfills all these desiderata.
Our key design choices are below.

#### Architecture

For our decoder-only hybrid SSM we choose Griffin \cite{de2024griffin}, which interleaves a gated variant of LRUs \cite{orvieto2023-lru} and local (sliding-window) multi-query attention (MQA) blocks in a fixed pattern (two recurrent, one local-MQA; see \Cref{fig:architecture}, left).
Local attention efficiently captures recent context, while the states of the gated recurrences transmit information across arbitrary distances.
Griffin's performance matched that of comparable Transformer LMs while significantly improving inference speed and enabling context-side extrapolation of at least 4x longer than seen in training.
Observing that RoPE \cite{su2024roformer} in the local-MQA blocks still encodes absolute position, we follow recent work on position embeddings (PEs) under causal self-attention (NoPE; \citealp{kazemnejad2024impact}) and remove all explicit PEs from ***SpeechSSM*** to promote extrapolation.

#### Initialization

Inspired by \citet{hassid2023textually}'s success with text-initialized spoken language models (TWIST), we initialize our model with RecurrentGemma-2B IT \cite{botev2024recurrentgemma}, an open-weight LM with the Griffin architecture, trained on 2 trillion text tokens.
We discard the pretrained text token embeddings and initialize new ones for our audio token vocabulary.

Following \citet{borsos2023audiolm}, we generate low-level acoustic tokens of a neural audio codec, conditioned on semantic tokens output from our ***SpeechSSM***.

#### Semantic Tokenizer

We use the pretrained USM-v2 speech tokenizer \citep{vashishth2024stab, rubenstein2023audiopalm} to give our semantic tokens.
The USM speech encoder \citep{zhang2023-usm} is trained with a masked language model (MLM) loss on untranscribed audio and an auxiliary automatic speech recognition (ASR) objective on transcribed audio.
Then, its intermediate embeddings are discretized using vector quantization to give 32k units that serve as fixed-rate (25Hz, or 40ms/token) pseudo-text for our speech LM.
\citet{vashishth2024stab} found that USM-v2 was by far the most speaker-invariant token out of a suite of common speech tokenizers.

#### Semantic-to-Acoustic Generation

We use a SoundStorm model \cite{borsos2023soundstorm} trained with USM-v2 conditioning to non-autoregressively generate SoundStream tokens \cite{zeghidour2021soundstream}, a neural audio codec that efficiently reconstructs to high-quality audio.
It is trained to support 3-second voice prompts (represented as a frozen prefix of semantic and acoustic tokens), such that output acoustic tokens reflect speaker characteristics.
By choosing a token and model decomposition that isolates speaker characteristics to the acoustic stage, ***SpeechSSM*** can focus capacity on modeling semantic coherence along the temporal axis.

#### Windowed Tokenization and Decoding

To process long-form speech while bounding the memory requirements of the (non-SSM) semantic tokenizer and acoustic decoder, we employ an overlapping window approach.
Specifically, the audio is divided into 30-second segments, with an overlap of 4 seconds with its neighboring segments.
Each 30-second window is tokenized independently.
Then, the windows are merged into a continuous stream by taking, at each overlap the first two seconds of boundary tokens from the preceding window and the last two seconds of tokens from the succeeding window (\Cref{fig:windowing}a).
Likewise, during decoding, the 30-second token windows are decoded independently, then merged with the same boundary overlap adjustment (\Cref{fig:windowing}b).
Although windowing operates at the audio level and merging occurs at the token level (and vice versa), we qualitatively find our approach minimizes the boundary artifacts while enabling tokenization and decoding of long and continuous speech.

#### Avoiding Implicit EOSes

We do not train with an end-of-sequence (EOS) token.
Despite this, our initial models generated only slightly beyond the training length (e.g., a 4min model reaching $\sim$4.5min before degrading to noise/silence).
As we found in non-causal semantic tokenizers like USM-v2, the length of the window can be encoded in the tokens of that window, making ends of training examples look ``different'' and act as implicit EOSes.
As evidence, padding the last window to 30s using silence, tokenizing, then dropping the extra tokens did not help, likely because the continuation being silence was also encoded in the kept tokens.
What worked was (1) to pad the last window to 30s using speech from the beginning of the example (depicted in \Cref{fig:windowing}a) so that final tokens were tokenized \text{as if} there was further speech, and (2) in the case of LibriLight, to still drop the last 10s of examples -- as the padded beginnings were disproportionately "Chapter \textit{<number>}"!

### Improved Evaluations for Spoken LMs

#### Updated NLG Evaluations

The evaluation shortcomings found recent in work (\Cref{sec:related-work-evals}) turn us to recent developments in natural language generation (NLG) evaluation, which have moved beyond intrinsic metrics like PPL and surface word-based metrics like auto-BLEU and self-BLEU \citep{zhu2018-selfbleu}, especially for open-ended generation.
One major shift has been the adoption of \textbf{embedding-based metrics}, where distances are computed between neural word or sentence-level embeddings of generated versus reference text \citep{sai2022survey}.

A more recent trend has been the use of instruction-tuned LLMs to perform automated evaluations \citep{li2024leveraging}.
This can occur in the form of Likert-scale absolute evaluations, as applied by \citet{zhang2023speechgpt, zhang2024speechagents} to text-instructed speech generation.
This evaluation method could be extended to assess speech-only continuations as well.
However, to tackle the growing issue of discriminability and to better leverage ground truths, we in particular propose \textbf{automated side-by-side evaluations}  (LLM-as-a-Judge; \citealp{zheng2023-llm-as-a-judge}) to scalably evaluate and rank systems against the ground truth and against each other.
This has particular advantages for spoken LMs.
First, it mitigates the noise from ASR issues highlighted by \citet{hassid2023textually}, as the compared generations will be both afflicted (when comparing versus the ground truth, one should (re)transcribe it for fairness).
Second, it also works around a subtle issue with the standard fixed-duration setup where slices often occur mid-word, which may cause the word to drop or appear twice between the prompt's and continuation's transcripts, degrading text PPLs; instead, one can transcribe prompt and continuation together, leaving the LLM judge to focus on the contrasts.

We describe our implementations of both evaluation types in \Cref{sec:proposed-semantic} for short-form and in \Cref{sec:long-form-exps} for long-form.

#### LibriSpeech-Long

To extend reference-based NLG metrics to long-form speech, we require a benchmark that provides long-form reference audio and transcripts.
Although 3s prompts from LibriSpeech \citep{panayotov2015librispeech} have been the standard benchmark for spoken LMs since \citet{lakhotia2021generative}, their provided references have an average length of 10s, making them unsuitable as references for anything beyond 7-10s continuations.
Observing that the LibriSpeech dev and test sets are derived from full chapters of public-domain audiobooks which have been excluded from the standard LibriLight training set \citep{kahn2020libri} used by many spoken LMs, we reprocess their source uncut audio files similar to the convenience script\footnote{\url{https://github.com/facebookresearch/libri-light/blob/main/data_preparation/cut_by_vad.py}} provided with LibriLight.
This agglomerates utterances up to a target length of 4 minutes (240s) along sentence-aligned timestamps.
These segments enable longer prompts (10s in this work), and longer references for reference-based side-by-side and similarity metrics.
The resulting statistics are shown in Table \ref{table:statistics}; 64\%--76\% of each split's examples are 3--4min long.

### Generation Quality over Time

While ASR can capture degenerate cases like repeated words (\Cref{fig:transcript}), we found that it can fail on cases exacerbated by long-form generation, such as extended silences and voiced non-speech, suggesting the continued importance of audio-native evaluations.
Furthermore, we find that generation failures generally increase as decoding progresses over time, which we must quantify to determine if our model has the desired property of generative length extrapolation (\Cref{sec:unbounded-speech-gen}).
Towards this, we propose computing \textbf{semantic and acoustic metrics that are stratified over the decoding process}.
This progression can be expressed in terms of semantic content (number of words into the ASR transcript), or acoustic content (time offset into the generated speech).
We describe our implementations of both in \Cref{sec:proposed-longform}.

## 4·Experiments: 实验

### Training and Generation

Following \citet{borsos2023audiolm, nachmani2023spoken} and others, we train on the unlab-60k split from LibriLight \citep{kahn2020libri}.
Unlike prior work, we study the effect of sequence length on long-form generation, segmenting the audiobooks into training sequences of up to one of 30s, 4m (240s), and 16m (960s) in duration.
For each preprocessed duration, we train a model on 16 TPU v5p chips using only data parallelism for 10k steps, with 768k tokens per batch, and select the best checkpoint using LibriSpeech-Long's dev sets.
This gives three models: \textbf{SpeechSSM-30s}, \textbf{SpeechSSM-4min}, and ***SpeechSSM***-16min (which we refer to simply as \textbf{SpeechSSM}).
More training details are in \Cref{sec:further-training}.

At each step, we sample over the full semantic (USM-v2) vocabulary with temperature 1.0.
For each window of resulting semantic tokens, we use a SoundStorm model to pass to acoustic (SoundStream) tokens, and a SoundStream codec to pass to waveform; see \Cref{sec:further-model-details} for details.
SoundStorm is speaker-prompted with the acoustic tokens of the first 3 seconds of the prompt to be continued.

### Baselines for Comparison

We compare ***SpeechSSM*** with state-of-the-art spoken language models.
For \textbf{GSLM} \cite{lee2021textless}, we use their best-performing HuBERT-L6 model with 200 token units, trained on the LibriLight-60k dataset.
For \textbf{TWIST} \cite{hassid2023textually}, we use both the OPT-1.3B and LLaMA-7B versions, trained on 150k speech hours.
For the 7B \textbf{Spirit LM} \citep{nguyen2024spirit}, we use the \textbf{Expressive} version model which enables expressive speech generation by using pitch and style tokens in addition to semantic HuBERT tokens.
We also cite numbers from \textbf{AudioLM} \citep{borsos2023audiolm} and \textbf{VoxtLM} \citep{maiti2023voxtlm}, which are both sub-2B models trained on LibriLight-60k.
VoxtLM and Spirit LM see text data during training.

We note that there is great variation in token, initialization, and training data choices.
Hence, to make the fairest comparison to ***SpeechSSM***, we construct a Transformer decoder-only model, namely \textbf{SpeechTransformer}, initialized with Gemma-2B\footnote{\citet{botev2024recurrentgemma} note that Gemma-2B was trained with 50\% more tokens than our RecurrentGemma-2B initialization.} \citep{team2024gemma}.
We only trained 30s and 4min variants, as training a Transformer on 16min sequences was not feasible with the same compute.

## 5·Short-Form Continuation Experiments

Before we consider the problem of long-form generation, we evaluate how ***SpeechSSM*** performs in existing short-form evaluation regimes compared to Transformer-based predecessors.
As in past work, we take 3s prefixes from LibriSpeech's test-clean set \cite{panayotov2015librispeech} and generate 7s continuations.
We used test examples with ground-truth continuations $\ge$5s.

For text-based evaluations, we transcribe the generated speech to enable the application of natural language generation (NLG) metrics \citep{lakhotia2021generative}.
Unless stated otherwise, we use wav2vec2-base-960h \cite{baevski2020wav2vec} for ASR, applied on 180s windows.
To reduce cost and to mitigate length/duration as an indicator for ground truth, for more intensive evaluations (N-MOS, side-by-sides) we subselect 200 test-clean utterances with ground-truth continuations $\ge 7$s.

\textbf{Transcript Perplexity (PPL).} As in prior work, we compute the log-perplexity of the transcript of the generated continuation under Gemma-2B, as an initial proxy for content fluency.

\textbf{Speaker Similarity (SpkrSim).} To analyze voice preservation, we speaker-embed both the prompt and its generated continuation and compute their cosine similarity.
We use a speaker classifier used in AudioLM \cite{borsos2023audiolm} as the speaker embedder.

\textbf{Naturalness Mean Opinion Score (N-MOS).} Following \citet{nachmani2023spoken}, we evaluate how natural the speech sounds.
Raters are instructed to ignore the grammar and content of the utterance; this focuses attention on issues not visible on transcripts, ranging from synthesis issues and unintelligible speech through to coherent but unnatural prosody.\footnote{This subsumes (intelligibility) MOS as originally done in GSLM \citep{lakhotia2021generative} by asking the listener to consider everything outside the grammar and content of the utterance.} MOS instructions and rater pool details are provided in \Cref{sec:mos-instruction}.

\textbf{sWUGGY and sBLiMP.} These benchmarks \citep{nguyen2020zero} probe whether the spoken language model can implicitly perform lexical and syntactic contrasts, respectively.
In sWUGGY, a real word and a fake but similar-sounding word are synthesized using the same voice.
In sBLIMP, a syntactically correct sentence and a syntactically incorrect one are synthesized similarly.
One reports the \% of time the model's log-likelihood ranks the semantic tokens of the correct utterance over the incorrect utterance's.

### 5.1·Proposed Metrics

As motivated in \Cref{sec:updated-nlg}, we propose two new metrics:

\textbf{Semantic Similarity (SBERT).} We measure the distance between the semantic embedding of the transcriptions of the generated speech and the reference, using Sentence-BERT \cite{reimers2019sentence}\footnote{\url{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}} as the semantic embedder.
This expresses contextual alignment between the generated text to the ground truth, focusing on semantic meaning over surface-form patterns.


\textbf{Side-by-Side Win Rates (Win\%$_\text{vs.\,model}$).} We ask the model to analyze \text{then} rate \citep{chiang2023-analyze-rate}, forking from the overall format of Arena-Hard-Auto's LLM-Judge System Instruction \citep{li2024-arena-hard}.
Given the audiobook domain and the relatively unconstrained nature of speech continuation, we define our criteria based on questionnaires from story generation evaluation \citep{xia2023-storytelling} around \textit{fluency}, \textit{coherence}, \textit{logicality}, and \textit{interestingness}.
See \Cref{sec:llm-judge-prompt} for the evaluation prompt with sample query and reply.
We evaluate each prompt twice, with the order of the speech completions flipped.
We use Gemini 1.5 Pro-002 \cite{team2024gemini} to retranscribe the generated and ground-truth audio (without windowing, and jointly with their prompts), and to perform judgments.

### 5.2·Results & Discussion: Existing Metrics

We see in Table \ref{table:shortform_eval} that ***SpeechSSM*** and SpeechTransformer's continuations have highest speaker similarity with the prompt, which we attribute to the choice of having a speaker-promptable acoustic stage; in contrast, GSLM and TWIST can only propagate speaker identity via their semantic tokens (plus coarse style and pitch tokens in Spirit LM Expressive).
Our N-MOS scores suggest high semantic-to-audio synthesis quality, which we attribute to USM-v2's large vocabulary (32k) and passing to existing acoustic tokens (SoundStream), respectively increasing the expressivity and focusing the capacity of the synthesizing model (SoundStorm).
SpeechSSM's naturalness is on par or better than a comparable Transformer, and very close to real speech; see samples.

Meanwhile, our sWUGGY scores are much worse, while our sBLiMP scores are neutral to above-average.
These do not positively correlate with the text-based scores or subjective quality; however, they are consistent with \citet{lakhotia2021generative}'s observations and \citet{borsos2023audiolm}'s Figure 2, where sWUGGY scores hit a relatively sharp maxima at vocabularies of a few hundred tokens.
That is, these probing tasks are highly sensitive to vocabulary size, and our work is likely first to use them on a 5k+ vocabulary.
Furthermore, one would expect increased noise with larger vocabulary, as the contrasted spoken words/phrases represent less and less of the probability mass of possible renditions of the underlying text.
Hence, and for our own reason separate from \citet{defossez2024moshi}'s, we suggest looking beyond these benchmarks and towards transcript-based evaluations, which from this perspective can be seen as \textit{marginalizing} over spoken renditions to get less noisy evaluations.

Finally, ***SpeechSSM***s had the lowest transcript perplexities; however, this is surprising given the other models are larger (7B) and/or have jointly trained with text, and one would not expect SSMs to confer a semantic edge in the $\le$10s speech horizon.
We note that \citet{lakhotia2021generative} has already caveated -- and \citet{hassid2023textually} has actively discouraged -- the use of ASR PPL given its sensitivity to e.g.\ audio sampling temperature; such metrics may simply indicate model repetitiveness at default settings.

### 5.3·Results and Discussion: Proposed NLG Metrics

The above points (noise from contrastive audio probes, saturation of N-MOS and SpkrSim, suspicious results from transcript perplexity) \textbf{all validate our proposed shift to newer, reference-based NLG metrics}.
Table \ref{table:shortform_eval}'s SBERT scores show our continuations are (slightly) semantically closer to the true continuations.
The benefit is most evident in the case of a  side-by-side versus a transcript of the ground truth, with results in line with expectations (larger models performing better, with Spirit LM Expressive underperforming TWIST perhaps from spending capacity on pitch and style).

Notably, transcripts of the best model (Spirit LM) only achieved a win-rate of 17\% versus a transcript of the ground truth, suggesting that (automated) \textbf{side-by-side comparison on transcripts was more discriminative than a holistic human side-by-side audio task} in selecting the synthetic sample, where humans performed at random in \citet{borsos2023audiolm}.
This is likely as it isolates focus on the content of the speech rather than its auditory naturalness.

## 6·Long-Form Generation Experiments

We conduct the first evaluation of long-form generation using our proposed LibriSpeech-Long benchmark.
We take extended prompts of 10s from LibriSpeech-Long test-clean and have each model continue them through to 4 or 16 minutes of speech.

### 6.1·Modifications for Evaluation

Recall that other off-the-shelf models trained on sequences well below 4 minutes and were not explicitly designed to generate beyond their training length (e.g., Transformers' use of position encodings.
As-is, these models are unable to generate beyond a minute without being stuck in noise, or silence.
To extend these models into functional baselines, we adapt the ``slide-and-prompt'' windowing strategy (\Cref{fig:windowing}b) to semantic-semantic modeling; specifically, we generate to each model's maximum completion length (\Cref{fig:prior_work}) to give a first window, and then prompt the model to generate further windows using the final 3 seconds of each's preceding window as context.

As in short-form evaluation, we measure transcript perplexity, semantic similarity, speaker similarity, and side-by-side win rates.
In this section, we replace Sentence-BERT with Gecko \citep{lee2024gecko} as the embedder for semantic similarity as Sentence-BERT's 512-token context cannot handle the transcripts of 4min generations.
Gecko is a text embedding model trained on a variety of tasks including document retrieval, semantic similarity, and classification using long passages, and is more suited for extracting semantic embedding of long texts in open-domain generation.
The choice of task is given via prompting; we use `search results' as the prompt which was used to train on clustering tasks.
For win-rates and MOS computations, to mitigate length bias we only consider examples $\geq$3.5min (71\% of test-clean).

### 6.2·Additional Proposed Metrics

As motivated in \Cref{sec:quality-over-time}, we propose two long-form-specific metrics:

\textbf{Mean Opinion Score over Time (MOS-$T$).} To balance cost and informativeness, for each example we select a 5 sec.\ span from each minute; specifically [$t_\text{prompt}$, 60), [60, 120), [120, 180), and [180, $t_\text{max}$) where $t_\text{prompt}$ is prompt duration and $t_\text{max}$ is ground truth duration, and extract each span's audio from every model's generated continuation.

\textbf{Semantic Coherence over Length (SC-$L$).} To evaluate semantic faithfulness to the original prompt over time while mitigating the effect of speech rate, we take each continuation's transcripts and divide them into spans of 200 words (as determined by whitespace).
Each 200-word segment represents approximately 1 minute of speech.
Our SC scores are the cosine similarities between the embedding of the original prompt $\mathbf{e}_\text{prompt}$ and the embedding of each 200-word segment $\mathbf{e}_{200n : 200(n + 1)}$.

### 6.3·Semantic and Acoustic Results

In Table~\ref{table:longform_eval}, we evaluate 4-minute long-form speech generation using the same speech- and text-based metrics as in short-form evaluation, as well as our proposed MOS-$T$.
Note that only Spirit LM, SpeechTransformer, and ***SpeechSSM*** were trained on sequences on the order of multiple minutes.

The speaker similarity of SpeechTransformer and ***SpeechSSM*** remain high, and have actually increased from 0.79 in the short-form setting (\Cref{table:shortform_eval}) to 0.89, likely from increased confidence over a longer prompt and continuation; the same holds for the ground truth.
In contrast, all other models have \textit{decreased} in speaker similarity, likely from speaker divergence over time.
This shows the advantage of our use of more speaker-invariant USM-v2 tokens and a speaker-prompted semantic-to-audio process, as speaker identity is primarily modulated by the semantic-to-acoustic model rather than consuming capacity and being imperfectly carried by the semantic LM and windowing.

***SpeechSSM*** achieves the best transcript perplexity and wins versus all other models, with roughly the same semantic similarity to the reference as SpeechTransformer.
Furthermore, the gap between ***SpeechSSM*** and the Transformer baseline widens when evaluating on 16min completions (Table~\ref{table:16min_eval}), which \textbf{may even suggest an active edge for SSM-based architectures in long-form modeling} (in line with past work like \citealp{gu2021efficiently}).

In side-by-side 4-minute evaluation, ***SpeechSSM*** wins a majority of the time over all baselines.
However, the models achieve effectively zero wins over the ground truth.
This is likely because as generation length increases, faults in fluency, coherence, logicality and interestingness become increasingly apparent; in all, showing that \textbf{side-by-side comparison versus LibriSpeech-Long ground-truths as a new and difficult benchmark} for (long-form) spoken language generation.

### 6.4·Extrapolation over Time

\textbf{Existing models do not generate past training.} The results of Table~\ref{table:longform_eval} and Table~\ref{table:16min_eval} suggest that windowed model extension is still a poor length extrapolator.
This can be seen explicitly through our proposed stratified metrics.
We see that MOS-$T$ scores are already low for GSLM and TWIST in the first minute, as they were trained on even shorter sequences.
Spirit LM, which has seen sequences up to 200s, also degrades in acoustic quality over time, though much slower.
This can seen as a failure to extrapolate, as Spirit LM trains on interleaved speech and text segments; unbroken strings of 200s of audio are still very rare.
Qualitatively, we see that Spirit LM has these acoustic/semantic issues, while ***SpeechSSM*** generates intelligible and (loosely) coherent speech over the 4min duration.
The earlier Figure \ref{fig:transcript} highlighted this capability, showing ongoing reference to the "Philip" mentioned in the prompt throughout the continuation.
Other entities like "Prince Maria", "Prince Albert", and "Horace Barrows" also consistently appear, maintaining contextual relevance Our proposed SC-$L$ is plotted for 4 minutes in \Cref{fig:SC-L-4min}, and 16 minutes in \Cref{fig:SC-L-16min}.
As the generation progresses, semantic similarity with the prompt decreases, as reflected by the gradual decline in ground-truth SC scores; this aligns with the natural flow of speech, where content starts from a topic and gradually diverges over time.
However, the comparison models exhibit a sharp drop in semantic coherence (SC) scores around 200 words (approx.
1min, which is around their training lengths) and plateaus thereafter.
In contrast, ***SpeechSSM*** maintains high SC further into the generation, closest to the ground truth where available, i.e., in the 4 min case.

\textbf{***SpeechSSM*** generalizes best past training.} In the 16min results (\Cref{table:16min_eval}, \Cref{fig:SC-L-16min}), we see no significant performance drop when ***SpeechSSM***-4m undergoes 16min evaluations, i.e., generates beyond its training length.
As an additional check, we computed the average number of words generated by ***SpeechSSM***-4m and ***SpeechSSM*** to be 3k and 2.9k respectively.
In contrast, SpeechTransformer initially achieves higher SC scores up to its training length of 4m but noticeably declines afterwards, falling below ***SpeechSSM***.
In all, these demonstrate that ***SpeechSSM***'s design and architecture lead to continuing performance during extrapolative generation.
See our demo page for 16min generations by the ***SpeechSSM***-4min model.

### 6.5·Sampling Efficiency

We evaluate sampling efficiency in terms of throughput and latency.
Throughput measures the maximum number of tokens that can be successfully decoded per second given fixed memory, e.g.
by increasing the batch size, while latency measures the time it takes to decode to a fixed sample length.

In \Cref{fig:throughput}, we see that ***SpeechSSM*** attains higher throughput due to its recurrent layers that maintain a constant-size state, unlike self-attention layers that must attend to all previously generated tokens as decoding proceeds.
As sample lengths increase, so does the memory use of self-attention, decreasing the maximum batch size and hence the throughput of both systems.
However, ***SpeechSSM***'s throughput remains constant and does not decrease as its self-attention is bounded to a sliding window of size 2048, confirming its bounded memory use and thus its capability of unbounded speech generation.
This advantage becomes clear when compared in a relative scale; see \Cref{fig:throughput_speedup}, which shows that ***SpeechSSM*** can attain >120x the throughput of SpeechTransformer if decoding 16k-token sequences in batch.

In \Cref{fig:latency}, we see the time taken to decode a single generation based on sample length.
Given the relative speedup shown here, we conclude that ***SpeechSSM***'s higher throughput is not just from enabling higher batch sizes, but also from its increased speed per step.
On the TPU v5e, the 2B ***SpeechSSM*** decodes 16384 tokens (roughly 10.9 minutes) in just over 100 seconds, a real-time factor well under 0.2x.

### 6.6·Extemporaneous Speech Generation

With few exceptions (dGSLM, Spirit LM Expressive), most spoken LMs are trained on read speech such as LibriLight.
However, long-form multimedia and assistant applications likely require modeling of spontaneous speech, which has its own long-term discourse structures (e.g., podcasts; \citealp{nishimura2024halle}).
Hence, we also developed \textbf{SpeechSSM-X}, a 2B ***SpeechSSM*** model trained on a 216k-hour corpus of eXtemporaneous monologues (see \Cref{sec:speechssm-x-details} for details).
We find that it is able to generate natural monologue speech in a more informal, extemporaneous style, while showing similar coherence at the multi-sentence level; see our website for examples.

## 7·Conclusions: 结论

In this work, we considered the task of generative modeling of long-form speech.
For modeling, this led us to ***SpeechSSM***, the first spoken LM that allows for generation than can go indefinitely without running out of memory.
For evaluation we created the LibriSpeech-Long benchmark and proposed new evaluations for long-form speech continuation.
Our analyses show that ***SpeechSSM*** matches speech LMs on short horizons while greatly outperforming them and a Transformer baseline on multi-minute generations.
We hope our work will simplify and enable new audio generation applications involving long-form media, such as audiobooks, podcasts, and video-related content without having to decompose into chunkwise processing out of necessity.
Future work could apply methods from text-intermediate and joint speech-text models to further improve the semantic quality of ***SpeechSSM***'s generations, especially over long contexts.