# SpeechSSM

<details>
<summary>基本信息</summary>

- 标题: "Long-Form Speech Generation with Spoken Language Models"
- 作者:
  - 01 Se Jin Park,
  - 02 Julian Salazar,
  - 03 Aren Jansen,
  - 04 Keisuke Kinoshita,
  - 05 Yong Man Ro,
  - 06 RJ Skerry-Ryan
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.18603)
  - [Publication]()
  - [Github]()
  - [Demo](https://google.github.io/tacotron/publications/speechssm/)
- 文件:
  - [ArXiv](_PDF/2412.18603v1__SpeechSSM__Long-Form_Speech_Generation_with_Spoken_Language_Models.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

We consider the generative modeling of speech over multiple minutes, a requirement for long-form multimedia generation and audio-native voice assistants.
However, current spoken language models struggle to generate plausible speech past tens of seconds, from high temporal resolution of speech tokens causing loss of coherence, to architectural issues with long-sequence training or extrapolation, to memory costs at inference time.
With these considerations we propose SpeechSSM, the first speech language model to learn from and sample long-form spoken audio (e.g., 16 minutes of read or extemporaneous speech) in a single decoding session without text intermediates, based on recent advances in linear-time sequence modeling.
Furthermore, to address growing challenges in spoken language evaluation, especially in this new long-form setting, we propose: new embedding-based and LLM-judged metrics; quality measurements over length and time; and a new benchmark for long-form speech processing and generation, LibriSpeech-Long.
Speech samples and the dataset are released at [this https URL](https://google.github.io/tacotron/publications/speechssm/).

## 1·Introduction: 引言

Generative spoken language models \citep{lakhotia2021generative, dieleman2021-variable, oord2017-vqvae} are autoregressive (AR) models of invertible audio representations, enabling the direct learning and generation of intelligible speech and its paralinguistic aspects, such as prosody \citep{kharitonov2022-pgslm} and turn-taking \citep{nguyen2023generative}.
These capabilities make speech LMs promising for applications like media understanding and co-creation, audio-native voice assistants, and textless NLP.
However, real-world use-cases of spoken language models require the ability to both understand and generate long-form audio.
For example, voice interactions can last many minutes, requiring a model to maintain a growing conversational history in real time, and expressive media like audiobooks and podcasts can require semantic, paralinguistic, and speaker coherence over a chapter or episode.

This presents significant challenges for existing spoken language models; spoken audio is inherently complex, as its textual semantic content is entangled with paralinguistic content and acoustic properties that may detract from learning higher-level speech features.
Furthermore, the audio representations used have high temporal rates; it can require 10+ speech tokens to cover the duration of a spoken word \citep{hassid2023textually}.
Hence, models must retain and aggregate semantics over a longer time horizon, as well as generate content that is coherent over the same horizon.
This is difficult for the vanilla Transformer \citep{vaswani2017-transformer} in implementation, as its initial cost grows quadratically with prompt length, and its per-step cost grows linearly with decoding length; and difficult in modeling, as suggested by lackluster performance on long-range dependency tasks \citep{tay2021-lra}.
Though a few works have improved speech coherence via joint modeling with text (\Cref{sec:related-work}), the challenges of directly modeling long-form speech, particularly generation, remain unstudied by existing work on spoken language models (\Cref{fig:prior_work}).
Finally, the novelty of generating long-form speech means that analyses and evaluations of such generations have not been studied to date.
Our work proposes and make initial progress on the topic of generative long-form speech:

**Modeling**.

We discuss the design choices required to enable the practical training, generation, and extrapolation to tens of minutes of audio, from tokenization to speaker conditioning to complexity with respect to sequence length.
The result is \textbf{SpeechSSM}, a new (textless) spoken language model designed for long-form generation.
To our knowledge, this is first to model and generate unbounded long-form speech in bounded memory, as well as being the first state-space spoken LM.
As a baseline, we also train spoken Transformer LMs to perform multi-minute generations.
Finally, we also describe \textbf{SpeechSSM-X}, an extemporaneous variant that produces naturalistic spontaneous speech.

**Evaluation**.

We observe that existing metrics in speech generation evaluation are noisy and poorly discriminative, and propose the use of reference-based semantic metrics, side-by-side LLM-as-judge, and time-stratified evaluations for speech generation.
To scale these to long-form evaluation, we introduce the \textbf{LibriSpeech-Long} benchmark, derived from raw LibriSpeech \cite{panayotov2015librispeech} dev and test sets.
Since original LibriSpeech dev and test sets are segmented into 10s, we reprocess the chapter-level raw audio files into longer 4-minute utterances.
LibriSpeech-Long enables extended prompts and provides 4-minute reference ground truth, facilitating reference-based evaluation for long-form speech continuation as well as future long-form speech tasks.

We find that SpeechSSM matches existing speech LMs in the short-time horizon, while greatly outperforming their sliding window-based extensions on long-form spoken audio generation (e.g.,  \Cref{fig:transcript}).
Furthermore, we find that our proposed metrics and benchmark help quantify the semantic quality gaps between past work, our work, and human-level speech generation, enabling future model improvements.
SpeechSSM matches a comparable Transformer while having constant memory consumption and per-token compute.
We release examples at https://google.github.io/tacotron/publications/speechssm/ of read- and extemporaneous-style generations of up to 16 minutes in length and the [LibriSpeech-Long [Github]](https://github.com/google-deepmind/librispeech-long/) evaluation dataset.

## 2·Related Works: 相关工作

### Generating with Spoken LMs

The family of GSLM models \cite{lakhotia2021generative} is a Transformer decoder LM trained on discrete units obtained from $k$-means clustering of HuBERT \cite{hsu2021hubert} features.
Reconstruction involved a unit-to-spectrogram model whose outputs are then synthesized with a pre-trained neural spectrogram vocoder, though this was later simplified to a unit-to-waveform GAN \cite{kharitonov2022-pgslm}.
Finding this approach gave promising temporal coherence but poor audio quality, AudioLM \cite{borsos2023audiolm} proposed learning separate LMs, one for semantic tokens as before, and two for modeling coarse-to-fine acoustic tokens conditional on a prefix of fine acoustic and semantic tokens, where acoustic tokens are the hierarchical discrete codes of a neural audio codec (SoundStream; \citealp{zeghidour2021soundstream}).
The acoustic stages were later simplified and made non-autoregressive by SoundStorm \cite{borsos2023soundstorm}.

TWIST \cite{hassid2023textually} found that initializing with a text LM improved content-level semantic coherence, atop which VoxtLM \citep{maiti2023voxtlm} and Spirit LM \citep{nguyen2024spirit} found that joint and interleaved training with text gave further improvements.
However, all these works evaluated 10s generations, with only Spirit LM releasing (but not assessing) a generation above 30s.
Though models with two speech channels and models that sample text intermediates are out of scope, we note that dGSLM \cite{nguyen2023generative} generated 90s dialogues given 30s prompts, though its semantic and human evaluations were restricted to 50-word or 20s continuations, and Moshi \citep{defossez2024moshi} extends two-channel models with hierarchical sampling via time-aligned text tokens to give 5-minute dialogue interactions.

### State-Space Models for Long-Form Audio

State-space models (SSMs; \citealp{gu2021combining}) have become popular among efficient (sub-quadratic) replacements for Transformer-based architectures, giving the first model (S4; \citealp{gu2021efficiently}) to perform all tasks in the Long-Range Arena \citep{tay2021-lra}, outperforming the vanilla Transformer.
They utilize constant computation and memory requirements to generate tokens during inference and can be efficiently trained.
Recent focus has shifted to hybrid models \cite{glorioso2024zamba, lieber2024jamba, de2024griffin} which integrate state-space layers and variants like linear recurrent units (LRU; \citealp{orvieto2023-lru}) with finite-context self-attention layers.

Recent works have considered SSMs in audio, primarily to support long speech \textit{inputs} for text-out tasks like automatic speech recognition (ASR) and summarization.
None are spoken LMs for speech continuation, with only one considering (acoustic-level) tokens \citep{gao2024speech}; most works involve spectrogram encoders or outputs \cite{shams2024ssamba, erol2024audio, lin2024audio, miyazaki2024exploring}.
Closest in spirit is SaShiMi \citep{goel2022s}, a multi-scale S4 operating directly on waveform samples; though they generate only 1s of speech, this corresponds to a sequence of 16k scalars.
For music, they also study generation-time extrapolation from 8s to 16s.
One can view our work as a difference in scale, with tokens instead of samples as the unit (our 16-min generation corresponds to a sequence of 24k token IDs).

### Evaluating Spoken LM Generations

\citet{lakhotia2021generative} was first to evaluate the generations of spoken LMs, proposing ASR as a path to automated text metrics like text perplexity (PPL) and proportion of repeated $k$-grams (auto-BLEU), along with human evaluations of intelligibility and meaningfulness with mean opinion scores (MOS and MMOS respectively).
For their spoken LMs, zero-shot (non-generative) metrics based on logprobs of contrastive pairs (sWUGGY and sBLIMP; \citealp{nguyen2020zero}) were predictive of generation performance, though they note scores vary with token vocabulary size.

However, these initial metrics seem to lack robustness or are saturating with respect to newer spoken LMs.
\citet{hassid2023textually} found transcript PPL and auto-BLEU to be noisy, favoring MMOS and expanding zero-shot metrics (sStoryCloze and tStoryCloze).
In turn, \citet{defossez2024moshi} found that performance on zero-shot sWUGGY and sBLIMP metrics degraded despite continued experiential improvement from noisy-condition training and instruction-tuning, instead favoring spoken question answering (SQA) tasks \citep{nachmani2023spoken} evaluated via ASR.
Closest to our work was the use of LLMs to assign absolute, reference-free scores to assess the instruction following of turn-based text-and-speech LMs in \citet{zhang2023speechgpt, zhang2024speechagents}.

As for saturation, \citet{borsos2023audiolm} found that humans could not distinguish between a synthetic 7s continuation versus the real 7s continuation of a 3s prompt on a holistic side-by-side evaluation, suggesting the need for more targeted and longer-form evaluations.

## 3·Methodology: 方法

### Unbounded Speech Generation

We begin by proposing a set of requirements for a general, unbounded, speech generation system:

- \textbf{Constant memory during decoding}, to enable indefinite AR sampling without running out of memory.
- \textbf{Infinite context}, so that arbitrarily distant dependencies can be (theoretically) expressed. To be concurrent with the above, necessary past context must fit in a fixed-size state.
- \textbf{Generative length extrapolation}, so that speech quality remains consistent over time, in particular beyond audio durations seen during training.

The first leads us to linear-complexity sequence modeling with a fixed-size state. The second leads us to models with aggregation mechanisms such as recurrences or compressive memories. We show that with some care, one can also achieve the third requirement of generative extrapolation. Finally, beyond these requirements, there is also a practical desire for efficient training (e.g., train-time dependence on sequence length that is subquadratic, to enable longer sequences and reduce reliance on extrapolation), requiring a parallel scheme for weight learning. This leads us to state-space models, broadly defined (i.e., including linear recurrence models and certain hybrid variants; \citealp{patro2024mamba360, dao2024transformers}) and thus SpeechSSM, a hybrid state-space speech language model for efficient long-form speech generation that fulfills all these desiderata. Our key design choices are below.

#### Architecture

For our decoder-only hybrid SSM we choose Griffin \cite{de2024griffin}, which interleaves a gated variant of LRUs \cite{orvieto2023-lru} and local (sliding-window) multi-query attention (MQA) blocks in a fixed pattern (two recurrent, one local-MQA; see \Cref{fig:architecture}, left). Local attention efficiently captures recent context, while the states of the gated recurrences transmit information across arbitrary distances. Griffin's performance matched that of comparable Transformer LMs while significantly improving inference speed and enabling context-side extrapolation of at least 4x longer than seen in training. Observing that RoPE \cite{su2024roformer} in the local-MQA blocks still encodes absolute position, we follow recent work on position embeddings (PEs) under causal self-attention (NoPE; \citealp{kazemnejad2024impact}) and remove all explicit PEs from SpeechSSM to promote extrapolation.

#### Initialization

Inspired by \citet{hassid2023textually}'s success with text-initialized spoken language models (TWIST), we initialize our model with RecurrentGemma-2B IT \cite{botev2024recurrentgemma}, an open-weight LM with the Griffin architecture, trained on 2 trillion text tokens. We discard the pretrained text token embeddings and initialize new ones for our audio token vocabulary.

Following \citet{borsos2023audiolm}, we generate low-level acoustic tokens of a neural audio codec, conditioned on semantic tokens output from our SpeechSSM.

#### Semantic Tokenizer

We use the pretrained USM-v2 speech tokenizer \citep{vashishth2024stab, rubenstein2023audiopalm} to give our semantic tokens. The USM speech encoder \citep{zhang2023-usm} is trained with a masked language model (MLM) loss on untranscribed audio and an auxiliary automatic speech recognition (ASR) objective on transcribed audio. Then, its intermediate embeddings are discretized using vector quantization to give 32k units that serve as fixed-rate (25Hz, or 40ms/token) pseudo-text for our speech LM. \citet{vashishth2024stab} found that USM-v2 was by far the most speaker-invariant token out of a suite of common speech tokenizers.

#### Semantic-to-Acoustic Generation

We use a SoundStorm model \cite{borsos2023soundstorm} trained with USM-v2 conditioning to non-autoregressively generate SoundStream tokens \cite{zeghidour2021soundstream}, a neural audio codec that efficiently reconstructs to high-quality audio. It is trained to support 3-second voice prompts (represented as a frozen prefix of semantic and acoustic tokens), such that output acoustic tokens reflect speaker characteristics. By choosing a token and model decomposition that isolates speaker characteristics to the acoustic stage, SpeechSSM can focus capacity on modeling semantic coherence along the temporal axis.

#### Windowed Tokenization and Decoding

To process long-form speech while bounding the memory requirements of the (non-SSM) semantic tokenizer and acoustic decoder, we employ an overlapping window approach. Specifically, the audio is divided into 30-second segments, with an overlap of 4 seconds with its neighboring segments. Each 30-second window is tokenized independently. Then, the windows are merged into a continuous stream by taking, at each overlap the first two seconds of boundary tokens from the preceding window and the last two seconds of tokens from the succeeding window (\Cref{fig:windowing}a). Likewise, during decoding, the 30-second token windows are decoded independently, then merged with the same boundary overlap adjustment (\Cref{fig:windowing}b). Although windowing operates at the audio level and merging occurs at the token level (and vice versa), we qualitatively find our approach minimizes the boundary artifacts while enabling tokenization and decoding of long and continuous speech.

#### Avoiding Implicit EOSes

We do not train with an end-of-sequence (EOS) token. Despite this, our initial models generated only slightly beyond the training length (e.g., a 4min model reaching $\sim$4.5min before degrading to noise/silence). As we found in non-causal semantic tokenizers like USM-v2, the length of the window can be encoded in the tokens of that window, making ends of training examples look ``different'' and act as implicit EOSes. As evidence, padding the last window to 30s using silence, tokenizing, then dropping the extra tokens did not help, likely because the continuation being silence was also encoded in the kept tokens. What worked was (1) to pad the last window to 30s using speech from the beginning of the example (depicted in \Cref{fig:windowing}a) so that final tokens were tokenized \text{as if} there was further speech, and (2) in the case of LibriLight, to still drop the last 10s of examples -- as the padded beginnings were disproportionately "Chapter \textit{<number>}"!

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论