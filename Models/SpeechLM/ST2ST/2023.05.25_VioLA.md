# VioLA

<details>
<summary>基本信息</summary>

- 标题: "VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation"
- 作者:
  - 01 Tianrui Wang,
  - 02 Long Zhou,
  - 03 Ziqiang Zhang,
  - 04 Yu Wu,
  - 05 Shujie Liu,
  - 06 Yashesh Gaur,
  - 07 Zhuo Chen,
  - 08 Jinyu Li,
  - 09 Furu Wei
- 链接:
  - [ArXiv](https://arxiv.org/abs/2305.16107)
  - [Publication]
  - [Github]
  - [Demo]
- 文件:
  - [ArXiv](../_PDF/2305.16107v1__VioLA__Unified_Codec_Language_Models_for_Speech_Recognition_Synthesis_and_Translation.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

Recent research shows a big convergence in model architecture, training objectives, and inference methods across various tasks for different modalities.
In this paper, we propose ***VioLA***, a single auto-regressive Transformer decoder-only network that unifies various cross-modal tasks involving speech and text, such as speech-to-text, text-to-text, text-to-speech, and speech-to-speech tasks, as a conditional codec language model task via multi-task learning framework.
To accomplish this, we first convert all the speech utterances to discrete tokens (similar to the textual data) using an offline neural codec encoder.
In such a way, all these tasks are converted to token-based sequence conversion problems, which can be naturally handled with one conditional language model.
We further integrate task IDs (TID) and language IDs (LID) into the proposed model to enhance the modeling capability of handling different languages and tasks.
Experimental results demonstrate that the proposed ***VioLA*** model can support both single-modal and cross-modal tasks well, and the decoder-only model achieves a comparable and even better performance than the strong baselines.

</td><td>

最近的研究表明, 不同模态的各种任务在模型架构, 训练目标和推理方法上存在很大的趋同.
在本文中, 我们提出了 ***VioLA***, 这是一个单一的自回归 Transformer 仅解码器网络, 通过多任务学习框架, 将涉及语音和文本的各种跨模态任务 (如语音到文本, 文本到文本, 文本到语音和语音到语音任务) 统一为一个条件编解码器语言模型任务.

为了实现这一点, 我们首先使用离线神经编解码器将所有语音话语转换为离散 Token (类似于文本数据).
通过这种方式, 所有这些任务都被转换为基于 Token 的序列转换问题, 这些问题可以自然地通过一个条件语言模型来处理.
我们进一步将任务 ID (TID) 和语言ID (LID) 集成到提出的模型中, 以增强处理不同语言和任务的建模能力.

实验结果表明, 提出的 ***VioLA*** 模型能够很好地支持单模态和跨模态任务, 并且仅解码器架构的模型在性能上与强大的基线相当, 甚至更好.

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td></tr></table>
