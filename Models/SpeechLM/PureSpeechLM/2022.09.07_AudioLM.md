# AudioLM

<details>
<summary>基本信息</summary>

- 标题: "AudioLM: A Language Modeling Approach to Audio Generation"
- 作者:
  - 01 Zalan Borsos
  - 02 Raphael Marinier
  - 03 Damien Vincent
  - 04 Eugene Kharitonov
  - 05 Olivier Pietquin
  - 06 Matt Sharifi
  - 07 Dominik Roblek
  - 08 Olivier Teboul
  - 09 David Grangier
  - 10 Marco Tagliasacchi
  - 11 Neil Zeghidour
- 链接:
  - [ArXiv](https://arxiv.org/abs/2209.03143)
  - [Publication](https://doi.org/10.1109/TASLP.2023.3288409)
  - [Github]
  - [Demo](https://google-research.github.io/seanet/audiolm/examples/)
- 文件:
  - [ArXiv](../_PDF/2209.03143v2__AudioLM__A_Language_Modeling_Approach_to_Audio_Generation.pdf)
  - [Publication](../_PDF/2209.03143p0__AudioLM__TASLP2023.pdf)

</details>

## Abstract: 摘要

<table><tr><td width="50%">

We introduce ***AudioLM***, a framework for high-quality audio generation with long-term consistency.
***AudioLM*** maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space.
We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives.
Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis.
By training on large corpora of raw audio waveforms, ***AudioLM*** learns to generate natural and coherent continuations given short prompts.
When trained on speech, and without any transcript or annotation, ***AudioLM*** generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers.
Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.

</td><td>

我们介绍了 ***AudioLM***, 一个用于高质量音频生成且具有长期一致性框架.
***AudioLM*** 将输入音频映射为一系列离散的 Token, 并将音频生成视为在该表示空间中的语言建模任务.
我们展示了现有的音频分词器如何在重构质量和长期结构之间提供不同的权衡, 并提出了一个混合分词方案, 以实现两者.
具体来说, 我们利用在音频上预训练的掩码语言模型的离散激活来捕获长期结构, 并利用神经音频编解码器生成的离散代码来实现高质量合成.
通过在大量原始音频波形的语料库上训练, ***AudioLM*** 学习生成短语提示下的自然且连贯的续写.
当在语音上训练, 且没有任何文字或注释时, ***AudioLM*** 生成符合语法和语义的语音续写, 同时保持发言人身份和语调的不变性, 对于未见过的发言人也是如此.
此外, 我们展示了我们的方法如何扩展到语音之外, 即生成连贯的钢琴音乐续写, 尽管在训练过程中没有任何音乐的符号表示.

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td></tr></table>
