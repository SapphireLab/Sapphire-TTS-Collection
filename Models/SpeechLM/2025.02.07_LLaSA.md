# LLaSA

<details>
<summary>基本信息</summary>

- 标题: "LLaSA: Scaling Train-Time and Inference-Time Compute for LLaMA-based Speech Synthesis"
- 作者:
  - 01 Zhen Ye,
  - 02 Xinfa Zhu, Chi-Min Chan, Xinsheng Wang, Xu Tan, Jiahe Lei, Yi Peng, Haohe Liu, Yizhu Jin, Zheqi DAI, Hongzhan Lin, Jianyi Chen, Xingjian Du, Liumeng Xue, Yunlin Chen, Zhifei Li, Lei Xie, Qiuqiang Kong, Yike Guo, Wei Xue
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.04128)
  - [Publication]()
  - [Github](https://github.com/zhenye234/X-Codec-2.0)
    - [Training](https://github.com/zhenye234/LLaSA_training)
    - [Inference](https://github.com/zhenye234/LLaSA_inference)
    - [HuggingFace](https://huggingface.co/collections/HKUSTAudio/llasa-679b87dbd06ac556cc0e0f44)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2502.04128v1__LLaSA__Scaling_Train-Time_and_Inference-Time_Compute_for_LLaMA-Based_Speech_Synthesis.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

Recent advances in text-based large language models (LLMs), particularly in the GPT series and the o1 model, have demonstrated the effectiveness of scaling both training-time and inference-time compute.
However, current state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring separate models (e.g., diffusion models after LLM), complicating the decision of whether to scale a particular model during training or testing.
This work makes the following contributions:
First, we explore the scaling of train-time and inference-time compute for speech synthesis.
Second, we propose a simple framework ***LLaSA*** for speech synthesis that employs a single-layer vector quantizer (VQ) codec and a single Transformer architecture to fully align with standard LLMs such as LLaMA.
Our experiments reveal that scaling train-time compute for LLaSA consistently improves the naturalness of synthesized speech and enables the generation of more complex and accurate prosody patterns.
Furthermore, from the perspective of scaling inference-time compute, we employ speech understanding models as verifiers during the search, finding that scaling inference-time compute shifts the sampling modes toward the preferences of specific verifiers, thereby improving emotional expressiveness, timbre consistency, and content accuracy.
In addition, we released the checkpoint and training code for our TTS model (1B, 3B, 8B) and codec model publicly available.

</td><td>

在基于文本的大语言模型的近期进展中, 特别是 GPT 系列和 o1 模型, 已经证明了缩放训练时间和推理时间计算的有效性.
然而, 当前最先进的 TTS 系统利用 LLM 的方式往往是多阶段的, 要求单独的模型 (如 LLM 后跟扩散模型), 使得在训练或推理时是否缩放特定模型的决策变得复杂.

本工作做出如下贡献:
首先, 我们探索了语音合成的训练时间和推理时间计算的缩放.
其次, 我们提出了一个简单的框架 ***LLaSA*** 用于语音合成, 它采用了单层向量量化编解码器和单个 Transformer 架构和标准 LLM 如 LLaMA 进行完全对齐.
我们的实验解释了 LLaSA 缩放训练时间计算能够一致地提升合成语音的自然度并实现生成更复杂和准确的韵律模式.
此外, 从缩放推理时间的角度, 我们在搜索时采用语音理解模型作为验证器, 发现缩放推理时间计算会将采样模型偏向特定验证器的偏好, 因此提升情感表现力, 音色一致性和内容准确性.
另外, 我们公开发布了我们 TTS 模型 (1B, 3B, 8B) 和编解码器模型的检查点和训练代码.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

Recent years have witnessed the remarkable success of large language models (LLMs) in the text domain, represented by the GPT series (**GPT-3** [^brown2020language]; **GPT-4** [^achiam2023gpt]; [^radford2019language]).
These advances have demonstrated that increasing model size and training data consistently yields better performance across a wide array of natural language understanding and generation tasks.
However, as the text domain approaches data saturation, new directions are emerging, such as **o1 models** [^jaech2024openai] that emphasize extensive computational effort at test time—thereby exhibiting an inference-time scaling effect.
By investing more resources during inference, these models are able to produce higher-quality outputs and handle more complex tasks, offering a flexible avenue for performance improvement after the training phase.

[^brown2020language]: [**GPT-3**: Language Models Are Few-Shot Learners.](../TextLM/2020.05.28_GPT-3.md) NeurIPS2020.
[^achiam2023gpt]: [**GPT-4** Technical Report.](../TextLM/2023.03.15_GPT-4.md) ArXiv2023.
[^radford2019language]: **Blog**: Language Models Are Unsupervised Multitask Learners. 2019.
[^jaech2024openai]: **GPT-o1**: OpenAI o1 System Card. ArXiv2024.

</td><td>

</td></tr>
<tr><td>

Meanwhile, text-to-speech (TTS) research has also made impressive strides.
Many existing TTS systems focus on devising better model architectures—leveraging well-designed modules, larger datasets, and increased model size—to push synthesis quality ever higher.
While these efforts have propelled the field forward, they also tend to narrow the community’s perspective: the focus on better architectures can overshadow investigations into broader, potentially transformative research questions.
In contrast, the text LLM community has converged on a relatively standard framework—a simple Transformer model with a tokenizer—which allows researchers to concentrate on fundamental issues such as training-time scaling laws [^kaplan2020scaling], inference-time scaling behaviors [^snell2024scaling], and downstream adaptations (e.g., fine-tuning (**LoRA** [^hu2021lora]), pruning, and quantization [^zhu2024survey]).
Such a common design philosophy has catalyzed rapid progress and deeper insights into the text domain.

[^kaplan2020scaling]: Scaling Laws for Neural Language Models. ArXiv2020.
[^snell2024scaling]: Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Model Parameters. ArXiv2024.
[^hu2021lora]: [**LoRA**: Low-Rank Adaptation of Large Language Models.](../../Modules/LoRA/2021.06.17_LoRA.md) ArXiv2021.
[^zhu2024survey]: **Survey**: A Survey on Model Compression for Large Language Models. ACL2024.

</td><td>

</td></tr>
<tr><td>

Motivated by these observations, we propose aligning TTS with the minimalist yet powerful paradigm of text LLMs.
We introduce a single Transformer-based TTS model that relies on a well-designed speech tokenizer.
More specifically,  our TTS system, named ***LLaSA***, is initialized from the **LLaMA** [^touvron2023Llama] model with an expanded vocabulary that incorporates speech tokens and is trained using
the next-token prediction paradigm.
Although this model may not always match the performance of highly customized TTS systems, its streamlined design creates a unified foundation for exploring a wider range of research questions—beyond architecture exploration.

[^touvron2023Llama]: [**LLaMA**: Open and Efficient Foundation Language Models.](../TextLM/2023.02.27_LLaMA.md) ArXiv2023.

</td><td>

</td></tr>
<tr><td>

In particular, we systematically investigate both training-time and inference-time scaling effects under this unified TTS framework.
Our experiments show that scaling training-time compute (e.g., increasing model size or training data) not only improves naturalness but also enhances expressive prosody, effectively capturing the meaning conveyed in text
without explicit labels.
Additionally, we examine the utility of inference-time scaling by incorporating speech understanding models as verifiers in the search process.
We find that spending more computational effort during inference aligns generation outputs more closely with specific verifier biases, yielding better emotional expressiveness, timbre consistency, and content accuracy.
Evaluations on **LibriSpeech test sets** [^panayotov2015librispeech], **Seed-TTS-Eval** [^anastassiou2024seed] and **ESD datasets** [^zhou2021emotional] demonstrate state-of-the-art results, and further highlight how in-context learning capabilities can be combined with search-based refinements to control factors such as speaker identity or emotion.

[^panayotov2015librispeech]: [**LibriSpeech**: An ASR Corpus Based on Public Domain Audio Books.](../../Datasets/2015.04.19_LibriSpeech.md) ICASSP2015.
[^anastassiou2024seed]: [**Seed-TTS**: A Family of High-Quality Versatile Speech Generation Models.](2024.06.04_Seed-TTS.md) ArXiv2024.
[^zhou2021emotional]: [Emotional Voice Conversion: Theory, Databases and ESD.](../../Datasets/2021.05.31_ESD.md) Speech Communication2022.

</td><td>

</td></tr>
<tr><td>

In summary, our paper makes several key contributions.
We design a TTS model named ***LLaSA*** that is fully aligned with standard LLM architectures by utilizing a single Transformer and a well-designed speech tokenizer, thereby creating a simple, flexible, and scalable system.
Additionally, we find that increasing training-time compute for ***LLaSA*** leads to significant improvements in speech naturalness and prosody accuracy, which reflects a deeper semantic understanding of the input text.
We further demonstrate that scaling inference-time compute—achieved by incorporating speech understanding verifiers—enhances emotional expressiveness, timbre consistency, and content accuracy in synthesized speech.
Furthermore, by providing open access to our models and frameworks, we aim to foster further research and development in the TTS community.

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

### Scaling Train-time and Inference-time Compute

<table><tr><td width="50%">

In the text domain, large language models (LLMs) like GPT (**GPT-3** [^brown2020language]; [^kaplan2020scaling]) show that increasing data, compute, and model size enhances performance, pushing LLMs toward cognitive intelligence.
However, further scaling during training faces limits due to data and compute constraints.
To overcome this, a scaling law during testing is proposed: greater computational effort in inference improves performance [^ji2025test].
Techniques like repeat sampling, self-correction, and tree search enhance reasoning depth and accuracy in complex tasks.
While this principle has been extensively validated in the text domain, its impact on speech remains largely unexplored.
**BASE-TTS** [^lajszczak2024base] provides a brief investigation into the emergent abilities arising from scaling both model size and data volume but does not separately examine the effects of data scaling versus model scaling.
Additionally, it does not explore performance across different languages.
To the best of our knowledge, we are the first to systematically investigate inference scaling in the speech modality.

[^ji2025test]: Test-Time Computing: From System-1 Thinking to System-2 Thinking. ArXiv2025.
[^lajszczak2024base]: [**BASE-TTS**: Lessons from Building a Billion-Parameter Text-to-Speech Model on 100k Hours of Data.](2024.02.12_BASE-TTS.md) ArXiv2024.

</td><td>

</td></tr></table>

### LLM-based TTS

<table><tr><td width="50%">

Significant progress has been made in using large language models (LLMs) for TTS tasks, including multi-speaker synthesis (**SPEAR-TTS** [^speartts]) and zero-shot voice cloning (**VALL-E** [^wang2023neural]; **VALL-E 2** [^chen2024vall]).
**VALL-E** [^wang2023neural] pioneered treating TTS as a conditional language modeling problem by converting waveforms into neural codec tokens.
However, it employed a multi-stage approach—a coarse autoregressive (AR) model followed by a non-autoregressive (NAR) residual model—which complicates training and inference.
Extensions like **VALL-E X** [^vallex] enable cross-lingual synthesis, and **SPEAR-TTS** [^speartts] integrates multiple AR models to support multi-speaker TTS with minimal supervision.
Recent TTS systems have often combined an AR language model with additional components, such as diffusion (**TorToise-TTS** [^tortoisetts]; **BASE TTS** [^lajszczak2024base]; **Seed-TTS** [^anastassiou2024seed]; **CosyVoice** [^du2024cosyvoice]; **FireRedTTS** [^guo2024fireredtts]), to generate more natural, controllable speech when trained on large datasets.
Although these multi-stage pipelines can yield high-quality results, they remain cumbersome and less amenable to large-scale training.
On the other hand, single-stage systems like **MELL-E** [^meng2024autoregressive] and **KALL-E** [^kalle] avoid multi-stage generation but rely on continuous acoustic features (e.g., spectrograms or latent variables).
Storing and processing these features at scale can be prohibitive, hindering training on tens or hundreds of billions of tokens.
In contrast, our approach uses a single-stage AR Transformer that directly models discrete speech tokens, similar to how text LLMs handle words and subwords.
This design avoids multi-stage complexity and the large memory footprint of continuous representations, making it far more scalable while retaining the flexibility of a standard LLM.

[^speartts]: [**SPEAR-TTS**: Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision.](2023.02.07_SPEAR-TTS.md) TACL2023.
[^wang2023neural]: [**VALL-E**: Neural Codec Language Models Are Zero-Shot Text to Speech Synthesizers.](2023.01.05_VALL-E.md) ArXiv2023.
[^chen2024vall]: [**VALL-E 2**: Neural Codec Language Models Are Human Parity Zero-Shot Text to Speech Synthesizers.](2024.06.08_VALL-E_2.md) ArXiv2024.
[^vallex]: [**VALL-E X**: Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling.](2023.03.07_VALL-E_X.md) ArXiv2023.
[^tortoisetts]: [**TorToise-TTS**: Better Speech Synthesis through Scaling.](../Diffusion/2023.05.12_TorToise-TTS.md) ArXiv2023.
[^du2024cosyvoice]: [**Cosyvoice**: A Scalable Multilingual Zero-Shot Text-to-Speech Synthesizer Based on Supervised Semantic Tokens.](2024.07.07_CosyVoice.md) ArXiv2024.
[^guo2024fireredtts]: [**FireRedTTS**: A Foundation Text-to-Speech Framework for Industry-Level Generative Speech Applications.](2024.09.05_FireRedTTS.md) ArXiv2024.
[^meng2024autoregressive]: [**MELLE**: Autoregressive Speech Synthesis without Vector Quantization.](2024.07.11_MELLE.md) ArXiv2024.
[^kalle]: [**KALL-E**: Autoregressive Speech Synthesis with Next-Distribution Prediction.](2024.12.22_KALL-E.md) ArXiv2024.

</td><td>

</td></tr></table>

## 3·Methodology: 方法

### Overview

<table><tr><td width="50%">

Our TTS framework is designed to fully align with the standard text LLM paradigm, keeping only two main components: (1) a tokenizer and (2) a single Transformer-based LLM.
We initialize the Transformer parameters $\phi$ from an existing LLM (e.g., **LLaMA** [^touvron2023Llama]), and inherit its tokenizer for the text portion.
Hence, the core new challenge is to convert raw speech waveforms into sequences of discrete tokens such that the Transformer can model them in an autoregressive manner.

To achieve this, we introduce our speech tokenizer, X-codec2, which encodes waveforms into speech tokens and can decode them back to high-quality audio.
Unlike some prior tokenizers for TTS, ours requires no additional information during decoding, ensuring that all aspects of the speech signal, such as content, prosody, and timbre, are captured by the LLM.

Formally, let:
1) $ \texttt{Tokenize}_\text{text}(X) = \{x_1, \dots, x_T\} $ be the text tokenizer, which converts input text $X$ into $T$ text tokens.
2) $ \texttt{Tokenize}_\text{speech}(Y) = \{y_1, \dots, y_S\} $ be the speech tokenizer, which converts a speech waveform $Y$ into $S$ discrete tokens.
3) $ \texttt{Detokenize}_\text{speech}(\{y_1, \dots, y_S\}) = \hat{Y} $ be the speech decoder, which reconstructs the waveform $\hat{Y}$ from its token representation.

Given a dataset $\mathcal{D} = \{(X_i, Y_i)\}_{i=1}^N$, where $X_i$ is the text transcription and $Y_i$ is the corresponding audio, we represent each pair $(X_i, Y_i)$ as a token sequence $(x_1, \dots, x_T, y_1, \dots, y_S)$.
Our Transformer, with parameters $\phi$, learns the joint distribution of text and speech tokens via

$$
\begin{aligned}
    &P(x_1, \ldots, x_T, y_1, \ldots, y_S)  \notag \\
  = & P(x_1, \ldots, x_T) \cdot  P(y_1, \ldots, y_S | x_1, \ldots, x_T) \notag
\end{aligned}
$$

Since the text tokens $ \{x_1, \ldots, x_T\} $ are always given as input during training and inference, the model focuses on learning the conditional probability:
$$
\begin{align}
    &P(y_1, \ldots, y_S | x_1, \ldots, x_T) \notag \\
    =& \prod_{s=1}^S P(y_s | x_1, \ldots, x_T, y_1, \ldots, y_{s-1}) \notag
\end{align}
$$

Therefore, the loss is calculated over the speech tokens $ \{y_1, \ldots, y_S\} $.
The objective is to minimize the negative log-likelihood:

$$
\mathcal{L} = - \sum_{s=1}^S \log P(y_s | x_1, \ldots, x_T, y_1, \ldots, y_{s-1})
$$

which makes the model learn to predict each speech token $ y_s $ conditioned on both the text tokens $ \{x_1, \ldots, x_T\} $ and the previously generated speech tokens $ \{y_1, \ldots, y_{s-1}\} $.

</td><td>

</td></tr></table>

### Speech Tokenizer

<table><tr><td width="50%">

As highlighted by **AudioLM** [^borsos2023audiolm], discrete speech representations can be categorized into both semantic tokens and acoustic tokens.
Language modeling on semantic tokens excels at capturing high-level information such as content and emotion, while modeling with acoustic tokens focuses on low-level details, including timbre and other acoustic characteristics.
Our X-codec2 tokenizer builds on the concepts from prior work **X-codec** [^ye2024codec].
We fuse these semantic and acoustic features into a unified codebook but introduce a crucial modification: replacing residual vector quantization with a single vector quantizer to ensure 1D causal dependency.
This design naturally aligns with the left-to-right autoregressive mechanism of LLMs and also reflects the inherently left-to-right temporal structure of audio signals.

[^borsos2023audiolm]: [**AudioLM**: A Language Modeling Approach to Audio Generation.](2022.09.07_AudioLM.md) TASLP2023.
[^ye2024codec]: [**X-Codec**: Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model.](../SpeechCodec/2024.08.30_X-Codec.md) ArXiv2024.

Our X-codec2 consists of three main components: the Encoder, the VQ module, and the Decoder.

</td><td>

</td></tr></table>

#### Encoder

<table><tr><td width="50%">

Given a raw speech waveform $\mathbf{Y}$, we employ two separate encoders to derive its semantic and acoustic representations:

- **Semantic Encoder** $\mathrm{Enc}_s$: We adopt a pre-trained Wav2Vec2-BERT (**Seamless** [^barrault2023seamless]) to obtain multilingual speech semantic features that capture content and emotional cues.
- **Acoustic Encoder** $\mathrm{Enc}_a$: Following the design in **BigCodec** [^xin2024bigcodec] and [^kumar2024high], this module uses multiple residual convolutional blocks with Snake activation functions to encode low-level acoustic details.

[^barrault2023seamless]: [**Seamless**: Multilingual Expressive and Streaming Speech Translation.](2023.12.08_Seamless.md) ArXiv2023.
[^xin2024bigcodec]: [**BigCodec**: Pushing the Limits of Low-Bitrate Neural Speech Codec.](../SpeechCodec/2024.09.09_BigCodec.md) ArXiv2024.
[^kumar2024high]: [**DAC**: High-Fidelity Audio Compression with Improved RVQGAN.](../SpeechCodec/2023.06.11_Descript-Audio-Codec.md) NeurIPS2024.

We then concatenate the two outputs to form a fused feature embedding,
$$
\mathbf{H} = \bigl[\mathrm{Enc}_s(\mathbf{Y}), \; \mathrm{Enc}_a(\mathbf{Y})\bigr],
$$
which serves as the input to our vector quantizer.

</td><td>

</td></tr></table>

#### Vector Quantization

<table><tr><td width="50%">

To obtain discrete tokens, we apply $\mathrm{FSQ}(\cdot)$ [^mentzer2024finite] to $\mathbf{H}$:
$$
\mathbf{H}_q \;=\; \mathrm{FSQ}(\mathbf{H}),
$$
where $\mathbf{H}_q$ is the quantized feature.
We adopt FSQ due to its stability in training and high codebook usage efficiency.
Notably, FSQ does not require an explicit VQ objective term (e.g., codebook commitment loss), simplifying optimization during training.

[^mentzer2024finite]: [**FSQ**: Finite Scalar Quantization: VQ-VAE Made Simple.](../../Modules/VQ/FSQ.md) ICLR2024.

</td><td>

</td></tr></table>

#### Decoder

<table><tr><td width="50%">

From the quantized representation $\mathbf{H}_q$, we aim to reconstruct both **semantic** and **acoustic** information:

- **Semantic Reconstruction**: We follow **X-Codec** [^ye2024codec] and employ a semantic decoder to predict semantic features, using an $\ell_2$ loss for reconstruction.
It is worth noting that during inference, predicting semantic features is unnecessary; this component is designed to provide a supervisory signal to ensure the codebook retains sufficient semantic information.
- **Acoustic Reconstruction**: Following **Vocos** [^siuzdakvocos], we replace the ConvNeXt backbone with a Transformer-based decoder that predicts the short-time Fourier transform (STFT) magnitude and phase; an inverse STFT (iSTFT) head then converts the predicted spectra back to time-domain waveforms.

[^siuzdakvocos]: [**Vocos**: Closing the Gap between Time-Domain and Fourier-Based Neural Vocoders for High-Quality Audio Synthesis.](../Vocoder/2023.03.01_Vocos.md) ICLR.

</td><td>

</td></tr></table>

#### Codec Training

<table><tr><td width="50%">

The training process closely follows that of X-Codec, simultaneously optimizing both semantic and acoustic reconstruction.
We incorporate a multi-period discriminator (MPD) [^kong2020hifi], a multi-scale STFT (MS-STFT) discriminator, and a spectral discriminator, with FFT sizes $\{78, 126, 206, 334, 542, 876, 1418, 2296\}$ [^parker2024scaling], for adversarial training.
Additionally, following [^parker2024scaling], we incorporate a perceptual loss during the final steps of the training process to further enhance intelligibility.

[^kong2020hifi]: [**HiFi-GAN**: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis.](../Vocoder/2020.10.12_HiFi-GAN.md) NeurIPS2020.
[^parker2024scaling]: [**StableCodec**: Scaling Transformers for Low-Bitrate High-Quality Speech Coding.](../SpeechCodec/2024.11.29_StableCodec.md) ICLR2025.

</td><td>

</td></tr></table>

### Scaling Train-Time Compute

<table><tr><td width="50%">

Our primary goal in this section is not to locate a compute-optimal configuration for TTS, but rather to show that, akin to text-based LLMs, increasing train-time resources (either by enlarging the model or expanding the training dataset) consistently improves performance.
Specifically, we investigate two scaling strategies:

- **Fix Training Data, Vary Model Size.**
We fix the training data at $250\text{k}$ hours and scale the size of the Transformer $\phi$.
Concretely, we adopt LLaMA~3.2 with 1B and 3B parameters, as well as LLaMA~3.1 with 8B parameters, which we denote as Llasa-1B-250k, Llasa-3B-250k, and Llasa-8B-250k, respectively, to observe how increased model capacity influences TTS quality.
- **Fix Model Size, Vary Training Data.**
We choose an LLM initialized from LLaMA~3.2 1B and train on three progressively larger subsets of our dataset $\mathcal{D}$, containing $80\text{k}$, $160\text{k}$, and $250\text{k}$ hours of speech, respectively.
Notably, the 80k and 160k subsets are randomly sampled as 1/3 and 2/3 partitions of the full 250k dataset, which we denote as Llasa-1B-80k, Llasa-1B-160k, and Llasa-1B-250k (identical to the previously mentioned Llasa-1B-250k model).

We evaluate these on two aspects:

</td><td>

</td></tr></table>

#### Text Understanding Ability

<table><tr><td width="50%">

A longstanding challenge in Text-to-Speech (TTS) technology is that TTS systems often fail to fully comprehend the meaning of text as humans do, which leads to mechanical pronunciation, lack of emotion, unnatural pauses, and difficulties in distinguishing homographs.
Following **BASE TTS** [^lajszczak2024base], we use seven categories of texts—Questions, Emotions, Compound Nouns, Complex Syntax, Foreign Words, Paralinguistics, and Punctuation—for English evaluation.
Additionally, we propose seven categories tailored for Chinese: Questions, Emotions, Paralinguistics, Chinese Poetry, Rare Characters, Polyphonic Words, and Tongue Twisters (details are provided in Appendix \ref{testset}).
In each case, the TTS system must exhibit a deeper textual understanding to produce natural and context-appropriate speech (e.g., correct pronunciation for polyphonic words and more expressive speech for emotional content).
By examining the synthesized audio for each category, we measure how increased training data or parameter count benefits the system's text understanding ability.

</td><td>

</td></tr></table>

#### In-context Learning Ability

<table><tr><td width="50%">

We also evaluate the model’s zero-shot TTS capabilities (**VALL-E** [^wang2023neural])—whether it can produce intelligible, high-quality speech for speakers unseen during training.
This aligns with prior zero-shot TTS protocols, which typically assess how well a model generalizes to new speaker identities, timbres, and emotional expressions with no additional fine-tuning.

</td><td>

</td></tr></table>

### Scaling Inference-Time Compute

<table><tr><td width="50%">

Recent research has begun exploring the scaling behavior of LLMs during inference, showing that additional computational resources—often in the form of sophisticated search strategies—can further enhance performance.
Concretely, such approaches adjust the model’s output distribution at test time by generating multiple candidate outputs from a baseline model and then applying post-hoc filtering and refinement via verifiers or scoring mechanisms, thereby elevating the quality of the generated content.
When extending this concept to text-to-speech (TTS), we hypothesize that generating multiple speech candidates and performing a targeted search among them can yield outputs that more closely match the task requirements.
In line with prior work [^snell2024scaling],[^ma2025inference], our search framework centers on two fundamental design choices:

[^ma2025inference]: Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps. ArXiv2025.

</td><td>

</td></tr></table>

#### Verifier Selection

<table><tr><td width="50%">

For TTS, many off-the-shelf speech understanding models can serve as verifiers (or reward models) to evaluate synthesized audio from multiple perspectives.
These include speaker verification models for measuring timbre similarity, emotion representation models [^ma2023emotion2vec] for gauging emotional content, prosodic analyzers (e.g., pitch and duration [^li2023diverse], **FastSpeech2** [^ren2022fastspeech2fasthighquality]) to ensure natural intonation and rhythm, speech quality metrics (**UTMOS** [^saeki2022utmos], **DNSMOS** [^reddy2021dnsmos]) (e.g., SpeechMos) to evaluate naturalness, and automatic speech recognition (ASR) models (**Whisper** [^radford2023robust], **FunASR** [^gao2023funasr]) to assess transcription accuracy.
By integrating feedback from these diverse verifiers, we ensure that the generated speech meets our requirements across multiple aspects, all in a fully automated process.

[^ma2023emotion2vec]: [**Emotion2Vec**: Self-Supervised Pre-Training for Speech Emotion Representation.](../SpeechRepresentation/2023.12.23_Emotion2Vec.md) ArXiv2023.
[^li2023diverse]: Diverse and Expressive Speech Prosody Prediction with Denoising Diffusion Probabilistic Model. ArXiv2023.
[^ren2022fastspeech2fasthighquality]: [**FastSpeech2**: Fast and High-Quality End-to-End Text-to-Speech.](../Acoustic/2020.06.08_FastSpeech2.md) ArXiv2022.
[^saeki2022utmos]: [UTMOS: UTokyo-Sarulab System For Voicemos Challenge 2022.](../../Evaluations/2022.04.05_UTMOS.md) InterSpeech2022.
[^reddy2021dnsmos]: [DNSMOS: A Non-Intrusive Perceptual Objective Speech Quality metric to evaluate Noise Suppressors.](../../Evaluations/2020.10.28_DNSMOS.md) ICASSP2021.
[^radford2023robust]: [**Whisper**: Robust Speech Recognition via Large-Scale Weak Supervision.](2022.12.06_Whisper.md) ICML2023.
[^gao2023funasr]: [**FunASR**: A Fundamental End-to-End Speech Recognition Toolkit.](../Toolkits/2023.05.18_FunASR.md) InterSpeech2023.

</td><td>

</td></tr></table>

#### Algorithms

<table><tr><td width="50%">

We categorize the two different reward methods as follows:

- **Output Reward Models (ORMs)**: These models assess the speech segment only after it has been fully generated, evaluating it holistically.
ORMs typically follow a simpler design but may be less efficient due to the absence of intermediate guidance signals.
A common search strategy based on ORMs is the Best-of-N approach, where multiple candidate outputs are generated, scored using a reward model, and the highest-scoring output is selected.
- **Process Reward Models (PRMs)**: These models evaluate the generation process step by step, optimizing at each incremental stage (e.g., every second in a 10-second clip).
Unlike conventional reward models that produce a single score for the final output, PRMs provide a sequence of scores, allowing for more fine-grained feedback throughout the generation process.
While this approach enables detailed control, it also increases the risk of overfitting to intermediate rewards and of converging to local optima.
The most common search algorithm leveraging PRMs is beam search, which systematically explores the solution space while optimizing both the sampling and evaluation of intermediate steps.

In our experiments, we explore both PRMs and ORMs to analyze how different search strategies impact the final quality of synthesized speech.
A more detailed discussion of these methods and their outcomes is provided in the next experiments section.

</td><td>

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

In this section, first, we compare our proposed speech tokenizer with existing codecs to assess its effectiveness.
Second, we evaluate the performance of our TTS systems.
We explore the effect of scaling both train-time and inference-time compute and compare our models against other baseline TTS systems.
Lastly, we evaluate the extensibility of our framework in Appendix \ref{asr}, particularly its applicability to speech understanding tasks, highlighting its versatility and potential for broader applications.

</td><td>

</td></tr></table>

### Codec Experiments

#### Training Details

<table><tr><td width="50%">

We train our codec model on a corpus of approximately 150k hours of multilingual speech, drawn from the **Emilia dataset** [^he2024emilia] (En/Zh/De/Fr/Ja/Ko) and **MLS** [^Pratap2020MLSAL] (En/Fr/De/Nl/Es/It/Pt/Pl).
All audio is sampled at 16\,kHz.
We set the total downsampling ratio $R$ to 320, use a codebook size of 65536, and employ a projection dimension of 8 in our VQ module.
During training, we randomly crop 6-second segments from the audio.
The learning rate is $1\times10^{-4}$, preceded by a 3000-step warmup.
In total, we train for 1.4 million steps, we activate perceptual loss at the final 0.2 million steps.

[^he2024emilia]: [**Emilia**: An Extensive, Multi-Lingual, and Diverse Speech Dataset for Large-Scale Speech Generation.](../../Datasets/2024.07.07_Emilia.md) SLT2024.
[^Pratap2020MLSAL]: [**MLS**: A Large-Scale Multilingual Dataset for Speech Research.](../../Datasets/2020.12.07_MLS.md) ArXiv2020.

</td><td>

</td></tr></table>

#### Evaluation Results

<table><tr><td width="50%">

For evaluation, we use the test-clean subset of **LibriSpeech** [^panayotov2015librispeech], which contains 2620 utterances at 16\,kHz.

We evaluate our system using several metrics.
a HuBERT-based ASR system for Word Error Rate (WER) [URL](https://huggingface.co/facebook/hubert-large-ls960-ft).
Short-Time Objective Intelligibility (STOI).
Perceptual Evaluation of Speech Quality (PESQ).
A  WavLM-based speaker verification model for speaker similarity (SPK SIM) [URL](https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification), and UTMOS [URL](https://github.com/tarepan/SpeechMOS).

We compare our codec against multiple baselines, including **DAC** [^kumar2024high], **SpeechTokenizer** [^zhang2024speechtokenizer], **BigCodec** [^xin2024bigcodec], **StableCodec** [^parker2024scaling], **SemantiCodec** [^liu2024semanticodec], X-codec [^ye2024codec], Mini [^defossez2024moshi], EnCodec [^defossez2022high], and WavTokenizer [^ji2024wavtokenizer].
All baseline results are obtained using their official checkpoints.

[^zhang2024speechtokenizer]: [**SpeechTokenizer**: Unified Speech Tokenizer for Speech Large Language Models.](../SpeechCodec/2023.08.31_SpeechTokenizer.md) ICLR2024.
[^liu2024semanticodec]: [**Semanticodec**: An Ultra Low Bitrate Semantic Audio Codec For General Sound.](../SpeechCodec/2024.04.30_SemantiCodec.md) ArXiv2024.
[^defossez2024moshi]: [**Moshi**: A Speech-Text Foundation Model for Real-Time Dialogue.](../SpokenDialogue/2024.09.17_Moshi.md) ArXiv2024.
[^defossez2022high]: [**EnCodec**: High Fidelity Neural Audio Compression.](../SpeechCodec/2022.10.24_EnCodec.md) ArXiv2022.
[^ji2024wavtokenizer]: [**WavTokenizer**: An Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling.](../SpeechCodec/2024.08.29_WavTokenizer.md) ArXiv2024.

As shown in Table \ref{codec},  X-codec2 achieves the best performance at a token rate of 50 for most metrics.
Moreover, its UTMOS score closely matches that of the ground truth, indicating that the reconstructed audio faithfully preserves the original speech quality.
We also observe that certain models exceed the ground truth in UTMOS when operating at low token rates.
We suspect this occurs because, under limited token constraints, the decoder behaves partly as a generative model—yielding plausible speech output but the alignment with the input was less precise.
Additionally, we found that metrics such as WER at a low token rate can achieve good results by integrating speech semantic information, as demonstrated by models like Mini, X-codec, and SpeechTokenizer.
Another important observation is that the acoustic reconstruction capability of codecs at low token rates remains relatively limited.
For instance, DAC operating at 600 tokens achieves a SPK SIM  of 0.95 and a PESQ score exceeding 4.
In contrast, current codecs at lower token rates attain SPK SIM values below 0.85 and PESQ scores around 3.
However, compared to earlier models like DAC and Encodec, there has been significant improvement in performance at low token rates.
We believe that there is substantial potential for further advancements in low token rate codecs.

</td><td>

</td></tr></table>

### TTS Experiments

#### Experimental Details

<table><tr><td width="50%">

All TTS models are trained for 3 epochs with a batch size of 2 million tokens and a maximum learning rate of 5e-5.
We employ a cosine learning rate schedule with a warmup phase covering 3\% of an epoch, and the final learning rate is set to 10\% of the peak learning rate.
During training, text sequences are tokenized and placed on the left, followed by tokenized speech sequences on the right, forming a concatenated sequence that is then cropped to a maximum length of 2048 tokens.

Our training dataset integrates multiple high-quality speech datasets, including **Libriheavy** [^kang2024libriheavy], the Chinese-English subset from the **Emilia corpus** [^he2024emilia], **WenetSpeech4TTS** [^ma2024wenetspeech4tts], and our internal data.
This diverse collection constitutes a large-scale training corpus of 250,000 hours, comprising mixed Mandarin Chinese and English speech data.
All textual content maintains its original punctuation.

Our TTS systems are evaluated through a series of comprehensive experiments designed to assess various aspects of performance.

[^kang2024libriheavy]: [**Libriheavy**: A 50,000 Hours ASR Corpus with Punctuation Casing and Context.](../../Datasets/2023.09.15_Libriheavy.md) ICASSP2024.
[^ma2024wenetspeech4tts]: [**WenetSpeech4TTS**: A 12,800-Hour Mandarin TTS Corpus for Large Speech Generation Model Benchmark.](../../Datasets/2024.06.09_WenetSpeech4TTS.md) ArXiv2024.

</td><td>

</td></tr></table>

##### Evaluation of Text Understanding Ability

<table><tr><td width="50%">

Similar to [^lajszczak2024base], we evaluated the models' text understanding capabilities by focusing on their ability to accurately comprehend and synthesize speech from complex textual inputs.
This assessment aimed to evaluate the text understanding abilities of TTS systems.
The full testset is in Appendix \ref{testset}, each sentence was synthesized five times by each model.
Due to the absence of speech prompts, timbre and style were generated randomly.
The evaluation criteria are also in Appendix \ref{testset}; a linguistic expert rated the TTS outputs using a discrete 3-point scale, and we calculated the average score for each category.

</td><td>

</td></tr></table>

##### Evaluation of In-Context Learning Ability

<table><tr><td width="50%">

To assess the in-context learning capability of our model, we conducted experiments on three test sets: Seed-TTS-Eval, LibriSpeech test-clean, and the ESD dataset.

The first two datasets primarily evaluate the model's ability to clone the voice of an unseen speaker given a short speech clip, focusing on speaker similarity.
For both, speaker similarity (SIM) and Word Error Rate (WER) are used as key evaluation metrics:

**Seed-TTS-Eval** [^anastassiou2024seed] [URL](https://github.com/BytedanceSpeech/seed-tts-eval) consists of three subsets: test-zh, test-en, and test-hard.
These experiments focus on cross-sentence speaker similarity and the generation of intelligible speech.
**LibriSpeech test-clean** [^panayotov2015librispeech] is a widely used benchmark for English zero-shot TTS evaluation (**VALL-E** [^wang2023neural]), providing a standardized setting to assess the model’s ability to generate natural and intelligible speech from unseen speakers.

The third test set, **ESD (Emotional Speech Dataset)** ([^zhou2021seen]; [^zhou2021emotional]), evaluates the model's ability to clone emotions in speech.
This dataset consists of 10 native English speakers and 10 native Chinese speakers, covering five emotion categories: neutral, happy, angry, sad and surprised.
For reproducibility, we selected the longest utterance from each speaker as the prompt and the second longest as the ground truth, resulting in a total of 100 evaluation samples (50 English, 50 Chinese).
We used Emotion2Vec-Plus-Large [^ma2023emotion2vec] [URL](https://huggingface.co/emotion2vec/emotion2vec\_plus\_large) to measure emotional similarity.

Through these evaluations, we aimed to provide a comprehensive assessment of our TTS models, ensuring their effectiveness in speaker identity preservation, intelligibility, and emotional expressiveness across diverse linguistic and contextual challenges.

[^zhou2021seen]: [**ESD**: Seen and Unseen Emotional Style Transfer for Voice Conversion with a New Emotional Speech Dataset](../../Datasets/2020.10.28_ESD.md) ICASSP2021.

</td><td>

</td></tr></table>

#### Scaling Train-Time Compute

<table><tr><td width="50%">

**Text Understanding Ability**

Figure \ref{fig:mos_comparison} presents expert scores for Chinese TTS tasks, including Emotion, Paralinguistics, Poetry, Polyphonic Characters, Tongue Twisters, Questions, and Rare Characters.
Increasing both model size (1B → 3B → 8B) and training data (80k → 160k → 250k hours) generally improves performance.
Specifically, simpler tasks like Questions already achieve strong results, with only marginal gains from further scaling.
In contrast, larger models (e.g., 8B parameters) yield significant improvements in emotional speech, Chinese poetry, and tongue twisters, where deeper semantic comprehension is essential.
Meanwhile, rare characters benefit most from broader data coverage, as increasing model size alone has little effect.
We also evaluate our models on English TTS tasks—Compound Nouns, Emotions, Foreign Words, Paralinguistics, Punctuations, Questions, and Syntactic Complexity.
As with Chinese, scaling up both the model size (1B → 3B → 8B) and training data (80k → 160k → 250k hours) generally yields higher expert scores.
We observe that both Compound Nouns and Foreign Words primarily benefit from increased training data rather than model scaling, suggesting that a wider variety of examples is necessary for correct pronunciation and lexical coverage.

</td></tr>
<tr><td>

**In-context Learning Ability**

Based on Table \ref{seedttseval}, Table \ref{emosim}, and Table \ref{librispeecheval}, we observe that as the model size and training data increase, the metrics for speaker similarity, word error rate, and emotional similarity consistently improve, reflecting the enhancement of in-context learning ability.

</td><td>

</td></tr></table>

#### Scaling Inference-Time Compute

<table><tr><td width="50%">

In this section, we conduct a series of experiments to explore how increasing inference-time compute affects the performance of different strategies, such as PRM and ORM.
All experiments are conducted using the Llasa-1B-250K model and evaluated on seed-tts-eval test-hard testset.
We begin by evaluating zero-shot TTS performance using  SIM-O  and WER  metrics, with direct inference serving as the baseline.
Our initial goal is to improve SIM, utilizing two key strategies: PRM with beam search and  ORM  with best-of-$N$.
The similarity verifier is a WavLM-finetuned speaker verification model.

For beam search, we generate $B$ candidate beams conditioned on the speech prompt, where each beam is expanded by $M$ tokens per step.
We set $M=25$, corresponding to $0.5$~seconds in our experiments.
Each beam then expands into $N=16$ new candidate sequences.
From the resulting pool of $B\times N$ sequences, we select the top~$B$ based on their similarity scores.
This process repeats until an end-of-sequence (EOS) token is generated or the sequence length reaches~$2048$.

For best-of-$N$, we generate multiple independent responses and select the one with the highest similarity score as the final output.
To match the compute budget of beam search, we also produce $B\times N$ candidates.

As shown in Figure~\ref{scaletest}, our simplest method for improving SIM is best-of-$N$.
The \textcolor{orange}{orange line} indicates that as inference-time compute grows, SIM improves markedly.
Next, the \textcolor{blue}{blue line} shows that PRM beam search outperforms ORM under the same compute budget.

However, when we also aim to optimize WER ( using Whisper~Large~v3 as a verifier), we select the lowest-WER candidate at the final PRM step from the $B\times N$ sequences.
The \textcolor{red}{red line} reveals that WER remains poor, especially for larger beam widths, sometimes lagging behind the \textbf{baseline}.
We suspect that maximizing SIM through PRM leads to locally optimal speech with inadequate diversity.
To address this, we propose a partial PRM strategy, applying PRM only during the first $n$~seconds, then switching to ORM.
Here, $n=2$ in our experiments.
This hybrid approach (the \textcolor{green}{green line}) achieves higher SIM than best-of-$N$ while maintaining WER near ground truth, indicating sufficient diversity.
Finally, substituting the later ORM step with a WER-based verifier (the \textcolor{purple}{purple line}) simultaneously boosts both SIM and WER as inference-time compute increases, demonstrating that this mixed strategy strikes an effective balance between speaker similarity and transcription accuracy.

From another perspective, we also compare the impact of inference-time scaling across different model sizes and test sets.
In these evaluations, we fix the beam width at 16.
As shown in Tables \ref{librispeecheval}, \ref{emosim}, and \ref{seedttseval}, our results show that inference-time scaling not only improves speaker similarity but also enhances emotion similarity.
Additionally, we generally observe that larger models benefit more from inference-time scaling.
However, for certain relatively simple tasks and metrics, the performance gap between large and small models after inference-time scaling remains minimal.
This finding suggests a possible parallel with text-based LLMs: rather than solely focusing on scaling model training, in some settings, a more efficient approach may be to train smaller models with less compute and then leverage inference-time compute to enhance outputs.

</td><td>

</td></tr></table>

#### Compare with Baseline Model

<table><tr><td width="50%">

In the previous sections, we analyzed our codec and explored the impact of scaling train-time and inference-time compute on the performance of TTS systems.
In this section, we directly compare our model against other TTS baselines.

</td></tr>
<tr><td>

For Seed-TTS-Eval, we select the following baseline models: **Seed-TTS** [^anastassiou2024seed], **MaskGCT** [^wang2024maskgct], **E2-TTS** (32 NFE)$^\dagger$ [^eskimez2024e2], **F5-TTS** (32 NFE) [^chen2024f5], **CosyVoice** [^du2024cosyvoice], **CosyVoice2** [^du2024cosyvoice2], and **FireRedTTS** [^guo2024fireredtts].
Our results are taken from the original papers whenever available; otherwise, they are sourced from CosyVoice 2.

[^wang2024maskgct]: [**MaskGCT**: Zero-Shot Text-To-Speech with Masked Generative Codec Transformer.](2024.09.01_MaskGCT.md) ArXiv2024.
[^eskimez2024e2]: [**E2 TTS**: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS.](../Diffusion/2024.06.26_E2_TTS.md) SLT2024.
[^chen2024f5]: [**F5-TTS**: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching.](../Diffusion/2024.10.09_F5-TTS.md) ArXiv2024.
[^du2024cosyvoice2]: [**Cosyvoice2**: Scalable Streaming Speech Synthesis with Large Language Models.](2024.12.13_CosyVoice2.md) ArXiv2024.

The results, as shown in Table \ref{seedttseval}, indicate that for direct inference, our model achieves WER performance comparable to these state-of-the-art (SOTA) models.
However, one notable limitation of our approach is SIM-O.
As discussed earlier, the reconstruction capability of single-token codecs remains constrained compared to mel-based vocoder reconstructions or RVQ codec-based reconstructions used in the baselines.

For LibriSpeech test-clean, we compare against the following baselines: **ELLA-V** [^song2024ella], **VALL-E R** [^han2024vall], **CLaM-TTS** [^kimclam], **VALL-E** [^wang2023neural], **VALL-E 2** [^chen2024vall], **VoiceBox** [^le2024voicebox], and **MELLE** [^meng2024autoregressive].

[^song2024ella]: [**ELLA-V**: Stable Neural Codec Language Modeling with Alignment-Guided Sequence Reordering.](2024.01.14_ELLA-V.md) ArXiv2024.
[^han2024vall]: [**VALL-E R**: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment.](2024.06.12_VALL-E_R.md) ArXiv2024.
[^kimclam]: [**CLaM-TTS**: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech.](2024.04.03_CLaM-TTS.md) ICLR.
[^le2024voicebox]: [**VoiceBox**: Text-Guided Multilingual Universal Speech Generation at Scale.](2023.06.23_VoiceBox.md) NeurIPS2024.

As shown in Table \ref{librispeecheval}, we observe similar trends in continuous TTS for LibriSpeech test-clean.
However, an interesting finding is that our model achieves a high SIM-r score, particularly on LibriSpeech test-clean, where our best SIM-R (0.626) is already very close to the ground truth (GT) codec resynthesis (0.638).
Given that the continuous generation task is fully aligned with the autoregressive training paradigm, this suggests that, from a generative modeling perspective, a single Transformer-based architecture does not inherently suffer disadvantages from metric Sim-r and WER compared to carefully designed AR+NAR hybrid architectures.
The only drawback of Sim-o may arise at the system level, particularly in the final step of codec acoustic reconstruction, where converting the intermediate representation back into a waveform may introduce limitations due to single VQ as mentioned in Sec \ref{codec_analyse}.

From an inference-time scaling perspective, our approach outperforms all baselines.
While this comparison might not be entirely fair—as our inference process utilizes more computational resources—it presents an alternative viewpoint: if the goal is to achieve the best possible quality, disregarding computational constraints, inference-time scaling provides a viable solution.
Notably, as shown in table \ref{seedttseval} our model achieves a WER of 3.12 on the test-hard of Seed-TTS-Eval, demonstrating that allocating more compute at inference time is particularly beneficial for synthesizing challenging speech, effectively addressing cases where previous models have struggled.

</td><td>

</td></tr></table>

## 5·Extending to Speech Understanding Tasks: 语音理解任务扩展

<table><tr><td width="50%">

While we have primarily demonstrated the viability of our single Transformer + tokenizer approach for TTS, we also explored its performance on speech understanding task, in particular, ASR.
The only modification is to swap the position of speech and text tokens: speech tokens come first, followed by text tokens, and during training we apply the cross-entropy loss solely to the text tokens.
We use the same tokenizer X-codec2.
ASR models are trained on **Libriheavy** [^kang2024libriheavy], **MLS English** [^Pratap2020MLSAL], and **GigaSpeech** [^chen2021gigaspeech] under a learning rate of $2\times10^{-5}$, a batch size of 1M tokens, a 0.03 warmup ratio, and 2 training epochs.
Table~\ref{tab:asr_results} presents ASR results on LibriSpeech.
Notably, on the test-clean set, our model is competitive with Whisper Large v3.
Performance on test-other, however, remains weaker—likely due to our smaller, relatively clean training set and the lack of data augmentation.
Despite these limitations, our experiments confirm that an entirely discrete ASR paradigm operating on quantized speech tokens can still achieve promising results, comparable in many respects to mainstream approaches like Whisper, which relies on continuous Mel features.

[^chen2021gigaspeech]: [**GigaSpeech**: An Evolving, Multi-Domain ASR Corpus with 10,000 Hours of Transcribed Audio.](../../Datasets/2021.06.13_GigaSpeech.md) ArXiv2021.

</td><td>

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

This paper presents ***LLaSA***, a scalable TTS system that aligns with text LLM architectures, using a single Transformer and a tokenizer.
We systematically explore train-time and inference-time compute scaling, showing that larger models and datasets improve speech naturalness, prosody, and text comprehension.
Additionally, inference-time scaling, leveraging speech understanding models as verifiers, enhances speaker similarity, emotional expressiveness, and content accuracy.
Our experiments confirm state-of-the-art performance with strong zero-shot TTS capabilities.
We release our models publicly to drive further research.

</td><td>

</td></tr></table>
