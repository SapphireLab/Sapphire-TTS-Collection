# VITA-1.5

<details>
<summary>基本信息</summary>

- 标题: "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction"
- 作者:
  - 01 Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.01957)
  - [Publication]()
  - [Github](https://github.com/VITA-MLLM/VITA)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2501.01957v1__VITA-1.5__Towards_GPT-4o_Level_Real-Time_Vision_and_Speech_Interaction.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction.
However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences.
In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction.
Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed.
By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction.

## 1·Introduction: 引言

Recent advancements in MLLMs~\citep{dai2024instructblip, liu2023visual, zhang2024mme,chen2024far,team2024chameleon,zhan2024anygpt,gpt4o,fu2024mme} have led to significant progress, particularly in the integration of visual and textual modalities.
The introduction of visual information into LLMs has notably enhanced model capabilities across a range of multimodal tasks.
However, with the growing appeal of human-computer interaction, the role of the speech modality has become increasingly prominent, especially in the multimodal dialogue system.
In such a system, speech not only serves as a key medium for information transmission but also greatly improves the naturalness and convenience of interactions.
Consequently, integrating both visual and speech modalities to achieve high-performance multimodal interactions has emerged as a critical research focus.

The integration of vision and speech in MLLMs is not straightforward due to their inherently differences~\citep{oneațua2022improving}.
For example, visual data, such as images, convey spatial information, while speech data convey dynamic changes in time series.
These fundamental differences pose challenges for simultaneous optimization of both modalities, often leading to conflicts during training.
For instance, the inclusion of speech data may degrade performance on vision tasks, and vice versa.
In addition, traditional speech-to-speech systems rely on separate modules for Automatic Speech Recognition (ASR) and Text-to-Speech, which can increase latency and reduce coherence, limiting their practicality in real-time applications~\citep{reddy2023speech,fu2024vita,zhang2023speechgpt}.

In this paper, we introduce VITA-1.5, a multimodal LLM that integrates vision, language, and speech through a carefully designed three-stage training methodology.
The training strategy progressively incorporates vision and speech data, relieving modality conflicts while maintaining strong multimodal performance.
In the first stage, we focus on vision-language by training visual adapters and fine-tuning the model with descriptive caption and visual QA data.
This step establishes the model’s foundational visual capabilities, enabling robust image and video understanding.
The second stage introduces audio input processing by training an audio encoder using speech-transcription paired data, followed by fine-tuning with speech QA data.
This stage equips the model with the ability to understand and respond to audio inputs effectively.
Finally, in the third stage, we train an audio decoder to enable end-to-end speech output, eliminating the need for external TTS modules.
This allows VITA-1.5 to generate fluent speech replies, enhancing the naturalness and interactivity of multimodal dialogue systems.

We have conducted extensive evaluations on various benchmarks related to image, video, and speech understanding, comparing the results with both open-source and proprietary models.
VITA-1.5 demonstrates comparable perception and reasoning capabilities comparable to leading image/video based MLLMs, and shows significant improvements in the speech capability.

## 2·Related Works: 相关工作

Recently, thanks to the rapid development of language models such as GPTs~\citep{gpt4,brown2020language}, LLaMA~\citep{touvron2023llama,touvron2023llama2}, Alpaca~\citep{taori2023stanford}, Vicuna~\citep{chiang2023vicuna}, and Mistral~\citep{jiang2023mistral}, researchers have successfully extended text comprehension to multimodal understanding/reasoning through techniques like multimodal alignment and instruction tuning.
For example, models such as LLaVA~\citep{liu2023visual}, Qwen-VL~\citep{bai2023qwen}, Cambrian-1~\citep{tong2024cambrian}, Mini-Gemini~\citep{li2024mini}, MiniCPM-V 2.5~\citep{hu2024minicpm}, DeepSeek-VL~\citep{lu2024deepseek}, and SliME~\citep{zhang2024beyond} have made significant advances in image perception and reasoning, while models like LongVA~\citep{zhang2024longva} and Video-LLaVA~\citep{lin2023video} have showcased the latest progress in video understanding.
These models are increasingly capable of handling diverse data types, driving the continuous improvement of multimodal perception and understanding capabilities.

However, compared to proprietary models that support multiple modalities, including audio, image, and text (e.g., GPT-4o~\citep{gpt4o} and Gemini-Pro 1.5~\citep{team2023gemini}), most open-source models have primarily focused on image and text modalities~\citep{zhan2024anygpt}.
Moreover, few open-source models have involved multimodal interaction capabilities, which is a relatively unexplored area.
While works like VITA-1.0~\citep{fu2024vita} have made initial attempts to introduce speech for human-computer interaction, introducing additional speech data poses challenges to the model’s original multimodal abilities.
Furthermore, speech generation typically relies on existing TTS systems, which often results in high latency, thus impacting user experience.
In this paper, we present VITA-1.5 that leverages a refined training strategies, excelling in perceiving data across four modalities (video, image, text, and audio), while also realizing near real-time vision and speech interaction.

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论