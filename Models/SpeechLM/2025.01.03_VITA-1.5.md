# VITA-1.5

<details>
<summary>基本信息</summary>

- 标题: "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction"
- 作者:
  - 01 Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.01957)
  - [Publication]()
  - [Github](https://github.com/VITA-MLLM/VITA)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2501.01957v1__VITA-1.5__Towards_GPT-4o_Level_Real-Time_Vision_and_Speech_Interaction.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction.
However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences.
In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction.
Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed.
By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction.

## 1·Introduction: 引言

Recent advancements in MLLMs~\citep{dai2024instructblip, liu2023visual, zhang2024mme,chen2024far,team2024chameleon,zhan2024anygpt,gpt4o,fu2024mme} have led to significant progress, particularly in the integration of visual and textual modalities.
The introduction of visual information into LLMs has notably enhanced model capabilities across a range of multimodal tasks.
However, with the growing appeal of human-computer interaction, the role of the speech modality has become increasingly prominent, especially in the multimodal dialogue system.
In such a system, speech not only serves as a key medium for information transmission but also greatly improves the naturalness and convenience of interactions.
Consequently, integrating both visual and speech modalities to achieve high-performance multimodal interactions has emerged as a critical research focus.

The integration of vision and speech in MLLMs is not straightforward due to their inherently differences~\citep{oneațua2022improving}.
For example, visual data, such as images, convey spatial information, while speech data convey dynamic changes in time series.
These fundamental differences pose challenges for simultaneous optimization of both modalities, often leading to conflicts during training.
For instance, the inclusion of speech data may degrade performance on vision tasks, and vice versa.
In addition, traditional speech-to-speech systems rely on separate modules for Automatic Speech Recognition (ASR) and Text-to-Speech, which can increase latency and reduce coherence, limiting their practicality in real-time applications~\citep{reddy2023speech,fu2024vita,zhang2023speechgpt}.

In this paper, we introduce VITA-1.5, a multimodal LLM that integrates vision, language, and speech through a carefully designed three-stage training methodology.
The training strategy progressively incorporates vision and speech data, relieving modality conflicts while maintaining strong multimodal performance.
In the first stage, we focus on vision-language by training visual adapters and fine-tuning the model with descriptive caption and visual QA data.
This step establishes the model’s foundational visual capabilities, enabling robust image and video understanding.
The second stage introduces audio input processing by training an audio encoder using speech-transcription paired data, followed by fine-tuning with speech QA data.
This stage equips the model with the ability to understand and respond to audio inputs effectively.
Finally, in the third stage, we train an audio decoder to enable end-to-end speech output, eliminating the need for external TTS modules.
This allows VITA-1.5 to generate fluent speech replies, enhancing the naturalness and interactivity of multimodal dialogue systems.

We have conducted extensive evaluations on various benchmarks related to image, video, and speech understanding, comparing the results with both open-source and proprietary models.
VITA-1.5 demonstrates comparable perception and reasoning capabilities comparable to leading image/video based MLLMs, and shows significant improvements in the speech capability.

## 2·Related Works: 相关工作

Recently, thanks to the rapid development of language models such as GPTs~\citep{gpt4,brown2020language}, LLaMA~\citep{touvron2023llama,touvron2023llama2}, Alpaca~\citep{taori2023stanford}, Vicuna~\citep{chiang2023vicuna}, and Mistral~\citep{jiang2023mistral}, researchers have successfully extended text comprehension to multimodal understanding/reasoning through techniques like multimodal alignment and instruction tuning.
For example, models such as LLaVA~\citep{liu2023visual}, Qwen-VL~\citep{bai2023qwen}, Cambrian-1~\citep{tong2024cambrian}, Mini-Gemini~\citep{li2024mini}, MiniCPM-V 2.5~\citep{hu2024minicpm}, DeepSeek-VL~\citep{lu2024deepseek}, and SliME~\citep{zhang2024beyond} have made significant advances in image perception and reasoning, while models like LongVA~\citep{zhang2024longva} and Video-LLaVA~\citep{lin2023video} have showcased the latest progress in video understanding.
These models are increasingly capable of handling diverse data types, driving the continuous improvement of multimodal perception and understanding capabilities.

However, compared to proprietary models that support multiple modalities, including audio, image, and text (e.g., GPT-4o~\citep{gpt4o} and Gemini-Pro 1.5~\citep{team2023gemini}), most open-source models have primarily focused on image and text modalities~\citep{zhan2024anygpt}.
Moreover, few open-source models have involved multimodal interaction capabilities, which is a relatively unexplored area.
While works like VITA-1.0~\citep{fu2024vita} have made initial attempts to introduce speech for human-computer interaction, introducing additional speech data poses challenges to the model’s original multimodal abilities.
Furthermore, speech generation typically relies on existing TTS systems, which often results in high latency, thus impacting user experience.
In this paper, we present VITA-1.5 that leverages a refined training strategies, excelling in perceiving data across four modalities (video, image, text, and audio), while also realizing near real-time vision and speech interaction.

## 3·Methodology: 方法

The overall architecture of VITA-1.5 is depicted in Fig.~\ref{fig:model}.
The input side is the same as that of the VITA-1.0 version~\citep{fu2024vita}, that is, adopting the configuration of ``Multimodal Encoder-Adaptor-LLM''.
It combines the Vision/Audio Transformer and the Multi-Layer Connector with an LLM for joint training, aiming to enhance the unified understanding of vision, language, and audio.
With respect to the output side, VITA-1.5 has its own end-to-end speech module, instead of using the external TTS model like the original VITA-1.0 version.

### Visional Modality

#### Visual Encoder

VITA-1.5 adopts [InternViT-300M [HF Hub]](https://huggingface.co/OpenGVLab/InternViT-300M-448px) as the visual encoder, with an input image size of 448×448 pixels, generating 256 visual tokens per image.
For high-resolution images, VITA-1.5 employs a dynamic patching~\citep{chen2024far} strategy to capture local details, improving the accuracy of image understanding.

#### Video Processing

Videos are treated as a special type of multiple-image input.
If the video length is shorter than 4 seconds, 4 frames are uniformly sampled; for videos between 4 and 16 seconds, one frame per second is sampled; for videos longer than 16 seconds, 16 frames are uniformly sampled.
No dynamic patching is applied to video frames to avoid excessive visual tokens that could hinder processing efficiency.

#### Vision Adapter

A two-layer MLP is used to map the visual features to visual tokens suitable for the subsequent understanding of LLM.

### Audio Modality

#### Speech Encoder

Similar to~\citep{wang2024freeze}, our audio encoding module consists of multiple downsampling convolutional layers (4x downsampling) and 24 Transformer blocks (with a hidden size of 1024).
The downsampling layers help reduce the frame rate of the audio features, improving the processing speed of LLM.
The audio encoder has about 350M parameters and an output frame rate of 12.5Hz.
Mel-filter bank features are used as the input of the audio encoder, with a window size of 25ms and a shift of 10ms~\citep{wang2024freeze}.

#### Speech Adapter

It consists of multiple convolutional layers with 2x downsampling.

#### Speech Decoder

TiCodec~\citep{ren2024fewer} is used as our codec model, customizing a single codebook with a size of 1024.
This single-codebook design simplifies the decoding process during the inference phase.
The codec model is responsible for encoding continuous speech signals into discrete speech tokens with the frequency of 40Hz, and at the same time has the ability to decode them back into speech signals with the sample rate of 24,000Hz.

The current LLM can only output text tokens, and the speech generation capability requires the LLM to be able to output speech tokens.
To this end, we add two speech decoders after the text tokens following~\citep{wang2024freeze}:
1) \textbf{Non-Autoregressive (NAR) Speech Decoder}, which processes text tokens globally and models semantic features, with the aim of generating an initial distribution of speech tokens;
2) \textbf{Autoregressive (AR) Speech Decoder} generates higher quality speech tokens step by step, based on the speech information produced by the NAR decoder.

The final sequence of speech tokens is then decoded into a continuous speech signal flow (waveform) using the speech decoder of the Codec model.
We adopt 4 LLaMA decoder layers for both NAR and AR speech decoders, where the hidden size is 896 and the parameter size is about 120M.

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论

In this paper, we has presented VITA-1.5, a multimodal LLM designed to integrate vision and speech through a carefully crafted three stage training strategy.
By relieving the inherent conflicts between modalities, VITA-1.5 achieves robust capabilities in both vision and speech understanding, enabling efficient speech-to-speech interactions without relying on separate ASR or TTS modules.
Extensive evaluations demonstrate that VITA-1.5 performs competitively across multimodal benchmarks.
We hope that VITA-1.5 can take over the banner of VITA-1.0 and continue to promote the progress of open-source models in the field of real-time multimodal interaction.