# VITA-1.5

<details>
<summary>基本信息</summary>

- 标题: "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction"
- 作者:
  - 01 Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.01957)
  - [Publication]()
  - [Github](https://github.com/VITA-MLLM/VITA)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2501.01957v1__VITA-1.5__Towards_GPT-4o_Level_Real-Time_Vision_and_Speech_Interaction.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction.
However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences.
In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction.
Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed.
By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction.

## 1·Introduction: 引言

Recent advancements in MLLMs ([13] [31] [67] [10] [49] [61] [42] [17]) ~\citep{dai2024instructblip, liu2023visual, zhang2024mme,chen2024far,team2024chameleon,zhan2024anygpt,gpt4o,fu2024mme} have led to significant progress, particularly in the integration of visual and textual modalities.
The introduction of visual information into LLMs has notably enhanced model capabilities across a range of multimodal tasks.
However, with the growing appeal of human-computer interaction, the role of the speech modality has become increasingly prominent, especially in the multimodal dialogue system.
In such a system, speech not only serves as a key medium for information transmission but also greatly improves the naturalness and convenience of interactions.
Consequently, integrating both visual and speech modalities to achieve high-performance multimodal interactions has emerged as a critical research focus.

The integration of vision and speech in MLLMs is not straightforward due to their inherently differences~\citep{oneațua2022improving} [40].
For example, visual data, such as images, convey spatial information, while speech data convey dynamic changes in time series.
These fundamental differences pose challenges for simultaneous optimization of both modalities, often leading to conflicts during training.
For instance, the inclusion of speech data may degrade performance on vision tasks, and vice versa.
In addition, traditional speech-to-speech systems rely on separate modules for Automatic Speech Recognition (ASR) and Text-to-Speech, which can increase latency and reduce coherence, limiting their practicality in real-time applications~\citep{reddy2023speech,zhang2023speechgpt} ([44] [VITA-1.0 [16]](2024.08.09_VITA.md); [63]).

In this paper, we introduce ***VITA-1.5***, a multimodal LLM that integrates vision, language, and speech through a carefully designed three-stage training methodology.
The training strategy progressively incorporates vision and speech data, relieving modality conflicts while maintaining strong multimodal performance.
In the first stage, we focus on vision-language by training visual adapters and fine-tuning the model with descriptive caption and visual QA data.
This step establishes the model’s foundational visual capabilities, enabling robust image and video understanding.
The second stage introduces audio input processing by training an audio encoder using speech-transcription paired data, followed by fine-tuning with speech QA data.
This stage equips the model with the ability to understand and respond to audio inputs effectively.
Finally, in the third stage, we train an audio decoder to enable end-to-end speech output, eliminating the need for external TTS modules.
This allows ***VITA-1.5*** to generate fluent speech replies, enhancing the naturalness and interactivity of multimodal dialogue systems.

We have conducted extensive evaluations on various benchmarks related to image, video, and speech understanding, comparing the results with both open-source and proprietary models.
***VITA-1.5*** demonstrates comparable perception and reasoning capabilities comparable to leading image/video based MLLMs, and shows significant improvements in the speech capability.

## 2·Related Works: 相关工作

Recently, thanks to the rapid development of language models such as GPTs ([GPT-3 [3]](../TextLM/2020.05.28_GPT-3.md); [GPT-4 [41]](../TextLM/2023.03.15_GPT-4.md)), LLaMA ([LLaMA [52]](../TextLM/2023.02.27_LLaMA.md); [LLaMA2 [53]](../TextLM/2023.07.18_LLaMA2.md)), [Alpaca [48]](../TextLM/Standford_Alpaca.md), Vicuna [12] ~\citep{chiang2023vicuna}, and [Mistral [24]](../TextLM/2023.10.10_Mistral-7B.md), researchers have successfully extended text comprehension to multimodal understanding/reasoning through techniques like multimodal alignment and instruction tuning.
For example, models such as LLaVA [31] ~\citep{liu2023visual}, Qwen-VL [2] ~\citep{bai2023qwen}, Cambrian-1 [51] ~\citep{tong2024cambrian}, Mini-Gemini [28] ~\citep{li2024mini}, MiniCPM-V 2.5 [23] ~\citep{hu2024minicpm}, DeepSeek-VL [36] ~\citep{lu2024deepseek}, and SliME [66] ~\citep{zhang2024beyond} have made significant advances in image perception and reasoning, while models like LongVA [65] ~\citep{zhang2024longva} and Video-LLaVA [29] ~\citep{lin2023video} have showcased the latest progress in video understanding.
These models are increasingly capable of handling diverse data types, driving the continuous improvement of multimodal perception and understanding capabilities.

However, compared to proprietary models that support multiple modalities, including audio, image, and text (e.g., [GPT-4o [42]] ~\citep{gpt4o} and [Gemini-Pro 1.5 [50]] ~\citep{team2023gemini}), most open-source models have primarily focused on image and text modalities ([AnyGPT [61]](2024.02.19_AnyGPT.md)).
Moreover, few open-source models have involved multimodal interaction capabilities, which is a relatively unexplored area.
While works like [VITA-1.0 [16]](2024.08.09_VITA.md) have made initial attempts to introduce speech for human-computer interaction, introducing additional speech data poses challenges to the model’s original multimodal abilities.
Furthermore, speech generation typically relies on existing TTS systems, which often results in high latency, thus impacting user experience.
In this paper, we present ***VITA-1.5*** that leverages a refined training strategies, excelling in perceiving data across four modalities (video, image, text, and audio), while also realizing near real-time vision and speech interaction.

## 3·Methodology: 方法

The overall architecture of ***VITA-1.5*** is depicted in Fig.02.
The input side is the same as that of the [VITA-1.0 version [16]](2024.08.09_VITA.md), that is, adopting the configuration of "Multimodal Encoder-Adaptor-LLM".
It combines the Vision/Audio Transformer and the Multi-Layer Connector with an LLM for joint training, aiming to enhance the unified understanding of vision, language, and audio.
With respect to the output side, ***VITA-1.5*** has its own end-to-end speech module, instead of using the external TTS model like the original VITA-1.0 version.

### Visional Modality

#### Visual Encoder

***VITA-1.5*** adopts [InternViT-300M [URL]](https://huggingface.co/OpenGVLab/InternViT-300M-448px) as the visual encoder, with an input image size of 448×448 pixels, generating 256 visual tokens per image.
For high-resolution images, ***VITA-1.5*** employs a dynamic patching [10] ~\citep{chen2024far} strategy to capture local details, improving the accuracy of image understanding.

#### Video Processing

Videos are treated as a special type of multiple-image input.
If the video length is shorter than 4 seconds, 4 frames are uniformly sampled; for videos between 4 and 16 seconds, one frame per second is sampled; for videos longer than 16 seconds, 16 frames are uniformly sampled.
No dynamic patching is applied to video frames to avoid excessive visual tokens that could hinder processing efficiency.

#### Vision Adapter

A two-layer MLP is used to map the visual features to visual tokens suitable for the subsequent understanding of LLM.

### Audio Modality

#### Speech Encoder

Similar to [Freeze-Omni [56]](2024.11.01_Freeze-Omni.md), our audio encoding module consists of multiple downsampling convolutional layers (4x downsampling) and 24 Transformer blocks (with a hidden size of 1024).
The downsampling layers help reduce the frame rate of the audio features, improving the processing speed of LLM.
The audio encoder has about 350M parameters and an output frame rate of 12.5Hz.
Mel-filter bank features are used as the input of the audio encoder, with a window size of 25ms and a shift of 10ms ([Freeze-Omni [56]](2024.11.01_Freeze-Omni.md)).

#### Speech Adapter

It consists of multiple convolutional layers with 2x downsampling.

#### Speech Decoder

TiCodec [45] ~\citep{ren2024fewer} is used as our codec model, customizing a single codebook with a size of 1024.
This single-codebook design simplifies the decoding process during the inference phase.
The codec model is responsible for encoding continuous speech signals into discrete speech tokens with the frequency of 40Hz, and at the same time has the ability to decode them back into speech signals with the sample rate of 24,000Hz.

The current LLM can only output text tokens, and the speech generation capability requires the LLM to be able to output speech tokens.
To this end, we add two speech decoders after the text tokens following [Freeze-Omni [56]](2024.11.01_Freeze-Omni.md):
1) \textbf{Non-Autoregressive (NAR) Speech Decoder}, which processes text tokens globally and models semantic features, with the aim of generating an initial distribution of speech tokens;
2) \textbf{Autoregressive (AR) Speech Decoder} generates higher quality speech tokens step by step, based on the speech information produced by the NAR decoder.

The final sequence of speech tokens is then decoded into a continuous speech signal flow (waveform) using the speech decoder of the Codec model.
We adopt 4 LLaMA decoder layers for both NAR and AR speech decoders, where the hidden size is 896 and the parameter size is about 120M.

## 4·Experiments: 实验

### Training Data

As shown in Table.01, the training data of multimodal instruction tuning encompass a wide range of categories, such as caption data and QA data, both Chinese and English.
During different training phases, subsets of the overall dataset are selectively sampled to serve different objectives.
Specifically, the datasets are categorized as follows:

- **Image Captioning Data**.
Datasets such as ShareGPT4V [9] ~\citep{chen2023sharegpt4v}, ALLaVA-Caption [6] ~\citep{chen2024allava}, [SharedGPT4o-Image [URL]](https://sharegpt4o.github.io/), and synthetic data are used to train the model to generate descriptive languages for images.

- **Image QA Data**.
Datasets like [LLaVA-150K [URL]](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K), [LLaVA-Mixture-sample [31]]~\citep{liu2023visual}, [LVIS-Instruct [55]]~\citep{wang2023instruct4v}, [ScienceQA [38]]~\citep{lu2022learn}, [ChatQA [35]]~\citep{liu2024chatqa}, and subsets sampled from [LLaVA-OV [26]]~\citep{li2024llava}, such as general image QA and mathematical reasoning datasets, are utilized to train the model in answering image-based questions and performing visual reasoning tasks.

- **OCR & Diagram Data**.
This category supports the model in understanding OCR and diagram content, using datasets such as [Anyword-3M [54]] ~\citep{tuo2023anytext}, [ICDAR2019-LSVT [URL]](http://icdar2019.org/), [UReader [58]]~\citep{ye2023ureader}, [SynDOG [URL]](http://naver-clova-ix/synthdog-en), [ICDAR2019-LSVT-QA [URL]](http://icdar2019.org/), and corresponding data sampled from LLaVA-OV.

- **Video Data**.
Datasets like [ShareGemini [47]]~\citep{sharegemini} and synthetic data are used to train the model to handle video inputs and perform tasks such as captioning and video-based QA.

- **Pure Text Data**.
This category enhances the model's capability to understand and generate languages, facilitating text-based QA tasks.

In addition to the image and video data listed in Table.01, 110,000 hours of internal speech-transcription paired ASR data, covering both Chinese and English, are incorporated to train the audio encoder and align the audio encoder with the LLM.
Furthermore, 3,000 hours of text-speech paired data generated by a TTS system are used to train the speech decoder.

### Three Stage Training Strategies

In order to ensure that ***VITA-1.5*** performs well in tasks involving vision, language, and audio, we have to face a key challenge, i.e., training conflicts between different modalities.
For example, adding the speech data could negatively impact the understanding of the vision data, as the features of speech differ significantly from those of vision, causing interference during the learning process.
To address this challenge, we devise a three-stage training strategy as shown in Fig.03.
The core idea is to gradually introduce different modalities into the model, allowing it to increase the power of a new modality while maintaining the power of the existing modalities.

During training, the encoder and adapter of the visual module, as well as the LLM, are trainable.
The key objective of this stage is to enable the model not only to understand visual content but also to answer questions following instructions.

#### Stage 1: Vision-Language Training

##### Stage 1.1 Vision Alignment

In this stage, our goal is to bridge the gap between vision and language.
The features of the former are extracted from the pre-trained vision encoder InternViT-300M, and the latter is introduced through the LLM.
We use 20\% of the descriptive caption data from Table.01 for training, where only the visual adapter is trainable, while the other modules are frozen.
This approach allows the LLM to initially align the visual modality.

##### Stage 1.2 Vision Understanding

In this stage, our goal is to teach the LLM to transcribe image content.
Toward this end, we use all the descriptive caption data from Table.01.
During this process, the encoder and adapter of the visual module, as well as the LLM, are trainable.
The focus is to enable the model to establish a strong connection between vision and language by learning from descriptive texts about images, allowing it to understand image content via generating natural language descriptions.

##### Stage 1.3 Vision SFT

Following Stage 1.2, the model has acquired a basic understanding of images and videos.
However, the instruction following ability is still limited, and it is difficult to cope with the visual QA task.
To achieve this, we use all the QA data from Table.01 while retaining 20\% of the descriptive caption data to increase the diversity of the dataset and the complexity of the tasks.

#### Stage 2: Audio Input Tuning

##### Stage 2.1 Audio Alignment

After completing the training of Stage 1, the model has developed a strong foundation in image and video understanding.
In this stage, our goal is to reduce the discrepancy between audio and language based on Stage 1, enabling the LLM to understand audio inputs.
The training data consists of 11,000 hours of speech-transcription pairs.
We follow a two-step approach:
(a) **Speech Encoder Training**:
We adopt a training framework used in common speech recognition systems, using a Connectionist Temporal Classification (CTC) loss function [18] ~\citep{graves2006connectionist} to train the speech encoder.
The aim is for the encoder to predict the transcription text from the speech input.
This step ensures that the audio encoder can extract speech features and map them to the text representation space.
(b) **Speech Adapter Training**:
After training the speech encoder, we integrate it with the LLM, using an audio adapter to introduce audio features into the input layer of the LLM.
The training objective at this stage is to enable the LLM to output the transcription text of the speech data.

Besides, in step (b), we introduce special trainable input tokens to guide the speech understanding process.
These tokens provide additional contextual information that guides the LLM used for the QA task to perform the ASR task.

##### Stage 2.2 Audio SFT

The focus of this stage is to introduce the QA functionality with speech questions and text answers.
To achieve this, we sample 4\% of the caption data and 20\% of the QA data from Table.01.
In terms of data processing, approximately half of the text-based questions are randomly replaced with their corresponding speech versions, generated using a TTS system.

In this stage, both the visual encoder and adapter, the audio encoder and adapter, as well as the LLM are trainable, aiming to improve the model's adaptability with multimodal inputs.
In addition, we add a classification head to the LLM's output.
This head is used to distinguish whether the input comes from speech or text.
As a result, the model can more accurately interpret speech inputs and process different modalities efficiently and flexibly.

#### Stage 3: Audio Output Tuning

In the first two stages of training, the ***VITA-1.5*** model has effectively developed its multimodal understanding capabilities.
However, a crucial capacity, i.e., speech output, remains absent, which is essential for its role as an interactive assistant.
To introduce speech output functionality without compromising the model’s fundamental abilities, we draw on the strategy ([Freeze-Omni [56]](2024.11.01_Freeze-Omni.md)), using 3,000 hours of text-speech data and employing a two-step training approach (see Fig.03).

##### Stage 3.1 Codec Training

The goal of this step is to train a codec model with a single codebook using speech data.
The encoder of the codec model has the ability to map speech to discrete tokens, while the decoder can map the discrete tokens back to speech stream.
During the inference phase of ***VITA-1.5***, only the decoder is used.

##### Stage 3.2 NAR + AR Decoder Training

The training of this stage uses text-speech paired data, where the text is fed into the tokenizer and the embedding later of the LLM to obtain its embedding vectors, and the speech is fed into the encoder of the codec model to obtain its speech tokens.
The text embedding vectors are sent to the NAR speech decoder to get global semantic features, and then the features are sent to the AR speech decoder, which predicts the corresponding speech tokens.
Note that the LLM is frozen during this stage, thus the multimodal performance is not affected.

## 5·Results: 结果

### Vision-Language Evaluation

#### Baselines

We compare a series of open-source MLLMs, including [VILA-1.5 [30]] ~\citep{lin2023vila}, [LLaVA-Next [25]]~\citep{li2024llavanext-strong}, [CogVLM2 [22]]~\citep{hong2024cogvlm2}, [InternLM-XComposer2.5 [64]]~\citep{zhang2023internlm}, [Cambrian-1 [51]]~\citep{tong2024cambrian}, [MiniCPM-V-2.6 [23]]~\citep{hu2024minicpm}, [Ovis1.5 [39]]~\citep{lu2024ovis}, [InternVL-Chat-1.5, InternVL-2 [11]]~\citep{chen2023internvl}, [LLaVA-OV [26]]~\citep{li2024llava}, and [Video-LLaVA [29]]~\citep{lin2023video}, [SliME [66]]~\citep{zhang2024beyond}, and [LongVA [65]]~\citep{zhang2024longva}, as well as 5 closed-source MLLMs, including [GPT-4V [URL]](https://openai.com/index/gpt-4v-system-card/), [GPT-4o [URL]](https://openai.com/index/hello-gpt-4o/), GPT-4o-mini, [Gemini 1.5 Pro [50]]~\citep{team2023gemini}, and [Claude 3.5 Sonnet [URL]](https://www.anthropic.com/news/claude-3-5-sonnet).

#### Evaluation Benchmarks

To assess the image perception and understanding capabilities of ***VITA-1.5***, we utilize several evaluation benchmarks, including [MME [14]]~\citep{fu2023mme}, [MMBench [32]]~\citep{liu2023mmbench}, [MMStar [8]]~\citep{chen2024we}, [MMMU [60]]~\citep{yue2024mmmu}, [MathVista [37]]~\citep{lu2023mathvista}, [HallusionBench [20]]~\citep{guan2024hallusionbench}, [AI2D [21]]~\citep{hiippala2021ai2d}, [OCRBench [34]]~\citep{liu2023hidden}, and [MMVet [59]]~\citep{yu2023mm}.
These benchmarks cover a wide range of aspects, including general multimodal capabilities (e.g., MME, MMBench, and MMMU), mathematical reasoning (MathVista), hallucination detection (HallusionBench), chart (AI2D) and OCR (OCRBench) understanding, providing a comprehensive evaluation results.
For video understanding, we use representative evaluation benchmarks including [Video-MME [15]]~\citep{fu2024video}, [MVBench [27]]~\citep{li2024mvbench}, and [TempCompass [33]]~\citep{liu2024tempcompass}.

#### Vision-Language Capabilities

Table.02 presents a comparison of ***VITA-1.5***'s image understanding performance.
After the training of the three stages, ***VITA-1.5*** performs comparably to the most advanced open-source models and even surpasses some closed-source models like GPT-4V and GPT-4o-mini.
This result highlights the robust capabilities of ***VITA-1.5*** in image-language tasks.
As shown in Table.03, ***VITA-1.5*** shows comparable performance to the top open-source models in the evaluation of video understanding.
The notable gap compared to proprietary models suggests that ***VITA-1.5*** still has significant room for improvement and potential for further enhancement in video understanding.
Please note that after the training of Stages 2 (Audio Input Tuning) and 3 (Audio Output Tuning), ***VITA-1.5*** retains almost its original visual-language capabilities in Stage 1 (Vision-Language Training).

### Speech Evaluation

#### Baselines

The following three baseline models are used for comparison: [Wav2Vec2-base [1]](../SpeechRepresentation/2020.06.20_Wav2Vec2.0.md), [Mini-Omini2 [57]](2024.10.15_Mini-Omni2.md), [Freeze-Omni [56]](2024.11.01_Freeze-Omni.md), and [VITA-1.0 [16]](2024.08.09_VITA.md).

#### Evaluation Benchmarks

The Mandarin Evaluation Sets consists of three datasets: [AISHELL-1 [4]](../../Datasets/2017.09.16_AISHELL-1.md), test net [GigaSpeech [7]](../../Datasets/2021.06.13_GigaSpeech.md), and test meeting ([WenetSpeech [62]](../../Datasets/2021.10.07_WenetSpeech.md)).
These datasets are used to evaluate the model's performance on Mandarin speech.
The evaluation metric is the Character Error Rate (CER).

The English Evaluation Sets include four datasets: dev-clean, dev-other, test-clean, and test-other ([LibriSpeech [43]](../../Datasets/2015.04.19_LibriSpeech.md)), which are used to evaluate the model's performance on English speech.
The evaluation metric is Word Error Rate (WER).

#### ASR Performance

The evaluation results in Table.04 indicate that ***VITA-1.5*** achieves leading accuracy in both Mandarin and English ASR tasks.
This demonstrates that ***VITA-1.5*** has successfully integrated advanced speech capability to support multimodal interaction.

## 6·Conclusions: 结论

In this paper, we has presented ***VITA-1.5***, a multimodal LLM designed to integrate vision and speech through a carefully crafted three stage training strategy.
By relieving the inherent conflicts between modalities, ***VITA-1.5*** achieves robust capabilities in both vision and speech understanding, enabling efficient speech-to-speech interactions without relying on separate ASR or TTS modules.
Extensive evaluations demonstrate that ***VITA-1.5*** performs competitively across multimodal benchmarks.
We hope that ***VITA-1.5*** can take over the banner of VITA-1.0 and continue to promote the progress of open-source models in the field of real-time multimodal interaction.