# VAT (Very Attentive Tacotron)

<details>
<summary>基本信息</summary>

- 标题: "Very Attentive Tacotron: Robust and Unbounded Length Generalization in Autoregressive Transformer-Based Text-to-Speech"
- 作者:
  - 01 Eric Battenberg (Google DeepMind) ebattenberg@google.com
  - 02 RJ Skerry-Ryan (Google DeepMind) rjryan@google.com
  - 03 Daisy Stanton (Google DeepMind) daisy@google.com
  - 04 Soroosh Mariooryad (Google DeepMind) soroosh@google.com
  - 05 Matt Shannon (Google DeepMind) mattshannon@google.com
  - 06 Julian Salazar (Google DeepMind) julsal@google.com
  - 07 David Kao (Google DeepMind) davidkao@google.com
- 链接:
  - [ArXiv](https://arxiv.org/abs/2410.22179)
  - [Publication]() Submitted to NAACL
  - [Github](https://github.com/google/sequence-layers/blob/main/examples/very_attentive_tacotron.py)
  - [Demo](https://google.github.io/tacotron/publications/very_attentive_tacotron/index.html)
- 文件:
  - [ArXiv](_PDF/2410.22179v1__VAT__Very_Attentive_Tacotron_Robust_and_Unbounded_Length_Generalization_in_Autoregressive_Transformer-Based_Text-to-Speech.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Autoregressive (AR) Transformer-based sequence models are known to have difficulty generalizing to sequences longer than those seen during training.
When applied to text-to-speech (TTS), these models tend to drop or repeat words or produce erratic output, especially for longer utterances.
In this paper, we introduce enhancements aimed at AR Transformer-based encoder-decoder TTS systems that address these robustness and length generalization issues.
Our approach uses an alignment mechanism to provide cross-attention operations with relative location information.
The associated alignment position is learned as a latent property of the model via backprop and requires no external alignment information during training.
While the approach is tailored to the monotonic nature of TTS input-output alignment, it is still able to benefit from the flexible modeling power of interleaved multi-head self- and cross-attention operations.
A system incorporating these improvements, which we call ***Very Attentive Tacotron***, matches the naturalness and expressiveness of a baseline T5-based TTS system, while eliminating problems with repeated or dropped words and enabling generalization to any practical utterance length.

</details>
<br>

已知基于 Transformer 的自回归序列模型在泛化到比训练时见过的序列长度更长的序列时存在困难.
当应用到文本转语音领域, 这些模型往往会出现丢失, 重复或产生不稳定的输出, 尤其是对于较长的句子.

本文介绍了针对基于 Transformer 的自回归编码器解码器文本转语音系统的增强措施, 旨在解决这些健壮性和长度泛化问题.
我们的方法使用了一种对齐机制来提供具有相对位置信息的交互式注意力操作.
相关联的对齐位置是作为模型的潜在属性通过反向传播学习的, 并不需要在训练时提供外部对齐信息.
虽然该方法针对文本转语音的输入输出对齐的单调性进行了优化, 但它仍然能够从交错的多头自注意力和交叉注意力操作的灵活建模能力中受益.
我们将整合了这些改进措施的系统称为 ***Very Attentive Tacotron***, 它与作为基线的基于 T5 的文本转语音系统在自然性和表现力相当, 并消除了重复或丢失单词的问题, 并使其能够泛化到任何实际的话语长度.

## 1.Introduction: 引言

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
