# VAT (Very Attentive Tacotron)

<details>
<summary>基本信息</summary>

- 标题: "Very Attentive Tacotron: Robust and Unbounded Length Generalization in Autoregressive Transformer-Based Text-to-Speech"
- 作者:
  - 01 Eric Battenberg (Google DeepMind) ebattenberg@google.com
  - 02 RJ Skerry-Ryan (Google DeepMind) rjryan@google.com
  - 03 Daisy Stanton (Google DeepMind) daisy@google.com
  - 04 Soroosh Mariooryad (Google DeepMind) soroosh@google.com
  - 05 Matt Shannon (Google DeepMind) mattshannon@google.com
  - 06 Julian Salazar (Google DeepMind) julsal@google.com
  - 07 David Kao (Google DeepMind) davidkao@google.com
- 链接:
  - [ArXiv](https://arxiv.org/abs/2410.22179)
  - [Publication]() Submitted to NAACL
  - [Github](https://github.com/google/sequence-layers/blob/main/examples/very_attentive_tacotron.py)
  - [Demo](https://google.github.io/tacotron/publications/very_attentive_tacotron/index.html)
- 文件:
  - [ArXiv](_PDF/2410.22179v1__VAT__Very_Attentive_Tacotron_Robust_and_Unbounded_Length_Generalization_in_Autoregressive_Transformer-Based_Text-to-Speech.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Autoregressive (AR) Transformer-based sequence models are known to have difficulty generalizing to sequences longer than those seen during training.
When applied to text-to-speech (TTS), these models tend to drop or repeat words or produce erratic output, especially for longer utterances.
In this paper, we introduce enhancements aimed at AR Transformer-based encoder-decoder TTS systems that address these robustness and length generalization issues.
Our approach uses an alignment mechanism to provide cross-attention operations with relative location information.
The associated alignment position is learned as a latent property of the model via backprop and requires no external alignment information during training.
While the approach is tailored to the monotonic nature of TTS input-output alignment, it is still able to benefit from the flexible modeling power of interleaved multi-head self- and cross-attention operations.
A system incorporating these improvements, which we call ***Very Attentive Tacotron***, matches the naturalness and expressiveness of a baseline T5-based TTS system, while eliminating problems with repeated or dropped words and enabling generalization to any practical utterance length.

</details>
<br>

已知基于 Transformer 的自回归序列模型在泛化到比训练时见过的序列长度更长的序列时存在困难.
当应用到文本转语音领域, 这些模型往往会出现丢失, 重复或产生不稳定的输出, 尤其是对于较长的句子.

本文介绍了针对基于 Transformer 的自回归编码器解码器文本转语音系统的增强措施, 旨在解决这些健壮性和长度泛化问题.
我们的方法使用了一种对齐机制来提供具有相对位置信息的交互式注意力操作.
相关联的对齐位置是作为模型的潜在属性通过反向传播学习的, 并不需要在训练时提供外部对齐信息.
虽然该方法针对文本转语音的输入输出对齐的单调性进行了优化, 但它仍然能够从交错的多头自注意力和交叉注意力操作的灵活建模能力中受益.
我们将整合了这些改进措施的系统称为 ***Very Attentive Tacotron***, 它与作为基线的基于 T5 的文本转语音系统在自然性和表现力相当, 并消除了重复或丢失单词的问题, 并使其能够泛化到任何实际的话语长度.

## 1.Introduction: 引言

Autoregressive (AR) Transformer-based sequence models \cite{NIPS2017:transformers} are used today in a majority of state-of-the-art systems across text, vision, and audio domains.
Though swiftly adopted for sequence-to-sequence text-output tasks like machine translation, video captioning \cite{video-captioning-transformer}, and speech recognition \cite{speech-transformer}, adoption for audio-output tasks like text-to-speech (TTS) is notably more recent.
Inherent difficulties in reliably evaluating TTS systems coupled with the low quality of public TTS datasets likely played a role in the delayed adoption.
This was exacerbated by the failure of AR models of continuous sequences (e.g., spectrograms) to match the performance of their discrete counterparts in the text domain.
It was only after the arrival of audio discretization techniques suitable for speech generation tasks \cite{audiolm} that the TTS community began to direct more effort toward AR Transformer-based models.

A key challenge faced by such models, however, stems from their extensive reliance on attention operations, which tend to cause robustness issues that make them unfit for use in a production TTS environment.
Early attention-based TTS models, such as Tacotron \cite{Wang:2017uz:tacotron,Shen:2017:tacotron2}, often exhibited problems like word omission or repetition, and struggled to generalize
beyond the training lengths.
Attempts to mitigate these issues using monotonic alignment mechanisms yielded some success
\cite{Raffel:2017vl:monotonic,Zhang:2018is:forward-attention,battenberg2020:location-relative},
but many ultimately turned to non-autoregressive, duration-based models, which were more robust and efficient during
synthesis \cite{NEURIPS2020_5c3b99e8:glow-tts,shen2020:non-attentive-tacotron,ren2021fastspeech2}.
While significant research has focused on improving robustness and length generalization for Transformers in general, these issues still persist in the latest AR TTS systems \citep{song2024ellavstableneuralcodec:ella-v, du2024valltdecoderonlygenerativetransducer:vall-t}.

In this paper we introduce a system called \textit{Very Attentive Tacotron} (VAT), a discrete AR Transformer-based encoder-decoder model designed for robust speech synthesis.
VAT augments a baseline T5-based \cite{raffel2020exploring:t5} TTS architecture with an alignment mechanism that exploits the monotonic nature of the text-to-speech task, while preserving the powerful modeling capabilities of stacked self- and cross-attention layers.
This leads to virtually limitless length generalization, while matching the naturalness and expressiveness of the T5-TTS baseline system and eliminating issues with repeated or dropped words.

## 2.Related Works: 相关工作

Our work was inspired by AudioLM \cite{audiolm}, which showed impressive speech generation results using a decoder-only AR Transformer trained to model discrete targets produced by a self-supervised speech representation and a neural audio codec.
This was extended with text inputs for TTS in the decoder-only case by VALL-E \cite{wang2023neural:arxiv:vall-e}, and in the encoder-decoder case by SPEAR-TTS \cite{kharitonov2023speak:spear-tts} and MQ-TTS \cite{chen2023vector:mq-tts}.

However, \citet{song2024ellavstableneuralcodec:ella-v} found that VALL-E was highly non-robust (in ways we note are reminiscent of early attention-based TTS)
and proposed ELLA-V, which essentially performs duration modeling by learning to predict ``end-of-phoneme'' markers via forced alignments of training data.
With similar motivations, VALL-T \cite{du2024valltdecoderonlygenerativetransducer:vall-t}
extends VALL-E by replacing the objective with a Transducer mechanism \cite{rnn-t} that marginalizes over hard alignments during training and interacts with alignment-shifted text-side relative position embeddings.
Unlike standard AR decoding, this scheme requires a costly sequence-wide inference pass each time the alignment shifts,
and it is unclear whether it can generalize significantly beyond the training lengths.
Finally, to prevent major transcript deviations, MQ-TTS had to limit cross-attention to a single layer and head which is applied over a very narrow (3-6 input tokens) monotonically advancing window.
In summary, current work has not resolved robustness issues without also sacrificing the power and flexibility of the underlying AR Transformer.

In this work, we focus on encoder-decoder models with cross-attention which we adapt for robustness and length generalization.
The cross-attention operations are informed by a single monotonic alignment position that produces stability without degrading the modeling power of the repeated multi-head cross-attention layers present in the original Transformer architecture.
The scalar alignment position is learned directly and doesn't rely on dynamic programming or forced alignments during training.

## 3.Methodology: 方法

### 3.1.System Overview

Our VAT model and the baseline T5-TTS model are based on the architecture of the T5 encoder-decoder Transformer originally used for a wide variety of NLP tasks \cite{raffel2020exploring:t5} and later used in SPEAR-TTS \cite{kharitonov2023speak:spear-tts}.
The diagrams in Figure~\ref{fig:vat-tts-overview} give an overview of our discrete TTS setup and then breakdown the differences
between the decoders used in the two models.

For audio discretization, we use a neural vocoder paired with a VQ-VAE \cite{van2017neural:vqvae} trained to autoencode the vocoder's input spectrograms.
This allows us to quickly retrain the spectrogram discretization for different bitrates or frame rates without having to retrain an entire neural audio codec model, which can be a time consuming and finicky process.
The VQ-VAE is trained using a simple L1 reconstruction loss,
and to improve reconstruction quality, its encoder yields multiple categorical codes per frame using product quantization (PQ) \cite{el-nouby2023image:product-quantization} over multiple codebooks.
The neural vocoder we use is GAN-based and combines features from Parallel WaveGAN \cite{parallel-wavegan} and Hifi-GAN \cite{NEURIPS2020_c5d73680:hifi-gan}.

Unlike the systems covered in Section~\ref{sec:related-work} that do ``zero-shot'' speaker cloning via audio prompting, we learn separate speaker embeddings for each speaker in the training set.
We find this better suited for medium-sized datasets and industry use cases, where we care about creating high quality voices for specific target speakers.

At the input side, we use a self-attention-based encoder to process the text followed by an autoregressive decoder that interacts with the encoded text via multiple cross-attention layers.
We train the decoder to model the sequence of PQ categorical codes produced by the VQ-VAE.
Not only is the decoder trained autoregressively across time, but
we also model the joint distribution of the PQ codes contained in one frame using an AR decomposition.
Full system details can be found in Appendix~\ref{app:model-details}.

### 3.2.T5 Relative Position Biases

T5 introduced an efficient parameterization of relative position biases (RPBs) used to encode locality information in self-attention operations,
which was subsequently shown to outperform other position encoding schemes with respect to length generalization on certain sequence tasks \cite{kazemnejad2023impactpositionalencodinglength:nope}.
RPBs are used in dot product self-attention when computing attention scores, $s_{i,j}^{(k)}$, and attention weights, $\alpha_{i,j}^{(k)}$, for attention head $k$:

$$
\begin{aligned}
    s_{i,j}^{(k)} &=
    \frac{\mathbf{q}_i^{(k)} \cdot \mathbf{k}_j^{(k)}}
    {\sqrt{L}}
    + b_{\lfloor f(i-j)\rfloor_0}^{(k)} \\
    \alpha_{i,j}^{(k)} &= \frac{\exp\left(s_{i,j}^{(k)}\right)}{\sum_l \exp\left(s_{i,l}^{(k)}\right)}
\end{aligned}
$$

where $\mathbf{q}_i^{(k)}$ and $\mathbf{k}_j^{(k)}$ are length-$L$ query and key vectors at positions $i$ and $j$, respectively, and $\lfloor x\rfloor_0 \vcentcolon= \text{sgn}(x)\lfloor |x| \rfloor$ rounds toward zero.
The bias, $b_{\lfloor f(i-j)\rfloor_0}^{(k)}$, is taken from a matrix of learned bias values using a function, $f(d)$, to map relative distances into $B$ different buckets:

$$
    f(d) =    \begin{cases}
    d,&
    d \in [0,\frac{B}{2}) \\
    \frac{B}{2} + \frac{\log\left(\frac{d}{\frac{B}{2}}\right)}{\log\left(\frac{D}{\frac{B}{2}}\right)}\left(\frac{B}{2} - 1\right),&
    d \in [\frac{B}{2},D) \\
    B - 1,&
    d \geq D \\
    -f(-d),&
    d < 0
    \end{cases}
$$

So the first half of the buckets are spaced linearly, the second half logarithmically, and distances beyond $D$ are all mapped to the final bucket.
Negative relative distances are associated with a separate set of $B-1$ buckets which we denote using negative indices.
The top of Figure~\ref{fig:irpb-mapping} shows an example of how distances are mapped to RPB buckets for $B=16$ and $D=64$.

While RPBs provide locality information for self-attention, they can't be used in cross-attention because there is no sense of relative distance between encoder and decoder time steps.
This lack of location information in cross-attention is a big reason why attention-based TTS systems tend to have issues with reliability and length generalization.
One way to introduce a sense of relative distance for use with cross-attention RPBs is to compute an alignment position for each decoder time step that serves as the ``zero'' relative position along the time dimension of the encoder outputs.

### 3.3.Interpolated Relative Position Biases

Our approach involves learning the alignment position directly via backprop; however, this means we need to be able to differentiate the RPBs with respect to the alignment position.
Since standard RPBs only deal with integer relative positions, we cannot do this directly.
Instead, we bypass the round-toward-zero operation from the bias index expression in eq.~\eqref{eq:rpb-attention-scores} and use $f(d)$ from eq.~\eqref{eq:dist-function} directly as a real-valued bucket index, $\eta$.
This non-integer index is then translated into a bias value by linearly interpolating between the two bias values at the adjacent integer indices in the bias matrix.
For real-valued bucket index $\eta = f(d)$ and bias matrix row $\mathbf{b}^{(k)}$, the interpolated bias for head $k$ and relative distance $d$ can be written as:

$$
    \beta^{(k)}(d) =
    b_{\lfloor \eta \rfloor_{0}}^{(k)} + \left(|\eta| - \lfloor|\eta|\rfloor\right)\left(b_{\lceil \eta \rceil_{0}}^{(k)}-b_{\lfloor \eta \rfloor_{0}}^{(k)}\right) \\
$$

where $\lceil x\rceil_0 \vcentcolon= \text{sgn}(x)\lceil |x| \rceil$ rounds away from zero.
The bottom of Figure~\ref{fig:irpb-mapping} shows an example of how relative distances are mapped to the bias index weights used to compute these \textit{interpolated} relative position biases, or IRPBs.

### 3.4.Alignment Layer

Because we can backprop through IRPBs, the alignment position can be learned directly as a latent property of the model.
We use an RNN-based alignment layer to update the position at each decoder time step as shown at the top of Figure~\ref{fig:vat-blocks}.
The RNN is fed with both the input to the alignment layer and the output of a multi-head, location-based cross-attention operation where the attention scores are produced using alignment-informed IRPBs and no content-based query-key comparisons:
\begin{equation}
    s_{i,j}^{(k)} = \beta^{(k)}(p_i-j)
    \label{eq:alignment-attention}
\end{equation}
where $j$ is the encoder position, $p_i$ is the alignment position at decoder step $i$, and attention weights are produced using the softmax function in eq.~\eqref{eq:attention-weights-softmax}.

The idea here is that the alignment layer has the fairly simple job of maintaining a rough alignment with the input, while finer-grained, phoneme-level awareness and deeper linguistic understanding can be handled by subsequent cross-attention layers that use content-based queries across multiple heads.
This idea is explored further in Appendix~\ref{app:learned-irpbs} where we visualize the learned IRPBs across all layers.

To enforce monotonicity of the alignment, alignment deltas are produced by projecting the RNN output to a scalar and passing it through a softplus function.
We further process the output of the RNN by composing the alignment layer into an alignment block as shown in the center of Figure~\ref{fig:vat-blocks}.

Because the alignment position is unobserved, it cannot be teacher forced, so the alignment layer needs to be executed serially during training;
however, subsequent decoder layers that consume the alignment position can still be run in parallel.
To minimize the impact of this serialization on training speed, we make the alignment layer as lightweight as possible by using a single-layer LSTM and the location-based attention mechanism in eq.~\eqref{eq:alignment-attention}.

### 3.5.Relative Cross-Attention

To stabilize multi-head cross-attention throughout the rest of the decoder, we augment standard dot product cross-attention with alignment-informed IRPBs:

$$
    s_{i,j}^{(k)} =
    \frac{\mathbf{q}_i^{(k)} \cdot \mathbf{k}_j^{(k)}}
    {\sqrt{L}}
    + \beta^{(k)}(p_i - j)
$$

where $\mathbf{q}_i^{(k)}$ is the query at decoder step i, $\mathbf{k}_j^{(k)}$ is the key at encoder position j,
and attention weights are produced using the softmax function in eq.~\eqref{eq:attention-weights-softmax}.

This operation is used within the relative cross-attention block pictured at the bottom of Figure~\ref{fig:vat-blocks}.
As shown on the right side of Figure~\ref{fig:vat-tts-overview}, the updated alignment position is fed to every instance of relative cross-attention throughout the model,
and each instance separately attends to the encoder outputs via multi-head relative cross-attention.

### 3.6.Initializing IRPBs

In order to reliably learn a meaningful emergent alignment position (as visualized in Figure~\ref{fig:alignment-trajectories}), we found it helpful to use a structured initialization scheme for the cross-attention IRPBs.
We chose a Gaussian window centered at zero relative distance with its maximum value normalized to 1.
Due to the softmax operation used in dot product attention, we take the log of the Gaussian window values when initializing the IRPB matrix, but we find it useful to visualize the exponentiated biases in all figures.

Figure~\ref{fig:irpb-init-scheme} shows three examples of this scheme at different standard deviations, $\sigma$.
Given these initial matrix values, the corresponding effective interpolated biases for different relative distances are shown in Figure~\ref{fig:effective-irpbs-mdp-0}.

In early experiments, we found it necessary to initialize the IRPBs using lower standard deviations that heavily suppress attention contributions from relative distances beyond the training lengths.
This was done to prevent undefined behavior at distances not seen during training and was sufficient to guarantee length generalization.

### 3.7.Maximum Distance Penalty

However, using lower standard deviations for IRPB initialization prevents the cross-attention layers from learning longer distance dependencies, which could degrade the model's text understanding capabilities.
To work around this issue, we incorporate a maximum distance penalty (MDP) that explicitly reduces contributions from relative distances greater than $D$:

$$
    \beta_{\textrm{MD}}^{(k)}(d) =
    \begin{cases}
    \beta^{(k)}(d),&
    |d| < D \\
    \beta^{(k)}(d) - P_\textrm{MD} (|d| - D),&
    |d| \geq D
    \end{cases}
$$

where $P_\textrm{MD}$ is the configurable maximum distance penalty.
This allows us to choose wider standard deviations for the Gaussian initialization and still generalize beyond the training lengths.
The effect of this penalty is shown in Figure~\ref{fig:effective-irpbs-mdp-1}.

Despite not being necessary for length generalization in our experiments, we felt it prudent to also apply the MDP to IRPBs used in self-attention layers in order to eliminate additional sources of undefined behavior at relative distances not seen during training.

## 4.Experiments: 实验

### 4.1.Model Configuration

Our main comparison is between the baseline T5-TTS model and the augmented VAT model.
We also compare against additional non-Transformer baseline models that were designed for stability, including Tacotron with GMM-based attention (Tacotron-GMMA) \cite{battenberg2020:location-relative} and the unsupervised duration variant of Non-Attentive Tacotron (NAT) \cite{shen2020:non-attentive-tacotron}, a duration-based model.

The reference configuration for the T5-TTS and VAT models uses 6 decoder blocks with 16 attention heads and hidden width 1024.
For VAT, the alignment block uses a single 256-width LSTM layer and 4 heads in the location-based attention mechanism.
The encoder contains 2 residual convolution stages that downsample the input phoneme sequence by 2x in time, followed by 3 non-causal self-attention blocks with 8 heads and width 512.
All dropout layers use a rate of 0.1.
At test time, we use a sample temperature of 0.7 when sampling from the AR categorical distribution at the decoder output.

The VQ-VAE operates on 80Hz mel spectrograms with 128 bins, downsampling by 2x to produce output at a rate of 40Hz.
Each output frame contains 8 PQ codes with codebook size 256 for an overall bitrate of 2.56 kbps.

For both T5-TTS and VAT, we use position biases with 32 buckets and learn a separate set of biases for every layer, unlike the original T5 paper which shares biases across layers.
For causal self-attention layers in the decoder, all 32 buckets are used for negative relative distances ($B=32$), whereas in cross-attention and non-causal self-attention, the 32 buckets are split evenly between positive and negative distances ($B=16$ per side).
Max distances, $D$, are set to be shorter than the maximum sequence lengths that appear during training,
which are 96 for the encoder outputs (192-length phoneme sequence downsampled by 2x) and 384 for the decoder (9.6-second utterances with 40Hz codes).
We chose max distances of $D=64$ for cross-attention and encoder self-attention and $D=128$ for decoder self-attention.

The T5-TTS model uses standard RPBs in all self-attention operations, including in the encoder.
VAT uses IRPBs in all self- and cross-attention operations, including the purely location-based attention in the alignment layer.
IRPBs use an MDP of $P_\textrm{MD} = 1.0$.
Self-attention bias matrices are randomly initialized using a truncated normal, while cross-attention biases use the Gaussian initialization scheme with $\sigma=15$.

Full model configuration details can be found in Appendix~\ref{app:model-details},
and reference implementations for the encoder and decoder are [available online](https://github.com/google/sequence-layers/blob/main/examples/very_attentive_tacotron.py)

### 4.2.Datasets

We run experiments using two different English language datasets.
The first is an internal multi-speaker dataset containing a variety of audio, including book-reading, news-reading, and assistant-like utterances.
This dataset consists of 670 hours of audio (\texttildelow700,000 total utterances) spoken by 117 distinct speakers.
Also included are audiobook recodings from the \emph{Lessac} dataset used in the 2013 Blizzard challenge \cite{lessac-blizzard-2013}, which we use as a common point of comparison with the single-speaker Tacotron-GMMA baseline model.
%
The second dataset is the clean-460 subset of LibriTTS \cite{zen2019librittscorpusderivedlibrispeech:libritts}, consisting of 213 hours (\texttildelow150,000 utterances) of book-reading audio spoken by 1226 speakers.

### 4.3.Training

We train our T5-TTS and VAT models for 650,000 steps using the Adam optimizer to minimize the negative log-likelihood of the spectrogram VQ codes.
When using the internal multi-speaker dataset, we use the reference configuration from Section~\ref{subsec:model-config}.
When using LibriTTS data, in order to prevent overfitting, we shrink all layer widths to 3/8 scale (e.g., 1024 to 384) compared to the reference configuration.

We use Tacotron-GMMA as a stability-oriented baseline for the internal dataset.
However, we found that single-speaker versions of the model sounded better than ones trained on the full multi-speaker dataset.
Therefore, we only train Tacotron-GMMA on the Lessac single-speaker data and only evaluate using the Lessac voice when comparing against models trained on the full internal multi-speaker dataset.

Non-Attentive Tacotron is used as a LibriTTS baseline and is trained in unsupervised duration mode, following \citet{shen2020:non-attentive-tacotron}.
Full training details are available in Appendix~\ref{app:training-details}.

### 4.4.Evaluation

- **MOS Naturalness.**
  We evaluate synthesis quality using a pool of raters to judge naturalness on a 5-point scale.
  For the Lessac voice, we use 885 utterances from the test set, and for LibriTTS we use 900 utterances from the test set.
  We report 99\% confidence intervals along with the mean rating for each model and the ground truth data.
- **AB7 Side-By-Side (SxS).**
  Since MOS ratings tend to have calibration and noisiness issues, we complement them with side-by-side naturalness ratings where blinded samples from two models are directly compared on a 7-point ([$-3, 3$]) comparative scale.
- **ASR-Based Robustness.**
  For the rated audio, we report character error rate (CER) computed by comparing the input text to the output of a pre-trained speech recognizer that is run on the synthesized audio.
  This addresses the fact that raters don't have access to target transcripts so can't account for dropped or repeated words in their ratings.
- **ASR-Based Length Generalization.**
  Additionally, for each model, we run a length generalization stress test using 1034 transcripts of varying lengths (100--1500 characters) and report CER as utterance length is increased.
- **Repeated Words Stress Test.**
  Attention-based TTS models tend to have trouble when repeated words appear in the input text.
  We test the ability of the T5-TTS and VAT models to correctly synthesize all the words in three repeated-word templates, each instantiated with 1--9 repetitions of a specific word.
  These templates along with additional evaluation details can be found in Appendix~\ref{app:evaluation-details}.

## 5.Results: 结果

To aid the reader, audio examples from the naturalness evaluation, length generalization assessment, and repeated words stress test are [available online](https://google.github.io/tacotron/publications/very_attentive_tacotron/index.html).

### 5.1.Naturalness Evaluation

MOS naturalness results are shown in the left column of Table~\ref{tab:results-tables}.
Note that for both datasets, the confidence intervals are overlapping for all models, so there are no clear winners.
This was surprising given that in informal comparisons the Transformer-based models (T5-TTS and VAT) sounded clearly more expressive and natural to us compared to the Tacotron and NAT baselines.

In cases such as this, side-by-side evals can be helpful due to their increased sensitivity and better calibration.
The naturalness side-by-sides with the VAT model do show that it is preferred over Tacotron-GMMA.
Due to its deterministic regression-based objective, Tacotron sounds less expressive than the Transformer-based models which are trained with a fully probabilistic objective.
VAT also seems to be preferred over NAT, though the result was not quite significant with respect to the 99\% confidence interval.
Listening to the NAT samples, it is clear that its unsupervised duration mechanism was unable to produce a naturally-varying duration predictor, so its samples sound quite monotonous and robotic.
However, some of the raters seemed to prefer its highly enunciated and hyper-intelligible style compared to the more varied and expressive samples produced by VAT and T5-TTS.
Lastly, VAT is shown to be very even with T5-TTS in terms of naturalness, which is expected given that the two models use very similar architectural backbones and identical training objectives.

### 5.2.ASR-Based Robustness

Despite the similarity in naturalness scores between T5-TTS and VAT, the ASR-based robustness results in Table~\ref{tab:results-tables} show that T5-TTS produced a significantly higher CER than other models.
This is due to the fact that it tends to drop or repeat words, especially on longer utterances
(some utterances in the test sets are up to 20 sec long, which exceeds the 9.6 sec training lengths).
Interesting, the NAT model produced a CER below that of the ground truth audio, which is a result of its overly enunciated style.

### 5.3.ASR-Based Length Generalization

Length generalization results are shown in the plots in Figure~\ref{fig:length-gen}
where we plot CER against input text length in characters.
The max training length for the Transformer-based models is 9.6 sec, and we see that soon after the training length is exceeded, the CER of the T5-TTS model sharply increases.
Not only does it drop or repeat individual words, but beyond the training length, it frequently drops or repeats entire clauses and sometimes babbles incomprehensibly.

The other models, including VAT and the two stability-oriented baselines, generalize well all the way up to the max tested lengths (1500 characters, or around 90 seconds).
We can also see that due to the hyper-intelligibility of the duration-based NAT model, it achieves slightly better CER than the VAT model across all text lengths -- though at the expense of expressivity, as is apparent in the audio examples.

### 5.4.Repeated Words Stress Test

The T5-TTS model has difficulty with the repeated words stress test as well.
Over the 27 test phrases, the VAT model makes no mistakes, whereas T5-TTS makes errors on 14 of the phrases (52\%).
These errors tend to become increasingly severe as the number of repetitions is increased.
For example, one of the phrases with 9 target repetitions produces 52 repetitions in the T5-TTS model.
The majority of the mistakes, however, are off by one errors in the number of repetitions, but T5-TTS produced errors on transcripts that contained as few as 2 repetitions of the target word.
Note that these repetition errors occur even when the synthesis length is shorter than the max training length.

## 6.Conclusions: 结论

### Discussion

We have shown that the proposed attention enhancements are able to eliminate robustness issues typically observed in Transformer-based TTS systems while matching the synthesis quality of a contemporary T5-based system.
The VAT model that incorporates these enhancements is able to reliably produce all words in the input text out to seemingly unbounded lengths,
and the alignment-informed, multi-layer, multi-head cross attention it uses is inherently more powerful than the single-phoneme alignment mechanisms used in other robustness-oriented TTS models (see Appendix~\ref{app:learned-irpbs}).

This approach can be directly applied to any encoder-decoder model that uses cross-attention layers in the decoder.
Because the alignment position is constrained to be monotonic, it is best suited for tasks that exhibit broad monotonic alignment between input and output (e.g., TTS and ASR).
However, the flexibility afforded by query-key comparisons within wide IRPB windows should allow the model to adapt well to off-monotonic alignments if needed (e.g., when using unverbalized text).

Due to the scalability of Transformer-based models, future work should test VAT on significantly larger datasets, potentially with encoder or decoder pre-training on unpaired text or audio, respectively.
Additionally,  using fancier, more powerful approaches to discretization and waveform generation is likely to yield low-level audio quality improvements especially if paired with cleaner, higher-quality datasets.

### Limitations

- **Implementation Effort.**
  There is additional implementation and configuration effort required for VAT compared to more homogeneous Transformer-based models;
  however, the code and example configurations we provide should be helpful for researchers attempting to reproduce our work.
- **Training Speed Impact.**
  The primary practical downside of VAT is the potential for slower training speed due to serialization of the alignment layer during training.
  Because most of the model is still able to be trained in parallel, at the training lengths we use, alignment layer serialization had a relatively small impact on training speed (\texttildelow 12 -- 20\% depending on model size).
  Ways to narrow this speed gap further include slimming down the alignment layer RNN or decreasing the VQ-VAE frame rate so that fewer decoder steps are required to model the same amount of audio.
- **Discrete TTS Comparisons.**
  Transformer-based discrete TTS is a rapidly developing area with a short history, so it is difficult to make meaningful quality comparisons with existing systems, especially since dataset size/quality and model scale can vary drastically.
  Additionally, most discrete TTS systems we encountered are based on prompt-based zero-shot speaker cloning which further complicates direct comparisons.
  Therefore, the discrete TTS baseline we use is based on an existing and well-known Transformer architecture (T5) applied directly to multi-speaker TTS.
- **Hyper-Parameter Exploration.**
  A deeper exploration of the effect of various hyper-parameter choices would be helpful, but was beyond the scope of this initial work.
  However, we do cover the most important choices when it comes to enabling robust length generalization in our model.
- **English Language Focus.**
  We use English language datasets in our experiments. Since text-speech alignment tends to be broadly monotonic for the vast majority of written languages, our approach should generalize to other languages; however, this needs to be tested experimentally.
- **Potential Risks.**
  Our work does not introduce any notable societal or ethical risks beyond those that may already exist for long-form text-to-speech in general.
