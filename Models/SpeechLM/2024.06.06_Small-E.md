# Small-E

<details>
<summary>基本信息</summary>

- 标题: "Small-E: Small Language Model with Linear Attention for Efficient Speech Synthesis"
- 作者:
  - 01 Theodor Lemerle 法国索邦大学
  - 02 Nicolas Obin 法国索邦大学
  - 03 Axel Roebel 法国索邦大学
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.04467)
  - [Publication]()
  - [Github](https://github.com/theodorblackbird/lina-speech)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2406.04467v2__Small-E__Small_Language_Model_with_Linear_Attention_for_Efficient_Speech_Synthesis.pdf)
  - [Publication] #TODO

</details>

## Abstract·摘要

Recent advancements in text-to-speech (TTS) powered by language models have showcased remarkable capabilities in achieving naturalness and zero-shot voice cloning.
Notably, the decoder-only transformer is the prominent architecture in this domain.
However, transformers face challenges stemming from their quadratic complexity in sequence length, impeding training on lengthy sequences and resource-constrained hardware.
Moreover they lack specific inductive bias with regards to the monotonic nature of TTS alignments.
In response, we propose to replace transformers with emerging recurrent architectures and introduce specialized cross-attention mechanisms for reducing repeating and skipping issues.
Consequently our architecture can be efficiently trained on long samples and achieve state-of-the-art zero-shot voice cloning against baselines of comparable size.

## 1·Introduction·引言

### 1.1·Context & Related Works

Over the recent years, neural text-to-speech synthesis (TTS) has gained spectacular improvements in terms of quality with a diversity of approaches and paradigms ([2019.05.22_FastSpeech](../TTS2_Acoustic/2019.05.22_FastSpeech.md), [2021.06.11_VITS](../E2E/2021.06.11_VITS.md); [2023.06.23_VoiceBox](2023.06.23_VoiceBox.md), [2023.09.10_VoiceFlow](../Flow/2023.09.10_VoiceFlow.md)).
In particular, discrete speech and audio representations allowed immediate use of well-established decoder-only transformers such as GPT [5] in many state-of-the-art text-to-audio and text-to-speech model.
However, transformers rely on the self-attention “time-mixing”[6] operation which can be efficiently trained in parallel but suffers from quadratic complexity with respect to the sequence length.
The challenge of designing sequence modeling architecture that can compete with transformers has sparked a resurgence in research on recurrent neural networks (RNNs).
This work introduces the broad term “linear attention” to denote this emerging class of RNNs that replaces self-attention for linear complexity “time-mixing” while keeping performances and high training throughput.

This paper primarily relates to speech models formulated as language models (LMs) or employing discrete audio codecs through Residual Vector Quantization (RVQ).
[2023.01.05_VALL-E](2023.01.05_VALL-E.md) employs an autoregressive transformer to predict the first quantizer and a parallel transformer for the residuals.
Before the rise of RVQ codecs, Tortoise [8] achieved a significant improvement through scaling up and leveraged a decoder-only transformer to predict a VQ representation of the mel Some other works introduce semantic codes as low frame rate audio latents, following advancements in self-supervised speech representations.
For instance Bark [9] separately predicts semantic codes from text, first quantizers from semantic codes, and residuals with three decoder-only transformers.
[2023.05.16_SoundStorm](2023.05.16_SoundStorm.md) predicts audio from semantic codes in parallel by leveraging a MaskGit [11] architecture.
In contrast, [2023.04.18_NaturalSpeech2](../Diffusion/2023.04.18_NaturalSpeech2.md) avoids the language model formulation by learning the continuous latents of an RVQ codec with a diffusion model, sidestepping autoregressive modeling or semantic encoding and instead relying on given durations and fundamental frequency.

### 1.2·Linear Surrogate of Decoder Transformer

Unlike previous RNNs such as LSTM or GRU, Transformers are significantly faster to train, do not suffer from vanishing gradient and demonstrate scalability with parameters reaching into the hundreds of billions.
Further hardware-aware implementation of self-attention has established it as a prevalent choice for sequence modeling, including applications in audio processing.
General softmax-based attention involves three sequences, denoted as $\mathbf{Q}\in \mathbb{R}^{N\times d}$, $\mathbf{K}\in \mathbb{R}^{N'\times d}$, and $\mathbf{V}\in \mathbb{R}^{N'\times d'}$, along with an optional mask $\mathbf{M}\in \mathbb{R}^{N\times N'}$.
The attention function in defined as:

$$
  Attention(\mathbf{Q},\mathbf{K},\mathbf{V}) = \text{softmax}(\dfrac{\mathbb{Q}\mathbb{K}^{\mathsf{T}}}{\sqrt{d}}\odot\mathbf{M})\mathbf{V}.\tag{01}
$$

When $\mathbf{Q}$, $\mathbf{K}$, and $\mathbf{V}$ represent different linear projections of the same input sequence $\mathbf{X}\in \mathbb{R}^{N\times d}$ (and are therefore function of $\mathbf{X}$), the resulting function $\mathbf{X}\mapsto Attention(\mathbf{Q},\mathbf{K},\mathbf{V})$ is referred to as self-attention.
Backing the success of GPT-2 [5] and successors for natural language modeling, the decoder-only Transformer architecture can be generalized with the terminology proposed in [6]:

$$

$$



### 1.3·TTS as A Conditional Codec Language Modeling

### 1.4·Positioning & Contributions

## 2·Methodology·方法

### 2.1·Model Architecture

## 3·Experiments·实验

### 3.1·Dataset

### 3.2·Implementation Details

### 3.3·Benchmark

### 3.4·Methodology

#### 3.4.1·Objective Evaluation

#### 3.4.2·Subjective Evaluation

## 4·Results & Discussion·结果与讨论

## 5·Conclusion·结论

In this paper we presented Small-E, a TTS model based on codec language model.
The proposed model tackles limitations of current LM TTS models.
Firstly, we introduced Linear Causal Language Model in place of the traditional decoder-only transformer.
Secondly, we introduced a cross-attention mechanism designed specifically to handle text and speech modalities in the context of TTS, with the idea of preventing the skip and repetition problem of auto-regressive models.
In contrast with existing work, we were able to show that training LM TTS model is interesting even on limited hardware and leads to state-of-the-art quality against model of the same size.
Experimental evaluation demonstrated the efficiency of the proposed model either in terms of training throughput, skip and repetition reduction, as well as naturalness and similarity These to the reference speaker of the generated speech. observations constitute encouraging results opening the way for small and efficient generative TTS models.
In future work we are interested in streaming TTS, taking advantage of the linear complexity of LCLM for very long or embedded synthesis.