# FreeV

<details>
<summary>基本信息</summary>

- 标题: "FreeV: Free Lunch For Vocoders Through Pseudo Inversed Mel Filter"
- 作者:
  - 01 Yuanjun Lv,
  - 02 Hai Li,
  - 03 Ying Yan,
  - 04 Junhui Liu,
  - 05 Danming Xie,
  - 06 Lei Xie
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.08196)
  - [Publication](https://doi.org/10.21437/Interspeech.2024-2407)
  - [Github](https://github.com/bakerbunker/freev)
  - [Demo](https://bakerbunker.github.io/FreeV/)
- 文件:
  - [ArXiv](_PDF/2406.08196v1__FreeV__Free_Lunch_For_Vocoders_Through_Pseudo_Inversed_Mel_Filter.pdf)
  - [Publication](_PDF/2406.08196p0__FreeV__InterSpeech2024.pdf)

</details>

## Abstract: 摘要

<details>
<summary>原文</summary>

Vocoders reconstruct speech waveforms from acoustic features and play a pivotal role in modern TTS systems.
Frequent-domain GAN vocoders like [Vocos](2023.03.01_Vocos.md) and [APNet2](2023.11.20_APNet2.md) have recently seen rapid advancements, outperforming time-domain models in inference speed while achieving comparable audio quality.
However, these frequency-domain vocoders suffer from large parameter sizes, thus introducing extra memory burden.
Inspired by [PriorGrad](2021.06.11_PriorGrad.md) and [SpecGrad](2022.03.31_SpecGrad.md), we employ pseudo-inverse to estimate the amplitude spectrum as the initialization roughly.
This simple initialization significantly mitigates the parameter demand for vocoder.
Based on [APNet2](2023.11.20_APNet2.md) and our streamlined Amplitude prediction branch, we propose our ***FreeV***, compared with its counterpart [APNet2](2023.11.20_APNet2.md), our ***FreeV*** achieves 1.8$\times$ inference speed improvement with nearly half parameters.
Meanwhile, our ***FreeV*** outperforms [APNet2](2023.11.20_APNet2.md) in resynthesis quality, marking a step forward in pursuing real-time, high-fidelity speech synthesis.
Code and checkpoints is available at: https://github.com/BakerBunker/FreeV.

</details>
<br>

声码器从声学特征重建语音波形, 在现代文本转语音 (TTS) 系统中扮演着关键角色.
频域 GAN 声码器，如 [Vocos](2023.03.01_Vocos.md) 和 [APNet2](2023.11.20_APNet2.md), 近期取得了迅速进展, 在保持音频质量相当的同时, 推理速度超过了时域模型.
然而, 这些频域声码器存在参数规模大的问题, 从而增加了额外的内存负担.
受 [PriorGrad](2021.06.11_PriorGrad.md) 和 [SpecGrad](2022.03.31_SpecGrad.md) 的启发, 我们采用伪逆方法来大致估计幅度谱作为初始化.
这种简单的初始化显著减少了声码器的参数需求.
基于 [APNet2](2023.11.20_APNet2.md) 和我们简化的幅度预测分支, 我们提出了 ***FreeV***, 与 [APNet2](2023.11.20_APNet2.md) 相比, 我们的 ***FreeV*** 实现了 1.8 倍的推理速度提升, 同时参数数量几乎减半.
同时, 我们的 ***FreeV*** 在重合成质量上超越了 [APNet2](2023.11.20_APNet2.md), 标志着在追求实时高保真语音合成方面迈出了重要一步.
代码和检查点可在以下链接获取：https://github.com/BakerBunker/FreeV.

## 1·Introduction: 引言

Recently, there has been a rapid advancement in the field of neural vocoders, which transform speech acoustic features into waveforms.
These vocoders play a crucial role in text-to-speech synthesis, voice conversion, and audio enhancement applications.
Within these contexts, the process typically involves a model that predicts a mel-spectrogram from the source text or speech, followed by a vocoder that produces the waveform from the predicted mel-spectrogram.
Consequently, the quality of the synthesized speech, the speed of inference, and the parameter size of the model constitute the three primary metrics for assessing the performance of neural vocoders.

Recent advancements in vocoders, including [iSTFTNet [1]](../Vocoder/2022.03.04_iSTFTNet.md), [Vocos [2]](../Vocoder/2023.03.01_Vocos.md), and [APNet [3]](../Vocoder/2023.05.13_APNet.md), have shifted from the prediction of waveforms in the time domain to the estimation of amplitude and phase spectra in the frequency domain, followed by waveform reconstruction via inverse short-time Fourier transform (ISTFT).
This method circumvents the need to predict extensive time-domain waveforms, thus reducing the models' computational burden.
[iSTFTNet [1]](../Vocoder/2022.03.04_iSTFTNet.md), for example, minimizes the computational complexity by decreasing the upsampling stages and focusing on frequency-domain spectra predictions before employing ISTFT for time-domain signal reconstruction.
[Vocos](2023.03.01_Vocos.md) extends these advancements by removing all upsampling layers and utilizing the [ConvNeXt V2 [4]](../_Basis/2023.01.02_ConvNeXt_V2.md) Block as its foundational layer.
[APNet [3]](../Vocoder/2023.05.13_APNet.md) and [APNet2 [5]](../Vocoder/2023.11.20_APNet2.md) further refine this approach by independently predicting amplitude and phase spectra and incorporating innovative supervision to guide phase spectra estimation.
Nonetheless, with comparable parameter counts, these models often underperform their time-domain counterparts, highlighting potential avenues for optimization in the parameter efficiency of frequency-domain vocoders.

Several diffusion-based vocoders have integrated signal-processing insights to reduce inference steps and improve reconstruction quality.
[PriorGrad [6]](../Vocoder/2021.06.11_PriorGrad.md) initially refines the model's priors by aligning the covariance matrix diagonals with the energy of each frame of the Mel spectrogram.
Extending this innovation, [SpecGrad [7]](../Vocoder/2022.03.31_SpecGrad.md) proposed to adjust the diffusion noise to align its dynamic spectral characteristics with those of the conditioning mel spectrogram.
Moreover, [GLA-Grad [8]](2024.02.09_GLA-Grad.md) enhances the perceived audio quality by embedding the estimated amplitude spectrum into each diffusion step's post-processing stage.
Nevertheless, the reliance on diffusion models results in slower inference speeds, posing challenges for their real-world application.

In this work, we introduce ***FreeV***, a streamlined GAN vocoder enhanced with prior knowledge from signal processing, and tested on the [LJSpeech dataset [9]](../../Datasets/2017.07.05_LJSpeech.md).
The empirical outcomes highlight ***FreeV***'s superior performance characterized by faster convergence in training, a near 50\% reduction in parameter size, and a notable boost in inference speed.
Our contributions can be summarized as follows:

- We innovated by using the product of the Mel spectrogram and the pseudo-inverse of the Mel filter, referred to as the pseudo-amplitude spectrum, as the model's input, effectively easing the model's complexity.
- Drawing on our initial insight, we substantially diminished the spectral prediction branch's parameters and the time required for inference without compromising the quality achieved by the original model.

## 2·Related Works: 相关工作

### PriorGrad & SpecGrad

Based on diffusion-based vocoder [WaveGrad [10]](../Vocoder/2020.09.02_WaveGrad.md), which direct reconstruct the waveform through a DDPM process, Lee \textit{et al.} proposed [PriorGrad [6]](../Vocoder/2021.06.11_PriorGrad.md) by introducing an adaptive prior $\mathcal{N}(\mathbf{0},\mathbf{\Sigma})$, where $\mathbf{\Sigma}$ is computed from input mel spectrogram $X$.
The covariance matrix $\mathbf{\Sigma}$ is given by: $\mathbf{\Sigma}=\mathrm{diag} [(\sigma_1^2,\sigma_2^2,\cdots,\sigma_D^2,)]$, where $\sigma_d^2,$ denotes the signal power at $d$th sample, which is calculated by interpolating the frame energy.  Compared to conventional DDPM-based vocoders, [PriorGrad](2021.06.11_PriorGrad.md) utilizes signal before making the source distribution closer to the target distribution, which simplifies the reconstruction task.

Based on [PriorGrad](2021.06.11_PriorGrad.md), [SpecGrad [7]](../Vocoder/2022.03.31_SpecGrad.md) proposed adjusting the diffusion noise in a way that aligns its dynamic spectral characteristics with those of the conditioning mel spectrogram.
[SpecGrad](2022.03.31_SpecGrad.md) introduced a decomposed covariance matrix and its approximate inverse using the idea from T-F domain filtering, which is conditioned on the mel spectrogram.
This method enhances audio fidelity, especially in high-frequency regions.
We denote the STFT by a matrix $G$, and the ISTFT by a matrix $G^+$, then the time-varying filter $L$ can be expressed as:

$$
    L=G^+DG,
$$

where $D$ is a diagonal matrix that defines the filter, and it is obtained from the spectral envelope.
Then we can obtain covariance matrix $\Sigma=LL^T$ of the standard Gaussian noise $\mathcal{N}(0,\Sigma)$ in the diffusion process.
By introducing more accurate prior to the model, [SpecGrad](2022.03.31_SpecGrad.md) achieves higher reconstruction quality and inference speech than [PriorGrad](2021.06.11_PriorGrad.md).

### APNet & APNet2

As illustrated in Figure~\ref{fig:overall}, [APNet2 [5]](../Vocoder/2023.11.20_APNet2.md) consists of two components: amplitude spectra predictor (ASP) and phase spectra predictor (PSP).
These two components predict the amplitude and phase spectra separately, which are then employed to reconstruct the waveform through ISTFT.
The backbone of [APNet2](2023.11.20_APNet2.md) is [ConvNeXt V2 [4]](../_Basis/2023.01.02_ConvNeXt_V2.md) block, which is proved has strong modeling capability.
In the PSP branch, [APNet [3]](../Vocoder/2023.05.13_APNet.md) proposed the parallel phase estimation architecture at the output end.
The parallel phase estimation takes the output of two convolution layers as the pseudo imaginary part $I$ and real part $R$, then obtains the phase spectra by:

$$
  \arctan(\frac{I}{R})-\frac{\pi}{2}\cdot sgn(I)\cdot[sgn(R)-1]
$$

where $sgn$ is the sign function.

A series of losses are defined in APNet to supervise the generated spectra and waveform.
In addition to the losses used in [HiFi-GAN [11]](../Vocoder/2020.10.12_HiFi-GAN.md), which include Mel loss $loss_{mel}$, generator loss $loss_{g}$, discriminator loss $loss_{d}$, feature matching loss $loss_{fm}$, APNet proposed:

- amplitude spectrum loss $loss_{A}$, which is the L2 distance of the predicted and real amplitude;
- phase spectrogram loss $loss_{P}$, which is the sum of instantaneous phase loss, group delay loss, and phase time difference loss, all phase spectrograms are anti-wrapped;
- STFT spectrogram loss $loss_{S}$, which includes the STFT consistency loss and L1 loss between predicted and real reconstructed STFT spectrogram.

## 3·Methodology: 方法

### 3.1.Amplitude Prior

In this section, we investigate how to obtain a prior signal closer to the real prediction target, which is the amplitude spectrum.
By employing the given Mel spectrum $X$ and the known Mel filter $M$, we aim to obtain an amplitude spectrum $\hat{A}$ that minimizes the distance with the actual amplitude spectrum $A$, while ensuring that the computation is performed with optimal speed, as the following equation:

$$
    \min\left \| \hat{A}M-A \right \|_2
$$

We investigated several existing implementations for this task.
In Section \ref{sec:specgrad}, the [SpecGrad](2022.03.31_SpecGrad.md) method, $G^+DG\epsilon$ requires prior noise $\epsilon$ as input, therefore unsuitable for our goals.
In the implementation by the librosa library, the estimation of $\hat{A}$ employs the Non-Negative Least Squares (NNLS) algorithm to maintain non-negativity.
However, this algorithm is slow due to the need for multiple iterations, prompting the pursuit of a swifter alternative.
TorchAudio's implementation calculates the estimated amplitude spectrum through a singular least squares operation followed by enforcing a minimum threshold to preserve non-negativity.
Despite this, the recurring need for the least squares calculation with each inference introduces speed inefficiencies.

Considering that the Mel filter $M$ remains unchanged throughout the calculations, we can pre-compute its pseudo-inverse, denoted as $M^+$.
Then, to guarantee the non-negativity of the amplitude spectrum and maintain numerical stability in training, we impose a lower bound of $10^{-5}$ on the values of the approximate amplitude spectrum.
We find there are some negative values in the pseudo-inversed mel filter, causing negative blocks in estimated amplitude, which can be easily found in Figure \ref{subfig:recon_wo_abs}, so we add an $\mathrm{Abs}$ function to the product of $M^+$ and $X$.
This allows us to derive the approximate amplitude spectrum $\hat{A}$ using the following equation:

$$
    \hat{A}=\mathrm{max}(\mathrm{Abs}(M^+X),10^{-5})
$$

This enables us to efficiently acquire the estimated amplitude spectrum through a single matrix multiplication operation.

### 3.2.Model Structure

Our model architecture is illustrated in Figure 2, which consists of PSP and ASP, and uses [ConvNeXt V2 [4]](../_Basis/2023.01.02_ConvNeXt_V2.md) as the model's basic block.
PSP includes an input convolutional layer, eight [ConvNeXt V2](../_Basis/2023.01.02_ConvNeXt_V2.md) blocks, and two convolutional layers for parallel phase estimation structure.

Diverging from [APNet2](2023.11.20_APNet2.md)'s ASP, our design substitutes the conventional input convolutional layer with the pre-computed pseudo-inverse Mel filter matrix $M^+$ of the Mel filter $M$ with frozen parameters.
Due to the enhancements highlighted in Section \ref{sec:prior} that substantially ease the model's complexity, the number of [ConvNeXt V2](../_Basis/2023.01.02_ConvNeXt_V2.md) blocks is reduced from eight to a single block, thereby substantially reducing both the parameter footprint and computation time.

Concurrently, the [ConvNeXt V2](../_Basis/2023.01.02_ConvNeXt_V2.md) module's input-output dimensions have been tailored to align with those of the amplitude spectrum, enabling the block to exclusively model the residual between the estimated and real amplitude spectra, further reducing the ASP module's modeling difficulty.
Because the input and output dimensions of the [ConvNeXt V2](../_Basis/2023.01.02_ConvNeXt_V2.md) module match the amplitude spectrum, we removed the output convolutional layer from ASP, further reducing the model's parameter count.

### 3.3.Training Criteria

In the choice of discriminators, we followed the setup in [APNet2 [5]](../Vocoder/2023.11.20_APNet2.md), using MPD and MRD as discriminators and adopting Hinge GAN Loss as the loss function for adversarial learning.
We also retained the other loss functions used by [APNet2](2023.11.20_APNet2.md), which is described in Section \ref{sec:apnet}, and the loss function of the generator and discriminator are denoted as:

$$
\begin{aligned}
    loss_{Gen}&=\lambda_{A}loss_{A}+\lambda_{P}loss_{P}+\lambda_{S}loss_{S}+\lambda_{W}(loss_{mel}+loss_{fm}+loss_{g}) \\
    loss_{Dis}&=loss_{d}
\end{aligned}
$$

where $\lambda_{A}$, $\lambda_{P}$, $\lambda_{S}$, $\lambda_{W}$ are the weights of the loss, which are kept the same as in [APNet2](2023.11.20_APNet2.md).

## 4·Experiments: 实验

To evaluate the effectiveness of our proposed ***FreeV***, we follow the training scheme in [APNet2](2023.11.20_APNet2.md) paper.
Our demos are placed at demo-site\footnote{\url{https://bakerbunker.github.io/FreeV/}}.

### 4.1.Dataset

To ensure consistency, the training dataset follows the same configuration of [APNet2](2023.11.20_APNet2.md).
Thus, the [LJSpeech dataset [9]](../../Datasets/2017.07.05_LJSpeech.md) is used for training and evaluation.
[LJSpeech dataset](../../Datasets/2017.07.05_LJSpeech.md) is a public collection of 13,100 short audio clips featuring a single speaker reading passages from 7 non-fiction books.
The duration of the clips ranges from 1 to 10 seconds, resulting in a total length of approximately 24 hours.
The sampling rate is 22050Hz.
We split the dataset to train, validation, and test sets according to open-source VITS repository\footnote{\url{https://github.com/jaywalnut310/vits/tree/main/filelists}}.

For feature extraction, we use STFT with 1024 bins, a hop size of 256, and a Hann window of length 1024.
For the mel filterbank, 80 filterbanks are used with a higher frequency cutoff at 16 kHz.

### 4.2.Model and Training Setup

We compare our proposed model with
- [HiFi-GAN [11]](../Vocoder/2020.10.12_HiFi-GAN.md): https://github.com/jik876/hifi-gan;
- [iSTFTNet [1]](../Vocoder/2022.03.04_iSTFTNet.md): https://github.com/rishikksh20/iSTFTNet-pytorch;
- [Vocos [2]](../Vocoder/2023.03.01_Vocos.md): https://github.com/gemelo-ai/vocos;
- [APNet2 [5]](../Vocoder/2023.11.20_APNet2.md): https://github.com/redmist328/APNet2.

In Our ***FreeV*** vocoder, the number of [ConvNeXt V2](../_Basis/2023.01.02_ConvNeXt_V2.md) blocks is 8 for PSP and 1 for ASP, the input-output dimension is 512 for PSP and 513 for ASP, the hidden dimension is 1536 for both ASP and PSP.

We trained ***FreeV*** for 1 million steps.
We set the segmentation size to 8192 and the batch size to 16.
We use the AdamW optimizer with $\beta_1=0.8$, $\beta_2=0.99$, and a weight decay of 0.01.
The learning rate is set to $2\times10^{-4}$ and exponentially decays with a factor of 0.99 for each epoch.

### 4.3.Evaluation

Multiple objective evaluations are conducted to compare the performance of these vocoders.
We use seven objective metrics for evaluating the quality of reconstructed speech, including mel-cepstrum distortion (MCD), root mean square error of log amplitude spectra and F0 (LAS-RMSE and F0-RMSE), V/UV F1 for voice and unvoiced part, [short time objective intelligence (STOI)](../../Evaluations/STOI.md) and [perceptual evaluation speech quality (PESQ)](../../Evaluations/PESQ.md).
To evaluate the efficiency of each vocoder, model parameter count (Params) and real-time factor (RTF) are also conducted on NVIDIA A100 for GPU and a single core of Intel Xeon Platinum 8369B for CPU.

For the computational efficiency of the prior, we also conducted RTF and LAS-RMSE evaluations to the NNLS algorithm of librosa, least square algorithm of torchaudio, pseudo-inverse algorithm, and pseudo-inverse algorithm with absolute function mentioned in Section \ref{sec:prior}.

## 5·Results: 结果

We conducted experiments to verify whether our method can improve the efficiency of the vocoder.

### 5.1.Computational Efficiency of Prior

The compute method of the estimated amplitude spectra $\hat{A}$ if our key component.
We find that the inference speed can be affected by the compute speed of the prior.
We compare the compute speed and accuracy on 100 2-second-long speech clips.
As shown in Table \ref{tab:prior_compute}, the pseudo-inverse method is the fastest way to compute the estimated amplitude spectra $\hat{A}$, and the result also shows that the $\mathrm{Abs}$ function can largely reduce the error of amplitude spectrogram estimation.

### 5.2.Model Convergence

In Figure \ref{subfig:AH_mel} and \ref{subfig:AH_amp}, we showcase the amplitude spectrum loss and mel spectrum loss curves related to amplitude spectrum prediction.
From these two curves, it can be seen that even though the number of parameters in the amplitude spectrum prediction branch is significantly reduced, the loss related to amplitude spectrum prediction still remains lower than the baseline [APNet2](2023.11.20_APNet2.md).
This observation affirms the efficacy of the approach described in Section \ref{sec:method}, substantiating a marked decrease in the challenge of amplitude spectrum prediction.
Furthermore, Figure \ref{subfug:AH_ptd} displays the Phase-Time Difference Loss, which bears significant relevance to phase spectrum prediction.
The improvement in amplitude spectrum prediction concurrently benefits phase spectrum accuracy.
We assume that the stability of the amplitude spectrum prediction branch's training engenders more effective optimization of the phase information by the waveform-related loss functions.

Furthermore, we extended our experimentation to the baseline model by substituting its input from the Mel spectrum with the estimated amplitude spectrum $\hat{A}$.
The loss curve illustrated in Figure \ref{fig:loss_other} reveals that this modification also enhanced the early-stage convergence of these models.
This finding suggests that integrating an appropriate prior is advantageous not only for our proposed vocoder but also holds potential efficacy for other vocoder frameworks.

### 5.3.Model Performance

The model's performance was evaluated on the test dataset referenced in Section \ref{sec:dataset}, the results of which are detailed in Table \ref{tab:perf}.
***FreeV*** outperformed in five out of six objective metrics and was surpassed only by HiFiGAN with estimated amplitude spectra in the PESQ metric.
These findings indicate that our method reduces the model's parameter size and elevates the quality of audio reconstruction.
Furthermore, the comparative analysis, which includes both scenarios, with and without the incorporation of the estimated amplitude spectrum $\hat{A}$, reveals that substituting the Mel spectrum $X$ input with the approximate amplitude spectrum $\hat{A}$ can also yield performance gains in standard vocoder configurations.
This observation corroborates the efficacy of our proposed approach.

In parallel, as shown by Table \ref{tab:efficiency}, our model's parameter size is confined to merely a half of that to [APNet2](2023.11.20_APNet2.md), while it achieves 1.8$\times$ inference speed on GPU.
When benchmarked against the time-domain prediction model [HiFi-GAN [11]](../Vocoder/2020.10.12_HiFi-GAN.md), ***FreeV*** not only exhibits a considerable speed enhancement, which is approximately 30$\times$, but also delivers superior audio reconstruction fidelity with comparable parameter count.
These results further underscore the practicality and advantage of our proposed method.

## 6·Conclusions: 结论

In this paper, we investigated the effectiveness of employing pseudo-inverse to roughly estimate the amplitude spectrum as the initial input of the model.
We introduce ***FreeV***, a vocoder framework that leverages estimated amplitude spectrum $\hat{A}$ to simplify the model's predictive complexity.
This approach not only reduces the parameter size but also improves the reconstruction quality compared to [APNet2](2023.11.20_APNet2.md).
Our experimental results demonstrated that our method could effectively reduce the modeling difficulty by simply replacing the input mel spectrogram with the estimated amplitude spectrum $\hat{A}$.
