# ESTVocoder

<details>
<summary>基本信息</summary>

- 标题: "ESTVocoder: An Excitation-Spectral-Transformed Neural Vocoder Conditioned on Mel Spectrogram"
- 作者:
  - 01 Xiao-Hang Jiang,
  - 02 Hui-Peng Du,
  - 03 Yang Ai,
  - 04 Ye-Xin Lu,
  - 05 Zhen-Hua Ling
- 链接:
  - [ArXiv](https://arxiv.org/abs/2411.11258)
  - [Publication]() Accepted by NCMMSC2024
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv]()
  - [Publication] #TODO

</details>

## Abstract: 摘要

This paper proposes ESTVocoder, a novel excitation-spectral-transformed neural vocoder within the framework of source-filter theory.
The ESTVocoder transforms the amplitude and phase spectra of the excitation into the corresponding speech amplitude and phase spectra using a neural filter whose backbone is ConvNeXt v2 blocks.
Finally, the speech waveform is reconstructed through the inverse short-time Fourier transform (ISTFT).
The excitation is constructed based on the F0: for voiced segments, it contains full harmonic information, while for unvoiced segments, it is represented by noise.
The excitation provides the filter with prior knowledge of the amplitude and phase patterns, expecting to reduce the modeling difficulty compared to conventional neural vocoders.
To ensure the fidelity of the synthesized speech, an adversarial training strategy is applied to ESTVocoder with multi-scale and multi-resolution discriminators.
Analysis-synthesis and text-to-speech experiments both confirm that our proposed ESTVocoder outperforms or is comparable to other baseline neural vocoders, e.g., HiFi-GAN, SiFi-GAN, and Vocos, in terms of synthesized speech quality, with a reasonable model complexity and generation speed.
Additional analysis experiments also demonstrate that the introduced excitation effectively accelerates the model's convergence process, thanks to the speech spectral prior information contained in the excitation.

## 1·Introduction: 引言

Text-to-speech (TTS) is a technique that utilizes machines to convert written text content into audible speech.
With the recent advancements in deep learning, there has been a significant improvement in the clarity and naturalness of synthesized speech.
A TTS system typically consists of two key components,i.e., an acoustic model and a vocoder.
The acoustic model is responsible for predicting acoustic features (e.g., mel spectrogram) from the input text.
Subsequently, the vocoder uses these features to generate the final speech waveform.
Besides TTS, other fields such as voice conversion (VC) and singing voice synthesis (SVS) also require a vocoder to reconstruct waveform.
Thus, a robust vocoder is crucial for the field of speech signal processing, which is also the focus of this paper.

Traditional vocoders, such as STRAIGHT \cite{kawahara1999restructuring} and WORLD \cite{morise2016world}, synthesize speech waveform using traditional signal processing methods.
Although they are computationally simple and fast, result in synthesized speech with lower naturalness.
With the development of deep learning, the WaveNet \cite{oord2016wavenet} represents a significant milestone in synthesized speech quality.
WaveNet is an autoregressive neural vocoder capable of producing natural and clear speech waveforms.
Unlike traditional vocoders that rely on traditional signal processing methods, WaveNet relies entirely on end-to-end neural network training.
However, autoregressive neural vocoders generate audio samples sequentially and use previously generated samples to create new ones, leading to extremely low efficiency and high computational complexity.
To address the issue of low efficiency in autoregressive models, researchers have proposed various alternative approaches, including knowledge distillation-based models \cite{oord2018parallel,ping2018clarinet}, flow-based models \cite{prenger2019waveglow,ping2020waveflow}, and glottis-based models \cite{juvela2019glotnet,valin2019lpcnet}.
Although these models have improved inference efficiency, their overall computational complexity remains high.

To overcome the aforementioned issues, non-autoregressive neural vocoders have gradually been proposed.
Non-autoregressive models generate all samples in parallel, offering high computational efficiency.
For instance, HiFi-GAN \cite{kong2020hifi} vocoder maintains high naturalness in synthesized audio thanks to the generative adversarial network (GAN) \cite{goodfellow2014generative} based training while balancing high generation speed.
However, there is still room for efficiency improvement with these vocoders, as they directly predict high-temporal-resolution waveforms from input acoustic features.
The substantial discrepancy in time resolution between the acoustic fratures and waveforms, results in extensive upsampling operations on the acoustic features, leading to significant computational demands.
Thus, subsequent neural vocoders have adopted the approach of predicting amplitude and phase spectrum and then reconstructing the waveform using the inverse short-time Fourier transform (ISTFT).
For instance, Vocos \cite{siuzdak2023vocos} with ConvNeXt blocks \cite{liu2022convnet} as backbone, directly predicts the amplitude and phase spectrum at the same temporal resolution from the input acoustic features, thereby maintaining the same feature resolution at all frame rates.
Vocos has increased its generation speed by more than tenfold compared to HiFi-GAN while maintaining high-quality synthesized speech.

Most of the aforementioned vocoders only use the mel spectrogram as input, which is convenient, but the mel spectrogram is a compressed representation of the amplitude spectrum and may lose some acoustic details.
Therefore, many vocoders that utilize other acoustic features have also been proposed.
A common approach is to enhance vocoder performance based on the source-filter theory framework by introducing the F0 as an additional acoustic feature.
Neural source filter (NSF) model \cite{wang2019neural} is a pioneer in applying neural networks within the source-filter framework, which synthesizes speech waveform directly based on explicit F0 and mel spectrograms.
Recently, some works combining source-filter vocoders with GAN-based training, such as SiFi-GAN \cite{yoneyama2023source} and SF-GAN \cite{lu2022source}, have been proposed.
This type of vocoders generates excitation based on the F0.
The excitation waveform is then processed through a neural filter conditioned on the mel spectrogram to directly produce the final speech waveform.
Experiments have shown that after introducing the additional F0 features, the quality of the speech generated by these vocoders is obviously improved.
However, their excitations are often constructed based on single F0, lacking harmonic information, which may impact the reconstruction performance of the neural filter.
In addition, these methods often rely on direct transformation of the excitation waveform, still leaving room for improvements in efficiency and model complexity.
Excitation-spectral-transformed methods in source-filter-based neural vocoders have not yet been thoroughly investigated.

To achieve high-fidelity speech synthesis as well as efficient training and rapid generation speed, we propose a novel excitation-spectral-transformed neural vocoder called ESTVocoder.
The ESTVocoder is designed based on the source-filter theory and first produces an excitation according to the F0.
Compared to the single-F0-based excitation used by SiFi-GAN \cite{yoneyama2023source} and SF-GAN \cite{lu2022source}, the proposed ESTVocoder utilizes a full-harmonic excitation which contains richer spectral information.
Subsequently, a neural filter with ConvNeXt v2 blocks \cite{woo2023convnext} as the backbone transforms the amplitude and phase spectra of the excitation into the corresponding amplitude and phase spectra of the speech, conditioned on the mel spectrogram.
Finally, the speech waveform is reconstructed via ISTFT.
Both analysis-synthesis and TTS experimental results show that our proposed ESTVocoder outperforms or is comparable to HiFi-GAN, SiFi-GAN, and Vocos, in terms of synthesized speech quality.
Our proposed ESTVocoder also has an extremely fast training convergence speed, thanks to the introduction of spectral prior information contained in the excitation.

This paper is organized as follows:
In Section \ref{sec:pagestyle}, we provide details on our proposed ESTVocoder.
In Section \ref{sec:exp}, we present our experimental results.
Finally, we give conclusions in Section \ref{sec:con}.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论