# WaveFlow

<details>
<summary>基本信息</summary>

- 标题: "WaveFlow: A Compact Flow-based Model for Raw Audio"
- 作者:
  - 01 Wei Ping (Baidu Research)
  - 02 Kainan Peng (Baidu Research)
  - 03 Kexin Zhao (Baidu Research)
  - 04 Zhao Song (Baidu Research)
- 链接:
  - [ArXiv](https://arxiv.org/abs/1912.01219)
  - [Publication](https://proceedings.mlr.press/v119/ping20a.html) ICML2020
  - [Github](https://github.com/PaddlePaddle/Parakeet/tree/develop/examples/waveflow)
  - [Demo](https://waveflow-demo.github.io)
- 文件:
  - [ArXiv](_PDF/1912.01219v4__WaveFlow__A_Compact_Flow-based_Model_for_Raw_Audio.pdf)
  - [Publication](_PDF/1912.01219p0__WaveFlow__ICML2020.pdf)

</details>

## Abstract: 摘要

In this work, we propose WaveFlow, a small-footprint generative flow for raw audio, which is directly trained with maximum likelihood.
It handles the long-range structure of 1-D waveform with a dilated 2-D convolutional architecture, while modeling the local variations using expressive autoregressive functions.
WaveFlow provides a unified view of likelihood-based models for 1-D data, including WaveNet and WaveGlow as special cases.
It generates high-fidelity speech as WaveNet, while synthesizing several orders of magnitude faster as it only requires a few sequential steps to generate very long waveforms with hundreds of thousands of time-steps.
Furthermore, it can significantly reduce the likelihood gap that has existed between autoregressive models and flow-based models for efficient synthesis.
Finally, our small-footprint WaveFlow has only 5.91M parameters, which is 15× smaller than WaveGlow.
It can generate 22.05 kHz high-fidelity audio 42.6× faster than real-time (at a rate of 939.3 kHz) on a V100 GPU without engineered inference kernels.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论