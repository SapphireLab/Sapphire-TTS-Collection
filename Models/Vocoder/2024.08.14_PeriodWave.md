# PeriodWave

<details>
<summary>基本信息</summary>

- 标题: "PeriodWave: Multi-Period Flow Matching for High-Fidelity Waveform Generation"
- 作者:
  - 01 Sang-Hoon Lee,
  - 02 Ha-Yeong Choi,
  - 03 Seong-Whan Lee
- 链接:
  - [ArXiv](https://arxiv.org/abs/2408.07547)
  - [Publication]
  - [Github](https://github.com/sh-lee-prml/PeriodWave)
  - [Demo](https://periodwave.github.io/demo/)
- 文件:
  - [ArXiv](_PDF/2408.07547v1__PeriodWave__Multi-Period_Flow_Matching_for_High-Fidelity_Waveform_Generation.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Recently, universal waveform generation tasks have been investigated conditioned on various out-of-distribution scenarios.
Although GAN-based methods have shown their strength in fast waveform generation, they are vulnerable to train-inference mismatch scenarios such as two-stage text-to-speech.
Meanwhile, diffusion-based models have shown their powerful generative performance in other domains; however, they stay out of the limelight due to slow inference speed in waveform generation tasks.
Above all, there is no generator architecture that can explicitly disentangle the natural periodic features of high-resolution waveform signals.
In this paper, we propose ***PeriodWave***, a novel universal waveform generation model.
First, we introduce a period-aware flow matching estimator that can capture the periodic features of the waveform signal when estimating the vector fields.
Additionally, we utilize a multi-period estimator that avoids overlaps to capture different periodic features of waveform signals.
Although increasing the number of periods can improve the performance significantly, this requires more computational costs.
To reduce this issue, we also propose a single period-conditional universal estimator that can feed-forward parallel by period-wise batch inference.
Additionally, we utilize discrete wavelet transform to losslessly disentangle the frequency information of waveform signals for high-frequency modeling, and introduce FreeU to reduce the high-frequency noise for waveform generation.
The experimental results demonstrated that our model outperforms the previous models both in Mel-spectrogram reconstruction and text-to-speech tasks.
All source code will be available at [Github](https://github.com/sh-lee-prml/PeriodWave).

</details>
<br>

最近, 以各种分布外场景为条件的通用波形生成任务被加以研究.
尽管基于生成对抗网络的方法在快速波形生成方面展示了它们的实力, 但它们在训练-推理不匹配的场景如两阶段文本转语音中容易受到影响.
同时, 基于扩散的模型在其他领域展示了其强大的生成性能, 然而由于其在波形生成任务中的推理速度较慢, 并未得到太多关注.
更重要的是, 目前还没有一种生成器架构能够明确地解耦高分辨率波形信号的自然周期特征.

在本文中, 我们提出了 ***PeriodWave***, 一种新型的通用波形生成模型.
首先, 我们引入了周期感知流匹配估计器, 能够在估计矢量场时捕捉波形信号的周期特征.
此外, 我们使用多周期估计器避免重叠, 以捕捉波形信号的不同周期特征.
虽然增加周期数量可以显著提高性能, 但这需要更多的计算成本.
为了减少这一问题, 我们还提出了一个单周期条件化通用估计器, 能够通过按周期维度的批处理推理来实现并行.
此外, 我们利用离散小波变换来无损地解耦波形信号的频率信息用于高频建模, 并引入了 FreeU 来减少波形生成时的高频噪声.
实验结果表明, 我们的模型在语音合成任务和梅尔频谱重建任务方面都优于之前的模型.
所有源代码将会在 [Github](https://github.com/sh-lee-prml/PeriodWave) 上提供.

## 1·Introduction: 引言

Deep generative models have achieved significant success in high-fidelity waveform generation.
In general, the neural waveform generation model which is called \textit{"Neural Vocoder"} transforms a low-resolution acoustic representation such as Mel-spectrogram or linguistic representations into a high-resolution waveform signal for regeneration learning \citep{tan2024regeneration}.
Conventional neural vocoder models have been investigated for text-to-speech ([WaveNet](2016.09.12_WaveNet.md); \citep{shen2018natural,ren2019fastspeech,kim2020glow,jiang2024megatts} and voice conversion \citep{lee2021voicemixer,choi2021neural}.
Furthermore, recent universal waveform generation models called \textit{"Universal Vocoder"} are getting more attention due to their various applicability in neural audio codec ([SoundStream](../SpeechCodec/2021.07.07_SoundStream.md); [EnCodec](../SpeechCodec/2022.10.24_EnCodec.md); [Decsript-Audio-Codec](../SpeechCodec/2023.06.11_Descript-Audio-Codec.md); ju2024naturalspeech), audio generation \citep{kreuk2023audiogen,roman2023from,yang2023diffsound,huang2023make,pmlr-v202-liu23f}, and zero-shot voice cloning systems \citep{lee2022hierspeech,huang2022generspeech,[VALL-E](../SpeechLM/2023.01.05_VALL-E.md); li2024styletts,le2024voicebox,kim2024p,shen2024naturalspeech} where models can generate high-fidelity waveform signal from the highly compressed representations beyond the traditional acoustic features, Mel-spectrogram.
In addition, universal vocoder requires generalization in various out-of-distribution scenarios including unseen voice, instruments, and dynamic environments ([BigVGAN](2022.06.09_BigVGAN.md); bak2023avocodo).

Previously, generative adversarial networks (GAN) models dominated the waveform generation tasks by introducing various discriminators that can capture the different characters of waveform signals.
[MelGAN](2019.10.08_MelGAN.md) used the multi-scale discriminator to capture different features from the different scales of waveform signal.
[HiFi-GAN](2020.10.12_HiFi-GAN.md) introduced the multi-period discriminator to capture the different periodic patterns of the waveform signal.
[UnivNet](2021.06.15_UnivNet.md) utilized the multi-resolution spectrogram discriminator that can reflect the spectral features of waveform signal.
[BigVGAN](2022.06.09_BigVGAN.md) proposed the Snake activation function for the out-of-distribution modeling and scaled up the neural vocoder for universal waveform generation.
[Vocos](2023.03.01_Vocos.md) significantly improved the efficiency of the neural vocoder without upsampling the time-axis representation.
Although GAN-based models can generate the high-fidelity waveform signal fast, GAN models possess three major limitations:
1) they should utilize a lot of discriminators to improve the audio quality, which increases training time;
2) this also requires hyper-parameter tuning to balance multiple loss terms;
3) they are vulnerable to train-inference mismatch scenarios such as two-state models, which induces metallic sound or hissing noise.

Recently, the multi-band diffusion (MBD) model \citep{roman2023from} sheds light on the effectiveness of the diffusion model for high-resolution waveform modeling.
Although previous diffusion-based waveform models ([DiffWave](2020.09.21_DiffWave.md); [WaveGrad](2020.09.02_WaveGrad.md)) existed, they could not model the high-frequency information so the generated waveform only contains low-frequency information.
Additionally, they still require a lot of iterative steps to generate high-fidelity waveform signals.
To reduce this issue, [PriorGrad](2021.06.11_PriorGrad.md) introduced a data-driven prior and [FastDiff](../Diffusion/2022.04.21_FastDiff.md) adopted an efficient structure and noise schedule predictor.
However, they do not model the high-frequency information so these models only generate the low-frequency information well.

Above all, there is no generator architecture to reflect the natural periodic features of high-resolution waveform signals.
In this paper, we propose ***PeriodWave***, a novel waveform generation model that can reflect different implicit periodic representations.
We also adopt the powerful generative model, flow matching that can estimate the vector fields directly using the optimal transport path for fast sampling.
Additionally, we utilize a multi-period estimator by adopting the prime number to avoid overlaps.
We observed that increasing the number of periods can improve the entire performance consistently.
However, this also induces a slow inference speed.
To simply reduce this limitation, we propose a period-conditional universal estimator that can feed-forward parallel by period-wise batch inference.
Furthermore, we utilize a discrete wavelet transformation (DWT) \citep{lee2022fre} for frequency-wise waveform modeling that can efficiently model the low and high-frequency information, respectively.

***PeriodWave*** achieves a better performance in objective and subjective metrics than other publicly available strong baselines on both speech and out-of-distribution samples.
Specifically, the experimental results demonstrated that our methods can significantly improve the pitch-related metrics including pitch distance, periodicity, and V/UV F1 score with unprecedented performance.
Furthermore, we only train the models for only three days while previous GAN models require over three weeks.

The main contributions of this study are as follows:
- We propose ***PeriodWave***, a novel universal waveform generator that can reflect different implicit periodic information when estimating the vector fields.
- This is the first success utilizing flow matching for waveform-level high-resolution signal modeling, and we thoroughly analyze different ODE methods for waveform generation.
- For efficient and fast inference, we propose a period-conditional universal estimator that can feed-forward the multiple period paths parallel by period-wise batch inference.
- We analyze the limitation of high-frequency modeling for flow matching-based waveform generation.
To reduce this issue, we adopt the DWT for more accurate frequency-wise vector field estimation and FreeU approach for high-frequency noise reduction.
- We will release all source code and checkpoints at [Github](https://github.com/sh-lee-prml/PeriodWave).

## 2·Related Works: 相关工作

### Neural Vocoder

[WaveNet](2016.09.12_WaveNet.md) has successfully paved the way for high-quality neural waveform generation tasks.
However, these auto-regressive (AR) models suffer from a slow inference speed.
To address this limitation, teacher-student distillation-based inverse AR flow methods ([Parallel WaveNet](../Vocoder/2017.11.28_Parallel_WaveNet.md_WaveNet.md); [ClariNet](../E2E/2018.07.19_ClariNet.md)) have been investigated for parallel waveform generation.
Flow-based models ([FloWaveNet](../Vocoder/2018.11.06_FloWaveNet.mdoWaveNet.md); 2018.10.31_WaveGlow.mdWaveGlow.md); 2020.06.11_NanoFlow.mdNanoFlow.md)) have also been utilized, which can be trained by simply maximizing the likelihood of the data using invertible transformation.

### GAN-based Neural Vocoder

[MelGAN](2019.10.08_MelGAN.md) successfully incorporated generative adversarial networks (GAN) into the neural vocoder by introducing a multi-scale discriminator to reflect different features from the different scales of waveform signal and feature matching loss for stable training.
[Parallel WaveGAN](2019.10.25_Parallel_WaveGAN.md) introduces multi-resolution STFT losses that can improve the perceptual quality and robustness of adversarial training.
[GAN-TTS](2019.09.25_GAN-TTS.md) utilized an ensemble of random window discriminators that operate on random segments of waveform signal.
GED \citep{gritsenko2020spectral} proposed a spectral energy distance with unconditional GAN for stable and consistent training.
[HiFi-GAN](2020.10.12_HiFi-GAN.md) introduced a novel discriminator, a multi-period discriminator (MPD) that can capture different periodic features of waveform signal.
[UnivNet](2021.06.15_UnivNet.md) employed adversarial feedback on the multi-resolution spectrogram to capture the spectral representations at different resolutions.
[BigVGAN](2022.06.09_BigVGAN.md) adopted periodic activation function and anti-aliased representation into the generator for generalization on out-of-distribution samples.
[Vocos](2023.03.01_Vocos.md) proposed an efficient waveform generation framework using ConvNeXt blocks and iSTFT head without any temporal domain upsampling.

Meanwhile, neural codec models ([SoundStream](../SpeechCodec/2021.07.07_SoundStream.md); [EnCodec](../SpeechCodec/2022.10.24_EnCodec.md); [Decsript-Audio-Codec](../SpeechCodec/2023.06.11_Descript-Audio-Codec.md)) and applications ([VALL-E](../SpeechLM/2023.01.05_VALL-E.md); [UniAudio](../SpeechLM/2023.10.01_UniAudio.md)) such as TTS and audio generation have been investigated together with the development of neural vocoder.

### Diffusion-based Neural Vocoder

[DiffWave](2020.09.21_DiffWave.md) and [WaveGrad](2020.09.02_WaveGrad.md) introduced a Mel-conditional diffusion-based neural vocoder that can estimate the gradients of the data density.
[PriorGrad](2021.06.11_PriorGrad.md) improves the efficiency of the conditional diffusion model by adopting a data-dependent prior distribution for diffusion models instead of a standard Gaussian distribution.
[FastDiff](../Diffusion/2022.04.21_FastDiff.md) proposed a fast conditional diffusion model by adopting an efficient generator structure and noise schedule predictor.
Multi-band Diffusion \citep{roman2023from} incorporated multi-band waveform modeling into diffusion models and it significantly improved the performance by band-wise modeling because previous diffusion methods could not model high-frequency information, which only generated the low-frequency representations.
This model also focused on raw waveform generation from discrete tokens of neural codec model for various audio generation applications including speech, music, and environmental sound.

## 3·Methodology: 方法

The flow matching model \citep{lipman2022flow,tong2023conditional} has emerged as an effective strategy for the swift and simulation-free training of continuous normalizing flows (CNFs), producing optimal transport (OT) trajectories that are readily incorporable.
We are interested in the use of flow matching models for waveform generation to understand their capability to manage complex transformations across waveform distributions.
Hence, we begin with the essential notation to analyze flow matching with optimal transport, followed by a detailed introduction to the proposed method.

### 3.1.Preliminary: Flow Matching with Optimal Transport Path

In the data space $\mathbb{R}^d$, consider an observation $x \in \mathbb{R}^d$ sampled from an unknown distribution $q(x)$.
CNFs transform a simple prior $p_0$ into a target distribution $p_1 \approx q$ using a time-dependent vector field $v_t$.
The flow $\phi_t$ is defined by the ordinary differential equation:

$$
\frac{d}{dt} \phi_t(x) = v_t(\phi_t(x); \theta), \quad \phi_0(x) = x, \quad x \sim p_0 ,
$$

The flow matching objective, as introduced by \citep{lipman2022flow}, aims to match the vector field $v_t(x)$ to an ideal vector field $u_t(x)$ that would generate the desired probability path $p_t$.
The flow matching training objective involves minimizing the loss function $L_{FM}(\theta)$, which is defined by regressing the model's vector field $v_{\theta}(t, x)$ to a target vector field $u_t(x)$ as follows:

$$
\mathcal{L}_{FM}(\theta) = \mathbb{E}_{t\sim[0,1],x \sim p_t(x)}\left|\left|v_{\theta}(t, x) -  u_t(x)\right|\right|_2^2 .
$$

Given the impracticality of accessing $u_t$ and $p_t$, conditional flow matching (CFM) is introduced:

$$
\mathcal{L}_{CFM}(\theta) = \mathbb{E}_{t\sim[0,1], x\sim p_t(x|z)}\left|\left|v_{\theta}(t, x) -  u_t(x|z)\right|\right|_2^2 .
$$

Generalizing this with the noise condition $x_0 \sim N(0, 1)$, the OT-CFM loss is:

$$
L_{\text{OT-CFM}}(\theta) = \mathbb{E}_{t, q(x_1), p_0(x_0)} \| u_t^{\text{OT}}(\phi_t^{\text{OT}}(x_0) \mid x_1) - v_t(\phi_t^{\text{OT}}(x_0) \mid \mu; \theta) \|^2,
$$

where $\phi_t^{\text{OT}}(x_0) = (1 - (1 - \sigma_{\text{min}})t)x_0 + tx_1$ and $u_t^{\text{OT}}(\phi_t^{\text{OT}}(x_0) \mid x_1) = x_1 - (1 - \sigma_{\text{min}})x_0$.
This approach efficiently manages data transformation and enhances training speed and efficiency by integrating optimal transport paths.
The detailed formulas are described in Appendix \ref{section:preliminary}.

### 3.2.Period-aware Flow Matching Estimator

In this work, we propose a period-aware flow matching estimator, which can reflect the different periodic features when estimating the vector field for high-quality waveform generation as illustrated in Figure \ref{fig:ode}.
First, we utilize a time-conditional UNet-based structure for time-specific vector field estimation.
Unlike previous UNet-based decoders, PeriodWave utilizes a mixture of reshaped input signals with different periods as illustrated in Figure \ref{fig:framework}.
Similar to \citep{kong2020hifi}, we reshape the 1D data sampled from $p_t(x)$ of length $T$ into 2D data of height $T/p$ and width $p$.
We will refer to this process as \textit{Periodify}.
Then, we condition the period embedding to indicate the specific period of each reshaped sample for period-aware feature extraction in a single estimator.
We utilize different periods of [1,2,3,5,7] that avoid overlaps to capture different periodic features from the input signal.
We utilize 2D convolution of down/upsampling layer and ResNet Blocks with a kernel size of 3 and dilation of 1, 2 for each UNet block.
Specifically, we downsample each signal by [4,4,4] so the representation of the middle block has height $T/(p\times64)$ and width $p$.
After extracting the representation for each period, we reshape the 2D representation into the original shape of the 1D signal for each period path.
We sum all representations from all period paths.
The final block estimates the vector fields from a mixture of period representations.

For Mel-spectrogram conditional generation, we only add the conditional representation extracted from Mel-spectrogram to the middle layer representation of UNet for each period path.
We utilize ConvNeXt V2 based Mel encoder to extract the conditional information for efficient time-frequency modeling.
Previously, Vocos \citep{siuzdak2024vocos} also demonstrated that ConvNeXt-based time-frequency modeling shows effectiveness on the low resolution features.
In this works, we utilize the improved ConvNeXt V2 \citep{woo2023convnext} blocks for Mel encoder, and the output of this block is fed to the period-aware flow matching estimator.
Because we utilize a hop size of 256, the Mel-spectrogram has a length of $T/256$.
To align the conditional representation, we upsample it by 4$\times$ and downsample it by the different strides as periods of [1,2,3,5,7] to get a shape of $T/(p\times64)$.

To boost the inference speed, we introduce two methods: 1) period-wise batch inference that can feed-forward parallel for multiple periods by a period-conditional universal estimator; 2) time-shared conditional representation extracted from Mel-spectrogram, which is utilized for every step.

### 3.3.Flow Matching for Waveform Generation

To the best of our knowledge, this is the first work to utilize flow matching for waveform generation.
In this subsection, we describe the problems we encountered and how to reduce these issues.
First, we found that the it is crucial to set the proper noise scale for $x_0$.
In general, waveform signal is ranged with -1 to 1, so standard normal distribution $\mathcal{N}(0, 1)$ would be large for optimal path.
This results in high-frequency information distortion, causing the generated sample to contain only low-frequency information.
To reduce this issue, we scale down the $x_0$ by multiplying a small value $\alpha$.
Although we successfully generate the waveform signal by small $\alpha$, we observed that the generated sample sometimes contains a small white noise.
We simply solve it by additionally multiplying temperature $\tau$ on the $x_0$ as analyzed in Table \ref{Table:Temperature}.
Furthermore, we adopt data-dependent prior \citep{lee2022priorgrad} to flow matching-based generative models.
Specifically, we utilize an energy-based prior which can be simply extracted by averaging the Mel-spectrogram along the frequency axis.
We set $\mathcal N(0,\Sigma)$ for the distribution of $p_0(x)$, and multiply $\Sigma$ by a small value of 0.5.
All of them significantly improve the sample quality and boost the training speed.

### 3.4.High-frequency Information Modeling for Flow Matching

Similar to the findings demonstrated by \citep{roman2023from}, we also observed that flow matching-based waveform generation models could not provide the high-frequency information well.
To address this limitation, we adopt three approaches including multi-band modeling and FreeU \citep{si2023freeu}

#### Multi-band Flow Matching with Discrete Wavelet Transform

Previously, MBD \citep{roman2023from} demonstrated that diffusion-based models are vulnerable to high-frequency noise so they introduce the multi-band diffusion models by disentangling the frequency bands and introducing specialized denoisers for each band.
Additionally, they proposed frequency equalizer (EQ) processor to reduce the white noise by regularizing the noise energy scale for each band.\footnote{We entirely acknowledged \citep{roman2023from} proposed a novel pre-processing method and combined it with diffusion models well.
However, we do not use any pre-processing methods for a fair comparison.
We introduce a novel architecture and method for high-fidelity waveform generation without any pre-processing.}.
Unlike MBD, we introduce a discrete wavelet Transform based multi-band modeling method which can disentangle the signal and reproduce the original signal without losing information\footnote{We observed that using band splitting of MBD without EQ processor results in white noise on the generated sample in our preliminary study so we introduce discrete wavelet Transform based multi-band modeling.}.
PeriodWave-MB consists of multiple vector field estimators for each band [0-3, 3-6, 6-9, 9-12 kHz].
Additionally, we first generate a lower band, and then concatenate the generated lower bands to the $x_0$ to generate higher bands.
We found that this significantly improve the quality even with small sampling steps.
During training, we utilize a ground-truth discrete wavelet Transform components for a conditional information.
Additionally, we also utilize a band-wise data-dependent prior by averaging Mel-spectrogram according to the frequency axis including overlapped frequency bands [0-61, 60-81, 80-93, 91-100 bins].
Moreover, we downsample each signal by [1,4,4] by replacing the first down/up-sampling with DWT/iDWT, and this also significantly reduce the computational cost by reducing time resolution.

#### Flow Matching with FreeU

FreeU \citep{si2023freeu} demonstrated that the features from the skip connection contain high-frequency information in UNet-based diffusion models, and this could ignore the backbone semantics during image generation.
We revisited this issue in high-resolution waveform generation task.
We also found that the skip features of our model contain a large ratio of high-frequency information.
Additionally, this also provided the noisy high-frequency information to the UBlock at the initial sampling steps.
Hence, the accumulated high-frequency noise prevents modeling the high-frequency information of waveform.
To reduce this issue, we adopt FreeU by scaling down the skip features $z_{skip}$ and scaling up the backbone features $x$ as follows:

$$
x = \alpha \cdot z_{skip} + \beta \cdot x
$$

where we found the optimal hyper-parameters through grid search: $\alpha=0.9$ and $\beta=1.1$ at the Table \ref{Table:FreeU}, and this significantly improve the high-frequency modeling performance in terms of spectral distances.
We also found that scaling up the backbone features could improve the perceptual quality by reducing the noisy sound which is included in ground-truth Mel-spectrogram.

## 4·Experiments: 实验

### Dataset

We train the models using LJSpeech \citep{ljspeech17} and LibriTTS \citep{zen19_interspeech} datasets.
LJSpeech is a high-quality single-speaker dataset with a sampling rate of 22,050 Hz.
LibriTTS is a multi-speaker dataset with a sampling rate of 24,000 Hz.
Following \citep{lee2023bigvgan}, we adopt the same configuration for Mel-spectrogram transformation.
For the LJSpeech, we use the Mel-spectrogram of 80 bins.
For the LibriTTS, we utilize the Mel-spectrogram of 100 bins.

### Training

For reproducibility, we will release all source code, checkpoints, and generated samples at \url{https://periodwave.github.io/demo/}.
For the LibriTTS dataset, we train PeriodWave using the AdamW optimizer with a learning rate of 5$\times$10$^{-4}$, batch size of 128 for 1M steps on four NVIDIA A100 GPUs.
Each band of PeriodWave-MB is trained using the AdamW optimizer with a learning rate of 2$\times$10$^{-4}$, batch size of 64 for 1M steps on two NVIDIA A100 GPUs.\footnote{Due to the limited resources, we only used two GPUs for each band.} It only takes three days to train the model while GAN-based models take over three weeks.
We do not apply any learning rate schedule.
For the ablation study, we train the model with a batch size of 128 for 0.5M steps on four NVIDIA A100 GPUs.
For the LJSpeech dataset, we only train the multi-band model for 0.5M steps.

### Sampling

For the ODE sampling, we utilize Midpoint methods with sampling steps of 16\footnote{The results in Appendix \ref{appendix:ode} show that increasing sampling steps improve the performance consistently.}.
Additionally, we compared the ODE methods including Euler, Midpoint, and RK4 methods according to different sampling steps in Appendix \ref{appendix:ode}.
The experimental details are described in Appendix \ref{appendix:experiment} and \ref{appendix:metrics}.

## 5·Results: 结果

### 5.1.LJSpeech: High-quality Single Speaker Dataset with 22,050 Hz

We conducted an objective evaluation to compare the performance of the single-speaker dataset.
We utilized the official implementation and checkpoints of HiFi-GAN, PriorGrad, and FreGrad, which have the same Mel-spectrogram configuration.
Table \ref{table1:LJSpeech} shows that our model achieved a significantly improved performance in all objective metrics without M-STFT.
It is worth noting that our model can achieve a better performance than diffusion baselines even with unprecedented small training steps of 0.05M while other models should be trained over 1M steps.
Additionally, GAN-based models take much more time to train the model due to the discriminators.
Furthermore, our proposed methods require smaller sampling steps than diffusion-based models.
We observed that diffusion-based model and flow matching-based models could not model the high-frequency information because their objective function does not guarantee the high-frequency information while GAN-based models utilize Mel-spectrogram loss and M-STFT-based discriminators.
To reduce this issue, we utilize multi-band modeling and FreeU operation, and the results also show improved performance in most metrics.

### 5.2.LibriTTS: Multi-speaker Dataset with 24,000 Hz

We conducted objective and subjective evaluations to compare the performance of the multi-speaker dataset.
We utilized the publicly available checkpoints of UnivNet, BigVGAN, and Vocos, which are trained with the LibriTTS dataset.
Table \ref{table2:LibriTTS} shows our model significantly improved performance in all metrics but the M-STFT metric.
Although other GAN-based models utilize Mel-spectrogram distance loss and multi-resolution spectrogram discriminators which can minimize the distance on the spectral domain, we only trained the model by minimizing the distance of the vector field on the waveform.
However, our model achieved better performance in subjective evaluation.
Specifically, our models have better performance on the periodicity metrics, and this means that our period-aware structure could improve the performance in terms of pitch and periodicity by significantly reducing the jitter sound.
Both PeriodWave-MB and PeriodWave demonstrated significantly lower pitch error distances compared to BigVGAN.
Specifically, PeriodWave-MB and PeriodWave (FreeU) achieved a pitch error distance of 16.829 and 18.730 (17.398), respectively, while BigVGAN's pitch error distance was 25.651.
Table \ref{Table:trainingsteps} also demonstrated the fast training speed of PeriodWave.
The model trained for 0.15M steps could achieve comparable performance compared to baseline models which are trained over 1M steps.

### 5.3.Sampling Robustness, Diversity, and Controllability

We utilize a flow matching model for PeriodWave, allowing it to generate diverse samples with different Gaussian noise.
However, our goal is a conditional generation using the Mel-spectrogram.
We need to decrease the diversity to improve the robustness of the model.
To achieve this, we can multiply the small scale of temperature $\tau$ to the Gaussian noise during inference.
Table \ref{Table:Temperature} shows that using $\tau$ of 0.667 could improve the performance.
We also observed that samples generated with a $\tau$ of 1.0 contain a small amount of white noise, which decreases perceptual quality despite having the lowest lowest M-STFT metrics.
Furthermore, we could control the energy for each band by using different scales of $\tau$.
This approach could be utilized for a neural EQ that can generate the signal by reflecting the conditioned energy, not merely manipulating the energy of the generated samples.

### 5.4.MUSDB18-HQ: Multi-track Music Audio Dataset for Out-Of-Distribution Robustness

To evaluate the robustness on the out-of-distribution samples, we measure performance on the MUSDB18-HQ dataset that consists of multi-track music audio including vocals, drums, bass, others, and a mixture.
We utilize all test samples including 50 songs with 5 tracks, and randomly sample the 10-second segments for each sample.
Table \ref{table4:oodobjective} shows our model has better performance on all metrics without M-STFT.
Table \ref{Table5:OODSMOS} shows that PeriodWave-MB outperformed the baseline models by improving the out-of-distribution robustness.
Specifically, we significantly improve the performance of bass, the frequency range of which is known between 40 to 400 Hz.
Additionally, we observed that our model significantly reduces the jitter sound in the out-of-distribution samples.

### 5.5.Analysis on Adaptive Sampling Steps for Multi-Band Models

We proposed an adaptive sampling for multi-band models.
We can efficiently reduce the sampling steps for high-frequency bands due to the hierarchical band modeling conditioned on the previously generated DWT components.
Table \ref{Table:AdaptiveSampling} shows that it is important to model the first DWT components.
After sampling the first band, we can significantly reduce the sampling steps for the remaining bands, maintaining the performance with only a small decrease.
The results from the sampling steps of [4,4,8,16] demonstrated that it is important to model the first band for high-fidelity waveform generation and accurate high-frequency modeling could improve the M-STFT metrics.

### 5.6.Ablation Study

#### Different Periods

We conduct ablation study for different periods at the same structure.
Table \ref{table:ablation} shows that the model with a period of 1 shows the lowest performance.
Increasing the number of periods could improve the entire performance in terms of most metrics, consistently.
However, this also improves the computational cost and requires more training steps for optimizing various periods in a single estimator so we fix the model with the period of [1,2,3,5,7].
Meanwhile, we compared the model with periods of [1,2,4,6,8] and [1,2,4,8,16] to demonstrate the effectiveness of the prime number for the period.
We observed that using prime number could improve the UTMOS slightly and the model with periods of [1,2,4,6,8] and [1,2,4,8,16] also have comparable performance, which can reflect the different period representations of the waveform.
We thought that the model with periods of [1,2,3,5,7,11,13,17] requires more training steps.
This also demonstrates that our new waveform generator structure is suitable for waveform generation.
Additionally, our structure could be simply adapted for any structure such as WaveNet and UNet-based models.

#### Prior

PriorGrad demonstrated that data-dependent prior information could improve the performance and sampling speed for diffusion models.
We also utilize the normalized energy which can be extracted Mel-spectrogram as prior information.
We observe that the data-dependent prior could improve the quality and sampling speed in flow matching based models.
Meanwhile, although we failed to implement the quality reported by SpecGrad \citep{koizumi22_interspeech}, we see that the spectrogram-based prior could improve the performance rather than the energy-based prior.

#### Mel Encoder

Our Mel encoder significantly improved the performance through efficient time-frequency modeling.
This only requires a small increase in computation cost because we reused the extracted features which are fed to the period-aware flow matching estimator for each sampling step.

#### Activation Function

We observed that SiLU activation has better performance than ReLU or Leaky-ReLU in our preliminary study.
Recently, the Snake activation function has been utilized for high-quality waveform generation.
However, we failed to train the model with Snake activation, resulting from unstable training issues.
Additionally, it increased the inference speed by 1.5$\times$.
However, we observe that optimizing the model with the Snake activation function could improve the performance so we have a plan to optimize the model with the Snake activation function by decreasing the learning rate, combining Leaky-ReLU with Snake activation for enhanced robustness.
\vspace{-0.2cm}

### 5.7.Single Speaker Text-to-Speech

We conduct two-stage TTS experiments to evaluate the robustness of the proposed models compared to previous GAN-based and diffusion-based models.
We utilized the official implementation of Glow-TTS which is trained with the LJSpeech dataset.
Table \ref{tts} demonstrated that our model has a higher performance on the two-stage TTS in terms of MOS and UTMOS.
Although HiFi-GAN shows a lower performance in reconstruction metrics, we observed that HiFi-GAN shows a high perceptual performance in terms of UTMOS.
BigVGAN-base (14M) has a higher performance than BigVGAN (112M).
We see that BigVGAN could reconstruct the waveform signal from the generated Mel-spectrogram even with the error that might be in the generated Mel-spectrogram.
Although our model has a higher reconstruction performance, our models could refine this phenomenon through iterative generative processes.
Additionally, we found that the generated Mel-spectrogram contains a larger scale of energy compared to the ground-truth Mel-spectrogram, so we utilized $\tau$ of 0.333 for scaling $x_0$.

### 5.8.Multi Speaker Text-to-Speech

We additionally conduct two-stage multi-speaker TTS experiments to further demonstrate the robustness of the proposed models compared to previous large-scale GAN-based models including BigVGAN and BigVSAN \citep{shibuya2024bigvsan}.
Note that BigVGAN and BigVSAN were trained for 5M and 10M steps, respectively.
We utilize ARDiT-TTS \citep{liu2024autoregressive} as zero-shot TTS model which was trained with LibriTTS dataset.
We convert 500 samples of generated Mel-spectrogram into waveform signal by each model.
The Table \ref{ardit-tts} shows that our model has better performance on the objective and subjective metrics in terms of UTMOS and MOS.
Furthermore, Our model with FreeU has much better performance than others.
We can discuss that FreeU could reduce the high-frequency noise resulting in better perceptual quality.

## 6·Conclusions: 结论

In this work, we proposed ***PeriodWave***, a novel universal waveform generation model with conditional flow matching.
Motivated by the multiple periodic characteristics of high-resolution waveform signals, we introduce the period-aware flow matching estimators which can reflect different implicit periodic representations when estimating vector fields.
Furthermore, we observed that increasing the number of periods can improve the performance, and we introduce a period-conditional universal estimator for efficient structure.
By adopting this, we also implement a period-wise batch inference for efficient inference.
The experimental results demonstrate the superiority of our model in high-quality waveform generation and OOD robustness.
GAN-based models still hold great potential and have shown strong performance but require multiple loss functions, resulting in complex training and long training times.
On the other hand, we introduced a new flow matching based approach using a single loss function, which offers a notable advantage.
Furthermore, we see that the pre-trained flow matching generator could be utilized as a teacher model for distillation or fine-tuning.
We hope that our approach will facilitate the study of waveform generation by reducing training time, so we will release all source code and checkpoints.

---

### Broader Impact and Limitation

#### Practical Application

We first introduce a high-fidelity waveform generation model using flow matching.
We demonstrated the out-of-distribution robustness of our model, and this means that the conventional neural vocoder can be replaced with our model.
We see that our models can be utilized for text-to-speech, voice conversion, audio generation, and speech language models for high-quality waveform decoding.
For future work, we will train and release Codec-based ***PeriodWave*** for audio generation and speech language models.

#### Social Negative Impact

Recently, speech AI technology has shown its practical applicability by synthesizing much more realistic audio.
Unfortunately, this also increases the risk of the potential social negative impact including malicious use and ethical issues by deceiving people.
It is important to discuss a countermeasure that can address these potential negative impacts such as fake audio detection, anti-spoofing techniques, and audio watermark generation.

#### Limitation

Although our models could generate the waveform with small sampling steps, Table \ref{appendix:inferencespeed} shows that our models have a slow synthesis speed compared to GAN-based neural vocoder.
To overcome this issue, we will explore distillation methods or adversarial training to reduce the sampling steps for much more fast inference by using our period-aware structure.
Additionally, our models still show a lack of robustness in terms of high-frequency information because we only train the model by estimating the vector fields on the waveform resolution.
Although we utilize multi-band modeling to reduce this issue, we have a plan to add a modified spectral objective function and blocks that can reflect the spectral representations when estimating vector fields by utilizing short-time Fourier convolution proposed in \citep{han22_interspeech} for audio super-resolution.
Moreover, we see that classifier-free guidance could be adapted to our model to improve the audio quality.
