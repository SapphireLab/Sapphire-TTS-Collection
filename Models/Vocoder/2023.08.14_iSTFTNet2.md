# iSTFTNet2

<details>
<summary>基本信息</summary>

- 标题: "iSTFTNet2: Faster and More Lightweight iSTFT-Based Neural Vocoder Using 1D-2D CNN"
- 作者:
  - 01 Takuhiro Kaneko,
  - 02 Hirokazu Kameoka,
  - 03 Kou Tanaka,
  - 04 Shogo Seki
- 链接:
  - [ArXiv](https://arxiv.org/abs/2308.07117)
  - [Publication](https://doi.org/10.21437/Interspeech.2023-1726)
  - [Github]()
  - [Demo](https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet2/)
- 文件:
  - [ArXiv](_PDF/2308.07117v1__iSTFTNet2__Faster_and_More_Lightweight_iSTFT-Based_Neural_Vocoder_Using_1D-2D_CNN.pdf)
  - [Publication](_PDF/2308.07117p0__iSTFTNet2__InterSpeech2023.pdf)

</details>

## Abstract: 摘要

The inverse short-time Fourier transform network (iSTFTNet) has garnered attention owing to its fast, lightweight, and high-fidelity speech synthesis.
It obtains these characteristics using a fast and lightweight 1D CNN as the backbone and replacing some neural processes with iSTFT.
Owing to the difficulty of a 1D CNN to model high-dimensional spectrograms, the frequency dimension is reduced via temporal upsampling.
However, this strategy compromises the potential to enhance the speed.
Therefore, we propose iSTFTNet2, an improved variant of iSTFTNet with a 1D-2D CNN that employs 1D and 2D CNNs to model temporal and spectrogram structures, respectively.
We designed a 2D CNN that performs frequency upsampling after conversion in a few-frequency space.
This design facilitates the modeling of high-dimensional spectrograms without compromising the speed.
The results demonstrated that iSTFTNet2 made iSTFTNet faster and more lightweight with comparable speech quality.
Audio samples are available at this https URL.

## 1·Introduction: 引言

Text-to-speech (TTS) synthesis and voice conversion (VC) have been extensively studied to obtain the desired speech.
The two-stage approach widely used in TTS and VC is as follows.
The first model predicts the intermediate representation (e.g., mel-spectrogram) from the input data (e.g., text or speech), whereas the second model synthesizes speech from the predicted intermediate representation.
This study focuses on the second model, the neural vocoder, and attempts to make it faster and more lightweight to broaden its applicability.

Various neural vocoders have been developed with advances in deep generative models.
The pioneer is an autoregressive model (e.g., WaveNet~\cite{AOordArXiv2016} and WaveRNN~\cite{NKalchbrennerICML2018}) that achieves high-fidelity speech synthesis but suffers from slow inference owing to sample-by-sample processing.
Various parallelizable non-autoregressive models have been developed to boost the inference speed.
For example, successful models include a distillation-based (e.g., Parallel WaveNet~\cite{AOordICML2018} and ClariNet~\cite{WPingICLR2019}), flow (e.g., Glow~\cite{DKingmaNeurIPS2018})-based (e.g., WaveGlow~\cite{RPrengerICASSP2019}), diffusion probabilistic model~\cite{YSongNeurIPS2019,JHoNeurIPS2020}-based (e.g., WaveGrad~\cite{NChenICLR2021} and DiffWave~\cite{ZKongICLR2021}), and generative adversarial network (GAN)~\cite{IGoodfellowNIPS2014}-based (e.g.,~\cite{KKumarNeurIPS2019,RYamamotoICASSP2020,JKongNeurIPS2020,JYangIS2020,GYangSLT2021,AMustafaICASSP2021,JKimIS2021,TOkamotoASRU2021,TKanekoICASSP2022,SHLeeICASSP2022,TKanekoIS2022,YKoizumiSLT2022,TKanekoICASSP2023}) models.
Among them, this study focuses on a GAN-based model while prioritizing the flexibility of the architectural design and the ability of fast inference.

Among the GAN-based neural vocoders, one of the fastest and most lightweight models is the inverse short-time Fourier transform network (iSTFTNet)~\cite{TKanekoICASSP2022}, which achieves fast, lightweight, and high-fidelity speech synthesis using a fast and lightweight 1D CNN (e.g., HiFi-GAN~\cite{JKongNeurIPS2020}) as the backbone and replacing some output-side neural processes with fast and lightweight inverse short-time Fourier transform (iSTFT).
Particularly, iSTFTNet applies iSTFT after sufficiently reducing the frequency dimension using large temporal upsampling (Figure~\ref{fig:concept}(a)) to avoid modeling high-dimensional spectrograms, which are difficult for a 1D CNN to represent.
This technique is essential for making the model faster and more lightweight while maintaining speech quality; however, it compromises the potential to improve the inference speed by applying iSTFT with fewer temporal upsampling.

One possible solution is to conduct spectrogram conversion using a fully 2D CNN (e.g.,~\cite{TKanekoIS2017,KOyamadaEUSIPCO2018,PNeekharaIS2019}).
However, the direct application of a 2D CNN requires a significant increase in the calculation cost because it increases linearly according to the frequency dimension (e.g., $80$ in a mel-spectrogram).
Alternatively, inspired by the success of combining 1D and 2D CNNs~\cite{TKanekoICASSP2019}, we propose \textit{iSTFTNet2}, an improved variant of iSTFTNet with a 1D-2D CNN, in which 1D and 2D CNNs are used to model the global temporal and local spectrogram structures, respectively (Figure~\ref{fig:concept}(b)).
Particularly, we designed a 2D CNN that conducts frequency upsampling after performing sufficient conversion in a few-frequency space using 1D and few-frequency 2D CNNs.
This design facilitates the modeling of high-dimensional spectrograms, which are difficult for a conventional 1D CNN-based iSTFTNet to model, without compromising the speed.
Furthermore, we propose an efficient module inspired by ShuffleNets~\cite{XZhangCVPR2018,NMaECCV2018} to improve the speed and model size further.

In the experiments, we examined the effectiveness of iSTFTNet2 on two representative datasets: \textit{LJSpeech}~\cite{ljspeech17} (single English speaker) and \textit{VCTK}~\cite{JYamagishiCSTR2016} (multiple English speakers).
The experimental results demonstrated that iSTFTNet2 made iSTFTNet faster and more lightweight with comparable speech quality.
Furthermore, we demonstrated the versatility of our ideas by applying iSTFTNet2 to multi-band modeling~\cite{CYuIS2020,GYangSLT2021}, another technique for improving the speed.
The results showed that this variant could further improve the speed with comparable speech quality.

The remainder of this paper is organized as follows.
Section~\ref{sec:istftnet} briefly reviews the conventional iSTFTNet.
Section~\ref{sec:istftnet2} presents details of the proposed iSTFTNet2.
Section~\ref{sec:experiments} presents our experimental results.
Finally, Section~\ref{sec:conclusions} concludes the study and discusses future research.

## 2·Related Works: 相关工作

iSTFTNet~\cite{TKanekoICASSP2022} is one of the fastest and most lightweight neural vocoders.
These characteristics were obtained by replacing some of the output-side layers of a fully neural vocoder with fast and lightweight iSTFT.
Particularly, it uses a fast and lightweight 1D CNN (e.g., HiFi-GAN~\cite{JKongNeurIPS2020}) as the backbone with a high processing speed.
However, it is challenging for a 1D CNN to model high-dimensional spectrograms because of the difficulty in capturing local structures in the frequency direction.
Hence, iSTFTNet reduces the frequency dimension using temporal upsampling as follows:

$$
  \text{iSTFT} (f_s, h_s, w_s)
  = \text{iSTFT} \left( \frac{f_1}{s}, \frac{h_1}{s}, \frac{w_1}{s} \right),
$$

where $f_s$, $h_s$, and $w_s$ indicate the FFT size, hop length, and window length, respectively, required for the iSTFT after $\times s$ temporal upsampling.
This equation is based on the time and frequency tradeoff, that is, $f_1 \cdot 1 = f_s \cdot s = \text{constant}$, and indicates that the frequency dimension can be reduced $s$ times by conducting $\times s$ temporal upsampling.

Figure~\ref{fig:architecture}(a) shows the overall architecture of iSTFTNet.
We present the architecture of iSTFTNet-\texttt{C8C8I4},\footnote{This is the same as iSTFTNet-\texttt{C8C8} described in~\cite{TKanekoICASSP2022}.
Here, we added \texttt{I4} to specify the temporal upsampling scale in the iSTFT.} which is the best balanced model that improves the speed and model size while maintaining speech quality, where \texttt{C}$x$ indicates the use of 1D CNN blocks with $\times x$ temporal upsampling, and \texttt{I}$y$ indicates the use of iSTFT with $\times y$ temporal upsampling.
When prioritizing the speed, a model that performs temporal upsampling fewer times, for example, iSTFTNet-\texttt{C8C1I32}, which conducts temporal upsampling once, is better.
However, it is shown that such a model deteriorates speech quality because of the difficulty of a 1D CNN in modeling high-dimensional spectrograms~\cite{TKanekoICASSP2022}.

## 3·Methodology: 方法

Considering the abovementioned facts, we attempted to construct an improved variant of iSTFTNet that can maintain speech quality even with fewer temporal upsampling.
A possible solution is to convert a spectrogram using a fully 2D CNN that can capture the local structures in spectrograms (e.g.,~\cite{TKanekoIS2017,KOyamadaEUSIPCO2018,PNeekharaIS2019}).
However, this replacement requires a significant increase in the calculation cost because it increases linearly in proportion to the frequency dimension (e.g., $80$ in a mel-spectrogram).

Alternatively, we developed \textit{iSTFTNet2}, which constitutes a 1D-2D CNN.
Figure~\ref{fig:architecture}(b) presents the overall architecture of iSTFTNet2.
As shown in this figure, to model temporal structures efficiently, iSTFTNet2 uses the same 1D CNN for the first three modules as that used in iSTFTNet, except that channel concatenation is used instead of addition when integrating the outputs of the multi-receptive fusion~\cite{JKongNeurIPS2020} in the 1D ResBlock to propagate more information to the subsequent 2D CNN.
Unlike iSTFTNet, iSTFTNet2 conducts 1D-to-2D conversion in an earlier step and applies a 2D CNN to effectively capture the local structures in the spectrograms, which are difficult for a 1D CNN to model.

When considering the detailed configuration of a 2D CNN, it is important to prevent an increase in the calculation cost driven by the introduction of a 2D CNN because its calculation cost increases linearly in proportion to the time and frequency dimensions.
To address this problem, iSTFTNet2 performs the main conversion in a few-frequency space (specifically, 2D blocks are applied in a space in which the frequency dimension is downsampled eight times, as shown in Figure~\ref{fig:architecture}(b)), and then conducts frequency upsampling in the last phase using transposed convolutions.

The design of the 2D blocks is a vital aspect to consider.
As shown in Figure~\ref{fig:blocks}, we developed two architectural designs.
The first is a \textit{2D ResBlock} (Figure~\ref{fig:blocks}(a)) that uses a residual connection~\cite{KHeCVPR2016} to propagate information efficiently.
We adjusted the model parameters (i.e., number of channels and kernel size) such that the model became faster and more lightweight than iSTFTNet-\texttt{C8C8I4} (the best-balanced model).
To further make the model faster and more lightweight, we introduced a second model, that is, a \textit{2D ShuffleBlock} (Figure~\ref{fig:blocks}(b)), which is inspired by efficient neural networks called ShuffleNets~\cite{XZhangCVPR2018,NMaECCV2018}.
In this block, the number of model parameters used in the 2D convolutional layers was adjusted such that it was half of that of the 2D ResBlock (Figure~\ref{fig:blocks}(a)).
Alternatively, in contrast to the residual connection, the half channels are propagated directly without any addition to preserve the model capacity.
A channel shuffle~\cite{XZhangCVPR2018,NMaECCV2018} is conducted to provide an interaction between the skip and non-skip branches.
Because the channel shuffle, channel split, and channel concat are weight-free operations, this block is faster and more lightweight than the 2D ResBlock.
We demonstrated the empirical performance difference between the two 2D blocks in the experiments presented in the next section.

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论

We proposed \textit{iSTFTNet2}, an improved variant of iSTFTNet that constitutes a 1D-2D CNN, in which 1D and 2D CNNs are used to model temporal and spectrogram structures, respectively.
The proposed architecture facilitated the application of iSTFT to higher-dimensional spectrograms without large temporal upsampling, and the experimental results demonstrated that iSTFTNet2 made iSTFTNet faster and more lightweight while maintaining speech quality.
Although we focused on a GAN-based neural vocoder, our ideas have high applicability, and applying them to other models, including other neural vocoders (e.g.,~\cite{RPrengerICASSP2019,NChenICLR2021,ZKongICLR2021}) and end-to-end text-to-speech synthesis (e.g.,~\cite{WPingICLR2019,YRenICLR2021,JDonahueICLR2021,JKimICML2021,NChenIS2021,XTanArXiv2022}), remains the subject of future research.
