# APNet2

<details>
<summary>基本信息</summary>

- 标题: "APNet2: High-quality and High-efficiency Neural Vocoder with Direct Prediction of Amplitude and Phase Spectra"
- 作者:
  - 01 Hui-Peng Du,
  - 02 Ye-Xin Lu,
  - 03 Yang Ai,
  - 04 Zhen-Hua Ling
- 链接:
  - [ArXiv](https://arxiv.org/abs/2311.11545)
  - [Publication](https://doi.org/10.1007/978-981-97-0601-3_6)
  - [Github](https://github.com/redmist328/APNet2)
  - [Demo](https://redmist328.github.io/APNet2_demo/)
- 文件:
  - [ArXiv](_PDF/2311.11545v1__APNet2__High-Quanlity_&_High-Efficiency_Neural_Vocoder_with_Direct_Prediction_of_Amplitude_&_Phase_Spectra.pdf)
  - [Publication](_PDF/2311.11545p0__APNet2__NCMMSC2023.pdf)

</details>

## Abstract: 摘要

<table><tr><td width="50%">

In our previous work, we proposed a neural vocoder called APNet, which directly predicts speech amplitude and phase spectra with a 5 ms frame shift in parallel from the input acoustic features, and then reconstructs the 16 kHz speech waveform using inverse short-time Fourier transform (ISTFT). APNet demonstrates the capability to generate synthesized speech of comparable quality to the HiFi-GAN vocoder but with a considerably improved inference speed.
However, the performance of the APNet vocoder is constrained by the waveform sampling rate and spectral frame shift, limiting its practicality for high-quality speech synthesis.
Therefore, this paper proposes an improved iteration of APNet, named APNet2. The proposed APNet2 vocoder adopts ConvNeXt v2 as the backbone network for amplitude and phase predictions, expecting to enhance the modeling capability.
Additionally, we introduce a multi-resolution discriminator (MRD) into the GAN-based losses and optimize the form of certain losses.
At a common configuration with a waveform sampling rate of 22.05 kHz and spectral frame shift of 256 points (i.e., approximately 11.6ms), our proposed APNet2 vocoder outperformed the original APNet and Vocos vocoders in terms of synthesized speech quality.
The synthesized speech quality of APNet2 is also comparable to that of HiFi-GAN and iSTFTNet, while offering a significantly faster inference speed.

</td><td>

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td><td>

</td></tr></table>
