# SnakeGAN

<details>
<summary>基本信息</summary>

- 标题: "SnakeGAN: A Universal Vocoder Leveraging DDSP Prior Knowledge and Periodic Inductive Bias"
- 作者:
  - 01 Sipan Li,
  - 02 Songxiang Liu,
  - 03 Luwen Zhang,
  - 04 Xiang Li,
  - 05 Yanyao Bian,
  - 06 Chao Weng,
  - 07 Zhiyong Wu,
  - 08 Helen Meng
- 链接:
  - [ArXiv](https://arxiv.org/abs/2309.07803)
  - [Publication](https://doi.org/10.1109/ICME55011.2023.00293)
  - [Github](https://github.com/thuhcsi/SnakeGAN)
  - [Demo](https://thuhcsi.github.io/SnakeGAN/)
- 文件:
  - [ArXiv](_PDF/2309.07803v1__SnakeGAN__A_Universal_Vocoder_Leveraging_DDSP_Prior_Knowledge_and_Periodic_Inductive_Bias.pdf)
  - [Publication](_PDF/2309.07803p0__SnakeGAN__ICME2023.pdf)

</details>

## Abstract: 摘要

<table><tr><td width="50%">

Generative adversarial network (GAN)-based neural vocoders have been widely used in audio synthesis tasks due to their high generation quality, efficient inference, and small computation footprint.
However, it is still challenging to train a universal vocoder which can generalize well to out-of-domain (OOD) scenarios, such as unseen speaking styles, non-speech vocalization, singing, and musical pieces.
In this work, we propose SnakeGAN, a GAN-based universal vocoder, which can synthesize high-fidelity audio in various OOD scenarios.
SnakeGAN takes a coarse-grained signal generated by a differentiable digital signal processing (DDSP) model as prior knowledge, aiming at recovering high-fidelity waveform from a Mel-spectrogram.
We introduce periodic nonlinearities through the Snake activation function and anti-aliased representation into the generator, which further brings the desired inductive bias for audio synthesis and significantly improves the extrapolation capacity for universal vocoding in unseen scenarios.
To validate the effectiveness of our proposed method, we train SnakeGAN with only speech data and evaluate its performance for various OOD distributions with both subjective and objective metrics.
Experimental results show that SnakeGAN significantly outperforms the compared approaches and can generate high-fidelity audio samples including unseen speakers with unseen styles, singing voices, instrumental pieces, and nonverbal vocalization.

</td><td>

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td><td>

</td></tr></table>
