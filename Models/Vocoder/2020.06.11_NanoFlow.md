# 标题

<details>
<summary>基本信息</summary>

- 标题: "NanoFlow: Scalable Normalizing Flows with Sublinear Parameter Complexity"
- 作者:
  - 01 Sang-gil Lee,
  - 02 Sungwon Kim,
  - 03 Sungroh Yoon
- 链接:
  - [ArXiv](https://arxiv.org/abs/2006.06280)
  - [Publication](https://dl.acm.org/doi/abs/10.5555/3495724.3496903)
  - [Github](https://github.com/L0SG/NanoFlow)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2006.06280v4__NanoFlow__Scalable_Normalizing_Flows_with_Sublinear_Parameter_Complexity.pdf)
  - [Publication](_PDF/2006.06280p0__NanoFlow__NeurIPS2020.pdf)

</details>

## Abstract: 摘要

Normalizing flows (NFs) have become a prominent method for deep generative models that allow for an analytic probability density estimation and efficient synthesis.
However, a flow-based network is considered to be inefficient in parameter complexity because of reduced expressiveness of bijective mapping, which renders the models unfeasibly expensive in terms of parameters.
We present an alternative parameterization scheme called NanoFlow, which uses a single neural density estimator to model multiple transformation stages.
Hence, we propose an efficient parameter decomposition method and the concept of flow indication embedding, which are key missing components that enable density estimation from a single neural network.
Experiments performed on audio and image models confirm that our method provides a new parameter-efficient solution for scalable NFs with significant sublinear parameter complexity.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论