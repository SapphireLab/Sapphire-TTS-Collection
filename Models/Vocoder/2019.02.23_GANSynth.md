# GANSynth

<details>
<summary>基本信息</summary>

- 标题: "GANSynth: Adversarial Neural Audio Synthesis"
- 作者:
  - 01 Jesse Engel (Google AI)
  - 02 Kumar Krishna Agrawal (Google AI)
  - 03 Shuo Chen (Google AI)
  - 04 Ishaan Gulrajani (Google AI)
  - 05 Chris Donahue (Google AI)
  - 06 Adam Roberts (Google AI)
- 链接:
  - [ArXiv](https://arxiv.org/abs/1902.08710)
  - [Publication](https://openreview.net/forum?id=H1xQVn09FX) ICLR2019Poster
  - [Github](https://github.com/magenta/magenta/tree/main/magenta/models/gansynth)
  - [Demo](http://goo.gl/magenta/gansynth-examples)
- 文件:
  - [ArXiv](_PDF/1902.08710v2__GANSynth__Adversarial_Neural_Audio_Synthesis.pdf)
  - [Publication](_PDF/1902.08710p0__GANSynth__ICLR2019Poster.pdf)

</details>

## Abstract: 摘要

Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence.
Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms.
Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain.
Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论