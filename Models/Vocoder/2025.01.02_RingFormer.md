# RingFormer

<details>
<summary>基本信息</summary>

- 标题: "RingFormer: A Neural Vocoder with Ring Attention and Convolution-Augmented Transformer"
- 作者:
  - 01 Seongho Hong
  - 02 Yong-Hoon Choi
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.01182)
  - [Publication]()
  - [Github](https://github.com/seongho608/RingFormer)
  - [Demo](https://seongho608.github.io/)
- 文件:
  - [ArXiv](_PDF/2501.01182v1__RingFormer__A_Neural_Vocoder_with_Ring_Attention_and_Convolution-Augmented_Transformer.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

While transformers demonstrate outstanding performance across various audio tasks, their application to neural vocoders remains challenging.
Neural vocoders require the generation of long audio signals at the sample level, which demands high temporal resolution.
This results in significant computational costs for attention map generation and limits their ability to efficiently process both global and local information.
Additionally, the sequential nature of sample generation in neural vocoders poses difficulties for real-time processing, making the direct adoption of transformers impractical.
To address these challenges, we propose ***RingFormer***, a neural vocoder that incorporates the ring attention mechanism into a lightweight transformer variant, the convolution-augmented transformer (Conformer).
Ring attention effectively captures local details while integrating global information, making it well-suited for processing long sequences and enabling real-time audio generation.
***RingFormer*** is trained using adversarial training with two discriminators.
The proposed model is applied to the decoder of the text-to-speech model VITS and compared with state-of-the-art vocoders such as HiFi-GAN, iSTFT-Net, and BigVGAN under identical conditions using various objective and subjective metrics.
Experimental results show that ***RingFormer*** achieves comparable or superior performance to existing models, particularly excelling in real-time audio generation.
Our code and audio samples are available on GitHub.

</details>
<br>

尽管 Transformers 在各种音频任务上都展现了卓越的性能, 但它们在神经声码器的应用上仍然存在挑战.
神经声码器需要在采样级别生成长音频信号, 需要高时域分辨率.
这导致对注意力映射生成的计算成本很高, 限制了它们有效处理全局和局部信息的能力.
此外, 神经声码器的采样生成的序列性质又使得实时处理变得困难, 使得直接采用 Transformers 实际上是不切实际的.

为了解决这些挑战, 我们提出 ***RingFormer***, 是在轻量 Transformer 变体 Conformer 中融入环注意力机制的神经声码器.
环注意力有效地捕捉局部细节的同时融合全局信息, 使得适合处理长序列, 并实现实时音频生成.
***RingFormer*** 使用具有两个判别器的对抗训练.

所提出的模型被应用到文本转语音模型 VITS 的解码器中, 并和其他先进声码器如 HiFi-GAN, iSTFTNet, BigVGAN 进行了相同条件下的各种客观和主观指标的比较.
实验结果表明 ***RingFormer*** 与现有模型相比有着相当或更好的性能, 尤其是在实时音频生成方面.
我们的代码和音频样本可在 [Github](https://github.com/seongho608/RingFormer) 上获得.

## 1·Introduction: 引言

<details>
<summary>展开原文</summary>

Audio generation models have become core technologies in various application fields such as speech synthesis, music generation, and sound effect creation.
Recent advancements have significantly enhanced generation quality and stability through generative adversarial network (GAN)-based models (e.g., [Parallel WaveGAN [1]](2019.10.25_Parallel_WaveGAN.md), [HiFi-GAN [2]](2020.10.12_HiFi-GAN.md), [BigVGAN [3]](2022.06.09_BigVGAN.md), [Avocodo [4]](2022.06.27_Avocodo.md)) and diffusion models (e.g., [Grad-TTS [5]](../Acoustic/2021.05.13_Grad-TTS.md), [WaveGrad [6]](2020.09.02_WaveGrad.md), [Diff-TTS [7]](../Acoustic/2021.04.03_Diff-TTS.md), [E3 TTS [8]](../Diffusion/2023.11.02_E3_TTS.md)), both aiming to achieve high-quality speech synthesis.

Text-to-speech (TTS) models, which map text input to speech output, have seen major improvements in recent years by leveraging advancements in generative models.
Among the components of a TTS system, vocoders play a pivotal role in determining the final audio quality.
They are responsible for converting intermediate audio representations, such as mel-spectrograms, into waveform audio.
A high-performing vocoder is essential for achieving natural and high-fidelity speech, as it directly impacts both the clarity and temporal consistency of the output audio.
Studies suggest that vocoders influence more than 50% of the overall system performance, underscoring their critical importance.

GAN-based vocoders ([Parallel WaveGAN [1]](2019.10.25_Parallel_WaveGAN.md), [HiFi-GAN [2]](2020.10.12_HiFi-GAN.md), [BigVGAN [3]](2022.06.09_BigVGAN.md), [Avocodo [4]](2022.06.27_Avocodo.md)) have emerged as a leading approach due to their ability to generate high-resolution speech in real-time.
This capability makes them suitable for tasks such as TTS and speech restoration.
However, GAN-based models face inherent challenges: while they produce sharp and detailed audio, they struggle with capturing long-term dependencies and complex patterns crucial for high-fidelity speech.
Furthermore, training GAN models can be unstable, leading to mode collapse or inconsistencies in the generated audio.
Despite these drawbacks, GAN-based vocoders remain a strong choice for real-time and high-resolution applications.

In contrast, diffusion models ([Grad-TTS [5]](../Acoustic/2021.05.13_Grad-TTS.md), [WaveGrad [6]](2020.09.02_WaveGrad.md), [Diff-TTS [7]](../Acoustic/2021.04.03_Diff-TTS.md), [E3 TTS [8]](../Diffusion/2023.11.02_E3_TTS.md)) have gained attention for their ability to enhance the stability and quality of the audio generation process.
By employing a step-by-step refinement process, diffusion models can produce consistent and natural-sounding speech, excelling in capturing complex and subtle audio details.
This makes them particularly well-suited for high-quality, non-real-time synthesis.
However, recent research has pointed out that these models may have limitations for time-sensitive applications due to slower generation speeds and higher computational demands.

In addition to GANs and diffusion models, flow-based models (e.g., [WaveGlow [9]](2018.10.31_WaveGlow.md), [Flow-TTS [10]](../Acoustic/2020.04.09_Flow-TTS.md), [P-Flow [11]](../Flow/P-Flow.md), [ReFlowTTS [12]](../Flow/2023.09.29_ReFlow-TTS.md)) and autoregressive models (e.g., [Tacotron [13]](../Acoustic/2017.03.29_Tacotron.md), [NaturalSpeech [14]](../E2E/2022.05.09_NaturalSpeech.md)) have contributed to advancements in efficiency and quality.
Autoregressive models excel at modeling the natural flow of speech but often sacrifice speed for quality.
Flow-based models strike a balance between speed and fidelity but are less widely used than GANs and diffusion models in speech synthesis.
Optimized architectures such as [iSTFT-Net [15]](2022.03.04_iSTFTNet.md) have further improved real-time processing efficiency, and multimodal audio generation models leveraging inputs such as text, images, and video have opened new possibilities for innovative applications.
Non-autoregressive approaches (e.g., [FastSpeech [16]](../Acoustic/2019.05.22_FastSpeech.md), [Parallel WaveGAN [1]](2019.10.25_Parallel_WaveGAN.md)) have also demonstrated significant strides in speed and quality, enabling real-time and interactive applications.

Despite these advancements, significant challenges persist.
GAN-based vocoders are effective for generating high-resolution audio but still struggle with capturing long-term dependencies, which can lead to quality degradation.
Diffusion models have improved stability but remain computationally expensive and unsuitable for real-time applications due to their sequential nature.

To address these challenges, we propose a novel GAN-based vocoder called ***RingFormer*** that incorporates convolution-augmented Transformers, known as [Conformer [17]](../ASR/2020.05.16_Conformer.md), and an efficient [ring attention [18]](../../Modules/Attention/RingAttention.md) mechanism introduced in previous research.
While GANs offer the speed and high resolution necessary for real-time synthesis, ***RingFormer*** leverages the Conformer architecture to better capture both local details and global dependencies, addressing key weaknesses of traditional GAN-based models.
Furthermore, ring attention enhances computational efficiency by focusing attention on localized regions while maintaining the ability to model long-range dependencies.
This hybrid architecture, ***RingFormer***, balances the tradeoffs between speed and resolution, achieving the temporal resolution and efficiency needed for real-time speech synthesis while maintaining the high-quality audio output expected from modern TTS systems.

The remainder of this paper is organized as follows: Section II reviews related work.
Section III describes the proposed model architecture, Section IV explains the loss functions, Section V presents experimental results and performance analysis, and Section VI concludes the paper.

</details>
<br>

音频生成模型已经变成各种应用领域如语音合成, 音乐生成的音效创作等的核心技术.
近期进展显著增强了生成质量和稳定性, 通过基于生成对抗网络 GAN 的模型 ([Parallel WaveGAN [1]](2019.10.25_Parallel_WaveGAN.md), [HiFi-GAN [2]](2020.10.12_HiFi-GAN.md), [BigVGAN [3]](2022.06.09_BigVGAN.md), [Avocodo [4]](2022.06.27_Avocodo.md)) 和扩散模型 ([Grad-TTS [5]](../Acoustic/2021.05.13_Grad-TTS.md), [WaveGrad [6]](2020.09.02_WaveGrad.md), [Diff-TTS [7]](../Acoustic/2021.04.03_Diff-TTS.md), [E3 TTS [8]](../Diffusion/2023.11.02_E3_TTS.md)) 实现, 旨在实现高质量语音合成.

文本转语音模型, 将输入文本映射到输出语音, 近年来通过利用生成式模型的最新进展取得了重大进展.
声码器作为 TTS 系统的组件之一, 在决定最终音频质量上扮演者至关重要的角色.
它们负责将中间音频表示, 如梅尔频谱图 (Mel-spectrograms), 转换为音频波形.
高性能的声码器对于实现自然且高保真语音至关重要, 因为它直接影响输出音频的清晰度和时序一致性.
研究表明声码器影响系统性能的比例高达 50%, 强调其重要性.

基于 GAN 的声码器 ([Parallel WaveGAN [1]](2019.10.25_Parallel_WaveGAN.md), [HiFi-GAN [2]](2020.10.12_HiFi-GAN.md), [BigVGAN [3]](2022.06.09_BigVGAN.md), [Avocodo [4]](2022.06.27_Avocodo.md)) 由于其能实时生成高分辨率语音的能力而成为主流方法.
这种能力使得它们适合用于 TTS 和语音修复等任务.
然而基于 GAN 的模型面临着固有挑战: 虽然它们产生清晰而详细的音频, 它们在捕捉长期依赖和复杂模式方面仍然存在困难.
此外, 训练 GAN 模型可能不稳定, 导致模式崩溃或生成音频不一致.
尽管存在这些缺陷, 基于 GAN 的声码器仍然是实时和高分辨率应用的强大选择.

扩散模型 ([Grad-TTS [5]](../Acoustic/2021.05.13_Grad-TTS.md), [WaveGrad [6]](2020.09.02_WaveGrad.md), [Diff-TTS [7]](../Acoustic/2021.04.03_Diff-TTS.md), [E3 TTS [8]](../Diffusion/2023.11.02_E3_TTS.md)) 则由于其能提升音频生成过程的稳定性和质量而获得了关注.
通过采用逐步细化过程, 扩散模型可以生成一致且听感自然的语音, 优于捕捉复杂和微妙的音频细节.
这使得它们特别适用于高质量, 非实时合成.
然而, 近期研究指出这些模型可能在时间敏感应用方面有所限制, 因为生成速度较慢且计算需求较高.

- 除了 GAN 和扩散模型, 基于流的模型 ([WaveGlow [9]](2018.10.31_WaveGlow.md), [Flow-TTS [10]](../Acoustic/2020.04.09_Flow-TTS.md), [P-Flow [11]](../Flow/P-Flow.md), [ReFlowTTS [12]](../Flow/2023.09.29_ReFlow-TTS.md)) 和自回归模型 ([Tacotron [13]](../Acoustic/2017.03.29_Tacotron.md), [NaturalSpeech [14]](../E2E/2022.05.09_NaturalSpeech.md)) 也为提升效率和质量做出了贡献.
自回归模型在建模语音自然流动方面表现优越但往往牺牲了速度以换取质量.
基于流的模型在速度和保真度之间取得了平衡, 但在语音合成领域中不如 GAN 和扩散模型广泛使用.
- 优化的架构如 [iSTFT-Net [15]](2022.03.04_iSTFTNet.md) 进一步提升实时处理效率
- 多模态音频生成模型利用文本, 图像, 视频等输入, 开辟了新的创新应用的可能性.
- 非自回归方法 (如 [FastSpeech [16]](../Acoustic/2019.05.22_FastSpeech.md), [Parallel WaveGAN [1]](2019.10.25_Parallel_WaveGAN.md)) 也在提升速度和质量方面取得了显著进展, 使得实时和交互式应用成为可能.

虽然有了这些进展, 仍然存在着重要挑战.
- 基于 GAN 的声码器在生成高分辨率音频上有效但仍难以捕捉长期依赖, 导致质量下降.
- 扩散模型提升了稳定性但计算代价高昂且不适用于实时应用, 因为其内在的序列性.

为了解决这些问题, 我们提出了一种新的基于 GAN 的声码器 ***RingFormer*** 融合了卷积增强的 Transformer ([Conformer [17]](../ASR/2020.05.16_Conformer.md)), 和一种高效的环注意力机制 ([Ring Attention [18]](../../Modules/Attention/RingAttention.md)).
- GAN 提供实时合成所需的速度和高分辨率, 而 ***RingFormer*** 则利用 Conformer 架构更好地捕捉局部细节和全局依赖, 克服了传统基于 GAN 模型的关键弱点.
- 此外, 环注意力增强计算效率, 通过将注意力集中在局部区域的同时保持对全局依赖的建模能力.

采用这种混合架构的 ***RingFormer*** 对速度和分辨率进行了权衡, 达到了实时语音合成所需的时序分辨率和效率的同时, 保持了现代 TTS 系统期望的高质量音频输出.

## 2·Related Works: 相关工作

<details>
<summary>展开原文</summary>

GANs have emerged as powerful models in the domain of audio synthesis, particularly for generating high-quality raw audio waveforms.
[WaveGAN [19]](2018.02.12_WaveGAN.md), introduced by Donahue et al., was the first GAN-based approach designed to directly generate raw audio waveforms by adapting the [DCGAN [20]](../CV/2015.11.19_DCGAN.md) architecture for one-dimensional audio data.
Although WaveGAN demonstrated the feasibility of unsupervised learning for audio generation, it faced limitations in capturing fine-grained details.
Building on this foundation, [MelGAN [21]](2019.10.08_MelGAN.md) introduced a multiscale discriminator that leveraged average pooling to downsample audio at multiple scales.
By incorporating window-based discriminators to model audio features across different resolutions, MelGAN achieved efficient and high-quality audio synthesis with improved fidelity.

[HiFi-GAN [2]](2020.10.12_HiFi-GAN.md), proposed by Kong et al., advanced the field by adopting a multi-period discriminator capable of capturing periodic structures in time-domain audio.
The model combined short-time Fourier transform (STFT) loss and mel-spectrogram loss, enabling it to generate high-resolution, natural-sounding audio suitable for speech synthesis and restoration tasks.
[GAN-TTS [22]](2019.09.25_GAN-TTS.md) further refined the use of GANs in audio synthesis by utilizing a conditional feed-forward generator alongside an ensemble of discriminators that operated on random windows of varying sizes.
This approach enabled GAN-TTS to achieve high-quality audio synthesis while maintaining both local coherence and global consistency.

[Parallel WaveGAN [1]](2019.10.25_Parallel_WaveGAN.md), introduced by Yamamoto et al., incorporated a combination of multi-resolution STFT loss and adversarial loss in the waveform domain.
This innovation allowed for parallel waveform generation, eliminating the need for complex probability density distillation techniques and significantly enhancing both generation speed and quality.
Similarly, [iSTFTNet [15]](2022.03.04_iSTFTNet.md) simplified the output layers of traditional CNN-based vocoders by replacing them with inverse STFT layers.
This design reduced model complexity and computational costs while maintaining audio quality.

[BigVGAN [3]](2022.06.09_BigVGAN.md), developed by Lee et al., pushed the boundaries of GAN-based audio synthesis by incorporating periodic activation functions to stabilize training and anti-aliasing techniques to reduce artifacts.
These features enhanced fidelity and robustness in the generated audio, making BigVGAN a notable advancement in high-resolution audio synthesis.

While these GAN-based models have driven significant advancements in audio generation, they often struggle to capture long-term dependencies due to their reliance on iterative upsampling processes to expand receptive fields.
This limitation can result in inconsistencies when modeling extended temporal relationships in audio data.
To address these challenges, we propose a novel generator architecture, ***RingFormer***, which integrates self-attention mechanisms with convolutional layers.
This hybrid approach enables the model to effectively capture long-term dependencies while maintaining computational efficiency.
Additionally, the incorporation of ring attention reduces computational overhead by focusing on fixed local regions, preserving both local and global relationships.
Enhanced loss functions are also introduced to enable more accurate and efficient audio synthesis.

</details>
<br>

GAN 已经称为音频合成领域的强力模型, 尤其是生成高质量原始音频波形.
- [WaveGAN [19]](2018.02.12_WaveGAN.md) 由 Donahue et al. 提出, 是第一个基于 GAN 的方法, 采用 [DCGAN [20]](../CV/2015.11.19_DCGAN.md) 架构用于一维的音频数据直接生成原始音频波形.
  尽管 WaveGAN 展示了无监督学习在音频生成中的可行性, 但它在捕获细粒度细节方面有所局限.
- [MelGAN [21]](2019.10.08_MelGAN.md) 基于此发现提出了一种多尺度判别器, 利用平均池化在多个尺度上对音频进行下采样.
  通过结合基于窗口的判别器来跨多个分辨率上对音频特征建模, MelGAN 实现了高效和高质量的音频合成, 并提高了音质.
- [HiFi-GAN [2]](2020.10.12_HiFi-GAN.md) 由 Kong et al. 提出, 采用多周期判别器捕捉时域音频中的周期结构, 进一步推动了音频合成领域的发展.
  该模型结合了短时傅里叶变换损失和梅尔频谱损失, 从而能够生成高分辨率, 听感自然的音频, 适用于语音合成和修复任务.
- [GAN-TTS [22]](2019.09.25_GAN-TTS.md) 通过使用条件化前馈生成器和一组在大小不一的随机窗口上运作的判别器, 进一步细化了 GAN 在音频合成中的应用.
  这种方法使得 GAN-TTS 能够实现高质量的音频合成, 同时保持局部连贯性和全局一致性.
- [Parallel WaveGAN [1]](2019.10.25_Parallel_WaveGAN.md) 由 Yamamoto et al. 提出, 在波形域中采用多分辨率 STFT 损失和对抗损失的组合.
  这一创新实现了并行波形生成, 消除了复杂概率密度蒸馏技术的需要, 并显著增强了生成速度和质量.
- [iSTFTNet [15]](2022.03.04_iSTFTNet.md) 类似地简化了传统基于 CNN 的声码器的输出层, 通过将它们替换为逆 STFT 层.
  这一设计减少了模型复杂度和计算成本, 同时保持音频质量.
- [BigVGAN [3]](2022.06.09_BigVGAN.md) 由 Lee et al. 提出, 采用周期激活函数来稳定训练, 和抗锯齿技术来减少伪影, 从而推动了基于 GAN 的音频生成的边界.
  这些特性增强了生成音频的真实度和鲁棒性, 使 BigVGAN 成为高分辨率音频合成领域的显著进步.

尽管这些基于 GAN 的模型已经在语音生成领域取得了重大进展, 但它们由于依赖于迭代式上采样过程以扩展感受野而难以捕获长期依赖关系.
这一局限性会导致模型在建模音频数据中的扩展时序关系时出现不一致性.

为了解决这些挑战, 我们提出了新式生成器架构 ***RingFormer***, 它结合了自注意力机制和卷积层.
这一混合方法使得模型能够有效地捕获长期依赖关系, 同时保持计算效率.
此外, 引入环注意力可以减少计算开销, 集中于固定局部区域, 同时保持局部和全局关系.
还引入了增强的损失函数, 使得模型能够更精确和高效地生成音频.

## 3·Methodology: 方法

### Architecture: 架构

The overall architecture of the proposed model consists of one generator and two discriminators, as shown in Figure 1.
The generator maps the spectrogram $z$ to an audio waveform $G_{\phi}(z)$, while the two discriminators $D_{\phi}$ and $D_{\psi}$ compare the real audio waveform $x$ and the generated waveform $G_{\phi}(z)$ in different ways.

#### Generator: 生成器

Recognizing that capturing long-term dependencies is crucial for modeling realistic speech audio, we propose a new generator architecture designed to learn these dependencies more effectively.
The proposed architecture, as shown in Figure 2, incorporates two stages of Conformer blocks with ring attention and ×4 upsampling between the input and output convolutions.
This approach contrasts with the upsampling process in HiFiGAN, which uses the multi-receptive field fusion (MRF) technique with [×8, ×8, ×2, ×2] transpose convolutions to perform upsampling and reconstruct raw audio.
In comparison, our proposed structure simplifies the upsampling process by using two stages of Conformer blocks with ring attention and ×4 upsampling, providing a more efficient and streamlined approach to generating high-quality audio.
The remaining Conformer blocks, excluding the ring attention, are identical to those in [Ring Attention [18]](../../Modules/Attention/RingAttention.md).
This modification improves the ability to capture long-term dependencies in the generated audio, enhancing the model's overall performance and synthesis quality.
In the upsampling block, the [snake activation function [23]](../../Modules/Activation/Snake.md) helps the model learn the periodic structure of speech signals more accurately.
Although the final output of the generator is the magnitude and phase of the spectrogram rather than the waveform, these components also exhibit periodic characteristics, making them suitable for modeling periodic structures.
Unlike [BigVGAN [3]](2022.06.09_BigVGAN.md), no anti-aliasing filter is used for upsampling, as smaller upsampling ratios allow for more stable high-frequency processing.
After upsampling, the inverse STFT reconstructs the signal in the frequency domain, separating amplitude and phase for better control.
This structure maintains memory efficiency for long sequences while improving the learning of long-term dependencies in speech signals.
Through these improvements, ***RingFormer*** achieves more precise speech synthesis without sacrificing speed.

#### Ring Attention: 环注意力

Capturing long-term dependencies is crucial for modeling realistic speech audio.
For instance, the duration of a phoneme can exceed 100ms, resulting in a high correlation between more than 2,200 adjacent samples in the raw waveform.
[Ring attention [18]](../../Modules/Attention/RingAttention.md) is a mechanism designed to efficiently process long sequences by leveraging block-wise parallel computation.
In ***RingFormer***, ring attention is tailored for vocoders to effectively handle long sequences of speech signals.

First, the mel-spectrogram upsampled from the MRF is divided into $N_d$ fixed-size blocks, and each block is assigned to an individual device.
Here, device refers to an individual computational unit in a parallel processing system, while block represents a segment of a long sequence divided into a fixed length.
Each device generates query, key, and value based on the divided mel-spectrogram, which are obtained through affine transformations using learnable weight matrices $W_Q$, $W_K$, and $W_V$.

Subsequently, a key-value exchange mechanism based on a ring topology allows each device to receive key and value data from its adjacent device.
This data exchange enables information to flow between blocks, thereby effectively integrating global dependencies and context across the entire sequence.
This structure is well-suited for modeling both the temporal dependencies of speech signals and the harmonic structure within frequency bands, allowing it to capture the periodic characteristics of speech in detail.
In particular, ring attention effectively resolves the memory bottleneck issue encountered when processing long sequences in vocoders.
Since key-value exchanges and attention computations are designed to be performed in parallel across devices, computational efficiency is maximized, significantly reducing memory and computational costs during the training process for long sequences of data.
Block-wise attention computations within the device are carried out as follows:

where $i$ denotes the device index, and $d_k$ represents the dimension of the key vector.
The query $Q_i$ performs a scaled dot product computation with the keys $K=\{K_i,\cdots,K_{i+d-1}\}$ in the same device, which is then multiplied with $V=\{V_i,\cdots,V_{i+d-1}\}$ to calculate the attention values.
This method overcomes the memory constraints of traditional [Transformer [24]](../_Transformer/2017.06.12_Transformer.md) models, allowing the context size to scale linearly with the number of devices.
As a result, ring attention maintains computational efficiency while achieving high performance in both training and inference for extremely large context sizes.

#### Discriminators: 判别器

We use two discriminators for generator training: the multi-period discriminator (MPD) and the multi-scale sub-band constant-Q transform (MS-SB-CQT) discriminator.

Since speech audio consists of sinusoidal signals with various periods, it is necessary to identify the diverse periodic patterns inherent in the audio data.
To this end, [HiFi-GAN [2]](2020.10.12_HiFi-GAN.md) proposed the MPD, and in this paper, we use the same MPD without modification.

Additionally, the [MS-SB-CQT discriminator [25]](2023.11.25_MS-SB-CQT.md) improves upon the multi-scale discriminator (MSD) of [MelGAN [21]](2019.10.08_MelGAN.md) by using constant-Q transform (CQT) to process more precise frequency band information.
This approach enhances both frequency and time resolution, capturing more detailed characteristics of the speech signal and enabling more natural speech synthesis results.
While the original MSD focused on capturing information across multiple frequency ranges, CQT allows for more detailed frequency band analysis, providing finer frequency interpretation.
In this paper, the MS-SB-CQT discriminator is used without modification.

By using these two discriminators, the diverse periodic patterns inherent in the audio can be distinguished, and detailed characteristics by frequency can be captured.

### Training Objective: 训练目标

We use various loss functions to optimize ***RingFormer***.
To evaluate speech quality, it is integrated into the widely used TTS model [VITS [26]](../E2E/2021.06.11_VITS.md) and trained by connecting it to the model.
As a result, the encoder parameters of VITS are also updated.

#### Adversarial Loss: 对抗损失

The ***RingFormer*** is trained using two discriminators.
The first is the MPD, originally proposed in [HiFi-GAN [2]](2020.10.12_HiFi-GAN.md), and the second is the [MS-SB-CQT discriminator [25]](2023.11.25_MS-SB-CQT.md).
The MPD is designed as a combination of sub-discriminators based on Markovian windows, with each sub-discriminator specializing in detecting different periodic patterns in the input waveform.
This structure allows for a systematic evaluation of speech data with diverse periodic characteristics.
However, a limitation of the MPD is that its sub-discriminators evaluate only isolated samples, potentially overlooking broader contextual information.
To address this limitation, the [MS-SB-CQT discriminator [25]](2023.11.25_MS-SB-CQT.md), is incorporated to enhance performance.
The adversarial loss is defined as follows:

$$
\begin{aligned}
\mathcal{L}_{adv} &= \mathcal{L}_G + \mathcal{L}_D \\
\mathcal{L}_G &= \alpha \mathbb{E}_z [(1-D_\theta(G_{\phi}(z)))^2] + (1-\alpha) \mathbb{E}_x [(1-D_{\psi}(x))^2] \\
\mathcal{L}_D &= \alpha \mathbb{E}_{x,z} [(1-D_\theta(x))^2 + (D_\theta(G_{\phi}(z)))^2] + (1-\alpha) \mathbb{E}_{x,z} [(1- D_{\psi}(x))^2 + (D_{\psi}(G_{\phi}(z)))^2] \\
\end{aligned}
$$

The contribution of each discriminator to the training loss is controlled by a weighting factor, $\alpha$, which is set to 0.5 to balance their roles during adversarial training.

#### Spectral Decomposition Loss: 频谱分解损失

In our work, we explicitly learn magnitude loss and phase loss, building on the findings of [The Importance of Phase in Speech Enhancement [27]](../_Full/The_Importance_of_Phase_in_Speech_Enhancement.md).
This approach ensures the accurate reproduction of spectral energy (magnitude) and precise temporal alignment (phase), reducing distortions and enhancing perceptual quality.
By separately optimizing magnitude and phase, we achieve a balanced trade-off between the time and frequency domains, resulting in better generalization across diverse audio data and more natural sound reconstruction.
The spectral decomposition loss is defined as follows:

$$
\begin{aligned}
\mathcal{L}_{sd} = \mathcal{L}_{mag} + \mathcal{L}_{arg} \\
\mathcal{L}_{mag} &= \mathbb{E}_{x,z} [\| |F(x)| - |F(G_{\phi}(z))| \|_1] \\
\mathcal{L}_{arg} &= \mathbb{E}_{x,z} [\| \angle F(x) - \angle F(G_{\phi}(z)) \|_1] \\
\end{aligned}
$$

Here, $F(\cdot)$ denotes the short-time Fourier transform of the input signal.
This loss compares the amplitude and phase of the audio signal generated by ***RingFormer*** with the amplitude and phase of the ground truth.

#### Feature Matching Loss: 特征匹配损失

The feature matching loss $\mathcal{L}_{fm}$ ([MelGAN [21]](2019.10.08_MelGAN.md)) minimizes the $l_1$ distance between the intermediate features extracted from the discriminator layers:

$$
\mathcal{L}_{fm} = \mathbb{E}_{x,z} [\sum_{i=1}^{T} \dfrac{1}{N_i} \| D_k^i(x) - D_k^i(G_{\phi}(z)) \|_1]
$$

where $T$ is the number of layers in the sub-discriminator $D_k$, and $N_i$ is the number of features in the 𝑖-th layer.
The feature matching loss encourages the generator to produce outputs whose intermediate features are similar to those of the real data, improving the generator's ability to match the discriminator's learned feature representations.

#### Final Loss: 最终损失

The proposed ***RingFormer*** is implemented to replace the decoder of the widely used end-to-end TTS model, VITS.
While the training environment is integrated with VITS, ***RingFormer*** is not dependent on it.
Unlike models such as [FastSpeech [16]](../Acoustic/2019.05.22_FastSpeech.md), VITS eliminates the need for a separate duration predictor or aligner (e.g., attention alignment in [Tacotron [13]](../Acoustic/2017.03.29_Tacotron.md)).
Additionally, VITS combines a GAN with a [variational autoencoder (VAE) [28]](../_Basis/VAE.md) to generate high-resolution and natural-sounding speech.

In this paper, the proposed ***RingFormer*** is optimized using two additional loss functions adopted from VITS.
The first is $\mathcal{L}_{dur}$, which facilitates learning text-to-speech alignment, and the second is $\mathcal{L}_{KL}$, which plays a critical role in modeling the relationship between text and speech in the latent space.
$\mathcal{L}_{KL}$ regulates the distribution of latent variables, enabling natural speech synthesis and supporting the alignment-free structure.
These two loss functions are applied without modification during the training of the proposed model.
The total loss function is defined as follows:

$$
\mathcal{L} = \mathcal{L}_{adv} + \lambda_{sd} \mathcal{L}_{sd} + \lambda_{fm} \mathcal{L}_{fm} + \lambda_{recon} \mathcal{L}_{recon} + \lambda_{KL} \mathcal{L}_{KL} + \lambda_{dur} \mathcal{L}_{dur}
$$

The hyperparameters $\lambda_{sd}$, $\lambda_{fm}$, $\lambda_{recon}$, $\lambda_{KL}$, $\lambda_{dur}$ are all set to 1 in this study.
This decision was made because the magnitudes of the individual observed loss values were similar.
By setting these hyperparameters to 1, we ensure that each loss component contributes equally to the total loss without introducing arbitrary scaling factors, thereby facilitating a balanced optimization process.

## 4·Experiments: 实验

To validate the performance of ***RingFormer***, it is applied to the decoder of the widely used TTS model, [VITS [26]](../E2E/2021.06.11_VITS.md), and the quality of the synthesized speech is evaluated.
For comparison, the baseline vocoders used are [HiFi-GAN [2]](2020.10.12_HiFi-GAN.md), [iSTFTNet [15]](2022.03.04_iSTFTNet.md), and [BigVGAN [3]](2022.06.09_BigVGAN.md), which are state-of-the-art models known for achieving top performance in the field.
These models are also applied to VITS with the same architecture and hyperparameters to ensure a fair comparison under equal conditions.
Our code and audio samples are available on [GitHub [34]](https://github.com/seongho608/RingFormer).

### Datasets: 数据集

In this study, we trained and evaluated the model using the widely used [LJSpeech dataset [29]](../../Datasets/2017.07.05_LJSpeech.md).
The LJSpeech dataset consists of 13,100 high-quality speech samples, totaling approximately 24 hours of speech data.
Each sample is recorded at a sampling rate of 22,050 Hz and is commonly used in TTS research with English text.
In this study, 500 samples from the 13,100 LJSpeech samples were allocated to the validation set to assess the model’s performance, while 12,500 samples were used for training.
To further evaluate the model's performance, 30 samples were randomly selected from the remaining 100 samples for final testing.
This data configuration ensured consistent and reliable training and evaluation.

### Experimental Setup: 实验设置

The proposed generator in this study employs two stages of upsampling.
Initially, the number of channels is set to 512, and at each stage, the number of channels is halved according to $2^i$, where $i$ denotes the upsampling step.
The Conformer block is configured by adjusting the input dimensions at each upsampling stage.
It utilizes 8 attention heads, a feed-forward network dimension that is half of the input dimension, 2 Conformer layers, a kernel size of 31, and a dropout rate of 0.1.
Key hyperparameters are summarized in Table 1.
The model training is conducted on an Ubuntu 20.04 LTS operating system, with Docker used for managing software dependencies.
The training process is carried out using a single Nvidia A100 GPU with 80GB of memory.

### Evaluation Metrics: 评价指标

In this study, the model's performance is evaluated from multiple perspectives using [mel-cepstral distortion (MCD) [30]](../../Evaluations/MCD.md), word error rate (WER), [short-time objective intelligibility (STOI) [31]](../../Evaluations/STOI.md), [NISQA [32]](../../Evaluations/2021.04.19_NISQA.md), mean opinion score (MOS), and comparison MOS (CMOS).

MCD measures the difference in mel-frequency cepstral coefficients between the synthesized and reference speech.
A lower MCD value indicates higher similarity between the two voices.
WER evaluates the accuracy of speech recognition by measuring the alignment of the recognized text with the original transcript.
A lower WER indicates fewer recognition errors.
STOI quantifies the intelligibility of the synthesized speech in relation to the reference, with values ranging from 0 to 1.
A value closer to 1 indicates higher intelligibility.
NISQA is a deep learning-based metric for assessing the quality and naturalness of speech by mimicking human auditory perception and quantifying the subjective quality of speech.

MOS is a subjective evaluation metric, where listeners rate the speech quality on a scale from 1 to 5.
However, in this study, we utilize the MOS prediction system from [UTMOS [33]](../../Evaluations/2022.04.05_UTMOS.md) for objective evaluation, which produces scores highly correlated with human ratings.
Finally, CMOS is used to compare the relative quality between two speech samples, allowing listeners to select the better-quality sample, thus performing a subjective comparison.
These diverse metrics enable a comprehensive evaluation of the model’s speech quality, intelligibility, and pronunciation accuracy.

## 5·Results: 结果

We report the performance of ***RingFormer*** and the baseline models evaluated on LJSpeech using the above objective and subjective metrics.
Table 2 presents the performance evaluation results of ***RingFormer*** and baseline vocoder models.

The proposed model demonstrates overall stable performance, achieving particularly strong results in MOS, which evaluates the naturalness and quality of speech, surpassing other models.
It also maintains consistent quality in the NISQA metric, confirming its ability to deliver reliable performance without introducing distortions to the speech signal.
While the proposed model slightly falls behind BigVGAN in MCD and STOI metrics, it achieves comparable performance, demonstrating competitiveness in terms of speech similarity and intelligibility.
In WER, which measures pronunciation accuracy, the proposed model performs on par with other models, confirming that the synthesized speech is clearly recognizable.
Additionally, in the subjective CMOS evaluation, the proposed model shows marginally better performance compared to BigVGAN and performs favorably against HiFi-GAN.
This suggests that the proposed model can provide high-quality audio in practical applications.
These results highlight that the proposed model generates more natural speech quality and clearer pronunciation compared to existing vocoders.

Table 3 presents the evaluation results for the number of parameters and inference speed of ***RingFormer*** and the comparison models.
The proposed model achieves high inference speed while maintaining a reasonable balance in terms of model size.
***RingFormer*** has 3.8 times fewer parameters than [BigVGAN [3]](2022.06.09_BigVGAN.md) and is approximately twice as fast in inference.
Considering the similar performance of both models, ***RingFormer*** can be regarded as sufficiently competitive.
On the other hand, ***RingFormer*** outperforms [HiFi-GAN [2]](2020.10.12_HiFi-GAN.md) and [iSTFTNet [15]](2022.03.04_iSTFTNet.md) in terms of performance but has 2.1 to 2.2 times more parameters with similar inference speed.
Furthermore, when ring attention is removed from the proposed model, the inference speed drops by up to 1.5 times, highlighting the significant role of ring attention in enabling fast speech generation.
These results demonstrate that our model generates high-quality speech while supporting real-time processing, further enhancing its practicality for various applications.

Table 4 presents the results of evaluating the impact of each component of ***RingFormer*** on the quality of synthesized speech.
The MOS of the complete ***RingFormer*** model, which includes all components, is 4.11.
When the magnitude loss ($\mathcal{L}_{mag}$) and phase loss ($\mathcal{L}_{arg}$) are removed, the MOS decreases to 4.05, suggesting that these loss terms contribute to capturing the detailed periodic and amplitude information of speech.
Removing the MS-SB-CQT discriminator also results in a decline in quality, indicating that continuous sequence discrimination positively contributes to synthesis quality.
When both components are removed, the MOS reaches its lowest value of 4.03, demonstrating that the combination of magnitude loss, phase loss, and the MS-SB-CQT discriminator is important for maximizing speech synthesis quality.

Additionally, to analyze ***RingFormer***'s performance in modeling long-term dependencies more precisely, experiments were conducted by reducing the size of the latent variable $z$ to 1/2 and 1/4, thereby reducing the receptive field.
The results showed a gradual decline in evaluation metrics as the receptive field decreased, confirming that a sufficient receptive field in ***RingFormer*** is crucial for learning long-term dependencies in speech and generating high-quality audio.

Table 5 evaluates the ability of the ***RingFormer*** model to capture long-term dependencies by analyzing the autocorrelation of the F0 contour.
The results show that the ***RingFormer*** model achieves the highest Pearson correlation coefficient with the ground truth, highlighting its strong performance in learning long-term dependencies.

Figure 4 visualizes this capability through an attention map, where bright patterns are maintained even in regions far from the diagonal, reflecting long-term dependencies.
Additionally, Figure 5 illustrates the attention scores for an arbitrary query.
As shown in the figure, the ***RingFormer*** model assigns notable attention scores even to temporally distant query-key pairs, indicating that it effectively utilizes long-term temporal information during audio generation.
The attention values are generally evenly distributed, supporting the role of the Conformer block in maintaining a stable and natural temporal structure in the synthesized audio.

## 6·Conclusions: 结论

In this paper, we propose ***RingFormer***, a vocoder that efficiently processes long sequences with long-term dependencies through a Conformer block with Ring Attention, while maintaining a reasonable memory usage to synthesize high-quality speech.
This structure captures both local and global dependencies in speech signals, enabling the generation of more natural-sounding speech.
Additionally, to improve generation speed, the output layer incorporates an inverse STFT structure, and by adding phase and magnitude losses to the loss function, it finely learns temporal patterns and amplitude information, thereby enhancing the quality of the synthesized speech.
For adversarial training, we introduce the recently released MS-SB-CQT discriminator, which improves the precision of speech synthesis by more accurately evaluating continuous sequences.
Through various objective metrics such as MCD, WER, STOI, and NISQA, as well as MOS and CMOS evaluations, we verify that ***RingFormer*** performs on par with or better than existing models, successfully achieving natural speech and clarity.
This study presents a model that balances fast speech synthesis speed and high quality, contributing to the advancement of speech synthesis technology.
Future research will aim to expand the applicability of ***RingFormer*** by optimizing it for multilingual datasets and various application environments.
