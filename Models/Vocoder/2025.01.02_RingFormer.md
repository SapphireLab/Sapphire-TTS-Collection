# RingFormer

<details>
<summary>基本信息</summary>

- 标题: "RingFormer: A Neural Vocoder with Ring Attention and Convolution-Augmented Transformer"
- 作者:
  - 01 Seongho Hong
  - 02 Yong-Hoon Choi
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.01182)
  - [Publication]()
  - [Github](https://github.com/seongho608/RingFormer)
  - [Demo](https://seongho608.github.io/)
- 文件:
  - [ArXiv](_PDF/2501.01182v1__RingFormer__A_Neural_Vocoder_with_Ring_Attention_and_Convolution-Augmented_Transformer.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

While transformers demonstrate outstanding performance across various audio tasks, their application to neural vocoders remains challenging.
Neural vocoders require the generation of long audio signals at the sample level, which demands high temporal resolution.
This results in significant computational costs for attention map generation and limits their ability to efficiently process both global and local information.
Additionally, the sequential nature of sample generation in neural vocoders poses difficulties for real-time processing, making the direct adoption of transformers impractical.
To address these challenges, we propose RingFormer, a neural vocoder that incorporates the ring attention mechanism into a lightweight transformer variant, the convolution-augmented transformer (Conformer).
Ring attention effectively captures local details while integrating global information, making it well-suited for processing long sequences and enabling real-time audio generation.
RingFormer is trained using adversarial training with two discriminators.
The proposed model is applied to the decoder of the text-to-speech model VITS and compared with state-of-the-art vocoders such as HiFi-GAN, iSTFT-Net, and BigVGAN under identical conditions using various objective and subjective metrics.
Experimental results show that RingFormer achieves comparable or superior performance to existing models, particularly excelling in real-time audio generation.
Our code and audio samples are available on GitHub.

## 1·Introduction: 引言

Audio generation models have become core technologies in various application fields such as speech synthesis, music generation, and sound effect creation.
Recent advancements have significantly enhanced generation quality and stability through generative adversarial network (GAN)-based models (e.g., Parallel WaveGAN [1], HiFi-GAN [2], BigVGAN [3], Avocodo [4]) and diffusion models (e.g., Grad-TTS [5], WaveGrad [6], Diff-TTS [7], E3 TTS [8]), both aiming to achieve high-quality speech synthesis.

Text-to-speech (TTS) models, which map text input to speech output, have seen major improvements in recent years by leveraging advancements in generative models.
Among the components of a TTS system, vocoders play a pivotal role in determining the final audio quality.
They are responsible for converting intermediate audio representations, such as mel-spectrograms, into waveform audio.
A high-performing vocoder is essential for achieving natural and high-fidelity speech, as it directly impacts both the clarity and temporal consistency of the output audio.
Studies suggest that vocoders influence more than 50% of the overall system performance, underscoring their critical importance.

GAN-based vocoders [1], [2], [3], [4] have emerged as a leading approach due to their ability to generate high-resolution speech in real-time.
This capability makes them suitable for tasks such as TTS and speech restoration.
However, GANbased models face inherent challenges: while they produce sharp and detailed audio, they struggle with capturing long-term dependencies and complex patterns crucial for high-fidelity speech.
Furthermore, training GAN models can be unstable, leading to mode collapse or inconsistencies in the generated audio.
Despite these drawbacks, GAN-based vocoders remain a strong choice for real-time and high-resolution applications.

In contrast, diffusion models [5], [6], [7], [8] have gained attention for their ability to enhance the stability and quality of the audio generation process.
By employing a step-by-step refinement process, diffusion models can produce consistent and natural-sounding speech, excelling in capturing complex and subtle audio details.
This makes them particularly well-suited for high-quality, non-real-time synthesis.
However, recent research has pointed out that these models may have limitations for time-sensitive applications due to slower generation speeds and higher computational demands.

In addition to GANs and diffusion models, flow-based models (e.g., WaveGlow [9], Flow-TTS [10], P-Flow [11], ReFlowTTS [12]) and autoregressive models (e.g., Tacotron [13], NaturalSpeech [14]) have contributed to advancements in efficiency and quality.
Autoregressive models excel at modeling the natural flow of speech but often sacrifice speed for quality.
Flow-based models strike a balance between speed and fidelity but are less widely used than GANs and diffusion models in speech synthesis.
Optimized architectures such as iSTFT-Net [15] have further improved real-time processing efficiency, and multimodal audio generation models leveraging inputs such as text, images, and video have opened new possibilities for innovative applications.
Non-autoregressive approaches (e.g., FastSpeech [16], Parallel WaveGAN [1]) have also demonstrated significant strides in speed and quality, enabling real-time and interactive applications.

Despite these advancements, significant challenges persist.
GAN-based vocoders are effective for generating high-resolution audio but still struggle with capturing long-term dependencies, which can lead to quality degradation.
Diffusion models have improved stability but remain computationally expensive and unsuitable for real-time applications due to their sequential nature.

To address these challenges, we propose a novel GAN-based vocoder called RingFormer that incorporates convolution-augmented Transformers, known as Conformer [17], and an efficient ring attention [18] mechanism introduced in previous research.
While GANs offer the speed and high resolution necessary for real-time synthesis, RingFormer leverages the Conformer architecture to better capture both local details and global dependencies, addressing key weaknesses of traditional GAN-based models.
Furthermore, ring attention enhances computational efficiency by focusing attention on localized regions while maintaining the ability to model long-range dependencies.
This hybrid architecture, RingFormer, balances the tradeoffs between speed and resolution, achieving the temporal resolution and efficiency needed for real-time speech synthesis while maintaining the high-quality audio output expected from modern TTS systems.

The remainder of this paper is organized as follows: Section II reviews related work.
Section III describes the proposed model architecture, Section IV explains the loss functions, Section V presents experimental results and performance analysis, and Section VI concludes the paper.

## 2·Related Works: 相关工作

GANs have emerged as powerful models in the domain of audio synthesis, particularly for generating high-quality raw audio waveforms.
WaveGAN [19], introduced by Donahue et al., was the first GAN-based approach designed to directly generate raw audio waveforms by adapting the DCGAN [20] architecture for one-dimensional audio data.
Although WaveGAN demonstrated the feasibility of unsupervised learning for audio generation, it faced limitations in capturing fine-grained details.
Building on this foundation, MelGAN [21] introduced a multiscale discriminator that leveraged average pooling to downsample audio at multiple scales.
By incorporating window-based discriminators to model audio features across different resolutions, MelGAN achieved efficient and high-quality audio synthesis with improved fidelity.

HiFi-GAN [2], proposed by Kong et al., advanced the field by adopting a multi-period discriminator capable of capturing periodic structures in time-domain audio.
The model combined short-time Fourier transform (STFT) loss and mel-spectrogram loss, enabling it to generate high-resolution, natural-sounding audio suitable for speech synthesis and restoration tasks.
GANTTS [22] further refined the use of GANs in audio synthesis by utilizing a conditional feed-forward generator alongside an ensemble of discriminators that operated on random windows of varying sizes.
This approach enabled GAN-TTS to achieve high-quality audio synthesis while maintaining both local coherence and global consistency.

Parallel WaveGAN [1], introduced by Yamamoto et al., incorporated a combination of multi-resolution STFT loss and adversarial loss in the waveform domain.
This innovation allowed for parallel waveform generation, eliminating the need for complex probability density distillation techniques and significantly enhancing both generation speed and quality.
Similarly, iSTFTNet [15] simplified the output layers of traditional CNN-based vocoders by replacing them with inverse STFT layers.
This design reduced model complexity and computational costs while maintaining audio quality.

BigVGAN [3], developed by Lee et al., pushed the boundaries of GAN-based audio synthesis by incorporating periodic activation functions to stabilize training and anti-aliasing techniques to reduce artifacts.
These features enhanced fidelity and robustness in the generated audio, making BigVGAN a notable advancement in high-resolution audio synthesis.

While these GAN-based models have driven significant advancements in audio generation, they often struggle to capture long-term dependencies due to their reliance on iterative upsampling processes to expand receptive fields.
This limitation can result in inconsistencies when modeling extended temporal relationships in audio data.
To address these challenges, we propose a novel generator architecture, RingFormer, which integrates self-attention mechanisms with convolutional layers.
This hybrid approach enables the model to effectively capture long-term dependencies while maintaining computational efficiency.
Additionally, the incorporation of ring attention reduces computational overhead by focusing on fixed local regions, preserving both local and global relationships.
Enhanced loss functions are also introduced to enable more accurate and efficient audio synthesis.

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论