# RingFormer

<details>
<summary>Âü∫Êú¨‰ø°ÊÅØ</summary>

- Ê†áÈ¢ò: "RingFormer: A Neural Vocoder with Ring Attention and Convolution-Augmented Transformer"
- ‰ΩúËÄÖ:
  - 01 Seongho Hong
  - 02 Yong-Hoon Choi
- ÈìæÊé•:
  - [ArXiv](https://arxiv.org/abs/2501.01182)
  - [Publication]()
  - [Github](https://github.com/seongho608/RingFormer)
  - [Demo](https://seongho608.github.io/)
- Êñá‰ª∂:
  - [ArXiv](_PDF/2501.01182v1__RingFormer__A_Neural_Vocoder_with_Ring_Attention_and_Convolution-Augmented_Transformer.pdf)
  - [Publication] #TODO

</details>

## Abstract: ÊëòË¶Å

While transformers demonstrate outstanding performance across various audio tasks, their application to neural vocoders remains challenging.
Neural vocoders require the generation of long audio signals at the sample level, which demands high temporal resolution.
This results in significant computational costs for attention map generation and limits their ability to efficiently process both global and local information.
Additionally, the sequential nature of sample generation in neural vocoders poses difficulties for real-time processing, making the direct adoption of transformers impractical.
To address these challenges, we propose RingFormer, a neural vocoder that incorporates the ring attention mechanism into a lightweight transformer variant, the convolution-augmented transformer (Conformer).
Ring attention effectively captures local details while integrating global information, making it well-suited for processing long sequences and enabling real-time audio generation.
RingFormer is trained using adversarial training with two discriminators.
The proposed model is applied to the decoder of the text-to-speech model VITS and compared with state-of-the-art vocoders such as HiFi-GAN, iSTFT-Net, and BigVGAN under identical conditions using various objective and subjective metrics.
Experimental results show that RingFormer achieves comparable or superior performance to existing models, particularly excelling in real-time audio generation.
Our code and audio samples are available on GitHub.

## 1¬∑Introduction: ÂºïË®Ä

Audio generation models have become core technologies in various application fields such as speech synthesis, music generation, and sound effect creation.
Recent advancements have significantly enhanced generation quality and stability through generative adversarial network (GAN)-based models (e.g., Parallel WaveGAN [1], HiFi-GAN [2], BigVGAN [3], Avocodo [4]) and diffusion models (e.g., Grad-TTS [5], WaveGrad [6], Diff-TTS [7], E3 TTS [8]), both aiming to achieve high-quality speech synthesis.

Text-to-speech (TTS) models, which map text input to speech output, have seen major improvements in recent years by leveraging advancements in generative models.
Among the components of a TTS system, vocoders play a pivotal role in determining the final audio quality.
They are responsible for converting intermediate audio representations, such as mel-spectrograms, into waveform audio.
A high-performing vocoder is essential for achieving natural and high-fidelity speech, as it directly impacts both the clarity and temporal consistency of the output audio.
Studies suggest that vocoders influence more than 50% of the overall system performance, underscoring their critical importance.

GAN-based vocoders [1], [2], [3], [4] have emerged as a leading approach due to their ability to generate high-resolution speech in real-time.
This capability makes them suitable for tasks such as TTS and speech restoration.
However, GANbased models face inherent challenges: while they produce sharp and detailed audio, they struggle with capturing long-term dependencies and complex patterns crucial for high-fidelity speech.
Furthermore, training GAN models can be unstable, leading to mode collapse or inconsistencies in the generated audio.
Despite these drawbacks, GAN-based vocoders remain a strong choice for real-time and high-resolution applications.

In contrast, diffusion models [5], [6], [7], [8] have gained attention for their ability to enhance the stability and quality of the audio generation process.
By employing a step-by-step refinement process, diffusion models can produce consistent and natural-sounding speech, excelling in capturing complex and subtle audio details.
This makes them particularly well-suited for high-quality, non-real-time synthesis.
However, recent research has pointed out that these models may have limitations for time-sensitive applications due to slower generation speeds and higher computational demands.

In addition to GANs and diffusion models, flow-based models (e.g., WaveGlow [9], Flow-TTS [10], P-Flow [11], ReFlowTTS [12]) and autoregressive models (e.g., Tacotron [13], NaturalSpeech [14]) have contributed to advancements in efficiency and quality.
Autoregressive models excel at modeling the natural flow of speech but often sacrifice speed for quality.
Flow-based models strike a balance between speed and fidelity but are less widely used than GANs and diffusion models in speech synthesis.
Optimized architectures such as iSTFT-Net [15] have further improved real-time processing efficiency, and multimodal audio generation models leveraging inputs such as text, images, and video have opened new possibilities for innovative applications.
Non-autoregressive approaches (e.g., FastSpeech [16], Parallel WaveGAN [1]) have also demonstrated significant strides in speed and quality, enabling real-time and interactive applications.

Despite these advancements, significant challenges persist.
GAN-based vocoders are effective for generating high-resolution audio but still struggle with capturing long-term dependencies, which can lead to quality degradation.
Diffusion models have improved stability but remain computationally expensive and unsuitable for real-time applications due to their sequential nature.

To address these challenges, we propose a novel GAN-based vocoder called RingFormer that incorporates convolution-augmented Transformers, known as Conformer [17], and an efficient ring attention [18] mechanism introduced in previous research.
While GANs offer the speed and high resolution necessary for real-time synthesis, RingFormer leverages the Conformer architecture to better capture both local details and global dependencies, addressing key weaknesses of traditional GAN-based models.
Furthermore, ring attention enhances computational efficiency by focusing attention on localized regions while maintaining the ability to model long-range dependencies.
This hybrid architecture, RingFormer, balances the tradeoffs between speed and resolution, achieving the temporal resolution and efficiency needed for real-time speech synthesis while maintaining the high-quality audio output expected from modern TTS systems.

The remainder of this paper is organized as follows: Section II reviews related work.
Section III describes the proposed model architecture, Section IV explains the loss functions, Section V presents experimental results and performance analysis, and Section VI concludes the paper.

## 2¬∑Related Works: Áõ∏ÂÖ≥Â∑•‰Ωú

GANs have emerged as powerful models in the domain of audio synthesis, particularly for generating high-quality raw audio waveforms.
WaveGAN [19], introduced by Donahue et al., was the first GAN-based approach designed to directly generate raw audio waveforms by adapting the DCGAN [20] architecture for one-dimensional audio data.
Although WaveGAN demonstrated the feasibility of unsupervised learning for audio generation, it faced limitations in capturing fine-grained details.
Building on this foundation, MelGAN [21] introduced a multiscale discriminator that leveraged average pooling to downsample audio at multiple scales.
By incorporating window-based discriminators to model audio features across different resolutions, MelGAN achieved efficient and high-quality audio synthesis with improved fidelity.

HiFi-GAN [2], proposed by Kong et al., advanced the field by adopting a multi-period discriminator capable of capturing periodic structures in time-domain audio.
The model combined short-time Fourier transform (STFT) loss and mel-spectrogram loss, enabling it to generate high-resolution, natural-sounding audio suitable for speech synthesis and restoration tasks.
GANTTS [22] further refined the use of GANs in audio synthesis by utilizing a conditional feed-forward generator alongside an ensemble of discriminators that operated on random windows of varying sizes.
This approach enabled GAN-TTS to achieve high-quality audio synthesis while maintaining both local coherence and global consistency.

Parallel WaveGAN [1], introduced by Yamamoto et al., incorporated a combination of multi-resolution STFT loss and adversarial loss in the waveform domain.
This innovation allowed for parallel waveform generation, eliminating the need for complex probability density distillation techniques and significantly enhancing both generation speed and quality.
Similarly, iSTFTNet [15] simplified the output layers of traditional CNN-based vocoders by replacing them with inverse STFT layers.
This design reduced model complexity and computational costs while maintaining audio quality.

BigVGAN [3], developed by Lee et al., pushed the boundaries of GAN-based audio synthesis by incorporating periodic activation functions to stabilize training and anti-aliasing techniques to reduce artifacts.
These features enhanced fidelity and robustness in the generated audio, making BigVGAN a notable advancement in high-resolution audio synthesis.

While these GAN-based models have driven significant advancements in audio generation, they often struggle to capture long-term dependencies due to their reliance on iterative upsampling processes to expand receptive fields.
This limitation can result in inconsistencies when modeling extended temporal relationships in audio data.
To address these challenges, we propose a novel generator architecture, RingFormer, which integrates self-attention mechanisms with convolutional layers.
This hybrid approach enables the model to effectively capture long-term dependencies while maintaining computational efficiency.
Additionally, the incorporation of ring attention reduces computational overhead by focusing on fixed local regions, preserving both local and global relationships.
Enhanced loss functions are also introduced to enable more accurate and efficient audio synthesis.

## 3¬∑Methodology: ÊñπÊ≥ï

### Architecture: Êû∂ÊûÑ

The overall architecture of the proposed model consists of one generator and two discriminators, as shown in Figure 1.
The generator maps the spectrogram $z$ to an audio waveform $G_{\phi}(z)$, while the two discriminators $D_{\phi}$ and $D_{\psi}$ compare the real audio waveform $x$ and the generated waveform $G_{\phi}(z)$ in different ways.

#### Generator: ÁîüÊàêÂô®

Recognizing that capturing long-term dependencies is crucial for modeling realistic speech audio, we propose a new generator architecture designed to learn these dependencies more effectively.
The proposed architecture, as shown in Figure 2, incorporates two stages of Conformer blocks with ring attention and √ó4 upsampling between the input and output convolutions.
This approach contrasts with the upsampling process in HiFiGAN, which uses the multi-receptive field fusion (MRF) technique with [√ó8, √ó8, √ó2, √ó2] transpose convolutions to perform upsampling and reconstruct raw audio.
In comparison, our proposed structure simplifies the upsampling process by using two stages of Conformer blocks with ring attention and √ó4 upsampling, providing a more efficient and streamlined approach to generating high-quality audio.
The remaining Conformer blocks, excluding the ring attention, are identical to those in [18].
This modification improves the ability to capture long-term dependencies in the generated audio, enhancing the model's overall performance and synthesis quality.
In the upsampling block, the snake activation function [23] helps the model learn the periodic structure of speech signals more accurately.
Although the final output of the generator is the magnitude and phase of the spectrogram rather than the waveform, these components also exhibit periodic characteristics, making them suitable for modeling periodic structures.
Unlike BigVGAN [3], no anti-aliasing filter is used for upsampling, as smaller upsampling ratios allow for more stable high-frequency processing.
After upsampling, the inverse STFT reconstructs the signal in the frequency domain, separating amplitude and phase for better control.
This structure maintains memory efficiency for long sequences while improving the learning of long-term dependencies in speech signals.
Through these improvements, RingFormer achieves more precise speech synthesis without sacrificing speed.

#### Ring Attention: ÁéØÊ≥®ÊÑèÂäõ

Capturing long-term dependencies is crucial for modeling realistic speech audio.
For instance, the duration of a phoneme can exceed 100ms, resulting in a high correlation between more than 2,200 adjacent samples in the raw waveform.
Ring attention [18] is a mechanism designed to efficiently process long sequences by leveraging block-wise parallel computation.
In RingFormer, ring attention is tailored for vocoders to effectively handle long sequences of speech signals.

First, the mel-spectrogram upsampled from the MRF is divided into $N_d$ fixed-size blocks, and each block is assigned to an individual device.
Here, device refers to an individual computational unit in a parallel processing system, while block represents a segment of a long sequence divided into a fixed length.
Each device generates query, key, and value based on the divided mel-spectrogram, which are obtained through affine transformations using learnable weight matrices $W_Q$, $W_K$, and $W_V$.

Subsequently, a key-value exchange mechanism based on a ring topology allows each device to receive key and value data from its adjacent device.
This data exchange enables information to flow between blocks, thereby effectively integrating global dependencies and context across the entire sequence.
This structure is well-suited for modeling both the temporal dependencies of speech signals and the harmonic structure within frequency bands, allowing it to capture the periodic characteristics of speech in detail.
In particular, ring attention effectively resolves the memory bottleneck issue encountered when processing long sequences in vocoders.
Since key-value exchanges and attention computations are designed to be performed in parallel across devices, computational efficiency is maximized, significantly reducing memory and computational costs during the training process for long sequences of data.
Block-wise attention computations within the device are carried out as follows:

where $i$ denotes the device index, and $d_k$ represents the dimension of the key vector.
The query $Q_i$ performs a scaled dot product computation with the keys $K=\{K_i,\cdots,K_{i+d-1}\}$ in the same device, which is then multiplied with $V=\{V_i,\cdots,V_{i+d-1}\}$ to calculate the attention values.
This method overcomes the memory constraints of traditional Transformer [24] models, allowing the context size to scale linearly with the number of devices.
As a result, ring attention maintains computational efficiency while achieving high performance in both training and inference for extremely large context sizes.

#### Discriminators: Âà§Âà´Âô®

We use two discriminators for generator training: the multi-period discriminator (MPD) and the multi-scale sub-band constant-Q transform (MS-SB-CQT) discriminator.

Since speech audio consists of sinusoidal signals with various periods, it is necessary to identify the diverse periodic patterns inherent in the audio data.
To this end, HiFi-GAN [2] proposed the MPD, and in this paper, we use the same MPD without modification.

Additionally, the MS-SB-CQT discriminator [25] improves upon the multi-scale discriminator (MSD) of Mel-GAN [21] by using constant-Q transform (CQT) to process more precise frequency band information.
This approach enhances both frequency and time resolution, capturing more detailed characteristics of the speech signal and enabling more natural speech synthesis results.
While the original MSD focused on capturing information across multiple frequency ranges, CQT allows for more detailed frequency band analysis, providing finer frequency interpretation.
In this paper, the MS-SB-CQT discriminator is used without modification.

By using these two discriminators, the diverse periodic patterns inherent in the audio can be distinguished, and detailed characteristics by frequency can be captured.

### Training Objective: ËÆ≠ÁªÉÁõÆÊ†á

We use various loss functions to optimize RingFormer.
To evaluate speech quality, it is integrated into the widely used TTS model VITS [26] and trained by connecting it to the model.
As a result, the encoder parameters of VITS are also updated.

#### Adversarial Loss: ÂØπÊäóÊçüÂ§±

The RingFormer is trained using two discriminators.
The first is the MPD, originally proposed in HiFi-GAN [2], and the second is the MS-SB-CQT discriminator [25].
The MPD is designed as a combination of sub-discriminators based on Markovian windows, with each sub-discriminator specializing in detecting different periodic patterns in the input waveform.
This structure allows for a systematic evaluation of speech data with diverse periodic characteristics.
However, a limitation of the MPD is that its sub-discriminators evaluate only isolated samples, potentially overlooking broader contextual information.
To address this limitation, the MS-SB-CQT discriminator, as proposed in [25], is incorporated to enhance performance.
The adversarial loss is defined as follows:

$$
\begin{aligned}
\mathcal{L}_{adv} &= \mathcal{L}_G + \mathcal{L}_D \\
\mathcal{L}_G &= \alpha \mathbb{E}_z [(1-D_\theta(G_{\phi}(z)))^2] + (1-\alpha) \mathbb{E}_x [(1-D_{\psi}(x))^2] \\
\mathcal{L}_D &= \alpha \mathbb{E}_{x,z} [(1-D_\theta(x))^2 + (D_\theta(G_{\phi}(z)))^2] + (1-\alpha) \mathbb{E}_{x,z} [(1- D_{\psi}(x))^2 + (D_{\psi}(G_{\phi}(z)))^2] \\
\end{aligned}
$$

The contribution of each discriminator to the training loss is controlled by a weighting factor, $\alpha$, which is set to 0.5 to balance their roles during adversarial training.

#### Spectral Decomposition Loss: È¢ëË∞±ÂàÜËß£ÊçüÂ§±

In our work, we explicitly learn magnitude loss and phase loss, building on the findings of [27].
This approach ensures the accurate reproduction of spectral energy (magnitude) and precise temporal alignment (phase), reducing distortions and enhancing perceptual quality.
By separately optimizing magnitude and phase, we achieve a balanced trade-off between the time and frequency domains, resulting in better generalization across diverse audio data and more natural sound reconstruction.
The spectral decomposition loss is defined as follows:

$$
\begin{aligned}
\mathcal{L}_{sd} = \mathcal{L}_{mag} + \mathcal{L}_{arg} \\
\mathcal{L}_{mag} &= \mathbb{E}_{x,z} [\| |F(x)| - |F(G_{\phi}(z))| \|_1] \\
\mathcal{L}_{arg} &= \mathbb{E}_{x,z} [\| \angle F(x) - \angle F(G_{\phi}(z)) \|_1] \\
\end{aligned}
$$

Here, $F(\cdot)$ denotes the short-time Fourier transform of the input signal.
This loss compares the amplitude and phase of the audio signal generated by RingFormer with the amplitude and phase of the ground truth.

#### Feature Matching Loss: ÁâπÂæÅÂåπÈÖçÊçüÂ§±

The feature matching loss $\mathcal{L}_{fm}$ [21] minimizes the $l_1$ distance between the intermediate features extracted from the discriminator layers:

$$
\mathcal{L}_{fm} = \mathbb{E}_{x,z} [\sum_{i=1}^{T} \dfrac{1}{N_i} \| D_k^i(x) - D_k^i(G_{\phi}(z)) \|_1]
$$

where $T$ is the number of layers in the sub-discriminator $D_k$, and $N_i$ is the number of features in the ùëñ-th layer.
The feature matching loss encourages the generator to produce outputs whose intermediate features are similar to those of the real data, improving the generator's ability to match the discriminator's learned feature representations.

#### Final Loss: ÊúÄÁªàÊçüÂ§±

The proposed RingFormer is implemented to replace the decoder of the widely used end-to-end TTS model, VITS.
While the training environment is integrated with VITS, RingFormer is not dependent on it.
Unlike models such as FastSpeech [16], VITS eliminates the need for a separate duration predictor or aligner (e.g., attention alignment in Tacotron [13]).
Additionally, VITS combines a GAN with a variational autoencoder (VAE) [28] to generate high-resolution and natural-sounding speech.

In this paper, the proposed RingFormer is optimized using two additional loss functions adopted from VITS.
The first is $\mathcal{L}_{dur}$, which facilitates learning text-to-speech alignment, and the second is $\mathcal{L}_{KL}$, which plays a critical role in modeling the relationship between text and speech in the latent space.
$\mathcal{L}_{KL}$ regulates the distribution of latent variables, enabling natural speech synthesis and supporting the alignment-free structure.
These two loss functions are applied without modification during the training of the proposed model.
The total loss function is defined as follows:

$$
\mathcal{L} = \mathcal{L}_{adv} + \lambda_{sd} \mathcal{L}_{sd} + \lambda_{fm} \mathcal{L}_{fm} + \lambda_{recon} \mathcal{L}_{recon} + \lambda_{KL} \mathcal{L}_{KL} + \lambda_{dur} \mathcal{L}_{dur}
$$

The hyperparameters $\lambda_{sd}$, $\lambda_{fm}$, $\lambda_{recon}$, $\lambda_{KL}$, $\lambda_{dur}$ are all set to 1 in this study.
This decision was made because the magnitudes of the individual observed loss values were similar.
By setting these hyperparameters to 1, we ensure that each loss component contributes equally to the total loss without introducing arbitrary scaling factors, thereby facilitating a balanced optimization process.

## 4¬∑Experiments: ÂÆûÈ™å

## 5¬∑Results: ÁªìÊûú

## 6¬∑Conclusions: ÁªìËÆ∫

In this paper, we propose RingFormer, a vocoder that efficiently processes long sequences with long-term dependencies through a Conformer block with Ring Attention, while maintaining a reasonable memory usage to synthesize high-quality speech.
This structure captures both local and global dependencies in speech signals, enabling the generation of more natural-sounding speech.
Additionally, to improve generation speed, the output layer incorporates an inverse STFT structure, and by adding phase and magnitude losses to the loss function, it finely learns temporal patterns and amplitude information, thereby enhancing the quality of the synthesized speech.
For adversarial training, we introduce the recently released MS-SB-CQT discriminator, which improves the precision of speech synthesis by more accurately evaluating continuous sequences.
Through various objective metrics such as MCD, WER, STOI, and NISQA, as well as MOS and CMOS evaluations, we verify that RingFormer performs on par with or better than existing models, successfully achieving natural speech and clarity.
This study presents a model that balances fast speech synthesis speed and high quality, contributing to the advancement of speech synthesis technology.
Future research will aim to expand the applicability of RingFormer by optimizing it for multilingual datasets and various application environments.
