# RingFormer

<details>
<summary>åŸºæœ¬ä¿¡æ¯</summary>

- æ ‡é¢˜: "RingFormer: A Neural Vocoder with Ring Attention and Convolution-Augmented Transformer"
- ä½œè€…:
  - 01 Seongho Hong
  - 02 Yong-Hoon Choi
- é“¾æ¥:
  - [ArXiv](https://arxiv.org/abs/2501.01182)
  - [Publication]()
  - [Github](https://github.com/seongho608/RingFormer)
  - [Demo](https://seongho608.github.io/)
- æ–‡ä»¶:
  - [ArXiv](_PDF/2501.01182v1__RingFormer__A_Neural_Vocoder_with_Ring_Attention_and_Convolution-Augmented_Transformer.pdf)
  - [Publication] #TODO

</details>

## Abstract: æ‘˜è¦

<details>
<summary>å±•å¼€åŸæ–‡</summary>

While transformers demonstrate outstanding performance across various audio tasks, their application to neural vocoders remains challenging.
Neural vocoders require the generation of long audio signals at the sample level, which demands high temporal resolution.
This results in significant computational costs for attention map generation and limits their ability to efficiently process both global and local information.
Additionally, the sequential nature of sample generation in neural vocoders poses difficulties for real-time processing, making the direct adoption of transformers impractical.
To address these challenges, we propose ***RingFormer***, a neural vocoder that incorporates the ring attention mechanism into a lightweight transformer variant, the convolution-augmented transformer (Conformer).
Ring attention effectively captures local details while integrating global information, making it well-suited for processing long sequences and enabling real-time audio generation.
***RingFormer*** is trained using adversarial training with two discriminators.
The proposed model is applied to the decoder of the text-to-speech model VITS and compared with state-of-the-art vocoders such as HiFi-GAN, iSTFT-Net, and BigVGAN under identical conditions using various objective and subjective metrics.
Experimental results show that ***RingFormer*** achieves comparable or superior performance to existing models, particularly excelling in real-time audio generation.
Our code and audio samples are available on GitHub.

</details>
<br>

å°½ç®¡ Transformers åœ¨å„ç§éŸ³é¢‘ä»»åŠ¡ä¸Šéƒ½å±•ç°äº†å“è¶Šçš„æ€§èƒ½, ä½†å®ƒä»¬åœ¨ç¥ç»å£°ç å™¨çš„åº”ç”¨ä¸Šä»ç„¶å­˜åœ¨æŒ‘æˆ˜.
ç¥ç»å£°ç å™¨éœ€è¦åœ¨é‡‡æ ·çº§åˆ«ç”Ÿæˆé•¿éŸ³é¢‘ä¿¡å·, éœ€è¦é«˜æ—¶åŸŸåˆ†è¾¨ç‡.
è¿™å¯¼è‡´å¯¹æ³¨æ„åŠ›æ˜ å°„ç”Ÿæˆçš„è®¡ç®—æˆæœ¬å¾ˆé«˜, é™åˆ¶äº†å®ƒä»¬æœ‰æ•ˆå¤„ç†å…¨å±€å’Œå±€éƒ¨ä¿¡æ¯çš„èƒ½åŠ›.
æ­¤å¤–, ç¥ç»å£°ç å™¨çš„é‡‡æ ·ç”Ÿæˆçš„åºåˆ—æ€§è´¨åˆä½¿å¾—å®æ—¶å¤„ç†å˜å¾—å›°éš¾, ä½¿å¾—ç›´æ¥é‡‡ç”¨ Transformers å®é™…ä¸Šæ˜¯ä¸åˆ‡å®é™…çš„.

ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜, æˆ‘ä»¬æå‡º ***RingFormer***, æ˜¯åœ¨è½»é‡ Transformer å˜ä½“ Conformer ä¸­èå…¥ç¯æ³¨æ„åŠ›æœºåˆ¶çš„ç¥ç»å£°ç å™¨.
ç¯æ³¨æ„åŠ›æœ‰æ•ˆåœ°æ•æ‰å±€éƒ¨ç»†èŠ‚çš„åŒæ—¶èåˆå…¨å±€ä¿¡æ¯, ä½¿å¾—é€‚åˆå¤„ç†é•¿åºåˆ—, å¹¶å®ç°å®æ—¶éŸ³é¢‘ç”Ÿæˆ.
***RingFormer*** ä½¿ç”¨å…·æœ‰ä¸¤ä¸ªåˆ¤åˆ«å™¨çš„å¯¹æŠ—è®­ç»ƒ.

æ‰€æå‡ºçš„æ¨¡å‹è¢«åº”ç”¨åˆ°æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ VITS çš„è§£ç å™¨ä¸­, å¹¶å’Œå…¶ä»–å…ˆè¿›å£°ç å™¨å¦‚ HiFi-GAN, iSTFTNet, BigVGAN è¿›è¡Œäº†ç›¸åŒæ¡ä»¶ä¸‹çš„å„ç§å®¢è§‚å’Œä¸»è§‚æŒ‡æ ‡çš„æ¯”è¾ƒ.
å®éªŒç»“æœè¡¨æ˜ ***RingFormer*** ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”æœ‰ç€ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½, å°¤å…¶æ˜¯åœ¨å®æ—¶éŸ³é¢‘ç”Ÿæˆæ–¹é¢.
æˆ‘ä»¬çš„ä»£ç å’ŒéŸ³é¢‘æ ·æœ¬å¯åœ¨ [Github](https://github.com/seongho608/RingFormer) ä¸Šè·å¾—.

## 1Â·Introduction: å¼•è¨€

<details>
<summary>å±•å¼€åŸæ–‡</summary>

Audio generation models have become core technologies in various application fields such as speech synthesis, music generation, and sound effect creation.
Recent advancements have significantly enhanced generation quality and stability through generative adversarial network (GAN)-based models (e.g., [Parallel WaveGAN [1]](2019.10.25_Parallel_WaveGAN.md), [HiFi-GAN [2]](2020.10.12_HiFi-GAN.md), [BigVGAN [3]](2022.06.09_BigVGAN.md), [Avocodo [4]](2022.06.27_Avocodo.md)) and diffusion models (e.g., [Grad-TTS [5]](../Acoustic/2021.05.13_Grad-TTS.md), [WaveGrad [6]](2020.09.02_WaveGrad.md), [Diff-TTS [7]](../Acoustic/2021.04.03_Diff-TTS.md), [E3 TTS [8]](../Diffusion/2023.11.02_E3_TTS.md)), both aiming to achieve high-quality speech synthesis.

Text-to-speech (TTS) models, which map text input to speech output, have seen major improvements in recent years by leveraging advancements in generative models.
Among the components of a TTS system, vocoders play a pivotal role in determining the final audio quality.
They are responsible for converting intermediate audio representations, such as mel-spectrograms, into waveform audio.
A high-performing vocoder is essential for achieving natural and high-fidelity speech, as it directly impacts both the clarity and temporal consistency of the output audio.
Studies suggest that vocoders influence more than 50% of the overall system performance, underscoring their critical importance.

GAN-based vocoders ([Parallel WaveGAN [1]](2019.10.25_Parallel_WaveGAN.md), [HiFi-GAN [2]](2020.10.12_HiFi-GAN.md), [BigVGAN [3]](2022.06.09_BigVGAN.md), [Avocodo [4]](2022.06.27_Avocodo.md)) have emerged as a leading approach due to their ability to generate high-resolution speech in real-time.
This capability makes them suitable for tasks such as TTS and speech restoration.
However, GAN-based models face inherent challenges: while they produce sharp and detailed audio, they struggle with capturing long-term dependencies and complex patterns crucial for high-fidelity speech.
Furthermore, training GAN models can be unstable, leading to mode collapse or inconsistencies in the generated audio.
Despite these drawbacks, GAN-based vocoders remain a strong choice for real-time and high-resolution applications.

In contrast, diffusion models ([Grad-TTS [5]](../Acoustic/2021.05.13_Grad-TTS.md), [WaveGrad [6]](2020.09.02_WaveGrad.md), [Diff-TTS [7]](../Acoustic/2021.04.03_Diff-TTS.md), [E3 TTS [8]](../Diffusion/2023.11.02_E3_TTS.md)) have gained attention for their ability to enhance the stability and quality of the audio generation process.
By employing a step-by-step refinement process, diffusion models can produce consistent and natural-sounding speech, excelling in capturing complex and subtle audio details.
This makes them particularly well-suited for high-quality, non-real-time synthesis.
However, recent research has pointed out that these models may have limitations for time-sensitive applications due to slower generation speeds and higher computational demands.

In addition to GANs and diffusion models, flow-based models (e.g., [WaveGlow [9]](2018.10.31_WaveGlow.md), [Flow-TTS [10]](../Acoustic/2020.04.09_Flow-TTS.md), [P-Flow [11]](../Flow/P-Flow.md), [ReFlowTTS [12]](../Flow/2023.09.29_ReFlow-TTS.md)) and autoregressive models (e.g., [Tacotron [13]](../Acoustic/2017.03.29_Tacotron.md), [NaturalSpeech [14]](../E2E/2022.05.09_NaturalSpeech.md)) have contributed to advancements in efficiency and quality.
Autoregressive models excel at modeling the natural flow of speech but often sacrifice speed for quality.
Flow-based models strike a balance between speed and fidelity but are less widely used than GANs and diffusion models in speech synthesis.
Optimized architectures such as [iSTFT-Net [15]](2022.03.04_iSTFTNet.md) have further improved real-time processing efficiency, and multimodal audio generation models leveraging inputs such as text, images, and video have opened new possibilities for innovative applications.
Non-autoregressive approaches (e.g., [FastSpeech [16]](../Acoustic/2019.05.22_FastSpeech.md), [Parallel WaveGAN [1]](2019.10.25_Parallel_WaveGAN.md)) have also demonstrated significant strides in speed and quality, enabling real-time and interactive applications.

Despite these advancements, significant challenges persist.
GAN-based vocoders are effective for generating high-resolution audio but still struggle with capturing long-term dependencies, which can lead to quality degradation.
Diffusion models have improved stability but remain computationally expensive and unsuitable for real-time applications due to their sequential nature.

To address these challenges, we propose a novel GAN-based vocoder called ***RingFormer*** that incorporates convolution-augmented Transformers, known as [Conformer [17]](../ASR/2020.05.16_Conformer.md), and an efficient [ring attention [18]](../../Modules/Attention/RingAttention.md) mechanism introduced in previous research.
While GANs offer the speed and high resolution necessary for real-time synthesis, ***RingFormer*** leverages the Conformer architecture to better capture both local details and global dependencies, addressing key weaknesses of traditional GAN-based models.
Furthermore, ring attention enhances computational efficiency by focusing attention on localized regions while maintaining the ability to model long-range dependencies.
This hybrid architecture, ***RingFormer***, balances the tradeoffs between speed and resolution, achieving the temporal resolution and efficiency needed for real-time speech synthesis while maintaining the high-quality audio output expected from modern TTS systems.

The remainder of this paper is organized as follows: Section II reviews related work.
Section III describes the proposed model architecture, Section IV explains the loss functions, Section V presents experimental results and performance analysis, and Section VI concludes the paper.

</details>
<br>

éŸ³é¢‘ç”Ÿæˆæ¨¡å‹å·²ç»å˜æˆå„ç§åº”ç”¨é¢†åŸŸå¦‚è¯­éŸ³åˆæˆ, éŸ³ä¹ç”Ÿæˆçš„éŸ³æ•ˆåˆ›ä½œç­‰çš„æ ¸å¿ƒæŠ€æœ¯.
è¿‘æœŸè¿›å±•æ˜¾è‘—å¢å¼ºäº†ç”Ÿæˆè´¨é‡å’Œç¨³å®šæ€§, é€šè¿‡åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œ GAN çš„æ¨¡å‹ ([Parallel WaveGAN [1]](2019.10.25_Parallel_WaveGAN.md), [HiFi-GAN [2]](2020.10.12_HiFi-GAN.md), [BigVGAN [3]](2022.06.09_BigVGAN.md), [Avocodo [4]](2022.06.27_Avocodo.md)) å’Œæ‰©æ•£æ¨¡å‹ ([Grad-TTS [5]](../Acoustic/2021.05.13_Grad-TTS.md), [WaveGrad [6]](2020.09.02_WaveGrad.md), [Diff-TTS [7]](../Acoustic/2021.04.03_Diff-TTS.md), [E3 TTS [8]](../Diffusion/2023.11.02_E3_TTS.md)) å®ç°, æ—¨åœ¨å®ç°é«˜è´¨é‡è¯­éŸ³åˆæˆ.

æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹, å°†è¾“å…¥æ–‡æœ¬æ˜ å°„åˆ°è¾“å‡ºè¯­éŸ³, è¿‘å¹´æ¥é€šè¿‡åˆ©ç”¨ç”Ÿæˆå¼æ¨¡å‹çš„æœ€æ–°è¿›å±•å–å¾—äº†é‡å¤§è¿›å±•.
å£°ç å™¨ä½œä¸º TTS ç³»ç»Ÿçš„ç»„ä»¶ä¹‹ä¸€, åœ¨å†³å®šæœ€ç»ˆéŸ³é¢‘è´¨é‡ä¸Šæ‰®æ¼”è€…è‡³å…³é‡è¦çš„è§’è‰².
å®ƒä»¬è´Ÿè´£å°†ä¸­é—´éŸ³é¢‘è¡¨ç¤º, å¦‚æ¢…å°”é¢‘è°±å›¾ (Mel-spectrograms), è½¬æ¢ä¸ºéŸ³é¢‘æ³¢å½¢.
é«˜æ€§èƒ½çš„å£°ç å™¨å¯¹äºå®ç°è‡ªç„¶ä¸”é«˜ä¿çœŸè¯­éŸ³è‡³å…³é‡è¦, å› ä¸ºå®ƒç›´æ¥å½±å“è¾“å‡ºéŸ³é¢‘çš„æ¸…æ™°åº¦å’Œæ—¶åºä¸€è‡´æ€§.
ç ”ç©¶è¡¨æ˜å£°ç å™¨å½±å“ç³»ç»Ÿæ€§èƒ½çš„æ¯”ä¾‹é«˜è¾¾ 50%, å¼ºè°ƒå…¶é‡è¦æ€§.

åŸºäº GAN çš„å£°ç å™¨ ([Parallel WaveGAN [1]](2019.10.25_Parallel_WaveGAN.md), [HiFi-GAN [2]](2020.10.12_HiFi-GAN.md), [BigVGAN [3]](2022.06.09_BigVGAN.md), [Avocodo [4]](2022.06.27_Avocodo.md)) ç”±äºå…¶èƒ½å®æ—¶ç”Ÿæˆé«˜åˆ†è¾¨ç‡è¯­éŸ³çš„èƒ½åŠ›è€Œæˆä¸ºä¸»æµæ–¹æ³•.
è¿™ç§èƒ½åŠ›ä½¿å¾—å®ƒä»¬é€‚åˆç”¨äº TTS å’Œè¯­éŸ³ä¿®å¤ç­‰ä»»åŠ¡.
ç„¶è€ŒåŸºäº GAN çš„æ¨¡å‹é¢ä¸´ç€å›ºæœ‰æŒ‘æˆ˜: è™½ç„¶å®ƒä»¬äº§ç”Ÿæ¸…æ™°è€Œè¯¦ç»†çš„éŸ³é¢‘, å®ƒä»¬åœ¨æ•æ‰é•¿æœŸä¾èµ–å’Œå¤æ‚æ¨¡å¼æ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾.
æ­¤å¤–, è®­ç»ƒ GAN æ¨¡å‹å¯èƒ½ä¸ç¨³å®š, å¯¼è‡´æ¨¡å¼å´©æºƒæˆ–ç”ŸæˆéŸ³é¢‘ä¸ä¸€è‡´.
å°½ç®¡å­˜åœ¨è¿™äº›ç¼ºé™·, åŸºäº GAN çš„å£°ç å™¨ä»ç„¶æ˜¯å®æ—¶å’Œé«˜åˆ†è¾¨ç‡åº”ç”¨çš„å¼ºå¤§é€‰æ‹©.

æ‰©æ•£æ¨¡å‹ ([Grad-TTS [5]](../Acoustic/2021.05.13_Grad-TTS.md), [WaveGrad [6]](2020.09.02_WaveGrad.md), [Diff-TTS [7]](../Acoustic/2021.04.03_Diff-TTS.md), [E3 TTS [8]](../Diffusion/2023.11.02_E3_TTS.md)) åˆ™ç”±äºå…¶èƒ½æå‡éŸ³é¢‘ç”Ÿæˆè¿‡ç¨‹çš„ç¨³å®šæ€§å’Œè´¨é‡è€Œè·å¾—äº†å…³æ³¨.
é€šè¿‡é‡‡ç”¨é€æ­¥ç»†åŒ–è¿‡ç¨‹, æ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆä¸€è‡´ä¸”å¬æ„Ÿè‡ªç„¶çš„è¯­éŸ³, ä¼˜äºæ•æ‰å¤æ‚å’Œå¾®å¦™çš„éŸ³é¢‘ç»†èŠ‚.
è¿™ä½¿å¾—å®ƒä»¬ç‰¹åˆ«é€‚ç”¨äºé«˜è´¨é‡, éå®æ—¶åˆæˆ.
ç„¶è€Œ, è¿‘æœŸç ”ç©¶æŒ‡å‡ºè¿™äº›æ¨¡å‹å¯èƒ½åœ¨æ—¶é—´æ•æ„Ÿåº”ç”¨æ–¹é¢æœ‰æ‰€é™åˆ¶, å› ä¸ºç”Ÿæˆé€Ÿåº¦è¾ƒæ…¢ä¸”è®¡ç®—éœ€æ±‚è¾ƒé«˜.

- é™¤äº† GAN å’Œæ‰©æ•£æ¨¡å‹, åŸºäºæµçš„æ¨¡å‹ ([WaveGlow [9]](2018.10.31_WaveGlow.md), [Flow-TTS [10]](../Acoustic/2020.04.09_Flow-TTS.md), [P-Flow [11]](../Flow/P-Flow.md), [ReFlowTTS [12]](../Flow/2023.09.29_ReFlow-TTS.md)) å’Œè‡ªå›å½’æ¨¡å‹ ([Tacotron [13]](../Acoustic/2017.03.29_Tacotron.md), [NaturalSpeech [14]](../E2E/2022.05.09_NaturalSpeech.md)) ä¹Ÿä¸ºæå‡æ•ˆç‡å’Œè´¨é‡åšå‡ºäº†è´¡çŒ®.
è‡ªå›å½’æ¨¡å‹åœ¨å»ºæ¨¡è¯­éŸ³è‡ªç„¶æµåŠ¨æ–¹é¢è¡¨ç°ä¼˜è¶Šä½†å¾€å¾€ç‰ºç‰²äº†é€Ÿåº¦ä»¥æ¢å–è´¨é‡.
åŸºäºæµçš„æ¨¡å‹åœ¨é€Ÿåº¦å’Œä¿çœŸåº¦ä¹‹é—´å–å¾—äº†å¹³è¡¡, ä½†åœ¨è¯­éŸ³åˆæˆé¢†åŸŸä¸­ä¸å¦‚ GAN å’Œæ‰©æ•£æ¨¡å‹å¹¿æ³›ä½¿ç”¨.
- ä¼˜åŒ–çš„æ¶æ„å¦‚ [iSTFT-Net [15]](2022.03.04_iSTFTNet.md) è¿›ä¸€æ­¥æå‡å®æ—¶å¤„ç†æ•ˆç‡
- å¤šæ¨¡æ€éŸ³é¢‘ç”Ÿæˆæ¨¡å‹åˆ©ç”¨æ–‡æœ¬, å›¾åƒ, è§†é¢‘ç­‰è¾“å…¥, å¼€è¾Ÿäº†æ–°çš„åˆ›æ–°åº”ç”¨çš„å¯èƒ½æ€§.
- éè‡ªå›å½’æ–¹æ³• (å¦‚ [FastSpeech [16]](../Acoustic/2019.05.22_FastSpeech.md), [Parallel WaveGAN [1]](2019.10.25_Parallel_WaveGAN.md)) ä¹Ÿåœ¨æå‡é€Ÿåº¦å’Œè´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•, ä½¿å¾—å®æ—¶å’Œäº¤äº’å¼åº”ç”¨æˆä¸ºå¯èƒ½.

è™½ç„¶æœ‰äº†è¿™äº›è¿›å±•, ä»ç„¶å­˜åœ¨ç€é‡è¦æŒ‘æˆ˜.
- åŸºäº GAN çš„å£°ç å™¨åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡éŸ³é¢‘ä¸Šæœ‰æ•ˆä½†ä»éš¾ä»¥æ•æ‰é•¿æœŸä¾èµ–, å¯¼è‡´è´¨é‡ä¸‹é™.
- æ‰©æ•£æ¨¡å‹æå‡äº†ç¨³å®šæ€§ä½†è®¡ç®—ä»£ä»·é«˜æ˜‚ä¸”ä¸é€‚ç”¨äºå®æ—¶åº”ç”¨, å› ä¸ºå…¶å†…åœ¨çš„åºåˆ—æ€§.

ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜, æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäº GAN çš„å£°ç å™¨ ***RingFormer*** èåˆäº†å·ç§¯å¢å¼ºçš„ Transformer ([Conformer [17]](../ASR/2020.05.16_Conformer.md)), å’Œä¸€ç§é«˜æ•ˆçš„ç¯æ³¨æ„åŠ›æœºåˆ¶ ([Ring Attention [18]](../../Modules/Attention/RingAttention.md)).
- GAN æä¾›å®æ—¶åˆæˆæ‰€éœ€çš„é€Ÿåº¦å’Œé«˜åˆ†è¾¨ç‡, è€Œ ***RingFormer*** åˆ™åˆ©ç”¨ Conformer æ¶æ„æ›´å¥½åœ°æ•æ‰å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ä¾èµ–, å…‹æœäº†ä¼ ç»ŸåŸºäº GAN æ¨¡å‹çš„å…³é”®å¼±ç‚¹.
- æ­¤å¤–, ç¯æ³¨æ„åŠ›å¢å¼ºè®¡ç®—æ•ˆç‡, é€šè¿‡å°†æ³¨æ„åŠ›é›†ä¸­åœ¨å±€éƒ¨åŒºåŸŸçš„åŒæ—¶ä¿æŒå¯¹å…¨å±€ä¾èµ–çš„å»ºæ¨¡èƒ½åŠ›.

é‡‡ç”¨è¿™ç§æ··åˆæ¶æ„çš„ ***RingFormer*** å¯¹é€Ÿåº¦å’Œåˆ†è¾¨ç‡è¿›è¡Œäº†æƒè¡¡, è¾¾åˆ°äº†å®æ—¶è¯­éŸ³åˆæˆæ‰€éœ€çš„æ—¶åºåˆ†è¾¨ç‡å’Œæ•ˆç‡çš„åŒæ—¶, ä¿æŒäº†ç°ä»£ TTS ç³»ç»ŸæœŸæœ›çš„é«˜è´¨é‡éŸ³é¢‘è¾“å‡º.

## 2Â·Related Works: ç›¸å…³å·¥ä½œ

<details>
<summary>å±•å¼€åŸæ–‡</summary>

GANs have emerged as powerful models in the domain of audio synthesis, particularly for generating high-quality raw audio waveforms.
[WaveGAN [19]](2018.02.12_WaveGAN.md), introduced by Donahue et al., was the first GAN-based approach designed to directly generate raw audio waveforms by adapting the [DCGAN [20]](../CV/2015.11.19_DCGAN.md) architecture for one-dimensional audio data.
Although WaveGAN demonstrated the feasibility of unsupervised learning for audio generation, it faced limitations in capturing fine-grained details.
Building on this foundation, [MelGAN [21]](2019.10.08_MelGAN.md) introduced a multiscale discriminator that leveraged average pooling to downsample audio at multiple scales.
By incorporating window-based discriminators to model audio features across different resolutions, MelGAN achieved efficient and high-quality audio synthesis with improved fidelity.

[HiFi-GAN [2]](2020.10.12_HiFi-GAN.md), proposed by Kong et al., advanced the field by adopting a multi-period discriminator capable of capturing periodic structures in time-domain audio.
The model combined short-time Fourier transform (STFT) loss and mel-spectrogram loss, enabling it to generate high-resolution, natural-sounding audio suitable for speech synthesis and restoration tasks.
[GAN-TTS [22]](2019.09.25_GAN-TTS.md) further refined the use of GANs in audio synthesis by utilizing a conditional feed-forward generator alongside an ensemble of discriminators that operated on random windows of varying sizes.
This approach enabled GAN-TTS to achieve high-quality audio synthesis while maintaining both local coherence and global consistency.

[Parallel WaveGAN [1]](2019.10.25_Parallel_WaveGAN.md), introduced by Yamamoto et al., incorporated a combination of multi-resolution STFT loss and adversarial loss in the waveform domain.
This innovation allowed for parallel waveform generation, eliminating the need for complex probability density distillation techniques and significantly enhancing both generation speed and quality.
Similarly, [iSTFTNet [15]](2022.03.04_iSTFTNet.md) simplified the output layers of traditional CNN-based vocoders by replacing them with inverse STFT layers.
This design reduced model complexity and computational costs while maintaining audio quality.

[BigVGAN [3]](2022.06.09_BigVGAN.md), developed by Lee et al., pushed the boundaries of GAN-based audio synthesis by incorporating periodic activation functions to stabilize training and anti-aliasing techniques to reduce artifacts.
These features enhanced fidelity and robustness in the generated audio, making BigVGAN a notable advancement in high-resolution audio synthesis.

While these GAN-based models have driven significant advancements in audio generation, they often struggle to capture long-term dependencies due to their reliance on iterative upsampling processes to expand receptive fields.
This limitation can result in inconsistencies when modeling extended temporal relationships in audio data.
To address these challenges, we propose a novel generator architecture, ***RingFormer***, which integrates self-attention mechanisms with convolutional layers.
This hybrid approach enables the model to effectively capture long-term dependencies while maintaining computational efficiency.
Additionally, the incorporation of ring attention reduces computational overhead by focusing on fixed local regions, preserving both local and global relationships.
Enhanced loss functions are also introduced to enable more accurate and efficient audio synthesis.

</details>
<br>

GAN å·²ç»ç§°ä¸ºéŸ³é¢‘åˆæˆé¢†åŸŸçš„å¼ºåŠ›æ¨¡å‹, å°¤å…¶æ˜¯ç”Ÿæˆé«˜è´¨é‡åŸå§‹éŸ³é¢‘æ³¢å½¢.
- [WaveGAN [19]](2018.02.12_WaveGAN.md) ç”± Donahue et al. æå‡º, æ˜¯ç¬¬ä¸€ä¸ªåŸºäº GAN çš„æ–¹æ³•, é‡‡ç”¨ [DCGAN [20]](../CV/2015.11.19_DCGAN.md) æ¶æ„ç”¨äºä¸€ç»´çš„éŸ³é¢‘æ•°æ®ç›´æ¥ç”ŸæˆåŸå§‹éŸ³é¢‘æ³¢å½¢.
  å°½ç®¡ WaveGAN å±•ç¤ºäº†æ— ç›‘ç£å­¦ä¹ åœ¨éŸ³é¢‘ç”Ÿæˆä¸­çš„å¯è¡Œæ€§, ä½†å®ƒåœ¨æ•è·ç»†ç²’åº¦ç»†èŠ‚æ–¹é¢æœ‰æ‰€å±€é™.
- [MelGAN [21]](2019.10.08_MelGAN.md) åŸºäºæ­¤å‘ç°æå‡ºäº†ä¸€ç§å¤šå°ºåº¦åˆ¤åˆ«å™¨, åˆ©ç”¨å¹³å‡æ± åŒ–åœ¨å¤šä¸ªå°ºåº¦ä¸Šå¯¹éŸ³é¢‘è¿›è¡Œä¸‹é‡‡æ ·.
  é€šè¿‡ç»“åˆåŸºäºçª—å£çš„åˆ¤åˆ«å™¨æ¥è·¨å¤šä¸ªåˆ†è¾¨ç‡ä¸Šå¯¹éŸ³é¢‘ç‰¹å¾å»ºæ¨¡, MelGAN å®ç°äº†é«˜æ•ˆå’Œé«˜è´¨é‡çš„éŸ³é¢‘åˆæˆ, å¹¶æé«˜äº†éŸ³è´¨.
- [HiFi-GAN [2]](2020.10.12_HiFi-GAN.md) ç”± Kong et al. æå‡º, é‡‡ç”¨å¤šå‘¨æœŸåˆ¤åˆ«å™¨æ•æ‰æ—¶åŸŸéŸ³é¢‘ä¸­çš„å‘¨æœŸç»“æ„, è¿›ä¸€æ­¥æ¨åŠ¨äº†éŸ³é¢‘åˆæˆé¢†åŸŸçš„å‘å±•.
  è¯¥æ¨¡å‹ç»“åˆäº†çŸ­æ—¶å‚…é‡Œå¶å˜æ¢æŸå¤±å’Œæ¢…å°”é¢‘è°±æŸå¤±, ä»è€Œèƒ½å¤Ÿç”Ÿæˆé«˜åˆ†è¾¨ç‡, å¬æ„Ÿè‡ªç„¶çš„éŸ³é¢‘, é€‚ç”¨äºè¯­éŸ³åˆæˆå’Œä¿®å¤ä»»åŠ¡.
- [GAN-TTS [22]](2019.09.25_GAN-TTS.md) é€šè¿‡ä½¿ç”¨æ¡ä»¶åŒ–å‰é¦ˆç”Ÿæˆå™¨å’Œä¸€ç»„åœ¨å¤§å°ä¸ä¸€çš„éšæœºçª—å£ä¸Šè¿ä½œçš„åˆ¤åˆ«å™¨, è¿›ä¸€æ­¥ç»†åŒ–äº† GAN åœ¨éŸ³é¢‘åˆæˆä¸­çš„åº”ç”¨.
  è¿™ç§æ–¹æ³•ä½¿å¾— GAN-TTS èƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„éŸ³é¢‘åˆæˆ, åŒæ—¶ä¿æŒå±€éƒ¨è¿è´¯æ€§å’Œå…¨å±€ä¸€è‡´æ€§.
- [Parallel WaveGAN [1]](2019.10.25_Parallel_WaveGAN.md) ç”± Yamamoto et al. æå‡º, åœ¨æ³¢å½¢åŸŸä¸­é‡‡ç”¨å¤šåˆ†è¾¨ç‡ STFT æŸå¤±å’Œå¯¹æŠ—æŸå¤±çš„ç»„åˆ.
  è¿™ä¸€åˆ›æ–°å®ç°äº†å¹¶è¡Œæ³¢å½¢ç”Ÿæˆ, æ¶ˆé™¤äº†å¤æ‚æ¦‚ç‡å¯†åº¦è’¸é¦æŠ€æœ¯çš„éœ€è¦, å¹¶æ˜¾è‘—å¢å¼ºäº†ç”Ÿæˆé€Ÿåº¦å’Œè´¨é‡.
- [iSTFTNet [15]](2022.03.04_iSTFTNet.md) ç±»ä¼¼åœ°ç®€åŒ–äº†ä¼ ç»ŸåŸºäº CNN çš„å£°ç å™¨çš„è¾“å‡ºå±‚, é€šè¿‡å°†å®ƒä»¬æ›¿æ¢ä¸ºé€† STFT å±‚.
  è¿™ä¸€è®¾è®¡å‡å°‘äº†æ¨¡å‹å¤æ‚åº¦å’Œè®¡ç®—æˆæœ¬, åŒæ—¶ä¿æŒéŸ³é¢‘è´¨é‡.
- [BigVGAN [3]](2022.06.09_BigVGAN.md) ç”± Lee et al. æå‡º, é‡‡ç”¨å‘¨æœŸæ¿€æ´»å‡½æ•°æ¥ç¨³å®šè®­ç»ƒ, å’ŒæŠ—é”¯é½¿æŠ€æœ¯æ¥å‡å°‘ä¼ªå½±, ä»è€Œæ¨åŠ¨äº†åŸºäº GAN çš„éŸ³é¢‘ç”Ÿæˆçš„è¾¹ç•Œ.
  è¿™äº›ç‰¹æ€§å¢å¼ºäº†ç”ŸæˆéŸ³é¢‘çš„çœŸå®åº¦å’Œé²æ£’æ€§, ä½¿ BigVGAN æˆä¸ºé«˜åˆ†è¾¨ç‡éŸ³é¢‘åˆæˆé¢†åŸŸçš„æ˜¾è‘—è¿›æ­¥.

å°½ç®¡è¿™äº›åŸºäº GAN çš„æ¨¡å‹å·²ç»åœ¨è¯­éŸ³ç”Ÿæˆé¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•, ä½†å®ƒä»¬ç”±äºä¾èµ–äºè¿­ä»£å¼ä¸Šé‡‡æ ·è¿‡ç¨‹ä»¥æ‰©å±•æ„Ÿå—é‡è€Œéš¾ä»¥æ•è·é•¿æœŸä¾èµ–å…³ç³».
è¿™ä¸€å±€é™æ€§ä¼šå¯¼è‡´æ¨¡å‹åœ¨å»ºæ¨¡éŸ³é¢‘æ•°æ®ä¸­çš„æ‰©å±•æ—¶åºå…³ç³»æ—¶å‡ºç°ä¸ä¸€è‡´æ€§.

ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜, æˆ‘ä»¬æå‡ºäº†æ–°å¼ç”Ÿæˆå™¨æ¶æ„ ***RingFormer***, å®ƒç»“åˆäº†è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå·ç§¯å±‚.
è¿™ä¸€æ··åˆæ–¹æ³•ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•è·é•¿æœŸä¾èµ–å…³ç³», åŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡.
æ­¤å¤–, å¼•å…¥ç¯æ³¨æ„åŠ›å¯ä»¥å‡å°‘è®¡ç®—å¼€é”€, é›†ä¸­äºå›ºå®šå±€éƒ¨åŒºåŸŸ, åŒæ—¶ä¿æŒå±€éƒ¨å’Œå…¨å±€å…³ç³».
è¿˜å¼•å…¥äº†å¢å¼ºçš„æŸå¤±å‡½æ•°, ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´ç²¾ç¡®å’Œé«˜æ•ˆåœ°ç”ŸæˆéŸ³é¢‘.

## 3Â·Methodology: æ–¹æ³•

### Architecture: æ¶æ„

The overall architecture of the proposed model consists of one generator and two discriminators, as shown in Figure 1.
The generator maps the spectrogram $z$ to an audio waveform $G_{\phi}(z)$, while the two discriminators $D_{\phi}$ and $D_{\psi}$ compare the real audio waveform $x$ and the generated waveform $G_{\phi}(z)$ in different ways.

#### Generator: ç”Ÿæˆå™¨

Recognizing that capturing long-term dependencies is crucial for modeling realistic speech audio, we propose a new generator architecture designed to learn these dependencies more effectively.
The proposed architecture, as shown in Figure 2, incorporates two stages of Conformer blocks with ring attention and Ã—4 upsampling between the input and output convolutions.
This approach contrasts with the upsampling process in HiFiGAN, which uses the multi-receptive field fusion (MRF) technique with [Ã—8, Ã—8, Ã—2, Ã—2] transpose convolutions to perform upsampling and reconstruct raw audio.
In comparison, our proposed structure simplifies the upsampling process by using two stages of Conformer blocks with ring attention and Ã—4 upsampling, providing a more efficient and streamlined approach to generating high-quality audio.
The remaining Conformer blocks, excluding the ring attention, are identical to those in [Ring Attention [18]](../../Modules/Attention/RingAttention.md).
This modification improves the ability to capture long-term dependencies in the generated audio, enhancing the model's overall performance and synthesis quality.
In the upsampling block, the [snake activation function [23]](../../Modules/Activation/Snake.md) helps the model learn the periodic structure of speech signals more accurately.
Although the final output of the generator is the magnitude and phase of the spectrogram rather than the waveform, these components also exhibit periodic characteristics, making them suitable for modeling periodic structures.
Unlike [BigVGAN [3]](2022.06.09_BigVGAN.md), no anti-aliasing filter is used for upsampling, as smaller upsampling ratios allow for more stable high-frequency processing.
After upsampling, the inverse STFT reconstructs the signal in the frequency domain, separating amplitude and phase for better control.
This structure maintains memory efficiency for long sequences while improving the learning of long-term dependencies in speech signals.
Through these improvements, ***RingFormer*** achieves more precise speech synthesis without sacrificing speed.

#### Ring Attention: ç¯æ³¨æ„åŠ›

Capturing long-term dependencies is crucial for modeling realistic speech audio.
For instance, the duration of a phoneme can exceed 100ms, resulting in a high correlation between more than 2,200 adjacent samples in the raw waveform.
[Ring attention [18]](../../Modules/Attention/RingAttention.md) is a mechanism designed to efficiently process long sequences by leveraging block-wise parallel computation.
In ***RingFormer***, ring attention is tailored for vocoders to effectively handle long sequences of speech signals.

First, the mel-spectrogram upsampled from the MRF is divided into $N_d$ fixed-size blocks, and each block is assigned to an individual device.
Here, device refers to an individual computational unit in a parallel processing system, while block represents a segment of a long sequence divided into a fixed length.
Each device generates query, key, and value based on the divided mel-spectrogram, which are obtained through affine transformations using learnable weight matrices $W_Q$, $W_K$, and $W_V$.

Subsequently, a key-value exchange mechanism based on a ring topology allows each device to receive key and value data from its adjacent device.
This data exchange enables information to flow between blocks, thereby effectively integrating global dependencies and context across the entire sequence.
This structure is well-suited for modeling both the temporal dependencies of speech signals and the harmonic structure within frequency bands, allowing it to capture the periodic characteristics of speech in detail.
In particular, ring attention effectively resolves the memory bottleneck issue encountered when processing long sequences in vocoders.
Since key-value exchanges and attention computations are designed to be performed in parallel across devices, computational efficiency is maximized, significantly reducing memory and computational costs during the training process for long sequences of data.
Block-wise attention computations within the device are carried out as follows:

where $i$ denotes the device index, and $d_k$ represents the dimension of the key vector.
The query $Q_i$ performs a scaled dot product computation with the keys $K=\{K_i,\cdots,K_{i+d-1}\}$ in the same device, which is then multiplied with $V=\{V_i,\cdots,V_{i+d-1}\}$ to calculate the attention values.
This method overcomes the memory constraints of traditional [Transformer [24]](../_Transformer/2017.06.12_Transformer.md) models, allowing the context size to scale linearly with the number of devices.
As a result, ring attention maintains computational efficiency while achieving high performance in both training and inference for extremely large context sizes.

#### Discriminators: åˆ¤åˆ«å™¨

We use two discriminators for generator training: the multi-period discriminator (MPD) and the multi-scale sub-band constant-Q transform (MS-SB-CQT) discriminator.

Since speech audio consists of sinusoidal signals with various periods, it is necessary to identify the diverse periodic patterns inherent in the audio data.
To this end, [HiFi-GAN [2]](2020.10.12_HiFi-GAN.md) proposed the MPD, and in this paper, we use the same MPD without modification.

Additionally, the [MS-SB-CQT discriminator [25]](2023.11.25_MS-SB-CQT.md) improves upon the multi-scale discriminator (MSD) of [MelGAN [21]](2019.10.08_MelGAN.md) by using constant-Q transform (CQT) to process more precise frequency band information.
This approach enhances both frequency and time resolution, capturing more detailed characteristics of the speech signal and enabling more natural speech synthesis results.
While the original MSD focused on capturing information across multiple frequency ranges, CQT allows for more detailed frequency band analysis, providing finer frequency interpretation.
In this paper, the MS-SB-CQT discriminator is used without modification.

By using these two discriminators, the diverse periodic patterns inherent in the audio can be distinguished, and detailed characteristics by frequency can be captured.

### Training Objective: è®­ç»ƒç›®æ ‡

We use various loss functions to optimize ***RingFormer***.
To evaluate speech quality, it is integrated into the widely used TTS model [VITS [26]](../E2E/2021.06.11_VITS.md) and trained by connecting it to the model.
As a result, the encoder parameters of VITS are also updated.

#### Adversarial Loss: å¯¹æŠ—æŸå¤±

The ***RingFormer*** is trained using two discriminators.
The first is the MPD, originally proposed in [HiFi-GAN [2]](2020.10.12_HiFi-GAN.md), and the second is the [MS-SB-CQT discriminator [25]](2023.11.25_MS-SB-CQT.md).
The MPD is designed as a combination of sub-discriminators based on Markovian windows, with each sub-discriminator specializing in detecting different periodic patterns in the input waveform.
This structure allows for a systematic evaluation of speech data with diverse periodic characteristics.
However, a limitation of the MPD is that its sub-discriminators evaluate only isolated samples, potentially overlooking broader contextual information.
To address this limitation, the [MS-SB-CQT discriminator [25]](2023.11.25_MS-SB-CQT.md), is incorporated to enhance performance.
The adversarial loss is defined as follows:

$$
\begin{aligned}
\mathcal{L}_{adv} &= \mathcal{L}_G + \mathcal{L}_D \\
\mathcal{L}_G &= \alpha \mathbb{E}_z [(1-D_\theta(G_{\phi}(z)))^2] + (1-\alpha) \mathbb{E}_x [(1-D_{\psi}(x))^2] \\
\mathcal{L}_D &= \alpha \mathbb{E}_{x,z} [(1-D_\theta(x))^2 + (D_\theta(G_{\phi}(z)))^2] + (1-\alpha) \mathbb{E}_{x,z} [(1- D_{\psi}(x))^2 + (D_{\psi}(G_{\phi}(z)))^2] \\
\end{aligned}
$$

The contribution of each discriminator to the training loss is controlled by a weighting factor, $\alpha$, which is set to 0.5 to balance their roles during adversarial training.

#### Spectral Decomposition Loss: é¢‘è°±åˆ†è§£æŸå¤±

In our work, we explicitly learn magnitude loss and phase loss, building on the findings of [The Importance of Phase in Speech Enhancement [27]](../_Full/The_Importance_of_Phase_in_Speech_Enhancement.md).
This approach ensures the accurate reproduction of spectral energy (magnitude) and precise temporal alignment (phase), reducing distortions and enhancing perceptual quality.
By separately optimizing magnitude and phase, we achieve a balanced trade-off between the time and frequency domains, resulting in better generalization across diverse audio data and more natural sound reconstruction.
The spectral decomposition loss is defined as follows:

$$
\begin{aligned}
\mathcal{L}_{sd} = \mathcal{L}_{mag} + \mathcal{L}_{arg} \\
\mathcal{L}_{mag} &= \mathbb{E}_{x,z} [\| |F(x)| - |F(G_{\phi}(z))| \|_1] \\
\mathcal{L}_{arg} &= \mathbb{E}_{x,z} [\| \angle F(x) - \angle F(G_{\phi}(z)) \|_1] \\
\end{aligned}
$$

Here, $F(\cdot)$ denotes the short-time Fourier transform of the input signal.
This loss compares the amplitude and phase of the audio signal generated by ***RingFormer*** with the amplitude and phase of the ground truth.

#### Feature Matching Loss: ç‰¹å¾åŒ¹é…æŸå¤±

The feature matching loss $\mathcal{L}_{fm}$ ([MelGAN [21]](2019.10.08_MelGAN.md)) minimizes the $l_1$ distance between the intermediate features extracted from the discriminator layers:

$$
\mathcal{L}_{fm} = \mathbb{E}_{x,z} [\sum_{i=1}^{T} \dfrac{1}{N_i} \| D_k^i(x) - D_k^i(G_{\phi}(z)) \|_1]
$$

where $T$ is the number of layers in the sub-discriminator $D_k$, and $N_i$ is the number of features in the ğ‘–-th layer.
The feature matching loss encourages the generator to produce outputs whose intermediate features are similar to those of the real data, improving the generator's ability to match the discriminator's learned feature representations.

#### Final Loss: æœ€ç»ˆæŸå¤±

The proposed ***RingFormer*** is implemented to replace the decoder of the widely used end-to-end TTS model, VITS.
While the training environment is integrated with VITS, ***RingFormer*** is not dependent on it.
Unlike models such as [FastSpeech [16]](../Acoustic/2019.05.22_FastSpeech.md), VITS eliminates the need for a separate duration predictor or aligner (e.g., attention alignment in [Tacotron [13]](../Acoustic/2017.03.29_Tacotron.md)).
Additionally, VITS combines a GAN with a [variational autoencoder (VAE) [28]](../_Basis/VAE.md) to generate high-resolution and natural-sounding speech.

In this paper, the proposed ***RingFormer*** is optimized using two additional loss functions adopted from VITS.
The first is $\mathcal{L}_{dur}$, which facilitates learning text-to-speech alignment, and the second is $\mathcal{L}_{KL}$, which plays a critical role in modeling the relationship between text and speech in the latent space.
$\mathcal{L}_{KL}$ regulates the distribution of latent variables, enabling natural speech synthesis and supporting the alignment-free structure.
These two loss functions are applied without modification during the training of the proposed model.
The total loss function is defined as follows:

$$
\mathcal{L} = \mathcal{L}_{adv} + \lambda_{sd} \mathcal{L}_{sd} + \lambda_{fm} \mathcal{L}_{fm} + \lambda_{recon} \mathcal{L}_{recon} + \lambda_{KL} \mathcal{L}_{KL} + \lambda_{dur} \mathcal{L}_{dur}
$$

The hyperparameters $\lambda_{sd}$, $\lambda_{fm}$, $\lambda_{recon}$, $\lambda_{KL}$, $\lambda_{dur}$ are all set to 1 in this study.
This decision was made because the magnitudes of the individual observed loss values were similar.
By setting these hyperparameters to 1, we ensure that each loss component contributes equally to the total loss without introducing arbitrary scaling factors, thereby facilitating a balanced optimization process.

## 4Â·Experiments: å®éªŒ

To validate the performance of ***RingFormer***, it is applied to the decoder of the widely used TTS model, [VITS [26]](../E2E/2021.06.11_VITS.md), and the quality of the synthesized speech is evaluated.
For comparison, the baseline vocoders used are [HiFi-GAN [2]](2020.10.12_HiFi-GAN.md), [iSTFTNet [15]](2022.03.04_iSTFTNet.md), and [BigVGAN [3]](2022.06.09_BigVGAN.md), which are state-of-the-art models known for achieving top performance in the field.
These models are also applied to VITS with the same architecture and hyperparameters to ensure a fair comparison under equal conditions.
Our code and audio samples are available on [GitHub [34]](https://github.com/seongho608/RingFormer).

### Datasets: æ•°æ®é›†

In this study, we trained and evaluated the model using the widely used [LJSpeech dataset [29]](../../Datasets/2017.07.05_LJSpeech.md).
The LJSpeech dataset consists of 13,100 high-quality speech samples, totaling approximately 24 hours of speech data.
Each sample is recorded at a sampling rate of 22,050 Hz and is commonly used in TTS research with English text.
In this study, 500 samples from the 13,100 LJSpeech samples were allocated to the validation set to assess the modelâ€™s performance, while 12,500 samples were used for training.
To further evaluate the model's performance, 30 samples were randomly selected from the remaining 100 samples for final testing.
This data configuration ensured consistent and reliable training and evaluation.

### Experimental Setup: å®éªŒè®¾ç½®

The proposed generator in this study employs two stages of upsampling.
Initially, the number of channels is set to 512, and at each stage, the number of channels is halved according to $2^i$, where $i$ denotes the upsampling step.
The Conformer block is configured by adjusting the input dimensions at each upsampling stage.
It utilizes 8 attention heads, a feed-forward network dimension that is half of the input dimension, 2 Conformer layers, a kernel size of 31, and a dropout rate of 0.1.
Key hyperparameters are summarized in Table 1.
The model training is conducted on an Ubuntu 20.04 LTS operating system, with Docker used for managing software dependencies.
The training process is carried out using a single Nvidia A100 GPU with 80GB of memory.

### Evaluation Metrics: è¯„ä»·æŒ‡æ ‡

In this study, the model's performance is evaluated from multiple perspectives using [mel-cepstral distortion (MCD) [30]](../../Evaluations/MCD.md), word error rate (WER), [short-time objective intelligibility (STOI) [31]](../../Evaluations/STOI.md), [NISQA [32]](../../Evaluations/2021.04.19_NISQA.md), mean opinion score (MOS), and comparison MOS (CMOS).

MCD measures the difference in mel-frequency cepstral coefficients between the synthesized and reference speech.
A lower MCD value indicates higher similarity between the two voices.
WER evaluates the accuracy of speech recognition by measuring the alignment of the recognized text with the original transcript.
A lower WER indicates fewer recognition errors.
STOI quantifies the intelligibility of the synthesized speech in relation to the reference, with values ranging from 0 to 1.
A value closer to 1 indicates higher intelligibility.
NISQA is a deep learning-based metric for assessing the quality and naturalness of speech by mimicking human auditory perception and quantifying the subjective quality of speech.

MOS is a subjective evaluation metric, where listeners rate the speech quality on a scale from 1 to 5.
However, in this study, we utilize the MOS prediction system from [UTMOS [33]](../../Evaluations/2022.04.05_UTMOS.md) for objective evaluation, which produces scores highly correlated with human ratings.
Finally, CMOS is used to compare the relative quality between two speech samples, allowing listeners to select the better-quality sample, thus performing a subjective comparison.
These diverse metrics enable a comprehensive evaluation of the modelâ€™s speech quality, intelligibility, and pronunciation accuracy.

## 5Â·Results: ç»“æœ

We report the performance of ***RingFormer*** and the baseline models evaluated on LJSpeech using the above objective and subjective metrics.
Table 2 presents the performance evaluation results of ***RingFormer*** and baseline vocoder models.

The proposed model demonstrates overall stable performance, achieving particularly strong results in MOS, which evaluates the naturalness and quality of speech, surpassing other models.
It also maintains consistent quality in the NISQA metric, confirming its ability to deliver reliable performance without introducing distortions to the speech signal.
While the proposed model slightly falls behind BigVGAN in MCD and STOI metrics, it achieves comparable performance, demonstrating competitiveness in terms of speech similarity and intelligibility.
In WER, which measures pronunciation accuracy, the proposed model performs on par with other models, confirming that the synthesized speech is clearly recognizable.
Additionally, in the subjective CMOS evaluation, the proposed model shows marginally better performance compared to BigVGAN and performs favorably against HiFi-GAN.
This suggests that the proposed model can provide high-quality audio in practical applications.
These results highlight that the proposed model generates more natural speech quality and clearer pronunciation compared to existing vocoders.

Table 3 presents the evaluation results for the number of parameters and inference speed of ***RingFormer*** and the comparison models.
The proposed model achieves high inference speed while maintaining a reasonable balance in terms of model size.
***RingFormer*** has 3.8 times fewer parameters than [BigVGAN [3]](2022.06.09_BigVGAN.md) and is approximately twice as fast in inference.
Considering the similar performance of both models, ***RingFormer*** can be regarded as sufficiently competitive.
On the other hand, ***RingFormer*** outperforms [HiFi-GAN [2]](2020.10.12_HiFi-GAN.md) and [iSTFTNet [15]](2022.03.04_iSTFTNet.md) in terms of performance but has 2.1 to 2.2 times more parameters with similar inference speed.
Furthermore, when ring attention is removed from the proposed model, the inference speed drops by up to 1.5 times, highlighting the significant role of ring attention in enabling fast speech generation.
These results demonstrate that our model generates high-quality speech while supporting real-time processing, further enhancing its practicality for various applications.

Table 4 presents the results of evaluating the impact of each component of ***RingFormer*** on the quality of synthesized speech.
The MOS of the complete ***RingFormer*** model, which includes all components, is 4.11.
When the magnitude loss ($\mathcal{L}_{mag}$) and phase loss ($\mathcal{L}_{arg}$) are removed, the MOS decreases to 4.05, suggesting that these loss terms contribute to capturing the detailed periodic and amplitude information of speech.
Removing the MS-SB-CQT discriminator also results in a decline in quality, indicating that continuous sequence discrimination positively contributes to synthesis quality.
When both components are removed, the MOS reaches its lowest value of 4.03, demonstrating that the combination of magnitude loss, phase loss, and the MS-SB-CQT discriminator is important for maximizing speech synthesis quality.

Additionally, to analyze ***RingFormer***'s performance in modeling long-term dependencies more precisely, experiments were conducted by reducing the size of the latent variable $z$ to 1/2 and 1/4, thereby reducing the receptive field.
The results showed a gradual decline in evaluation metrics as the receptive field decreased, confirming that a sufficient receptive field in ***RingFormer*** is crucial for learning long-term dependencies in speech and generating high-quality audio.

Table 5 evaluates the ability of the ***RingFormer*** model to capture long-term dependencies by analyzing the autocorrelation of the F0 contour.
The results show that the ***RingFormer*** model achieves the highest Pearson correlation coefficient with the ground truth, highlighting its strong performance in learning long-term dependencies.

Figure 4 visualizes this capability through an attention map, where bright patterns are maintained even in regions far from the diagonal, reflecting long-term dependencies.
Additionally, Figure 5 illustrates the attention scores for an arbitrary query.
As shown in the figure, the ***RingFormer*** model assigns notable attention scores even to temporally distant query-key pairs, indicating that it effectively utilizes long-term temporal information during audio generation.
The attention values are generally evenly distributed, supporting the role of the Conformer block in maintaining a stable and natural temporal structure in the synthesized audio.

## 6Â·Conclusions: ç»“è®º

In this paper, we propose ***RingFormer***, a vocoder that efficiently processes long sequences with long-term dependencies through a Conformer block with Ring Attention, while maintaining a reasonable memory usage to synthesize high-quality speech.
This structure captures both local and global dependencies in speech signals, enabling the generation of more natural-sounding speech.
Additionally, to improve generation speed, the output layer incorporates an inverse STFT structure, and by adding phase and magnitude losses to the loss function, it finely learns temporal patterns and amplitude information, thereby enhancing the quality of the synthesized speech.
For adversarial training, we introduce the recently released MS-SB-CQT discriminator, which improves the precision of speech synthesis by more accurately evaluating continuous sequences.
Through various objective metrics such as MCD, WER, STOI, and NISQA, as well as MOS and CMOS evaluations, we verify that ***RingFormer*** performs on par with or better than existing models, successfully achieving natural speech and clarity.
This study presents a model that balances fast speech synthesis speed and high quality, contributing to the advancement of speech synthesis technology.
Future research will aim to expand the applicability of ***RingFormer*** by optimizing it for multilingual datasets and various application environments.
