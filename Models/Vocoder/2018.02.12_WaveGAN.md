# WaveGAN

<details>
<summary>基本信息</summary>

- 标题: "Adversarial Audio Synthesis"
- 作者:
  - 01 Chris Donahue (Department of Music UC San Diego)
  - 02 Julian McAuley (Department of Music UC San Diego)
  - 03 Miller Puckette (Department of Music UC San Diego)
- 链接:
  - [ArXiv](https://arxiv.org/abs/1802.04208)
  - [Publication](https://openreview.net/forum?id=ByMVTsR5KQ) ICLR2019Poster
  - [Github](https://github.com/chrisdonahue/wavegan)
  - [Demo](https://chrisdonahue.com/wavegan_examples/)
- 文件:
  - [ArXiv](_PDF/1802.04208v3__WaveGAN__Adversarial_Audio_Synthesis.pdf)
  - [Publication](_PDF/1802.04208p0__WaveGAN__ICLR2019.pdf)

</details>

## Abstract: 摘要

Audio signals are sampled at high temporal resolutions, and learning to synthesize audio requires capturing structure across a range of timescales. Generative adversarial networks (GANs) have seen wide success at generating images that are both locally and globally coherent, but they have seen little application to audio generation. In this paper we introduce WaveGAN, a first attempt at applying GANs to unsupervised synthesis of raw-waveform audio. WaveGAN is capable of synthesizing one second slices of audio waveforms with global coherence, suitable for sound effect generation. Our experiments demonstrate that, without labels, WaveGAN learns to produce intelligible words when trained on a small-vocabulary speech dataset, and can also synthesize audio from other domains such as drums, bird vocalizations, and piano. We compare WaveGAN to a method which applies GANs designed for image generation on image-like audio feature representations, finding both approaches to be promising.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论