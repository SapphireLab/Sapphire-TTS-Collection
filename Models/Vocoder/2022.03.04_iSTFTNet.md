# iSTFTNet

<details>
<summary>基本信息</summary>

- 标题: "iSTFTNet: Fast and Lightweight Mel-Spectrogram Vocoder Incorporating Inverse Short-Time Fourier Transform"
- 作者:
  - 01 Takuhiro Kaneko,
  - 02 Kou Tanaka,
  - 03 Hirokazu Kameoka,
  - 04 Shogo Seki
- 链接:
  - [ArXiv](https://arxiv.org/abs/2203.02395)
  - [Publication](https://doi.org/10.1109/ICASSP43922.2022.9746713)
  - [Github]
  - [Demo](https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet/)
- 文件:
  - [ArXiv](_PDF/2203.02395v1__iSTFTNet__Fast_and_Lightweight_Mel-Spectrogram_Vocoder_Incorporating_Inverse_Short-Time_Fourier_Transform.pdf)
  - [Publication](_PDF/2203.02395p0__iSTFTNet__ICASSP2022.pdf)

</details>

## Abstract: 摘要

<table><tr><td width="50%">

In recent text-to-speech synthesis and voice conversion systems, a mel-spectrogram is commonly applied as an intermediate representation, and the necessity for a mel-spectrogram vocoder is increasing.
A mel-spectrogram vocoder must solve three inverse problems: recovery of the original-scale magnitude spectrogram, phase reconstruction, and frequency-to-time conversion.
A typical convolutional mel-spectrogram vocoder solves these problems jointly and implicitly using a convolutional neural network, including temporal upsampling layers, when directly calculating a raw waveform.
Such an approach allows skipping redundant processes during waveform synthesis (e.g., the direct reconstruction of high-dimensional original-scale spectrograms). By contrast, the approach solves all problems in a black box and cannot effectively employ the time-frequency structures existing in a mel-spectrogram.
We thus propose iSTFTNet, which replaces some output-side layers of the mel-spectrogram vocoder with the inverse short-time Fourier transform (iSTFT) after sufficiently reducing the frequency dimension using upsampling layers, reducing the computational cost from black-box modeling and avoiding redundant estimations of high-dimensional spectrograms.
During our experiments, we applied our ideas to three HiFi-GAN variants and made the models faster and more lightweight with a reasonable speech quality.
Audio samples are available at this https URL.

</td><td>

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td><td>

</td></tr></table>
