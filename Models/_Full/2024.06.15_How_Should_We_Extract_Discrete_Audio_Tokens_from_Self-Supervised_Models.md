# How Should We Extract Discrete Audio Tokens from Self-Supervised Models? <br> 我们应该如何从自监督模型中提取离散音频标识符?

<details>
<summary>基本信息</summary>

- 标题: How Should We Extract Discrete Audio Tokens from Self-Supervised Models?
- 作者:
  - 01 [Pooneh Mousavi](../../Authors/Pooneh_Mousavi.md)
  - 02 [Jarod Duret](../../Authors/Jarod_Duret.md)
  - 03 [Salah Zaiem](../../Authors/Salah_Zaiem.md)
  - 04 [Luca Della Libera](../../Authors/Luca_Della_Libera.md)
  - 05 [Artem Ploujnikov](../../Authors/Artem_Ploujnikov.md)
  - 06 [Cem Subakan](../../Authors/Cem_Subakan.md)
  - 07 [Mirco Ravanelli](../../Authors/Mirco_Ravanelli.md)
- 机构:
  - 机构 
- 时间:
  - 预印时间: 2024.06.15 ArXiv v1
  - 更新笔记: 2024.07.04
- 发表:
  - [InterSpeech 2024](../../Publications/InterSpeech.md)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.10735)
  - [DOI]()
  - [Github](https://github.com/speechbrain/benchmarks/tree/DASB)
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - [SSL](../../Tags/Learning_Self-Supervised.md)
  - [开源](../../Tags/OpenSource.md)
- 页数: 5
- 引用: 45
- 被引: ?
- 数据:
  - [LJSpeech](../../Datasets/2017.07.05_LJSpeech.md)
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. 
> Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details.
> Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). 
> Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear.
>
> This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks.
> We propose a scalable solution to train a universal vocoder across multiple SSL layers.
> Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.

## 1.Introduction: 引言

> Learning effective, efficient, and robust representations is a core problem in modern audio and speech processing systems.
> Over the past few years, continuous representations learned by large self-supervised models such as [Wav2Vec2](../Speech_Representaion/2020.06.20_Wav2Vec2.0.md), [WavLM](../Speech_Representaion/2021.10.26_WavLM.md), and [HuBERT](../Speech_Representaion/2021.06.14_HuBERT.md) have achieved unprecedented performance.
> A recent research trend consists of learning discrete audio representations instead of continuous ones, resulting in what is known as audio tokens.
> These discrete tokens offer several potential advantages.
> Firstly, they facilitate the development of audio language models (LMs) ([AudioLM](../Speech_LLM/2022.09.07_AudioLM.md); [AudioPaLM](../Speech_LLM/2023.06.22_AudioPaLM.md); [LauraGPT](../Speech_LLM/2023.10.07_LauraGPT.md); [VioLA](../Speech_LLM/2023.05.25_VioLA.md); [VALL-E](../Speech_LLM/2023.01.05_VALL-E.md); [SpeechX](../Speech_LLM/2023.08.14_SpeechX.md)) and the creation of multi-modal large language models ([Gemini](../LLM/Gemini.md)), which can emit audio, text, and visual tokens.
> Additionally, their compression potential can contribute to efficient data transmission and storage.
> Discrete tokens also enable us to address audio generation tasks such as speech enhancement and synthesis using classification methods, instead of relying on complex high-dimensional regression models.
>
> Following the terminology from ([AudioLM](../Speech_LLM/2022.09.07_AudioLM.md); [SpeechTokenizer](../Speech_Neural_Codec/2023.08.31_SpeechTokenizer.md)), audio tokenization techniques can be broadly categorized into Compression-based (codecs) tokens and Semantic tokens.
> Compression-based tokens ([SoundStream](../Speech_Neural_Codec/2021.07.07_SoundStream.md); defossez2022high, kumar2024high, [HiFi-Codec](../Speech_Neural_Codec/2023.05.04_HiFi-Codec.md)) utilize encoder-decoder architectures coupled with [Residual Vector Quantization (RVQ)](../Speech_Neural_Codec/2021.07.07_SoundStream.md).
> They are explicitly trained to accurately reconstruct the original audio, making them particularly suitable for audio generation tasks.
> Semantic tokens ([Speech Resynthesis](../_Full/Speech_Resynthesis_from_Discrete_Disentangled_Self-Supervised_Representations.md); [Phonetic Analysis](../_Full/Phonetic_Analysis_of_Self-Supervised_Representations_of_English_Speech.md); [W2V-BERT](../Speech_Representaion/2021.08.07_W2V-BERT.md)), on the other hand, are generated through clustering or quantization of the layers of Self-Supervised Learning (SSL) models ([Wav2Vec2](../Speech_Representaion/2020.06.20_Wav2Vec2.0.md); [WavLM](../Speech_Representaion/2021.10.26_WavLM.md); [HuBERT](../Speech_Representaion/2021.06.14_HuBERT.md)).
> Often, this involves selecting a layer from the pretrained SSL model and clustering its representations, typically with the k-means algorithm.
> Semantic tokens primarily capture coarse information such as phonetic, semantics, and syntactic details.
> Since they are not explicitly trained to achieve accurate waveform reconstruction, it is more natural to use them in discriminative tasks like Automatic Speech Recognition (ASR).
> Recent research, however, has shown that semantic tokens can be effective for generative tasks as well ([SELM](../_tmp/SELM.md); [Towards Universal Speech Discrete Tokens](../_Full/Towards_Universal_Speech_Discrete_Tokens__A_Case_Study_for_ASR_&_TTS.md)).
> Additionally, semantic tokens have been used in a hybrid tokenizer ([SpeechTokenizer](../Speech_Neural_Codec/2023.08.31_SpeechTokenizer.md); [FunCodec](../Speech_Neural_Codec/2023.09.14_FunCodec.md)).
> This hybrid approach combines semantic and compression-based tokens, separating content information in the initial layer while preserving paralinguistic details in subsequent layers.
> A similar strategy has been widely adopted in audio LLMs ([AudioLM](../Speech_LLM/2022.09.07_AudioLM.md); [AudioPaLM](../Speech_LLM/2023.06.22_AudioPaLM.md); [LauraGPT](../Speech_LLM/2023.10.07_LauraGPT.md)).
> Nevertheless, the most effective setting for extracting semantic tokens remains largely unclear.
> Recent studies have primarily focused on ASR and Speech Translation ([Chang2023Exploring](../_Full/Exploring_Speech_Recognition_Translation_&_Understanding_with_Discrete_Speech_Units__A_Comparative_Study.md); [DUB](../_tmp/DUB.md); [Chang2023Exploration](../_Full/Exploration_of_Efficient_E2E_ASR_Using_Discretized_Input_from_SSL.md)), without considering a broader range of discriminative and generative tasks.
>
> This paper addresses this gap by evaluating the effects of different heuristics required to derive semantic tokens for several discriminative and generative tasks, such as speech recognition, speaker recognition, emotion classification, speech enhancement, and text-to-speech.
> We investigate various crucial aspects, including the impact of the number of clusters and the selection of the intermediate layer of the SSL model to discretize.
> The latter factor turned out to be crucial and task-dependent, as early layers capture low-level information and higher layers encode content and semantic nuances.
> Common strategies include using the middle layer ([SELM](../_tmp/SELM.md); [Speech Resynthesis](../_Full/Speech_Resynthesis_from_Discrete_Disentangled_Self-Supervised_Representations.md)) or leveraging the last layer ([Chang2023Exploration](../_Full/Exploration_of_Efficient_E2E_ASR_Using_Discretized_Input_from_SSL.md)).
> Instead of relying on partial information only, we introduced a novel technique based on an informed layer selection mechanism.
> We propose to cluster all layers and inject their information into the acoustic models using learnable attention weights.
> This approach significantly boosts performance while also providing valuable insights into the importance of each layer.
>
> Since there is no built-in decoder in semantic tokens, a vocoder model for converting the semantic tokens into audio must be trained ([HiFi-GAN](../TTS3_Vocoder/2020.10.12_HiFi-GAN.md), [WaveGlow](../TTS3_Vocoder/2018.10.31_WaveGlow.md)).
> Training such a vocoder is computationally demanding, making it highly impractical to train a separate vocoder for each layer or combination of layers.
> To address this challenge, we propose a novel scalable vocoder capable of operating with various layer combinations at no additional cost.
> This is achieved through a layer dropout training scheme, inspired by the bitrate scalability mechanism used in [SoundStream](../Speech_Neural_Codec/2021.07.07_SoundStream.md).
> Interestingly, our results show that the scalable vocoder outperforms all vocoders trained on every specific layer.
> Finally, for a comprehensive comparison, we provide experimental evidence using both in-domain and out-of-domain datasets for training k-means.
> For reproducibility and to encourage further research, we release the code, built on the popular [SpeechBrain](../_tmp/SpeechBrain.md) toolkit, and pretrained models publicly at https://github.com/speechbrain/benchmarks/tree/DASB.

## 2.Related Works: 相关工作

None

## 3.Methodology: 方法

> The proposed architecture, illustrated in Figure.01, consists of four components:
> - Tokenizer
> - Informed Layer Selector
> - Acoustic Model
> - Scalable Vocoder
>
> The following subsections will describe each module.

![](Images/2024.06.15.Fig.01.png)

> Figure.01: The proposed method for audio token extraction from SSL models: 
> (A) k-means discretizes the continuous representations of each layer, 
> (B) an attention mechanism merges the discrete layer representations, 
> (C) the mixed representations train acoustic models for discriminative and generative tasks, 
> (D) our scalable vocoder generates waveforms (if needed).

### 3.1.Tokenizer

> For quantization, we cluster five layers taken from two pretrained SSL models using the k-means algorithm independently for each layer.
> We consider two widely-used models: WavLM-large ([Checkpoints](https://huggingface.co/microsoft/wavlm-large)) and HuBERT-large ([Checkpoints](https://huggingface.co/facebook/hubert-large-ll60k)), both having 24 layers.
> We choose two layers from the lower part (3, 7) to capture fine-grained information, the middle layer (12), and two layers from the higher part (18, 23) for encoding content and meaning.
> This selection is based on observation from prior research ([WavLM](../Speech_Representaion/2021.10.26_WavLM.md); [SUPERB](../../Evaluations/SUPERB.md)) which studied the contribution patterns of different layers across various tasks.
> As a result, this set of discrete hierarchical tokens captures rich information from the original audio signal.
> Each of the K clusters is assigned a unique index.
> Additionally, we store the continuous coordinates of each centroid for studying the effect of initializing input embeddings in downstream acoustic models.
> The outcome of this tokenization process is a tensor $\mathbf{d}$ of shape $B \times T \times n_l$, where B represents the batch size, T is the sequence length, and $n_l$ is the number of discretized layers.

### 3.2.Informed Layer Selector

> As evident from the SSL literature ([Zaiem2023Speech](../_Full/Speech_Self-Supervised_Representations_Benchmarking__Are_We_Doing_it_Right?.md); [SUPERB](../../Evaluations/SUPERB.md); [Zaiem2023FineTuning](Fine-Tuning_Strategies_for_Faster_Inference_Using_Speech_Self-Supervised_Models__A_Comparative_Study.md)), the choice of the layer within the SSL model significantly influences the performance of the downstream task of interest.
> This decision is equally critical for semantic tokens.
> Unlike prior methods that rely on heuristic layer selection ([SELM](../_tmp/SELM.md); [Speech Resynthesis](../_Full/Speech_Resynthesis_from_Discrete_Disentangled_Self-Supervised_Representations.md); [Chang2023Exploration](../_Full/Exploration_of_Efficient_E2E_ASR_Using_Discretized_Input_from_SSL.md)), we integrate the information from our hierarchical multi-layer audio tokens with an attention mechanism.
> The attention mechanisms comprise a straightforward multi-layer perceptron (MLP) fed by the embeddings of the audio tokens from each layer.
> The MLP generates a score for each selected layer, that is normalized by a softmax function as shown in the following equations:

$$
    z_{l,t} = f\big(emb(d_{l,t})\big) \\
$$

$$
    a_{l,t} = \frac{\exp(z_{l,t})}{\sum_{k=1}^{nl} \exp(z_{k,t})}, \quad h_t = \sum_l a_{l, t} z_{l, t},
$$

> where, $z_{l,t}$ represents the score assigned to layer $l$ at time $t$ by the MLP function $f$.
> The variable $emb$ refers to the lookup table that assigns embeddings to discrete tokens in $d_l$.
> The variable $a_{l,t}$ denotes the attention assigned to layer $l$ at time $t$, and lastly $h_t$ is the representation that is fed to the downstream MLP model.
> Note that we learn different layer combinations at each time-step, making this mechanism particularly effective.
>
> This simple yet effective approach offers several advantages.
> Firstly, it enhances flexibility by reducing reliance on heuristic layer selection.
> The model can now dynamically capture information from different layers for each task.
> Additionally, as shown in [Results](#5results-结果), this mechanism yields performance improvements when compared with models utilizing information from a single SSL layer.
> Lastly, the informed layer selections enhance interpretability, enabling us to analyze the learned weights and understand the relative importance of each layer for each downstream task.

### 3.3.Acoustic Model

> The mixed representations are fed to a neural model trained to address various downstream tasks.
> We train the attention mechanism, embeddings, and the acoustic models jointly.
> While previous studies ([Chang2023Exploring](../_Full/Exploring_Speech_Recognition_Translation_&_Understanding_with_Discrete_Speech_Units__A_Comparative_Study.md), [DUB](../_tmp/DUB.md), [Chang2023Exploration](../_Full/Exploration_of_Efficient_E2E_ASR_Using_Discretized_Input_from_SSL.md)) have primarily focused on a few discriminative tasks, we aim to provide evidence across a diverse range of speech applications, considering both discriminative and generative tasks.
> We consider ASR, speaker identification, and emotion recognition as discriminative tasks.
> For generative tasks, we focus on text-to-speech and speech enhancement.
> The details for each task are reported in [Experiments](#4experiments-实验).

### 3.4.Scalable Vocoder

> Although SSL models such as Wav2vec2, HuBERT, and WavLM are not designed for accurate waveform reconstruction, we can potentially adapt them for generative tasks by training a vocoder on top of their representations.
> The dominant approach involves training a separate vocoder for each possible layer combination.
> However, this approach is impractical and computationally demanding since each downstream task may require a different set of layers.
> In this work, we propose a universal and scalable vocoder capable of accommodating various layer combinations.
> To train such a model, we modify [HiFi-GAN](../Speech_Neural_Codec/2023.05.04_HiFi-Codec.md) to accept a variable number of multi-layer discrete tokens as input.
> We introduce a layer dropout mechanism, similar to structured [dropout](../_Basis/Dropout.md).
> For each input example, we randomly sample $k$ layers from the range $[1, n_l]$, as shown in the following equations:

$$
    \mathbf{d}_S \sim \text{Sample}(\mathbf{d}, k) , \quad \mathbf{o} = V(\mathbf{d}_S),\\
$$

> where `Sample($\cdot$)' randomly selects \( k \) layers from the discrete representations \( \mathbf{d} \), and \( V \) represents the vocoder function that outputs the waveform \( \mathbf{o} \).
> Layers are combined with an attention mechanism that assigns weights to different layers and ensures that the dimensionality of the embeddings remains consistent regardless of the number of layers. 
> The model is trained to decode audio by considering all possible combinations of layers.
> During inference, the desired combination of layers can be selected.
> In addition to its flexibility, this vocoder has demonstrated superior performance compared to vocoders trained on single layers, as we will show in [Results](#5results-结果).

## 4.Experiments: 实验

> The tasks in our experiments are divided into two groups: Discriminative tasks involving transcription and classification, and generative tasks producing audio.
> For the downstream architecture choices and training procedures, we follow the best-performing approaches for classic continuous self-supervised representations ([Zaiem2023Speech](../_Full/Speech_Self-Supervised_Representations_Benchmarking__Are_We_Doing_it_Right?.md)).
> We employ 1000 centroids across all tasks, except for ASR and emotion recognition, where we adopt 2000 centroids based on insights from prior research on ASR with discrete representations ([Chang2023Exploration](../_Full/Exploration_of_Efficient_E2E_ASR_Using_Discretized_Input_from_SSL.md)).
> The effect of this selection is probed in [Effect of Number of Clusters](#53effect-of-number-of-clusters).

### 4.1.Discriminative Tasks

#### Automatic Speech Recognition (ASR)

> We consider two CTC-based speech recognition tasks.
> The first one is English ASR using Librispeech train-clean-100 for training and test-clean, test-other for testing.
> The second one uses French data coming from the [CommonVoice (CV)](../../Datasets/CommonVoice.md) 16.1 Corpus.
> We select $100$ hours for training, keeping the original validation and test sets.
> We use two layers of BiLSTM as a downstream head.
> The evaluation metric is the Word Error Rate (WER).

#### Speaker Identification (SID)

> We employ an [ECAPA-TDNN](../_tmp/ECAPA-TDNN.md) model to determine the speaker identity of each utterance.
> The widely used [VoxCeleb](../../Datasets/2017.06.26_VoxCeleb.md) is adopted, and the evaluation metric is accuracy (ACC).

#### Emotion Recognition (ER)

> We use ECAPA-TDNN for emotion recognition on the [IEMOCAP](../../Datasets/IEMOCAP.md) dataset.
> The task consists of predicting one of the four considered classes: happy, sad, angry, and neutral.
> The evaluation metric is accuracy (ACC).

### 4.2.Generative Tasks

#### Speech Enhancement (SE)

> We utilize a non-autoregressive [Transformer](../_Transformer/2017.06.12_Transformer.md) encoder, which consists of 6 layers, 4 attention heads, a model dimension of 256, and a feed-forward layer dimension of 2048.
> Input tokens are extracted from the noisy signal, and target tokens from the clean one.
> Training is conducted end-to-end using cross-entropy loss. 
> Noisy samples are generated by mixing clean samples from [LJSpeech](../../Datasets/2017.07.05_LJSpeech.md) with noise from [WHAM!](../_tmp/WHAM!.md).
> The signal-to-noise ratios (SNRs) are uniformly distributed between 0 and 5 dB.
> Due to the misalignment of the vocoder's output with the target at the sample level, metrics like Si-SNR can be degraded.
> Therefore, we use the [deep noise suppression mean opinion score (DNSMOS)](../../Evaluations/DNSMOS.md) for the speech quality metric, following a previous study ([SELM](../_tmp/SELM.md)).
> Intelligibility is evaluated through the [differential word error rate (dWER)](../../Evaluations/dWER.md), which measures the WER between the transcribed enhanced signal and the transcribed target signal.
> Transcriptions are obtained using the small version of [Whisper](../Speech_LLM/2022.12.06_Whisper.md).

#### Text-to-Speech (TTS)

> We train an end-to-end autoregressive [Transformer](../_Transformer/2017.06.12_Transformer.md) with 6 layers in the encoder, 12 layers in the decoder, 4 attention heads, a model dimension of 512, and a feed-forward layer in 2048.
> To facilitate convergence, we employ [guided attention](../_tmp/Guided_Attention.md).
> The model takes text embeddings as its input and generates the audio tokens for each considered layer.
> We utilize a shared transformer decoder, where each tokenizer head has its own learned embedding, and there is a distinct final linear layer for each token.
> We train all models on the [LJSpeech](../../Datasets/2017.07.05_LJSpeech.md) dataset.
> For assessing speech quality, we use [UTMOS](../../Evaluations/UTMOS.md) to estimate human quality ratings.
> To evaluate fidelity to the text, we assess generated samples using the WER computed with the small version of [Whisper](../Speech_LLM/2022.12.06_Whisper.md).

## 5.Results: 结果

### 5.1.Scalable Vocoder

> Our results cover findings from two distinct setups: 1) a scalable vocoder trained across five layers, and 2) a vocoder trained on a single layer.
> In both setups, the tokenizers and the vocoders are trained with LJSpeech (in-domain condition).
> In the first scenario, models are trained with HuBERT discrete tokens and WavLM discrete tokens, each with the number of clusters set to 1000.
> To further explore the influence of k-means cluster size on speech quality, we introduce an additional model with the number of clusters set to 2000.
> In the second setup, we focus on models trained specifically on a single layer (3 or 23) using HuBERT discrete tokens and in-domain tokenizer.
> This experiment aims to compare the performance of the scalable vocoder against the vocoder trained on a single layer.
>
> The results are summarized in Figure.02, WavLM combined with an in-domain tokenizer achieves higher UTMOS and lower dWER scores across all setups.
> About the impact of the number of clusters, our experiment shows that setting $k$ to 2000 degrades the quality of synthesized speech.
> Finally, both models trained on a single layer are outperformed on both evaluation metrics by the one trained on five layers, confirming the benefits of the scalable approach.
> Lastly, we explore an out-of-domain scenario where the tokenizers are trained on LibriSpeech and the vocoders are on LJSpeech.
> As shown in Table.02 (last column), we do not observe any significant performance degradation when using the scalable vocoder in an out-of-domain condition.

### 5.2.Layer Analysis

> Figure.03 depicts the average weights assigned to different layers in the WavLM model across various downstream tasks on the test dataset. In both TTS and the scalable vocoder, lower levels get greater importance as they prioritize effective reconstruction.
> Conversely, for ASR, the upper layers become more crucial in capturing the semantic aspects of spoken utterances.
> In the case of ER and SID, the third layer receives the highest weight.
> Our findings align with the observed pattern in continuous representations (["Speech Self-Supervised Representations Benchmarking"](../_Full/2023.08.28_Speech_Self-Supervised_Representations_Benchmarking__A_Case_for_Larger_Probing_Heads.md)).
> For SE, all layers are equally weighted, indicating the necessity of all hierarchical levels to achieve optimal audio quality while preserving the semantic content of the input. 

### 5.3.Effect of Number of Clusters

> We train k-means models with both 1000 and 2000 centroids and examine the impact of the number of clusters across different tasks, as illustrated in Table.01. 
> In Generative tasks, TTS and SE, no significant differences are observed between models trained with 1000 and 2000 clusters.
> However, for ASR in both English and French, as well as ER, models with a higher number of clusters outperform those with fewer clusters.
> In the case of SID, the model trained with 1000 clusters exhibits comparable accuracy to the model with 2000 centroids.
> As expected, the ideal number of clusters is task-dependent.
> For multi-modal LLMs where a single set of tokens is desired to solve multiple tasks, we recommend a cluster count between 1000 and 2000.

### 5.4.Effect of Embedding Initialization

> We study various configurations for initializing the embedding layers of audio tokens (Table.01).
> Three options are considered:
> - Random initialization of the embedding layers,
> - Initialization of the embedding layer with the corresponding centroid's embedding, while freezing the layer, and 
> - Initialization of the embedding layer with the corresponding centroid's embedding, without freezing the layer.
> 
> Across all tasks, there is no advantage observed in initializing the embedding with pretrained centroid embeddings, and random initialization consistently outperforms it in all scenarios.
> However, discriminative tasks show greater benefits from random initialization, while generative tasks exhibit comparable performance across all three settings.
> This observation eliminates the need for having the same embedding size as the SSL models, allowing the choice of a smaller and more efficient embeddings. %In all tasks, except for ER, achieving better or comparable performance is possible when fine-tuning the pretrained embeddings rather than freezing them. 

### 5.5.Out-of-Distribution Generation

> To evaluate the robustness of discrete representations under distribution shifts, we train tokenizers on both in-domain and out-of-domain datasets (Table.02).
> In discriminative tasks, k-means models are trained using the same dataset employed for training acoustic models.
> In the out-of-domain scenarios, k-means models are trained on train-clean-100, train-clean-360, and train-other-500.
> For generative tasks, k-means models are trained on LJSpeech for in-domain evaluation and LibriSpeech-960h for OOD evaluation, while both the acoustic model and vocoder are trained on LJSpeech.
> For all discriminative tasks, the in-domain tokenizer outperforms its OOD counterpart.
> Interestingly, in all generative tasks, training the model using the OOD tokenizer does not adversely affect performance and, in some instances, even improves the results.
> We speculate that this trend may arise because generative tasks primarily depend on tokens capturing low-level information, which tends to be more ``universal" and transferable across different domains.

## 6.Conclusions: 结论

> Discrete semantic tokens, derived from the quantization of SSL models, play an important role, providing "pseudo-text" valuable for training text-free speech language models and multimodal LLMs.
> We explore the optimal configuration of semantic tokens across discriminative and generative tasks.
> We introduce a novel technique involving an informed layer selection mechanism, utilizing learnable attention weights to integrate information from different SSL layers.
> This approach significantly enhances the performance and interpretability of the model.
> Furthermore, we propose a scalable solution for training a universal vocoder across multiple SSL layers, demonstrating its superiority over vocoders trained on specific layers.
> As future work, we plan to explore more diverse tasks and quantization methods, and the development of a multi-speaker vocoder.
