# HALL-E

<details>
<summary>基本信息</summary>

- 标题: "HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis"
- 作者:
  - 01 Yuto Nishimura,
  - 02 Takumi Hirose,
  - 03 Masanari Ohi,
  - 04 Hideki Nakayama,
  - 05 Nakamasa Inoue
- 链接:
  - [ArXiv](https://arxiv.org/abs/2410.04380)
  - [Publication]
  - [Github]
  - [Demo](https://yutonishimura-v2.github.io/HALL-E_DEMO/)
- 文件:
  - [ArXiv](_PDF/2410.04380v1__HALL-E__Hierarchical_Neural_Codec_Language_Model_for_Minute-Long_Zero-Shot_Text-to-Speech_Synthesis.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Recently, Text-to-speech (TTS) models based on large language models (LLMs) that translate natural language text into sequences of discrete audio tokens have gained great research attention, with advances in neural audio codec (NAC) models using residual vector quantization (RVQ).
However, long-form speech synthesis remains a significant challenge due to the high frame rate, which increases the length of audio tokens and makes it difficult for autoregressive language models to generate audio tokens for even a minute of speech.
To address this challenge, this paper introduces two novel post-training approaches: 1) Multi-Resolution Requantization (MReQ) and 2) HALL-E.
MReQ is a framework to reduce the frame rate of pre-trained NAC models.
Specifically, it incorporates multi-resolution residual vector quantization (MRVQ) module that hierarchically reorganizes discrete audio tokens through teacher-student distillation.
HALL-E is an LLM-based TTS model designed to predict hierarchical tokens of MReQ.
Specifically, it incorporates the technique of using MRVQ sub-modules and continues training from a pre-trained LLM-based TTS model.
Furthermore, to promote TTS research, we create MinutesSpeech, a new benchmark dataset consisting of 40k hours of filtered speech data for training and evaluating speech synthesis ranging from 3s up to 180s.
In experiments, we demonstrated the effectiveness of our approaches by applying our post-training framework to VALL-E.
We achieved the frame rate down to as low as 8 Hz, enabling the stable minitue-long speech synthesis in a single inference step.
Audio samples, dataset, codes and pre-trained models are available at [this https URL](https://yutonishimura-v2.github.io/HALL-E_DEMO/).

</details>
<br>

近期, 随着使用残差向量量化的神经音频编解码器模型的进步, 将自然语言文本转换为离散音频标记序列的基于大语言模型的文本转语音模型引起了广泛的研究关注.
然而, 长语音合成仍然是一个重要的挑战, 因为高帧率会增加音频标记的长度, 使得自回归语言模型甚至很难为一分钟的语音生成音频标记.
为了应对这一挑战, 本文引入了两种新颖的后训练方法: 1. 多分辨率重量化 (MReQ) 和 2. HALL-E.

- MReQ 是用于减少预先训练的 NAC 模型帧率的框架. 具体来说, 它采用多分辨率残差向量量化 (MRVQ) 模块, 通过教师-学生蒸馏的方式重新组织离散音频标记.
- HALL-E 是基于 LLM 的 TTS 模型, 旨在预测 MReQ 的层次化标记. 具体来说, 它采用 MRVQ 子模块, 并从预训练的 LLM-based TTS 模型继续训练.

为了促进 TTS 研究, 我们创建了 MinutesSpeech 新的数据集, 其中包含 40k 小时的过滤语音数据, 用于训练和评估语音合成, 范围从 3s 到 180s.
在实验中, 我们展示了我们的方法的有效性, 通过将我们的后训练框架应用于 VALL-E.
我们实现了帧率低至 8 Hz 的稳定分钟长语音合成, 使得单次推断可以实现分钟长度的语音合成.
音频样本、数据集、代码和预训练模型可在 [此 https URL](https://yutonishimura-v2.github.io/HALL-E_DEMO/) 获得.

## 1.Introduction: 引言

Recent advances in large language models (LLMs) have enabled us to model complex linguistic structures and patterns with unprecedented precision in natural language processing tasks~\citep{brown2020language, chowdhery2023palm, zeng2023glm130b}.
Motivated by these developments, LLM-based text-to-speech (TTS) models have gained significant research interest for their ability to model complex speech structures and patterns, enabling the synthesis of more natural-sounding speech in a zero-shot manner~\citep{wang2023valle, zhang2023vallex, song2024ella, han2024valler, chen2024valle2, xin2024ralle, meng2024melle}.
The core concept behind LLM-based TTS models is to translate natural language text into a sequence of audio tokens, typically using frozen natural audio codec (NAC) models that quantize audio signals into discrete tokens via vector quantization techniques~\citep{zeghidour2021soundstream, defossez2022encodec, kumar2024DAC, Zhang2024SpeechTokenizer, wu2023audiodec, huang2023repcodec, du2024funcodec, yang2023hificodec}.

Since LLMs are becoming capable of capturing long context in text~\citep{guo2022longt5, chen2024longlora, han2024hyperattention}, LLM-based TTS models are also expected to handle long context to synthesize speech over extended periods.
However, this presents significant challenges, mainly because the length of audio tokens per second is typically large.
More specifically, when using an autoregressive language model to predict audio tokens, as in the VALL-E architecture~\citep{wang2023valle}, the high frame rate at the first layer of residual vector quantization (RVQ)~\citep{zeghidour2021soundstream} in NAC models often becomes a major factor that hinders the synthesis of long speech.
We refer to the number of audio tokens per second as the frame rate (Hz).
As recently discussed and investigated by~\cite{han2024valler, chen2024valle2}, reducing the frame rate is essential, but not straightforward. Simply reducing the frame rate in NAC models results in degraded audio quality and incorrect phoneme pronunciation.
Addressing this issue requires novel solutions that bridge NAC models and LLM-based TTS models.

In this paper, we propose a novel approach to tackle the challenge of minitue-long speech synthesis in LLM-based TTS models by introducing a hierarchical post-training framework that effectively manages the trade-off between reducing frame rate and producing high-quality speech.
Our contributions are summarized as follows:
1. We propose \textbf{Multi-Resolution Requantization (MReQ)}, a post-training framework for hierarchically reorganizing pre-trained RVQ module to reduce the frame rate at lower quantization layers. MReQ incorporates a multi-resolution residual vector quantization (MRVQ) module into a pre-trained NAC model and continues training in a teacher-student distillation manner.
This results in reducing the frame rate at the first quantization layer to 8 Hz.
2. We propose \textbf{HALL-E}, a hierarchical LLM-based TTS model designed to predict hierarchical tokens of MReQ.
{The AR model is trained using 8Hz tokens, while the NAR model is trained by using sub-modules in MRVQ, and continues training from a pre-trained LLM-based TTS model.}
3. We introduce \textbf{MinutesSpeech}, a new benchmark dataset to promote TTS research, particularly for minute-long speech synthesis.
The training set consists of 40k hours of automatically filtered and balanced speech data.
The test set consists of 8 hours of speech data with transcriptions created by professional transcribers.
4. We thoroughly conducted experiments to provide best practices for managing the trade-off between reducing frame rate and producing high-quality speech, while demonstrating the effectiveness and efficiency of our approach.
We open-source dataset, codes and pre-trained models along with audio samples at [Demo Page](https://yutonishimura-v2.github.io/HALL-E_DEMO/).

## 2.Related Works: 相关工作

### Neural Audio Codec Models

NAC models produce discrete audio tokens by quantizing audio signals.
SoundStream~\citep{zeghidour2021soundstream} and Encodec~\citep{defossez2022encodec} are pioneering NAC models, which significantly improved compression efficiency over traditional audio codecs.
Recent studies have proposed NAC models with a focus on maintaining performance in speech processing tasks. Examples include
SpeechTokenizer~\citep{Zhang2024SpeechTokenizer},
Descript Audio Code~\citep{kumar2024DAC},
AcademiCodec~\citep{yang2023hificodec},
AudioDec~\citep{wu2023audiodec},
RepCodec~\citep{huang2023repcodec}, and FunCodec~\citep{du2024funcodec}.
Many studies have focused on reducing bps, while few have explored lowering the frame rate. \cite{defossezmoshi} achieved the lowest frame rate for a NAC model, reaching 12.5Hz by incorporating Transformer models and SpeechTokenizer.
We report achieving an even lower frame rate of 8Hz, which is about 1.5 times shorter than it.

### Zero-Shot TTS

Zero-Shot TTS aims to synthesize speech from text in the voice of a target speaker using only a short reference audio segment from that speaker.
Early models relied on speaker embeddings or speaker adaptation~\citep{casanova22yourtts, arik2018neuralvoicecloning, chen2019sample},
while recent studies have focused on LLM-based models that use NAC models in conjunction with LLMs.
%~\citep{wang2023valle, zhang2023vallex, song2024ella, xin2024rall, chen2024valle2, meng2024melle}.
VALL-E~\citep{wang2023valle} was the first LLM-based model, demonstrating impressive capabilities in zero-shot TTS tasks.
Follow-up studies have explored various extensions such as VALL-E~X for cross-lingual TTS~\citep{zhang2023vallex}, ELLA-V using Montreal forced aligner~\citep{song2024ella},
RALL-E using prosody features~\citep{xin2024ralle}, VALL-E R using monotonic alignment~\citep{han2024valler}, VALL-E 2 using grouped code modeling~\citep{chen2024valle2}, and MELLE using mel-spectrogram features~\citep{meng2024melle}.
Prosody information can also be modeled by LLMs
latent language mode
Mega-TTS~\citep{jiang2023mega} and Mega-TTS 2~\citep{jiang2024megatts2} introduced prosody LLMs to generate more natural prosody.
Meanwhile, diffusion-based models ({\it e.g.,} NaturalSpeech2/3~\citep{shen2023naturalspeech2, ju2024naturalspeech3} and Voicebox~\citep{le2024voicebox}) and prompt-based models ({\it e.g.,} Prompt-TTS2~\citep{guo2023prompttts, leng2024prompttts2}) are also known to be effective to generate high-quality controllable speech.
Beyond speech synthesis, several studies proposed audio generation models such as
UniAudio~\citep{yang2023uniaudio},
Audiobox~\citep{vyas2023audiobox}.
In contrast, this work explores post-training methods to reduce the frame rate of LLM-based models, aiming at minute-long speech synthesis given a pre-trained NAC model such as Encodec.

### Preliminaries

#### Neural Audio Codec

A NAC model typically consists of three components: an encoder $\mathrm{Enc}(\cdot)$, a vector quantizer $\mathrm{VQ}(\cdot)$, and a decoder $\mathrm{Dec}(\cdot)$.
The quantizer is the core component that produces discrete audio tokens.
This work assumes that an RVQ module~\citep{zeghidour2021soundstream} is used as the quantizer, which is defined as follows:

$$
\bm{z}_{l} = \mathrm{VQ}_{l} (\bm{x}_{l - 1}),\quad \bm{x}_{l+1} = \bm{x}_{l} - \tilde{\bm{z}}_{l},\quad \bm{h} = \sum_{l=1}^{L} \tilde{\bm{z}}_{l},
$$

where $\bm{x}_{0} = \mathrm{Enc}(\bm{x}_{\mathrm{in}}) \in \mathbb{R}^{d \times n}$ is the encoder output for the input audio $\bm{x}_{\mathrm{in}}$,
$d$ is the latent dimension,
$n$ is the sequence length,
$\mathrm{VQ}_{l}$ is a vector quantizer,
$\bm{z}_{l} \in \mathbb{N}^{n}$ is a discrete token sequence,
$\tilde{\bm{z}}_{l} = \mathrm{Emb}_{l}(\bm{z}_{l}) \in \mathbb{R}^{d \times n}$ is a sequence of embeddings corresponding to $\bm{z}_{l}$ obtained through a learnable embedding layer $\mathrm{Emb}_{l}(\cdot)$\footnote{In this paper, the tilde symbol (~$\tilde{}$~) denotes the embeddings corresponding to a token sequence.},
$l \in \{1, 2, \cdots, L\}$ is the layer index, and $L$ is the number of layers. The output $\bm{h} \in \mathbb{R}^{d \times n}$ is then fed into the decoder to reconstruct the input audio as $\bm{y} = \mathrm{Dec}(\bm{h})$.

#### LLM-based TTS.

An LLM-based TTS typically consists of two decoder-only language models: an autoregressive (AR) model $T_{\mathrm{ar}}$ and a non-autoregressive (NAR) model $T_{\mathrm{nar}}$ \citep{wang2023valle}.
The speech synthesis procedure is given by the following equations:

$$
\hat{\bm{z}}_{1} = T_{\mathrm{ar}}(\bm{t}, \bm{z}_{1}^{prompt}),\quad \hat{\bm{z}}_{l+1} = T_{\mathrm{nar}} (\bm{t}, \bm{h}^{prompt}_{L}, \hat{\bm{h}}_{l}, l),\\
\hat{\bm{h}}_{l} = \sum_{l^{\prime}=1}^{l} \hat{\tilde{\bm{z}}}_{l^{\prime}},\quad \hat{\bm{y}} = \mathrm{Dec} ([\bm{h}^{prompt}_{L}, \hat{\bm{h}}_{L}]),
$$

where $[\bm{h}^{prompt}_{L}, \hat{\bm{h}}_{L}]$ denotes the concatenation of these two matrices along the time axis.
In Eq.~(\ref{eq:valle1}), $T_{\mathrm{ar}}$ generates an audio token sequence $\hat{\bm{z}}_{1} \in \mathbb{N}^{n^{\prime}}$ corresponding to the first layer of RVQ given two inputs: a text prompt $\bm{t}$ and {an audio prompt} $\bm{z}^{prompt}_{1} = \mathrm{VQ}_{1} (\mathrm{Enc}(\bm{x}_{prompt}))$ {extracted from an audio input} $\bm{x}_{prompt}$.
In Eq.~(\ref{eq:valle2}), $T_{\mathrm{nar}}$ iteratively generates token sequences $\hat{\bm{z}}_{l+1} \in \mathbb{N}^{n^{\prime}}$ from the accumulated hidden features $\hat{\bm{h}}_{l}$ in Eq.~(\ref{eq:valle3}) and the audio prompt's hidden features $\bm{h}^{prompt}_{L}$.
Finally, in Eq.~(\ref{eq:valle4}), speech $\hat{\bm{y}}$ is generated.
Note that $\mathrm{Enc}, \mathrm{VQ}_{1}$, and $\mathrm{Dec}$ are from a frozen NAC model.

#### Preliminary Experiments

LLM-based TTS models have a predefined context window size and are typically trained with speech data ranging from several seconds to several tens of seconds. To generate long speech segments, a straightforward approach is to reduce the frame rate in the NAC model.
However, reducing the frame rate below 48 Hz significantly
decreases speech reconstruction performance as shown in
Figure~\ref{fig:preliminary}, where we evaluated the performance of Encodec~\citep{defossez2022encodec} in terms of the Perceptual Evaluation of Speech Quality (PESQ) scores and word error rates (WERs) as functions of frame rates.
Specifically, it is confirmed that training becomes entirely difficult at 8Hz.
Therefore, in this study, we propose a NAC model that works even at an 8Hz, demonstrating a significant improvement over existing limitations.

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

We introduced two novel approaches for minute-long zero-shot text-to-speech synthesis: MReQ and HALL-E.
MReQ reduced the frame rate of the Encodec model to 8 Hz by reorganizing audio tokens via MRVQ.
HALL-E efficiently synthesized minute-long speech by using 8Hz tokens in the AR model and MRVQ sub-modules in the NAR model.
We demonstrated the effectiveness of these approaches on MinutesSpeech, the newly introduced dataset consisting of 40,000 hours of speech data.
Our work contributed to promote zero-shot TTS research.

### Limitations and Future Work

By reducing the frame rate to 8 Hz, our AR model can utilize longer contexts, enhancing the naturalness of the synthesized speech. We believe that to handle extended context is particularly advantageous for larger AR models such as AudioLM~\citep{borsos2023audiolm}, SpeechGPT~\citep{zhang2023speechgpt}, and PSLM~\citep{mitsui2024pslm}. Demonstrating the effectiveness of our approach not only in TTS but also with these models remains a future work.
Furthermore, as shown in Table~\ref{tab:dataset}, we have achieved shorter audio token length than  the corresponding text token length. However, in our current AR model, we concatenate these tokens, which results in the text tokens becoming a bottleneck in terms of sequence length. Small-E~\citep{lemerle2024small} propose methods to mitigate this issue by processing each token individually and fusing them using cross-attention. Exploring such architectural enhancements is an important direction for future work.
Lastly, as shown in Table~\ref{tab:nacmodels}, our method brought significant improvements with SpeechTokenizer, even more so than when applied to Encodec. It means that our approach can further enhance the objective of preserving linguistic information. This indicates that our method could serve as a replacement for traditional SSL models~\citep{hsu2021hubert, chen2022wavlm} or ASR encoders~\citep{lakhotia2021generative, chu2023qwen}, marking an important direction for future research.
