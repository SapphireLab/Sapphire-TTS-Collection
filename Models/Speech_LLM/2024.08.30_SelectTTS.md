# SelectTTS

<details>
<summary>基本信息</summary>

- 标题: SelectTTS: Synthesizing Anyone's Voice via Discrete Unit-Based Frame Selection
- 作者:
  - 01 [Ismail Rasim Ulgen](../../Authors/Ismail_Rasim_Ulgen.md)
  - 02 [Shreeram Suresh Chandra](../../Authors/Shreeram_Suresh_Chandra.md)
  - 03 [Junchen Lu](../../Authors/Junchen_Lu.md)
  - 04 [Berrak Sisman](../../Authors/Berrak_Sisman.md)
- 机构:
  - 机构 
- 时间:
  - 预印时间: 2024.08.30 ArXiv v1
  - 更新笔记: 2024.09.02
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2408.17432)
  - [DOI]()
  - [Github]()
  - [Demo](https://kodhandarama.github.io/selectTTSdemo/)
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: ?
- 引用: ?
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Synthesizing the voices of unseen speakers is a persisting challenge in multi-speaker text-to-speech (TTS). 
> Most multi-speaker TTS models rely on modeling speaker characteristics through speaker conditioning during training. 
> Modeling unseen speaker attributes through this approach has necessitated an increase in model complexity, which makes it challenging to reproduce results and improve upon them. 
> We design a simple alternative to this. 
> We propose SelectTTS, a novel method to select the appropriate frames from the target speaker and decode using frame-level self-supervised learning (SSL) features. 
> We show that this approach can effectively capture speaker characteristics for unseen speakers, and achieves comparable results to other multi-speaker TTS frameworks in both objective and subjective metrics. 
> With SelectTTS, we show that frame selection from the target speaker's speech is a direct way to achieve generalization in unseen speakers with low model complexity. 
> We achieve better speaker similarity performance than SOTA baselines XTTS-v2 and VALL-E with over an 8x reduction in model parameters and a 270x reduction in training data.

## 1.Introduction: 引言

> Recent text-to-speech approaches  \cite{ le2024voicebox,wang2023neural, casanova2024xtts} have demonstrated that given sufficient data and large enough model capacity, TTS models are capable of producing speech of remarkably high quality and naturalness. While the development of large-scale TTS models \cite{lajszczak2024base} offers its benefits, it also introduces challenges associated with model reproducibility. %increasing model complexity comes with its caveats as the model behavior becomes unexplainable. 
> This issue becomes especially relevant in multi-speaker TTS frameworks.
>
> Multi-speaker TTS for unseen speakers is a challenging problem as the objective is multifold - learning text-to-speech semantic prediction while simultaneously capturing the speaker timbre and acoustics\cite{chen20r_interspeech}. Previously, multi-speaker TTS  predominantly relied on speaker labels and embeddings \cite{jia2018transfer, casanova2022yourtts} to condition models to learn speaker traits \cite{gibiansky2017deep, cooper2020zero}.
> Speaker embeddings were initially created for tasks such as speaker recognition \cite{8462665}, but applying these embeddings to model unseen speaker characteristics in synthesis has proven to be an uphill task \cite{wang2024usat}.
>
> More recently,  zero-shot TTS frameworks have started to exploit neural codec language modeling \cite{wang2023neural, xin2024rall}. They replace traditional mel-spectrograms with audio codec codes and use in-context learning capability to enable prompt-based zero-shot TTS.
>
> Both the embedding and language modeling approach rely on speaker conditioning, either in the form of speaker embeddings or acoustic prompts, requiring the network to learn the modeling of speaker timbre given the condition. While this has led to state-of-the-art (SOTA) performance \cite{ peng2024voicecraft,chen2024vall, wang2024speechx}, with the cost of increased model capacity and complexity, as well as data requirements \cite{ peng2024voicecraft, du2024cosyvoice}, hindering reproducibility and further development of those methods.
> With SelectTTS, we propose a straightforward and effective alternative to conditional learning of speaker traits where speaker modeling is performed by non-parametric frame selection of SSL features. This approach allows training a more parameter efficient and less complex model without huge data requirements, while still achieving performance comparable to larger models in reproducing speaker timbre for multi-speaker TTS.
>
> This is enabled by the advancements in self-supervised learning speech models \cite{baevski2020wav2vec,chen2022wavlm}. These SSL features show tremendous potential in capturing the linguistic, speaker, and prosody information \cite{pasad2021layer}. Their masked prediction objective \cite{hsu2021hubert} ensures that they possess strong semantic information while also being able to capture speaker acoustic information, which is necessary for multi-speaker TTS.  A recent work, kNN-VC \cite{baas23_interspeech}, has directly leveraged this capability of SSL features in unit selection-based voice conversion \cite{9262021} by replacing each frame-level feature in the source utterance with the closest neighbours in the reference speech to construct the target feature sequence. It demonstrates that selecting SSL features on a frame-by-frame basis from the reference speech can synthesize voices with a high degree of similarity to the target speaker.
> Persuaded by the idea of selecting frames in the SSL feature space, we develop frame selection algorithms to leverage the generalization ability of SSL for synthesizing voices of unseen speakers through multi-speaker TTS. 
>
> Speech Samples: https://kodhandarama.github.io/selectTTSdemo/
> Codes and pre-trained models will be released upon acceptance.
>
> With SelectTTS, we introduce a new paradigm of frame selection-based multi-speaker TTS that directly utilizes frames from unseen target speakers to clone their voice. Rather than conditioning input text with speaker representations or using speech tokens as prompts to model speaker timbre, our method splits the task into two stages.
> In the first stage, we predict frame-level semantic units from text to model speech semantic content. In the second stage, we obtain intermediate representations with the necessary speaker and acoustic information by selecting frame-level features from the reference speech according to the predicted semantic units.
> Combining frame selection with rich SSL features achieves SOTA speaker similarity, as it uses the target speaker's frames directly. 
> This framework, with its multi-stage structure and non-parametric approach, offers reduced model complexity and greater customization compared to larger baselines.
>
> The main contributions of the paper are summarized as:
> 1) We propose a multi-speaker text-to-speech strategy that completely separates and simplifies the tasks of semantic prediction and speaker modeling in TTS, making the overall framework easily reproducible and open to further development; 
> 2) We introduce novel frame selection algorithms, sub-sequence matching and inverse k-means sampling, that directly select frames from the target speaker to accurately reproduce speaker timbre; 
> 3) We utilize the discrete SSL features for text-to-semantic modeling and frame selection; the continuous SSL features for vocoding, showing the benefits of leveraging both feature spaces.

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

> In this paper, we propose SelectTTS, a multi-speaker TTS framework with lower model complexity that directly utilizes frames from the unseen target speaker to synthesize high-quality speech that closely resembles the target speaker's voice. We demonstrate that our approach of combining frame selection based on semantic units with vocoding using a sequence of SSL features offers a much simpler yet effective method for modeling unseen speakers in the multi-speaker TTS task attaining SOTA target speaker similarity. 
