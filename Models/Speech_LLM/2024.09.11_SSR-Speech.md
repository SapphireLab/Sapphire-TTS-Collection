# SSR-Speech

<details>
<summary>基本信息</summary>

- 标题: SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech Editing and Synthesis
- 作者:
  1. Helin Wang (Johns Hopkins University)
  2. Meng Yu (Tencent AI Lab)
  3. Jiarui Hai (Johns Hopkins University)
  4. Chen Chen (Nanyang Technological University)
  5. Yuchen Hu (Nanyang Technological University)
  6. Rilin Chen (Tencent AI Lab)
  7. Najim Dehak (Johns Hopkins University)
  8. Dong Yu (Tencent AI Lab)
- 机构:
  1. Johns Hopkins University: 03/08
  2. Tencent AI Lab: 03/08
  3. Nanyang Technological University: 02/08
- 时间:
  - 预印时间: 2024.09.11 ArXiv v1
  - 更新笔记: 2024.09.13
- 发表:
  - Submitted to ICASSP2025
- 链接:
  - [ArXiv](https://arxiv.org/abs/2409.07556)
  - [DOI]()
  - [Github](https://github.com/WangHelin1997/SSR-Speech)
  - [Demo](https://WangHelin1997.github.io/SSR-Speech-Demo)
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: 5
- 引用: 35
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> In this paper, we introduce SSR-Speech, a neural codec autoregressive model designed for stable, safe, and robust zero-shot text-based speech editing and text-to-speech synthesis. 
> SSR-Speech is built on a Transformer decoder and incorporates classifier-free guidance to enhance the stability of the generation process. 
> A watermark Encodec is proposed to embed frame-level watermarks into the edited regions of the speech so that which parts were edited can be detected. 
> In addition, the waveform reconstruction leverages the original unedited speech segments, providing superior recovery compared to the Encodec model. 
> Our approach achieves the state-of-the-art performance in the RealEdit speech editing task and the LibriTTS text-to-speech task, surpassing previous methods. Furthermore, SSR-Speech excels in multi-span speech editing and also demonstrates remarkable robustness to background sounds. 
> [Source code](https://github.com/WangHelin1997/SSR-Speech) and [demos](https://wanghelin1997.github.io/SSR-Speech-Demo/) are released.

## 1.Introduction: 引言

> Nowadays, zero-shot text-based speech generation \cite{DBLP:conf/icassp/CooperLYFWCY20,DBLP:conf/icml/CasanovaWSJGP22,DBLP:conf/icml/BaiZCML022,DBLP:journals/corr/abs-2406-00654} has garnered significant attention in the speech community, particularly in areas like speech editing (SE) and text-to-speech (TTS) synthesis. Given an unseen speaker during training, zero-shot SE focuses on modifying specific words or phrases within an utterance to align with a target transcript while preserving the unchanged portions of the original speech, and zero-shot TTS is concerned with generating the whole speech following a target transcript.
> Recently proposed approaches based on large-scale speech data have significantly streamlined speech generation systems.
> Non-autoregressive (NAR) models, such as SoundStorm \cite{DBLP:journals/corr/abs-2305-09636}, FluentSpeech \cite{DBLP:conf/acl/JiangYZYHRZ23}, NaturalSpeech 3 \cite{DBLP:conf/icml/JuWS0XYLLST000024}, and VoiceBox \cite{DBLP:conf/nips/LeVSKSMWMAMH23}, have been proposed for their high inference speed and stability. However, they face challenges due to their reliance on phoneme-acoustic alignment and the complexity of the training process \cite{DBLP:journals/corr/abs-2406-02328}.
> In contrast, language model (LM) based autoregressive (AR) models, such as VALL-E \cite{DBLP:journals/corr/abs-2301-02111}, UniAudio \cite{DBLP:conf/icml/YangT0HLGCSZ0ZW24}, and VoiceCraft \cite{DBLP:conf/acl/Peng00MH24}, simplify the training process but are hindered by slow and unstable inference. 
> For the SE task, existing methods struggle with handling multiple spans, speech with background noise or music, and preserving the unchanged portions effectively \cite{DBLP:conf/asru/TanDYJCL21,9829827,DBLP:conf/icassp/MorrisonRJBCP21}.
> In addition, as these models can easily clone a human voice, AI safety becomes a potential concern \cite{DBLP:journals/algorithms/AlmutairiE22,DBLP:conf/icassp/JuvelaW24,DBLP:conf/icml/RomanFEDFT24}.
>
> In this work, we focus on AR models for zero-shot text-based SE and TTS, and proposed a novel Transformer-based AR model called SSR-Speech. The main contributions of this paper are summarized as follows:
> (i) SSR-Speech leads to stable inference. Previous AR models may generate the long silence and scratching sound during generation, which produce unnatural sounding speech. The inference-only classifier-free guidance is applied to enhance the stability of the generation process.
> (ii) The generated speech by SSR-Speech contains frame-level watermarks, which provides information whether the audio has been produced by SSR-Speech and which part of the audio has been edited or synthesized. To achieve this, a watermark Encodec model is proposed to introduce frame-level watermarks while reconstructing the waveform.
> (iii) SSR-Speech is robust to multi-span editing and background sounds. The training pipeline of SSR-Speech includes single-span and multi-span editing, and editing any parts of the speech, so that there is no gap between training and inference for insertion, deletion and substitution. In addition, the watermark encodec leverages the original unedited speech segments for the waveform reconstruction, which provides better recovery compared to the Encodec model, especially for speech with background noise or music.
> (iv) Extensive experimental results show the effectiveness of SSR-Speech, which significantly outperforms existing methods on both the zero-shot SE and TTS tasks.

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
