# SSR-Speech

<details>
<summary>基本信息</summary>

- 标题: SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech Editing and Synthesis
- 作者:
  1. Helin Wang (Johns Hopkins University)
  2. Meng Yu (Tencent AI Lab)
  3. Jiarui Hai (Johns Hopkins University)
  4. Chen Chen (Nanyang Technological University)
  5. Yuchen Hu (Nanyang Technological University)
  6. Rilin Chen (Tencent AI Lab)
  7. Najim Dehak (Johns Hopkins University)
  8. Dong Yu (Tencent AI Lab)
- 机构:
  1. Johns Hopkins University: 03/08
  2. Tencent AI Lab: 03/08
  3. Nanyang Technological University: 02/08
- 时间:
  - 预印时间: 2024.09.11 ArXiv v1
  - 更新笔记: 2024.09.13
- 发表:
  - Submitted to ICASSP2025
- 链接:
  - [ArXiv](https://arxiv.org/abs/2409.07556)
  - [DOI]()
  - [Github](https://github.com/WangHelin1997/SSR-Speech)
  - [Demo](https://WangHelin1997.github.io/SSR-Speech-Demo)
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: 5
- 引用: 35
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> In this paper, we introduce SSR-Speech, a neural codec autoregressive model designed for stable, safe, and robust zero-shot text-based speech editing and text-to-speech synthesis. 
> SSR-Speech is built on a Transformer decoder and incorporates classifier-free guidance to enhance the stability of the generation process. 
> A watermark Encodec is proposed to embed frame-level watermarks into the edited regions of the speech so that which parts were edited can be detected. 
> In addition, the waveform reconstruction leverages the original unedited speech segments, providing superior recovery compared to the Encodec model. 
> Our approach achieves the state-of-the-art performance in the RealEdit speech editing task and the LibriTTS text-to-speech task, surpassing previous methods. Furthermore, SSR-Speech excels in multi-span speech editing and also demonstrates remarkable robustness to background sounds. 
> [Source code](https://github.com/WangHelin1997/SSR-Speech) and [demos](https://wanghelin1997.github.io/SSR-Speech-Demo/) are released.

## 1.Introduction: 引言

> Nowadays, zero-shot text-based speech generation \cite{DBLP:conf/icassp/CooperLYFWCY20,DBLP:conf/icml/CasanovaWSJGP22,DBLP:conf/icml/BaiZCML022,DBLP:journals/corr/abs-2406-00654} has garnered significant attention in the speech community, particularly in areas like speech editing (SE) and text-to-speech (TTS) synthesis. Given an unseen speaker during training, zero-shot SE focuses on modifying specific words or phrases within an utterance to align with a target transcript while preserving the unchanged portions of the original speech, and zero-shot TTS is concerned with generating the whole speech following a target transcript.
> Recently proposed approaches based on large-scale speech data have significantly streamlined speech generation systems.
> Non-autoregressive (NAR) models, such as SoundStorm \cite{DBLP:journals/corr/abs-2305-09636}, FluentSpeech \cite{DBLP:conf/acl/JiangYZYHRZ23}, NaturalSpeech 3 \cite{DBLP:conf/icml/JuWS0XYLLST000024}, and VoiceBox \cite{DBLP:conf/nips/LeVSKSMWMAMH23}, have been proposed for their high inference speed and stability. However, they face challenges due to their reliance on phoneme-acoustic alignment and the complexity of the training process \cite{DBLP:journals/corr/abs-2406-02328}.
> In contrast, language model (LM) based autoregressive (AR) models, such as VALL-E \cite{DBLP:journals/corr/abs-2301-02111}, UniAudio \cite{DBLP:conf/icml/YangT0HLGCSZ0ZW24}, and VoiceCraft \cite{DBLP:conf/acl/Peng00MH24}, simplify the training process but are hindered by slow and unstable inference. 
> For the SE task, existing methods struggle with handling multiple spans, speech with background noise or music, and preserving the unchanged portions effectively \cite{DBLP:conf/asru/TanDYJCL21,9829827,DBLP:conf/icassp/MorrisonRJBCP21}.
> In addition, as these models can easily clone a human voice, AI safety becomes a potential concern \cite{DBLP:journals/algorithms/AlmutairiE22,DBLP:conf/icassp/JuvelaW24,DBLP:conf/icml/RomanFEDFT24}.
>
> In this work, we focus on AR models for zero-shot text-based SE and TTS, and proposed a novel Transformer-based AR model called SSR-Speech. The main contributions of this paper are summarized as follows:
> (i) SSR-Speech leads to stable inference. Previous AR models may generate the long silence and scratching sound during generation, which produce unnatural sounding speech. The inference-only classifier-free guidance is applied to enhance the stability of the generation process.
> (ii) The generated speech by SSR-Speech contains frame-level watermarks, which provides information whether the audio has been produced by SSR-Speech and which part of the audio has been edited or synthesized. To achieve this, a watermark Encodec model is proposed to introduce frame-level watermarks while reconstructing the waveform.
> (iii) SSR-Speech is robust to multi-span editing and background sounds. The training pipeline of SSR-Speech includes single-span and multi-span editing, and editing any parts of the speech, so that there is no gap between training and inference for insertion, deletion and substitution. In addition, the watermark encodec leverages the original unedited speech segments for the waveform reconstruction, which provides better recovery compared to the Encodec model, especially for speech with background noise or music.
> (iv) Extensive experimental results show the effectiveness of SSR-Speech, which significantly outperforms existing methods on both the zero-shot SE and TTS tasks.

## 2.Related Works: 相关工作

## 3.Methodology: 方法

> SSR-Speech introduces a causal Transformer decoder \cite{DBLP:conf/nips/VaswaniSPUJGKP17} that takes both text tokens and audio neural codec tokens as input and predicts the masked audio tokens in a language modeling manner.

### Modeling

> Given a speech signal, the Encodec model \cite{DBLP:journals/tmlr/DefossezCSA23} is first applied to quantize it into discrete tokens $A = \{a_1, a_2, ..., a_{T}\}$, where $T$ represents the length of the audio tokens, and each token $a_i = \{a_{i,1}, a_{i,2}, ..., a_{i,K}\}$ corresponds to $K$ codebooks of the Encodec.
>
> As shown in Fig.~\ref{fig:1}, during training, we randomly mask $P$ continuous spans of the audio (\textit{e.g.} $P=1$ in Fig.~\ref{fig:1}). The masked tokens are concatenated with special tokens $[m_1],[m_2],...,[m_P]$, each followed by a special token $[eog]$. The unmasked tokens, also known as context tokens, are similarly concatenated with the special tokens $[m_1],[m_2],...,[m_P]$, with additional special tokens $[sos]$ and $[eos]$ at the beginning and end of the sequence, respectively. The entire set of audio tokens is then combined to form the new audio sequence $A^{\prime}=\{a^{\prime}_1, a^{\prime}_2, ..., a^{\prime}_{T^{\prime}}\}$, where $T^{\prime}$ represents the new length.
>
> We employ a Transformer decoder to autoregressively model the masked tokens, conditioned on the speech transcript, which is embedded as a phoneme sequence $Y = \{y_1, y_2, ..., y_{L}\}$, where $L$ is the length of the phoneme tokens. At each timestep $t$ in $A^{\prime}$, the model predicts $a^{\prime}_t$ using several linear layers, conditioned on the phoneme sequence $Y$ and all preceding tokens in $A^{\prime}$ up to $a^{\prime}_t$, denoted as $X_t$.

$$
\begin{aligned}
    \mathbb{P}_\theta(A^{\prime} \mid Y)= \prod_t \mathbb{P}_\theta\left(a^{\prime}_t \mid Y, X_t\right)
\end{aligned}
$$

> where $\theta$ denote the parameters of the model.

> The training loss is defined as the negative log likelihood:

$$
\begin{aligned}
    \mathcal{L}(\theta)=-\log \mathbb{P}_\theta(A^{\prime} \mid Y)
\end{aligned}
$$

> Following \cite{DBLP:conf/acl/Peng00MH24}, we implement causal masking, delayed stacking, and apply larger weights to the first codebook than the later ones. Unlike \cite{DBLP:conf/acl/Peng00MH24}, we calculate the prediction loss only on the masked tokens, excluding special tokens, rather than on all tokens. This approach yields similar results while reducing training costs in our experiments. Additionally, we mask all regions of the audio, including the beginning and end of the speech, to better align with real-world applications. To further enhance TTS training, we also enforce speech continuation \cite{DBLP:conf/icassp/MaitiPCJC024} by consistently masking the end of the speech with a certain probability.

### Inference

> For the SE task, we compare the original transcript with the target transcript to identify the words that need to be masked. Using word-level forced alignment\footnote{https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner} of the original transcript, we locate the corresponding masked spans of audio tokens. The phoneme tokens from the target transcript and the unmasked audio tokens are then concatenated and fed into the SSR-Speech model to autoregressively predict new audio tokens. Similar to \cite{DBLP:conf/acl/Peng00MH24}, when editing speech, the neighboring words surrounding the span to be edited also need to be slightly adjusted to accurately model co-articulation effects. Thus, we introduce a small margin hyperparameter $\alpha$, extending the length of the masked span by $\alpha$ on both the left and right sides.
>
> For the TTS task, the transcript of a voice prompt is combined with the target transcript of the speech to be generated. Along with the audio tokens of the voice prompt, these inputs are fed into the SSR-Speech model.
>
> Due to the stochastic nature of autoregressive generation, the model occasionally produces excessively long silences or drags out certain sounds, resulting in unnatural-sounding speech. Previous methods address this issue by generating multiple output utterances using different random seeds and discarding the longest ones, but this approach is unstable and time-consuming. In this paper, we propose to use classifier-free guidance (CFG) \cite{ho2021classifierfree} to resolve this problem.
>
> CFG is particularly useful in controlling the trade-off between fidelity to the input and the quality or creativity of the output for diffusion models, also used in AR generation \cite{DBLP:conf/iclr/KreukSPSDCPTA23}.
> Existing methods involves training the model in two modes: conditioned and unconditioned, learning both how to generate general outputs and how to generate outputs that match a specific conditioning input.
> During inference, CFG guides the model by combining the outputs from the conditioned and unconditioned modes. 
> In our initial experiments, we found that traditional CFG cannot solve the dead loop of AR models well, and it may make the training unstable at the beginning.
> To address this issue, we propose to use inference-only CFG that we do not need unconditioned training. More specifically, at inference time we use a random text sequence as the unconditional input, and sample from a distribution obtained by a linear combination of the conditional and unconditional probabilities.
> Formally we sample from,

$$
\begin{align}
\gamma \mathbb{P}_\theta(A^{\prime} \mid Y) + (1-\gamma)\mathbb{P}_\theta(A^{\prime} \mid Y^{\prime})
\end{align}
$$

> where $\gamma$ is the guidance scale and $Y^{\prime}$ is a random phoneme sequence with the same length of $Y$ to enable GPU parallel processing.

### Watermark Encodec

> In this section, we introduce the watermark Encodec, a neural codec model specifically designed for the SE task, capable of watermarking the generated audio and better preserving the unedited regions. Watermark Encodec can also be applied to the TTS task. As shown in Fig.~\ref{fig:2}, the watermark Encodec consists of a speech encoder, a quantizer, a speech decoder, a masked encoder, and a watermark predictor.

#### Watermarking (WM)

> The speech encoder shares the same network architecture as the encoder in Encodec. The watermark predictor also adopts the same architecture as the Encodec encoder, with the addition of a final linear layer for binary classification. We first pretrain the Encodec\footnote{https://github.com/facebookresearch/audiocraft} model and initialize the parameters of the speech encoder and watermark predictor using the pretrained Encodec encoder parameters. The quantizer is identical to the Encodec quantizer, with the same parameters copied over.
>
> The speech decoder, which takes watermarks and audio codes as input, reconstructs the speech and shares the same architecture as the Encodec decoder. The only difference is extra linear layers to project the combined features into the same dimension as the audio features. We also initialize the speech decoder's parameters from the Encodec model. During training, the speech encoder and quantizer are frozen. The watermark is a binary sequence with the same length as the audio frames output by the speech encoder, where masked frames are marked with a value of 1, and unmasked frames are marked with 0.
> An embedding layer is applied to the watermarks to obtain the watermark features.

#### Context-Aware Decoding (CD)

> Encodec reconstructs the waveform using audio codes. However, for the SE task, it's crucial that the unedited spans of the speech remain unchanged. To better utilize the information from these unedited spans during decoding, we propose a context-aware decoding method, which uses the original unedited waveform as an additional input to the watermark Encodec decoder.
>
> Specifically, we mask the edited segments of the original waveform with silence clips and then use a masked encoder to extract the features from this masked waveform. The masked encoder shares the same architecture as the Encodec encoder and is initialized with parameters from Encodec. Consequently, the input to the speech decoder includes the audio codes, the watermarks, and the masked features.
>
> Moreover, we found that using skip connections \cite{DBLP:conf/miccai/RonnebergerFB15} improves reconstruction quality and accelerates model convergence. Therefore, we fuse multi-scale features between each block, following the approach used in UNet \cite{DBLP:conf/miccai/RonnebergerFB15}.

## 4.Experiments: 实验

> For the SSR-Speech model, we use the Gigaspeech XL set \cite{DBLP:conf/interspeech/ChenCWDZWSPTZJK21} as the training data, which contains 10k hours of audio at a 16kHz sampling rate. Audio files shorter than 2 seconds or longer than 15 seconds are excluded. The Encodec model and the Watermark Encodec model are trained on the Gigaspeech M set, comprising 1k hours of audio data.
>
> For the zero-shot SE task, we use the RealEdit dataset \cite{DBLP:conf/acl/Peng00MH24}, which includes 310 manually-crafted speech editing examples. For the zero-shot TTS task, we construct a dataset of 500 prompt-transcript pairs from the LibriTTS test set \cite{DBLP:conf/interspeech/ZenDCZWJCW19}. The voice prompts are between 2.5 and 4 seconds in length, and the target transcripts are randomly selected from different utterances across the entire LibriTTS test set.

### Data

> For the SSR-Speech model, we use the Gigaspeech XL set \cite{DBLP:conf/interspeech/ChenCWDZWSPTZJK21} as the training data, which contains 10k hours of audio at a 16kHz sampling rate. Audio files shorter than 2 seconds or longer than 15 seconds are excluded. The Encodec model and the Watermark Encodec model are trained on the Gigaspeech M set, comprising 1k hours of audio data.
>
> For the zero-shot SE task, we use the RealEdit dataset \cite{DBLP:conf/acl/Peng00MH24}, which includes 310 manually-crafted speech editing examples. For the zero-shot TTS task, we construct a dataset of 500 prompt-transcript pairs from the LibriTTS test set \cite{DBLP:conf/interspeech/ZenDCZWJCW19}. The voice prompts are between 2.5 and 4 seconds in length, and the target transcripts are randomly selected from different utterances across the entire LibriTTS test set.

### Setups

> Following \cite{DBLP:conf/acl/Peng00MH24}, both the Encodec and Watermark Encodec models use 4 RVQ codebooks, each with a vocabulary size of 2048. They operate with a stride of 320 samples, resulting in a codec framerate of 50Hz for audio recorded at a 16kHz sampling rate. The base dimension is 64, doubling at each of the 5 convolutional layers in the encoder.
> The number of spans to be masked, denoted as $P$, is uniformly sampled between 1 and 3. The maximum masking length is set to $90\%$ of the original audio length. During training, we apply a probability of 0.5 to enhance TTS training. Text transcripts are phonemized using an IPA phoneset toolkit\footnote{https://github.com/bootphon/phonemizer} \cite{DBLP:journals/jossw/BernardT21a}.
>
> The SSR-Speech model has the same architecture as VoiceCraft, which consists of 16 Transformer layers with hidden size of 2048 and 12 attention heads. The output of the final layer is passed through four separate 2-layer MLP modules to generate prediction logits. Following VoiceCraft, we employ the ScaledAdam optimizer and Eden scheduler \cite{DBLP:journals/corr/abs-2401-07333}, with a base learning rate of 0.05, a batch size of 400k frames, and a total of 50k training steps with gradient accumulation. The weighting hyperparameters for the 4 codebooks are set to $(5, 1, 0.5, 0.1)$. The SSR-Speech model has 830M parameters and was trained on 8 NVIDIA V100 GPUs for 2 weeks.
>
> For inference, we use nucleus sampling \cite{DBLP:conf/iclr/HoltzmanBDFC20} with $p = 0.8$ and a temperature of 1. The extended masked span $\alpha$ is set to $0.12$ seconds. Based on initial experiments, we determined the optimal value for the hyperparameter $\gamma$ to be $1.5$.

### Baselines

> For the SE task, we compare SSR-Speech with the state-of-the-art model VoiceCraft and a diffusion-based model FluentSpeech.
> For the TTS task, we compare SSR-Speech with state-of-the-art autoregressive models, including VALL-E, Phonme \cite{DBLP:journals/corr/abs-2401-02839} and VoiceCraft.
> For a fair comparison, we take the original VoiceCraft\footnote{https://github.com/jasonppy/VoiceCraft} and Phonme\footnote{https://github.com/PolyAI-LDN/pheme} that were trained on the GigaSpeech dataset, and train FluentSpeech\footnote{https://github.com/Zain-Jiang/Speech-Editing-Toolkit} and VALL-E\footnote{https://github.com/open-mmlab/Amphion/tree/main/models/tts/valle} on the GigaSpeech dataset.

### Metrics

> Following prior studies, we use WER and SIM as objective evaluation metrics, calculated with pre-trained Whisper-medium.en\footnote{https://huggingface.co/openai/whisper-medium.en} \cite{DBLP:conf/icml/RadfordKXBMS23} and WavLM-TDCNN\footnote{https://huggingface.co/microsoft/wavlm-base-plus-sv} \cite{DBLP:journals/jstsp/ChenWCWLCLKYXWZ22} models for speech and speaker recognition, respectively. Additionally, we employ MOSNet\footnote{https://github.com/nii-yamagishilab/mos-finetune-ssl} \cite{DBLP:conf/icassp/CooperHTY22} to estimate an objective MOS score for reference.
>
> For the SE task, we also report MOS estimates for noisy test samples with estimated SNRs below $20$ dB using the Brouhaha\footnote{https://github.com/marianne-m/brouhaha-vad} \cite{DBLP:conf/asru/LavechinMTBCRBCDB23} (MOSNet-N), which includes $36$ samples in the RealEdit dataset.
> In addition, we test MOS estimates for multi-span editing in the RealEdit dataset (MOSNet-M), in which we have $40$ samples with $2$-span editing in total.
>
> For subjective evaluation, we invited $20$ listeners to conduct MOS and Similar MOS (SMOS) assessments, using $60$ randomly selected samples from the RealEdit and LibriTTS test sets.
> We report all these metrics with $95\%$ confidence interval.

## 5.Results: 结果

## 6.Conclusions: 结论
