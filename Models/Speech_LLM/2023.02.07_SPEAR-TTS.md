# SPEAR-TTS

<details>
<summary>基本信息</summary>

- 标题: Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision
- 作者:
  - 01 [Eugene Kharitonov](../../Authors/Eugene_Kharitonov.md)
  - 02 [Damien Vincent](../../Authors/Damien_Vincent.md)
  - 03 [Zalan Borsos](../../Authors/Zalan_Borsos.md)
  - 04 [Raphael Marinier](../../Authors/Raphael_Marinier.md)
  - 05 [Sertan Girgin](../../Authors/Sertan_Girgin.md)
  - 06 [Olivier Pietquin](../../Authors/Olivier_Pietquin.md)
  - 07 [Matt Sharifi](../../Authors/Matt_Sharifi.md)
  - 08 [Marco Tagliasacchi](../../Authors/Marco_Tagliasacchi.md)
  - 09 [Neil Zeghidour](../../Authors/Neil_Zeghidour.md)
- 机构:
  - [Google](../../Institutions/USA-Google.md)
- 时间:
  - 预印时间: 2023.02.07 ArXiv v1
  - 发表时间: 2023.12.21 MIT-TACL
  - 更新笔记: 2024.06.06
- 发表:
  - [TACL](../../Publications/MIT-TACL.md)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2302.03540)
  - [DOI](https://doi.org/10.1162/tacl_a_00618)
  - [Demo](https://google-research.github.io/seanet/speartts/examples/)
- 标签:
  - [语音合成](../../Tags/SpeechSynthesis.md)
- 页数: 19
- 引用: ?
- 被引: 68

</details>

## Abstract

> We introduce ***SPEAR-TTS***, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision.
> By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to "reading") and from semantic tokens to low-level acoustic tokens ("speaking").
> Decoupling these two tasks enables training of the "speaking" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the "reading" component.
> To control the speaker identity, we adopt example prompting, which allows ***SPEAR-TTS*** to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels.
> Our experiments demonstrate that ***SPEAR-TTS*** achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.
