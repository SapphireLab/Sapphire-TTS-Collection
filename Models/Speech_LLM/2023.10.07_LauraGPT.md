# LauraGPT

<details>
<summary>基本信息</summary>

- 标题: LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT
- 作者:
  - 01 [Zhihao Du](../../Authors/Zhihao_Du.md)
  - 02 [Jiaming Wang](../../Authors/Jiaming_Wang.md)
  - 03 [Qian Chen](../../Authors/Qian_Chen.md)
  - 04 [Yunfei Chu](../../Authors/Yunfei_Chu.md)
  - 05 [Zhifu Gao](../../Authors/Zhifu_Gao.md)
  - 06 [Zerui Li](../../Authors/Zerui_Li.md)
  - 07 [Kai Hu](../../Authors/Kai_Hu.md)
  - 08 [Xiaohuan Zhou](../../Authors/Xiaohuan_Zhou.md)
  - 09 [Jin Xu](../../Authors/Jin_Xu.md)
  - 10 [Ziyang Ma](../../Authors/Ziyang_Ma_(马子阳).md)
  - 11 [Wen Wang](../../Authors/Wen_Wang.md)
  - 12 [Siqi Zheng](../../Authors/Siqi_Zheng.md)
  - 13 [Chang Zhou](../../Authors/Chang_Zhou.md)
  - 14 [Zhijie Yan](../../Authors/Zhijie_Yan.md)
  - 15 [Shiliang Zhang](../../Authors/Shiliang_Zhang.md)
- 机构:
  - [阿里巴巴达摩院](../../Institutions/Alibaba.md)
- 时间:
  - 预印时间: 2023.10.07 ArXiv v1
  - 预印时间: 2023.10.10 ArXiv v2
  - 预印时间: 2023.10.11 ArXiv v3
  - 预印时间: 2024.07.03 ArXiv v4
  - 更新笔记: 2024.08.12
- 发表:
  - ~~ICLR 2024 Reject~~
- 链接:
  - [ArXiv](https://arxiv.org/abs/2310.04673)
  - [DOI]()
  - [Github]()
  - [Demo](https://lauragpt.github.io)
  - [Scholar](https://scholar.google.com/scholar?cluster=2748385677497297657)
- 标签:
  - ?
- 页数: 20
- 引用: ?
- 被引: 20+9
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Generative Pre-trained Transformer~(GPT) models have achieved remarkable performance on various natural language processing tasks, and have shown great potential as backbones for audio-and-text large language models (LLMs). Previous mainstream audio-and-text LLMs use discrete audio tokens to represent both input and output audio; however, they suffer from performance degradation on tasks such as automatic speech recognition, speech-to-text translation, and speech enhancement over models using continuous speech features. In this paper, we propose \textbf{LauraGPT}, a novel unified audio-and-text GPT-based LLM for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities. We propose a novel data representation that combines continuous and discrete features for audio: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. We propose a one-step codec vocoder to overcome the prediction challenge caused by the multimodal distribution of codec tokens. We fine-tune LauraGPT using supervised multi-task learning. Extensive experiments show that LauraGPT consistently achieves comparable to superior performance compared to strong baselines on a wide range of audio tasks related to content, semantics, paralinguistics, and audio-signal analysis, such as automatic speech recognition, speech-to-text translation, text-to-speech synthesis, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding. 

## 1.Introduction: 引言

> Large language models (LLMs) are neural networks that generate natural language texts based on a given context. 
> LLMs can learn from massive amounts of text data and mimic human language to acquire human knowledge. LLMs such as GPT-4~\citep{DBLP:journals/corr/abs-2303-08774}, PaLM2~\citep{DBLP:journals/corr/abs-2305-10403}, LLaMA~\citep{DBLP:journals/corr/abs-2302-13971} have demonstrated impressive capabilities across various domains, exhibiting zero-shot generalization without the need for task-specific fine-tuning. However, these models are primarily limited to processing text data. 
>
> Recent research aims to seamlessly integrate text and audio since they are two important modalities for human communication. These efforts include \textbf{Audio-to-Text LLMs}~\citep{DBLP:journals/corr/abs-2212-04356,DBLP:journals/corr/abs-2303-01037,DBLP:journals/corr/abs-2305-11834,DBLP:journals/corr/abs-2310-02973,DBLP:journals/corr/abs-2310-13289,DBLP:journals/corr/abs-2311-07919}, which can convert audio input into text and perform tasks such as automatic speech recognition (ASR) and spoken language understanding (SLU); \textbf{Text-to-Audio LLMs}~\citep{DBLP:journals/corr/abs-2310-00704,DBLP:journals/corr/abs-2312-15821,DBLP:conf/iclr/KreukSPSDCPTA23,DBLP:journals/corr/abs-2308-05734,DBLP:conf/icml/HuangHY0LLYLYZ23,DBLP:journals/corr/abs-2301-02111}, which can convert text input into audio and perform tasks such as text-to-speech synthesis (TTS) and text-to-music synthesis. An emerging line of research focuses on develop more universal and comprehensive \textbf{Audio-and-Text LLMs}~\citep{DBLP:conf/acl/AoWZ0RW0KLZWQ0W22,DBLP:journals/corr/abs-2105-03070,DBLP:journals/corr/abs-2305-11000,DBLP:journals/corr/abs-2305-16107,DBLP:journals/corr/abs-2306-12925,DBLP:journals/corr/abs-2304-12995}, which can support audio-and-text tasks, that is, process and generate both audio and text and perform tasks such as speech enhancement (SE) and speech-to-speech translation (S2ST), in addition to tasks supported by audio-to-text and text-to-audio LLMs. Audio-to-text and text-to-audio LLMs can be considered as subsets of audio-and-text LLMs. 
>
> Audio-and-Text LLMs can be categorized into two directions. One direction builds \textbf{a collaborative AI system} using LLMs as controllers to interface specialized audio models, such as ASR and TTS models, to support various audio-and-text tasks~\citep{DBLP:journals/corr/abs-2303-17580,DBLP:journals/corr/abs-2304-12995}.  These methods have serious drawbacks, including high complexity, significant resource consumption, and unavoidable error accumulation problems. The other direction develops a \textbf{unified Audio-and-Text LLM} leveraging LLMs as the backbone to support audio-and-text tasks~\citep{DBLP:conf/acl/AoWZ0RW0KLZWQ0W22,DBLP:journals/corr/abs-2105-03070,DBLP:journals/corr/abs-2305-16107,DBLP:journals/corr/abs-2306-12925}. Decoder-only audio-and-text LLMs~\citep{DBLP:journals/corr/abs-2305-11000,DBLP:journals/corr/abs-2305-16107,DBLP:journals/corr/abs-2306-12925} are the dominant technique under this category. These models convert continuous audio into discrete tokens and integrate text and audio tokens into unified vocabulary. These models suffer from information loss from quantization of speech signals into discrete tokens, which leads to notable performance degradation on ASR compared to models using continuous speech features~\citep{DBLP:journals/corr/abs-2311-04534,DBLP:journals/corr/abs-2305-18108,DBLP:journals/corr/abs-2309-07377,DBLP:journals/corr/abs-2309-10922}. In this paper, we focus on improving the second category of unified Audio-and-Text LLMs. Moreover, recent advances in audio generation from unified audio-and-text LLMs~\citep{DBLP:journals/corr/abs-2301-02111,DBLP:journals/corr/abs-2305-16107} discretize speech into codec codes, then use an autoregressive language model (LM) to predict output tokens from the first quantizer and use a non-autoregressive model to predict tokens from the other quantizers individually. One limitation of this mechanism is that it needs many prediction steps (hence called \textbf{multi-step audio synthesis scheme}) to generate good quality speech. Another limitation is that predicting the indices of the other codec groups is challenging due to the multi-modal distribution nature of codec tokens~\citep{DBLP:journals/icassp/lmcodec}.
>
> To overcome the drawbacks of existing \textit{unified audio-and-text LLMs}, we propose \textbf{LauraGPT}, a novel \textbf{unified Audio-and-Text LLM} based on the GPT framework for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities, with a single model. We propose \textbf{a novel data representation that combines continuous and discrete features for audio}: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. This data representation improves the performance of audio-input tasks and also facilitates joint autoregressive modeling of audio and text features for audio generation tasks. 
>
> We also propose \textbf{a one-step codec vocoder in LauraGPT to address the two limitations of the popular multi-step audio synthesis scheme}. Our one-step codec vocoder uses a transformer-based predictor to estimate the sum of all codec token groups instead of the individual indices, by minimizing the reconstruction losses. Our approach simplifies the audio generation process to a \textit{single} feed-forward calculation and also overcomes the prediction challenge caused by the multi-modal distribution of codec tokens.
>
> We fine-tune LauraGPT using \textbf{supervised multi-task learning on diverse audio tasks}, including tasks focusing on content, semantics, paralinguistics, and audio-signal analysis, such as ASR, speech-to-text translation (S2TT), TTS, SE, automated audio captioning (AAC), speech emotion recognition (SER), and SLU. \textbf{Comprehensive experiments show that, to the best of our knowledge, LauraGPT\footnote{Demos  are available at \url{https://lauragpt.github.io}} consistently achieves comparable to superior performance compared to strong baselines on the largest and the most diverse set of audio recognition, understanding, and generation tasks among existing decoder-only unified audio-and-text LLMs focusing on these tasks}~\citep{DBLP:journals/corr/abs-2305-11000,DBLP:journals/corr/abs-2305-16107,DBLP:journals/corr/abs-2306-12925}. The results are remarkable since existing general speech models either focus solely on speech recognition and understanding tasks but neglect speech generative tasks, or support speech generation but suffer from severe performance degradation on speech recognition and understanding tasks.

## 2.Related Works: 相关工作

### Audio-to-Text LLMs

> Audio-to-Text LLMs can generate text from audio inputs. Whisper~\citep{DBLP:journals/corr/abs-2212-04356}  and USM~\citep{DBLP:journals/corr/abs-2303-01037} can perform speech recognition and translation across multiple languages and domains. Pengi~\citep{DBLP:journals/corr/abs-2305-11834} is an audio LM that formulates audio tasks as text-generation tasks. UniverSLU~\citep{DBLP:journals/corr/abs-2310-02973} is a universal SLU model that supports various speech classification and sequence generation tasks. SALMONN~\citep{DBLP:journals/corr/abs-2310-13289} and Qwen-Audio~\citep{DBLP:journals/corr/abs-2311-07919} integrate pre-trained text LLMs with separate speech and audio encoders into a single multimodal model. 

### Text-to-Audio LLMs

> Text-to-Audio LLMs can convert text input into audio output and perform tasks such as TTS or text-to-music synthesis.
> Recently, two prominent categories of approaches have emerged for generating audio from text prompts.
> In the first category, continuous representations such as utterance-level embeddings~\citep{DBLP:journals/corr/abs-2206-04769, DBLP:conf/icml/LiuCYMLM0P23, DBLP:conf/icml/HuangHY0LLYLYZ23} and Mel-frequency spectrograms~\citep{DBLP:journals/corr/abs-2305-15255} are used as the targets. 
> However, continuous representations present a challenge for unified modeling of text and audio within a single LM.
> In the second category, discrete codec tokens are employed as audio representations and generated by diffusion models~\citep{DBLP:journals/taslp/YangYWWWZY23} or autoregressive LMs~\citep{DBLP:conf/iclr/KreukSPSDCPTA23,DBLP:journals/taslp/BorsosMVKPSRTGTZ23,DBLP:journals/corr/abs-2306-05284,DBLP:journals/corr/abs-2301-02111}. 
> Among models in the second category, in models such as AudioGen~\citep{DBLP:conf/iclr/KreukSPSDCPTA23}, AudioLM~\citep{DBLP:journals/taslp/BorsosMVKPSRTGTZ23}, and MusicGen~\citep{DBLP:journals/corr/abs-2306-05284}, multiple output heads are used after the LM to predict synchronized or delayed groups of codec tokens. 
> However, this mechanism is only suitable for audio generation and may not be applicable to diverse audio-and-text tasks. 
> Alternatively, in VALL-E~\citep{DBLP:journals/corr/abs-2301-02111}, the LM predicts output tokens of the first quantizer, while tokens of the remaining quantizers are predicted by a non-autoregressive model one by one.
> This mechanism requires numerous prediction procedures to generate acceptable speech quality. Moreover, the indices of the remaining codec groups are challenging to predict due to the multi-modal distribution nature of codec tokens~\citep{DBLP:journals/icassp/lmcodec}.

### Audio-and-Text LLMs

> Audio-and-Text LLMs can process and generate both audio and text, which can be categorized into two directions. One direction uses LLMs as controllers to interface specialized audio models, such as ASR and TTS models, to enable direct audio interaction with LLMs and support various audio-and-text tasks, such as HuggingGPT~\citep{DBLP:journals/corr/abs-2303-17580} and AudioGPT~\citep{DBLP:journals/corr/abs-2304-12995}. However, these models are complex, resource-intensive, and prone to error accumulation. The second direction uses LLMs as the backbone for a unified model that handles audio-and-text tasks~\citep{DBLP:conf/acl/AoWZ0RW0KLZWQ0W22,DBLP:journals/corr/abs-2105-03070,DBLP:journals/corr/abs-2305-16107,DBLP:journals/corr/abs-2306-12925}. SpeechT5~\citep{DBLP:conf/acl/AoWZ0RW0KLZWQ0W22} and SpeechNet~\citep{DBLP:journals/corr/abs-2105-03070} perform various speech tasks with an encoder-decoder model, but they require modal-specific pre-nets and post-nets to deal with different input\&output modalities. VioLA~\citep{DBLP:journals/corr/abs-2305-16107}, AudioPaLM~\citep{DBLP:journals/corr/abs-2306-12925}, SpeechGPT~\citep{DBLP:journals/corr/abs-2305-11000}, and SpeechGen~\citep{DBLP:journals/corr/abs-2306-02207} use decoder-only Transformers to model discrete audio tokens and text tokens as a shared vocabulary, but they suffer from information loss from quantization of audio signals into discrete tokens~\citep{DBLP:journals/corr/abs-2311-04534,DBLP:journals/corr/abs-2305-18108,DBLP:journals/corr/abs-2309-07377,DBLP:journals/corr/abs-2309-10922}.

## 3.Methodology: 方法

> Figure~\ref{fig:overall} depicts the architecture of the proposed LauraGPT.
> Section \ref{sec:gpt-backbone} describes the audio encoder, the text tokenizer, and the modified GPT LM for unified audio-and-text modeling. 
> Section~\ref{sec:audio-tokenizer} elaborates the audio tokenizer.
> Section~\ref{sec:codec_vocoder} introduces an efficient one-step codec vocoder for converting audio tokens into high-quality raw waveforms.
> Section~\ref{sec:task-details} describes the multi-task fine-tuning and shows that LauraGPT provides an extensible framework for supporting more complex tasks.

### 3.1.Modified Language Model for Unifying Audio-and-Text Modeling

> For audio inputs, different from other audio-and-text LLMs using discrete tokens to represent audio inputs, we extract the log-compressed Mel spectrogram features and convert them into  \textit{continuous representations} using a Conformer-based audio encoder.
> Text inputs and outputs are tokenized using the Qwen tokenizer \cite{qwen}, which inherits the tiktoken tokenizer~\citep{tiktoken} and incorporates additional augmentations for commonly used characters and words in different languages.
> The tokenized input text undergoes embedding matrix transformation to generate dense vectors.
> The audio representations and text embeddings have the same dimension $D$.
> The Conformer-based encoder is initialized with weights from a pre-trained ASR model~\citep{gao2023funasr}. 
> Since batch normalization can lead to endless loop decoding, we replace it with layer normalization in the Conformer-based encoder (details are in Appendix~\ref{sec:normlization}).
>
> To achieve audio generation capabilities, the audio outputs are discretized into tokens using an audio tokenizer (Section~\ref{sec:audio-tokenizer}) to obtain \textit{discrete representations} and the softmax output layer is augmented with the audio tokens. 
> As a result, the weight matrix $\mathbf{W}$ in the output layer is of size $(N+M+L) \times D$ and is utilized to calculate the logits for audio and text tokens at each position, where $N$, $M$, and $L$ denote the vocabulary sizes of text, audio, and task tokens, respectively. Task tokens are used to inform the model which task should be performed. Note that in order to control the sequence length, we perform the low frame rate~(LFR) method~\citep{gao2020san-m} to downsample audio inputs to 60ms and only select the first codec group of the audio outputs.
>
> Based on the aforementioned representations, the GPT backbone is trained to model various audio and text tasks by minimizing the cross-entropy loss:

$$
    \mathcal{L}_{LM}=-\frac{1}{T_v}\sum_{j=1}^{T_v}{
    \log p_\theta\left(
    \mathbf{v}_j | \mathbf{u}_{1:T_u}, \mathbf{u}_{task},\mathbf{v}_{1:j-1}
    \right)
    }
$$

> where $\mathbf{u}$ denotes the input embeddings with a sequence length $T_u$ and $\mathbf{v}$ represents the sequence of target tokens with a length $T_v$. 
> To specify a task, a special task-related token $\mathbf{u}_{task}$ is inserted between the input embeddings and output tokens.
> Note that only the losses of outputs are taken into account, while losses on inputs and task token embeddings are masked out.
> After the final output layer, audio tokens are decoded to raw waveforms using a codec vocoder (Section \ref{sec:codec_vocoder}).
> Since it is challenging to train an LLM from scratch with limited data and computational resources, we use the open-source GPT LLM, Qwen~\citep{qwen}, as the backbone. Qwen is pre-trained on a diverse corpus covering various domains in English and Chinese and supports 8192 context length. Compared with other open-source GPT models with similar model sizes, Qwen models demonstrate impressive competitiveness, achieving better performance on widely used benchmarks, especially on Chinese tasks~\citep{qwen}.
> Within LauraGPT, all parameters including the Qwen backbone are jointly optimized, except for the codec vocoder, which is trained independently and kept frozen during both training and inference stages of LauraGPT.

### 3.2.Audio tokenizer

> For audio generation, we utilize a codec model as the audio tokenizer to extract \textit{discrete} representations.
> Our codec model shares a similar architecture as EnCodec~\citep{defossez2022highfi}, which comprises convolutional recurrent encoder and decoder~\citep{DBLP:conf/interspeech/TagliasacchiLMR20} and a residual vector quantizer (RVQ)~\citep{vasuki2006review}.
> We enhance the original EnCodec model with the following modifications:
> 1) Add reconstruction losses in the magnitude spectrum domain to improve the quality of middle- and high-frequency signals.
> 2) Stack five strided convolution blocks with strides of $[8, 5, 4, 2, 2]$ to address the challenge of long sequence lengths, resulting in a token rate of 25Hz for each token group. 
> 3) Use 32 quantizers with structured dropout in the RVQ module, each with vocabulary size 1024. This revision improves speech quality with more quantizers while preserving most information in the shallow quantizers.
> The encoder and the \textit{first RVQ quantizer} are used as the audio tokenizer, and \textbf{the outputs of the first quantizer are used as the audio tokens}. The choice of the first $N$ RVQ quantizers to use is a tradeoff between performance and sequence length (hence efficiency). The remaining quantizers and the decoder are only used when training the codec model. Details of training and the pre-trained codec model are in ~\cite{du2023funcodec}. 

### 3.3.One-step Codec Vocoder for Audio Generation

> We propose a one-step codec vocoder in LauraGPT to generate waveforms from the audio tokens, which are extracted from the \textit{first} quantizer as described in Section~\ref{sec:audio-tokenizer}. Our vocoder comprises two components: a transformer-based predictor and a codec decoder.
> The predictor is trained to estimate the summation of codec embeddings from the 32 RVQ quantizers by minimizing the L1 and L2 distances between the predicted embeddings $\hat{\mathbf{E}}$ and their corresponding ground truth $\mathbf{E}$:

$$
    \mathcal{L}_{pre}=\sum_{t,i}^{T,D_c}{|\mathbf{E}_{t,i}-\hat{\mathbf{E}}_{t,i}|_1 + |\mathbf{E}_{t,i}-\hat{\mathbf{E}}_{t,i}|_2}
$$

> where $T$ denotes the total number of frames and $D_{c}$ denotes the dimension of the codec embeddings. 
> After obtaining the estimated embeddings, the decoder of an pre-trained codec model is utilized to reconstruct the raw audio waveforms. 

> \textbf{Alongside the predicted audio tokens from the LLM, text and audio inputs are used as conditions and fed to the predictor}.
> For zero-shot TTS task, the text inputs serve as a condition as well as the prompt audio features. For SE task, the input noisy speech features are employed as conditions.
> Such text and audio conditionings allow the model to generate high-quality audio signals by leveraging the diverse information in prompt audios and noisy speeches, which is lacked in the discrete tokens (output from the first quantizer). Therefore, different from existing Text-to-Audio LLMs, \textbf{our approach simplifies the audio generation process to a single feed-forward calculation and overcomes the prediction challenge caused by the multi-modal distribution of codec tokens}.

### 3.4.Multi-task Finetuning

#### Basic Tasks

> We unify modeling of the following \textit{basic tasks} in the single LauraGPT model and use these tasks for multi-task fine-tuning:
> Automatic Speech Recognition (\textbf{ASR}), Spoken Language Understanding (\textbf{SLU}), Speech-to-Text Translation (\textbf{S2TT}), Speech Emotion Recognition (\textbf{SER}), Automated Audio Captioning (\textbf{AAC}), Speech Enhancement (\textbf{SE}), and Text-to-speech Synthesis (\textbf{TTS}). Task definitions are in Appendix~\ref{sec:task-intro}.

#### Unified Task Expression

> LauraGPT operates based on a unified task expression: \texttt{[input embeddings, task ID, output tokens]}.
> With the same inputs, the desired outputs can differ across tasks. For instance, ASR and S2TT tasks require different outputs even for the same audio input.
> Task tokens are included in both input embedding and output weight matrices.
> The TTS task takes text embeddings as inputs, while the ASR, S2TT, SLU, SE, ACC, and SER tasks take audio encodings as inputs. 
> The TTS and SE tasks use audio tokens as the target outputs, while the remaining tasks use text tokens as the target outputs.

#### Support More Complex Tasks

> With its modularized design, LauraGPT provides an extensible framework to support complex tasks. By breaking a task into sub-tasks among the basic tasks and cascading the raw inputs and model outputs of sub-tasks, LauraGPT can perform more complex tasks.
> For example, we demonstrate that LauraGPT is capable of performing the advanced speech-to-speech translation (S2ST) task by combining the S2TT and TTS tasks.
> Initially, a sequence is constructed to translate the speech content into the target language text using the S2TT task token: \texttt{[audio encoding, <S2TT>]}. 
> Subsequently, the translated text is combined with the TTS task token to synthesize speech: \texttt{[text embedding, <TTS>]}. 
> If maintaining the speaker identity is desired, the original inputs and content can be incorporated to perform \textit{personalized TTS}. This can be achieved with an input sequence as \texttt{[ASR recognized text embedding, S2TT translated text embedding, <TTS>, audio token of input speech]}, where \texttt{ASR recognized text embedding} is obtained using the ASR task: \texttt{[audio encoding, <ASR>]}. This approach treats the bilingual text as the complete input and allows the model to generate an output sequence of codec tokens while maintaining the same speaker identity. Audio samples of S2ST can be found on the demo site. More examples of complex tasks are in Appendix~\ref{appendix:more-complex-tasks}.

## 4.Experiments: 实验

### Model Architecture

> The Conformer-based audio encoder consists of 32 conformer blocks.
> Each block consists of a feed-forward module with 1536 units, an attention module with 16 heads and a dimension of 512, a convolutional module including the pointwise and depthwise convolution layers, and a second feed-forward module with 1536 units. 
> Sinusoidal positional encoding is applied on the audio inputs. 
> For a trade-off between performance and training efficiency, we use Qwen-1.8B\footnote{\url{https://github.com/QwenLM/Qwen}} as the backbone and LauraGPT has 2B parameters.
> Qwen-1.8B comprises 24 transformer layers with a hidden size 2048 and 16 attention heads. 
> \textbf{Although Conformer and Qwen-1.8B are selected as the audio encoder and GPT backbone, they can be replaced by other encoders and GPT models}.

### Training Setup

> In all experiments, we initialize the Qwen backbone and audio encoder with the pre-trained checkpoints. We then optimize the model parameters through multi-task fine-tuning. The training\&test datasets and evaluation metrics are presented in Appendix~\ref{sec:training-datasets} and ~\ref{sec:evaldata-metrics}. Appendix~\ref{sec:detail_training_setup} describes the three-stage training process to address the significant variation in data volume across different tasks, and details the inference process. 

## 5.Results: 结果

## 6.Conclusions: 结论

> We propose LauraGPT that can handle both audio and text inputs and outputs and perform audio recognition, understanding, and generation. We propose combining continuous and discrete features for audio and a one-step codec vocoder, and employ multi-task learning. Experiments demonstrate that LauraGPT achieves comparable to superior performance compared to strong baselines on a wide range of speech tasks on content, semantics, paralinguistics, and audio-signal analysis. 

## 7.Limitations: 局限性

> In this work, in order to support a wide range of audio recognition, understanding, and generation tasks, we choose to train all parameters in LauraGPT during supervised multi-task finetuning, including the Qwen backbone, except for the codec vocoder. This strategy results in substantial computations for training. In future work, we plan to investigate parameter-efficient fine-tuning to reduce computation demands. Also, due to the limited computation resources, our comparisons between the multi-task trained LauraGPT and single-task models are focused on the low-resource tasks, that is, AAC, SLU, and SER tasks. We find that multi-task learning for LauraGPT consistently achieves better performance than single-task training for tasks with limited training data. Next, we plan to complete comparisons of LauraGPT and single-task models on all tasks, including relatively rich-resource tasks such as ASR. These studies will promote understandings on where tasks could benefit from each other, including tasks with even conflicting objectives. We also plan to conduct deeper analysis on the potential risk of catastrophic forgetting of the original text capabilities of the pre-trained text LLM, due to multi-task learning of speech tasks. Note that exploration of parameter-efficient fine-tuning may also help preserve the original text capabilities of the pre-trained text LLMs.
>
> LauraGPT relies on discrete audio tokens for speech generative tasks. Our research shows that the performance of this paradigm strongly depends on the quality of the audio tokenizer. We plan to systematically analyze the impact of various audio tokenizers on diverse audio generative tasks. We plan to develop new audio tokenizers that are more suitable for unified Auio-and-Text LLMs and provide desirable representations for generative tasks.
>
> There are great emerging interests in fundamental speech models that are similar to those in the field of NLP. This is a tremendously valuable research direction. Our work achieves  important milestone for this research question, as we explore and provide promising answers to the following question: \textit{How to design more efficient and scalable unified GPT-style Audio-and-Text LLMs than existing approaches that can leverage large-scale labeled data and achieve highly competitive performance on a diverse set of speech tasks, including speech recognition, understanding and generation, using a single model?} Note that previous general speech models either focus solely on speech recognition and understanding tasks but neglect speech generative tasks, or support speech generation but suffer from severe performance degradation on speech recognition and understanding tasks.
>
> Inspired by the recent advances of LLMs in NLP, we envision that the fundamental speech models should have the following capabilities:
> - In-context learning ability like GPT-3, which can learn from few-shot examples and adapt to new tasks, such as predicting the age of the speaker from a speech sample.
> - Instruction-following ability like InstructGPT and ChatGPT, which can perform the appropriate speech-related task given a natural language instruction, such as synthesizing a speech with a specific emotion or style.
> - General audio modeling abilities, i.e., speech, non-speech audio, and music, such as music generation.
>
> Our work demonstrates that the current LauraGPT has made solid progress and reached one important milestone toward a speech foundation model. 
> From LauraGPT to the next-generation speech foundation model we envision, most remaining efforts are in more task data collection and more self-supervised and/or supervised pre-training and supervised fine-tuning. There is no need to modify the model architecture.
