# LauraGPT

<details>
<summary>基本信息</summary>

- 标题: LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT
- 作者:
  - 01 [Zhihao Du](../../Authors/Zhihao_Du.md)
  - 02 [Jiaming Wang](../../Authors/Jiaming_Wang.md)
  - 03 [Qian Chen](../../Authors/Qian_Chen.md)
  - 04 [Yunfei Chu](../../Authors/Yunfei_Chu.md)
  - 05 [Zhifu Gao](../../Authors/Zhifu_Gao.md)
  - 06 [Zerui Li](../../Authors/Zerui_Li.md)
  - 07 [Kai Hu](../../Authors/Kai_Hu.md)
  - 08 [Xiaohuan Zhou](../../Authors/Xiaohuan_Zhou.md)
  - 09 [Jin Xu](../../Authors/Jin_Xu.md)
  - 10 [Ziyang Ma](../../Authors/Ziyang_Ma_(马子阳).md)
  - 11 [Wen Wang](../../Authors/Wen_Wang.md)
  - 12 [Siqi Zheng](../../Authors/Siqi_Zheng.md)
  - 13 [Chang Zhou](../../Authors/Chang_Zhou.md)
  - 14 [Zhijie Yan](../../Authors/Zhijie_Yan.md)
  - 15 [Shiliang Zhang](../../Authors/Shiliang_Zhang.md)
- 机构:
  - [阿里巴巴达摩院](../../Institutions/Alibaba.md)
- 时间:
  - 预印时间: 2023.10.07 ArXiv v1
  - 预印时间: 2023.10.10 ArXiv v2
  - 预印时间: 2023.10.11 ArXiv v3
  - 预印时间: 2024.07.03 ArXiv v4
  - 更新笔记: 2024.08.12
- 发表:
  - ~~ICLR 2024 Reject~~
- 链接:
  - [ArXiv](https://arxiv.org/abs/2310.04673)
  - [DOI]()
  - [Github]()
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: 20
- 引用: ?
- 被引: 20+9
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Generative Pre-trained Transformer~(GPT) models have achieved remarkable performance on various natural language processing tasks, and have shown great potential as backbones for audio-and-text large language models (LLMs). Previous mainstream audio-and-text LLMs use discrete audio tokens to represent both input and output audio; however, they suffer from performance degradation on tasks such as automatic speech recognition, speech-to-text translation, and speech enhancement over models using continuous speech features. In this paper, we propose \textbf{LauraGPT}, a novel unified audio-and-text GPT-based LLM for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities. We propose a novel data representation that combines continuous and discrete features for audio: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. We propose a one-step codec vocoder to overcome the prediction challenge caused by the multimodal distribution of codec tokens. We fine-tune LauraGPT using supervised multi-task learning. Extensive experiments show that LauraGPT consistently achieves comparable to superior performance compared to strong baselines on a wide range of audio tasks related to content, semantics, paralinguistics, and audio-signal analysis, such as automatic speech recognition, speech-to-text translation, text-to-speech synthesis, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding. 

## 1.Introduction: 引言

> Large language models (LLMs) are neural networks that generate natural language texts based on a given context. 
> LLMs can learn from massive amounts of text data and mimic human language to acquire human knowledge. LLMs such as GPT-4~\citep{DBLP:journals/corr/abs-2303-08774}, PaLM2~\citep{DBLP:journals/corr/abs-2305-10403}, LLaMA~\citep{DBLP:journals/corr/abs-2302-13971} have demonstrated impressive capabilities across various domains, exhibiting zero-shot generalization without the need for task-specific fine-tuning. However, these models are primarily limited to processing text data. 
>
> Recent research aims to seamlessly integrate text and audio since they are two important modalities for human communication. These efforts include \textbf{Audio-to-Text LLMs}~\citep{DBLP:journals/corr/abs-2212-04356,DBLP:journals/corr/abs-2303-01037,DBLP:journals/corr/abs-2305-11834,DBLP:journals/corr/abs-2310-02973,DBLP:journals/corr/abs-2310-13289,DBLP:journals/corr/abs-2311-07919}, which can convert audio input into text and perform tasks such as automatic speech recognition (ASR) and spoken language understanding (SLU); \textbf{Text-to-Audio LLMs}~\citep{DBLP:journals/corr/abs-2310-00704,DBLP:journals/corr/abs-2312-15821,DBLP:conf/iclr/KreukSPSDCPTA23,DBLP:journals/corr/abs-2308-05734,DBLP:conf/icml/HuangHY0LLYLYZ23,DBLP:journals/corr/abs-2301-02111}, which can convert text input into audio and perform tasks such as text-to-speech synthesis (TTS) and text-to-music synthesis. An emerging line of research focuses on develop more universal and comprehensive \textbf{Audio-and-Text LLMs}~\citep{DBLP:conf/acl/AoWZ0RW0KLZWQ0W22,DBLP:journals/corr/abs-2105-03070,DBLP:journals/corr/abs-2305-11000,DBLP:journals/corr/abs-2305-16107,DBLP:journals/corr/abs-2306-12925,DBLP:journals/corr/abs-2304-12995}, which can support audio-and-text tasks, that is, process and generate both audio and text and perform tasks such as speech enhancement (SE) and speech-to-speech translation (S2ST), in addition to tasks supported by audio-to-text and text-to-audio LLMs. Audio-to-text and text-to-audio LLMs can be considered as subsets of audio-and-text LLMs. 
>
> Audio-and-Text LLMs can be categorized into two directions. One direction builds \textbf{a collaborative AI system} using LLMs as controllers to interface specialized audio models, such as ASR and TTS models, to support various audio-and-text tasks~\citep{DBLP:journals/corr/abs-2303-17580,DBLP:journals/corr/abs-2304-12995}.  These methods have serious drawbacks, including high complexity, significant resource consumption, and unavoidable error accumulation problems. The other direction develops a \textbf{unified Audio-and-Text LLM} leveraging LLMs as the backbone to support audio-and-text tasks~\citep{DBLP:conf/acl/AoWZ0RW0KLZWQ0W22,DBLP:journals/corr/abs-2105-03070,DBLP:journals/corr/abs-2305-16107,DBLP:journals/corr/abs-2306-12925}. Decoder-only audio-and-text LLMs~\citep{DBLP:journals/corr/abs-2305-11000,DBLP:journals/corr/abs-2305-16107,DBLP:journals/corr/abs-2306-12925} are the dominant technique under this category. These models convert continuous audio into discrete tokens and integrate text and audio tokens into unified vocabulary. These models suffer from information loss from quantization of speech signals into discrete tokens, which leads to notable performance degradation on ASR compared to models using continuous speech features~\citep{DBLP:journals/corr/abs-2311-04534,DBLP:journals/corr/abs-2305-18108,DBLP:journals/corr/abs-2309-07377,DBLP:journals/corr/abs-2309-10922}. In this paper, we focus on improving the second category of unified Audio-and-Text LLMs. Moreover, recent advances in audio generation from unified audio-and-text LLMs~\citep{DBLP:journals/corr/abs-2301-02111,DBLP:journals/corr/abs-2305-16107} discretize speech into codec codes, then use an autoregressive language model (LM) to predict output tokens from the first quantizer and use a non-autoregressive model to predict tokens from the other quantizers individually. One limitation of this mechanism is that it needs many prediction steps (hence called \textbf{multi-step audio synthesis scheme}) to generate good quality speech. Another limitation is that predicting the indices of the other codec groups is challenging due to the multi-modal distribution nature of codec tokens~\citep{DBLP:journals/icassp/lmcodec}.
>
> To overcome the drawbacks of existing \textit{unified audio-and-text LLMs}, we propose \textbf{LauraGPT}, a novel \textbf{unified Audio-and-Text LLM} based on the GPT framework for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities, with a single model. We propose \textbf{a novel data representation that combines continuous and discrete features for audio}: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. This data representation improves the performance of audio-input tasks and also facilitates joint autoregressive modeling of audio and text features for audio generation tasks. 
>
> We also propose \textbf{a one-step codec vocoder in LauraGPT to address the two limitations of the popular multi-step audio synthesis scheme}. Our one-step codec vocoder uses a transformer-based predictor to estimate the sum of all codec token groups instead of the individual indices, by minimizing the reconstruction losses. Our approach simplifies the audio generation process to a \textit{single} feed-forward calculation and also overcomes the prediction challenge caused by the multi-modal distribution of codec tokens.
>
> We fine-tune LauraGPT using \textbf{supervised multi-task learning on diverse audio tasks}, including tasks focusing on content, semantics, paralinguistics, and audio-signal analysis, such as ASR, speech-to-text translation (S2TT), TTS, SE, automated audio captioning (AAC), speech emotion recognition (SER), and SLU. \textbf{Comprehensive experiments show that, to the best of our knowledge, LauraGPT\footnote{Demos  are available at \url{https://lauragpt.github.io}} consistently achieves comparable to superior performance compared to strong baselines on the largest and the most diverse set of audio recognition, understanding, and generation tasks among existing decoder-only unified audio-and-text LLMs focusing on these tasks}~\citep{DBLP:journals/corr/abs-2305-11000,DBLP:journals/corr/abs-2305-16107,DBLP:journals/corr/abs-2306-12925}. The results are remarkable since existing general speech models either focus solely on speech recognition and understanding tasks but neglect speech generative tasks, or support speech generation but suffer from severe performance degradation on speech recognition and understanding tasks.

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
