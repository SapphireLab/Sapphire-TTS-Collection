# LauraGPT

<details>
<summary>基本信息</summary>

- 标题: LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT
- 作者:
  - 01 [Zhihao Du](../../Authors/Zhihao_Du.md)
  - 02 [Jiaming Wang](../../Authors/Jiaming_Wang.md)
  - 03 [Qian Chen](../../Authors/Qian_Chen.md)
  - 04 [Yunfei Chu](../../Authors/Yunfei_Chu.md)
  - 05 [Zhifu Gao](../../Authors/Zhifu_Gao.md)
  - 06 [Zerui Li](../../Authors/Zerui_Li.md)
  - 07 [Kai Hu](../../Authors/Kai_Hu.md)
  - 08 [Xiaohuan Zhou](../../Authors/Xiaohuan_Zhou.md)
  - 09 [Jin Xu](../../Authors/Jin_Xu.md)
  - 10 [Ziyang Ma](../../Authors/Ziyang_Ma_(马子阳).md)
  - 11 [Wen Wang](../../Authors/Wen_Wang.md)
  - 12 [Siqi Zheng](../../Authors/Siqi_Zheng.md)
  - 13 [Chang Zhou](../../Authors/Chang_Zhou.md)
  - 14 [Zhijie Yan](../../Authors/Zhijie_Yan.md)
  - 15 [Shiliang Zhang](../../Authors/Shiliang_Zhang.md)
- 机构:
  - [阿里巴巴达摩院](../../Institutions/Alibaba.md)
- 时间:
  - 预印时间: 2023.10.07 ArXiv v1
  - 预印时间: 2023.10.10 ArXiv v2
  - 预印时间: 2023.10.11 ArXiv v3
  - 预印时间: 2024.07.03 ArXiv v4
  - 更新笔记: 2024.08.12
- 发表:
  - ~~ICLR 2024 Reject~~
- 链接:
  - [ArXiv](https://arxiv.org/abs/2310.04673)
  - [DOI]()
  - [Github]()
  - [Demo](https://lauragpt.github.io)
  - [Scholar](https://scholar.google.com/scholar?cluster=2748385677497297657)
- 标签:
  - ?
- 页数: 20
- 引用: ?
- 被引: 20+9
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Generative Pre-trained Transformer~(GPT) models have achieved remarkable performance on various natural language processing tasks, and have shown great potential as backbones for audio-and-text large language models (LLMs). Previous mainstream audio-and-text LLMs use discrete audio tokens to represent both input and output audio; however, they suffer from performance degradation on tasks such as automatic speech recognition, speech-to-text translation, and speech enhancement over models using continuous speech features. In this paper, we propose \textbf{LauraGPT}, a novel unified audio-and-text GPT-based LLM for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities. We propose a novel data representation that combines continuous and discrete features for audio: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. We propose a one-step codec vocoder to overcome the prediction challenge caused by the multimodal distribution of codec tokens. We fine-tune LauraGPT using supervised multi-task learning. Extensive experiments show that LauraGPT consistently achieves comparable to superior performance compared to strong baselines on a wide range of audio tasks related to content, semantics, paralinguistics, and audio-signal analysis, such as automatic speech recognition, speech-to-text translation, text-to-speech synthesis, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding. 

## 1.Introduction: 引言

> Large language models (LLMs) are neural networks that generate natural language texts based on a given context. 
> LLMs can learn from massive amounts of text data and mimic human language to acquire human knowledge. LLMs such as GPT-4~\citep{DBLP:journals/corr/abs-2303-08774}, PaLM2~\citep{DBLP:journals/corr/abs-2305-10403}, LLaMA~\citep{DBLP:journals/corr/abs-2302-13971} have demonstrated impressive capabilities across various domains, exhibiting zero-shot generalization without the need for task-specific fine-tuning. However, these models are primarily limited to processing text data. 
>
> Recent research aims to seamlessly integrate text and audio since they are two important modalities for human communication. These efforts include \textbf{Audio-to-Text LLMs}~\citep{DBLP:journals/corr/abs-2212-04356,DBLP:journals/corr/abs-2303-01037,DBLP:journals/corr/abs-2305-11834,DBLP:journals/corr/abs-2310-02973,DBLP:journals/corr/abs-2310-13289,DBLP:journals/corr/abs-2311-07919}, which can convert audio input into text and perform tasks such as automatic speech recognition (ASR) and spoken language understanding (SLU); \textbf{Text-to-Audio LLMs}~\citep{DBLP:journals/corr/abs-2310-00704,DBLP:journals/corr/abs-2312-15821,DBLP:conf/iclr/KreukSPSDCPTA23,DBLP:journals/corr/abs-2308-05734,DBLP:conf/icml/HuangHY0LLYLYZ23,DBLP:journals/corr/abs-2301-02111}, which can convert text input into audio and perform tasks such as text-to-speech synthesis (TTS) and text-to-music synthesis. An emerging line of research focuses on develop more universal and comprehensive \textbf{Audio-and-Text LLMs}~\citep{DBLP:conf/acl/AoWZ0RW0KLZWQ0W22,DBLP:journals/corr/abs-2105-03070,DBLP:journals/corr/abs-2305-11000,DBLP:journals/corr/abs-2305-16107,DBLP:journals/corr/abs-2306-12925,DBLP:journals/corr/abs-2304-12995}, which can support audio-and-text tasks, that is, process and generate both audio and text and perform tasks such as speech enhancement (SE) and speech-to-speech translation (S2ST), in addition to tasks supported by audio-to-text and text-to-audio LLMs. Audio-to-text and text-to-audio LLMs can be considered as subsets of audio-and-text LLMs. 
>
> Audio-and-Text LLMs can be categorized into two directions. One direction builds \textbf{a collaborative AI system} using LLMs as controllers to interface specialized audio models, such as ASR and TTS models, to support various audio-and-text tasks~\citep{DBLP:journals/corr/abs-2303-17580,DBLP:journals/corr/abs-2304-12995}.  These methods have serious drawbacks, including high complexity, significant resource consumption, and unavoidable error accumulation problems. The other direction develops a \textbf{unified Audio-and-Text LLM} leveraging LLMs as the backbone to support audio-and-text tasks~\citep{DBLP:conf/acl/AoWZ0RW0KLZWQ0W22,DBLP:journals/corr/abs-2105-03070,DBLP:journals/corr/abs-2305-16107,DBLP:journals/corr/abs-2306-12925}. Decoder-only audio-and-text LLMs~\citep{DBLP:journals/corr/abs-2305-11000,DBLP:journals/corr/abs-2305-16107,DBLP:journals/corr/abs-2306-12925} are the dominant technique under this category. These models convert continuous audio into discrete tokens and integrate text and audio tokens into unified vocabulary. These models suffer from information loss from quantization of speech signals into discrete tokens, which leads to notable performance degradation on ASR compared to models using continuous speech features~\citep{DBLP:journals/corr/abs-2311-04534,DBLP:journals/corr/abs-2305-18108,DBLP:journals/corr/abs-2309-07377,DBLP:journals/corr/abs-2309-10922}. In this paper, we focus on improving the second category of unified Audio-and-Text LLMs. Moreover, recent advances in audio generation from unified audio-and-text LLMs~\citep{DBLP:journals/corr/abs-2301-02111,DBLP:journals/corr/abs-2305-16107} discretize speech into codec codes, then use an autoregressive language model (LM) to predict output tokens from the first quantizer and use a non-autoregressive model to predict tokens from the other quantizers individually. One limitation of this mechanism is that it needs many prediction steps (hence called \textbf{multi-step audio synthesis scheme}) to generate good quality speech. Another limitation is that predicting the indices of the other codec groups is challenging due to the multi-modal distribution nature of codec tokens~\citep{DBLP:journals/icassp/lmcodec}.
>
> To overcome the drawbacks of existing \textit{unified audio-and-text LLMs}, we propose \textbf{LauraGPT}, a novel \textbf{unified Audio-and-Text LLM} based on the GPT framework for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities, with a single model. We propose \textbf{a novel data representation that combines continuous and discrete features for audio}: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. This data representation improves the performance of audio-input tasks and also facilitates joint autoregressive modeling of audio and text features for audio generation tasks. 
>
> We also propose \textbf{a one-step codec vocoder in LauraGPT to address the two limitations of the popular multi-step audio synthesis scheme}. Our one-step codec vocoder uses a transformer-based predictor to estimate the sum of all codec token groups instead of the individual indices, by minimizing the reconstruction losses. Our approach simplifies the audio generation process to a \textit{single} feed-forward calculation and also overcomes the prediction challenge caused by the multi-modal distribution of codec tokens.
>
> We fine-tune LauraGPT using \textbf{supervised multi-task learning on diverse audio tasks}, including tasks focusing on content, semantics, paralinguistics, and audio-signal analysis, such as ASR, speech-to-text translation (S2TT), TTS, SE, automated audio captioning (AAC), speech emotion recognition (SER), and SLU. \textbf{Comprehensive experiments show that, to the best of our knowledge, LauraGPT\footnote{Demos  are available at \url{https://lauragpt.github.io}} consistently achieves comparable to superior performance compared to strong baselines on the largest and the most diverse set of audio recognition, understanding, and generation tasks among existing decoder-only unified audio-and-text LLMs focusing on these tasks}~\citep{DBLP:journals/corr/abs-2305-11000,DBLP:journals/corr/abs-2305-16107,DBLP:journals/corr/abs-2306-12925}. The results are remarkable since existing general speech models either focus solely on speech recognition and understanding tasks but neglect speech generative tasks, or support speech generation but suffer from severe performance degradation on speech recognition and understanding tasks.

## 2.Related Works: 相关工作

### Audio-to-Text LLMs

> Audio-to-Text LLMs can generate text from audio inputs. Whisper~\citep{DBLP:journals/corr/abs-2212-04356}  and USM~\citep{DBLP:journals/corr/abs-2303-01037} can perform speech recognition and translation across multiple languages and domains. Pengi~\citep{DBLP:journals/corr/abs-2305-11834} is an audio LM that formulates audio tasks as text-generation tasks. UniverSLU~\citep{DBLP:journals/corr/abs-2310-02973} is a universal SLU model that supports various speech classification and sequence generation tasks. SALMONN~\citep{DBLP:journals/corr/abs-2310-13289} and Qwen-Audio~\citep{DBLP:journals/corr/abs-2311-07919} integrate pre-trained text LLMs with separate speech and audio encoders into a single multimodal model. 

### Text-to-Audio LLMs

> Text-to-Audio LLMs can convert text input into audio output and perform tasks such as TTS or text-to-music synthesis.
> Recently, two prominent categories of approaches have emerged for generating audio from text prompts.
> In the first category, continuous representations such as utterance-level embeddings~\citep{DBLP:journals/corr/abs-2206-04769, DBLP:conf/icml/LiuCYMLM0P23, DBLP:conf/icml/HuangHY0LLYLYZ23} and Mel-frequency spectrograms~\citep{DBLP:journals/corr/abs-2305-15255} are used as the targets. 
> However, continuous representations present a challenge for unified modeling of text and audio within a single LM.
> In the second category, discrete codec tokens are employed as audio representations and generated by diffusion models~\citep{DBLP:journals/taslp/YangYWWWZY23} or autoregressive LMs~\citep{DBLP:conf/iclr/KreukSPSDCPTA23,DBLP:journals/taslp/BorsosMVKPSRTGTZ23,DBLP:journals/corr/abs-2306-05284,DBLP:journals/corr/abs-2301-02111}. 
> Among models in the second category, in models such as AudioGen~\citep{DBLP:conf/iclr/KreukSPSDCPTA23}, AudioLM~\citep{DBLP:journals/taslp/BorsosMVKPSRTGTZ23}, and MusicGen~\citep{DBLP:journals/corr/abs-2306-05284}, multiple output heads are used after the LM to predict synchronized or delayed groups of codec tokens. 
> However, this mechanism is only suitable for audio generation and may not be applicable to diverse audio-and-text tasks. 
> Alternatively, in VALL-E~\citep{DBLP:journals/corr/abs-2301-02111}, the LM predicts output tokens of the first quantizer, while tokens of the remaining quantizers are predicted by a non-autoregressive model one by one.
> This mechanism requires numerous prediction procedures to generate acceptable speech quality. Moreover, the indices of the remaining codec groups are challenging to predict due to the multi-modal distribution nature of codec tokens~\citep{DBLP:journals/icassp/lmcodec}.

### Audio-and-Text LLMs

> Audio-and-Text LLMs can process and generate both audio and text, which can be categorized into two directions. One direction uses LLMs as controllers to interface specialized audio models, such as ASR and TTS models, to enable direct audio interaction with LLMs and support various audio-and-text tasks, such as HuggingGPT~\citep{DBLP:journals/corr/abs-2303-17580} and AudioGPT~\citep{DBLP:journals/corr/abs-2304-12995}. However, these models are complex, resource-intensive, and prone to error accumulation. The second direction uses LLMs as the backbone for a unified model that handles audio-and-text tasks~\citep{DBLP:conf/acl/AoWZ0RW0KLZWQ0W22,DBLP:journals/corr/abs-2105-03070,DBLP:journals/corr/abs-2305-16107,DBLP:journals/corr/abs-2306-12925}. SpeechT5~\citep{DBLP:conf/acl/AoWZ0RW0KLZWQ0W22} and SpeechNet~\citep{DBLP:journals/corr/abs-2105-03070} perform various speech tasks with an encoder-decoder model, but they require modal-specific pre-nets and post-nets to deal with different input\&output modalities. VioLA~\citep{DBLP:journals/corr/abs-2305-16107}, AudioPaLM~\citep{DBLP:journals/corr/abs-2306-12925}, SpeechGPT~\citep{DBLP:journals/corr/abs-2305-11000}, and SpeechGen~\citep{DBLP:journals/corr/abs-2306-02207} use decoder-only Transformers to model discrete audio tokens and text tokens as a shared vocabulary, but they suffer from information loss from quantization of audio signals into discrete tokens~\citep{DBLP:journals/corr/abs-2311-04534,DBLP:journals/corr/abs-2305-18108,DBLP:journals/corr/abs-2309-07377,DBLP:journals/corr/abs-2309-10922}.

## 3.Methodology: 方法

> Figure~\ref{fig:overall} depicts the architecture of the proposed LauraGPT.
> Section \ref{sec:gpt-backbone} describes the audio encoder, the text tokenizer, and the modified GPT LM for unified audio-and-text modeling. 
> Section~\ref{sec:audio-tokenizer} elaborates the audio tokenizer.
> Section~\ref{sec:codec_vocoder} introduces an efficient one-step codec vocoder for converting audio tokens into high-quality raw waveforms.
> Section~\ref{sec:task-details} describes the multi-task fine-tuning and shows that LauraGPT provides an extensible framework for supporting more complex tasks.

### 3.1.Modified Language Model for Unifying Audio-and-Text Modeling

> For audio inputs, different from other audio-and-text LLMs using discrete tokens to represent audio inputs, we extract the log-compressed Mel spectrogram features and convert them into  \textit{continuous representations} using a Conformer-based audio encoder.
> Text inputs and outputs are tokenized using the Qwen tokenizer \cite{qwen}, which inherits the tiktoken tokenizer~\citep{tiktoken} and incorporates additional augmentations for commonly used characters and words in different languages.
> The tokenized input text undergoes embedding matrix transformation to generate dense vectors.
> The audio representations and text embeddings have the same dimension $D$.
> The Conformer-based encoder is initialized with weights from a pre-trained ASR model~\citep{gao2023funasr}. 
> Since batch normalization can lead to endless loop decoding, we replace it with layer normalization in the Conformer-based encoder (details are in Appendix~\ref{sec:normlization}).
>
> To achieve audio generation capabilities, the audio outputs are discretized into tokens using an audio tokenizer (Section~\ref{sec:audio-tokenizer}) to obtain \textit{discrete representations} and the softmax output layer is augmented with the audio tokens. 
> As a result, the weight matrix $\mathbf{W}$ in the output layer is of size $(N+M+L) \times D$ and is utilized to calculate the logits for audio and text tokens at each position, where $N$, $M$, and $L$ denote the vocabulary sizes of text, audio, and task tokens, respectively. Task tokens are used to inform the model which task should be performed. Note that in order to control the sequence length, we perform the low frame rate~(LFR) method~\citep{gao2020san-m} to downsample audio inputs to 60ms and only select the first codec group of the audio outputs.
>
> Based on the aforementioned representations, the GPT backbone is trained to model various audio and text tasks by minimizing the cross-entropy loss:

$$
    \mathcal{L}_{LM}=-\frac{1}{T_v}\sum_{j=1}^{T_v}{
    \log p_\theta\left(
    \mathbf{v}_j | \mathbf{u}_{1:T_u}, \mathbf{u}_{task},\mathbf{v}_{1:j-1}
    \right)
    }
$$

> where $\mathbf{u}$ denotes the input embeddings with a sequence length $T_u$ and $\mathbf{v}$ represents the sequence of target tokens with a length $T_v$. 
> To specify a task, a special task-related token $\mathbf{u}_{task}$ is inserted between the input embeddings and output tokens.
> Note that only the losses of outputs are taken into account, while losses on inputs and task token embeddings are masked out.
> After the final output layer, audio tokens are decoded to raw waveforms using a codec vocoder (Section \ref{sec:codec_vocoder}).
> Since it is challenging to train an LLM from scratch with limited data and computational resources, we use the open-source GPT LLM, Qwen~\citep{qwen}, as the backbone. Qwen is pre-trained on a diverse corpus covering various domains in English and Chinese and supports 8192 context length. Compared with other open-source GPT models with similar model sizes, Qwen models demonstrate impressive competitiveness, achieving better performance on widely used benchmarks, especially on Chinese tasks~\citep{qwen}.
> Within LauraGPT, all parameters including the Qwen backbone are jointly optimized, except for the codec vocoder, which is trained independently and kept frozen during both training and inference stages of LauraGPT.

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
