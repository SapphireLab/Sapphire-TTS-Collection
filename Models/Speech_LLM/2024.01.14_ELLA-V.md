# ELLA-V

<details>
<summary>基本信息</summary>

- 标题: ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering
- 作者:
  - [Yakun Song](../../Authors/Yakun_Song.md)
  - [Zhuo Chen](../../Authors/Zhuo_Chen_(陈卓).md)
  - [Xiaofei Wang](../../Authors/Xiaofei_Wang_(王晓飞).md)
  - [Ziyang Ma](../../Authors/Ziyang_Ma_(马子阳).md)
  - [Chen Xie](../../Authors/Xie_Chen_(陈谐).md)
- 机构:
  - [上海交通大学](../../Institutions/SJTU_上海交通大学.md)
  - [Microsoft](../../Institutions/Microsoft.md)
- 时间:
  - 预印时间: 2024.01.14 ArXiv v1
  - 更新笔记: 2024.06.06
- 发表:
  - 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2401.07333)
  - [Demo](https://ereboas.github.io/ELLAV/)
- 标签:
  - [语音合成](../../Tags/SpeechSynthesis.md)
  - [零样本](../../Tags/Zero-Shot.md)
- 页数: 12
- 引用: ?
- 被引: 4

</details>

## Abstract·摘要

> The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation.
> However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy.
> To alleviate these issues, we propose ELLA-V1, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level.
> The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens.
> The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies.
> The code of ELLA-V will be open-sourced after cleanups2.
> Audio samples are available at https://ereboas.github.io/ELLAV/. 

## 1.Introduction·引言

> Recently, deep generative AI has achieved remarkable results in various tasks, leading to the emergence of many transformative real-world applications (Brown et al., 2020; Ramesh et al., 2022; Ho et al., 2020; Rombach et al., 2022; Borsos et al., 2023; Kim et al., 2021; Chiang et al., 2019).
> With the advancement of generative models, there have been rapid developments in the field of speech synthesis as well.
> In particular, zero-shot TTS technology has gained increasing attention because it can synthesize high-quality target voices without the need of specified speaker’s training data.
> As a state-of-the-art generative model family, diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020;
> Song and Ermon, 2020) progressively add noise to the training data and then learn the reverse process to generate samples.
> By leveraging diffusion models and their variants (Sohl-Dickstein et al., 2015;
> Ho et al., 2020; Song and Ermon, 2020; Song et al., 2021; Lipman et al., 2023), many works have successfully applied them to the audio domain (Popov et al., 2021; Huang et al., 2022, 2023; Shen et al., 2023).
> Another major class of generative models is language modeling based on Transformer (Vaswani et al., 2017a).
> Devlin et al. (2019); Raffel et al. (2020); Lewis et al. (2020) utilize encoder-only or encoder-decoder architectures to build masked language models so that they selectively focus on relevant segments and effectively model relationships in long sequences.
> However, masked language model often requires fine-tuning to adapt to specific tasks, which can be inconvenient for practical usage and deployment.
> On the other hand, AR language models use a decoder-only architecture to predict the next token in a sequence as the training objective, which has demonstrated extremely powerful few-shot and zero-shot capabilities in many generative tasks (Brown et al., 2020; Thoppilan et al., 2022; Chowdhery et al., 2023).
> In light of this, VALL-E (Wang et al., 2023a) and subsequent works (Kharitonov et al., 2023; Rubenstein et al., 2023; Wang et al., 2023b) have successfully employed decoder-only language model for zero-shot TTS.
> These approaches first quantize the speech signal into a series of discrete acoustic tokens.
> Subsequently, they employ an AR language model to predict coarse-grained acoustic tokens, eliminating the necessity for explicit duration predictors or speaker encoders.
> Once trained on a large-scale corpus, such as LibriLight (Kahn et al., 2020), these approaches are capable of synthesizing speech with competitive fidelity and naturalness in a zero-shot manner.

> While VALL-E and its variants have achieved numerous impressive milestones, they still possess certain limitations that impact practical deployment.
> For instance, existing methods (Wang et al., 2023a; Kharitonov et al., 2023) directly concatenate phoneme tokens and acoustic tokens as a whole sequence to train language models.
> In this way, the alignment between audio and phoneme sequences is completely learned through the self-attention in the transformer, making it potentially unstable as self-attention does not explicitly capture the monotonic alignment between audio and phoneme.
> Additionally, the decoder-only language model architecture can lead to potential attention degradation issues (Fu et al., 2023), where the alignment quality between the target audio sequence and the source phoneme sequence deteriorates as the generated sequence increases, resulting in inaccurate or low-quality speech outputs.
> Another limitation stems from the nature of AR language modeling.
> Specifically, given a sequence x, the standard AR language model factorizes the likelihood p(x)over the dimensions of x via the chain rule p(x) = QTt=0p(xt|x<t).
> AR models predict the current tokens solely based on the historical tokens without users’ control in the inference process, and sometimes generate semantic repetitions or incoherence in the generated output (Yang et al., 2019; Brown et al., 2020).
> In the TTS task, correspondingly, VALL-E cannot directly determine which segment of the output audio corresponds to which prompt phoneme, thus there is no trivial way to promptly detect and prevent issues occurring in the generation process.
> These drawbacks can manifest as meaningless phoneme repetitions, transpositions, omissions, or even catastrophic infinite silence, i.e., during the process of generation, the model anomalously outputs silence or noise tokens for an extended period of time without stopping.
> Specifically, Tab.01 demonstrates the word error rate (WER) and the probability of the infinite silence in VALL-E samples at different threshold top-p for nuclear sampling (Holtzman et al., 2019).
> The detailed experimental setup is described in Section 4.
> Notably, a shift in the decoding strategy of VALL-E from fully sampling-based to fully greedy-based leads to a marked decline in sample quality.
> It should be emphasized that while sampling-based stochastic decoding strategies have advantages in terms of synthesis diversity, deterministic decoding strategies (e.g., beam search and its variants) are more suitable for cases where there is less tolerance for synthesis errors and more emphasis on fluency and coherence (Ippolito et al., 2019).

> Faced with the pros and cons of the existing methods, we introduce ELLA-V, a simple but effective language model approach for zero-shot TTS.
> ELLA-V proposes a generalized AR (GAR) language model to generate the first layer of residual vector quantizer (RVQ) codes of a neural codec model.
> Then as with VALL-E, ELLA-V employs a non-autoregressive (NAR) language model to obtain codes of the other RVQs.
> Our core innovation lies in 3 fold:
> - Firstly, ELLA-V inserts phone tokens into the corresponding positions of the acoustic sequence.
> Unlike existing methods, Connecting phoneme tokens with their corresponding acoustic tokens can help the language model capture the alignment between phoneme and acoustic modalities in local dependencies.
> - Secondly, instead of maximizing the expected log-likelihood of the hybrid sequence under a conventional casual mask or a prefix mask like VALL-E and UniLM (Bao et al., 2020),
> ELLA-V computes loss only on acoustic tokens and special tokensEndOfPhone( EOP ) andEndOfSentence(EOS).
> This training objective not only reduces the redundant computation of cross-modal alignment in the output based on experimental results, but also provides a natural way to have fine-grained control in inference: the model predicts EOP , and then the user provides the next phone token.
> Meanwhile, ELLA-V’s GAR model always maintains awareness of the phoneme it is currently synthesizing, allowing it to promptly detect and truncate any abnormal phoneme to avoid any possible infinite silence issue.
> - Thirdly, we further propose an improvement to the input sequence.
> We introduce local advance, which involves shifting the EOP token and the next-word phoneme token a few frames ahead.
> Intuitively, the pronunciation of a phoneme, especially its ending, is not only influenced by the context in history but also by the upcoming phonemes.
> By advancing these special tokens, the GAR model can better utilize local dependencies to predict the pronunciation of the current phoneme.

> Experimental results, using comparable model configurations and 960 hours of speech data from
> LibriSpeech (Panayotov et al., 2015) as a training set, demonstrate the superiority of ELLA-V.
> Compared to the state-of-the-art zero-shot TTS system VALL-E, ELLA-V significantly improves the accuracy of synthesized speech, and demonstrates comparable or superior speaker similarity and speech naturalness on a series of subjective and objective experiments.
> ELLA-V achieves a
> WER of 2.28% on the test-clean set of LibriSpeech.
> Notably, ELLA-V works well on a wide spectrum of decoding strategies – even greedy decoding, and still has a substantially better speech accuracy than the best of VALL-E.
> We further conducted ablation experiments to investigate the effects of our proposed modifications.
> The results indicate that the global advance in ELLA-V significantly improves the model’s performance, while the local advance enhances the stability of the generated output.

## 2.Related Work·相关工作

### Language Modeling·语言建模

> Recently, language models have garnered increasing interest in both the academic and industrial communities.
> Compared to models that are confined to specific tasks, language models have been proven to possess the capability to solve a wide array of tasks, shining across various domains such as text (Brown et al., 2020;
> Chowdhery et al., 2023; Rae et al., 2021; Yu et al., 2022), images (Alayrac et al., 2022; Tsimpoukelli et al., 2021), and videos (Yang et al., 2022; Wang et al., 2022).
> In the audio domain, AudioLM (Borsos et al., 2023) trains language models on discretized audio tokens, achieving speech synthesis tasks through hierarchical prediction of these tokens.
> AudioGen (Kreuk et al., 2023) employs an auto-encoding approach to extract discrete encodings of raw audio, and trains a language model conditioned on textual features for controlled audio generation.
> LM-VC (Wang et al., 2023d) employs three language models—a masked prefix language model, an external LM, and a prefix LM—to achieve zero-shot voice conversion.Kakouros et al. (2023) investigates the role of word surprisal, extracted from language models, in influencing the prosody of speech synthesized by TTS systems.
> For zero-shot TTS, Wang et al. (2023a) approaches TTS as a conditional language modeling task rather than a continuous signal regression.
> By employing discrete audio codes obtained from pre-trained neural codec, it trains a discrete audio language model, achieving improved naturalness in speech and preservation of speaker characteristics.
> VALL-E-X (Zhang et al., 2023) extends
> VALL-E by utilizing source language speech and target language text as prompts when predicting the acoustic marker sequence of the target language speech.
> This approach supports high-quality zero-shot cross-lingual voice synthesis.
> These methods require only a single utterance of an unknown speaker as a prompt to generate high-quality, specified speech.

### Speech Synthesis·语音合成

> Speech synthesis has long been a significant topic in the fields of artificial intelligence, natural language processing, and speech processing.
> Early methods were based on Statistical Parametric Speech Synthesis (SPSS) (Zen et al., 2009), typically involving complex components such as text analysis models, acoustic models, and vocoders (e.g., hidden Markov model(HMM) (Yoshimura et al., 1999) based).
> While cost-effective in terms of data, the generated speech of SPSS still exhibited noticeable differences from natural human speech.
> With the advancement of modern neural networks, some work initially replaced HMMs with recurrent neural networks (RNNs) but still followed the SPSS paradigm (Fan et al., 2014; Zen and Sak, 2015;
> Valentini-Botinhao et al., 2016).
> Later, end-to-end neural TTS models were introduced, which synthesize Mel spectrograms and employ a vocoder (Oord et al., 2017; Prenger et al., 2019) for speech synthesis (Wang et al., 2017; Arık et al., 2017; Ren et al., 2019).
> Some methods, utilizing techniques such as
> VAE (Hsu et al., 2019; Lee et al., 2022), flow (Miao et al., 2020; Kim et al., 2020), diffusion (Jeong et al., 2021; Kim et al., 2022; Popov et al., 2021), and others (Wu and Shi, 2022), have achieved promising performance in end-to-end speech synthesis.On the other hand, models like VALL-E (Wang et al., 2023a) and AudioLM (Borsos et al., 2023) utilize autoregressive Transformers to model discrete audio tokens, achieving great in-context learning performance.When it comes to zero-shot speech synthesis, autoregressive Transformer-based models can predict and generate audio without the need for an additional duration model, which strikes a favorable balance between efficiency and performance, and has been garnering increasing attention.

## 3.Method·方法

### 3.1.Overview·概览

> Fig.01 demonstrates the overall architecture of
> ELLA-V.
> ELLA-V primarily follows a two-stage framework similar to VALL-E, considering zero-shot TTS as a conditional codec language modeling task.
> ELLA-V maps input text prompts and speech prompts into a unified vocabulary space with a text encoder and a neural codec, respectively.
> Different from VALL-E, an additional sequence order rearranging step is performed to the text-audio token sequence, after which, ELLA-V utilizes a decoder-only language model to learn to perform conditional generation on the hybrid sequences of phoneme and audio tokens.
> Detailed information about the language model will be presented in Section 3.2.
> To obtain discrete audio representations, we employ a pre-trained neural audio codec model, EnCodec (Défossez et al., 2023), following VALL-E (Wang et al., 2023a).
> EnCodec transforms 24 kHz raw waveforms into 75 Hz discrete tokens usingLRVQ layers.
> The discrete acoustic tokens have a hierarchical structure, where the first layer quantizer contains semantic information and coarse-grained acoustic contours, while subsequent
> L − 1quantizers learn fine-grained acoustic details.
> In our experiments, we use the same settings as
> VALL-E, withL = 8.
> For each quantizer, we set the codebook size to 1024.
> In this setting, each second of the waveform is represented by75 × 8 discrete tokens from RVQ.

> To obtain phoneme sequences, we apply the
> Montreal Forced Aligner (MFA) (McAuliffe et al., 2017) to the input audio and text transcriptions.
> Notably, MFA not only serves as a text tokenizer but also extracts alignment relationships between phonemes and the corresponding speech.
> The forced alignment information is essential for
> ELLA-V to change sequence order.In Section 3.2, we will provide a detailed explanation of how this information is used to construct the target sequence.

### 3.2.Training: Codec Language Model·训练：编解码语言模型

> ELLA-V employs a Generalized Autoregressive
> Codec language model for the prediction of the first quantization layer in the EnCodec, which corresponds to capturing semantic information and coarse-grained acoustic profiles.
> Subsequently, a non-autoregressive language model is utilized to generate codes for the subsequent quantization layers, aimed at reconstructing fine-grained acoustic details.
> Specifically, given a speech corpus
> D = {xi, yi}, wherexrepresents an audio sample, andyis its text transcription.
> We utilize the
> EnCodec to extract the discrete representation ofx, formulated as
 whereCrepresents the two-dimensional acoustic code matrix, andTis the downsampled utterance length.
> We employ MFA to obtain the phoneme sequenceP1:ncorresponding to the transcriptiony, while also extracting forced alignment information between the audio x and the transcription y:
 wherenis the number of phonemes of the audio samplex, andlidenotes the length of thei-th phoneme of the discrete audio sequence.
> MFA treats silence also as a kind of phoneme, so that the original audio sequence is partitioned intonconsecutive intervals corresponding tonphonemes.
> Specifically, let⟨Ci⟩li×8represent the audio sequence corresponding to the i-th phoneme:

> After quantization, we utilize the EnCodec decoder to reconstruct the audio waveform from the discrete acoustic sequenceC, formulated as
ˆx ≈ DeCodec(C).
> For the zero-shot TTS task, the optimization objective is max p(C|P, ˆC), whereˆCis the acoustic prompt of the unseen speaker.
> We use language modeling to generate acoustic tokens for the unseen speaker, by learning on the mixed sequence composed of phonemes and codec codes, consistent with previous works (Wang et al., 2023a; Rubenstein et al., 2023).
> Unlike existing approaches, ELLA-V does not concatenate phoneme tokens and acoustic tokens directly to form the target sequence for training the language model.
> Instead, ELLA-V interleaves phoneme and acoustic tokens in order to make it easier for language models to learn the alignment between audio and text.
> Specifically, we insert each phoneme tokenPi(except the silence phoneme) into the corresponding position of the audio sequence, so that each phoneme’s audio⟨Ci⟩ is sandwiched between Pi and EOP tokens.
> We also prepend the phoneme sequence to the beginning of the mixed sequence, which is referred to as global advance.
> In Section 3.4, we further propose a variant sequence order with higher generation stability, named local advance, which moves the non-acoustic tokens of the sequence several frames forward.

#### 3.2.1.
> Generalized Autoregressive Codec (GAR) Codec Language Model·通用自回归编解码语言模型

> As shown in Fig.02, ELLA-V first constructs a hybrid sequenceH:,1of acoustic and phoneme tokens, structured as:

> It is worth noting that the MFA (Montreal Forced
> Aligner) treats silence as a distinct phoneme, whereas our phoneme sequencePexclusively comprises phonemes other than silence.
> To clarify, we retain the acoustic component associated with silence but do not sandwich it with an EOP and a specific silence phoneme, nor do we use a silence phoneme in the global advance part.
> We design a GAR language model to learn the continuation task on the aforementioned hybrid sequence, to generate the discrete acoustic code sequenceC:,1.
> The GAR model consists of multiple
> Transformer decoder layers (Vaswani et al., 2017b).
> After training, it can generate discrete audio codes for a specified text prompt and acoustic prompt.
> GAR is also responsible for predicting EOP and
> EOSto indicate the conclusion of a phoneme and the entire sentence, respectively.
> The optimization of GAR is achieved by maximizing the likelihood of the acoustic partC:,1of the hybrid sequenceH:,1, as well as the special EOP andEOStokens.
> Under forward factorization, this process is formulated as:

> where H has a size ofTH× 8,{P}denotes the phoneme set,�˜Ci�is the concatenation of ⟨Ci⟩ along with its broadcast trailing EOP and/or
> EOStokens,˜Cis then the concatenation of⟨Ci⟩, and θ GAR represents neural network parameters of GAR model.The factorization of the training objective naturally encapsulates the core intuition of the GAR model: GAR generates the audio sequence phoneme-by-phoneme.
> GAR produces maximum likelihood predictions for each phoneme token successively, indicating the end of generating a specified phoneme by predicting EOP .
> Through global advancement, GAR can directly infer the next phoneme to be generated without relying on network predictions.
> After the prediction for the last phoneme is completed, GAR stops the generation process by predictingEOS.
> The generated sequence by GAR is self-aligned, as it can instantly know the corresponding position of any generated acoustic token in relation to the phoneme prompt.
> During training, we apply a bidirectional mask to the phoneme sequence before the BOS in the hybrid sequence, while a unidirectional mask is used for the part after BOS .
> We frame the training as a next-token-prediction language modeling task on the hybrid sequence.
> However, it’s important to note that the model does not predict phonemes (or BOS).
> In other words, as shown in Fig.02, we only compute loss when the token to be predicted is not a phoneme (or BOS).
> During inference, whenever the model predicts an EOP for a phoneme, the next phoneme token is directly appended to the end of the sequence, which will be further discussed in
> Section 4.

#### 3.2.2.Non-Autoregressive (NAR) Codec Language Model·非自回归编解码语言模型

> In the second stage, the NAR language model is employed to predict the codes from the second to the last quantization layers in parallel.
> The input-output sequence construction of the NAR model follows the same pattern as used in the GAR model discussed in Section 3.2.1.
> Specifically, the i-th columnH:,iof the hybrid sequence matrixHis structured as:

> And in practice if Pi represents the silence,C:,i will not be sandwiched by Pi and EOP .
> The NAR model takes the previously generated hybrid sequence of the previous j − 1layers as input and predicts the codes of the j-th layer in parallel, formulated as:

> where {C:,j} denotes the acoustic token set of the j-th quantizer.
> In this formulation, The embeddings of tokens from the previous j − 1quantizers are summed up to feed the NAR model to predict the j-th layer.
> Intuitively, both the GAR and
> NAR model of ELLA-V compute the loss on the acoustic tokens of the target sequence, and GAR additionally computes loss for EOP and EOS.


### 3.3.Inference·推理

> ELLA-V can use a short clip of speech from an unseen speaker as an acoustic prompt to synthesize speech for a specified text prompt.
> Fig.03 illustrates the inference process of the GAR model.
> While VALL-E may get stuck in an infinite loop during inference, resulting in the synthesis of either infinite silence or repetitive pronunciation (Wang et al., 2023a), ELLA-V is capable of generating EOP and promptly truncating abnormally long phonemes.
> Following an EOP , we can directly append the next phoneme token to the end of the generated sequence, ensuring the proper generation of speech without abnormal pauses or repetitions.
> For the GAR model, we employ a sampling-based decoding strategy, whereas for the NAR model, we use a greedy decoding approach to strike a balance between efficiency and performance.

### 3.4.Local Advance·局部进步

> One intuition is that the pronunciation of a phoneme is strongly related to the pronunciation of the phonemes just before and after it.
> However, due to the autoregressive nature of the GAR model, an acoustic token cannot attend to the following phoneme tokens, even though we can leverage the transformer’s ability to model long-term dependencies through global advance to provide complete context for the acoustic token generation.
> To further harness the powerful capability of the transformer in modeling local dependencies, ELLA-V introduces an additional change in the sequence order based on Section 3.2.
> Specifically, we move the phoneme token and the EOP token ahead by a few frames, referred to as local advance.


## 4.Experiment·实验

### 4.1.Experi-mental Setup·实验设置

#### Data & Tasks·数据与任务

> We trained ELLA-V using the
> Librispeech (Panayotov et al., 2015) 960h training dataset.
> We utilized Montreal Forced Aligner (MFA) (McAuliffe et al., 2017) to obtain forced alignment information for the audio-transcription pairs.
> Sentences with unrecognized or unknown phones by MFA were excluded.
> The open-source 24kHz checkpoint3of EnCodec(Défossez et al., 2023) was used as the codec to generate discrete acoustic tokens.
> The LibriSpeech training data was upsampled to 24 kHz before feeding it into
> EnCodec.
> In evaluating the model, two zero-shot TTS tasks were considered.
> For the zero-shot TTS continuation task, we adhered to methodologies established by previous works (Wang et al., 2023a; Le et al., 2023; Wang et al., 2023c), selecting examples ranging from 4 seconds to 10 seconds from the LibriSpeech test-clean dataset as our test set.
> In this task, we used the complete phoneme transcription as the text prompt and the first 3 seconds of the test audio sample as the acoustic prompt.
> The model was required to generate continuations.
> For the zero-shot TTS cross-speaker task, we designed a hard case set comprising 100 hard sentences, as outlined in the demo page .
> These sentences included challenging phonetic patterns, alliteration, and unusual (abnormal) combinations of words that might pose difficulties for a TTS system to generate natural-sounding speech.
> In this case, we randomly picked 3-second sentences from the LibriSpeech test-clean subset as the acoustic prompt.
> We then concatenated the transcription of this segment and the target phoneme sequence in the hard case set to form the text prompt.
> The model was tasked with cloning the voice of the speaker to say the specified target text in the hard case set.

#### Training Configuration·训练配置

> For both GAR and
> NAR models, we stacked 12 Transformer decoder layers with an embedding dimension of 1024, a hidden state dimension of 1024, and a feed-forward layer dimension of 4096.
> All models were trained in parallel using 8 NVIDIA Tesla V100 GPUs with a batch size of 16384 tokens for GAR and 12288 tokens for NAR per GPU, respectively, learning a total of 320k steps.
> We used the AdamW optimizer with β1= 0.9,β2= 0.999,ϵ = 10−9.
> We employed an inverse-sqrt learning rate scheduler with warm-up.
> For the first32000updates, we linearly increased the learning rate from10−7to a peak of 5 × 10−4.
> The weight decay was 0.01.

#### Baseline·基线

> In our research, we benchmarked the performance of zero-shot speech synthesis against
> VALL-E (Wang et al., 2023a).
> This system was originally trained on a substantial 60k hours of audio from the Librilight dataset (Kahn et al., 2020).
> To ensure a rigorous evaluation, we reproduced the
> VALL-E model and adapted it to train on the LibriSpeech 960h dataset.
> We also adjusted the model dimensions and the number of layers to match the parameter settings of ELLA-V and VALL-E.
> Both
> GAR (or AR) and NAR models of VALL-E and
> ELLA-V have 154.3M parameters.
> Moreover, to mitigate any potential bias introduced by the audio codec, we pre-processed the authentic speech samples using EnCodec’s encoder and decoder.
> We also include the result for Encodec reconstructed speech for reference, denoted as Ground-Truth Encodec.

#### Evaluation Metrics·评估指标

> We evaluated our system with several objective metrics.
> Speaker similarity (SPK) and WER served as our primary measures.
> SPK was assessed using the fine-tuned WavLMTDNN model4(Chen et al., 2022), scoring similarity on a scale of -1 to 1, with values above 0.86 indicate the same speaker identity (This value comes from the release model card page).
> The WER was determined by comparing the synthesized speech to the original text using the Conformer-Transducer model5(Gulati et al., 2020).
> In addition to these standard metrics, we introduced two novel measures: INF% and CUT%.
> INF% quantified the frequency of generating infinitely long audio, indicative of a failure in synthesis.
> It is used to measure the likelihood of the model falling into abnormal repetition (such as infinite silence).
> A higher INF% indicates poorer stability in the generated output of the model.
> In the practical implementation, INF% referred to the proportion of sentences for which generation was not stopped when the length of the generated audio reached twice the original, serving as a proxy for infinite generation.
> On the other hand, as discussed in the previous session, the design of ELLA-V enables the control of the duration for each phoneme during inference, thus avoiding the synthesis failure.
> In our experiments, we forcibly truncate the synthesis of phonemes with a length greater than 0.4 seconds.
> CUT% is used to measure the frequency of forced cuts of phonemes in synthesis by
> ELLA-V.
> For each objective metric, we reported average values over three experimental runs with different random seeds.
> For subjective analysis, we relied on the mean opinion score (MOS). 30 test samples were chosen for this purpose, with each sample being evaluated by at least 15 listeners for aspects like naturalness and speaker similarity.
> The comparative mean option score (CMOS) and the similarity mean option score (SMOS) were the key subjective metrics used.
> SMOS was rated on a 1 to 5 scale, in 0.5point increments, to gauge speaker similarity, while
> CMOS, ranging from -1 to 1, assessed the overall naturalness and quality of the synthesized speech against the baseline.

### 4.2.Results·结果

#### Zero-Shot TTS Continuation Task·零样本TTS续写任务

> We present the evaluation results in Tab.02, where a comparison between ELLA-V and VALL-E is shown.
> First, regarding speaker similarity, both subjective (SMOS) and objective (SPK) results indicate that
> ELLA-V and VALL-E performed similarly, which can be attributed to their shared backbone approach, combining (G)AR and NAR.
> Meanwhile, CMOS testing shows that ELLA-V achieved a +0.10 score, demonstrating a higher generation quality (i.e., naturalness) compared to VALL-E.
> Additionally,
> WERs calculated between the recognized text of synthesized audio and the ground-truth text show that ELLA-V is significantly better than VALL-E (2.28 versus 5.00).
> This underscores ELLA-V’s enhanced capability in synthesizing higher-quality and more robust speech.
> Overall, ELLA-V substantially improved the synthesis accuracy and robustness of the language model-based TTS framework without affecting the naturalness and speaker similarity.
> This conclusion is not only corroborated by this easy continuation task, but also validated via the challenging synthesis sets in the subsequent section.

#### Zero-Shot TTS Cross-Speaker Task on hard cases·零样本TTS跨说话任务(困难案例)

> VALL-E utilized a traditional AR model that frequently resulted in alignment errors, including repetitions, transpositions, and omissions, particularly in more challenging synthesis cases (see Section 4.1 for details of the challenging synthesis set).
> Tab.03 presents the WER comparison of VALL-E and ELLA-V on the 100 particularly hard synthesis sentences.
> In contrast to VALL-E, ELLA-V demonstrates markedly lower WER, signifying its enhanced robustness.
> This substantial reduction in errors translates to more accurate and reliable voice synthesis applications, significantly improving user experience in real-world scenarios.
> Regarding VALL-E’s tendency to fall into infinite silence, an intuitive explanation is that the silence patterns in the training data are relatively simple and many of them are repetitive.
> In this case, a traditional language model is prone to overfitting to these patterns.
> During testing, when the model encounters silence, it assigns a high probability to silence.
> This leads to issues such as beam search, which is based on maximum likelihood, getting stuck in a loop.
> However, ELLA-V does not face this problem.

#### Analysis of Decoding Strategies·解码策略分析

> To demonstrate the stability of ELLA-V under different decoding strategies, we conducted an ablation study, testing the decoding performance with different top-p values for nuclear sampling, by varyingp ∈ {1, 0.99, 0.95, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0(greedy)}.The results are shown in Fig.05.
> We can observe that as top_p decreases, the accuracy of VALL-E’s synthesized speech significantly decreases.
> At this point, VALL-E is more prone to generating a large number of overfit silence tokens, leading to a significant increase in INF%.
> And compared to VALL-E, the audio synthesized by ELLA-V is less sensitive to rate changes in the top_p sampling strategy, whose WER consistently outperforms VALL-E.
> When the local advance is set to 5 or 10 tokens, the generated audio exhibits significant stronger robustness.
> On the other hand, as shown in Fig.05 (right), as top_p decreases, VALL-E tends to get stuck in infinite loops of failed generation, while the generation of ELLA-V remains significantly stable.
> Moreover, ELLA-V can promptly handle (truncate) the synthesis of exceptional phonemes, resulting in significantly higher robustness.

#### Ablation Study·消融实验

> In this paragraph, we conduct ablation experiments. (1) To investigate the impact of global phoneme information on synthesized speech, we removed the global phoneme sequence at the beginning of the trained sequence (abbr.
> ELLA-V-noglobal). (2) To investigate whether it is necessary to provide the specific phoneme token before its corresponding acoustic tokens during both training and inference, rather than just using the EOP separator, we removed all phoneme tokens following BOS in the mixed sequence (abbr. ELLA-V-nophn).
> The experimental results are shown in Tab.04.
> It is observed that the accuracy of synthesized speech significantly deteriorated either when global phoneme tokens were not used or when local phoneme tokens were disabled within the hybrid sequence.
> It is also notable that even in the absence of global advance (i.e., in the
> ELLA-V-no global configuration), the SPK and
> WER of the synthesized audio were comparable to those of VALL-E.
> These findings indicate the importance of both local and global information in achieving more accurate synthesized audios, meanwhile, combining both of them potentially leads to further enhancements in accuracy.

## 5.Conclusion·结论

> In this paper, we introduce ELLA-V, a simple and efficient two-stage zero-shot TTS framework based on language modeling.
> By learning interleaved sequences of acoustic and text tokens, our proposed
> GAR model can provide fine-grained control over synthesized audio at the phoneme level and can better leverage local dependencies to predict the pronunciation of the current phoneme.
> Experimental results demonstrate that ELLA-V achieves higher accuracy and more stable results under different threshold top-p for nuclear sampling.
> We aspire for this work to advance research in enhancing the robustness of speech generation.