# Mega-TTS

<details>
<summary>基本信息</summary>

- 标题: Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias
- 作者:
  - 01 [Ziyue Jiang](../../Authors/Ziyue_Jiang.md)
  - 02 [Yi Ren](../../Authors/Yi_Ren_(任意).md)
  - 03 [Zhenhui Ye](../../Authors/Zhenhui_Ye.md)
  - 04 [Jinglin Liu](../../Authors/Jinglin_Liu_(刘静林).md)
  - 05 [Chen Zhang](../../Authors/Chen_Zhang.md)
  - 06 [Qian Yang](../../Authors/Qian_Yang.md)
  - 07 [Shengpeng Ji](../../Authors/Shengpeng_Ji.md)
  - 08 [Rongjie Huang](../../Authors/Rongjie_Huang_(黄融杰).md)
  - 09 [Chunfeng Wang](../../Authors/Chunfeng_Wang.md)
  - 10 [Xiang Yin](../../Authors/Xiang_Yin.md)
  - 11 [Zejun Ma](../../Authors/Zejun_Ma.md)
  - 12 [Zhou Zhao](../../Authors/Zhou_Zhao_(赵洲).md)
- 机构:
  - [浙江大学](../../Institutions/CHN-ZJU_浙江大学.md)
  - [字节跳动](../../Institutions/CHN-ByteDance.md)
- 时间:
  - 预印时间: 2023.06.06 ArXiv v1
  - 更新笔记: 2024.07.07
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2306.03509)
  <!-- - [DOI]() -->
  <!-- - [Github]() -->
  - [Demo](https://mega-tts.github.io/demo-page)
  - [Scholar](https://scholar.google.com/scholar?cluster=15692405188854768212)
- 标签:
  - [语音合成](../../Tags/SpeechSynthesis.md)
  - [零样本](../../Tags/Zero-Shot.md)
  - [GAN](../../Tags/Model_GAN.md)
  - [语言模型](../../Tags/LanguageModel.md)
- 页数: 20
- 引用: 67
- 被引: 33
- 数据:
  - 训练: [WeNetSpeech](../../Datasets/2021.10.07_WenetSpeech.md)
  - 训练: [GigaSpeech](../../Datasets/2021.06.13_GigaSpeech.md)
  - 评估: [VCTK](../../Datasets/2012.08.00_VCTK.md)
  - 评估: [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md)
  - 评估: [AISHELL-3](../../Datasets/2020.10.22_AISHELL-3.md)
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Scaling text-to-speech to a large and wild dataset has been proven to be highly effective in achieving timbre and speech style generalization, particularly in zero-shot TTS.
> However, previous works usually encode speech into latent using audio codec and use autoregressive language models or diffusion models to generate it, which ignores the intrinsic nature of speech and may lead to inferior or uncontrollable results.
> We argue that speech can be decomposed into several attributes (e.g., content, timbre, prosody, and phase) and each of them should be modeled using a module with appropriate inductive biases.
> From this perspective, we carefully design a novel and large zero-shot TTS system called ***Mega-TTS***, which is trained with large-scale wild data and models different attributes in different ways: 
> 1) Instead of using latent encoded by audio codec as the intermediate feature, we still choose spectrogram as it separates the phase and other attributes very well.
> Phase can be appropriately constructed by the GAN-based vocoder and does not need to be modeled by the language model. 
> 2) We model the timbre using global vectors since timbre is a global attribute that changes slowly over time. 
> 3) We further use a VQGAN-based acoustic model to generate the spectrogram and a latent code language model to fit the distribution of prosody, since prosody changes quickly over time in a sentence, and language models can capture both local and long-range dependencies.
> We scale ***Mega-TTS*** to multi-domain datasets with 20K hours of speech and evaluate its performance on unseen speakers.
> Experimental results demonstrate that ***Mega-TTS*** surpasses state-of-the-art TTS systems on zero-shot TTS, speech editing, and cross-lingual TTS tasks, with superior naturalness, robustness, and speaker similarity due to the proper inductive bias of each module.
> Audio samples are available at https://mega-tts.github.io/demo-page.

## 1.Introduction: 引言

> Text-to-speech (TTS) synthesis [53, 2, 49, 35, 48, 45, 29, 66, 43, 28] aims to generate human-like speech from text and has gained significant attention in the field of machine learning.
> Traditional TTS systems [13, 11, 60, 8, 21] are usually trained on limited datasets, which impairs their models’ ability to produce diverse and generalizable results.
> In contrast, large-scale TTS systems [58, 67, 27] are trained on tens of thousands of hours of speech data, which significantly improves their zero-shot capability [58, 67].
> Current large-scale TTS systems typically encode the speech waveform into latent with neural codec models [14] as the intermediate representation and model it with autoregressive language models (LM) [58] or diffusion models [50].

> As presented in Table 1, human speech can be decoupled into several attributes: content, timbre, prosody, phase, etc.
> However, current large-scale TTS systems directly use neural audio codec models to encode the entire speech into latent and ignore the following intrinsic nature of speech: 
> 1) phase is highly dynamic and irrelevant to semantics, which means people are much less sensitive to perceive phase than to prosody and timbre, especially for monaural audio.
> Therefore, only one reasonable phase is needed for waveform reconstruction, and it is not necessary to model all possible phases.
> Modeling phase with LM or diffusion model can waste a lot of model parameters since they model the full distribution of phase4. 
> 2) Timbre should remain stable within the sentence as a global vector.
> Modeling timbre with time-varying latent is costly5. 
> 3) Prosody typically has both local and long-term dependencies and changes rapidly over time with a weak correlation to text, which makes conditional phoneme-level LLMs inherently ideal for generating prosody sequences. 
> 4) Content has monotonic alignment with speech while the autoregressive language model cannot guarantee that, which can lead to repeating or missing word issues [59, 58, 67].

> To make use of the large and wild training datasets while matching the inductive bias of the model and the intrinsic nature of speech, we propose a zero-shot text-to-speech model called ***Mega-TTS***.
> Specifically, 
> 1) considering the limitations of neural audio codec models, we select mel-spectrogram as the intermediate representation to separate the phase and other attributes.
> We adopt a GAN-based vocoder to reconstruct the phase information to improve our model’s efficiency. 
> 2) To model timbre information, we employ global vectors since timbre is a global attribute that changes slowly over time.
> We extract the global information from a different speech of the same speaker with the global speaker encoder to decompose the timbre and content information. 
> 3) To capture prosody information in a sentence, we adopt a VQGAN-based acoustic model to generate the mel-spectrogram and a latent code language model called P-LLM to fit the distribution of prosody.
> The P-LLM is capable of capturing both local and long-range dependencies for prosody modeling.

> To evaluate the zero-shot performance of ***Mega-TTS***, we perform experiments on VCTK [57], AISHELL-3 [51] and LibriSpeech test-clean [42] datasets.
> All of the test speakers are unseen in the training corpus.
> Our ***Mega-TTS*** surpasses the state-of-the-art zero-shot TTS systems [8, 58] in terms of speaker similarity, speech naturalness, and generation robustness, which demonstrates the superiority of introducing appropriate inductive biases.
> Moreover, ***Mega-TTS*** outperforms state-ofthe-art models on speech editing [52, 3] and cross-lingual TTS [67] tasks.
> The main contributions of this work are summarized as follows:
> - We propose ***Mega-TTS***, a zero-shot text-to-speech system that considers intrinsic inductive biases.
> Instead of using latent encoded by audio codec as the intermediate representation [64, 14, 58], we decompose mel-spectrogram into content, timbre, prosody, and phase attributes and model each of them according to their intrinsic properties.
> - We train ***Mega-TTS*** on a multi-domain and multi-lingual dataset that contains 20k hours of speech data.
> It is worth noting that existing large-scale TTS systems [58, 50] are typically trained with speech corpora from audiobooks, while our system is trained on multi-domain speech corpora.
> - We evaluate ***Mega-TTS*** on 3 down-stream speech generation tasks (i.e., zero-shot TTS, speech editing, and cross-lingual TTS), demonstrating that ***Mega-TTS*** can be applied to various speech generation tasks.
> We also propose a novel sampling strategy for speech editing via the discrete prosody tokens extracted by ***Mega-TTS***.

## 2.Related Works: 相关工作

> In this section, we briefly overview the background of this work, including zero-shot text-to-speech (TTS) and generative models for speech synthesis.

> ### Zero-shot text-to-speech. 
> Text-to-speech models usually generate mel-spectrogram from text [59, 2, 35, 48, 29, 47, 36, 22] and then synthesize speech waveform from the generated mel-spectrogram using a separately pre-trained vocoder [41, 31, 62, 20], or directly generate waveform from text in an end-to-end manner [45, 15, 30, 37].
> For decades, the increasing demand for personalized speech generation in various applications has posed challenges for TTS models [53], especially in zero-shot multi-speaker scenarios regarding domain shifts.
> Previous approaches can be categorized into speaker adaptation [13, 11, 60, 23] and speaker encoding [25, 1, 26, 61] methods.
> Traditional works are typically trained on small datasets [11, 23, 21, 8], while some recent works [4, 58, 27, 67] are trained on large-scale datasets and demonstrate the effectiveness in zero-shot scenarios.
> These systems utilize the neural audio codec models [64, 14] to convert audio waveform into latent and consider it as the intermediate representation for speech generation.
> Among them, SPEAR-TTS [27] splits the TTS task into two sequence-to-sequence tasks, which enables the training using abundant audio-only data.
> NaturalSpeech 2 [50] uses a text-conditioned diffusion model to generate the latent vectors of the neural audio codec model.
> VALL-E [58, 67] proposes the first neural codec language model for text-to-speech, exhibiting strong in-context learning abilities to overcome challenges in zero-shot speech generation.
> However, these methods ignore the intrinsic property of speech and may lead to inferior or uncontrollable results (e.g., word skipping, repeating, and collapse [58, 67]).
> Considering the nature of different speech attributes, the autoregressive language model is ideally suitable for prosody modeling.
> ProsoSpeech [46] has proposed to improve the prosody modeling for TTS with latent prosody vectors predicted by a language model.
> Nevertheless, it lacks the in-context learning capacity, which restricts its application scenarios.

> ### Generative models for speech synthesis. 
> Generative models, like language models [4, 33], VAE [34, 47], GAN [31, 30], Normalizing flow [39, 29], and diffusion model [32, 24, 43, 22], have been applied to speech or audio synthesis for years.
> Previous works of autoregressive generative model mainly aim at waveform generation [41, 18] and continuous acoustic feature generation [59, 49].
> Recently, speech generation systems like AudioLM [4] and VALL-E [58] propose to utilize neural audio codec models [64, 14] to convert audio waveform into discrete codes as the intermediate representation and design LLMs to generate these codes to achieve speech synthesis.
> Although good reconstruction quality can be achieved by neural audio codec models, they ignore the intrinsic nature of speech [14] and may not be suitable to serve as the generator of intermediate representation for speech generation.
> The encoded latent contains the phase, content, and timbre attributes and language models are not suitable for predicting these due to the error propagation problem.

## 3.Methodology: 方法

<details>
<summary>原文</summary>

> To introduce proper inductive biases into large-scale TTS systems, we propose ***Mega-TTS***, a zero-shot TTS system for natural and robust speech generation in various scenarios (i.e., zero-shot prompt-based TTS, speech editing, and cross-lingual TTS).
> As shown in Figure.01, ***Mega-TTS*** consists of a VQGAN-based [16] TTS model and a prosody large language model (P-LLM).
> We carefully model different speech attributes in different ways.
> First, we choose the mel-spectrogram as the intermediate representation as it separates the phase from other attributes very well.
> Secondly, we extract the global vector from the random previous sentence of the same speaker with the global timbre encoder to disentangle the timbre and content information.
> Finally, we further use a VQGAN-based acoustic model to generate the mel-spectrogram and propose a latent code language model called P-LLM to fit the distribution of prosody, since language models are capable of capturing both local and long-range dependency.
> During inference, we propose to use the content from the given text sequence, the timbre extracted from the prompt speech, and the prosody predicted by our P-LLM to generate the target speech, which is a novel TTS decoding mechanism called prosody-oriented speech decoding.
> Finally, to demonstrate that our model can be applied to various scenarios, we design inference strategies for downstream tasks.
> We describe these designs and the training and inference procedures in detail in the following subsections.

</details>
<br>

为了向大规模文本转语音系统引入适当的归纳偏置, 我们提出了 ***Mega-TTS***, 一个用于在各种场景下进行自然且健壮的语音合成的零样本文本转语音系统 (即零样本基于提示的文本转语音, 语音编辑, 以及跨语言文本转语音).
如图 01 所示, ***Mega-TTS*** 由基于 VQGAN 的文本转语音模型和韵律大语言模型 (P-LLM) 组成.
我们仔细地用不同方式建模不同语音属性.
首先, 我们选择梅尔频谱作为中间表示, 因为它可以很好地分离相位和其他属性.
然后, 我们从同一个说话人随机先前句子中用全局音色编码器提取全局向量以解耦音色和内容信息.
最后, 我们进一步使用基于 VQGAN 的声学模型用于生成梅尔频谱, 然后提出一个隐编码语言模型称为 P-LLM 用于拟合韵律的分布, 因为语言模型能够捕获局部和长期依赖性.
在推理时, 我们使用从给定文本序列提取的内容, 从参考语音提取的音色, 以及由 P-LLM 预测的韵律用于生成目标语音, 这是一种新的文本转语音解码机制, 称为韵律导向语音解码.
最后, 为了展示我们的模型能够应用于各种场景, 我们为下游任务设计了推理策略.
我们将在后续小节中详细描述这些设计和训练推理过程.

### 3.1.Disentangling Speech into Different Components: 将语音解耦成不同成分

<details>
<summary>原文</summary>

> To introduce appropriate inductive biases into different speech attributes, we need to separately express these attributes and carefully design different architectures for them.
> The overall model architecture of ***Mega-TTS*** is shown in Figure.01.
> We use three types of encoders to separately encode content, prosody, and timbre representations.
> Then we adopt a GAN-based mel-spectrogram decoder to generate mel-spectrograms with these representations.
> We describe the disentangling strategy and detailed design of the proposed encoders as follows.

</details>
<br>

为了给不同语音属性引入适当的归纳偏置, 我们需要单独表达这些属性, 并为它们仔细设计不同架构.
***Mega-TTS*** 的整体模型架构如图 01 所示.
我们使用三种编码器以分别编码内容, 韵律和音色表示.
然后我们采用基于生成对抗网络的梅尔频谱解码器利用这些表示生成梅尔频谱.
解耦策略和编码器的详细设计如下所示

<details>
<summary>原文</summary>

> #### Disentangling Strategy
> We disentangle the mel-spectrogram into content, prosody, and timbre representations with the reconstruction loss of the autoencoder and a carefully designed bottleneck [44]: 
> 1) we feed the mel-spectrogram into the prosody encoder, and we also introduce carefully-tuned dimension reduction and phoneme-level downsampling to the prosody encoder to constrain the information flow; 
> 2) the content encoder encodes the phoneme sequence into the content representation; 
> 3) we feed the reference mel-spectrogram sampled from a different speech of the same speaker to disentangle the timbre and content information and temporally average the output of the timbre encoder to get a one-dimensional global timbre vector.
> The correctly-designed bottleneck will learn to remove the content information and the global timbre information from the output of the prosody encoder, which ensures the performance of disentanglement.
> Due to the limited page space, we put more details about the hyperparameter selection for the information bottleneck in Appendix D.

</details>
<br>

#### 解耦策略
我们利用自编码器的重构损失和仔细设计的瓶颈将梅尔频谱解耦成内容, 韵律和音色表示:
1. 将梅尔频谱输入到韵律编码器, 此外引入仔细调整的降维和音素级别下采样到韵律编码器中用于约束信息流;
2. 内容编码器将音素序列编码为内容表示;
3. 输入同一说话人的不同语音中采样的参考梅尔频谱以解耦音色和内容信息, 并且对音色编码器的输出进行时序平均以获得一维全局音色向量.

正确设计的瓶颈将学习从韵律编码器的输出中移除内容信息和全局音色信息, 确保解耦效果.
由于页面空间有限, 我们将在附录 D 中详细介绍信息瓶颈的超参数选择.

<details>
<summary>原文</summary>

> #### Architecture Design of Encoders. 
> 1) The prosody encoder consists of two convolution stacks, a phoneme-level pooling layer, and a vector quantization (VQ) bottleneck.
> The first convolution stacks compress mel-spectrograms into phoneme-level hidden states according to the phoneme boundary and the second stacks capture phoneme-level correlations.
> The vector quantization layer [54] then utilizes these hidden states to obtain phoneme-level prosody codes $u = \{u_1, u_2, \cdots, u_T \}$ and hidden states $H_{prosody}$.
> To ease the difficulty of disentanglement, only the low-frequency band of the mel-spectrogram (the first 20 bins in each mel-spectrogram frame) is used as input, as it contains almost complete prosody and significantly less timbre/content information compared to the full band [46]; 
> 2) The content encoder is composed of several feed-forward Transformer layers.
> To achieve the monotonic alignment between the speech content and generated speech, we adopt the duration predictor and length regulator following common practice in non-autoregressive TTS systems [48, 50].
> Differently, we feed the prosody information extracted by the prosody encoder to the duration predictor in order to ease the one-to-many mapping problem [48, 45]; 
> 3) The timbre encoder is designed to extract a global vector $H_{timbre}$ that contains the speaker identity of the given speech.
> The timbre encoder consists of several stacks of convolution layers.
> To ensure the stability of timbre information across the time axis, we temporally average the output of the timbre encoder to get a one-dimensional timbre vector Htimbre.

> To keep good perceptual quality, we introduce a GAN-based mel-spectrogram decoder.
> We adopt the multi-length discriminator [10, 63] based on random windows of different lengths as the discriminator.
> Overall, the first-stage training loss $Loss$ of ***Mega-TTS*** can be formulated as:

$$
  Loss_{VQ} = \| y_t - y'_t\|^2 + \| \text{sg}[E(y_t)]-z_q\|^2_2 + \| \text{sg}[z_q]-E(y_t)\|^2_2
$$

$$
 Loss = \mathbb{E}[Loss_{VQ} + Loss_{Adv}]
$$

> where $y_t$ is the target speech and $y'_t$ is the generated speech.
> $Loss_{rec} = \|y_t − y'_t\|^2$ is the reconstruction loss, $\text{sg}[\cdot]$ denotes the stop-gradient operation, and $z_q$ is the temporal collection of codebook entries.
> $Loss_{VQ}$ is the VQVAE loss function [54, 16] and $Loss_{Adv}$ is the LSGAN-styled adversarial loss [38] whose objective is to minimize the distribution distance between the predicted mel-spectrograms and the ground truth mel-spectrograms.

</details>
<br>

#### 编码器架构设计

1. 韵律编码器由两个卷积堆栈, 一个音素级别池化层, 向量量化瓶颈组成.
  第一个卷积堆栈根据音素边界将梅尔频谱压缩为音素级别的隐状态;
  第二个卷积堆栈捕获音素级别的相关性.
  向量量化层利用这些隐状态以获得音素级别的韵律编码 $u = \{u_1, u_2, \cdots, u_T \}$ 和隐状态 $H_{prosody}$.
  为了减轻解耦的难度, 只有梅尔频谱的低频带 (每个梅尔频谱帧中的前 20 bin) 作为输入, 因为它包含几乎全部的韵律, 并且相比全频带携带更少的音色和内容信息.
2. 内容编码器由数个前馈 Transformer 层组成.
  为了实现语音内容和生成语音之间的单调对齐, 我们采用时长预测器和长度调节器, 这些方法在非自回归文本转语音系统中普遍采用.
  不同的是, 我们将由韵律编码器提取的韵律信息输入到时长预测器中, 以缓解一对多映射问题.
3. 音色编码器被设计用于提取全局向量 $H_{timbre}$ 包含给定语音的说话人身份.
  音色编码器由多个卷积堆栈组成.
  为了确保时轴上的音色信息稳定, 我们对音色编码器的输出进行时序平均以获得一维音色向量 $H_{timbre}$.

为了保持良好的感知质量, 我们引入基于 GAN 的梅尔频谱解码器.
我们采用基于不同长度窗口的多长度判别器作为判别器.

总体来说, ***Mega-TTS*** 的第一阶段训练损失可以表示为:

$$
  Loss_{VQ} = \| y_t - y'_t\|^2 + \| \text{sg}[E(y_t)]-z_q\|^2_2 + \| \text{sg}[z_q]-E(y_t)\|^2_2
$$

$$
 Loss = \mathbb{E}[Loss_{VQ} + Loss_{Adv}]
$$

其中:
- $y_t$: 目标语音;
- $y'_t$: 生成语音;
- $z_q$: 码本元素的时序集合;
- $Loss_{rec}$: 重构损失;
- $\text{sg}[\cdot]$: 停止梯度操作;
- $Loss_{VQ}$: VQVAE 损失函数;
- $Loss_{Adv}$: LSGAN 风格的对抗损失, 其目标是最小化预测梅尔频谱和真实梅尔频谱之间的分布距离.

### 3.2.P-LLM

<details>
<summary>原文</summary>

> The P-LLM is a latent code language model that captures local and long-range dependency for prosody modeling.
> We describe the prosody-oriented speech decoding mechanism and details of the P-LLM as follows.

</details>
<br>

P-LLM 是一个隐编码语言模型, 用于捕获韵律建模的局部和长期依赖.
我们将介绍韵律导向语音解码机制和 P-LLM 的详细设计.

<details>
<summary>原文</summary>

> #### Prosody-Oriented Speech Decoding. 
> Denote $(y_p, x_p)$ and $(y_t, x_t)$ as the prompt and target speech-transcription pairs.
> Our goal is to synthesize the high-quality target speech $y_t$ given an unseen speech prompt $y_p$.
> During inference, the timbre of the target speech $H'_{timbre}$ is expected to be the same as that of the prompt speech.
> Therefore, to generate the target speech $y_t$, we only need the prosody information $u'$ of the target speech.
> Therefore, the prosody-oriented speech decoding procedure can be formulated as follows:

$$
  \text{Encoder}: u = E_{prosody}(y_p), H_{content} = E_{content}(x_p), H'_{timbre}=E_{timbre}(y_p)
$$

$$
  \text{Prosody Prediction}: u' = f(u'|u, H_{content}, H'_{timbre}, H'_{content};\theta)
$$

$$
  \text{Decoder}: y'_t = D(u', H'_{timbre}, H'_{content})
$$

> where $E_{prosody}$, $E_{timbre}$, $E_{content}$, and $D$ denote the prosody encoder, timbre encoder, content encoder, and mel decoder. 
> $u$ is the prosody tokens of the prompt speech, $u'$ is the predicted prosody tokens of the target speech, $f$ is the prosody prediction function, and $\theta$ is the parameter of the P-LLM.
> $y'_t$ is the generated speech.

</details>
<br>

#### 韵律导向语音解码

记 $(y_p, x_p)$ 和 $(y_t, x_t)$ 为提示和目标语音-文本对.
我们的目标是根据未见过的提示语音 $y_p$ 合成高质量的目标语音 $y_t$.
推理过程中, 目标语音的音色 $H'_{timbre}$ 应当与提示语音的音色相同.
因此, 为了生成目标语音 $y_t$, 我们只需要目标语音的韵律信息 $u'$ .
因此, 韵律导向语音解码过程可以表示为:

$$
  \text{编码器}: u = E_{prosody}(y_p), H_{content} = E_{content}(x_p), H'_{timbre}=E_{timbre}(y_p)
$$

$$
  \text{韵律预测}: u' = f(u'|u, H_{content}, H'_{timbre}, H'_{content};\theta)
$$

$$
  \text{解码器}: y'_t = D(u', H'_{timbre}, H'_{content})
$$

其中 $E_{prosody}$, $E_{timbre}$, $E_{content}$, 和 $D$ 分别为韵律编码器, 音色编码器, 内容编码器, 和梅尔解码器.
$u$ 为提示语音的韵律标记, $u'$ 为目标语音的预测韵律标记, $f$ 为韵律预测函数, $\theta$ 为 P-LLM 的参数.
$y'_t$ 为生成语音.

<details>
<summary>原文</summary>

> #### Generating Prosody Codes. 
> The proposed prosody-oriented speech decoding mechanism requires the predicted prosody codes $u'$ of the target speech.
> Leveraging the powerful in-context learning capability of LLMs, we design the P-LLM module to predict $u'$.
> The P-LLM is a decoder-only transformer-based architecture [7] for prosody modeling, which uses prosody codes $u$ from $y_p$ as the prompt and $H_{content}$, $H'_{content}$, and $H'_{timbre}$ as the condition.
> The autoregressive prosody prediction process of P-LLM can be formulated as:

$$
  p(u'|u,H_{content}, H'_{timbre}, H'_{content};\theta) = \prod_{t=0}^{T} p(u'_t|u'_{<t},u,H_{content}, H'_{timbre}, H'_{content};\theta)
$$

> where $\theta$ is the parameter of our P-LLM.
> Since the discrete prosody sequence $u$ is phoneme-level, we directly concatenate it with $H_{content}$, $H'_{content}$, and $H'_{timbre}$ as the input.
> The P-LLM is trained in a teacher-forcing mode in the training stage via the cross-entropy loss.

</details>

#### 生成韵律编码

韵律导向语音解码机制需要目标音频的预测韵律编码 $u'$
利用大语言模型的强力上下文学习能力, 我们设计了 P-LLM 模块来预测 $u'$._
P-LLM 是仅基于 Transformer 解码器架构用于韵律建模, 以来自 $y_p$ 的韵律编码 $u$ 作为提示, $H_{content}$, $H'_{content}$, 和 $H'_{timbre}$ 为条件.
P-LLM 的自回归韵律预测过程可以表示为:

$$
  p(u'|u,H_{content}, H'_{timbre}, H'_{content};\theta) = \prod_{t=0}^{T} p(u'_t|u'_{<t},u,H_{content}, H'_{timbre}, H'_{content};\theta)
$$

其中 $\theta$ 是 P-LLM 的参数.
由于韵律序列 $u$ 是音素级的, 我们直接将其与 $H_{content}$, $H'_{content}$, 和 $H'_{timbre}$ 连接作为输入.
P-LLM 在训练阶段通过交叉熵损失进行教师强迫训练.

### 3.3.Speech Prompting for Inference: 用于推理的语音提示

<details>
<summary>原文</summary>

> To facilitate in-context learning for various speech generation tasks, we design different speech prompting mechanisms to encourage ***Mega-TTS*** to follow the information in the speech prompt.

</details>
<br>

为了促进各种语音生成任务的上下文学习, 我们设计不同的语音提示机制以鼓励 ***Mega-TTS*** 遵循语音提示中的信息.

<details>
<summary>原文</summary>

> #### Inference for TTS. 
> For zero-shot TTS, P-LLM uses $u$, $H_{content}$, $H'_{timbre}$, $H'_{content}$ to generate the target prosody codes $u'$ for the target speech according to Equation.04.
> We use the top-k random sampling scheme [17] to sample the results since we observe that the sampling-based method could increase the diversity of the generated speech.
> Then, we concatenate the content $H'_{content}$, timbre $H'_{timbre}$, and prosody $u'$ information to generate the target speech $y_t$ using the mel decoder.
> Leveraging the proper inductive biases and powerful in-context learning capability of our P-LLM, the generated speech can retain not only similar timbre but also the rhythmic habits of the prompt speech.
> For cross-lingual TTS, $u$, $H_{content}$, $H'_{timbre}$, $H'_{content}$ are extracted from the prompt speech in a foreign language, and the subsequent procedure keeps the same as that of zero-shot TTS.

</details>
<br>

#### 文本转语音推理

对于零样本文本转语音, P-LLM 使用 $u$, $H_{content}$, $H'_{timbre}$, $H'_{content}$ 等信息生成目标语音的韵律编码 $u'$ 并根据公式 04 进行生成.
我们使用 Top-K 随机采样方案来采样结果因为我们观察到基于采样的方法能够增加生成语音的多样性.
然后我们将内容 $H'_{content}$, 音色 $H'_{timbre}$, 以及韵律 $u'$ 信息拼接使用梅尔解码器生成目标语音 $y_t$.
利用适当的归纳偏置和 P-LLM 的强力的上下文学习能力, 生成的语音不仅能够保持相似音色还能保持提示语音的节奏习惯.
对于跨语言文本转语音, $u$, $H_{content}$, $H'_{timbre}$, $H'_{content}$ 从其他语言的提示语音中提取并使用, 后续过程与零样本文本转语音相同.

<details>
<summary>原文</summary>

> #### Inference for Speech Editing. 
> In speech editing, the predicted prosody codes should achieve smooth transitions at both the left and right boundaries of the masked region.
> Previous works like EditSpeech [52] propose to perform left and right autoregressive inferences separately and concat the mel-spectrogram at the least L2-norm difference fusion point.
> However, the L2-norm difference of the mel-spectrogram is far from human perception, leading to poor audio naturalness.
> Since the prosody representations in ***Mega-TTS*** is discrete, we can solve the transition problem by operating on discrete prosody representations.
> First, we regard the area on the left side of the mask as a prompt to generate N candidate paths with top-k random sampling strategy.
> Secondly, the N generated paths are used as new prompts to generate the probability matrix of the area on the right side of the mask and the ground-truth prosody codes are used to obtain the probabilities of each decoding step from the probability matrix.
> In the third stage, we sum up the log probabilities of each decoding step for the candidate paths.
> Finally, we choose the path that achieves the maximum probability in the second step as the predicted result.
> The decoding strategy for speech editing can be formulated as follows:

$$
  \max_{i\in [1,N]} \text{Likelihood} = \max_{i\in [1,N]} \prod_{t=L}^{R} p(u_t^i | u_{<t}^i, H_{content}, H'_{timbre}, H'_{content};\theta)\cdot\prod_{t=R}^{T} p(u_t^{gt}|u_{<t}^{i}, H_{content}, H'_{timbre}, H'_{content};\theta)
$$

> where $L$ and $R$ are the left and right boundaries of the mask.
> $T$ is the length of the mel-spectrogram. 
> $u_t^i$ is the prosody code in the $i$-th candidate path. 
> $u^{gt}_t$ is the ground-truth prosody codes.
> Since our decoding strategy considers the prosody information of the boundaries on both sides, the edited region can achieve smooth transitions.

</details>
<br>

#### 语音编辑推理

在语音编辑中, 预测的韵律编码应该在掩码区域的左右边界处实现平滑过渡.
之前的工作如 EditSpeech [52] 提出了分别进行左右自回归推理并在最小 L2 范数融合点拼接梅尔频谱.
然而, 音频自然度的 L2 范数差距远远偏离人类的感知, 导致语音质量较差.
由于 ***Mega-TTS*** 中的韵律表示是离散的, 我们可以通过离散韵律表示来解决过渡问题.
首先, 我们将掩码区域的左侧区域视为提示生成 N 个候选路径, 使用 Top-K 随机采样策略生成.
其次, N 个生成路径被用作新的提示, 用于生成掩码区域的右侧区域的概率矩阵, 并使用概率矩阵从候选路径中获得每个解码步的概率.
第三步, 我们对每个候选路径的解码步的概率求和.
最后, 我们在第二步中选择概率最大的路径作为预测结果.
语音编辑的解码策略可以表示为:

$$
  \max_{i\in [1,N]} \text{Likelihood} = \max_{i\in [1,N]} \prod_{t=L}^{R} p(u_t^i | u_{<t}^i, H_{content}, H'_{timbre}, H'_{content};\theta)\cdot\prod_{t=R}^{T} p(u_t^{gt}|u_{<t}^{i}, H_{content}, H'_{timbre}, H'_{content};\theta)
$$

其中 $L$ 和 $R$ 是掩码区域的左右边界.
$T$ 是梅尔频谱的长度.
$u_t^i$ 是第 i 个候选路径中的韵律编码.
$u^{gt}_t$ 是目标韵律编码.
由于我们的解码策略考虑了掩码区域的左右边界, 编辑区域可以实现平滑过渡.

## 4.Experiments: 实验

> In this section, we present the evaluation results of ***Mega-TTS*** and the comparison with baselines in terms of the objective and subjective metrics.

### 4.1.Experimental setup

> #### Training datasets. 
> We use GigaSpeech [9] and WenetSpeech [65] as the training corpora, which contains 20k hours of multi-domain speeches in English and Chinese in total.
> Since the speech in GigaSpeech and WenetSpeech does not have speaker identities and multiple speakers may appear in a speech clip, we process the datasets with an open-source automatic speaker diarization model6 [6, 5].
> We also extract the phoneme-level alignments with the external alignment tool7.
> More information can be found in Appendix A.3.

> #### Evaluation datasets. 
> We employ two datasets for evaluation: 
> 1) VCTK dataset [57], an English dataset that contains 108 speakers; 
> 2) LibriSpeech [42] test-clean, an English dataset that contains 40 speakers.
> For each of these datasets, we randomly sample 10 utterances for each of the 40 speakers, resulting in a subset of 400 utterances for evaluation; Specifically, to synthesize each sample, we randomly select a different utterance of the same speaker to form the speech prompt.
> Note that all speakers in the evaluation datasets are unseen during training.

> #### Model configuration. 
> Our ***Mega-TTS*** consists of three encoders, a prosody large language model, a mel decoder, and a discriminator.
> The prosody encoder, timbre encoder, and mel generator consist of 5 convolutional blocks with 320 hidden size, 5 convolution 1D kernel size.
> The content encoder is a 4-layer Transformer [56] with 2 attention heads, 320 embedding dimensions, 1280 1D convolution filter size, and 5 convolution 1D kernel size.
> The duration predictor is a 3-layer 1D convolution with ReLU activation and layer normalization, which have 320 hidden size.
> The discriminator follows the architecture proposed in SyntaSpeech [63].
> The P-LLM model is a decoder-only architecture that contains 8 Transformer layers with 8 attention heads, 512 embedding dimensions, 2048 1D convolution filter size, and 5 convolution 1D kernel size.
> The overall number of model parameters is 222.5M.
> We add more detailed model configurations in Appendix A.1.

> #### Training and inference. 
> In the training stage, we train ***Mega-TTS*** on 8 NVIDIA A100 GPUs, with a batch size of 30 sentences on each GPU.
> We use the Adam optimizer with β1 = 0.9, β2 = 0.98, ϵ = 10−9 and follow the same learning rate schedule in [56].
> It takes 320k steps for the VQ-GAN TTS model’s training and 100K steps for the P-LLM’s training until convergence.
> The predicted mel-spectrograms are transformed into audio samples using pre-trained HiFi-GAN V18 [31].
> In the inference stage, we use the top-5 random sampling scheme [17] to sample diverse results.

> #### Objective metrics. 
> We evaluate the pitch distance and speaker similarity for zero-shot TTS.
> In terms of the pitch distance, we compute the average dynamic time warping (DTW) [40] distances between the pitch contours of ground-truth speech and synthesized speech.
> And for the cosine speaker similarity, we use the WavLM model [12] finetuned for speaker verification9 to compute the cosine speaker similarity score between the ground-truth speech and synthesized speech.
> The similarity score is in the range of [−1, 1], where a larger value indicates a higher similarity of input samples.
> In addition, we also evaluate the word error rate (WER) for cross-lingual TTS.
> We use the ASR system from the released HuBERT-Large model [19] to transcribe the generated speech into text.
> Then, the WER between the transcribed text and the original target text is measured.
> We use all samples in the test set for the objective evaluation.
> We put more information in Appendix A.4 and Appendix A.5.

> #### Subjective metrics. 
> We conduct the MOS (mean opinion score) and CMOS (comparative mean opinion score) evaluation on the test set to measure the audio naturalness via Amazon Mechanical Turk.
> We keep the text content and prompt speech consistent among different models to exclude other interference factors.
> We randomly choose 50 samples from the test set of each dataset for the subjective evaluation and each audio is listened to by at least 20 testers.
> We analyze the MOS in three aspects: MOS-Q (Quality: clarity, high-frequency, and original timbre reconstruction), MOS-P (Prosody: naturalness of pitch, energy, and duration), and MOS-S (Speaker similarity).
> We also analyze the CMOS in terms of audio quality and speech prosody.
> We tell the tester to focus on one corresponding aspect and ignore the other aspect when scoring.
> We put more information about the subjective evaluation in Appendix A.2.

### 4.2.Results of zero-shot synthesis

> We compare the zero-shot synthesis performance of ***Mega-TTS*** with baseline systems, including: 
> 1)YourTTS [8], a powerful zero-shot TTS model trained on 1k hours of speech dataset.
> We use the official code and released checkpoint10; 
> 2) VALL-E, a large-scale zero-shot TTS model using the audio codec model to generate discrete speech codes and LLM to generate them.
> For VALL-E, we directly download the first 16 utterances from the VALL-E demo page.
> The audio samples consist of 8 samples from LibriSpeech and 8 samples from VCTK11.
> As shown in Table 2, ***Mega-TTS*** significantly outperforms YourTTS in terms of audio quality and speech prosody.
> And in terms of speaker similarity, ***Mega-TTS*** significantly outperforms YourTTS with +0.51 MOS-S on VCTK and +0.68 MOS-S on LibriSpeech, demonstrating the effectiveness of ***Mega-TTS*** in zero-shot scenarios.
> Besides, as shown in Table 3, ***Mega-TTS*** outperforms VALL-E in all metrics.
> It can be seen that ***Mega-TTS*** is able to generate more natural speeches than VALL-E, demonstrating the effectiveness of introducing intrinsic inductive biases.
> To further investigate the performance of disentanglement, we also visualize the distribution of the timbre and prosody representations in Appendix C.

### 4.3.Results of zero-shot speech editing

> We compare the quality of generated audio samples of our ***Mega-TTS*** with SOTA speech editing baselines, including 1) EditSpeech [52]; 2) A3T [3].
> Since the text content of the generated speech has been edited in the speech editing evaluation, the ground truth is missing.
> Therefore, we only conduct the subjective evaluation.
> We manually define modification operations (i.e., insertion, replacement, and deletion) of the test samples.
> We then conduct the experiments on the VCTK dataset.
> We evaluate the audio quality, speech prosody, and speaker similarity for each audio sample.
> The results are presented Table 4.
> It can be seen that ***Mega-TTS*** achieves the highest perceptual quality, prosody, and speaker similarity score, which demonstrates the effectiveness of our proposed speech prompting mechanism for speech editing and the powerful in-context learning capability of ***Mega-TTS***.

### 4.4.Results of zero-shot cross-lingual TTS

> To compare ***Mega-TTS*** with the zero-shot cross-lingual TTS models VALL-E X [67], we directly download the utterances from the VALL-E X demo page, which consists of 6 speech pairs from LibriSpeech, EMIME, and AISHELL-3.
> Since YourTTS [8] is built only for English TTS, we evaluate the performance of English TTS with Chinese samples as prompts.
> The results are listed in Table 5.
> It can be seen that ***Mega-TTS*** surpasses VALL-E X in terms of audio quality, speech prosody, and speaker similarity scores, which further demonstrates the superiority of introducing proper inductive biases to different speech attributes.
> For objective evaluations, we use all of the text samples in the LibriSpeech test-clean set as the target sentences and randomly select one audio from AISHELL-3 as the speech prompt for each target sentence.
> The results show that ***Mega-TTS*** achieves a significantly lower WER than YourTTS, demonstrating the effectiveness of our method.

### 4.5.Results of robustness evaluation

> To further evaluate the robustness of the proposed model, we adopt the 50 particularly hard sentences following FastSpeech [48].
> As shown in Table 6, Tacotron [59] and VALL-E [58] show poor robustness on these complicated sentences.
> As a comparison, our ***Mega-TTS*** shows equivalent robustness to the non-autoregressive models, such as FastSpeech [48], without any repeat or skip issues.
> It can be seen that directly modeling the discrete speech tokens with LLMs like VALL-E [58] would cause robustness issues.
> As a comparison, ***Mega-TTS*** not only leverages the in-context learning capability of LLMs, but also maintains good robustness by introducing the proper inductive bias to each speech component.

## 5.Conclusions: 结论

> In this paper, we proposed ***Mega-TTS***, which aims to introduce proper inductive biases into large-scale zero-shot TTS systems.
> We disentangle speech into different attributes (i.e., content, timbre, prosody, and phase) and model different attributes in different ways.
> We train ***Mega-TTS*** with 20K hours of multi-domain speech data and evaluate its performance on unseen datasets.
> Our experimental results on three speech synthesis tasks show that ***Mega-TTS*** outperforms state-of-the-art zero-shot TTS models regarding audio quality, speech prosody, speaker similarity, and robustness.
> Due to limited page space, we discuss the limitations and future works in Appendix F and the broader impacts in Appendix G.


## Appendix
<details>
<summary> 原文 </summary>

### A.
#### A.2.Details in Subjective Evaluation

> We perform the audio quality, speech prosody, and speaker similarity evaluations on Amazon Mechanical Turk (MTurk).
> For each dataset, we randomly select 50 samples from the test set and use the TTS systems to generate the audio samples.
> Each audio has been listened to by at least 20 listeners.
> For MOS, each tester is asked to evaluate the subjective score of a sentence on a 1-5 Likert scale.
> For CMOS, listeners are asked to compare pairs of audio generated by systems A and B, indicating which of the two audio they prefer, and choose one of the following scores according to the degree of superiority: 0 indicating no difference, 1 indicating source slightly better, 2 indicating source mostly better and 3 indicating source completely better.
> For audio quality evaluation (MOS-Q and CMOS-Q), we tell listeners to “Please focus on the audio quality and ignore other factors”.
> For prosody evaluations (MOS-P and CMOS-P), we tell listeners to “Please focus on the prosody and style, and ignore the differences of grammar, audio quality, or other factors. ”.
> For speaker similarity evaluations (MOS-S), we tell listeners to “ Please focus only on the similarity of the speaker to the reference, and ignore the differences of content, grammar, prosody, audio quality, or other factors.”.
> The screenshots of instructions for testers are shown in Figure.03.
> We paid $12 to participants hourly and totally spent about $1000 on participant compensation.
> We tell the participants that the data will be used in scientific research.

#### A.3.Details of Speaker Diarization Model

> To obtain the speaker information from GigaSpeech and WenetSpeech, we use a released automatic speaker diarization model called pyannote.audio12, which achieves DER=11.24% on the VoxConverse dataset and DER=14.09% on the AISHELL-4 dataset.
> We only assign the speaker ID to the audio clip when its activation score is higher than 70% and abandon other audio clips.
> We also abandon the audio clips that contain multiple speakers speaking simultaneously.

#### A.4.Details of Speaker Similarity Model

> To measure the speaker similarity, we use the WavLM [12] model finetuned for speaker verification from https://huggingface.co/microsoft/wavlm-base-plus-sv to extract the speaker embedding.
> Then the cosine similarity between the synthesized speech’s speaker embedding and the ground-truth speech’s speaker embedding is calculated as the speaker similarity score.
> The WavLM model is pretrained on 94,000 hours of speech data and finetuned on the VoxCeleb1 dataset using an X-Vector head with an Additive Margin Softmax loss, which achieves 0.84%, 0.928%, and 1.758% EER (Equal Error Rate) on the Vox1-O, Vox1-E, and Vox1-H trial lists.

#### A.5.Details of ASR Model
> To measure the audio quality and speech intelligibility for cross-lingual TTS systems, we evaluate the word error rate (WER) metric.
> We use the finetuned HuBERT-Large model to transcribe the synthesized speech into text and calculate the WER between the transcribed text and the original target text.
> The finetuned HuBERT-Large model from https://huggingface.co/facebook/ hubert-large-ls960-ft is finetuned on 960h of Librispeech and achieves 1.5%, 3.0%, 1.9%, and 3.3% WER on the dev-clean, dev-other, test-clean, and test-other set of Librispeech.

#### A.6.Error Bars and Random Seeds

> For the subjective evaluations, we report confidence intervals of the results of MOS tests in Table 2, Table 3, Table 4, and Table 5.
> For the objective evaluations, we ran the experiments 10 times with 10 different random seeds ([1234, 1111, 2222, 3333, 4444, 5555, 6666, 7777, 8888, 9999]) and obtained the averaged results.

### B.Visualizations of Mel-Spectrograms

> We put more visualizations of mel-spectrograms with different random seeds in Figure.04.
> We can see that with different random seeds, ***Mega-TTS*** can generate diverse results that have different prosody and frequency details.

### C.Visualization of Representations

> To validate the effectiveness of disentanglement for speech components in Section 3.1, we adopt T-SNE [55] to visualize timbre embedding and prosody embedding for unseen speakers on the VCTK dataset.
> We randomly select 10 speakers and directly use the encoders proposed in Section 3.1 to extract the timbre and prosody information from their audio samples.
> The results are shown in Figure.05 and Figure.06.
> It can be seen that the timbre embeddings are ideally located according to the speaker ID.
> However, the prosody embeddings of different speakers have similar distributions.
> It shows that our proposed prosody and timbre encoders are able to disentangle the corresponding representations from the mel-spectrograms, which further ensures the effectiveness of our P-LLM.

### D.Hyperparameter Selection for the Information Bottleneck

> In this section, we describe the details of the hyperparameter selection for the information bottleneck proposed in Section 3.1.
> The information bottleneck of ***Mega-TTS*** mainly contains two key hyperparameters: the channel size and the embedding size of the vector quantization (VQ) layer.
> When the channel size and the embedding size are too small or large, the performance of disentanglement will be poor.
> Therefore, we should carefully select these hyperparameters.
> We train the VQGAN-based TTS models with different VQ hyperparameters and evaluate their pitch distance and speaker similarity following Section 4.
> Differently, we use the proposed encoders to extract the timbre, content, and prosody embeddings of the test samples.
> Then, we randomly shuffle the timbre embedding sequence and reconstruct the mel-spectrogram with the original content, original prosody, and shuffled timbre information.
> We calculate the pitch distance between the ground-truth speech and the generated speech, but we calculate the speaker similarity between the shuffled ground-truth speech and the generated speech.
> As shown in Table 8, when the channel size is 256 and the embedding size is 2048, the VQGAN-based TTS model shows the best pitch accuracy and speaker similarity, i.e., the disentanglement performance is the best.

### E.Ablation Studies of Dataset Size and Model Size

> In this section, we evaluate the influences of the training dataset size and model size on the zero-shot TTS task for ***Mega-TTS***.
> We evaluate the pitch distance, speaker similarity, and the average absolute duration error in milliseconds on the LibriSpeech test-clean set.
> As shown in Table 9, when the dataset size grows, the zero-shot performance of ***Mega-TTS*** is significantly improved.
> Moreover, from Table 10, we can see that when the hidden size of P-LLM grows, the pitch distance significantly drops, demonstrating that the in-context learning capability of P-LLM can be greatly improved by the size of the model.

### F.Limitations and Future Works

> Although achieving superior performance on various zero-shot speech synthesis tasks, ***Mega-TTS*** still suffers from two main limitations.

> Data coverage.
> Although we use 20K hours of multi-domain data for training, our model still cannot cover everyone’s voice.
> Especially for some speakers with extremely heavy accents, our model cannot imitate their speaking style very well.
> In the future, we will further scale up the training data to 200K hours to further improve the performance of the model.
> Reconstruction Robustness.
> Although the reconstruction quality of the proposed VQGAN-based TTS model is satisfying on the clean dataset, it will be influenced by the background music or the extremely loud reverberation.
> In future work, we will explore a new model structure that is more robust against the acoustic environment noises.

### G.Broader Impacts

> ***Mega-TTS*** improves the quality and efficiency of zero-shot speech synthesis, which makes it easier for people to synthesize personalized speeches.
> In most cases, people will utilize this technique to facilitate movies, games, podcasts, and other services only.
> However, it may carry potential risks in misuse of the model, such as spoofing voice or other deepfake-related usages.
> To handle this, potential solutions like building a corresponding deepfake detection model should be considered.
> We also plan to include restrictions in the open-source license of the ***Mega-TTS*** project to prevent the misuse of the model.

</details>