# CoFi-Speech

- 标题: "Speaking from Coarse to Fine: Improving Neural Codec Language Model via Multi-Scale Speech Coding and Generation"

## Abstract: 摘要

> The neural codec language model (CLM) has demonstrated remarkable performance in text-to-speech (TTS) synthesis. 
> However, troubled by `recency bias`, CLM lacks sufficient attention to coarse-grained information at a higher temporal scale, often producing unnatural or even unintelligible speech. 
> This work proposes CoFi-Speech, a coarse-to-fine CLM-TTS approach, employing multi-scale speech coding and generation to address this issue. 
> We train a multi-scale neural codec, ***CoFi-Codec***, to encode speech into a multi-scale discrete representation, comprising multiple token sequences with different time resolutions. 
> Then, we propose ***CoFi-LM*** that can generate this representation in two modes: the single-LM-based chain-of-scale generation and the multiple-LM-based stack-of-scale generation. 
> In experiments, CoFi-Speech significantly outperforms single-scale baseline systems on naturalness and speaker similarity in zero-shot TTS. 
> The analysis of multi-scale coding demonstrates the effectiveness of ***CoFi-Codec*** in learning multi-scale discrete speech representations while keeping high-quality speech reconstruction. 
> The coarse-to-fine multi-scale generation, especially for the stack-of-scale approach, is also validated as a crucial approach in pursuing a high-quality neural codec language model for TTS.

## 1.Introduction: 引言

> The success of large language models (LLMs) in text domain \cite{brown2020language,openai2023gpt4,touvron2023llama2} has demonstrated their great capability in discrete sequence generation. 
> It also inspires the birth of a new text-to-speech synthesis (TTS) paradigm based on the neural codec language model (CLM) \cite{VALLEX, tortoise, lajszczak2024base}, which treats TTS as a next-token prediction task. 
> This framework usually relies on a neural codec \cite{encodec, hifi-codec, dac} to encode the speech audio into discrete tokens, which can be incorporated with the text sequence and generated by the LM, i.e. an auto-regressive decoder. 
> Finally, we obtain the speech audio from these generated speech tokens via the codec decoder.
>
> However, different from the text, the discrete speech sequence is much longer to keep sufficient capacity to preserve phonetic and acoustic information. 
> This long sequence length not only increases the complexity of TTS modeling but aggregates the `recency bias` of LMs \cite{peysakhovich2023attention, wang2024eliminating}, i.e. overly focusing on recent tokens during auto-regressive generation. 
> This issue makes LM focus less on coarse-grained information \cite{guo2023msmc}, e.g. phonetics, prosody, and speaking style at higher and different temporal scales, hence causing unstable TTS performance, producing unnatural or even unintelligible speech. 
> Although monotonic attention constraints \cite{han2024vall, du2024vall, wang2024attention} are proposed to fix stability issues, they still cannot solve `recency bias` fundamentally. 
> Some works \cite{tortoise, socodec, li2024single} turn to directly model shorter speech sequences with a larger frameshift to avoid this issue, but limits the fine-grained expression of LMs in TTS. 
> This dilemma implies the necessity of applying guidance to LMs to pay sufficient attention to both coarse-grained and fine-grained information of speech. 
>
> In this work, we propose a novel CLM-based zero-shot TTS approach, CoFi-Speech, that generates speech in a coarse-to-fine manner via a multi-scale speech coding and generation approach. 
> In this framework, the multi-scale speech codec, ***CoFi-Codec***, decomposes speech into multiple discrete sequences with different temporal resolutions and decodes them back with a high reconstruction quality. 
> Then, we propose two LM-based approaches to predict this multi-scale speech representation from coarse to fine: single-LM-based chain-of-scale generation and multiple-LM-based stack-of-scale generation. 
> In experiments, we present subjective and objective evaluations to demonstrate that CoFi-Speech significantly outperforms baseline systems based on single-scale speech sequences on naturalness and similarity, where stack-of-scale generation performs best. 
> Finally, we conduct detailed ablation studies to analyze multi-scale coding and generation, to further validate the effectiveness of `speaking from coarse to fine` in achieving high-quality CLM-based TTS.

## 2.Related Works: 相关工作

## 3.Methodology: 方法

### CoFi-Codec

> ***CoFi-Codec*** aims to decompose speech into multiple discrete sequences with different resolutions to provide a multi-scale speech representation. 
> Fig. \ref{fig:cofi} (a) presents a three-scale ***CoFi-Codec*** model architecture. 
> We first convert speech signals into the Mel spectrogram, and employ multiple encoder blocks to down-sample it into sequences at different temporal scales. 
> Specifically, each encoder block is applied with a 1-D convolutional ResNet block and a strided convolutional layer for down-sampling. 
> Meanwhile, we follow SoCodec \cite{socodec} to apply an ECAPA-TDNN-based \cite{dawalatabad2021ecapa} reference encoder to extract a global embedding $g$ from the Mel spectrogram to capture time-invariant information, e.g., speaker identity, global speaking style, acoustic environment, etc.
>
> In decoding, we apply an equal number of decoder modules to quantize and decode these encoding sequences in a reversed order to reconstruct the Mel spectrogram from coarse to fine. 
> For example, in $i$-th decoder block, we first extract the residual sequence at this stage by performing $\mathbf{e}^i - \mathbf{d}^i$, i.e. removing high-scale information from the decoding sequence $\mathbf{d}^i$ out of the encoding sequence $\mathbf{e}^i$. 
> We then apply single-stream \cite{vqvae} or multi-stream vector quantization, e.g. 
> RQ \cite{chen2010approximate} and PQ \cite{jegou2010product}, to obtain the quantized speech sequence $\mathbf{s}^i$. 
> It is added with $\mathbf{d}^i$ and global embedding $g$, subsequently processed by a ResNet block and a transposed convolutional layer for up-sampling to produce the next decoding sequence $\mathbf{d}^{i-1}$. 
> Notably, the highest-scale decoder block adopts an all-zero sequence as the decoding sequence. 
> Finally, the Mel spectrogram is reconstructed from the decoder, and converted to the waveform via a pre-trained neural vocoder.

> In training, the model tends to overfit low-scale sequences with richer information, leading to high-scale representation collapse, i.e. nothing is preserved in the sequence. 
> Hence, we propose scale-wise nested dropout (SWND), which masks quantized sequences $\mathbf{s}_{1:b}$ given an index $b$ randomly sampled from $[0, N_s - 1]$, where $b=0$ indicates that no sequence is masked. 
> This approach forces high-scale sequence to preserve effective information for reconstruction, avoiding representation collapse. 
> Moreover, we apply GAN training \cite{li2024single, socodec} on ***CoFi-Codec*** to improve the generation quality of the Mel spectrogram. 
> The loss function is written as follows:

$$
\begin{aligned}
    \mathcal{L}_c &= \lambda_{vq} * \mathcal{L}_{vq} + \lambda_{reg} *\mathcal{L}_{reg} + 
    \lambda_{adv} * \mathcal{L}_{adv}
\end{aligned}
$$

> where $\mathcal{L}_{vq}$ is the vector-quantization loss, i.e. the averaged L2 loss between embeddings before and after VQ, $\mathcal{L}_{reg}$ is the L2-based regression loss on Mel spectrogram, and $\mathcal{L}_{adv}$ is the adversarial loss following \cite{socodec}.

### CoFi-LM

> To predict the multi-scale discrete representation in a coarse-to-fine manner, we propose ***CoFi-LM*** in two styles: chain-of-scale (CoS) generation and stack-of-scale (SoS) generation.

> **Chain-of-scale generation** is a simple extension of CLM to the multi-scale speech representation, which employs one LM to predict speech sequences in descending order of the temporal scale, as shown in Fig. \ref{fig:cofi} (b). 
> In training, the text and speech sequences are embedded into the same space and processed by a decoder-only Transformer to estimate the probabilities of the next speech tokens. 
> Notably, for multi-stream speech sequence, we follow the `delay pattern` \cite{copet2024simple, dang2024livespeech, lyth2024natural} to achieve the multi-stream prediction. 
> This approach explicitly constrains the LM to model multi-scale information to alleviate `recency bias`. 
> However, concatenating multiple sequences leads to a longer chain, causing higher computing costs and modeling complexity. 
> To mitigate this issue, we propose stack-of-scale generation.

> **Stack-of-scale generation** employs a stack of LMs to generate speech sequences at different scales in cascade, as shown in Fig. \ref{fig:cofi} (c). 
> It first generates the highest-scale sequence from the text and the reference embedding. 
> The hidden states $\textbf{h}^3_{1:T_3}$ in the last transformer layer are employed to generate the next-scale sequence. 
> It is placed at the head of the input sequence as the prompt, and also upsampled to be added to the lower-scale speech sequence to emphasize the alignment between these two sequences. 
> In this way, we recursively generate all speech sequences from coarse to fine. 
> This stage-wise approach enables us to better model multi-scale information while avoiding introducing long context to transformers.

> Finally, we apply in-context learning (ICL) \cite{anastassiou2024seed, du2024cosyvoice} to ***CoFi-LM***s for zero-shot TTS. 
> Specifically, we extract speaker embedding and speech tokens from the reference audio as speech prompts. 
> The transcript of the reference audio is also concatenated with the input text as the text prompt.

## 4.Experiments: 实验

### Training Configuration

> We conduct experiments with WenetSpeech4TTS (Basic) \cite{ma2024wenetspeech4tts}, a Chinese dataset with 7k hours of speech data. 
> All audio files are normalized to the sample rate of 16kHz and converted to 80-dim Mel spectrograms with a frameshift of 10ms. 
> We create a byte-pair encoding (BPE) based text dictionary with 8192 tokens to encode the text.
>
> ***CoFi-Codec*** is applied with 512-dim ResNet modules, each consisting of four residual units with two 1-D convolutional layers. $\text{enc}_1$ and $\text{dec}_1$ are applied with four more residual units to better encode and reconstruct Mel spectrograms. 
> We train a three-scale ***CoFi-Codec*** with ordered product quantization (OPQ) \cite{socodec} to compress the Mel spectrogram into two single-stream sequences with frameshifts of 120ms and 40ms and one four-stream sequence with a frameshift of 20ms. 
> Each stream in each sequence has a codebook with 16,384 codewords. 
> In training, we set $\lambda_{vq}=1, \lambda_{reg}=1, \lambda_{adv}=0.1$ and use EMA \cite{vqvae} to update codebooks with the decay rate of 0.99. 
> The discriminator in Mega-TTS \cite{jiang2023mega} is employed for adversarial training. 
> The sampling probabilities for SWND are ${p_0 = 0.8, p_1 = 0.1, p_2 = 0.1}$. 
> Finally, a pre-trained BigVGAN-based \cite{bigvgan} neural vocoder is employed to convert the Mel spectrogram to the waveform. 
>
> LMs are applied with 12-layer decoder-only Transformers with a feature dimension of 1024. 
> For multi-stream speech sequences, we apply a `delay pattern`. 
> We employ the sampling strategy with a top-p of 0.8, a top-k of 50, and a repetition penalty of 2.0. 
> ***CoFi-Codec*** and LMs are trained using AdamW \cite{Loshchilov2018} for 100k iterations with a batch size of 1.6k seconds. 
> The learning rate exponentially decays from $3\times10^{-4}$ to $1\times10^{-4}$. %\footnote{The implementation and checkpoint of our proposed system will be released after the double-blind peer review.}

### Evaluation Metrics

> We create an 860-utterance test set with high diversity in speaking style and audio quality for system evaluation. 
> In zero-shot TTS, each utterance is paired with another out-of-training-set audio file as the reference audio. 
> We calculate character error rate (CER, \%), and speaker similarity (SIM, $\times10^{-2}$) to measure intelligibility, and speaker similarity.\footnote{The ASR tool for transcribing is available at \url{https://github.com/modelscope/FunASR}. 
> The tool for extracting speaker embedding is available at \url{https://huggingface.co/Wespeaker/wespeaker-cnceleb-resnet34}} In \ref{ssec:tts_comparison}, we use a subset with 100 utterances for subjective evaluation. 
> There are 10 native speakers invited to the test to rate each audio with a score ranging from 1 to 5 in terms of naturalness (NMOS) and speaker similarity (SMOS), respectively. 
> In codec evaluation, We adopt Mel-cepstrum distortion (MCD, dB) and CER to measure reconstruction quality.

## 5.Results: 结果

### TTS System Comparison

> As shown in Table \ref{tab:mos}, we compare CoFi-Speech with baseline systems: VALL-E and SoCodec-TTS, trained with the same dataset as ours, and the SotA industrial TTS system, CosyVoice trained with over 100k hours of data. 
> The baseline system VALL-E auto-regressively generates an eight-stream speech sequence with a short frameshift of 20ms along both time and stream axes. 
> This model troubled by `recency bias` presents serious stability issues on this challenging test set, performing the worst synthesis quality. 
> SoCodec-TTS compresses speech into a short speech sequence with a frameshift of only 120ms to achieve stable LM inference. 
> It presents high naturalness but low speaker similarity due to the lack of fine-grained information. 
> In contrast, CoFi-Speech leverages both coarse-grained and fine-grained information, presenting better naturalness and similarity, especially for the SoS-based approach with the highest NMOS and SMOS, even outperforming CosyVoice trained with much more data. 
> This significant improvement underscores the efficacy of CoFi-Speech in CLM-TTS.\footnote{Samples are available at \url{https://hhguo.github.io/DemoCoFiSpeech}}

### Multi-Scale Speech Coding

> We conduct objective evaluations to investigate if ***CoFi-Codec*** can decompose speech into sequences at multiple temporal scales while keeping high reconstruction quality. 
> As shown in Fig. \ref{fig:msc}, we evaluate speech reconstructed from top-$b$ scales. 
> For example, for a three-scale ***CoFi-Codec***, `$b=2$` means that we only reconstruct speech using $\textbf{s}^3$ and $\textbf{s}^2$, and mask $\textbf{s}^1$ with all-zero vectors. 
> The result demonstrates that ***CoFi-Codec*** learns a good multi-scale representation with the help of SWND. 
> The high-scale sequence preserves more effective information, presenting lower reconstruction loss over that of codec without SWND. 
> Moreover, compared with the single-scale ***CoFi-Codec*** and Encodec \cite{encodec} with a four-stream sequence on a frameshift of 20ms, ***CoFi-Codec*** presents the lowest MCD and CER. 
> This result validates ***CoFi-Codec*** as the expected codec, providing multi-scale discrete speech representation and keeping high reconstruction quality.

### Recency Bias: Attention Visualization

> Finally, we attempt to investigate the `recency bias` issue in CLM-TTS by visualizing attention maps of different LMs when synthesizing the same utterance. 
> We aggregate attention maps across all heads and layers in one model by adding them together and apply the map with a scaling factor of 0.25 and a max clipping of 1.0 for clearer visualization. 
> As shown in Fig. \ref{fig:attn}, single-scale LM shows significant recency bias, where each speech frame pays much more attention to the nearest speech tokens and text tokens only relevant to the current pronunciation. 
> However, in CoFi-Speech-CoS, the high-scale sequence generation presents wider attention to the text and speech context, effectively modeling coarse-grained information. 
> It then guides the next-scale sequence generation to render fine-grained details. 
> Similarly, CoFi-Speech-SoS employs multiple LMs to achieve coarse-to-fine generation. 
> The first LM absorbs richer contextual information to generate the high-scale sequence and guides the next LMs to generate lower-scale sequences recursively. 
> This validates the effectiveness of `speaking from coarse to fine` in addressing `recency bias` via explicit multi-scale modeling.

## 6.Conclusions: 结论

> This work introduces CoFi-Speech, a novel approach to enhance CLM-based TTS through coarse-to-fine generation. 
> The framework comprises two key components: 1) ***CoFi-Codec***, a multi-scale speech codec that encodes speech into the discrete, multi-scale representation and decodes them back, and 2) ***CoFi-LM***, which predicts this representation using two strategies: single-LM-based chain-of-scale generation and multiple-LM-based stack-of-scale generation. 
> The zero-shot TTS experiment demonstrates that CoFi-Speech, especially for the stack-of-scale approach, significantly outperforms baseline systems using single-scale speech sequences in terms of naturalness and speaker similarity. 
> The ablation study further validates that: 1) ***CoFi-Codec*** is capable of learning multi-scale discrete representations while preserving high-quality speech reconstruction, and 2) ***CoFi-LM*** effectively improves TTS performance via explicit multi-scale modeling. 
> Finally, the attention visualization substantiates the efficacy of our approach in addressing `recency bias`, providing compelling evidence for its effectiveness in improving CLM-based TTS.
