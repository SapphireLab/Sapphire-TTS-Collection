# CoFi-Speech

- 标题: "Speaking from Coarse to Fine: Improving Neural Codec Language Model via Multi-Scale Speech Coding and Generation"

## Abstract: 摘要

> The neural codec language model (CLM) has demonstrated remarkable performance in text-to-speech (TTS) synthesis. However, troubled by `recency bias`, CLM lacks sufficient attention to coarse-grained information at a higher temporal scale, often producing unnatural or even unintelligible speech. 
> This work proposes CoFi-Speech, a coarse-to-fine CLM-TTS approach, employing multi-scale speech coding and generation to address this issue. 
> We train a multi-scale neural codec, CoFi-Codec, to encode speech into a multi-scale discrete representation, comprising multiple token sequences with different time resolutions. 
> Then, we propose CoFi-LM that can generate this representation in two modes: the single-LM-based chain-of-scale generation and the multiple-LM-based stack-of-scale generation. 
> In experiments, CoFi-Speech significantly outperforms single-scale baseline systems on naturalness and speaker similarity in zero-shot TTS. 
> The analysis of multi-scale coding demonstrates the effectiveness of CoFi-Codec in learning multi-scale discrete speech representations while keeping high-quality speech reconstruction. 
> The coarse-to-fine multi-scale generation, especially for the stack-of-scale approach, is also validated as a crucial approach in pursuing a high-quality neural codec language model for TTS.

## 1.Introduction: 引言

> The success of large language models (LLMs) in text domain \cite{brown2020language,openai2023gpt4,touvron2023llama2} has demonstrated their great capability in discrete sequence generation. It also inspires the birth of a new text-to-speech synthesis (TTS) paradigm based on the neural codec language model (CLM) \cite{VALLEX, tortoise, lajszczak2024base}, which treats TTS as a next-token prediction task. This framework usually relies on a neural codec \cite{encodec, hifi-codec, dac} to encode the speech audio into discrete tokens, which can be incorporated with the text sequence and generated by the LM, i.e. an auto-regressive decoder. Finally, we obtain the speech audio from these generated speech tokens via the codec decoder.
>
> However, different from the text, the discrete speech sequence is much longer to keep sufficient capacity to preserve phonetic and acoustic information. This long sequence length not only increases the complexity of TTS modeling but aggregates the `recency bias` of LMs \cite{peysakhovich2023attention, wang2024eliminating}, i.e. overly focusing on recent tokens during auto-regressive generation. This issue makes LM focus less on coarse-grained information \cite{guo2023msmc}, e.g. phonetics, prosody, and speaking style at higher and different temporal scales, hence causing unstable TTS performance, producing unnatural or even unintelligible speech. Although monotonic attention constraints \cite{han2024vall, du2024vall, wang2024attention} are proposed to fix stability issues, they still cannot solve `recency bias` fundamentally. Some works \cite{tortoise, socodec, li2024single} turn to directly model shorter speech sequences with a larger frameshift to avoid this issue, but limits the fine-grained expression of LMs in TTS. This dilemma implies the necessity of applying guidance to LMs to pay sufficient attention to both coarse-grained and fine-grained information of speech. 
>
> In this work, we propose a novel CLM-based zero-shot TTS approach, CoFi-Speech, that generates speech in a coarse-to-fine manner via a multi-scale speech coding and generation approach. In this framework, the multi-scale speech codec, CoFi-Codec, decomposes speech into multiple discrete sequences with different temporal resolutions and decodes them back with a high reconstruction quality. Then, we propose two LM-based approaches to predict this multi-scale speech representation from coarse to fine: single-LM-based chain-of-scale generation and multiple-LM-based stack-of-scale generation. In experiments, we present subjective and objective evaluations to demonstrate that CoFi-Speech significantly outperforms baseline systems based on single-scale speech sequences on naturalness and similarity, where stack-of-scale generation performs best. Finally, we conduct detailed ablation studies to analyze multi-scale coding and generation, to further validate the effectiveness of ``speaking from coarse to fine'' in achieving high-quality CLM-based TTS.

## 2.Related Works: 相关工作

## 3.Methodology: 方法

### CoFi-Codec

> CoFi-Codec aims to decompose speech into multiple discrete sequences with different resolutions to provide a multi-scale speech representation. Fig. \ref{fig:cofi} (a) presents a three-scale CoFi-Codec model architecture. We first convert speech signals into the Mel spectrogram, and employ multiple encoder blocks to down-sample it into sequences at different temporal scales. Specifically, each encoder block is applied with a 1-D convolutional ResNet block and a strided convolutional layer for down-sampling. Meanwhile, we follow SoCodec \cite{socodec} to apply an ECAPA-TDNN-based \cite{dawalatabad2021ecapa} reference encoder to extract a global embedding $g$ from the Mel spectrogram to capture time-invariant information, e.g., speaker identity, global speaking style, acoustic environment, etc.
>
> In decoding, we apply an equal number of decoder modules to quantize and decode these encoding sequences in a reversed order to reconstruct the Mel spectrogram from coarse to fine. For example, in $i$-th decoder block, we first extract the residual sequence at this stage by performing $\mathbf{e}^i - \mathbf{d}^i$, i.e. removing high-scale information from the decoding sequence $\mathbf{d}^i$ out of the encoding sequence $\mathbf{e}^i$. We then apply single-stream \cite{vqvae} or multi-stream vector quantization, e.g. RQ \cite{chen2010approximate} and PQ \cite{jegou2010product}, to obtain the quantized speech sequence $\mathbf{s}^i$. It is added with $\mathbf{d}^i$ and global embedding $g$, subsequently processed by a ResNet block and a transposed convolutional layer for up-sampling to produce the next decoding sequence $\mathbf{d}^{i-1}$. Notably, the highest-scale decoder block adopts an all-zero sequence as the decoding sequence. Finally, the Mel spectrogram is reconstructed from the decoder, and converted to the waveform via a pre-trained neural vocoder.

> In training, the model tends to overfit low-scale sequences with richer information, leading to high-scale representation collapse, i.e. nothing is preserved in the sequence. Hence, we propose scale-wise nested dropout (SWND), which masks quantized sequences $\mathbf{s}_{1:b}$ given an index $b$ randomly sampled from $[0, N_s - 1]$, where $b=0$ indicates that no sequence is masked. This approach forces high-scale sequence to preserve effective information for reconstruction, avoiding representation collapse. Moreover, we apply GAN training \cite{li2024single, socodec} on CoFi-Codec to improve the generation quality of the Mel spectrogram. The loss function is written as follows:

$$
\begin{aligned}
    \mathcal{L}_c &= \lambda_{vq} * \mathcal{L}_{vq} + \lambda_{reg} *\mathcal{L}_{reg} + 
    \lambda_{adv} * \mathcal{L}_{adv}
\end{aligned}
$$

> where $\mathcal{L}_{vq}$ is the vector-quantization loss, i.e. the averaged L2 loss between embeddings before and after VQ, $\mathcal{L}_{reg}$ is the L2-based regression loss on Mel spectrogram, and $\mathcal{L}_{adv}$ is the adversarial loss following \cite{socodec}.

### CoFi-LM

> To predict the multi-scale discrete representation in a coarse-to-fine manner, we propose CoFi-LM in two styles: chain-of-scale (CoS) generation and stack-of-scale (SoS) generation.

> **Chain-of-scale generation** is a simple extension of CLM to the multi-scale speech representation, which employs one LM to predict speech sequences in descending order of the temporal scale, as shown in Fig. \ref{fig:cofi} (b). In training, the text and speech sequences are embedded into the same space and processed by a decoder-only Transformer to estimate the probabilities of the next speech tokens. Notably, for multi-stream speech sequence, we follow the `delay pattern` \cite{copet2024simple, dang2024livespeech, lyth2024natural} to achieve the multi-stream prediction. This approach explicitly constrains the LM to model multi-scale information to alleviate `recency bias`. However, concatenating multiple sequences leads to a longer chain, causing higher computing costs and modeling complexity. To mitigate this issue, we propose stack-of-scale generation.

> **Stack-of-scale generation** employs a stack of LMs to generate speech sequences at different scales in cascade, as shown in Fig. \ref{fig:cofi} (c). It first generates the highest-scale sequence from the text and the reference embedding. The hidden states $\textbf{h}^3_{1:T_3}$ in the last transformer layer are employed to generate the next-scale sequence. It is placed at the head of the input sequence as the prompt, and also upsampled to be added to the lower-scale speech sequence to emphasize the alignment between these two sequences. In this way, we recursively generate all speech sequences from coarse to fine. This stage-wise approach enables us to better model multi-scale information while avoiding introducing long context to transformers.

> Finally, we apply in-context learning (ICL) \cite{anastassiou2024seed, du2024cosyvoice} to CoFi-LMs for zero-shot TTS. Specifically, we extract speaker embedding and speech tokens from the reference audio as speech prompts. The transcript of the reference audio is also concatenated with the input text as the text prompt.

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
