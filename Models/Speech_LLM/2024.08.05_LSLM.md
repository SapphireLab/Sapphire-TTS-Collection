# LSLM

<details>
<summary>基本信息</summary>

- 标题: Language Model Can Listen While Speaking
- 作者:
  - 01 [Ziyang Ma](../../Authors/Ziyang_Ma_(马子阳).md)
  - 02 [Yakun Song](../../Authors/Yakun_Song.md)
  - 03 [Chenpeng Du](../../Authors/Chenpeng_Du.md)
  - 04 [Jian Cong](../../Authors/Jian_Cong.md)
  - 05 [Zhuo Chen](../../Authors/Zhuo_Chen_(陈卓).md)
  - 06 [Yuping Wang](../../Authors/Yuping_Wang.md)
  - 07 [Yuxuan Wang](../../Authors/Yuxuan_Wang_(王雨轩).md)
  - 08 [Xie Chen](../../Authors/Xie_Chen_(陈谐).md)
- 机构:
  - [上海交通大学 X-LANCE](../../Institutions/SJTU_上海交通大学.md)
  - [字节跳动](../../Institutions/ByteDance.md)
- 时间:
  - 预印时间: 2024.08.05 ArXiv v1
  - 更新笔记: 2024.08.08
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2408.02622)
  - [DOI]()
  - [Github]()
  - [Demo](https://ziyang.tech/LSLM/)
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: ?
- 引用: ?
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Dialogue serves as the most natural manner of human-computer interaction (HCI). 
> Recent advancements in speech language models (SLM), have significantly enhanced speech-based conversational AI. 
> However, these models are limited to turn-based conversation, lacking the ability to interact with humans in real-time spoken scenarios, for example, being interrupted when the generated content is not satisfactory. 
> To address these limitations, we explore full duplex modeling (FDM) in interactive speech language models (iSLM), focusing on enhancing real-time interaction and, more explicitly, exploring the quintessential ability of interruption. 
> We introduce a novel model design, namely listening-while-speaking language model (LSLM), an end-to-end system equipped with both listening and speaking channels. 
> Our LSLM employs a token-based decoder-only TTS for speech generation and a streaming self-supervised learning (SSL) encoder for real-time audio input. 
> LSLM fuses both channels for autoregressive generation and detects turn-taking in real time. 
> Three fusion strategies—early fusion, middle fusion, and late fusion—are explored, with middle fusion achieving an optimal balance between speech generation and real-time interaction. 
> Two experimental settings, command-based FDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivity to diverse instructions. 
> Our results highlight LSLM's capability to achieve duplex communication with minimal impact on existing systems. 
> This study aims to advance the development of interactive speech dialogue systems, enhancing their applicability in real-world contexts.
> Demo can be found at https://ddlbojack.github.io/LSLM.

## 1.Introduction: 引言

> Dialogue is the most natural way of human-computer interaction (HCI). 
> With the rapid development of GPT-style~\citep{radford2018improving} large language models (LLM) and the scaling of Transformer-style~\citep{vaswani2017attention} architectures, textual conversational AI, such as ChatGPT~\citep{ouyang2022training, achiam2023gpt} and LLaMA~\citep{touvron2023llama1, touvron2023llama2}, have become a significant part of daily life. 
> However, these models are limited to text input and output and cannot interact directly with humans in arbitrary scenarios.
>
> Incorporating spoken and auditory interfaces into conversational AI enhances HCI convenience. 
> Leveraging techniques from text LLMs, the speech language model (SLM) processes speech similarly to text. 
> This paradigm involves encoding the speech signal into discrete tokens or continuous embeddings, modeling them with a language model, and decoding the speech tokens or embeddings back to the speech signal. Some studies~\citep{gslm, pgslm, dgslm} utilizes this paradigm for speech continuation, generating expressive speech and natural multi-round dialogue. 
> Other research employs this paradigm to task-specific applications, such as decoder-only high-fidelity TTS~\citep{wang2023neural, borsos2023audiolm, anastassiou2024seed, du2024cosyvoice} and decoder-only streaming ASR~\citep{seide2024speech, tsunoo2024decoder, chen2024streaming, chen2024bestow}
> Moreover, SpeechGPT~\citep{zhang2023speechgpt} and LauraGPT~\citep{chen2023lauragpt} initialize SLMs using LLMs, expanding speech tokens to the LLM vocabulary and continuing training on speech. This empowers SLM to comprehend semantic information and equips SLM with dialogue capability. 
> Despite these advances, all these models are limited to turn-based conversations and cannot handle real-time sound or interruptions, limiting their applicability in real-life scenarios. 
>
> Interaction and turn-taking are essential abilities for natural communication among humans. 
> At the dawn of the end-to-end speech dialogue system explosion, we focus on investigating \textbf{F}ull \textbf{D}uplex \textbf{M}odeling (\textbf{FDM}) in \textbf{i}nteractive \textbf{S}peech \textbf{L}anguage \textbf{M}odels (\textbf{iSLM}),  a crucial topic affecting user experience. 
> Lin et. al~\cite{lin2022duplex} proposes to process real-time audio input with a separate comprehension module. 
> Other works~\citep{zhang2024beyond, wang2024full} suggest modifying the order in which text tokens are organized in the LLM to tackle the duplex modeling problem. 
> All these models are based on text-centric LLMs that require external ASR and TTS modules for spoken dialogue. 
> As a result, latency remains perceivable and the paralinguistic ability is still lacking. 
> We believe the FDM capability should be an intrinsic capability of SLMs, enabling simultaneous listening and speaking. 
>
> To engage FDM capability for iSLM, we propose \textbf{L}istening-while-\textbf{S}peaking \textbf{L}anguage \textbf{M}odel (LSLM), an end-to-end model with both listening and speaking channels. 
> The proposed LSLM uses a token-based decoder-only TTS to model the ability to speak and a streaming self-supervised learning (SSL) encoder to model the ability to listen. 
> LSLM fuses these two channels and detects turn-taking in real time.
> We explore three strategies for fusing duplex signals: \textbf{Early Fusion}, \textbf{Middle Fusion}, and \textbf{Late Fusion}. 
> Experiments demonstrate that middle fusion achieves a good balance between speech generation and real-time interaction capabilities.
>
> In addition, interactive dialogue systems for realistic scenarios have two important features: 
> (1) Listening channels are not always clean. 
> Users may interact with iSLMs in different scenarios, containing high-frequency noise (e.g., telephone ringing) and low-frequency noise (e.g., white noise). 
> (2) It is possible that the iSLM interacts with an unseen speaker. iSLMs should recognize and respond to new voices and instructions, not dismiss them as noise. 
> Therefore, iSLM should have both robustness to noise and sensitivity to unseen speakers. 
> To test LSLM, we designed two scenarios: **Command-based FDM**, where LSLM is interrupted by a specific command, and **Voice-based FDM**, where LSLM can be interrupted by various words from unseen speakers. 
> Experimental results show that LSLM with a listening channel is robust to noisy input and sensitive to turning-taking.

> Our contributions are summarized as follows:
>
> - We formulate an important task, \textbf{F}ull \textbf{D}uplex \textbf{M}odeling (\textbf{FDM}), applied in the interactive speech language model (\textbf{iSLM}). 
> - We propose \textbf{L}istening-while-\textbf{S}peaking \textbf{L}anguage \textbf{M}odel (LSLM), an end-to-end single model with the focus of modeling the turn-taking problem. LSLM can listen to the outside signal and provide feedback in real time while speaking.  
> - We introduce three methods for fusing duplex signals: \textbf{Early Fusion}, \textbf{Middle Fusion}, and \textbf{Late Fusion}, with Middle Fusion providing the optimal tradeoff between speech generation and real-time interaction. 
> - We tested the FDM ability of the proposed LSLM in two scenarios: \textbf{Command-based FDM} and \textbf{Voice-based FDM}. Experiments indicate that our proposed LSLM can achieve duplexing capability with little impact on the previous system. 

## 2.Related Works: 相关工作

> Figure~\ref{fig:duplex} illustrates the distinctions between simplex, half duplex, and full duplex speech language models from a telecommunication perspective. An SLM with full duplex modeling (FDM) capability can be referred to as an interactive speech language model (iSLM). 

### Simplex and Half Duplex Speech Language Model

> Simplex SLMs, depicted in Figure~\ref{fig:duplex}(A) and \ref{fig:duplex}(B), are limited to a single channel, either for listening or speaking. 
> With the assistance of LLM, simplex SLMs exhibit strong understanding capabilities. 
> Representative works include LLM-based ASR~\citep{yu2023connecting, ma2024embarrassingly, yang2024mala, bai2024seed}, LLM-based speech translation~\citep{pan2023cosmic, chen2024salm, huang2024investigating, chen2024llast}, and LLM-based speech emotion understanding~\citep{xu2024secap, lin2024advancing, lian2024affectgpt}. 
> Similarly, simplex SLMs have demonstrated robust generation capabilities, as seen in LLM-based TTS~\citep{hao2023boosting, neekhara2024improving, lajszczak2024base, anastassiou2024seed}. 
> Some research leverages the powerful in-context learning capabilities of LLMs to extend task-specific abilities to more universal applications, such as speech understanding~\citep{deng2024wav2prompt}, audio understanding~\citep{gong2023listen}, or both~\citep{tang2023salmonn, chu2023qwen, chu2024qwen2}. 
> Despite their growing power and versatility, simplex SLMs are limited to one-way communication (either human $\rightarrow$ machine or machine $\rightarrow$ human). 
> LLMs have facilitated a paradigm shift from simplex models to half-duplex models, also known as turn-based models, as shown in Figure~\ref{fig:duplex}(C).  
> Prominent models include SpeechGPT~\citep{zhang2023speechgpt}, LauraGPT~\citep{chen2023lauragpt}, and VioLA~\citep{wang2023viola}. 
> While these half duplex models can both listen and speak, they are constrained to performing only one action at the same instant, thus failing to address the turn-taking problem. 

### Full Duplex Speech Language Model

> Full duplex SLMs, as shown in Figure~\ref{fig:duplex}(D), have the capability to listen and speak simultaneously, allowing for turn-taking whenever a human interrupts the machine. 
> Recent efforts~\citep{zhang2024beyond, wang2024full} have attempted to build full duplex capabilities on text-centric LLMs with cascade ASR and TTS modules. 
> Cutting-edge products like GPT-4o~\footnote{\url{https://openai.com/index/hello-gpt-4o}} and Moshi~\footnote{\url{https://moshi.chat}} exhibit full duplex capability in their spoken dialogue systems. 
> Despite these advancements, there are no publicly available open-source models or detailed analyses of full duplex SLMs. This gap highlights the need for further research and development to fully understand and optimize full duplex capability in speech language models. 

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
