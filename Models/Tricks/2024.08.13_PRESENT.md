# PRESENT

<details>
<summary>基本信息</summary>

- 标题: PRESENT: Zero-Shot Text-to-Prosody Control
- 作者:
  - 01 [Perry Lam](../../Authors/Perry_Lam.md)
  - 02 [Huayun Zhang](../../Authors/Huayun_Zhang.md)
  - 03 [Nancy F. Chen](../../Authors/Nancy_F._Chen.md)
  - 04 [Berrak Sisman](../../Authors/Berrak_Sisman.md)
  - 05 [Dorien Herremans](../../Authors/Dorien_Herremans.md)
- 机构:
  - 机构 
- 时间:
  - 预印时间: 2024.08.13 ArXiv v1
  - 更新笔记: 2024.08.15
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2408.06827)
  - [DOI]()
  - [Github](https://github.com/iamanigeeit/present)
  - [Demo](https://present2023.web.app/)
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: ?
- 引用: ?
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Current strategies for achieving fine-grained prosody control in speech synthesis entail extracting additional style embeddings or adopting more complex architectures. To enable zero-shot application of pretrained text-to-speech (TTS) models, we present PRESENT (PRosody Editing without Style Embeddings or New Training), which exploits explicit prosody prediction in FastSpeech2-based models by modifying the inference process directly. We apply our text-to-prosody framework to zero-shot language transfer using a JETS model exclusively trained on English LJSpeech data. We obtain character error rates (CER) of 12.8\%, 18.7\% and 5.9\% for German, Hungarian and Spanish respectively, beating the previous state-of-the-art CER by over 2$\times$ for all three languages. Furthermore, we allow subphoneme-level control, a first in this field. To evaluate its effectiveness, we show that PRESENT can improve the prosody of questions, and use it to generate Mandarin, a tonal language where vowel pitch varies at subphoneme level. We attain 25.3\% hanzi CER and 13.0\% pinyin CER with the JETS model.
> All our code and audio samples are available online.
> Code: https://github.com/iamanigeeit/present
> Audio Samples: https://present2023.web.app/

## 1.Introduction: 引言

> Recent neural text-to-speech (TTS) models have approached human-like naturalness in read speech. However, attaining similar expressiveness levels remains a challenge. A growing body of research aims to add and control speech prosody variations, progressing from digital signal processing (DSP) methods to style and emotion embeddings built into TTS architectures or even entire models to extract and transfer prosody.
>
> On the waveform level, prosody control can be achieved through operations like time-stretching and pitch-shifting. DSP methods such as TD-PSOLA \cite{TD-PSOLA} and WORLD \cite{WORLD}, despite their known artifacts, are still widely applied due to their speed and ease of use. Remarkably, they can perform as effectively as neural approaches like Controllable LPCNet \cite{CLPCNet}.
>
> In contrast, expressive TTS systems \cite{expressive-tts} allow the user to specify a style or emotion label during inference.
> Recent TTS models incorporate style or emotion information by extracting a reference embedding that represents the prosody or emotion from labelled audio, and adding it to the model encoder. This can be combined with a style bank for smooth style variation, such as in Global Style Tokens \cite{gst}. Further extensions include phoneme-level prosody control and hierarchical autoencoders to ensure coherence over the whole utterance \cite{chive}. 
>
> All of these approaches, however, require extra model components and/or further training. Therefore, to combine the simplicity of DSP methods with the naturalness of neural speech generation, we empower users to directly control prosody using the input text and inference parameters without the need for any fine-tuning or architectural modifications. We contribute significantly in the following three areas:
>
> - **Extraction of prosodic effects from text**, such as extended duration in `A looooong time` or the intonation variations in questions like `What was that?`. We take these prosodic parameters and modify the inference method of any TTS model with explicit duration, pitch, and energy (DPE) predictions to generate varying speech.
> - **Zero-shot language transfer** with no target-language audio, relying solely on linguistic knowledge and modifying DPE to create new phonemes and speech patterns.
> - **Subphoneme-level control**, achieved by subdividing phonemes and applying custom pitch and energy over the subdivisions, which helps us change long vowel intonation and model tonal languages like Mandarin.
>
> Though our primary goal is to explore the limits of editing inference-time prosody predictions, in doing so, we achieve state-of-the-art results in zero-shot language transfer.
> 
> The rest of this paper is organized as follows: Section 2 summarizes relevant research, Section 3 describes our approach, Section 4 lists our experiment results and Section 5 concludes our paper. 

## 2.Related Works: 相关工作

> Based on our main contributions, we divide the related work into the broad categories of (1) speech effect tagging, (2) zero-shot language transfer, and (3) fine-grained prosody control.

### A.Speech Effect Tagging

> Text-based methods for manipulating speech can be categorized into explicit and implicit forms. Explicit speech descriptors such as gender and emphasis have been integrated into the industry standard Speech Synthesis Markup Language (SSML) over the past two decades \cite{SSML}. 
> Yet, there has been relatively limited published research on SSML, even though there have been notable introductions of TTS models with new style tags, as demonstrated in \cite{style-tag}.
>
> Implicit methods establish connections between prosodic features and text, such that a sentence like "this is interesting!" would sound excited. Typically, this means that the text embeddings from a language model are used as input either at the subword \cite{cauliflow} \cite{speech-bert} or phoneme level \cite{phoneme-bert} \cite{png-bert}.  However, due to their inherent limitations in customizing prosody changes, recent projects inspired by advancements in computer vision and language processing let user input a natural-language style prompt like `sighing with helpless feeling` to generate prosodic output \cite{instruct-tts}.

### B.Zero-Shot Language Transfer

> While multilingual TTS models have existed for some time, they rely on large multilingual corpora, which disadvantages lower-resourced languages. Transfer learning \cite{meta-learning} \cite{low-resource-transfer} and data balancing \cite{universal-tts} techniques have been employed, but these still require at least some audio data. With only International Phonetic Alphabet (IPA) transcriptions in the target language, \cite{phone-features-0-shot} proposed using IPA phonological features to extend existing models on unseen phonemes, whereas two very recent large models have proposed zero-shot TTS with only text data available in the target language.
>
> The first model, VALL-E X \cite{VALL-E-X}, uses AudioLM \cite{audio-lm} codec codes as acoustic tokens in place of mel spectrograms as intermediate features, and treats the cross-lingual TTS model as a massive language model (LLM) that can be trained with self-supervised masking. Given a speech sample in the source language, plus source and target language phoneme sequences, it extracts the source acoustic tokens from the speech sample and the LM predicts the target acoustic tokens. Since the acoustic tokens contain speaker, recording conditions, and subphoneme information, the decoder can reconstruct the waveform for the target language in the source speaker's voice.
>
> The second model, ZM-Text-TTS \cite{zero-shot-tts}, also uses masked multilingual training, but on IPA / byte tokens and raw text. The pretraining results in a language-aware embedding layer that is fed to a conventional multilingual TTS system for training with seen languages, and the model can accept IPA / byte tokens for unseen languages during inference. Nevertheless, VALL-E X is not publicly available, and ZM-Text-TTS does not account for prosody in language transfer.

### C.Fine-Grained Prosody Control

> As utterance-level styles are now commonplace, research has shifted to controlling prosody at the phoneme level. Since acceptable prosodies are obtained by learning and sampling from a variational latent space, hierarchical variational auto-encoders (VAEs) \cite{hierarchical-vae} can achieve fine prosodic gradations, down to the syllable, phone or even frame level \cite{chive}.
>
> Alternatively, others use phone-level DPE for interpretable prosody control. This was the approach of earlier research~\cite{robust-prosody}, but to improve output naturalness, \cite{prosody-clustering} and \cite{prosody-tokens} used k-means clustering on duration and pitch for each speaker, and kept the resulting centroids as discrete prosody tokens. This allows the tokens to be substituted at inference time to customize prosody, while decoding with a prosody attention module ensures information flows to the output. Meanwhile, since the advent of explicit DPE models like FastSpeech2 \cite{fastspeech2}, models like \cite{emotional-prosody} and \cite{empathic} have extra modules attached that accept emotional dimensions (valence, arousal, dominance) that feed into phone-level DPE predictors, allowing for continuous emotion control.

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

> We have introduced PRESENT, a novel approach that explores the limits of using only DPE predictions in a single-speaker English-only JETS model, without any additional embeddings or training. Our technique allows us to create prosodic effects from text and synthesize speech in unseen languages.
> Our zero-shot language transfer far outstrips previous state-of-the-art for European languages. 
> Furthermore, the phoneme conversion and tone contour techniques we develop could enable direct accented speech generation (as the results are all American-accented), or TTS for hundreds of tonal minority languages within the Mainland Southeast Asian linguistic area that are only recorded in phonetic transcriptions.
