# Super-MAS (Super Monotonic Alignment Search)

<details>
<summary>基本信息</summary>

- 标题: Super Monotonic Alignment Search
- 作者:
  1. Junhyeok Lee (Johns Hopkins University/Supertone Inc.)
  2. Hyeongju Kim (Supertone Inc.)
- 机构:
  1. Johns Hopkins University
  2. Supertone Inc.
- 时间:
  - 预印时间: 2024.09.12 ArXiv v1
  - 更新笔记: 2024.09.14
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2409.07704)
  - [DOI]()
  - [Github](https://github.com/supertone-inc/super-monotonic-align)
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: 5
- 引用: ?
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most popular algorithm in TTS to estimate unknown alignments between text and speech. Since this algorithm needs to search for the most probable alignment with dynamic programming by caching all paths, the time complexity of the algorithm is $O(T \times S)$. The authors of Glow-TTS run this algorithm on CPU, and while they mentioned it is difficult to parallelize, we found that MAS can be parallelized in text-length dimension and CPU execution consumes an inordinate amount of time for inter-device copy. Therefore, we implemented a Triton kernel and PyTorch JIT script to accelerate MAS on GPU without inter-device copy.
> As a result, Super-MAS Triton kernel is up to 72 times faster in the extreme-length case. The code is available at [Github](https://github.com/supertone-inc/super-monotonic-align).

## 1.Introduction: 引言

> Monotonic alignment search (MAS) is an algorithm introduced by Kim \textit{et al.} \cite{glowtts}, which is applied to estimate unknown alignments between text and speech in a self-supervised manner.
> Since this algorithm only needs text and speech pairs, many non-autoregressive TTS models are utilizing MAS during training \cite{glowtts,vits, pits, dualspeech}.
> While original MAS utilized mel spectrogram, there are other studies utilizing Yingram \cite{nansy,pits} or NANSY feature \cite{nansypp,dualspeech} to MAS, we use the term speech representation instead of mel spectrogram.
> Since this algorithm needs to search for the most probable alignment with dynamic programming by caching all paths, the time complexity of the algorithm is $O(T \times S)$, where $T$ is the length of text and $S$ is the length of speech representation. 
> Official implementation of MAS is implemented with Cython \cite{cython} and calculating it on CPU with nested loops, while the authors mentioned it is difficult to parallelize.
> However, we found that MAS can be parallelized in text-length dimension and CPU execution is needed to copy large-size tensors between CPU and GPU, which consumes an inordinate amount of time.
> Therefore, we implemented MAS with Triton kernel \cite{triton} and PyTorch JIT script \cite{pytorch} to accelerate MAS on GPU without the nested loops and inter-device copy.
> Especially, the Super-MAS Triton kernel is at least 19 times faster and up to 72 times faster than the original Cython implementation.

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

> In this report, we implemented parallelized MAS in multiple frameworks and versions.
> Our Super-MAS Triton kernel shows the best time that it is at least 19 times faster and up to 72 times after than the original Cython implementation.
> Our kernel does not contain kernel fusing for calculating log-likelihood and other additional optimization, and it should be improved by additional efforts.
> We believe this work can be utilized in various applications, including future non-autoregressive TTS models, alignment estimation for automatic speech recognition models, and other scenarios requiring monotonic alignment. 
