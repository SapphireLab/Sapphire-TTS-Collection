# TS3-Codec (Transformer-Based Simple Streaming Single Codec)

<details>
<summary>基本信息</summary>

- 标题: "TS3-Codec: Transformer-Based Simple Streaming Single Codec"
- 作者:
  - 01 Haibin Wu,
  - 02 Naoyuki Kanda,
  - 03 Sefik Emre Eskimez,
  - 04 Jinyu Li
- 链接:
  - [ArXiv](https://arxiv.org/abs/2411.18803)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2411.18803v1__TS3-Codec__Transformer-Based_Simple_Streaming_Single_Codec.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Neural audio codecs (NACs) have garnered significant attention as key technologies for audio compression as well as audio representation for speech language models.
While mainstream NAC models are predominantly convolution-based, the performance of NACs with a purely transformer-based, and convolution-free architecture remains unexplored.
This paper introduces ***TS3-Codec***, a Transformer-Based Simple Streaming Single Codec.
***TS3-Codec*** consists of only a stack of transformer layers with a few linear layers, offering greater simplicity and expressiveness by fully eliminating convolution layers that require careful hyperparameter tuning and large computations.
Under the streaming setup, the proposed ***TS3-Codec*** achieves comparable or superior performance compared to the codec with state-of-the-art convolution-based architecture while requiring only 12% of the computation and 77% of bitrate.
Furthermore, it significantly outperforms the convolution-based codec when using similar computational resources.

</details>
<br>

神经音频编解码器 (Neural Audio Codecs, NACs) 作为音频压缩和语音语言模型的音频表示的核心技术, 已经获得了广泛关注.
主流的 NAC 模型大多数都是基于卷积的, 而基于 Transformer 无卷积架构的 NAC 性能仍然未被探索.

本文介绍了 ***TS3-Codec***, 一种基于 Transformer 的简单流式单一编解码器.
***TS3-Codec*** 由 Transformer 层和少量线性层的堆叠组成, 完全消除了卷积层, 这使得模型的复杂度和表达能力大大降低, 并且不需要进行精心的超参数调整和大量计算.
在流式设置下, 所提出的 ***TS3-Codec*** 与具有最先进卷积架构的编解码器相比, 取得了相当或更好的性能, 仅需 12% 的计算量和 77% 的比特率.
此外, 它在相同的计算资源下, 相比于卷积架构, 表现出了明显的优势.

## 1·Introduction: 引言

Neural audio codec (NAC) is a technique to compress audio signals into a sequence of discretized codes for efficient data storage and transmission \cite{kim2024neural, wu-etal-2024-codec, shi2024espnet, wu2024codec, mousavi2024dasb}.
More recently, NAC has also gained significant attention
as a key technology for speech language modeling (SLM) \cite{audiolm,wang2023neural,wang2024speechx}.
By converting continuous audio into discrete codes, large language modeling (LLM) techniques—already highly successful in text processing—is able to be applied to versatile speech processing \cite{wu2024towards}.

Numerous high-performance NACs have been proposed\footnote{\url{https://github.com/ga642381/speech-trident}}, addressing various aspects, e.g. better audio quality, bitrate efficiency, and low computational cost.
Most models rely on convolutional layers as the dominant architecture, with only a few \cite{ji2024wavtokenizer, defossez2024moshi} incorporating transformers (or self-attention mechanism) \cite{vaswani2017attention} as intermediate layers within the convolutional encoder-decoder framework.
However, the performance of a purely transformer-based and convolution-free architecture in NACs remains unexplored.
This study aims to fill the existing gap by developing a NAC exclusively based on transformer models. It leverages the benefits of transformers, such as simplicity in model design and enhanced computational efficiency when compared to convolution-based models.

When the NAC is used as the token representation for SLMs, the following properties are particularly important.

- Streaming: Full-duplex communication, where users and machines can speak and respond simultaneously, is a popular and ongoing challenge in the SLM field \cite{defossez2024moshi,dGSLM}. To enable seamless real-time interactions, the codec must support streaming processing, allowing it to encode user speech and generate speech response with low latency.
- Single codebook: A single codebook-based model is preferable to a multiple-codebook-based model, such as residual vector quantization (RVQ)~\cite{zeghidour2021soundstream, defossez2022high}, because the latter introduces additional complexity to the architecture of SLMs, such as the combination of auto-regressive and non-autoregressive models \cite{wang2023neural}, the temporal and depth transformers \cite{yu2023megabyte,yang2023uniaudio}, etc.
- Low computation: Low-computation NACs enable faster encoding and decoding, reducing computational demands and leaving more computation resources available for SLMs.
- Low token rate: Long sequences generally make LLM training slow and unstable. Therefore, it is preferable to use low-token-rate NAC models for SLM.

This paper introduces TS3-Codec (Transformer-Based Simple Streaming Single Codec), the first attempt to develop a convolution-free, transformer-only NAC.
TS3-Codec consists of only a stack of transformer layers with a few linear layers, offering greater simplicity and expressiveness by fully eliminating convolution layers that require careful hyperparameter tuning and large computations.
The proposed TS3-Codec offers several advantages, namely, streaming capability, low computational requirements, low bitrate, and a single codebook design while maintaining high audio quality.
In the streaming setup, the proposed TS3-Codec delivers comparable or superior performance than convolution-based codecs with just 12\% of the computation and 77\% of bitrate.
TS3-Codec also achieves significantly better audio quality when using the same computational resources.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论