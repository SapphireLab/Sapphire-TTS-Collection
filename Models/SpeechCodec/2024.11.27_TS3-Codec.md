# TS3-Codec (Transformer-Based Simple Streaming Single Codec)

<details>
<summary>基本信息</summary>

- 标题: "TS3-Codec: Transformer-Based Simple Streaming Single Codec"
- 作者:
  - 01 Haibin Wu,
  - 02 Naoyuki Kanda,
  - 03 Sefik Emre Eskimez,
  - 04 Jinyu Li
- 链接:
  - [ArXiv](https://arxiv.org/abs/2411.18803)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2411.18803v1__TS3-Codec__Transformer-Based_Simple_Streaming_Single_Codec.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Neural audio codecs (NACs) have garnered significant attention as key technologies for audio compression as well as audio representation for speech language models.
While mainstream NAC models are predominantly convolution-based, the performance of NACs with a purely transformer-based, and convolution-free architecture remains unexplored.
This paper introduces ***TS3-Codec***, a Transformer-Based Simple Streaming Single Codec.
***TS3-Codec*** consists of only a stack of transformer layers with a few linear layers, offering greater simplicity and expressiveness by fully eliminating convolution layers that require careful hyperparameter tuning and large computations.
Under the streaming setup, the proposed ***TS3-Codec*** achieves comparable or superior performance compared to the codec with state-of-the-art convolution-based architecture while requiring only 12% of the computation and 77% of bitrate.
Furthermore, it significantly outperforms the convolution-based codec when using similar computational resources.

</details>
<br>

神经音频编解码器 (Neural Audio Codecs, NACs) 作为音频压缩和语音语言模型的音频表示的核心技术, 已经获得了广泛关注.
主流的 NAC 模型大多数都是基于卷积的, 而基于 Transformer 无卷积架构的 NAC 性能仍然未被探索.

本文介绍了 ***TS3-Codec***, 一种基于 Transformer 的简单流式单一编解码器.
***TS3-Codec*** 由 Transformer 层和少量线性层的堆叠组成, 完全消除了卷积层, 这使得模型的复杂度和表达能力大大降低, 并且不需要进行精心的超参数调整和大量计算.
在流式设置下, 所提出的 ***TS3-Codec*** 与具有最先进卷积架构的编解码器相比, 取得了相当或更好的性能, 仅需 12% 的计算量和 77% 的比特率.
此外, 它在相同的计算资源下, 相比于卷积架构, 表现出了明显的优势.

## 1·Introduction: 引言

Neural audio codec (NAC) is a technique to compress audio signals into a sequence of discretized codes for efficient data storage and transmission \cite{kim2024neural, wu-etal-2024-codec, shi2024espnet, wu2024codec, mousavi2024dasb}.
More recently, NAC has also gained significant attention as a key technology for speech language modeling (SLM) \cite{audiolm,wang2023neural,wang2024speechx}.
By converting continuous audio into discrete codes, large language modeling (LLM) techniques—already highly successful in text processing—is able to be applied to versatile speech processing \cite{wu2024towards}.

Numerous high-performance NACs have been proposed\footnote{\url{https://github.com/ga642381/speech-trident}}, addressing various aspects, e.g. better audio quality, bitrate efficiency, and low computational cost.
Most models rely on convolutional layers as the dominant architecture, with only a few \cite{ji2024wavtokenizer, defossez2024moshi} incorporating transformers (or self-attention mechanism) \cite{vaswani2017attention} as intermediate layers within the convolutional encoder-decoder framework.
However, the performance of a purely transformer-based and convolution-free architecture in NACs remains unexplored.
This study aims to fill the existing gap by developing a NAC exclusively based on transformer models. It leverages the benefits of transformers, such as simplicity in model design and enhanced computational efficiency when compared to convolution-based models.

When the NAC is used as the token representation for SLMs, the following properties are particularly important.

- Streaming: Full-duplex communication, where users and machines can speak and respond simultaneously, is a popular and ongoing challenge in the SLM field \cite{defossez2024moshi,dGSLM}. To enable seamless real-time interactions, the codec must support streaming processing, allowing it to encode user speech and generate speech response with low latency.
- Single codebook: A single codebook-based model is preferable to a multiple-codebook-based model, such as residual vector quantization (RVQ)~\cite{zeghidour2021soundstream, defossez2022high}, because the latter introduces additional complexity to the architecture of SLMs, such as the combination of auto-regressive and non-autoregressive models \cite{wang2023neural}, the temporal and depth transformers \cite{yu2023megabyte,yang2023uniaudio}, etc.
- Low computation: Low-computation NACs enable faster encoding and decoding, reducing computational demands and leaving more computation resources available for SLMs.
- Low token rate: Long sequences generally make LLM training slow and unstable. Therefore, it is preferable to use low-token-rate NAC models for SLM.

This paper introduces TS3-Codec (Transformer-Based Simple Streaming Single Codec), the first attempt to develop a convolution-free, transformer-only NAC.
TS3-Codec consists of only a stack of transformer layers with a few linear layers, offering greater simplicity and expressiveness by fully eliminating convolution layers that require careful hyperparameter tuning and large computations.
The proposed TS3-Codec offers several advantages, namely, streaming capability, low computational requirements, low bitrate, and a single codebook design while maintaining high audio quality.
In the streaming setup, the proposed TS3-Codec delivers comparable or superior performance than convolution-based codecs with just 12\% of the computation and 77\% of bitrate.
TS3-Codec also achieves significantly better audio quality when using the same computational resources.

## 2·Related Works: 相关工作

Neural audio codec models typically consist of an encoder, a vector quantization (VQ) module, and a decoder.
The encoder downsamples the time-domain audio to extract frame-wise audio features, typically with a frame rate of 12.5–100 Hz.
The vector quantization module — whether single-quantizer vector quantization \cite{van2017neural}, residual vector quantization (RVQ) \cite{zeghidour2021soundstream, defossez2022high}, or scalar quantization (SQ) \cite{mentzer2023finite} — converts each frame-wise audio feature into discrete tokens.
These discrete tokens can be used for efficient transmission or as input for SLMs.
Finally, the decoder reconstructs the time-domain audio signal from the discrete tokens.
To meet the requirements of SLMs for real-time conversational agents with full-duplex mode, a suitable codec is better to support streaming, low computational complexity, a single codebook, and a low token rate.

### Streaming

Encodec \cite{defossez2022high} is one of the pioneers in achieving real-time, high-quality audio coding by utilizing a streaming encoder-decoder architecture with residual vector quantization.
To enable full-duplex operation in SLMs, Mimi \cite{defossez2024moshi} is proposed as a streaming RVQ codec with a 12.5 Hz token rate.
It employs techniques, e.g. semantic distillation, discriminator-only training without reconstruction loss, and the integration of transformer layers between the encoder and decoder backbone to enhance feature modeling.
Mimi achieves a low bitrate of 1.1 kbps while maintaining very high speech quality.

### Single codebook

A single codebook-based model is preferable to a multiple-codebook-based model, because it offer the simplicity design of the SLMs.
Multiple-codebook-based codecs (e.g. RVQ-based codec) compress speech into multiple streams of tokens. Unlike text tokens, which form a single stream, modeling multiple audio token streams requires a more complex model design for decoding. Though various decoding strategies have been proposed, including decoding different streams in separate steps \cite{audiolm,wang2023neural}, introducing delay patterns across various stream tokens \cite{copet2024simple}, and combining temporal and depth transformers for generation \cite{yu2023megabyte,yang2023uniaudio}, these multi-stream approaches obviously increase the complexity of model design. In contrast, single-stream codecs offer simple model design.

TiCodec \cite{ren2024fewer} and SingleCodec \cite{li2024single} are designed to encode speech using fewer tokens by disentangling time-invariant global information (e.g., speaker timbre and acoustic environment) into a single utterance-level vector, while representing time-varying information (e.g., phonetic content) with a single stream of frame-level tokens.
The time-invariant utterance embeddings and frame-level token sequences are then combined to reconstruct the audio.
However, a limitation of these codecs is their reliance on global utterance-level features for decoding, which restricts their streaming capability.
WavTokenizer \cite{ji2024wavtokenizer} employs several techniques, including codebook size optimization, k-means initialization for codebook embeddings, the Vocos decoder \cite{siuzdak2023vocos}, and training on 80,000 hours of data, to achieve high fidelity in a single-codebook low-bitrate codec.
BigCodec \cite{xin2024bigcodec} achieves exceptionally high reconstruction quality at a low bitrate of 1.04 kbps by scaling the convolutional model size to 160M parameters, making it a strong convolutional baseline for our work.

### Uniqueness and contributions of this work

This paper presents the first transformer-only codec architecture designed for streaming, featuring low computational complexity and a simple model design.
Previous works have utilized transformers \cite{defossez2024moshi,ji2024wavtokenizer} solely as intermediate layers within predominantly convolutional backbones for feature engineering.
Furthermore, previous streaming codecs \cite{defossez2024moshi,defossez2022high,wu2023audiodec} rely on residual vector quantization (RVQ), and single-codebook codecs \cite{ren2024fewer,li2024single,ji2024wavtokenizer,xin2024bigcodec} have lacked a streaming design.
Our work is the first to introduce a single-codebook codec specifically designed for streaming.
This work is also the first to explore single codec designs with 65k and 130k codebook sizes.

## 3·Methodology: 方法

### Model Architecture: 模型架构

TS3-Codec follows the conventional NAC structure, which includes an encoder, a quantizer, and a decoder, as shown in Figure~\ref{fig:framework}. The model is trained using the generative adversarial network (GAN) framework \cite{goodfellow2020generative}.

The encoder consists of two linear layers and a stack of transformer layers with sliding window attention only on the left context.
The input audio signal with a shape of $\mathbb{R}^T$, where $T$ is the signal length, is first reshaped into a two-dimensional tensor with a shape of $\mathbb{R}^{F\times N}$, where $F$ is a window-size and $N=\frac{T}{F}$ is a number of windowed frames.\footnote{For simplicity, we assume $T$ is divisible by $F$. We can pad the original signal to satisfy this assumption.}
Two linear layers without activations (the one close to the waveform is without bias, and the second linear is with bias) are then applied to convert the reshaped windowed frames into a shape of $\mathbb{R}^{D\times N}$.
Finally, a stack of Transformer layers with an embedding size of $D$ is applied to output an encoder embedding with a shape of $\mathbb{R}^{D\times N}$.
The Transformer layer employs a sliding window with the size of either 16 or 32 on the left-context for the self-attention operation, meaning only a fixed number of previous frames are considered.
This design not only ensures that the transformer's computational complexity does not brow up to the quadratic with respect to $N$, but also improves the generalization ability of the model for an audio longer than the training data.
If not specified, the encoder has 8 transformer layers, 16 attention heads, and a feed-forward dimension of 4096.

After the encoder module, a factorized VQ layer \cite{van2017neural} discretizes the encoder output in a low-dimensional space. We used a codebook with a codebook dimension of 8 or 16, and a codebook size of 65,536 or 131,072.

The decoder has a symmetric structure with the encoder, consisting of a stack of transformer layers with two linear layers.
Unless otherwise specified, the decoder uses the same parameters as the encoder, namely, 8 transformer layers, 16 attention heads, and a feed-forward dimension of 4096.
The output from the transformer layer has a shape of $\mathbb{R}^{D\times N}$.
It is then converted into a tensor with a shape of $\mathbb{R}^{F\times N}$ based on two linear layers without activations (the first one is with bias, and the second one doesn't have bias).
Finally, the tensor is reshaped into a single-dimensional tensor with a shape of $\mathbb{R}^{T}$.\footnote{ Our decoder is largely inspired by the WaveNeXt vocoder \cite{okamoto2023wavenext}, which demonstrated that up-sampling at the final layer using a simple linear layer is sufficient to reconstruct high-quality speech, as opposed to progressive up-sampling through stacked convolution layers. We refer to the implementation in \url{https://github.com/wetdog/wavenext_pytorch}}

### Design Principle: 设计原则

We chose transformers as the backbone for several potential advantages over convolution-based architectures.

- Convolutional layers are well-known for their parameter efficiency and reusability. On the other hand, for models of similar parameter sizes, convolutions typically require significantly more computation than transformers \cite{yuzhang2024lti}.
Surprisingly, we discovered that the state-of-the-art neural audio codec model, BigCodec \cite{xin2024bigcodec}, with 160M parameters, has a computational cost comparable to that of a \textbf{1-billion-parameter} transformer.\footnote{ At a bitrate of 1k, TS3-Codec (1.6B parameters) requires 60.52G MACs, while BigCodec (160M parameters) requires 61.1G MACs. At a bitrate of 0.6k, TS3-Codec (1.2B parameters) uses 36.34G MACs, while BigCodec (160M parameters) uses 39.6G MACs.}
The computational cost increases significantly as convolutional neural audio codec models are scaled up, making it impractical to scale these models further.
- Convolutions have inherent biases. Convolutions apply fixed weighted-sum weights across all intermediate feature maps across different time stamps, whereas transformers use self-attention, dynamically determining weights tailored to each feature map. Transformers also incorporate positional embeddings, enabling distinct embeddings to be added to different feature maps at different time stamps.
- Transformers offer simplicity in model design. Unlike convolutions, which require careful selection of kernels and up- and down-sampling mechanisms due to their inherent biases (they are sensitive to hyperparameter settings), transformers could avoid these complexities.

### Training Objective: 训练目标

Similar to most NACs, TS3-Codec is trained based on the GAN framework.
We use multiple losses following BigCodec \cite{xin2024bigcodec}.

- Reconstruction Loss: % Inspired by DAC \cite{kumar2024high},
We adopt a multi-scale mel-spectrogram reconstruction loss, calculated using the $L_{1}$ distance in the spectral domain across multiple scales. The mel-spectrogram is closely related to perceptual audio quality.
- Least-square GAN loss \cite{mao2017least}:
Following the BigCodec \cite{xin2024bigcodec}, we employ two types of discriminators to train our model.
The first is the Multi-Period Discriminator (MPD), adapted from HiFi-GAN \cite{kong2020hifi}, which captures various periodic patterns in speech signals. The second is the Multi-Scale Short-Time Fourier Transform (MS-STFT) Discriminator, which is used in EnCodec \cite{defossez2022high}. We use the same discriminator configurations as BigCodec.
- Feature loss: we use the $L_{1}$ feature matching loss for the discriminator features.
- VQ loss: The codebook is trained using the $L_{1}$ loss, which is calculated between the features before and after quantization, employing a stop-gradient operation \cite{van2017neural}. Following the approach used in BigCodec, we do not use a moving average to update the codebook. To prevent the encoder output from becoming excessively large, a commitment loss with a loss weight as 0.25 is introduced.

The loss weights for the reconstruction loss, GAN loss, feature loss are set to 15.0, 1.0, and 1.0, respectively.
Unlike the official BigCodec implementation, we set the VQ loss proportional to the codebook size: 4.0 for a codebook size of 8192, 32.0 for a codebook size of 65,536, and 64.0 for a codebook size of 131,072.
This configuration yielded the best results in our preliminary experiments. Note that the same loss settings are applied during the training of the streaming version of the BigCodec models.

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论