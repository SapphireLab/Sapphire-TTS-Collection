# RepCodec

<details>
<summary>基本信息</summary>

- 标题: "RepCodec: A Speech Representation Codec for Speech Tokenization"
- 作者:
  - 01 Zhichao Huang (ByteDance)
  - 02 Chutong Meng (ByteDance)
  - 03 Tom Ko (ByteDance)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2309.00169)
  - [Publication](https://aclanthology.org/2024.acl-long.314/) ACL 2024
  - [Github](https://github.com/mct10/repcodec)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2309.00169v3__RepCodec__A_Speech_Representation_Codec_for_Speech_Tokenization.pdf)
  - [Publication](_PDF/2309.00169p0__RepCodec__ACL2024.pdf)

</details>

## Abstract: 摘要

With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs.
However, this discretization gives rise to a loss of information, consequently impairing overall performance.
To improve the performance of these discrete speech tokens, we present ***RepCodec***, a novel speech representation codec for semantic speech tokenization.
In contrast to audio codecs which reconstruct the raw audio, ***RepCodec*** learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec.
Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens.
The extensive experiments illustrate that ***RepCodec***, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation.
Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of ***RepCodec***.
We believe our method can facilitate large language modeling research on speech processing.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论