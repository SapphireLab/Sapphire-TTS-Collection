# Descript-Audio-Codec (DAC)

<details>
<summary>基本信息</summary>

- 标题: "High-Fidelity Audio Compression with Improved RVQGAN"
- 作者:
  - 01 Rithesh Kumar (Descript.Inc.)
  - 02 Prem Seetharaman (Descript.Inc.)
  - 03 Alejandro Luebs (Descript.Inc.)
  - 04 Ishaan Kumar (Descript.Inc.)
  - 05 Kundan Kumar (Descript.Inc.)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2306.06546)
  - [Publication](https://dl.acm.org/doi/abs/10.5555/3666122.3667336)
  - [Github](https://github.com/descriptinc/descript-audio-codec)
  - [Demo](https://descript.notion.site/Descript-Audio-Codec-11389fce0ce2419891d6591a68f814d5)
- 文件:
  - [ArXiv](_PDF/2306.06546v2__DAC__High-Fidelity_Audio_Compression_with_Improved_RVQGAN.pdf)
  - [Publication](_PDF/2306.06546p0__DAC__NeurIPS2023.pdf)

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Language models have been successfully used to model natural signals, such as images, speech, and music.
A key component of these models is a high quality neural compression model that can compress high-dimensional natural signals into lower dimensional discrete tokens.
To that end, we introduce a high-fidelity universal neural audio compression algorithm that achieves ~90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth.
We achieve this by combining advances in high-fidelity audio generation with better vector quantization techniques from the image domain, along with improved adversarial and reconstruction losses.
We compress all domains (speech, environment, music, etc.) with a single universal model, making it widely applicable to generative modeling of all audio.
We compare with competing audio compression algorithms, and find our method outperforms them significantly.
We provide thorough ablations for every design choice, as well as open-source code and trained model weights.
We hope our work can lay the foundation for the next generation of high-fidelity audio modeling.

</details>
<br>

语言模型已成功用于建模自然信号, 如图像语音和音乐.
这些模型的关键组件是一个高质量的神经压缩模型, 能够将高维自然信号压缩成低维离散 Token.
为此, 我们引入了一种高保真通用神经音频压缩算法, 该算法在仅 8 kbps 带宽下将 44.1 KHz 音频压缩至约 90 倍.
我们通过结合高保真音频生成的进展, 图像领域更好的向量量化技术以及改进的对抗性和重建损失来实现这一点.
我们使用单一通用模型压缩所有领域 (语音, 环境, 音乐等), 使其广泛适用于所有音频的生成建模.
我们与竞争性音频压缩算法进行了比较, 发现我们的方法显著优于它们.
我们为每个设计选择提供了彻底的消融实验, 并提供了开源代码和训练好的模型权重.
我们希望我们的工作能为下一代高保真音频建模奠定基础.

## 1·Introduction: 引言

Generative modeling of high-resolution audio is difficult due to high dimensionality (\textasciitilde44,100 samples per second of audio) \citep{mehri2016samplernn, kumar2019melgan}, and presence of structure at different time-scales with both short and long-term dependencies.
To mitigate this problem, audio generation is typically divided into two stages: 1) predicting audio conditioned on some intermediate representation such as mel-spectrograms \citep{mehri2016samplernn, ping2017deep, kumar2019melgan, prenger2019waveglow} and 2) predicting the intermediate representation given some conditioning information, such as text \citep{shen2018natural, ren2020fastspeech}.
This can be interpreted as a hierarchical generative model, with observed intermediate variables.
Naturally, an alternate formulation is to learn the intermediate variables using the variational auto-encoder (VAE) framework, with a learned conditional prior to predict the latent variables given some conditioning.
This formulation, with continuous latent variables and training an expressive prior using normalizing flows has been quite successful for speech synthesis \citep{kim2021conditional, tan2022naturalspeech}.

A closely related idea is to train the same varitional-autoencoder with discrete latent variables using VQ-VAE \citep{van2017neural}.
Arguably, discrete latent variables are a better choice since expressive priors can be trained using powerful autoregressive models that have been developed for modeling distributions over discrete variables \citep{oord2016wavenet}.
Specifically, transformer language models \citep{vaswani2017attention} have already exhibited the capacity to scale with data and model capacity to learn arbitrarily complex distributions such as text\citep{brown2020language}, images\citep{esser2021taming, yu2021vector}, audio \citep{borsos2022audiolm, wang2023neural}, music \citep{agostinelli2023musiclm}, etc.
While modeling the prior is straightforward, modeling the discrete latent codes using a quantized auto-encoder remains a challenge.

Learning these discrete codes can be interpreted as a lossy compression task, where the audio signal is compressed into a discrete latent space by vector-quantizing the representations of an autoencoder using a fixed length codebook.
This audio compression model needs to satisfy the following properties: 1) Reconstruct audio with high fidelity and free of artifacts 2) Achieve high level of compression along with temporal downscaling to learn a compact representation that discards low-level imperceptible details while preserving high-level structure \citep{van2017neural, razavi2019generating} 3) Handle all types of audio such as speech, music, environmental sounds, different audio encodings (such as mp3) as well as different sampling rates using a single universal model.

While the recent neural audio compression algorithms such as SoundStream \citep{zeghidour2021soundstream} and EnCodec \citep{defossez2022high} partially satisfy these properties, they often suffer from the same issues that plague GAN-based generation models.
Specifically, such models exhibit audio artifacts such as tonal artifacts \cite{pons2021upsampling}, pitch and periodicity artifacts \citep{morrison2021chunked} and imperfectly model high-frequencies leading to audio that are clearly distinguishable from originals.
These models are often tailored to a specific type of audio signal such as speech or music and struggle to model generic sounds.
We make the following contributions:

- We introduce \textbf{Improved RVQGAN} a high fidelity universal audio compression model, that can compress 44.1 KHz audio into discrete codes at 8 kbps bitrate (\textasciitilde 90x compression) with minimal loss in quality and fewer artifacts.
Our model outperforms state-of-the-art methods by a large margin even at lower bitrates (higher compression) , when evaluated with both quantitative metrics and qualitative listening tests.
- We identify a critical issue in existing models which don't utilize the full bandwidth due to \textbf{codebook collapse} (where a fraction of the codes are unused) and fix it using improved codebook learning techniques.
- We identify a side-effect of \textbf{quantizer dropout} - a technique designed to allow a single model to support variable bitrates, actually hurts the full-bandwidth audio quality and propose a solution to mitigate it.
- We make impactful design changes to existing neural audio codecs by adding periodic inductive biases, multi-scale STFT discriminator, multi-scale mel loss and provide thorough ablations and intuitions to motivate them.
- Our proposed method is a universal audio compression model, capable of handling speech, music, environmental sounds, different sampling rates and audio encoding formats.

We provide [code [Github]](https://github.com/descriptinc/descript-audio-codec), models, and [audio samples [URL]](https://descript.notion.site/Descript-Audio-Codec-11389fce0ce2419891d6591a68f814d5) that we encourage the reader to listen to.

## 2·Related Works: 相关工作

**High Fidelity Neural Audio Synthesis**

Recently, generative adversarial networks (GANs) have emerged as a solution to generate high-quality audio with fast inference speeds, due to the feed-forward (parallel) generator. MelGAN \citep{kumar2019melgan} successfully trains a GAN-based spectrogram inversion (neural vocoding) model. It introduces a multi-scale waveform discriminator (MSD) to penalize structure at different audio resolutions and a feature matching loss that minimizes L1 distance between discriminator feature maps of real and synthetic audio. HifiGAN \citep{kong2020hifi} refines this recipe by introducing a multi-period waveform discriminator (MPD) for high fidelity synthesis, and adding an auxiliary mel-reconstruction loss for fast training. UnivNet \citep{jang2021univnet} introduces a multi-resolution spectrogram discriminator (MRSD) to generate audio with sharp spectrograms. BigVGAN \citep{lee2022bigvgan} extends the HifiGAN recipe by introducing a periodic inductive bias using the Snake activation function \citep{ziyin2020neural}. It also replaces the MSD in HifiGAN with the MRSD to improve audio quality and reduce pitch, periodicity artifacts \citep{morrison2021chunked}. While these the GAN-based learning techniques are used for vocoding, these recipes are readily applicable to neural audio compression. Our Improved RVQGAN model closely follows the BigVGAN training recipe, with a few key changes. Our model uses a new multi-band multi-scale STFT discriminator that alleviates aliasing artifacts, and a multi-scale mel-reconstruction loss that better model quick transients.

**Neural Audio Compression Models**

VQ-VAEs \cite{van2017neural} have been the dominant paradigm to train neural audio codecs. The first VQ-VAE based speech codec was proposed in \citep{garbacea2019low} operating at 1.6 kbps. This model used the original architecture from \citep{van2017neural} with a convolutional encoder and an autoregressive wavenet \citep{oord2016wavenet} decoder. SoundStream \citep{zeghidour2021soundstream} is one of the first universal compression models capable of handling diverse audio types, while supporting varying bitrates using a single model. They use a fully causal convolutional encoder and decoder network, and perform residual vector quantization (RVQ). The model is trained using the VQ-GAN \citep{esser2021taming} formulation, by adding adversarial and feature-matching losses along with the multi-scale spectral reconstruction loss. EnCodec \citep{defossez2022high} closely follows the SoundStream recipe, with a few modifications that lead to improved quality. EnCodec uses a multi-scale STFT discriminator with a multi-scale spectral reconstruction loss. They use a loss balancer which adjusts loss weights based on the varying scale of gradients coming from the discriminator.

Our proposed method also uses a convolutional encoder-decoder architecture, residual vector quantization and adversarial, perceptual losses. However, our recipe has the following key differences: 1) We introduce a periodic inductive bias using Snake activations \citep{ziyin2020neural, lee2022bigvgan} 2) We improve codebook learning by projecting the encodings into a low-dimensional space \citep{yu2021vector} 3) We obtain a stable training recipe using best practices for adversarial and perceptual loss design, with fixed loss weights and without requiring a sophisticated loss balancer. We find that our changes lead to a near-optimal effective bandwidth usage. This allows our model to outperform EnCodec even with 3x lower bitrate.

**Language Modeling of Natural Signals**

Neural language models have demonstrated great success in diverse tasks such as open-ended text generation \citep{brown2020language} with in-context learning capabilities. A key-component of these models is self-attention \citep{vaswani2017attention}, which is capable of modeling complex and long-range dependencies but suffers from a quadratic computational cost with the length of the sequence. This cost is unacceptable for natural signals such as images and audio with very high dimensionality, requiring a compact mapping into a discrete representation space. This mapping is typically learnt using VQ-GANs \citep{esser2021taming, yu2021vector}, followed by training an autoregressive Transformer on the discrete tokens. This approach has shown success across image \citep{yu2022scaling, yu2021vector,ramesh2021zero}, audio \citep{borsos2022audiolm, wang2023neural}, video and music \citep{dhariwal2020jukebox, agostinelli2023musiclm} domains. Codecs like SoundStream and EnCodec have already been used in generative audio models, like AudioLM \citep{borsos2022audiolm}, MusicLM \citep{agostinelli2023musiclm}, and VALL-E \citep{wang2023neural}. Our proposed model can serve as a drop-in replacement for the audio tokenization model used in these methods, allowing for highly superior audio fidelity, and more efficient learning due to our maximum entropy code representation.

## 3·Methodology: 方法

Our model is built on the framework of VQ-GANs, following the same pattern as SoundStream \citep{zeghidour2021soundstream} and EnCodec \citep{defossez2022high}. Our model uses the fully convolutional encoder-decoder network from SoundStream, that performs temporal downscaling with a chosen striding factor. Following recent literature, we quantize the encodings using Residual Vector Quantization (RVQ), a method that recursively quantizes residuals following an initial quantization step with a distinct codebook. Quantizer dropout is applied during training to enable a single model that can operate at several target bitrates. Our model is similarly trained using a frequency domain reconstruction loss along with adversarial and perceptual losses.

An audio signal with sampling rate $f_s$ (Hz), encoder striding factor $M$, and $N_q$ layers of RVQ produce a discrete code matrix of shape $S \times N_q$, where $S$ is the frame rate defined as $f_s / M$. Table \ref{tabs:compression_comparison} compares our proposed model against baselines to contrast the compression factors and the frame rate of latent codes. Note that the target bitrate mentioned is an upper bound, since all models support variable bitrates. Our model achieves a higher compression factor compared to all baseline methods while outperforming them in audio quality, as we show later. Finally, a lower frame rate is desirable when training a language model on the discrete codes, as it results in shorter sequences.

### Periodic Activation Function

Audio waveforms are known to exhibit high periodicity (especially in voiced components, music, etc.) While current non-autoregressive audio generation architectures are capable of generating high fidelity audio, they often exhibit jarring pitch and periodicity artifacts \citep{morrison2021chunked}. Moreover, common neural network activations (such as Leaky ReLUs) are known to struggle with extrapolating periodic signals, and exhibit poor out-of-distribution generalization for audio synthesis \citep{lee2022bigvgan}.

To add a periodic inductive bias to the generator, we adopt the Snake activation function proposed by Liu et al. \cite{ziyin2020neural} and introduced to the audio domain in the BigVGAN neural vocoding model \citep{lee2022bigvgan}. It is defined as $\text{snake}(x) = x + \frac{1}{\alpha}\sin^2(\alpha x)$ , where $\alpha$ controls the frequency of periodic component of the signal. In our experiments, we find replacing Leaky ReLU activations with Snake function to be an influential change that significantly improves audio fidelity (Table \ref{tabs:vqgan_ablation}).

### Improved Residual Vector Quantization


While vector quantization (VQ) is a popular method to train discrete auto-encoder, there are many practical struggles when training them. Vanilla VQ-VAEs struggle from low codebook usage due to poor initialization, leading to a significant portion of the codebook being unused. This reduction in effective codebook size leads to an implicit reduction in target bitrate, which translates to poor reconstruction quality.

To mitigate this, recent audio codec methods use k-means clustering to initialize the codebook vectors, and manually employ randomized restarts \citep{dhariwal2020jukebox} when certain codebooks are unused for several batches. However, we find that the EnCodec model trained at 24kbps target bitrate, as well as our proposed model with the same codebook learning method (Proposed w/ EMA) still suffers from codebook under-utilization (Figure \ref{fig:codebook_usage}).

To address this issue, we use two key techniques introduced in the Improved VQGAN image model\citep{yu2021vector} to improve codebook usage: factorized codes and L2-normalized codes. Factorization decouples code lookup and code embedding, by performing code lookup in a low-dimensional space (8d or 32d) whereas the code embedding resides in a high dimensional space (1024d). Intuitively, this can be interpreted as a code lookup using only the principal components of the input vector that maximally explain the variance in the data. The L2-normalization of the encoded and codebook vectors converts euclidean distance to cosine similarity, which is helpful for stability and quality \citep{yu2021vector}.

These two tricks along with the overall model recipe significantly improve codebook usage, and therefore bitrate efficiency (Figure \ref{fig:codebook_usage}) and reconstruction quality (Table \ref{tabs:vqgan_ablation}), while being simpler to implement. Our model can be trained using the original VQ-VAE codebook and commitment losses \citep{van2017neural}, without k-means initialization or random restarts. The equations for the modified codebook learning procedure are written in Appendix \ref{appendix:equation}

### Quantizer Dropout Rate


Quantizer dropout was introduced in SoundStream \citep{zeghidour2021soundstream} to train a single compression model with variable bitrate. The number of quantizers $N_q$ determine the bitrate, so for each input example we randomly sample $n \sim \{1, 2, \ldots, N_q\}$ and only use the first $n_q$ quantizers while training. However, we noticed that applying quantizer dropout degrades the audio reconstruction quality at full bandwidth (Figure \ref{fig:quantizer_dropout}).

To address this problem, we instead apply quantizer dropout to each input example with some probability $p$. Interestingly, we find that dropout probability $p=0.5$ closely matches the reconstruction quality of baseline at lower bitrates, while closing the gap to full-bandwidth quality of a model trained without quantizer dropout ($p=0.0$).

Moreover, we provide additional insight into the practical behavior of quantizer dropout and it's interaction with RVQ. Firstly, we find that these techniques put together lead the quantized codes to learn most-significant to least significant bits of information with each additional quantizer. When the codes are reconstructed with $1 \ldots N_q$ codebooks, we can see each codebook adds increasing  amounts of fine-scale detail. We believe this interaction is beneficial when training hierarchical generative models on top of these codes \citep{borsos2022audiolm, wang2023neural, agostinelli2023musiclm}, for example to partition the codes into "coarse" tokens (denoting the most significant codes) and "fine" tokens.

### Discriminator Design

Like prior work, we use multi-scale (MSD) and multi-period waveform discriminators (MPD) which lead to improved audio fidelity. However, spectrograms of generated audio can  still appear blurry, exhibiting over-smoothing artifacts in high frequencies\citep{jang2021univnet}. The multi-resolution spectrogram discriminator (MRSD) was proposed in UnivNet to fix these artifacts and BigVGAN \citep{lee2022bigvgan} found that it also helps to reduce pitch and periodicity artifacts. However, using magnitude spectrograms discards phase information which could've been otherwise utilized by the discriminator to penalize phase modeling errors. Moreover, we find that high-frequency modeling is still challenging for these models especially at high sampling rates.

To address these issues, we use a complex STFT discriminator \citep{zeghidour2021soundstream} at multiple time-scales \citep{defossez2022high} and find that it works better in practice and leads to improved phase modeling. Additionally we find that splitting the STFT into sub-bands slightly improves high frequency prediction and mitigates aliasing artifacts, since the discriminator can learn discriminative features about a specific sub-band and provide a stronger gradient signal to the generator. Multi-band processing was earlier proposed in \citep{yang2021multi} to predict audio in sub-bands which are subsequently summed to produce the full-band audio.

### Loss Functions

**Frequency domain reconstruction loss**

While the mel-reconstruction loss \citep{kong2020hifi} is known to improve stability, fidelity and convergence speed, the multi-scale spectral losses\citep{yamamoto2020parallel, engel2020ddsp, gritsenko2020spectral} encourage modeling of frequencies in multiple time-scales. In our model, we combine both methods by using a L1 loss on mel-spectrograms computed with window lengths of $[32, 64, 128, 256, 512, 1024, 2048]$ and hop length set to $\text{window\_length}\ /\ 4$. We especially find that using the lowest hop size of 8 improves modeling of very quick transients that are especially common in the music domain.

EnCodec \citep{defossez2022high} uses a similar loss formulation, but with both L1 and L2 loss terms, and a fixed mel bin size of 64. We find that fixing mel bin size leads to holes in the spectrogram especially at low filter lengths. Therefore, we use mel bin sizes $[5, 10, 20, 40, 80, 160, 320]$ corresponding to the above filter lengths which were verified to be correct by manual inspection.

**Adversarial loss**

our model uses the multi-period discriminator \citep{kong2020hifi} for waveform discrimination, as well as the proposed multi-band multi-scale STFT discriminator for the frequency domain. We use the HingeGAN \citep{lim2017geometric} adversarial loss formulation, and apply the L1 feature matching loss \citep{kumar2019melgan}.

**Codebook learning**

we use the simple codebook and commitment losses with stop-gradients from the original VQ-VAE formulation \citep{van2017neural}, and backpropagate gradients through the codebook lookup using the straight-through estimator \citep{bengio2013estimating}.

**Loss weighting**

we use the loss weightings of $15.0$ for the multi-scale mel loss, $2.0$ for the feature matching loss, $1.0$ for the adversarial loss and $1.0$, $0.25$ for the codebook and commitment losses respectively. These loss weightings are in line with recent works \citep{kong2020hifi, lee2022bigvgan} (which use $45.0$ weighting for the mel loss), but simply rescaled to account for the multiple scales and $\log_{10}$ base we used for computing the mel loss. We don't use a loss balancer as proposed in EnCodec \citep{defossez2022high}.

## 4·Experiments: 实验

### Data sources

We train our model on a large dataset compiled of speech, music, and environmental sounds. For speech, we use the DAPS dataset \cite{mysore2014can}, the clean speech segments from DNS Challenge 4 \cite{dns}, the Common Voice dataset \cite{ardila2019common}, and the VCTK dataset \cite{veaux2017cstr}. For music, we use the MUSDB dataset \cite{rafii2017musdb18}, and the Jamendo dataset \cite{bogdanov2019mtg}. Finally, for environmental sound, we use both the balanced and unbalanced train segments from AudioSet \cite{gemmeke2017audio}. All audio is resampled to 44kHz.

During training, we extract short excerpts from each audio file, and normalize them to -24 dB LUFS. The only data augmentation we apply is to randomly shift the phase of the excerpt, uniformly. For evaluation, we use the evaluation segments from AudioSet \cite{gemmeke2017audio}, two speakers that are held out from DAPS \cite{mysore2014can} (F10, M10) for speech, and the test split of MUSDB \cite{rafii2017musdb18}. We extract 3000 10-second segments (1000 from each domain), as our test set.

### Balanced data sampling

We take special care in how we sample from our dataset. Though our dataset is resampled to 44kHz, the data within it may be band-limited in some way. That is, some audio may have had an original sampling rate much lower than 44kHz. This is especially prevalent in speech data, where the true sampling rates of the underlying data can vary greatly (e.g. the Common Voice data is commonly 8-16kHz). When we trained models on varying sampling rates, we found that the resultant model often would not reconstruct data above a certain frequency. When investigating, we found that this threshold frequency corresponded to the average true sampling rate of our dataset. To fix this, we introduce a \textit{balanced data sampling} technique.

We first split our dataset into data sources that we know to be \textit{full-band} - they are confirmed to contain energy in frequencies up to the desired Nyquist frequency (22.05kHz) of the codec - and data sources where we have no assurances of the max frequency. When sampling batches, we make sure that a full-band item is sampled. Finally, we ensure that in each batch, there are an equal number of items from each domain: speech, music, and environmental sound. In our ablation study, we examine how this balanced sampling technique affects model performance.

### Model and training recipe

Our model consists of a convolutional encoder, a residual vector quantizer, and a convolutional decoder. The basic building block of our network is a convolutional layer which either upsamples or downsamples with some stride, followed by a residual layer consisting of convolutional layers interleaved with non-linear Snake activations. Our encoder has 4 of these layers, each of which downsamples the input audio waveform at rates $[2, 4, 8, 8]$. Our decoder has 4 corresponding layers, which upsample at rates $[8, 8, 4, 2]$. We set the decoder dimension to $1536$. In total, our model has 76M parameters, with 22M in the encoder, and 54M in the decoder. We also examine decoder dimensions of 512 (31M parameters) and 1024 (49M parameters).

We use the multi-period discriminator \cite{kong2020hifi}, and a complex multi-scale STFT discriminator. For the first, we use periods of $[2, 3, 5, 7, 11]$, and for the second, we use window lengths $[2048, 1024, 512]$, with a hop-length that is $1/4$ the window length. For band-splitting of the STFT, we use the band-limits $[0.0, 0.1, 0.25, 0.5, 0.75, 1.0]$. For the reconstruction loss, we use distance between log-mel spectrograms with window lengths $[32, 64, 128, 256, 512, 1024, 2048]$, with corresponding number of mels for each of $[5, 10, 20, 40, 80, 160, 320]$. The hop length is $1/4$ of the window length. We use feature matching and codebook losses, as described in Section \ref{sec:loss}.

For our ablation study, we train each model with a batch size of $12$ for $250$k iterations. In practice, this takes about 30 hours to train on a single GPU.  For our final model, we train with a batch size of $72$ for $400$k iterations. We train with excerpts of duration 0.38s. We use the AdamW optimizer \cite{adamw} with a learning rate of $1e-4$, $\beta_1 = 0.8$, and $\beta_2 = 0.9$, for both the generator
and the discriminator. We decay the learning rate at every step, with $\gamma = 0.999996$.

### Objective and subjective metrics

To evaluate our models, we use the following objective metrics:

- ViSQOL \cite{chinen2020visqol}: an intrusive perceptual quality metric that uses spectral similarity to the ground truth to estimate a mean opinion score.
- Mel distance: distance between log mel spectrograms of the reconstructed and ground truth waveforms. The configuration of this loss is the same as described in \ref{sec:loss}.
- STFT distance: distance between log magnitude spectrograms of the reconstructed and ground truth waveforms. We use window lengths $[2048, 512]$. This metric captures the fidelity in higher frequencies better than the mel distance.
- Scale-invariant source-to-distortion ratio (SI-SDR) \cite{le2019sdr}: distance between waveforms, similar to signal-to-noise ratio, with modifications so that it is invariant to scale differences. When considered alongside spectral metrics, SI-SDR indicates the quality of the phase reconstruction of the audio.
- Bitrate efficiency: We calculate bitrate efficiency as the sum of the entropy (in bits) of each codebook when applied on a large test set divided by the number of bits across all codebooks. For efficient bitrate utilization this should tend to 100\% and lower percentages indicate that the bitrate is being underutilized.

We also conduct a MUSHRA-inspired listening test, with a hidden reference, but no low-passed anchor. In it each one of ten expert listeners rated 12 randomly selected 10-second samples from our evaluation set, 4 of each domain; speech, music and environmental sounds. We compare our proposed system at 2.67kbps, 5.33kbps and 8kbps to EnCodec at 3kbps, 6kbps and 12kbps.

## 5·Results: 结果

### Ablation Study

We conduct a thorough ablation study of our model, varying components of our training recipe and model configuration one-by-one. To compare models, we use the four objective metrics described in Section \ref{sec:metrics}. The results of our ablation study can be seen in Table \ref{tabs:vqgan_ablation}.

Architecture:} We find that varying the decoder dimension has some effect on performance, with smaller models having consistently worse metrics. However, the model with decoder dimension $1024$ has similar performance to the baseline, indicating that smaller models can still be competitive. The change with the biggest impact was switching out the \textit{relu} activation for the \textit{snake} activation. This change resulted in much better SI-SDR and other metrics. Similar to the results in BigVGAN \cite{lee2022bigvgan}, we find that the periodic inductive bias of the snake activation is helpful for waveform generation. For our final model, we use the largest decoder dimension ($1536$), and the snake activation.

\textbf{Discriminator:} Next, we removed or changed the discriminators one-by-one, to see their impact on the final result. First, we find that the multi-band STFT discriminator does \textit{not} result in significantly better metrics, except for SI-SDR, where it is slightly better. However, when inspecting spectrograms of generated waveforms, we find that the multi-band discriminator alleviates aliasing of high frequencies. The upsampling layers of the decoder introduce significant aliasing artifacts \citep{pons2021upsampling}. The multi-band discriminator is more easily able to detect these aliasing artifacts and give feedback to the generator to remove them. Since aliasing artifacts are very small in terms of magnitude, their effect on our objective metrics is minimal. Thus, we keep the multi-band discriminator.

We find that adversarial losses are critical to both the quality of the output audio, as well as the bitrate efficiency. When training with only reconstruction loss, the bitrate efficiency drops from 99\% to 62\%, and the SI-SDR drops from 9.12 to 1.07. The other metrics capture spectral distance, and are relatively unaffected. However, the audio from this model has many artifacts, including buzzing, as it has not learned to reconstruct phase. Finally, we found that swapping the multi-period discriminator for the single-scale waveform discriminator proposed in MelGAN \cite{kumar2019melgan} resulted in worse SI-SDR. We retain the multi-period discriminator.

\textbf{Impact of low-hop reconstruction loss:} We find that low-hop reconstruction is critical to both the waveform loss and the modeling of fast transients and high frequencies. When replaced with a single-scale high-hop mel reconstruction (80 mels, with a window length of 512), we find significantly lower SI-SDR (7.68 from 9.12). Subjectively, we find that this model does much better at capturing certain sorts of sounds, such as cymbal crashes, beeping and alarms, and singing vocals. We retain the multi-scale mel reconstruction loss in our final recipe.

\textbf{Latent dimension of codebook:} the latent dimension of the codebook has a significant impact on bitrate efficiency, and consequently the reconstruction quality. If set too low or too high (e.g. 2, 256), quantitative metrics are significantly worse with drastically lowered bitrate efficiency. Lower bitrate efficiency results in effectively lowered bandwidth, which harms the modeling capability of the generator. As the generator is weakened, the discriminator tends to ``win'', and thus the generator does not learn to generate audio with high audio quality. We find $8$ to be optimal for the latent dimension.

\textbf{Quantization setup:} we find that using exponential moving average as the codebook learning method, as in EnCodec\cite{defossez2022high}, results in worse metrics especially for SI-SDR. It also results in poorer codebook utilization across all codebooks (Figure \ref{fig:codebook_usage}). When taken with its increased implementation complexity (requiring K-Means initialization and random restarts), we retain our simpler projected lookup method for learning codebooks, along with a commitment loss. Next, we note that the quantization dropout rate has a significant effect on the quantitative metrics. However, as seen in Figure \ref{fig:quantizer_dropout}, a dropout of $0.0$ results in poor reconstruction with fewer codebooks. As this makes usage of the codec challenging for downstream generative modeling tasks, we instead use a dropout rate of $0.5$ in our final model. This achieves a good trade-off between audio quality at full bitrate as well as lower bitrates. Finally, we show that we can increase the max bitrate of our model from 8kbps to 24kbps and achieve excellent audio quality, surpassing all other model configurations. However, for our final model, we train at the lower bitrates, in order to push the compression rate as much as possible.

\textbf{Balanced data sampling:} When removed, this results in worse metrics across the board. Empirically, we find that without balanced data sampling, the model produces waveforms that have a max frequency of around 18kHz. This corresponds to the max frequency preserved by various audio compression algorithms like MPEG, which make up the vast majority of our datasets. With balanced data sampling, we sample full-band audio from high-quality datasets (e.g. DAPS) just as much as possibly band-limited audio from datasets of unknown quality (e.g. Common Voice). This alleviates the issue, allowing our codec to reconstruct full-band audio, as well as band-limited audio.

### Comparison to other methods

We now compare the performance of our final model with competitive baselines: EnCodec \cite{defossez2022high}, Lyra \cite{zeghidour2021soundstream}, and Opus \cite{valin2012definition}, a popular open-source audio codec. For EnCodec, Lyra, and Opus, we use publicly available open-source implementations provided by the authors. We compare using both objective and subjective evaluations, at varying bitrates. The results are shown in Table \ref{tabs:vqgan_comparison}. We find that the proposed codec out-performs all competing codecs at all bitrates in terms of both objective and subjective metrics, while modeling a much wider bandwidth of 22kHz.

In Figure 3, we show the result of our MUSHRA study, which compares EnCodec to our proposed codec at various bitrates. We find that our codec achieves much higher MUSHRA scores than EnCodec at all bitrates. However, even at the highest bitrate, it still falls short of the reference MUSHRA score, indicating that there is room for improvement. We note that the metrics of our final model are still lower than the 24kbps model trained in our ablation study, as can be seen in Table \ref{tabs:vqgan_ablation}. This indicates that the remaining performance gap may be closed by increasing the maximum bitrate.

In Figure 4 and Table 4, we compare our proposed model trained with the same exact configuration as EnCodec (24 KHz sampling rate, 24 kbps bitrate, 320 stride, 32 codebooks of 10 bits each) to existing baselines, in both quantitative and qualitative metrics. In Figure 5, we show qualitative results by sound category.

## 6·Conclusions: 结论

We have presented a high-fidelity universal neural audio compression algorithm that achieves remarkable compression rates while maintaining audio quality across various types of audio data. Our method combines the latest advancements in audio generation, vector quantization techniques, and improved adversarial and reconstruction losses. Our extensive evaluation against existing audio compression algorithms demonstrates the superiority of our approach, providing a promising foundation for future high-fidelity audio modeling. With thorough ablations, open-source code, and trained model weights, we aim to contribute a useful centerpiece to the generative audio modeling community.

**Broader Impact and Limitations**

Our model has the capability to make generative modeling of full-band audio much easier to do. While this unlocks many useful applications, such as media editing, text-to-speech synthesis, music synthesis, and more, it can also lead to harmful applications like deepfakes. Care should be taken to avoid these applications. One possibility is to add watermarking and/or train a classifier that can detect whether or not the codec is applied, in order to enable the detection of synthetic media generated based on our codec.
Also, our model is not perfect, and still has difficulty reconstructing some challenging audio. By slicing the results by domain we find that, even though the proposed codec outperforms competing approaches across all of the domains, it performs best for speech and has more issues with environmental sounds. Finally, we notice that it does not model some musical instruments perfectly, such as glockenspeil, or synthesizer sounds.