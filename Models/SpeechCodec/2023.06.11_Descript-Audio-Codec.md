# Descript-Audio-Codec (DAC)

<details>
<summary>基本信息</summary>

- 标题: "High-Fidelity Audio Compression with Improved RVQGAN"
- 作者:
  - 01 Rithesh Kumar (Descript.Inc.)
  - 02 Prem Seetharaman (Descript.Inc.)
  - 03 Alejandro Luebs (Descript.Inc.)
  - 04 Ishaan Kumar (Descript.Inc.)
  - 05 Kundan Kumar (Descript.Inc.)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2306.06546)
  - [Publication](https://dl.acm.org/doi/abs/10.5555/3666122.3667336)
  - [Github](https://github.com/descriptinc/descript-audio-codec)
  - [Demo](https://descript.notion.site/Descript-Audio-Codec-11389fce0ce2419891d6591a68f814d5)
- 文件:
  - [ArXiv](_PDF/2306.06546v2__DAC__High-Fidelity_Audio_Compression_with_Improved_RVQGAN.pdf)
  - [Publication](_PDF/2306.06546p0__DAC__NeurIPS2023.pdf)

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Language models have been successfully used to model natural signals, such as images, speech, and music.
A key component of these models is a high quality neural compression model that can compress high-dimensional natural signals into lower dimensional discrete tokens.
To that end, we introduce a high-fidelity universal neural audio compression algorithm that achieves ~90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth.
We achieve this by combining advances in high-fidelity audio generation with better vector quantization techniques from the image domain, along with improved adversarial and reconstruction losses.
We compress all domains (speech, environment, music, etc.) with a single universal model, making it widely applicable to generative modeling of all audio.
We compare with competing audio compression algorithms, and find our method outperforms them significantly.
We provide thorough ablations for every design choice, as well as open-source code and trained model weights.
We hope our work can lay the foundation for the next generation of high-fidelity audio modeling.

</details>
<br>

语言模型已成功用于建模自然信号, 如图像语音和音乐.
这些模型的关键组件是一个高质量的神经压缩模型, 能够将高维自然信号压缩成低维离散 Token.
为此, 我们引入了一种高保真通用神经音频压缩算法, 该算法在仅 8 kbps 带宽下将 44.1 KHz 音频压缩至约 90 倍.
我们通过结合高保真音频生成的进展, 图像领域更好的向量量化技术以及改进的对抗性和重建损失来实现这一点.
我们使用单一通用模型压缩所有领域 (语音, 环境, 音乐等), 使其广泛适用于所有音频的生成建模.
我们与竞争性音频压缩算法进行了比较, 发现我们的方法显著优于它们.
我们为每个设计选择提供了彻底的消融实验, 并提供了开源代码和训练好的模型权重.
我们希望我们的工作能为下一代高保真音频建模奠定基础.

## 1·Introduction: 引言

Generative modeling of high-resolution audio is difficult due to high dimensionality (\textasciitilde44,100 samples per second of audio) \citep{mehri2016samplernn, kumar2019melgan}, and presence of structure at different time-scales with both short and long-term dependencies.
To mitigate this problem, audio generation is typically divided into two stages: 1) predicting audio conditioned on some intermediate representation such as mel-spectrograms \citep{mehri2016samplernn, ping2017deep, kumar2019melgan, prenger2019waveglow} and 2) predicting the intermediate representation given some conditioning information, such as text \citep{shen2018natural, ren2020fastspeech}.
This can be interpreted as a hierarchical generative model, with observed intermediate variables.
Naturally, an alternate formulation is to learn the intermediate variables using the variational auto-encoder (VAE) framework, with a learned conditional prior to predict the latent variables given some conditioning.
This formulation, with continuous latent variables and training an expressive prior using normalizing flows has been quite successful for speech synthesis \citep{kim2021conditional, tan2022naturalspeech}.

A closely related idea is to train the same varitional-autoencoder with discrete latent variables using VQ-VAE \citep{van2017neural}.
Arguably, discrete latent variables are a better choice since expressive priors can be trained using powerful autoregressive models that have been developed for modeling distributions over discrete variables \citep{oord2016wavenet}.
Specifically, transformer language models \citep{vaswani2017attention} have already exhibited the capacity to scale with data and model capacity to learn arbitrarily complex distributions such as text\citep{brown2020language}, images\citep{esser2021taming, yu2021vector}, audio \citep{borsos2022audiolm, wang2023neural}, music \citep{agostinelli2023musiclm}, etc.
While modeling the prior is straightforward, modeling the discrete latent codes using a quantized auto-encoder remains a challenge.

Learning these discrete codes can be interpreted as a lossy compression task, where the audio signal is compressed into a discrete latent space by vector-quantizing the representations of an autoencoder using a fixed length codebook.
This audio compression model needs to satisfy the following properties: 1) Reconstruct audio with high fidelity and free of artifacts 2) Achieve high level of compression along with temporal downscaling to learn a compact representation that discards low-level imperceptible details while preserving high-level structure \citep{van2017neural, razavi2019generating} 3) Handle all types of audio such as speech, music, environmental sounds, different audio encodings (such as mp3) as well as different sampling rates using a single universal model.

While the recent neural audio compression algorithms such as SoundStream \citep{zeghidour2021soundstream} and EnCodec \citep{defossez2022high} partially satisfy these properties, they often suffer from the same issues that plague GAN-based generation models.
Specifically, such models exhibit audio artifacts such as tonal artifacts \cite{pons2021upsampling}, pitch and periodicity artifacts \citep{morrison2021chunked} and imperfectly model high-frequencies leading to audio that are clearly distinguishable from originals.
These models are often tailored to a specific type of audio signal such as speech or music and struggle to model generic sounds.
We make the following contributions:

- We introduce \textbf{Improved RVQGAN} a high fidelity universal audio compression model, that can compress 44.1 KHz audio into discrete codes at 8 kbps bitrate (\textasciitilde 90x compression) with minimal loss in quality and fewer artifacts.
Our model outperforms state-of-the-art methods by a large margin even at lower bitrates (higher compression) , when evaluated with both quantitative metrics and qualitative listening tests.
- We identify a critical issue in existing models which don't utilize the full bandwidth due to \textbf{codebook collapse} (where a fraction of the codes are unused) and fix it using improved codebook learning techniques.
- We identify a side-effect of \textbf{quantizer dropout} - a technique designed to allow a single model to support variable bitrates, actually hurts the full-bandwidth audio quality and propose a solution to mitigate it.
- We make impactful design changes to existing neural audio codecs by adding periodic inductive biases, multi-scale STFT discriminator, multi-scale mel loss and provide thorough ablations and intuitions to motivate them.
- Our proposed method is a universal audio compression model, capable of handling speech, music, environmental sounds, different sampling rates and audio encoding formats.

We provide [code [Github]](https://github.com/descriptinc/descript-audio-codec), models, and [audio samples [URL]](https://descript.notion.site/Descript-Audio-Codec-11389fce0ce2419891d6591a68f814d5) that we encourage the reader to listen to.

## 2·Related Works: 相关工作

**High Fidelity Neural Audio Synthesis**

Recently, generative adversarial networks (GANs) have emerged as a solution to generate high-quality audio with fast inference speeds, due to the feed-forward (parallel) generator. MelGAN \citep{kumar2019melgan} successfully trains a GAN-based spectrogram inversion (neural vocoding) model. It introduces a multi-scale waveform discriminator (MSD) to penalize structure at different audio resolutions and a feature matching loss that minimizes L1 distance between discriminator feature maps of real and synthetic audio. HifiGAN \citep{kong2020hifi} refines this recipe by introducing a multi-period waveform discriminator (MPD) for high fidelity synthesis, and adding an auxiliary mel-reconstruction loss for fast training. UnivNet \citep{jang2021univnet} introduces a multi-resolution spectrogram discriminator (MRSD) to generate audio with sharp spectrograms. BigVGAN \citep{lee2022bigvgan} extends the HifiGAN recipe by introducing a periodic inductive bias using the Snake activation function \citep{ziyin2020neural}. It also replaces the MSD in HifiGAN with the MRSD to improve audio quality and reduce pitch, periodicity artifacts \citep{morrison2021chunked}. While these the GAN-based learning techniques are used for vocoding, these recipes are readily applicable to neural audio compression. Our Improved RVQGAN model closely follows the BigVGAN training recipe, with a few key changes. Our model uses a new multi-band multi-scale STFT discriminator that alleviates aliasing artifacts, and a multi-scale mel-reconstruction loss that better model quick transients.

**Neural Audio Compression Models**

VQ-VAEs \cite{van2017neural} have been the dominant paradigm to train neural audio codecs. The first VQ-VAE based speech codec was proposed in \citep{garbacea2019low} operating at 1.6 kbps. This model used the original architecture from \citep{van2017neural} with a convolutional encoder and an autoregressive wavenet \citep{oord2016wavenet} decoder. SoundStream \citep{zeghidour2021soundstream} is one of the first universal compression models capable of handling diverse audio types, while supporting varying bitrates using a single model. They use a fully causal convolutional encoder and decoder network, and perform residual vector quantization (RVQ). The model is trained using the VQ-GAN \citep{esser2021taming} formulation, by adding adversarial and feature-matching losses along with the multi-scale spectral reconstruction loss. EnCodec \citep{defossez2022high} closely follows the SoundStream recipe, with a few modifications that lead to improved quality. EnCodec uses a multi-scale STFT discriminator with a multi-scale spectral reconstruction loss. They use a loss balancer which adjusts loss weights based on the varying scale of gradients coming from the discriminator.

Our proposed method also uses a convolutional encoder-decoder architecture, residual vector quantization and adversarial, perceptual losses. However, our recipe has the following key differences: 1) We introduce a periodic inductive bias using Snake activations \citep{ziyin2020neural, lee2022bigvgan} 2) We improve codebook learning by projecting the encodings into a low-dimensional space \citep{yu2021vector} 3) We obtain a stable training recipe using best practices for adversarial and perceptual loss design, with fixed loss weights and without requiring a sophisticated loss balancer. We find that our changes lead to a near-optimal effective bandwidth usage. This allows our model to outperform EnCodec even with 3x lower bitrate.

**Language Modeling of Natural Signals**

Neural language models have demonstrated great success in diverse tasks such as open-ended text generation \citep{brown2020language} with in-context learning capabilities. A key-component of these models is self-attention \citep{vaswani2017attention}, which is capable of modeling complex and long-range dependencies but suffers from a quadratic computational cost with the length of the sequence. This cost is unacceptable for natural signals such as images and audio with very high dimensionality, requiring a compact mapping into a discrete representation space. This mapping is typically learnt using VQ-GANs \citep{esser2021taming, yu2021vector}, followed by training an autoregressive Transformer on the discrete tokens. This approach has shown success across image \citep{yu2022scaling, yu2021vector,ramesh2021zero}, audio \citep{borsos2022audiolm, wang2023neural}, video and music \citep{dhariwal2020jukebox, agostinelli2023musiclm} domains. Codecs like SoundStream and EnCodec have already been used in generative audio models, like AudioLM \citep{borsos2022audiolm}, MusicLM \citep{agostinelli2023musiclm}, and VALL-E \citep{wang2023neural}. Our proposed model can serve as a drop-in replacement for the audio tokenization model used in these methods, allowing for highly superior audio fidelity, and more efficient learning due to our maximum entropy code representation.

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论
