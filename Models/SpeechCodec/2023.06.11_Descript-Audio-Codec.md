# Descript-Audio-Codec (DAC)

<details>
<summary>基本信息</summary>

- 标题: "High-Fidelity Audio Compression with Improved RVQGAN"
- 作者:
  - 01 Rithesh Kumar (Descript.Inc.)
  - 02 Prem Seetharaman (Descript.Inc.)
  - 03 Alejandro Luebs (Descript.Inc.)
  - 04 Ishaan Kumar (Descript.Inc.)
  - 05 Kundan Kumar (Descript.Inc.)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2306.06546)
  - [Publication](https://dl.acm.org/doi/abs/10.5555/3666122.3667336)
  - [Github](https://github.com/descriptinc/descript-audio-codec)
  - [Demo](https://descript.notion.site/Descript-Audio-Codec-11389fce0ce2419891d6591a68f814d5)
- 文件:
  - [ArXiv](_PDF/2306.06546v2__DAC__High-Fidelity_Audio_Compression_with_Improved_RVQGAN.pdf)
  - [Publication](_PDF/2306.06546p0__DAC__NeurIPS2023.pdf)

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Language models have been successfully used to model natural signals, such as images, speech, and music.
A key component of these models is a high quality neural compression model that can compress high-dimensional natural signals into lower dimensional discrete tokens.
To that end, we introduce a high-fidelity universal neural audio compression algorithm that achieves ~90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth.
We achieve this by combining advances in high-fidelity audio generation with better vector quantization techniques from the image domain, along with improved adversarial and reconstruction losses.
We compress all domains (speech, environment, music, etc.) with a single universal model, making it widely applicable to generative modeling of all audio.
We compare with competing audio compression algorithms, and find our method outperforms them significantly.
We provide thorough ablations for every design choice, as well as open-source code and trained model weights.
We hope our work can lay the foundation for the next generation of high-fidelity audio modeling.

</details>
<br>

语言模型已成功用于建模自然信号, 如图像语音和音乐.
这些模型的关键组件是一个高质量的神经压缩模型, 能够将高维自然信号压缩成低维离散 Token.
为此, 我们引入了一种高保真通用神经音频压缩算法, 该算法在仅 8 kbps 带宽下将 44.1 KHz 音频压缩至约 90 倍.
我们通过结合高保真音频生成的进展, 图像领域更好的向量量化技术以及改进的对抗性和重建损失来实现这一点.
我们使用单一通用模型压缩所有领域 (语音, 环境, 音乐等), 使其广泛适用于所有音频的生成建模.
我们与竞争性音频压缩算法进行了比较, 发现我们的方法显著优于它们.
我们为每个设计选择提供了彻底的消融实验, 并提供了开源代码和训练好的模型权重.
我们希望我们的工作能为下一代高保真音频建模奠定基础.

## 1·Introduction: 引言

Generative modeling of high-resolution audio is difficult due to high dimensionality (\textasciitilde44,100 samples per second of audio) \citep{mehri2016samplernn, kumar2019melgan}, and presence of structure at different time-scales with both short and long-term dependencies.
To mitigate this problem, audio generation is typically divided into two stages: 1) predicting audio conditioned on some intermediate representation such as mel-spectrograms \citep{mehri2016samplernn, ping2017deep, kumar2019melgan, prenger2019waveglow} and 2) predicting the intermediate representation given some conditioning information, such as text \citep{shen2018natural, ren2020fastspeech}.
This can be interpreted as a hierarchical generative model, with observed intermediate variables.
Naturally, an alternate formulation is to learn the intermediate variables using the variational auto-encoder (VAE) framework, with a learned conditional prior to predict the latent variables given some conditioning.
This formulation, with continuous latent variables and training an expressive prior using normalizing flows has been quite successful for speech synthesis \citep{kim2021conditional, tan2022naturalspeech}.

A closely related idea is to train the same varitional-autoencoder with discrete latent variables using VQ-VAE \citep{van2017neural}.
Arguably, discrete latent variables are a better choice since expressive priors can be trained using powerful autoregressive models that have been developed for modeling distributions over discrete variables \citep{oord2016wavenet}.
Specifically, transformer language models \citep{vaswani2017attention} have already exhibited the capacity to scale with data and model capacity to learn arbitrarily complex distributions such as text\citep{brown2020language}, images\citep{esser2021taming, yu2021vector}, audio \citep{borsos2022audiolm, wang2023neural}, music \citep{agostinelli2023musiclm}, etc.
While modeling the prior is straightforward, modeling the discrete latent codes using a quantized auto-encoder remains a challenge.

Learning these discrete codes can be interpreted as a lossy compression task, where the audio signal is compressed into a discrete latent space by vector-quantizing the representations of an autoencoder using a fixed length codebook.
This audio compression model needs to satisfy the following properties: 1) Reconstruct audio with high fidelity and free of artifacts 2) Achieve high level of compression along with temporal downscaling to learn a compact representation that discards low-level imperceptible details while preserving high-level structure \citep{van2017neural, razavi2019generating} 3) Handle all types of audio such as speech, music, environmental sounds, different audio encodings (such as mp3) as well as different sampling rates using a single universal model.

While the recent neural audio compression algorithms such as SoundStream \citep{zeghidour2021soundstream} and EnCodec \citep{defossez2022high} partially satisfy these properties, they often suffer from the same issues that plague GAN-based generation models.
Specifically, such models exhibit audio artifacts such as tonal artifacts \cite{pons2021upsampling}, pitch and periodicity artifacts \citep{morrison2021chunked} and imperfectly model high-frequencies leading to audio that are clearly distinguishable from originals.
These models are often tailored to a specific type of audio signal such as speech or music and struggle to model generic sounds.
We make the following contributions:

- We introduce \textbf{Improved RVQGAN} a high fidelity universal audio compression model, that can compress 44.1 KHz audio into discrete codes at 8 kbps bitrate (\textasciitilde 90x compression) with minimal loss in quality and fewer artifacts.
Our model outperforms state-of-the-art methods by a large margin even at lower bitrates (higher compression) , when evaluated with both quantitative metrics and qualitative listening tests.
- We identify a critical issue in existing models which don't utilize the full bandwidth due to \textbf{codebook collapse} (where a fraction of the codes are unused) and fix it using improved codebook learning techniques.
- We identify a side-effect of \textbf{quantizer dropout} - a technique designed to allow a single model to support variable bitrates, actually hurts the full-bandwidth audio quality and propose a solution to mitigate it.
- We make impactful design changes to existing neural audio codecs by adding periodic inductive biases, multi-scale STFT discriminator, multi-scale mel loss and provide thorough ablations and intuitions to motivate them.
- Our proposed method is a universal audio compression model, capable of handling speech, music, environmental sounds, different sampling rates and audio encoding formats.

We provide [code [Github]](https://github.com/descriptinc/descript-audio-codec), models, and [audio samples [URL]](https://descript.notion.site/Descript-Audio-Codec-11389fce0ce2419891d6591a68f814d5) that we encourage the reader to listen to.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论
