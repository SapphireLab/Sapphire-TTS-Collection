# SoundStream

<details>
<summary>基本信息</summary>

- 标题: "SoundStream: An End-to-End Neural Audio Codec"
- 作者:
  - 01 Neil Zeghidour, Research Google Inc.
  - 02 Alejandro Luebs, Research Google Inc.
  - 03 Ahmed Omran, Research Google Inc.
  - 04 Jan Skoglund, Research Google Inc.
  - 05 Marco Tagliasacchi, Research Google Inc.
- 链接:
  - [ArXiv](https://arxiv.org/abs/2107.03312)
  - [Publication](https://doi.org/10.1109/TASLP.2021.3129994)
  - [Github]()
  - [Demo](https://google-research.github.io/seanet/soundstream/examples/)
- 文件:
  - [ArXiv](_PDF/2107.03312v1__SoundStream__An_End-to-End_Neural_Audio_Codec.pdf)
  - [Publication](_PDF/2107.03312p0__SoundStream__IEEE@TASLP2021.pdf)

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

We present ***SoundStream***, a novel neural audio codec that can efficiently compress speech, music and general audio at bitrates normally targeted by speech-tailored codecs.
***SoundStream*** relies on a model architecture composed by a fully convolutional encoder/decoder network and a residual vector quantizer, which are trained jointly end-to-end.
Training leverages recent advances in text-to-speech and speech enhancement, which combine adversarial and reconstruction losses to allow the generation of high-quality audio content from quantized embeddings.
By training with structured dropout applied to quantizer layers, a single model can operate across variable bitrates from 3 kbps to 18 kbps, with a negligible quality loss when compared with models trained at fixed bitrates.
In addition, the model is amenable to a low latency implementation, which supports streamable inference and runs in real time on a smartphone CPU.
In subjective evaluations using audio at 24 kHz sampling rate, ***SoundStream*** at 3 kbps outperforms Opus at 12 kbps and approaches EVS at 9.6 kbps.
Moreover, we are able to perform joint compression and enhancement either at the encoder or at the decoder side with no additional latency, which we demonstrate through background noise suppression for speech.

</details>
<br>

本文介绍 ***SoundStream***, 一种新的神经音频编解码器, 能够以通常针对语音定制的编解码器的比特率高效压缩语音, 音乐和一般音频.
***SoundStream*** 依赖于由全卷积编码器/解码器网络和残差矢量量化器组成的模型架构, 它们以端到端的方式联合训练.
训练利用了文本转语音和语音增强领域的最新进展, 这些进展结合了对抗损失和重构损失以使得从量化嵌入中生成高质量音频内容.
通过在量化器层应用结构化随机失活, 单个模型可以在 3 kbps 到 18 kbps 的可变比特率下运行, 与训练在固定比特率下时相比, 质量损失微乎其微.
此外, 该模型适合低延迟实现, 支持流式推理, 并在智能手机 CPU 上实时运行.
在使用 24kHz 采样率音频的主观评估中, ***SoundStream*** 在 3 kbps 比特率下优于 Opus 在 12 kbps 时的效果, 且接近 EVS 在 9.6 kbps 的性能.
此外, 我们能够在编码器或解码器短进行联合压缩和增强, 没有额外延迟, 我们通过语音背景噪声抑制展示了这一点.

## 1·Introduction: 引言

<details>
<summary>展开原文</summary>

Audio codec can be partitioned into two broad categories: waveform codecs and parametric codecs.
Waveform codecs aim at producing at the decoder side a faithful reconstruction of the input audio samples.
Often these codecs rely on transform coding techniques: a (usually invertible) transform is used to map an input time-domain waveform to the time-frequency domain.
Then, transform coefficients are quantized and entropy coded.
At the decoder side the transform is inverted to reconstruct a time-domain waveform.
Many codecs today combine transform coding with linear predictive coding in the time-domain, especially at medium bitrates and/or narrower signal bandwidths.
The bit allocation at the encoder is driven by a perceptual model, which determines the quantization process.
Generally, waveform codecs make little or no assumptions about the type of audio content and can thus operate on general audio.
As a consequence of this, they produce very high-quality audio at medium-to-high bitrates, but they tend to introduce coding artifacts when operating at low bitrates.
Parametric codecs [1] take a different approach, by making specific assumptions about the source audio to be encoded (in most cases, speech) and introducing strong priors in the form of a parametric model that describes the audio synthesis process.
The encoder estimates the parameters of the model, which are then quantized.
The decoder generates a time-domain waveform using a synthesis model driven by quantized parameters.
Unlike waveform codecs, the goal is not to obtain a faithful reconstruction on a sample-by-sample basis, but rather to generate audio that is perceptually similar to the original.

Traditional waveform and parametric codecs rely on signal processing pipelines and carefully engineered design choices, which exploit in-domain knowledge of psycho-acoustics and speech production to improve coding efficiency.
More recently, machine learning models have been successfully applied to the field of audio compression, demonstrating the additional value brought by data-driven solutions.
For example, it is possible to apply them as a post-processing step to improve the quality of existing codecs.
This can be accomplished either via audio superresolution, i.e., extending the frequency bandwidth [2], via audio denoising, i.e., removing lossy coding artifacts [3], or via packet loss concealment [4], [5].

Other solutions adopt models based on machine learning as an integral part of the audio codec architecture.
In these areas, recent advances in text-to-speech (TTS) technology proved to be a key ingredient.
For example, [WaveNet](../Vocoder/2016.09.12_WaveNet.md), a strong generative model originally applied to generate speech from text, was adopted as a decoder in a neural codec [7],[8].
Other neural audio codecs adopt different model architectures, e.g., WaveRNN in LPCNet [9] and WaveGRU in Lyra [10], all targeting speech at low bitrates.

In this paper we propose ***SoundStream***, a novel audio codec that can compress speech, music and general audio more efficiently than previous codecs, as illustrated in Fig.01.
***SoundStream*** leverages state-of-the-art solutions in the field of neural audio synthesis, and introduces a new learnable quantization module, to deliver audio at high perceptual quality, while operating at low-to-medium bitrates.
Fig.02 illustrates the high-level model architecture of the codec.
A fully convolutional encoder receives as input a time-domain waveform and produces a sequence of embeddings at a lower sampling rate, which are quantized by a residual vector quantizer.
A fully convolutional decoder receives the quantized embeddings and reconstructs an approximation of the original waveform.
The model is trained end-to-end using both reconstruction and adversarial losses.
To this end, one (or more) discriminators are trained jointly, with the goal of distinguishing the decoded audio from the original audio and, as a by-product, to provide a space where a feature-based reconstruction loss can be computed.
Both the encoder and the decoder only use causal convolutions, so the overall architectural latency of the model is determined solely by the temporal resampling ratio between the original time-domain waveform and the embeddings.
In summary, we make the following key contributions:
- We propose ***SoundStream***, a neural audio codec in which all the constituent components (encoder, decoder and quantizer) are trained end-to-end with a mix of reconstruction and adversarial losses to achieve superior audio quality.
- We introduce a new residual vector quantizer, and investigate the rate-distortion-complexity trade-off simplied by its design.
In addition, we propose a novel “quantizer dropout” technique for training the residual vector quantizer, which enables a single model to handle different bitrates.
- We demonstrate that learning the encoder brings a very significant coding efficiency improvement, with respect to a solution that adopts mel-spectrogram features.
- We demonstrate by means of subjective quality metrics that ***SoundStream*** outperforms both Opus and EVS over a wide range of bitrates.
- We design our model to support streamable inference, which can operate at low-latency.
When deployed on a smartphone, it runs in real-time on a single CPU thread.
- We propose a variant of the ***SoundStream*** codec that performs joint audio compression and enhancement, without introducing additional latency.

</details>
<br>

## 2·Related Work: 相关工作

<details>
<summary>展开原文</summary>

**Traditional Audio Codecs**:
Opus [11], EVS [12], and USAC [13] are state-of-the-art audio codecs, which combine traditional coding tools, such as linear predictive techniques and the modified discrete cosine transform, to deliver high coding efficiency over different content types, bitrates and sampling rates, while ensuring low-latency for real-time audio communications.
We compare ***SoundStream*** with both Opus and EVS in our subjective evaluation.

**Audio Generative Models**:
Several generative models have been developed for converting text or coded features into audio waveforms.
[WaveNet (2016)](../Vocoder/2016.09.12_WaveNet.md) allows for global and local signal conditioning to synthesize both speech and music.
[SampleRNN (2016)](../Vocoder/2016.12.22_SampleRNN.md) uses recurrent networks in a similar fashion, but it relies on previous samples at different scales.
These auto-regressive models deliver very high-quality audio, at the cost of an increased computational complexity, since samples are generated one by one.
To overcome this issue, Parallel WaveNet [15] allows for parallel computation, yielding considerable speedup during inference.
Other approaches involve lightweight and sparse models [16] and networks mimicking the fast Fourier transform as part of the model [9], [17].
More recently, generative adversarial models have emerged as a solution able to deliver high-quality audio with a lower computational complexity.
[MelGAN (2019)](../Vocoder/2019.10.08_MelGAN.md) is trained to produce audio waveforms when conditioned on mel-spectrograms, training a multi-scale waveform discriminator together with the generator.
[HiFi-GAN (2020)](../Vocoder/2020.10.12_HiFi-GAN.md) takes a similar approach but it applies discriminators to both multiple scales and multiple periods of the audio samples.
The design of the decoder and the losses in ***SoundStream*** is based on this class of audio generative models.

**Audio Enhancement**:
Deep neural networks have been applied to different audio enhancement tasks, ranging from denoising [20]–[24] to dereverberation [25], [26], lossy coding denoising [3] and frequency bandwidth extension [2], [27].
In this paper we show that it is possible to jointly perform audio compression and speech enhancement with a single model, without introducing additional latency.

**Vector Quantization**:
Learning the optimal quantizer is a key element to achieve high coding efficiency.
Optimal scalar quantization based on Lloyd’s algorithm [28] can be extended to a high-dimensional space via the generalized Lloyd algorithm (GLA) [29], which is very similar to k-means clustering [30].
In vector quantization [31], a point in a high-dimensional space is mapped onto a discrete set of code vectors.
Vector quantization has been commonly used as a building block of traditional audio codecs [32].
For example, Code-excited Linear Prediction (CELP [33]) encodes an excitation signal via a vector quantizer codebook.
More recently, vector quantization has been applied in the context of neural network models to compress the latent representation of input features.
For example, in variational autoencoders, vector quantization has been used to generate images [34], [35] and music [36], [37].
Vector quantization can become prohibitively expensive, as the size of the codebook grows exponentially when rate is increased.
For this reason, structured vector quantizers [38], [39] (e.g., residual, product, lattice vector quantizers, etc.) have been proposed to obtain a trade-off between computational complexity and coding efficiency in traditional codecs.
In ***SoundStream***, we extend the learnable vector quantizer of VQ-VAE [34] and introduce a residual (a.k.a. multi-stage) vector quantizer, which is learned end-to-end with the rest of the model.
To the best of the authors knowledge, this is the first time that this form of vector quantization is used in the context of neural networks and trained end-to-end with the rest of the model.

**Neural Audio Codecs**:
End-to-end neural audio codecs rely on data-driven methods to learn efficient audio representations, instead of relying on handcrafted signal processing components.
Autoencoder networks with quantization of hidden features were applied to speech coding early on [40].
More recently, a more sophisticated deep convolutional network for speech compression was described in [41].
Efficient compression of audio using neural networks has been demonstrated in several works, mostly targeting speech coding at low bitrates.
A VQ-VAE speech codec was proposed in [8], operating at 1.6 kbps.
Lyra [10] is a generative model that encodes quantized mel-spectrogram features of speech, which are decoded with an auto-regressive WaveGRU model to achieve state-of-the-art results at 3 kbps.
A very low-bitrate codec proposed in [42] decodes speech representations obtained via self-supervised learning.
An end-to-end audio codec targeting general audio at high bitrates (i.e., above 64 kbps) was proposed in [43].
The model architecture adopts a residual coding pipeline, which consists of multiple autoencoding modules and a psycho-acoustic model is used to drive the loss function during training.

Unlike [42] which specifically targets speech by combining speaker, phonetic and pitch embeddings, ***SoundStream*** does not make assumptions on the nature of the signal it encodes, and thus works for diverse audio content types.
While [10] learns a decoder on fixed features, ***SoundStream*** is trained in an end-to-end fashion.
Our experiments (see [Section 4](#Sec04)) show that learning the encoder increases the audio quality substantially.
***SoundStream*** achieves bitrate scalability, i.e., the ability of a single model to operate at different bitrates at no additional cost, thanks to its residual vector quantizer and to our original quantizer dropout training scheme (see [Section 3.3](#Sec03-03)).
This is unlike the work in [41], [43], [44] which enforce a specific bitrate and require training a different model for each target bitrate.
A single ***SoundStream*** model is able to compress speech, music and general audio, while operating at a 24 kHz sampling rate and low-to-medium bitrates (3 kbps to 18 kbps in our experiments), in real time on a smartphone CPU.
This is the first time that a neural audio codec is shown to outperform state-of-the-art codecs like Opus and EVS over this broad range of bitrates.

**Joint Compression and Enhancement**:
Recent work has explored joint compression and enhancement.
The work in [45] trains a speech enhancement system with a quantized bottleneck.
Instead, ***SoundStream*** integrates a time-dependent conditioning layer, which allows for real-time controllable denoising.
As we design ***SoundStream*** as a general-purpose audio codec, controlling when to denoise allows for encoding acoustic scenes and natural sounds that would be otherwise removed.

</details>
<br>

## 3·Methodology: 方法

<a id="Sec03"></a>

<details>
<summary>展开原文</summary>

We consider a single channel recording $x\in\mathbb{R}^T$ of duration $T$ and sampled at $f_s$.
The ***SoundStream*** model consists of a sequence of three building blocks, as illustrated in Fig.02:
- An encoder, which maps $x$ to a sequence of embeddings (see [Section 3.1](#Sec03-01)),
- A residual vector quantizer, which replaces each embedding by the sum of vectors from a set of finite codebooks, thus compressing the representation with a target number of bits (see [Section 3.3](#Sec03-03)),
- A decoder, which produces a lossy reconstruction $\hat{x}\in\mathbb{R}^T$ from quantized embeddings (see [Section 3.2](#Sec03-02)).

The model is trained end-to-end together with a discriminator (see [Section 3.4](#Sec03-04)), using the mix of adversarial and reconstruction losses described in [Section 3.5](#Sec03-05).
Optionally, a conditioning signal can be added, which determines whether denoising is applied at the encoder or decoder side, as detailed in [Section 3.6](#Sec03-06).

</details>
<br>

### 3.1·Encoder Architecture: 编码器架构

<a id="Sec03-01"></a>

<details>
<summary>展开原文</summary>

The encoder architecture is illustrated in Fig.03 and follows the same structure as the streaming SEANet encoder described in [2], but without skip connections.
It consists of a 1D convolution layer (with $C_{enc}$ channels), followed by $B_{enc}$ convolution blocks.
Each of the blocks consists of three residual units, containing  dilated convolutions with dilation rates of $1$, $3$, and $9$, respectively, followed by a down-sampling layer in the form of a strided convolution.
The number of channels is doubled whenever down-sampling, starting from $C_{enc}$.
A final 1D convolution layer with a kernel of length $3$ and a stride of $1$ is used to set the dimensionality of the embeddings to $D$ ($D = 256$ in our experiments).
To guarantee real-time inference, all convolutions are causal.
This means that padding is only applied to the past but not the future in both training and offline inference, whereas no padding is used in streaming inference.
We use the ELU activation [46] and we do not apply any normalization.
The number $B_{enc}$ of convolution blocks and the corresponding striding sequence determines the temporal resampling ratio between the input waveform and the embeddings.
For example, when $B_{enc}=4$ and using $(2,4,5,8)$ as strides, one embedding is computed every $M = 2 \times 4 \times 5 \times 8 = 320$ input samples.
Thus, the encoder outputs $enc(x) \in\mathbb{R}^{S\times D}$, with $S = T/M$.

</details>
<br>

### 3.2·Decoder Architecture: 解码器架构

<a id="Sec03-02"></a>

<details>
<summary>展开原文</summary>

The decoder architecture follows a similar design, as illustrated in Fig.03.
A 1D convolution layer is followed by a sequence of $B_{dec}$ convolution blocks.
The decoder block mirrors the encoder block, and consists of a transposed convolution for up-sampling followed by the same three residual units.
We use the same strides as the encoder, but in reverse order, to reconstruct a waveform with the same resolution as the input waveform.
The number of channels is halved whenever upsampling, so that the last decoder block outputs $C_{dec}$ channels.
A final 1D convolution layer with one filter, a kernel of size $7$ and stride $1$ projects the embeddings back to the waveform domain to produce $\hat{x}$.
In Fig.03, the same number of channels in both the encoder and the decoder is controlled by the same parameter, i.e., $C_{enc}= C_{dec}=C$.
We also investigate cases in which $C_{enc}\neq C_{dec}$, which results in a computationally lighter encoder and a heavier decoder, or vice-versa (see Section 5.4).

</details>
<br>

### 3.3·Residual Vector Quantizer: 残差向量量化器

<a id="Sec03-03"></a>

<details>
<summary>展开原文</summary>

The goal of the quantizer is to compress the output of the encoder $enc(x)$ to a target bitrate $R$, expressed in bits/second (bps).
In order to train ***SoundStream*** in an end-to-end fashion, the quantizer needs to be jointly trained with the encoder and the decoder by backpropagation.
The **Vector Quantizer (VQ)** proposed in [34], [35] in the context of [VQ-VAEs]() meets this requirement.
This vector quantizer learns a codebook of $N$ vectors to encode each $D$-dimensional frame of $enc(x)$.
The encoded audio $enc(x) \in\mathbb{R}^{S\times D}$ is then mapped to a sequence of one-hot vectors of shape $S \times N$, which can be represented using $S \log_{2} N$ bits.

#### Limitations of Vector Quantization: 向量量化的局限性

As a concrete example, let us consider a codec targeting a bitrate $R = 6000$ bps.
When using a striding factor $M = 320$, each second of audio at a sampling rate $f_{s}= 24000$ Hz is represented by $S = 75$ frames at the output of the encoder.
This corresponds to $r = 6000/75 = 80$ bits allocated to each frame.
Using a plain vector quantizer, this requires storing a codebook with $N = 2^{80}$ vectors, which is obviously unfeasible.

#### Residual Vector Quantizer: 残差向量量化器

To address this issue we adopt a Residual Vector Quantizer (a.k.a. multi-stage vector quantizer [39]), which cascades $N_q$ layers of VQ as follows.
The unquantized input vector is passed through a first VQ and quantization residuals are computed.
The residuals are then iteratively quantized by a sequence of additional $N_q−1$ vector quantizers, as described in [Algorithm 1]().
The total rate budget is uniformly allocated to each VQ, i.e., $r_{i}= r/N_{q}= \log_{2}N$.
For example, when using $N_{q}=8$, each quantizer uses a codebook of size $N = 2^{r/N_{q}}=2^{80/8}=1024$.
For a target rate budget $r$, the parameter $N_q$ controls the trade-off between computational complexity and coding efficiency, which we investigate in [Section 5.4]().

The codebook of each quantizer is trained with exponential moving average updates, following the method proposed in [VQ-VAE-2]() [35].
We also experimented with the original VQ-VAE layer [34], as well as one using a Gumbel softmax [47] but they performed significantly worse.
To improve the usage of the codebooks we use two additional methods.
- First, instead of using a random initialization for the codebook vectors, we run the k-means algorithm on the first training batch and use the learned centroids as initialization.
This allows the codebook to be close to the distribution of its inputs at initialization.
- Second, as proposed in [37], when a codebook vector has not been assigned any input frame for several batches, we replace it with an input frame randomly sampled within the current batch.
More precisely, we track the exponential moving average of the assignments to each vector (with a decay factor of 0.99) and replace the vectors of which this statistic falls below 2.

#### Enabling Bitrate Scalability with Quantizer Dropout: 利用量化器失活实现比特率可扩展性

Residual vector quantization provides a convenient framework for controlling the bitrate.
For a fixed size $N$ of each codebook, the number of VQ layers Nq determines the bitrate.
Since the vector quantizers are trained jointly with the encoder/decoder, in principle a different ***SoundStream*** model should be trained for each target bitrate.
Instead, having a single bitrate scalable model that can operate at several target bitrates is much more practical, since this reduces the memory footprint needed to store model parameters both at the encoder and decoder side.

To train such a model, we modify [Algorithm 1]() in the following way: for each input example, we sample $n_q$ uniformly at random in $[1; N_{q}]$ and only use quantizers $Q_{i}$ for $i = 1\cdots n_{q}$.
This can be seen as a form of structured dropout [48] applied to quantization layers.
Consequently, the model is trained to encode and decode audio for all target bitrates corresponding to the range $n_{q}= 1\cdots N_{q}$.
During inference, the value of nq is selected based on the desired bitrate.
Previous models for neural compression have relied on product quantization (e.g.[wav2vec 2.0](../SpeechRepresentation/2020.06.20_Wav2Vec2.0.md)), or on concatenating the output of several VQ layers [7], [8].
With such approaches, changing the bitrate requires either changing the architecture of the encoder and/or the decoder, as the dimensionality changes, or retraining an appropriate codebook.
A key advantage of our residual vector quantizer is that the dimensionality of the embeddings does not change with the bitrate.
Indeed, the additive composition of the outputs of each VQ layer progressively refines the quantized embeddings, while keeping the same shape.
Hence, no architectural changes are needed in neither the encoder nor the decoder to accommodate different bitrates.
In [Section 5.3](), we show that this method allows one to train a single ***SoundStream*** model, which matches the performance of models trained specifically for a given bitrate.

</details>
<br>

### 3.4·Discriminator Architecture: 判别器架构

<a id="Sec03-04"></a>

<details>
<summary>展开原文</summary>

To compute the adversarial losses described in [Section 3.5](), we define two different discriminators:
- a wave-based discriminator, which receives as input a single waveform;
- an STFT-based discriminator, which receives as input the complex-valued STFT of the input waveform, expressed in terms of real and imaginary parts.

Since both discriminators are fully convolutional, the number of logits in the output is proportional to the length of the input audio.
Consistently with previous work [19], we observed during development that a wave-based discriminator was enough to reconstruct speech with high quality, however using both the wave-based and STFT-based discriminators reduces artifacts when compressing music.

For the wave-based discriminator, we use the same multi-resolution convolutional discriminator proposed in [18] and adopted in [50].
Three structurally identical models are applied to the input audio at different resolutions: original, $2$-times down-sampled, and $4$-times down-sampled.
Each single-scale discriminator consists of an initial plain convolution followed by four grouped convolutions, each of which has a group size of $4$, a down-sampling factor of $4$, and a channel multiplier of $4$ up to a maximum of 1024 output channels.
They are followed by two more plain convolution layers to produce the final output, i.e., the logits.

The STFT-based discriminator is illustrated in [Fig.04]() and operates on a single scale, computing the STFT with a window length of $W = 1024$ samples and a hop length of $H = 256$ samples.
A 2D-convolution (with kernel size $7 \times 7$ and $32$ channels) is followed by a sequence of residual blocks.
Each block starts with a $3 \times 3$ convolution, followed by a $3 \times 4$ or a $4 \times 4$ convolution, with strides equal to $(1,2)$ or $(2,2)$, where $(s_{t},s_{f})$ indicates the down-sampling factor along the time and frequency axes.
We alternate between $(1,2)$ and $(2,2)$ strides, for a total of 6 residual blocks.
The number of channels is progressively increased with the depth of the network.
At the output of the last residual block, the activations have shape $T/(H \cdot 2^{3}) \times F/2^{6}$, where $T$ is the number of samples in the time domain and $F = W/2$ is the number of frequency bins.
The last layer aggregates the logits across the(down-sampled)frequency bins with a fully connected layer (implemented as a $1 \times F/2^{6}$ convolution), to obtain a 1-dimensional signal in the (down-sampled) time domain.

</details>
<br>

### 3.5·Training Objective: 训练目标

<a id="Sec03-05"></a>

<details>
<summary>展开原文</summary>

Let $\mathcal{G}(x) = dec(Q(enc(x)))$ denote the ***SoundStream*** generator, which processes the input waveform x through the encoder, the quantizer and the decoder, and ˆx = G(x) be the decoded waveform.
We train ***SoundStream*** with a mix of losses to achieve both signal reconstruction fidelity and perceptual quality,following the principles of the perception-distortion trade-off discussed in [51].

The adversarial loss is used to promote perceptual quality and it is defined as a hinge loss over the logits of the discriminator, averaged over multiple discriminators and over time.
More formally, let $k \in \{0,\cdots,K\}$ index over the individual discriminators, where $k = 0$ denotes the STFT-based discriminator and $k \in \{1,\cdots,K\}$ the different resolutions of the waveform-based discriminator ($K = 3$ in our case).
Let $T_k$ denote the number of logits at the output of the $k$-th discriminator along the time dimension.
The discriminator is trained to classify original vs decoded audio, by minimizing

$$
\begin{aligned}
  \mathcal{L}_{\mathcal{D}}
  &= \mathbb{E}_x\left[\dfrac{1}{K}\sum_{k}\dfrac{1}{T_{k}}\sum_{t}\max(0, 1-\mathcal{D}_{k,t}(x))\right]\\
  &+ \mathbb{E}_x\left[\dfrac{1}{K}\sum_{k}\dfrac{1}{T_{k}}\sum_{t}\max(0, 1+\mathcal{D}_{k,t}(\mathcal{G}(x)))\right]\\
\end{aligned}\tag{01}
$$

while the adversarial loss for the generator is

$$
\begin{aligned}
  \mathcal{L}_{\mathcal{G}}^{adv}
  &= \mathbb{E}_x\left[\dfrac{1}{K}\sum_{k}\dfrac{1}{T_{k}}\sum_{t}\max(0, 1-\mathcal{D}_{k,t}(\mathcal{G}(x)))\right]
\end{aligned}\tag{02}
$$

To promote fidelity of the decoded signal $\hat{x}$ with respect to the original $x$ we adopt two additional losses:
1. a “feature” loss $\mathcal{L}_{\mathcal{G}}^{feat}$, computed in the feature space defined by the discriminator(s) [18];
2. a multi-scale spectral reconstruction loss $\mathcal{L}_{\mathcal{G}}^{rec}$ [52].

More specifically, the feature loss is computed by taking the average absolute difference between the discriminator’s internal layer outputs for the generated audio and those for the corresponding target audio.

$$
\begin{aligned}
  \mathcal{L}_{\mathcal{G}}^{feat}
  &= \mathbb{E}_x\left[\dfrac{1}{KL}\sum_{k,l}\dfrac{1}{T_{k,l}}\sum_{t}|\mathcal{D}_{k,t}^{(l)}(x)-\mathcal{D}_{k,t}^{(l)}(\mathcal{G}(x))|\right]
\end{aligned}\tag{03}
$$

where $L$ is the number of internal layers, $\mathcal{D}_{k,t}^{(l)}, l\in\{1,\cdots,L\}$ is the $t$-th output of layer $l$ of discriminator $k$, and $T_{k,l}$ denotes the length of the layer in the time dimension.

The multi-scale spectral reconstruction loss follows the specifications described in [53]:

$$
\begin{aligned}
  \mathcal{L}_{\mathcal{G}}^{\mathrm{rec}}
  &=\sum_{s\in 2^{6},\ldots,2^{11}}\sum_{t}\|\mathcal{S}_{t}^{s}(x)-\mathcal{S}_{t}^{s}(\mathcal{G}(x))\|_{1}\\
  &+\alpha_{s}\sum_{t}\|\log\mathcal{S}_{t}^{s}(x)-\log\mathcal{S}_{t}^{s}(\mathcal{G}(x))\|_{2},
\end{aligned}\tag{04}
$$

where $S_{t}^{s}(x)$ denotes the $t$-th frame of a 64-bin mel-spectrogram computed with window length equal to $s$ and hop length equal to $s/4$.
We set $\alpha_{s}=\sqrt{s/2}$ as in [53].

The overall generator loss is a weighted sum of the different loss components:

$$
  \mathcal{L}_{\mathcal{G}} = \lambda_{adv} \mathcal{L}_{\mathcal{G}}^{adv} + \lambda_{feat} \mathcal{L}_{\mathcal{G}}^{feat} + \lambda_{rec} \mathcal{L}_{\mathcal{G}}^{rec}\tag{05}
$$

In all our experiments we set $\lambda_{adv}=1$, $\lambda_{feat}=100$ and $\lambda_{rec}=1$

</details>
<br>

### 3.6·Joint Compression and Enhancement: 联合压缩与增强

<a id="Sec03-06"></a>

<details>
<summary>展开原文</summary>

In traditional audio processing pipelines, compression and enhancement are typically performed by different modules.
For example, it is possible to apply an audio enhancement algorithm at the transmitter side, before audio is compressed, or at the receiver side, after audio is decoded.
In this setup, each processing step contributes to the end-to-end latency, e.g., due to buffering the input audio to the expected frame length determined by the specific algorithm adopted.
Conversely, we design ***SoundStream*** in such a way that compression and enhancement can be carried out jointly by the same model, without increasing the overall latency.

The nature of the enhancement can be determined by the choice of the training data.
As a concrete example, in this paper we show that it is possible to combine compression with background noise suppression.
More specifically, we train a model in such a way that one can flexibly enable or disable denoising at inference time, by feeding a conditioning signal that represents the two modes (denoising enabled or disabled).
To this end, we prepare the training data to consist of tuples of the form: `(inputs,targets,denoise)`.
When `denoise = false`, `targets = inputs`;
when `denoise = true`, `targets` contain the clean speech component of the corresponding `inputs`.
Hence, the network is trained to reconstruct noisy speech if the conditioning signal is disabled, and to produce a clean version of the noisy input if it is enabled.
Note that when `inputs` consist of clean audio (speech or music), `targets = inputs` and `denoise` can be either true or false.
This is done to prevent ***SoundStream*** from adversely affecting clean audio when denoising is enabled.

To process the conditioning signal, we use Feature-wise Linear Modulation (FiLM) layers [54] in between residual units, which take network features as inputs and transform them as

$$
  \tilde{a}_{n,c} = \gamma_{n,c} a_{n,c} + \beta_{n,c}\tag{06}
$$

where $a_{n,c}$ is the $n$-th activation in the $c$-th channel.
The coefficients $\gamma_{n,c}$ and $\beta_{n,c}$ are computed by a linear layer that takes as input a (potentially time-varying) two-dimensional one-hot encoding that determines the denoising mode.
This allows one to adjust the level of denoising over time.

In principle, FiLM layers can be used anywhere throughout the encoder and decoder architecture.
However, in our preliminary experiments, we found that applying conditioning at the bottleneck either at the encoder or at the decoder side (as illustrated in [Fig.03]()) was effective and no further improvements were observed by applying FiLM layers at different depths.
In [Section 5.5](#Sec05-05), we quantify the impact of enabling denoising at either the encoder or decoder side both in terms of audio quality and bitrate.

</details>
<br>

## 4·Experiments: 实验

### 4.1·Datasets & Training: 数据集与训练

<details>
<summary>展开原文</summary>

We train a single ***SoundStream*** model on three types of audio content: clean speech, noisy speech and music, all at 24 kHz sampling rate.
For clean speech, we use the LibriTTS dataset [55], with the following training splits: `train-clean-100`, `train-clean-360` and `train-other-500`.
We discard samples that do not contain frequencies above 8 kHz, that is, those which were upsampled from 16 kHz to 24 kHz.
This filter results in 30 k, 102 k and 186 k in each of the three splits.
Note that some of these samples, especially in the `train-other-500` split, might contain a mild level of early reverberation.
For noisy speech, we synthesize samples by mixing speech from LibriTTS with noise from Freesound [56].
This results in a dataset of 37 k noisy samples 2 to 20 s long, selecting noise segments which do not contain speech and have a CC0 license.
We apply peak normalization to randomly selected crops of 3 seconds and adjust the mixing gain of the noise component sampling uniformly in the interval `[−30 dB,0 dB]`.
For music, we use the MagnaTagATune dataset [57], sampling a random subset of 114 k 5-second samples.
In addition, we collect a real-world dataset, which contains two speakers (one male and one female speaker) with spontaneous English speech collected indoors in different rooms, located either in an apartment or in an office environment.
Each clip is collected with two microphones, one located close to the mouth of the speaker and one at a varying distance of 2 to 5 meters.
We select half of the samples from the near-field microphone and half from the far-field microphone, to evaluate on various reverberation conditions.
For a subset of the samples, a loudspeaker positioned in the same room is used to generate background noise.

We train with Adam [58], using a learning rate of $10_{−4}$ and a batch size of $128$, for $10^6$ steps.
As sequences have variable lengths, we prepare batches by cropping random segments of $360$ milliseconds.
Each segment is normalized to a peak value of $0.95$ and multiplied by a random gain in the interval $[0.3, 1.0]$ to ensure that the model is robust to a wide range of amplitudes.
We evaluate our models on disjoint test splits of the datasets above.
Unless stated otherwise, objective and subjective metrics are computed on a set of 200 audio clips 2–4 seconds long, with 50 samples from each of the four datasets listed above (i.e., clean speech, noisy speech, music, noisy/reverberant speech).

</details>
<br>

### 4.2·Evaluation Metrics: 评价指标

<details>
<summary>展开原文</summary>

To evaluate ***SoundStream***, we perform subjective evaluations by human raters.
We have chosen a crowd-sourced methodology inspired by MUSHRA [59], with a hidden reference but no lowpass-filtered anchor.
Each of the 200 samples of the evaluation dataset, which include clean, noisy and reverberant speech, as well as music, was rated 20 times.
The raters were required to be native English speakers and be using headphones.
The number of raters for the 3 kbps, 6 kbps and 12 kbps MUSHRA tests were 275, 273 and 273 respectively.
Additionally, to avoid noisy data, a post-screening was put in place to exclude listeners who rated the reference below 90 more than 20% of the time or rated non-reference samples above 90 more than 50% of the time.
After post-screening the number of raters was reduced to 131, 76 and 29 respectively.
We checked a-posteriori the impact of changing the rejection threshold, but this did not change the outcome of the experiments.

For development and hyperparameter selection, we rely on computational, objective metrics.
Numerous metrics have been developed in the past for assessing the perceived similarity between a reference and a processed audio signal.
The ITU-T standards PESQ [60] and its replacement POLQA [61] are commonly used metrics.
However, both are inconvenient to use owing to licensing restrictions.
We choose the freely available and recently open-sourced ViSQOL [62], [63] metric, which has previously shown comparable performance to POLQA.
In particular we use “audio” ViSQOL, which operates on audio resampled at 48 kHz.
In early experiments, we found this metric to be strongly correlated with subjective evaluations.
We thus use it for model selection and ablation studies.

</details>
<br>

### 4.3·Baselines: 基线

<details>
<summary>展开原文</summary>

Opus [11] is a versatile speech and audio codec supporting signal bandwidths from 4 kHz to 24 kHz and bitrates from 6 kbps to 510 kbps.
Since its standardization by the IETF in 2012 it has been widely deployed for speech communication over the internet.
As the audio codec in applications such as Zoom and applications based on WebRTC [64], [65], such as Microsoft Teams and Google Meet, Opus has hundreds of millions of daily users.
Opus is also one of the main audio codecs used in YouTube for streaming.
Enhanced Voice Services (EVS) [12] is the latest codec standardized by the 3GPP and was primarily designed for Voice over LTE (VoLTE).
Like Opus, it is a versatile codec operating at multiple signal bandwidths, 4 kHz to 20 kHz, and bitrates, 5.9 kbps to 128 kbps.
It is replacing AMR-WB [66] and retains full backward operability.
We use these two systems as baselines for comparison with the ***SoundStream*** codec.
For the lowest bitrates, we also compare ***SoundStream*** to the recently proposed Lyra codec [10], an autoregressive generative codec operating at 3 kbps.
We provide audio processed by ***SoundStream*** and baselines at different bitrates on a public webpage.

</details>
<br>

## 5·Results: 结果

### 5.1·Comparison with Other Codecs: 与其他编解码器的比较

<details>
<summary>展开原文</summary>

Fig.05 reports the main result of the paper, where we compare ***SoundStream*** to Opus and EVS at different bitrates.
Namely, we repeated a subjective evaluation based on a MUSHRA-inspired crowd sourced scheme, when ***SoundStream*** operates at three different bitrates: i) low (3 kbps); ii) medium (6 kbps); iii) high (12 kbps).
Fig.05(a) shows that ***SoundStream*** at 3 kbps significantly outperforms both Opus at 6 kbps and EVS at 5.9 kbps (i.e., the lowest bitrates at which these codecs can operate), despite using half of the bitrate.
To match the quality of ***SoundStream***, EVS needs to use at least 9.6 kbps and Opus at least 12 kbps, i.e., 3.2× to 4× more bits than ***SoundStream***.
We also observe that ***SoundStream*** outperforms Lyra when they both operate at 3 kbps.
We observe similar results when ***SoundStream*** operates at 6 kbps and 12 kbps.
At medium bitrates, EVS and Opus require, respectively, 2.2× to 2.6× more bits to match the same quality.
At high bitrates, 1.3× to 1.6× more bits.

Fig.06 illustrates the results of the subjective evaluation by content type.
The quality of ***SoundStream*** remains consistent when encoding clean speech and noisy speech.
In addition, ***SoundStream*** can encode music when using as little as 3 kbps, with quality significantly better than Opus at 12 kbps and EVS at 5.9 kbps.
This is the first time that a codec is shown to operate on diverse content types at such a low bitrate.

</details>
<br>

### 5.2·Objective Quality Metrics: 客观质量指标

<details>
<summary>展开原文</summary>

Fig.07a shows the rate-quality curve of ***SoundStream*** over a wide range of bitrates, from 3 kbps to 18 kbps.
We observe that quality, as measured by means of ViSQOL, gracefully decreases as the bitrate is reduced and it remains above 3.7 even at the lowest bitrate.
In our work, ***SoundStream*** operates at constant bitrate, i.e., the same number of bits is allocated to each encoded frame.
At the same time, we measure the bitrate lower bound by computing the empirical entropy of the quantization symbols of the vector quantizers, assuming each vector quantizer to be a discrete memoryless source, i.e., no statistical redundancy is exploited across different layers of the residual vector quantizer, nor across time.
Fig.07a indicates a potential rate saving between 7% and 20%.

We also investigate the rate-quality trade-off achieved when encoding different content types, as illustrated in Fig.07b.
Unsurprisingly, the highest quality is achieved when encoding clean speech.
Music represents a more challenging case, due to its inherent diversity of content.

</details>
<br>

### 5.3·Bitrate Scalability: 比特率可扩展性

<details>
<summary>展开原文</summary>

We investigate the bitrate scalability provided by training a single model that can serve different bitrates.
To evaluate this aspect, for each bitrate R we consider three ***SoundStream*** configurations:
a) a non-scalable model trained and evaluated at bitrate R (bitrate specific);
b) a non-scalable model trained at 18 kbps and evaluated at bitrate R by using only the first nq quantizers during inference (18 kbps - no dropout);
c) a scalable model trained with quantizer dropout and evaluated at bitrate $R$ (bitrate scalable).

[Fig.07c]() shows the ViSQOL scores for these three scenarios.
Remarkably, a model trained specifically at 18 kbps retains good performance when evaluated at lower bitrates, even though the model was not trained in these conditions.
Unsurprisingly, the quality drop increases as the bitrate decreases, i.e., when there is a more significant difference between training and inference.
This gap vanishes when using the quantizer dropout strategy described in [Section 3.3]().
Surprisingly, the bitrate scalable model seems to marginally outperform bitrate specific models at 9 kbps and 12 kbps.
This suggests that quantizer dropout, beyond providing bitrate scalability, may act as a regularizer.

We confirm these results by including the bitrate scalable variant of ***SoundStream*** in the MUSHRA subjective evaluation (see [Fig.05]()).
When operating at 3 kbps, the bitrate scalable variant of ***SoundStream*** is only slightly worse than the bitrate specific variant.
Conversely, both at 6 kbps and 12 kbps it matches the same quality as the bitrate specific variant.

</details>
<br>

### 5.4·Ablation Studies: 消融实验

<details>
<summary>展开原文</summary>

We carried out several additional experiments to evaluate the impact of some of the design choices applied to ***SoundStream***.
Unless stated otherwise, all these experiments operate at 6 kbps.

**Advantage of Learning the Encoder**:
We explore the impact of replacing the learnable encoder of ***SoundStream*** with a fixed mel-filterbank, similarly to Lyra [10].
In this setting, we learn both the quantizer and the decoder and observe a significant drop in objective quality, with ViSQOL going from 3.96 to 3.33.
Note that this is significantly worse than what can be achieved when learning the encoder and halving the bitrate (i.e., ViSQOL equal to 3.76 at 3 kbps).
This demonstrates that the additional complexity of having a learnable encoder translates to a very significant improvement in the rate-quality trade-off.

**Encoder and Decoder Capacity**:
The main drawback of using a learnable encoder is the computational cost of the neural architecture, which can be significantly higher than computing fixed, non-learnable features such as mel-filterbanks.
For ***SoundStream*** to be competitive with traditional codecs, not only should it provide a better perceptual quality at an equivalent bitrate, but it must also run in real-time on resource-limited hardware.
Table I shows how computational efficiency and audio quality are impacted by the number of channels in the encoder $C_{enc}$ and the decoder $C_{dec}$.
We measured the real-time factor (RTF), defined as the ratio between the temporal length of the input audio and the time needed for encoding/decoding it with ***SoundStream***.
We profiled these models on a single CPU thread of a Pixel4 smartphone.
We observe that the default model ($C_{enc}=C_{dec}= 32$) runs in real-time ($RTF 2.3times$).
Decreasing the model capacity by setting $C_{enc}=C_{dec}= 16$ only marginally affects the reconstruction quality while increasing the real-time factor significantly ($RTF 7.1\times$).
We also investigated configurations with asymmetric model capacities.
Using a smaller encoder, it is possible to achieve a significant speedup without sacrificing quality (ViSQOL drops from 3.96 to 3.94, while the encoder RTF increases to $18.6\times$).
Instead, decreasing the capacity of the decoder has a more significant impact on quality (ViSQOL drops from 3.96 to 3.84).
This is aligned with recent findings in the field of neural image compression [67], which also adopt a lighter encoder and a heavier decoder.

**Vector Quantizer Depth and Codebook Size**:
The number of bits used to encode a single frame is equal to Nqlog2N, where Nq denotes the number of quantizers and N the codebook size.
Hence, it is possible to achieve the same target bitrate for different combinations of Nqand N.
Table II shows three configurations, all operating at 6 kbps.
As expected, using fewer vector quantizers, each with a larger codebook, achieves the highest coding efficiency at the cost of higher computational complexity.
Remarkably, using a sequence of 80 1-bit quantizers leads only to a modest quality degradation.
This demonstrates that it is possible to successfully train very deep residual vector quantizers without facing optimization issues.
On the other side, as discussed in [Section 3.3](), growing the codebook size can quickly lead to unmanageable memory requirements.
Thus, the proposed residual vector quantizer offers a practical and effective solution for learning neural codecs operating at high bitrates, as it scales gracefully when using many quantizers, each with a smaller codebook.

**Latency**:
The architectural latency M of the model is defined by the product of the strides, as explained in [Section 3.1]().
In our default configuration, $M = 2 \times 4 \times 5 \times 8 = 320$ samples, which means that one frame corresponds to 13.3ms of audio at 24 kHz.
The bit budget allocated to the residual vector quantizer needs to be adjusted based on the target architectural latency.
For example, when operating at 6 kbps, the residual vector quantizer has a budget of 80 bits per frame.
If we double the latency, one frame corresponds to 26.6 ms, so the per-frame budget needs to be increased to 160 bits.
Table III compares three configurations, all operating at 6 kbps, where the budget is adjusted by changing the number of quantizers, while keeping the codebook size fixed.
We observe that these three configurations are equivalent in terms of audio quality.
At the same time, increasing the latency of the model significantly increases the real-time factor, as encoding/decoding of a single frame corresponds to a longer audio sample.

</details>
<br>

### 5.5·Joint Compression and Enhancement: 联合压缩与增强

<details>
<summary>展开原文</summary>

We evaluate a variant of ***SoundStream*** that is able to jointly perform compression and background noise suppression, which was trained as described in [Section 3.6]().
During training, we set the denoise flag 50% of the time in each batch.
We consider two configurations, in which the conditioning signal is applied to the embeddings:
- one where the conditioning signal is added at the encoder side, just before quantization;
- another where it is added at the decoder side.

For each configuration, we train models at different bitrates.
For evaluation we use 1000 samples of noisy speech, generated as described in [Section 4.1]() and compute ViSQOL scores when denoising is enabled or disabled, using clean speech references as targets.
Figures 8 shows a substantial improvement of quality when denoising is enabled, with no significant difference between denoising either at the encoder or at the decoder.
We observe that the proposed model, which is able to flexibly enable or disable denoising at inference time, does not incur a cost in performance, when compared with a model in which denoising is always enabled.
This can be seen comparing Fig.08c with Fig.08a and Fig.08b.

We also investigate whether denoising affects the potential bitrate savings that would be achievable by entropy coding.
To evaluate this aspect, we first measured the empirical probability distributions $p_{i}^{(q)},i = 1 \cdots N,q = 1 \cdots N_{q}$ on $3200$ samples of training data.
Then, we measured the empirical distribution r(q)ion the 1000 test samples and computed the cross-entropy $H(r,p) = − \sum_{i,q}r_{i}^{(q)}\log_{2}p_{i}^{(q)}$, as an estimate of the bitrate lower bound needed to encode the test samples.
Fig.08 shows that both the encoder-side denoising and fixed denoising offer substantial bitrate savings when compared with decoder-side denoising.
Hence, applying denoising before quantization leads to a representation that can be encoded with fewer bits.

</details>
<br>

### 5.6·Joint vs Disjoint Compression and Enhancement: 联合与分离

<details>
<summary>展开原文</summary>

We compare the proposed model, which is able to perform joint compression and enhancement, with a configuration in which compression is performed by ***SoundStream*** (with denoising disabled) and enhancement by a dedicated denoising model.
For the latter, we adopt [SEANet](../_Basis/2020.09.04_SEANet.md), which features a very similar model architecture, with the notable exception of skip connections between encoder and decoder layers and the absence of quantization.
We consider two variants:
- one in which compression is followed by denoising (i.e., denoising is applied at the decoder side);
- another one in which denoising is followed by compression (i.e., denoising is applied at the encoder side).

We evaluate the different models using the [VCTK dataset](../../Datasets/2012.08.00_VCTK.md), which was neither used for training ***SoundStream*** nor SEANet.
The input samples are 2 s clips of noisy speech cropped to reduce periods of silence and resampled at 24 kHz.
For each of the four input signal-to-noise ratios (0 dB, 5 dB, 10 dB and 15 dB) and clean audio, we run inference on 1000 samples and compute ViSQOL scores.
As shown in Table IV, one single model trained for joint compression and enhancement achieves a level of quality that is almost on par with using two disjoint models.
Also, the former requires only half of the computational cost and incurs no additional architectural latency, which would be introduced when stacking disjoint models.
We also observe that the performance gap decreases as the input SNR increases.

</details>
<br>

## 6·Conclusion: 结论

<details>
<summary>展开原文</summary>

We propose ***SoundStream***, a novel neural audio codec that outperforms state-of-the-art audio codecs over a wide range of bitrates and content types.
***SoundStream*** consists of an encoder, a residual vector quantizer and a decoder, which are trained end-to-end using a mix of adversarial and reconstruction losses to achieve superior audio quality.
The model supports streamable inference and can run in real-time on a single smartphone CPU.
When trained with quantizer dropout, a single ***SoundStream*** model achieves bitrate scalability with a minimal loss in performance when compared with bitrate-specific models.
In addition, we show that it is possible to combine compression and enhancement in a single model without introducing additional latency.
In future work, we plan to extend the range of bitrates over which ***SoundStream*** operates, covering higher bitrates, aiming to attain perceptually lossless audio and encoding multi-channel audio.

</details>
<br>

本文提出了 ***SoundStream***, 一种新式神经音频编解码器, 其性能在不同比特率和内容类型上的性能表现优于当前最佳的音频编解码器.
***SoundStream*** 由一个编码器, 一个残差向量量化器和一个解码器组成, 采用对抗损失和重构损失的混合以端到端的方式进行训练以达到优质的音频质量.
该模型支持流式推理, 且可以在单个智能手机 CPU 上实时运行.
当采用量化器失活时, 与其他特定比特率的模型相比, 单个 ***SoundStream*** 模型在获得性能上损失最小时获得比特率可扩展性.
此外, 我们还展示了在单个模型上结合压缩和增强是可行的, 且不会引入额外的延迟.
在未来的工作中, 我们计划扩展 ***SoundStream*** 能处理的比特率范围, 包括高比特率, 目标是实现无损音频的感知质量, 并编码多通道音频.