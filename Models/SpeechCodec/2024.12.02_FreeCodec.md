# FreeCodec

<details>
<summary>基本信息</summary>

- 标题: "FreeCodec: A Disentangled Neural Speech Codec with Fewer Tokens"
- 作者:
  - 01 Youqiang Zheng,
  - 02 Weiping Tu,
  - 03 Yueteng Kang,
  - 04 Jie Chen,
  - 05 Yike Zhang,
  - 06 Li Xiao,
  - 07 Yuhong Yang,
  - 08 Long Ma
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.01053)
  - [Publication]() Submiited to ICASSP 2025
  - [Github](https://github.com/exercise-book-yq/FreeCodec) 暂未开源
  - [Demo](https://exercise-book-yq.github.io/FreeCodec-Demo/)
- 文件:
  - [ArXiv](_PDF/2412.01053v2__FreeCodec__A_Disentangled_Neural_Speech_Codec_with_Fewer_Tokens.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Neural speech codecs have gained great attention for their outstanding reconstruction with discrete token representations.
It is a crucial component in generative tasks such as speech coding and large language models (LLM).
However, most works based on residual vector quantization perform worse with fewer tokens due to low coding efficiency for modeling complex coupled information.
In this paper, we propose a neural speech codec named ***FreeCodec*** which employs a more effective encoding framework by decomposing intrinsic properties of speech into different components:
1) a global vector is extracted as the timbre information,
2) a prosody encoder with a long stride level is used to model the prosody information,
3) the content information is from a content encoder.

Using different training strategies, ***FreeCodec*** achieves state-of-the-art performance in reconstruction and disentanglement scenarios.
Results from subjective and objective experiments demonstrate that our framework outperforms existing methods.
The related code will be released at [Github](https://github.com/exercise-book-yq/FreeCodec).

</details>
<br>

神经语音编解码器由于其通过离散 Token 表示实现的优秀重构能力而获得了广泛关注.
它是生成任务 (如语音编码和大语言模型) 中的关键组件.
然而, 大多数基于残差向量量化的工作在较少的 Token 下表现不佳, 因为建模复杂耦合信息的编码效率较低.

在本文中, 我们提出了一个名为 ***FreeCodec*** 的神经语音编解码器, 它将语音的内在属性分解到不同的组件来实现更有效的编码框架:
1. 全局向量作为音色信息提取,
2. 具有较长步长级别的韵律编码器用于模拟韵律信息,
3. 内容信息来自内容编码器.

使用不同的训练策略, ***FreeCodec*** 在重构和解耦场景中实现了最先进的性能.
主观和客观实验中的结果表明我们的框架优于现有方法.
相关代码将在 [Github](https://github.com/exercise-book-yq/FreeCodec) 上发布.

## 1·Introduction: 引言

Neural speech codecs are widely used to compress speech signals for a limited number of bits with minimal distortion.
Compared to traditional parametric algorithms ([Codec2 [1]](Codec2.md); [MELP [2]](1997.04.21_MELP.md)), it has progressed significantly in medium- or low-bitrate scenarios.
With the development of large language models (LLM), the discrete codes of neural speech codecs also play a pivotal role in LLM-driven generative speech models.

Existing mainstream end-to-end (E2E) works ([SoundStream [3]](2021.07.07_SoundStream.md); [EnCodec [4]](2022.10.24_EnCodec.md); [TF-Codec [5]](2022.07.18_TF-Codec.md); [SuperCodec [6]](2024.07.30_SuperCodec.md); [CBRC [7]](2024.02.02_CBRC.md); [WavTokenizer [8]](2024.08.29_WavTokenizer.md)) rely on the [VQ-VAE [9]](../../Modules/VQ/2017.11.02_VQ-VAE.md) architecture to learn an encoder, a vector quantizer, and a decoder by data-driven.
These techniques utilize vector quantization to compress or discrete the latent features from the encoder.
Many researches ([CBRC [7]](2024.02.02_CBRC.md); [HiFi-Codec [10]](2023.05.04_HiFi-Codec.md); [Descript-Audio-Codec (DAC) [11]](2023.06.11_Descript-Audio-Codec.md); [SRCodec [12]](2024.03.18_SRCodec.md)) are based on optimizing vector quantization to improve the reconstructed speech quality.
For instance, [SoundStream [3]](2021.07.07_SoundStream.md) introduces a residual vector quantizer (RVQ) into neural speech codecs to achieve state-of-the-art (SOTA) performance from 3 to 18 kbps.
It is more efficient and has lower complexity than the plain vector quantizer.
In [HiFi-Codec [10]](2023.05.04_HiFi-Codec.md), the group-residual vector quantization (GRVQ) is proposed for enjoying better performance while containing four quantizers at 2 kbps.
Furthermore, [Descript-Audio-Codec (DAC) [11]](2023.06.11_Descript-Audio-Codec.md) introduces factorized and L2-normalized codes to improve codebook usage, operating a higher compression rate than [EnCodec [4]](2022.10.24_EnCodec.md).
However, when using two codebooks even less, the performance of these methods is struggling, like content loss resulting in unintelligible.

Several works ([Polyak et al [13]](../_Full/2021.04.01_Speech_Resynthesis_from_Discrete_Disentangled_Self-Supervised_Representations.md); [Omran et al [14]](../_Full/2022.03.29_Disentangling_Speech_from_Surroundings_with_Neural_Embeddings.md); [TiCodec [15]](2023.09.15_TiCodec.md); [SpeechTokenizer [16]](2023.08.31_SpeechTokenizer.md); [Single-Codec [17]](2024.06.11_Single-Codec.md); [SemantiCodec [18]](2024.04.30_SemantiCodec.md)) have explored speech reconstruction with the disentangled feature under the VQ-VAE paradigm.
Similar to voice conversion (VC), these methods disentangle a global speaker identity and content representations.
[Polyak et al [13]](../_Full/2021.04.01_Speech_Resynthesis_from_Discrete_Disentangled_Self-Supervised_Representations.md) utilizes a pretrained self-supervised learning models to disentangle content information at small datasets.
Recently, [TiCodec [15]](2023.09.15_TiCodec.md) explores an additional global encoder to extract time-invariant information out of speech.
It reduces the redundancy of frame-level information to attain improved encoding efficiency and exhibits improved performance using one or two tokens.
However, speech includes several attributes(not just global and non-global), and each of them should be modeled using a module ([Mega-TTS [19]](../SpeechLM/2023.06.06_Mega-TTS.md)).
Inspired by this, we explore a more detailed disentanglement of representations framework for better reconstruction.
This framework also can be flexibly used in disentanglement scenarios.

In this paper, we propose a more detailed representation of the neural speech codec - ***FreeCodec***.
By modeling complex speech into intrinsic attributes(speaker, prosody, and content), it achieves better performance in reconstruction and disentanglement scenarios.
Meanwhile, we adopt different frame-level representations for different attributes, enabling more effective quantization and higher compression.

Our main contributions are as follows:

- We propose ***FreeCodec***, a more-grained disentanglement neural speech codec that encodes intrinsic properties in speech with self-supervision.
- We show that our proposed framework can be flexibly used in reconstruction(e.g., zero-shot TTS, speech coding) and disentanglement(e.g., voice conversion) scenarios when using different training strategies.
- Our proposed method using approximately 57 tokens per second, surpasses the existing state-of-the-art models in subjective and objective evaluation.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

### A.Overall

As illustrated in Fig.\ref{a}, our proposed method consists of three components: encoders, quantizers, and decoders.
Unlike existing works, our encoder proposes a more detailed modeling focus on different intrinsic properties in human speech.
It can alleviate the pressure on a single encoder component and provides a more detailed capability to model intrinsic speech attributes.
Specifically, we introduce three types of encoders to encode content, prosody, and timbre information, respectively.
The input raw speech signal is first used to generate three latent feature representations.
Then the quantization layers produce compressed representations.
Finally, the decoders reconstruct the speech signal from the compressed latent representations.
In addition, we utilize different training strategies to provide three versions for reconstruction(zero-shot TTS, speech coding) and disentanglement(voice conversion) scenarios.
The details of training strategies are described in Section \ref{quantization} and \ref{decoder}.

### B.Encoder

#### Speaker Encoder

Existing approaches extract a global embedding in the encoder in an unsupervised manner, and then the decoder aggregates the global embedding into the frame-level embeddings.
They assume that the global embedding can represent time-invariant information, such as the characteristics of the speaker and speaking style.
Here, we follow this unsupervised manner and further extract the speaker's information more precisely.
We utilize a pre-trained speaker encoder, [ECAPA-TDNN [20]](../_tmp/ECAPA-TDNN.md), a state-of-the-art speaker recognition network based on convolution neural networks with an attentive statistics pooling layer.
The mel-spectrogram sampled from the raw speech signal is fed into the speaker encoder to get one global timbre vector.

#### Content Encoder

The architecture of the content encoder follows the [SuperCodec [6]](2024.07.30_SuperCodec.md); encoder using (2, 4, 5, 8) as strides, a number \textbf{${B}_{enc}$} = 4 of convolution blocks.
It indicates a total downsampling of 320 times and outputs 256-dimensional content features with a frame rate of 50 Hz from 16 kHz speech.
In order to reduce the redundancy of the content encoder, we propose to use a self-supervised model to explicitly model the content information, as shown in Fig.\ref{a}.

#### Prosody Encoder

The prosody encoder extracts the information apart from the speaker and content information, as shown in Fig.\ref{b}.
In [ProsoSpeech [21]](../_tmp/2022.02.16_ProsoSpeech.md); [Mega-TTS2 [22]](../SpeechLM/2023.07.14_Mega-TTS2.md), the first 20 bins in each mel-spectrogram frame are taken as input to extract prosody because it contains almost complete prosody and much less speaker and content information than the full band.
Following the related work ([Mega-TTS2 [22]](../SpeechLM/2023.07.14_Mega-TTS2.md)), we adopt the prosody encoder consisting of two convolution stacks, a max pooling layer with a stride of 8 to remove content and speaker information further.
Our proposed method sets the FFT and hop size to 1024 and 320.
With these setups, the prosody encoder results in roughly a frame rate of 7 Hz feature embeddings with 256 dimensions.

### C.Quantization

We adopt different methods to quantize different features.
For the content and prosody information, we adopt a plain vector quantizer with one codebook, and the codebook size is set to 256.
As for the speaker embedding, we use two types: continuous representation for ***FreeCodec-v1*** and ***FreeCodec-v3*** and discrete representation for ***FreeCodec-v2***.
Specifically, we compress the speaker embedding by group vector quantization (GVQ) in ***FreeCodec-v2*** for speech coding.
It divides the speaker embedding into eight groups that are quantized by one codebook with 1024 codebook size respectively.
As for ***FreeCodec-v1*** and ***FreeCodec-v3***, we provide the continuous representation to the decoder for better reconstruction in such as zero-shot TTS and voice conversion scenarios, similar to ([NaturalSpeech3 [23]](../Diffusion/2024.03.05_NaturalSpeech3.md); [PromptCodec [24]](2024.04.03_PromptCodec.md)).

### D.Decoders and Training Strategy

***FreeCodec*** also employs a mirrored decoder upsampling structure ([Github·SuperCodec](https://github.com/exercise-book-yq/Supercodec)).
It uses (8, 5, 4, 2) as strides, resulting in a total upsampling of 320 times.
However, reconstructing in the high compression rate scenarios is particularly challenging.
Before upsampling, we first adopt 4 layers Transformer encoder to enhance the semantic modeling.
Then, we use [ConvNeXt [26]](../_Basis/2022.01.10_ConvNeXt.md) as a fundamental backbone to condition the prosody and speaker representations.

We incorporate adversarial training to promote perceptual quality, using a multi-scale STFT-based (MS-STFT) discriminator.
The training loss of the proposed method comprises five components: reconstruction loss, VQ commitment loss, content loss, feature matching loss, and adversarial losses.
The reconstruction loss, feature loss, and adversarial losses follow EnCodec ([SoundStream [3]](2021.07.07_SoundStream.md)?).

For the content loss, we extract the last layer representation from a pre-trained [WavLM-Large [27]](../SpeechRepresentation/2021.10.26_WavLM.md) model as the semantic learning target at 50 Hz.
In ***FreeCodec-v1*** and ***FreeCodec-v2***, we use it to reduce the redundancy of the content encoder.
It maximizes the cosine similarity at the level of the dimensions across all timesteps between the outputs of the content encoder and semantic learning target.
In ***FreeCodec-v3***, we only use the semantic learning target at the decoder to prevent additional speaker information from leaking to the content encoder and quantizer.
We also utilize spectrogram-resize based data augmentation on the prosody and content encoder in the training ([FreeVC [28]](../Voice_Conversion/2022.10.27_FreeVC.md)).
This approach achieves better performance in disentanglement scenarios.

## 4·Experiments: 实验

### A.Training Details and Baselines

We trained our model on [LibriSpeech [29]](../../Datasets/2015.04.19_LibriSpeech.md), which consists of approximately 1000 hours of speech at 16khz.
We use train-clean-100, train-clean-360, and train-other-500 subsets.
For a fair comparison, we adopt two recent neural codecs, [TiCodec [Github]](https://github.com/y-ren16/TiCodec) and [Descript-Audio-Codec (DAC) [Github]](https://github.com/descriptinc/descript-audio-codec), which have demonstrated success in the domain of neural speech codecs.
The baselines are re-trained with 1 and 2 codebooks, indicating 0.5 kbps and 1 kbps.
***FreeCodec*** and TiCodec are trained on two V100 GPUs with 400 k iterations and batchsize of 20 per GPU.
DAC is trained on two V100 GPUs with 800 k iterations and batchsize of 10 per GPU.
In addition, we also consider several open-source speech codecs as baselines,  [EnCodec [Github]](https://github.com/facebookresearch/encodec) at 3 kbps, and [Lyra-v2 [Github]](https://github.com/google/lyra) at 3.2 kbps, and [SpeechTokenizer [16]](2023.08.31_SpeechTokenizer.md) at 3 kbps, and [SemantiCodec [18]](2024.04.30_SemantiCodec.md) at 1.3 kbps, and [WavTokenizer-small [8]](2024.08.29_WavTokenizer.md) at 0.9 kbps.
For Encodec and Wavtokenizer-small, we use the 24 kHz pre-trained model to synthesize speech, corresponding to the same compression rate of 2 kbps and 0.6 kbps for the 16 kHz sampling rate, respectively.

As for the voice conversion, three baseline models are selected to be compared with ***FreeCodec-v3***:
[VQMIVC [31]](../Voice_Conversion/VQMIVC.md); [YourTTS [32]](../E2E/2021.12.04_YourTTS.md); [Wav2Vec-VC [33]](../Voice_Conversion/Wav2Vec-VC.md).
These models are trained on the VCTK datasets.

### B.Evaluation

We evaluate ***FreeCodec*** from two aspects:

1) Reconstruction Quality.
   We conduct it on [VCTK [30]](../../Datasets/2012.08.00_VCTK.md) and test-clean subset of LibriSpeech.
   For VCTK, we randomly select data from 8 speakers and 2911 utterances for the test.
   For LibriSpeech, we use the test-clean subset, 2620 utterances for the test.
   All audio samples are downsampled to 16 kHz.

2) Disentanglement Ability.
   we evaluate it based on the voice conversion benchmark.
   We randomly select 200 utterances from LibriSpeech test-clean subset as source speech and 6 speakers from VCTK as the target speaker.
   All models are evaluated in LibriSpeech Test-clean to VCTK scenarios.

#### Subjective Evaluation

We follow the established [MUSHRA [34]](../../Evaluations/MUSHRA.md) methodology to evaluate the subjective quality of our baselines and ***FreeCodec-v2***.
A group of fifteen listeners participate in the subjective tests.
Fifteen utterances are randomly selected from our test sets for evaluation.
In addition, we also adopt the [Speex [35]](2016.02.28_Speex.md) at 4 kbps as our low anchor.

#### Objective Evaluation

For objective evaluation of reconstruction, we employ the automatic Mean Opinion Score prediction system ([UTMOS [36]](../../Evaluations/2022.04.05_UTMOS.md)), and the short-time objective intelligibility ([STOI [38]](../../Evaluations/STOI.md)), and the [WARP-Q [37]](../../Evaluations/WARP-Q.md), and the Speaker Embedding Cosine Similarity ([SECS [Github]](https://github.com/resemble-ai/Resemblyzer)) to evaluate the overall speech quality.
In addition, we use Word error rate (WER), character error rate (CER), and F0-PCC to evaluate the objective evaluation of voice conversion.
Among them, WER and CER between source and converted speech are calculated by an ASR model ([HuggingFace](https://huggingface.co/openai/whisper-large)).
F0-PCC is the Pearson correlation coefficient used to evaluate ${f}_{0}$ consistency between source and converted speech.

## 5·Results: 结果

### A.Reconstruction Quality

Table \ref{speech} summarizes the results of objective reconstruction experiments.
The ***FreeCodec-v1*** performs best in almost all objective metrics in test sets.
Especially in out-of-domain environments, our proposed method achieves the best reconstruction performance using only approximately 57 tokens per second.
Compared to the ***FreeCodec-v2***, the ***FreeCodec-v1*** is better especially in speaker similarity.
It shows that the continuous global representation is more effective in reconstruction scenarios.
Although slightly lower STOI and SECS than DAC at 1 kbps, ***FreeCodec-v2*** gets better objective speech quality according to UTMOS and WARP-Q.
The same result can also be concluded in subjective evaluation, as illustrated in Fig. \ref{fig:mushra}.
In addition, ***FreeCodec-v2*** achieves higher quality than Lyra-v2 at 3.2 kbps, EnCodec at 3 kbps, and TiCodec at 1 kbps.
It demonstrates that the more-grained disentanglement framework leads to lower bitrate but higher reconstruction quality.

Furthermore, we also conducted an ablation study to validate the explicit effect of content loss in the content encoder.
It can be observed that removing the content loss causes the performance drop in all objective metrics, especially the UTMOS and STOI.

### B.Disentanglement ability

In this section, we describe the disentanglement ability on the voice conversion experiments.
***FreeCodec-v3*** achieves voice conversion by using the speaker information from the target speech.
As shown in Table \ref{vc}, the ***FreeCodec-v3*** achieves lower WER and CER than all baseline models, especially the text-based models.
Meanwhile, the F0 PCC and speaker similarity of ***FreeCodec-v3*** is also the highest.
This indicates that our proposed method achieves superior disentanglement in intrinsic attributes of human speech.

## 6·Conclusions: 结论

In this paper, we propose a more grained disentanglement framework that factorizes speech into the intrinsic attributes in a self-supervised manner.
We show that a more-grained disentanglement framework can be used in reconstruction and disentanglement scenarios by different training strategies.
Compared to existing methods, we use fewer tokens and lower bandwidth to achieve high-quality reconstruction.
Our experiments show a significant improvement over existing methods, highlighting the effectiveness of our approach in reconstruction quality and disentanglement ability.