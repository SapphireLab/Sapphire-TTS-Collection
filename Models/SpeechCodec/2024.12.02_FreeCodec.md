# FreeCodec

<details>
<summary>基本信息</summary>

- 标题: "FreeCodec: A Disentangled Neural Speech Codec with Fewer Tokens"
- 作者:
  - 01 Youqiang Zheng,
  - 02 Weiping Tu,
  - 03 Yueteng Kang,
  - 04 Jie Chen,
  - 05 Yike Zhang,
  - 06 Li Xiao,
  - 07 Yuhong Yang,
  - 08 Long Ma
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.01053)
  - [Publication]() Submiited to ICASSP 2025
  - [Github](https://github.com/exercise-book-yq/FreeCodec) 暂未开源
  - [Demo](https://exercise-book-yq.github.io/FreeCodec-Demo/)
- 文件:
  - [ArXiv](_PDF/2412.01053v2__FreeCodec__A_Disentangled_Neural_Speech_Codec_with_Fewer_Tokens.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Neural speech codecs have gained great attention for their outstanding reconstruction with discrete token representations.
It is a crucial component in generative tasks such as speech coding and large language models (LLM).
However, most works based on residual vector quantization perform worse with fewer tokens due to low coding efficiency for modeling complex coupled information.
In this paper, we propose a neural speech codec named ***FreeCodec*** which employs a more effective encoding framework by decomposing intrinsic properties of speech into different components:
1) a global vector is extracted as the timbre information,
2) a prosody encoder with a long stride level is used to model the prosody information,
3) the content information is from a content encoder.

Using different training strategies, ***FreeCodec*** achieves state-of-the-art performance in reconstruction and disentanglement scenarios.
Results from subjective and objective experiments demonstrate that our framework outperforms existing methods.
The related code will be released at [Github](https://github.com/exercise-book-yq/FreeCodec).

## 1·Introduction: 引言

Neural speech codecs are widely used to compress speech signals for a limited number of bits with minimal distortion.
Compared to traditional parametric algorithms \cite{rowe2011codec,supplee1997melp}, it has progressed significantly in medium- or low-bitrate scenarios.
With the development of large language models (LLM), the discrete codes of neural speech codecs also play a pivotal role in LLM-driven generative speech models.

Existing mainstream end-to-end (E2E) works \cite{zeghidour2022soundstream, defossez2022high,jiang2023latent, zheng2024supercodec, xu23_interspeech, ji2024wavtokenizer} rely on the VQ-VAE \cite{van2017neural} architecture to learn an encoder, a vector quantizer, and a decoder by data-driven.
These techniques utilize vector quantization to compress or discrete the latent features from the encoder.
Many researches ~\cite{yang2023hifi, xu23_interspeech, kumar2024high, zheng2024srcodec} are based on optimizing vector quantization to improve the reconstructed speech quality.
For instance, Soundstream \cite{zeghidour2022soundstream} introduces a residual vector quantizer (RVQ) into neural speech codecs to achieve state-of-the-art (SOTA) performance from 3 to 18 kbps.
It is more efficient and has lower complexity than the plain vector quantizer.
In \cite{yang2023hifi}, the group-residual vector quantization (GRVQ) is proposed for enjoying better performance while containing four quantizers at 2 kbps.
Furthermore, Descript-audio-codec (DAC) \cite{kumar2024high} introduces factorized and L2-normalized codes to improve codebook usage, operating a higher compression rate than EnCodec \cite{defossez2022high}.
However, when using two codebooks even less, the performance of these methods is struggling, like content loss resulting in unintelligible.

Several works \cite{polyak2021speech,omran2023disentangling, ren2024fewer, zhang2024speechtokenizer,li2024single, liu2024semanticodec} have explored speech reconstruction with the disentangled feature under the VQ-VAE paradigm.
Similar to voice conversion (VC), these methods disentangle a global speaker identity and content representations.
\cite{polyak2021speech} utilizes a pretrained self-supervised learning models to disentangle content information at small datasets.
Recently, TiCodec \cite{ren2024fewer} explores an additional global encoder to extract time-invariant information out of speech.
It reduces the redundancy of frame-level information to attain improved encoding efficiency and exhibits improved performance using one or two tokens.
However, speech includes several attributes(not just global and non-global), and each of them should be modeled using a module \cite{jiang2023mega}.
Inspired by this, we explore a more detailed disentanglement of representations framework for better reconstruction.
This framework also can be flexibly used in disentanglement scenarios.

In this paper, we propose a more detailed representation of the neural speech codec - FreeCodec.
By modeling complex speech into intrinsic attributes(speaker, prosody, and content), it achieves better performance in reconstruction and disentanglement scenarios.
Meanwhile, we adopt different frame-level representations for different attributes, enabling more effective quantization and higher compression.

Our main contributions are as follows:

- We propose FreeCodec, a more-grained disentanglement neural speech codec that encodes intrinsic properties in speech with self-supervision.
- We show that our proposed framework can be flexibly used in reconstruction(e.g., zero-shot TTS, speech coding) and disentanglement(e.g., voice conversion) scenarios when using different training strategies.
- Our proposed method using approximately 57 tokens per second, surpasses the existing state-of-the-art models in subjective and objective evaluation.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论