# SECodec

<details>
<summary>基本信息</summary>

- 标题: "SECodec: Structural Entropy-based Compressive Speech Representation Codec for Speech Language Models"
- 作者:
  - 01 Linqin Wang,
  - 02 Yaping Liu,
  - 03 Zhengtao Yu,
  - 04 Shengxiang Gao,
  - 05 Cunli Mao,
  - 06 Yuxin Huang,
  - 07 Wenjun Wang,
  - 08 Ling Dong
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.00018)
  - [Publication]() AAAI2025
  - [Github](https://github.com/wlq2019/SECodec)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2501.00018v1__SECodec__Structural_Entropy-based_Compressive_Speech_Representation_Codec_for_Speech_Language_Models.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

With the rapid advancement of large language models (LLMs), discrete speech representations have become crucial for integrating speech into LLMs.
Existing methods for speech representation discretization rely on a predefined codebook size and Euclidean distance-based quantization.
However,
1) the size of codebook is a critical parameter that affects both codec performance and downstream task training efficiency.
2) The Euclidean distance-based quantization may lead to audio distortion when the size of the codebook is controlled within a reasonable range.

In fact, in the field of information compression, structural information and entropy guidance are crucial, but previous methods have largely overlooked these factors.
Therefore, we address the above issues from an information-theoretic perspective, we present SECodec, a novel speech representation codec based on structural entropy (SE) for building speech language models.
Specifically, we first model speech as a graph, clustering the speech features nodes within the graph and extracting the corresponding codebook by hierarchically and disentangledly minimizing 2D SE.
Then, to address the issue of audio distortion, we propose a new quantization method.
This method still adheres to the 2D SE minimization principle, adaptively selecting the most suitable token corresponding to the cluster for each incoming original speech node.
Furthermore, we develop a Structural Entropy-based Speech Language Model (SESLM) that leverages SECodec.
Experimental results demonstrate that SECodec performs comparably to EnCodec in speech reconstruction, and SESLM surpasses VALL-E in zero-shot text-to-speech tasks.
Code, demo speeches, speech feature graph, SE codebook, and models are available at [Github](https://github.com/wlq2019/SECodec).

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论