# SECodec

<details>
<summary>基本信息</summary>

- 标题: "SECodec: Structural Entropy-based Compressive Speech Representation Codec for Speech Language Models"
- 作者:
  - 01 Linqin Wang,
  - 02 Yaping Liu,
  - 03 Zhengtao Yu,
  - 04 Shengxiang Gao,
  - 05 Cunli Mao,
  - 06 Yuxin Huang,
  - 07 Wenjun Wang,
  - 08 Ling Dong
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.00018)
  - [Publication]() AAAI2025
  - [Github](https://github.com/wlq2019/SECodec)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2501.00018v1__SECodec__Structural_Entropy-based_Compressive_Speech_Representation_Codec_for_Speech_Language_Models.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

With the rapid advancement of large language models (LLMs), discrete speech representations have become crucial for integrating speech into LLMs.
Existing methods for speech representation discretization rely on a predefined codebook size and Euclidean distance-based quantization.
However,
1) the size of codebook is a critical parameter that affects both codec performance and downstream task training efficiency.
2) The Euclidean distance-based quantization may lead to audio distortion when the size of the codebook is controlled within a reasonable range.

In fact, in the field of information compression, structural information and entropy guidance are crucial, but previous methods have largely overlooked these factors.
Therefore, we address the above issues from an information-theoretic perspective, we present SECodec, a novel speech representation codec based on structural entropy (SE) for building speech language models.
Specifically, we first model speech as a graph, clustering the speech features nodes within the graph and extracting the corresponding codebook by hierarchically and disentangledly minimizing 2D SE.
Then, to address the issue of audio distortion, we propose a new quantization method.
This method still adheres to the 2D SE minimization principle, adaptively selecting the most suitable token corresponding to the cluster for each incoming original speech node.
Furthermore, we develop a Structural Entropy-based Speech Language Model (SESLM) that leverages SECodec.
Experimental results demonstrate that SECodec performs comparably to EnCodec in speech reconstruction, and SESLM surpasses VALL-E in zero-shot text-to-speech tasks.
Code, demo speeches, speech feature graph, SE codebook, and models are available at [Github](https://github.com/wlq2019/SECodec).

## 1·Introduction: 引言

Large language models (LLMs)~\cite{achiam2023gpt,touvron2023llama} have exhibited exceptional capabilities in a wide range of natural language processing tasks.
This success has spurred extensive research efforts in developing speech language models~\cite{zhang2023speechtokenizer,huang2023repcodec,borsos2023audiolm}, leading to notable advancements in numerous speech processing applications~\cite{wang2023neural,tu2024smart,rubenstein2023audiopalm,dong2023polyvoice,tu2023view}.
To bridge the gap between continuous speech and token-based language models, a crucial method called speech discretization is employed.
This process transforms an audio signal into a finite set of tokens.
By converting speech into discrete tokens, language models are able to predict future semantic content and generate coherent and realistic speech with long-term consistency~\cite{nguyen2022discrete,tu20222}.

Current discrete speech representations for
speech language models can be categorized into three types: semantic
tokens, acoustic tokens, and hybrid/unified tokens~\cite{borsos2023audiolm,zhang2023speechtokenizer}. 1) Semantic tokens~\cite{hsu2021hubert,baevski2020wav2vec,chung2021w2v} are typically generated from self-supervised pre-trained models using masked language modeling as the training objective, which are derived through $k$-means clustering on representations from a specific intermediate layer, resulting in sequences with a one-dimensional structure.
Speech language models that use semantic tokens~\cite{lakhotia2021generative,zhang2023speechgpt,hassid2024textually} can be externally connected to a vocoder for speech synthesis.
While these models effectively capture semantically accurate content, the resulting speech generation often suffers from poor quality and a loss of acoustic details.
2) Acoustic tokens~\cite{zeghidour2021soundstream,defossez2022high,yang2023hifi,du2024funcodec} are extracted from neural audio codecs, which use reconstruction as the training objective.
By employing residual vector quantization (RVQ)~\cite{gray1984vector,vasuki2006review} with hierarchical quantizers for discretization, acoustic tokens are represented as matrices with two dimensions: timesteps and quantizers.
VALL-E~\cite{wang2023neural} is a representative model of speech language models that utilize acoustic tokens.
Despite achieving impressive zero-shot text-to-speech (TTS) capabilities, it still faces issues such as inaccurate content, stemming from the complex information contained within acoustic tokens.
3) Hybrid or unified tokens~\cite{borsos2023audiolm,zhang2023speechtokenizer} employ different strategies to combine semantic tokens and acoustic tokens.
Hybrid tokens adopt a hierarchical approach, encompassing both semantic token language models and acoustic token language models, to capture content information and acoustic details, respectively~\cite{dong2023polyvoice,borsos2023audiolm,rubenstein2023audiopalm}.
Recently, unified tokens, exemplified by SpeechTokenizer~\cite{zhang2023speechtokenizer}, have emerged.
These tokens distill semantic information into acoustic tokens, effectively unifying semantic and acoustic representations.
SpeechTokenizer has achieved superior results in downstream tasks such as speech synthesis.
The ideal speech representation for speech language models should meet two key characteristics: i) Effective preservation of speech information; ii) Sufficient compressiveness for efficient training of speech language models.
However, i) existing speech discretization methods rely on $k$-means to initialize the codebook space, the size of codebook is a critical parameter that significantly impacts the performance of the codec and the training efficiency of downstream tasks, yet its size is typically determined through empirical judgment. ii) Additionally, when attempting to control the size of the codebook within a reasonable range, the quantization process, which relies on Euclidean distance, may lead to substantial differences between codebook's vector and original vector, resulting in audio distortion.
These issues result in a loss of information and produce overly long tokens that are difficult to train, thereby impairing overall performance.

In this work, we address the aforementioned issues from an information-theoretic perspective, drawing inspiration from structural entropy (SE)~\cite{li2016structural,cao2024hierarchical}, a metric that assesses the amount of information contained in a graph~\cite{yang2024hierarchical,zeng2024scalable,yang2024adaptive,yang2024sebot,cao2024multi,peng2024unsupervised,zou2024multispans}.
We present SECodec, a novel speech representation codec tokenizer based on structural entropy, which can automatically determine the appropriate codebook size and integrate structural information into the quantization process.
Experiments demonstrate that these approaches effectively mitigate the information loss problem prevalent in existing speech discretization methods.

Our main contributions are:
-  We model the speech representation codec from an information-theoretic perspective.
Compared to previous methods that use $k$-means, the proposed SECodec, by introducing structural information and entropy guidance, learns a more compressive and informative codebook without requiring a predetermined codebook size.
To the best of our knowledge, we are the first to apply structural entropy (SE) minimization for a speech representation codec.
- To address the issue of audio distortion when controlling codebook size, we propose a new quantization method that iteratively selects appropriate clusters for the added original speech features using a SE heuristic function.
This approach significantly enhances the quality of information in the speech tokens.
- Extensive experimental results demonstrate that SECodec performs comparably to EnCodec in speech reconstruction, while SESLM surpasses VALL-E in zero-shot text-to-speech tasks on a multi-speaker benchmark dataset.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论