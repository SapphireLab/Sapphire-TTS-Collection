# VALL-E R

<details>
<summary>基本信息</summary>

- 标题: "VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment"
- 作者:
  - 01 Bing Han
  - 02 Long Zhou (周龙)
  - 03 Shujie Liu (刘树杰)
  - 04 Sanyuan Chen (陈三元)
  - 05 Lingwei Meng
  - 06 Yanming Qian
  - 07 Yanqing Liu
  - 08 Sheng Zhao (赵胜)
  - 09 Jinyu Li (李劲宇)
  - 10 Furu Wei (韦福如)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.07855)
  - [Publication]()
  - [Github]()
  - [Demo](https://aka.ms/valler)
- 文件:
  - [ArXiv](../SpeechLM_TTS/PDF/2024.06.12_2406.07855v1__VALL-E_R__Robust_and_Efficient_Zero-Shot_Text-to-Speech_Synthesis_via_Monotonic_Alignment.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis.
However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition.
In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression.
To address these issues, we propose ***VALL-E R***, a robust and efficient zero-shot TTS system, building upon the foundation of VALL-E.
Specifically, we introduce a phoneme monotonic alignment strategy to strengthen the connection between phonemes and acoustic sequence, ensuring a more precise alignment by constraining the acoustic tokens to match their associated phonemes.
Furthermore, we employ a codec-merging approach to downsample the discrete codes in shallow quantization layer, thereby accelerating the decoding speed while preserving the high quality of speech output.
Benefiting from these strategies, ***VALL-E R*** obtains controllability over phonemes and demonstrates its strong robustness by approaching the WER of ground truth.
In addition, it requires fewer autoregressive steps, with over 60% time reduction during inference.
This research has the potential to be applied to meaningful projects, including the creation of speech for those affected by aphasia.
Audio samples will be available at: [this https URL](https://aka.ms/valler).

</td><td>

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

In recent years, Large Language Models (LLMs) have achieved impressive performance in various fields including natural language processing (NLP) tasks[^01], [^02] and computer vision (CV) tasks[^03], [^04].
In audio domain, with the emergence of neural audio codecs[^05], [^06], they can tokenize audio with high fidelity, making language modeling of audio modalities feasible[^07].
For zero-shot text-to-speech (TTS) task, traditional methods often rely on speaker adaptation[^08], [^09] or speaker encoding[^10], [^11], requiring additional fine-tuning, complex pre-designed features, or heavy structure engineering.
And there is always a significant performance decline when facing unseen speakers.
Leveraging the in-context learning capabilities of LLM[^12], several works (**VALL-E**[^13], **SPEAR-TTS**[^14], [^15], [^16], [^17]) attempt to apply LLM to model discrete audio tokens from neural codecs, and have shown amazing performance in zero-shot TTS, which can clone a timbre and prosody with just a few seconds of audio prompt.

</td><td>

</td></tr>
<tr><td>

Although the TTS model based on LLM has achieved impressive results in maintaining naturalness and timbre, it still suffers from issues including robustness and efficiency when applied: (1) TTS task is a monotonic sequence to sequence task, but the decoder-only transformer structure in LLM only captures the monotonic association between phoneme sequences and audio through self-attention mechanism, which can easily lead to potential attention degradation problems when facing complex or long sequences, resulting in inaccurate generation results.
(2) Neural codecs have a high sampling rate (75Hz in Encodec[^06]), and autoregressive can better model temporal information, bringing better results while also incurring significant computational costs in the inference stage.
Recently, there have been some attempts to introduce phoneme information or transducer[^18], [^19] to help solve robustness problems, but this has also brought about issues such as high inference consumption[^20] and slow training speed[^21].
Some other works attempt to improve efficiency by changing the pattern of token organization, but the effect is not very obvious[^07], [^13], [^22], [^23].

</td><td>

</td></tr>
<tr><td>

In this paper, we introduce ***VALL-E R***, a robust and efficient neural codec language model for zero-shot TTS, aiming to alleviate the issues of robustness and inference efficiency encountered by its predecessor **VALL-E**[^13].
As shown in Figure~\ref{fig:overview}, ***VALL-E R*** incorporates phoneme prediction into acoustic token prediction during the training process and utilize monotonic alignment strategy to guide the inference, with almost no computational burden introduced throughout the entire process.
Moreover, ***VALL-E R*** employs a novel codec-merging method that enables the use of condensed codec codes within its autoregressive model, greatly improving the inference speed via reducing the number of autoregressive steps in the inference stage.
In summary, our proposed ***VALL-E R*** provides the following key contributions:

- ***VALL-E R*** employs a codec-merging approach, which can reduce the sample rate of codes in first layer without affecting the generated speech quality and retraining the codec model, thereby significantly improving the inference speed of neural codec LM-based TTS model.
- ***VALL-E R*** introduces phoneme monotonic alignment strategy, which can enhance the alignment between phonemes and acoustic sequence to improve the robustness of decoder-only Transformer TTS.
- ***VALL-E R*** exhibits stronger controllability in prosody due to its phoneme-based input mechanism in inference.
Experimental results demonstrate that ***VALL-E R*** can clone timbre while adjusting pronunciation prosody, achieving the goal of voice conversion.

</td><td>

</td></tr>
<tr><td>

***VALL-E R*** is a purely reseach project with no immediate plans to transition it into a commercial product or to broaden public access.
This technology has the potential to create synthesized voices that reflect individual identities, lending itself to a variety of applications such as education, entertainment, journalism, personal content creation, assistive technologies, automated voice systems, language translation, and chatbots.
The ability of ***VALL-E R*** to replicate a specific person's voice convincingly varies based on the length and clarity of the provided speech sample, any present background noise, and other influencing elements.
There are inherent risks associated with the technology, including the possibility of misusing it to mimic voice recognition systems or impersonate individuals.
Our experiments were conducted with the presumption of consent from the individuals whose voices were being reproduced.
In any future applications involving speakers not initially included in the model, measures must be taken to ensure consent for the use of their voice, along with the implementation of tools for detecting synthetic speech.
Should there be any concerns about ***VALL-E R*** being utilized inappropriately, illegally, or in a manner that infringes upon personal or others' rights, reports can be submitted through the designated Report Abuse Portal.

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

### Neural Codec Language Modeling

<table><tr><td width="50%">

Recently, language models have increasingly garnered attention within both the academic and industrial sectors, demonstrating their formidable capabilities in feature modeling and problem-solving across domains such as text[^02], images[^24], and videos[^25].
In the audio domain,
AudioLM[^07] has been trained on discrete audio tokens, accomplishing audio synthesis tasks through hierarchical prediction of these tokens, and text conditional control can also be achieved through models such as CLAP[^26].
For zero-shot TTS, **VALL-E**[^13] pioneered the integration of discrete codecs and approached TTS as a conditional language modeling task, enabling voice generation in unseen speakers from a mere short speech sample, thereby underscoring the powerful representational capacity of language models in TTS.
Following it, numerous improvements have been made.
VALL-E X[^27] expanded upon multilingual capabilities, supporting a broader range of tasks.
VioLA [^28] further integrated various cross-modal tasks involving speech and text into a single codec-based Transformer decoder-only model.
In terms of robustness, EALL-V[^20] enhanced robustness by implicitly establishing a connection between text and audio during the synthesis process through text insertion.
VALL-T[^21] improved robustness by combining with a Transducer and introducing text control by adjusting relative position encoding.
Meanwhile, CLaM-TTS[^16] leveraged a pre-trained language model and probabilistic discrete learning to enhance the expressiveness of synthesized speech, marking significant advancements in the field.

Concurrently with our work, RALL-E [^29] attempts to enhance  the robustness of VALL-E through the adoption of chain-of-thought (CoT) prompting techniques.
Although the above works have effectively improved the performance of decoder-only transformer based text-to-speech system, no one has yet attempted to improve the efficiency of autoregression, which is currently a pain point.

</td><td>

</td></tr></table>

### Monotonic Alignment

<table><tr><td width="50%">

In autoregressive acoustic modeling, issues such as word skipping, repetition, and attention collapse often occur due to inaccuracies in the attention alignments learned within the encoder-decoder framework.
To alleviate this problem, considering some properties of the alignments between text and waveform sequence, applying monotonic mechanism to enhance attention has demonstrated to be particularly effective to strictly preserve monotonicity and locality[^30].
Considering the alignments between text and speech are depending on their positions, numerous TTS models have incorporated location-based attention mechanisms to exploit the positional information for more accurate alignment[^31], [^32], [^33].
For monotonic attention, it leverages the prior that the alignments between text and speech are monotonic[^30], [^34].
In each decoding step, the attention alignment position moves forward at most one step, so there is no situation where any input unit is skipped or repeated.
While such methods have enhanced stability by introducing linear alignment, the constraints of hard attention significantly hinders the encapsulation of global contextual information, where the model only focuses on one token at a time.
Therefore, models in[^35], [^36] restrict the attention on the source sequence into a sliding window, replacing hard attention.
Furthermore, Chen et al.[^37] introduce an innovative approach to penalize off-diagonal attention weights by constructing a band mask that promotes the concentration of attention weights within a diagonal band, thus enhancing alignment fidelity while maintaining contextual awareness.
Although these methods have effectively improved the robustness of autoregressive TTS systems, they are all based on the encoder-decoder architecture and are not compatible with the current popular decoder-only architecture.

</td><td>

</td></tr></table>


## 3·Methodology: 方法

<table><tr><td width="50%">

In this study, we propose a robust and efficient zero-shot TTS system named ***VALL-E R***.
We first introduce the codec-merging approach in Section.\ref{sec:merge_codec} which can improve inference speed without retraining the codec, and then illustrate the monotonic alignment strategy in decoder-only neural codec LM in Section.\ref{sec:monotonic_alignment}.

</td><td>

</td></tr></table>

### Codec-Merging Approach

<table><tr><td width="50%">


Building upon the foundational work of Encodec[^06], we introduce the concept of a merged codec.
This innovative approach enables the downsampling of discrete codes across various layers through alterations in the inference forward process.
Remarkably, this is achieved without necessitating any retraining or fine-tuning of the model, presenting a significant advancement in the efficient manipulation of audio data representations.


The visual description of the proposed codec can be seen in [Figure.02](#Fig.02).

The overall architecture of the model remains consistent with Encodec, which comprises of three components:
(1) a convolution-based encoder that maps waveform data $x^{1\times L}$ into a sequence of latent representations $z^{F\times T}$, where $F$ is channel and $T$ is length of extracted codes;
(2) a decoder that reconstructs the data $\hat{x}^{1\times L}$ from a sequence of the quantized latent representations $\hat{z}^{F\times T}$;
(3) an 8-layer residual vector quantizer (RVQ) module, which can convert the continual latent vector $z^{F\times T}$ into the discrete code representation $C^{8\times T}$ iteratively.
The main difference is that our merged codec inserts a codec-merging module before the vector quantizer module to downsample the representation $z^{F\times T}$.


Assuming the merged rate of layer $d$ is $m_d$, $r_d^{F\times T}$ represents the residual input of layer $d$.
Then the codec-merging module consists of two steps: the first one is downsampling the residual input $r_d^{F\times T}$ to $r_{md}^{F\times (T/m_d)}$ by average pooling, and then upsampling $r_{md}$ to its original length through repeat operation.

Next, the residual representation processed by the Merge module will be feed into the following VQ layer to quantized into discrete code $C_d^{1 \times T}$ through nearest-neighbour lookup over the codebook embeddings.
Through the merge module, we reduced the resolution of $C_d^{1 \times T}$ by ensuring consistent code of consecutive $m_d$ frames.

</td><td>

</td></tr></table>

### Neural Codec LM with Monotonic Alignment

<table><tr><td width="50%">

Previously, monotonic strategies were only applicable to encoder-decoder structures.
To address the robustness issue in the decoder-only transformer based TTS, we integrated phonemes prediction into the training of neural codec LM and design the monotonic alignment stratege during the inference process.
The overview is illustrated in Figure.~\ref{fig:model} and details of training and inference are discussed in the subsequent sections.

</td><td>

</td></tr>
<tr><td>

#### Training with Phoneme Prediction

To achieve a good trade-off between speech quality and inference speed, our ***VALL-E R*** includes two models: autoregressive (AR) and non-autoregressive (NAR), which is following **VALL-E**[^13].
Specifically, given a training data pair $\{\mathbf{s}, \mathbf{p}\}$, where $\mathbf{s}$ is speech sample, and $\mathbf{p}=\{p_1, p_2, \dots, p_L\}$ is its corresponding phoneme transcription.
Then, the codec-merging model introduced in Section.~\ref{sec:merge_codec} is utilized to compress speech waveform $\mathbf{s}$ into discrete acoustic tokens $\mathbf{A}$ with 8 quantizers, formulated as: $\mathrm{MergeCodec}(\mathbf{x})=\mathbf{A}^{8\times T}=\{\mathbf{a}^1, \dots, \mathbf{a}^8\}$, where $T$ is length of discrete codes and $\mathbf{a}^i=\{a_1, \dots, a_T\}$ represent the tokens in the $i$-th layer.
Because the training of ***VALL-E R*** requires the aligned phonemes and acoustic tokens, aligner tool is adopted here to align $\mathbf{p}$ with acoustic tokens $\mathbf{A}$, denoted as $\hat{\mathbf{p}}_{1:T}=\{\hat{p_1}, \hat{p_2}, \dots, \hat{p_L}\}$ where $\hat{p_i}$ contains $N_i$ repetitions of  $p_i$ and $\sum_{i=1}^{L}N_i=T$.

For AR stage, to enhance the connection between phoneme and acoustic sequence, we build a neural codec LM $\theta_{AR}$ to model the discrete acoustic tokens $\mathbf{a}^1_{1:T}$ from the first quantizer of codec-merging model with phoneme prediction.
As shown in Figure.\ref{fig:model}, it's conditioned on the phoneme sequence $\mathbf{p}$ to generate both the acoustic token $\mathbf{a}^1_{1:T}$ and aligned phonemes $\hat{p}_{1:T}$ simultaneously, formulated as maximizing the following probability:
$$
p(\mathbf{a}^1_{1:T}, \hat{\mathbf{p}}_{1:T} | \mathbf{p}; \theta_{AR}) = \prod_{t=1}^{T} p(a_t, p_t|a_{1:t-1},\hat{\mathbf{p}}_{1:t-1},\mathbf{p}_{1:L};\theta_{AR})
$$

In the second stage, we train a NAR LM $\theta_{NAR} $to generate the acoustic tokens from 2$nd$ to 8-$th$ layer quantizers iteratively.
It's conditioned on phoneme sequences $\mathbf{p}$, the previous few layers of generated acoustic tokens $\mathbf{a}^{1:n}$ and phonemes alignment $l_{1:T}$ to predict next layer of acoustic token $\mathbf{a}^{n+1}$, formulated as maximizing:
$$
p(\mathbf{a}^{2:8}_{1:T}|\hat{\mathbf{p}}_{1:T}, \mathbf{p}_{1:L};\theta_{NAR}) = \prod_{n=2}^{8} p(\mathbf{a}^{n}_{1:T}|\hat{\mathbf{p}}_{1:T}, \mathbf{a}^{1:n-1}_{1:T}, \mathbf{p}_{1:L};\theta_{NAR})
$$

We also share the parameters of the acoustic embedding layer and the output prediction layer, which means the weights of the $j$-th prediction layer are the same as the $(j + 1)$-th acoustic embedding layer.

</td><td>

</td></tr>
<tr><td>

#### Inference with Monotonic Alignment

After training with teacher forcing, the neural codec LM we obtained is surprising in context learning ability.
With just a 3 seconds acoustic prompt, we can replicate the timbre and prosody of unseen speakers without any fine-tuning or adaptation methods (**VALL-E**[^13]).
Take Figure.~\ref{fig:model} as an example of autoregressive inference, we convert the text input into phonemes sequence $\mathbf{p}^t=\{p_1^t, \dots, p_3^t\}$ and concatenate it with transcription phoneme of acoustic prompt $\mathbf{p}^p=\{p_1^p, p_2^p\}$ to form phoneme tokens for model.
In the following, we will use acoustic tokens $\mathbf{a}=\{a_1^p, \dots, a_n^t\}$ and corresponding aligned phoneme tokens $\mathbf{p}^a=\{p_1^p, \dots, p_n^t\}$ as condition to predict the audio and phoneme of next step autoregressively.

In order to improve the robustness of the model, we adopted the Monotonic Alignment (MA) strategy during the inference process, which means that the predicted phone needs to be aligned with the text input $\mathbf{p}^t$ and can only maintain the current or jump to the next phoneme.
Specifically, at each step $i$, the current input phoneme token is $p_j^t$.
The output representation is $\mathbf{e_i}$, and corresponding probability of phoneme $p_j^t$ and $p_{j+1}^t$ is denoted as $e_{i,j}$ and $e_{i,j+1}=1-e_{i,j}$ respectively.
Then, the phoneme pointer would decide to keep $p_j^t$ unmoved or jump to $p_{j+1}^t$ by sampling:
$$
  z_{i,j} \sim \text{Bernoulli}(\dfrac{1}{1+\exp(e_{i,j})})
$$

The sampled phoneme will be used as input for the $i+1$ step and this AR process will repeat until all phonemes have been covered.
To increase the diversity of synthesis process, sampling based decoding is used for acoustic tokens prediction.
And for NAR model, we adopt greedy search method to generate the $(j+1)$-th layer acoustic tokens based on the self-aligned phoneme sequence and previous few layers of generated acoustic tokens.

Finally, decoder of neural codec is adopted here to convert generated discrete codes into waveform.
In summary, our inference process has the following three characteristics[^38] by using MA strategy:

- Locality: Each phoneme token can correspond to one or several consecutive acoustic tokens, ensuring a flexible and accurate mapping.
Conversely, each acoustic token is uniquely aligned to a single phoneme token.
This one-to-one alignment strategy effectively prevents issues such as misreading, enhancing the clarity and stability of the model.
- Monotonicity: If phoneme $p_a$ follows $p_b$ in the phonemes sequence, the corresponding acoustic tokens for $p_a$ are also positioned sequentially after those for $p_b$.
This sequential alignment is critical as it inherently prevents the repetition of words.
- Completeness: It is mandated that each phoneme token is represented by at least one corresponding acoustic token.
This coverage requirement is essential for preventing the word skipping.

</td><td>

</td></tr>
<tr><td>

#### Inference for Controlling Prosody

In the inference process, benefiting from the powerful in context learning ability of the LM, our proposed ***VALL-E R*** can automatically clone both timbre and prosody of the speaker in the prompt by predicting the acoustic and phoneme autoregressively.
Because ***VALL-E R*** explicitly models phoneme, it has strong control over prosody: when we use preset phoneme sequences to replace the self-predicted phoneme sequences in the inference process, we can use the preset prosody to generate speech, thereby achieving the effect of controlling prosody and timbre separately.
It can also be regarded as a voice conversion task, whose goal is to make the timbre of target speech sound like that of prompt speech without changing the linguistic information and prosody of source speech.

</td><td>

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

### Dataset

We use LibriSpeech[^39] dataset in our experiments, which contains 960 hours of multi-speaker transcribed English speech data.
Because VALL-R requires phoneme alignment information during training and inference processing, Montreal Forced Aligner (MFA)[^40] is utilized to align transcript and audio.
For the neural codecs, we use Encodec[^06], which applies Residual Vector Quantization (RVQ) to compress 24khz waveform into 8-layers discrete codes at 75hz.
To synthesize higher quality audio, we adopt Vocos[^41] as vocoder which is aligned with Encodec.

</td><td>

</td></tr>
<tr><td>

### Evaluation Metrics

**Objective Metrics**

We evaluated robustness, speaker similarity, sound quality and efficiency with several objective metrics.
For robustness and intelligibility, we measure them by computing the word error rate (WER) of the synthesized transcription from generated speech concerning the input text, with [Conformer-Transducer ASR model [URL]](https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge)[^42].

For speaker similarity, we employ [**WavLM-TDNN** model [URL]](https://huggingface.co/microsoft/wavlm-base-plus-sv) [^43], [^44] which extract the speaker vector representing the voice attribute of the speakers.
We measure the speaker similarity by computing the cosine similarity between the generated and the prompt speeches, which is denoted as Spk-Sim.

To evaluate the sound quality of the reconstruction audio, we adopt the metrics from speech enhancement fields, such as the Perceptual Evaluation of Speech Quality (PESQ) and Short-Time Objective Intelligibility (STOI) to evaluate the performance.

**Subjective Metrics**

We measure the quality of the generated speech from human evaluations via three types of Mean Opinion Score (MOS):
(1) Quality MOS (QMOS) for the speech quality assessment,
(2) Similarity MOS (SMOS) to measure speaker similarity between the prompt and the generated speech,
(3) Comparative MOS (CMOS) to assess the overall naturalness and quality of the synthesized speech against the baseline, which is ranging from -1 to 1.


</td><td>

</td></tr>
<tr><td>

### Baseline

To demonstrate the robustness and inference efficiency of our system, we selected multiple classic baseline systems for comparison.
For robustness, besides its predecessor **VALL-E**[^13], we mainly compare two recent works which are proposed to improve robustness.
One is VALL-T, it adopts transducer to enhance text controllability via adjusting relative position encoding.
And another one is ELLA-V[^20], which interleaves sequences of acoustic and phoneme tokens to achieve the fine-grained control at the phoneme level.
To ensure fair comparison, all the systems including ***VALL-E R*** and baselines, are trained with the same dataset (LibriSpeech) and architecture (stacked 12-layers Transformer).
As for efficiency, since the inference time of neural codec LM based TTS system mainly depends on the number of autoregressive steps, we selected representative methods including AudioLM[^07], **VALL-E**[^13], ELLA-V[^20], RALL-E[^29], MusicGen[^22], and compared the impact of different token arrangement methods on efficiency.

</td><td>

</td></tr>
<tr><td>

### Training Configuration

During the training, we trained the autoregressive (AR) model in the teacher-forcing manner and selected the first 3 seconds as the prompt to optimize the non-autoregressive (NAR) model.

Regarding the model architecture, both our AR and NAR models are structured with a 12-layer Transformer framework.
Key parameters including the number of attention heads, embedding dimension, hidden state dimension, feed-forward network dimension, and dropout rate are configured to 16, 1024, 1024, 4096, and $0.1$, respectively.
These models are trained on 8 NVIDIA V100 16GB GPUs across 400k steps utilizing the AdamW optimizer.
The training protocol initiates with a learning rate warm-up phase over the first 32k updates, reaching a peak at $5 \times 10^{-4}$.
Subsequently, the learning rate undergoes a linear decay coupled with a weight decay factor of $0.01$.

</td><td>

</td></tr></table>

## 5·Results: 结果

### Zero-shot Text-to-Speech tasks

<table><tr><td width="50%">

We measure the performances of the proposed ***VALL-E R*** under two different tasks following the configuration used in **VALL-E**[^13], [^16] on LibriSpeech test-clean ranging from 4 to 10 seconds:
1) Continuation: we use the first 3 seconds of the utterance and the corresponding text transcription as prompt respectively, and ask model to synthesize the subsequent portion of the speech seamlessly.
2) Cross-sentence: Given the target text, a 3-second prompted speech, and its corresponding transcript, which contains a different sentence from the target text, the task is to synthesize the speech with target text following the style of the provided speech prompt.

</td><td>

</td></tr>
<tr><td>

**Objective Evaluation**

We list the objective evaluation results of continuation and cross-sentence in Table~\ref{tab:results_overview}.

Under the continuation configuration, although ELLA-V achieves a certain improvement over the VALL-E via connecting phoneme tokens with their corresponding acoustic tokens, our proposed ***VALL-E R*** employs a monotonic alignment strategy for more precise control and approaches the performance of Encodec in terms of Word Error Rate (WER), which can be considered as upper bound.

In the cross-sentence task, due to the discontinuity of content, which mismatches the continuation training manner, all systems experience varying degrees of degradation in WER.
Among these, ELLA-V, suffers a more significant impact because of its strategies such as local advance.
But ***VALL-E R*** is still the best in WER, reflecting the robustness and intelligibility of our model.
In terms of speaker similarity (Spk-Sim), ***VALL-E R*** is comparable to VALL-E and ELLA-V, indicating that although our method explicitly encodes text contents, it does not affect the performance of timbre cloning.


</td><td>

</td></tr>
<tr><td>

**Subjective Evaluation**

Table~\ref{tab:res_mos} showcases the results of subjective audio evaluations.
***VALL-E R*** outperforms the baseline (VALL-E), in quality and intelligibility, as indicated by QMOS.
According to SMOS measurements, our compliance with prompt audio is comparable to baseline.
The comparative scores (CMOS) highlight ***VALL-E R***’s proximity to the Ground Truth regarding naturalness, clarity, and comprehensibility.
Overall, ***VALL-E R***’s generated speech exceed the baseline in naturalness, quality and intelligibility, which is consistent with the results of objective evaluation.

</td><td>

</td></tr></table>

### Controllability of VALL-E R

<table><tr><td width="50%">

Benefiting from the explicit modeling of phonetic sequences, ***VALL-E R*** is capable of precisely controlling the duration of each phoneme's articulation, thereby achieving timbre conversion while preserving prosody.
In this experiment, we randomly selected an audio sample from the LibriSpeech test set to serve as acoustic prompt, providing the reference timbre.
Another audio sample was randomly chosen, with its phonetic transcription serving as both the textual prompt and prosody reference.
We conducted a comparison among multiple baseline systems, calculating the Mel Cepstral Distortion with Dynamic Time Warping and weighted Speech Length (MCD-DTW-SL)[^45] between the generated audio and the target audio to evaluate the similarity in speech length and alignment quality as a measure of prosody.
Results are presented in Table~\ref{tab:mcd}, neither VALL-E nor ELLA-V is capable of precise duration control, thus limiting their speech generation capabilities to the prosody of the given acoustic prompt.
For ***VALL-E R***-Prosody, we can employ the aligned phoneme sequence from the target prosody audio as input for the neural codec language model.
This enables the preservation of target prosody while cloning the timbre from the acoustic prompt.
According to the results, this approach also achieved the lowest MCD-DTW-SL, indicating a superior preservation of the prosody.
It also indicates that our ***VALL-E R*** has better controllability.

</td><td>

</td></tr></table>

### Efficiency Comparison

<table><tr><td width="50%">

We conducted a comparison of the inference times required to generate 10 seconds of speech among several neural codec language model inference patterns, detailing the time needed for AR and NAR module in Table~\ref{tab:efficiency}.
The first row is the flatten pattern in AudioLM, which generates all 8 layers of discrete audio codecs autoregressively, necessitating 750*8 steps to generate 10 seconds of audio, resulting in extremely slow speeds.
VALL-E adopts a coarse-to-fine pattern, generating the first layer autoregressively, then employing non-autoregressive module for the codec of subsequent 7 layers, significantly reducing inference time.
ELLA-V, building upon VALL-E, introduces a large number of phoneme tokens (around 2*105 in 10s speech) into the autoregressive process.
While this enhances robustness, it considerably slows down inference efficiency.
Similarly, RALL-E introduces control information for duration and pitch in autoregression through Chain-of-Thought prompting, which also increases inference time.
MusicGen, on the other hand, is based on a delay pattern.
Despite discarding the non-autoregressive module, its speed does not show a significant improvement over VALL-E.

By employing our proposed codec-merging method, which reduces the bitrates at the first layer of RVQ from 75hz to 37.5hz, we can effectively decrease the number of autoregressive steps required, thereby significantly enhancing inference speed.
Additionally, due to the exponential complexity of the Transformer's self-attention mechanism, halving the sampling rate results in a speed increase of more than two times.
This method also outpaces both the flow-matching based NAR model VoiceBox and the Mel-VAE based CLaM-TTS in terms of speed.

</td><td>

</td></tr></table>

### Analysis and Discussion

<table><tr><td width="50%">

**Effect of Merged Codec**

The codec-merging strategy results in a reduction of the model's bitrate, thus we have explored the impact of different configurations of the merged codec on audio quality, presenting the results in Table~\ref{tab:results_codec}.
These experiments involved reconstruction tasks performed on the LibriSpeech test set, where the quality of audio was assessed by comparing the ground truth with the reconstructed audio using metrics PESQ and STOI.

%Analysis of the results presented in Table~\ref{tab:results_codec}
Results reveal that when we only reduce the sampling rate by half for the first layer codec, the audio quality and intelligibility are almost unaffected.
However, as we expand this reduction to 4 layers and 8 layers, a significant decrease in performance is observed, which does not enhance the inference speed.
Thus, downsampling only the first layer achieve the trade-off between quality and efficiency.
Although all our experiments are conducted with 2x downsampling rate, we also explored the effects of increasing the downsampling to 3x and 4x.
It was found that tripling and quadrupling the downsampling rate only resulted in a slight decrease in audio quality, which underscores the potential of ***VALL-E R*** to further improve inference efficiency.

</td><td>

</td></tr>
<tr><td>

**Effect of Top-p in Inference**

In order to demonstrate the stability and robustness of our proposed ***VALL-E R***, we conducted ablation experiments during the decoding stage, attempting to analyze the impact of the Top-p in sampling on WER.
And the figure of WER changing with Top-p is shown in Figure.~\ref{fig:res}.
For VALL-E, due to its lack of control over text, reducing the Top-p parameter leads to a loss of randomness in the model.
This makes the model inclined to predict silent frames to maximize the overall probability of the generated content, resulting in an endless loop and consequently poor Word Error Rate (WER).
However, by incorporating a monotonic alignment strategy, our proposed ***VALL-E R*** becomes possible to finely control the pronunciation text.
Therefore, even when reducing Top-p, ***VALL-E R*** not only maintains diversity but also ensures the normal generation of content, exhibiting strong robustness.

</td><td>

</td></tr>
<tr><td>

**Visualization of Attention Weights**

To better demonstrate the mechanism behind the stability of our proposed ***VALL-E R*** model, we also conducted a deeper analysis through visualization.
Given a speech sample $y$ and its transcription $x$, we calculated the average attention weights among 16 heads between the speech and text prompts within the first layer of the transformer during autoregressive inference process.
And the weights are displayed in the Figure~\ref{fig:visual}.

In the visualization for VALL-E, as depicted in Figure (a), we can observe a faint bright line along the diagonal.
This indicates that during audio synthesis, VALL-E constantly needs to pay attention to which phoneme it is currently processing from the given prompt sequence.
However, as the audio lengthens, the attention becomes increasingly scattered.
This suggests that without explicit indications, the model becomes confused about which element to focus on, leading to misreadings, omissions, and repetitions.
In contrast, for our proposed ***VALL-E R***, depicted in Figure (b), there is no distinct bright line, and the attention is evenly distributed.
This demonstrates that ***VALL-E R*** introduces explicit phoneme indications, allowing the model to focus on the global information of the text without worrying about the progress of reading.
Finally, in Figure (c), we provide the alignment path of phonemes in ***VALL-E R*** during the inference process.
An uninterrupted and monotonous diagonal can be observed, which indicates that all phonemes are covered by the generated speech one by one, satisfying the characteristics of locality, monotonicity, and completeness defined in Section.~\ref{sec:inference}.

</td><td>

</td></tr>
<tr><td>

**Ablation Study**

We also conduct ablation experiments to study the effectiveness of the methods we proposed, with the results presented in Table~\ref{tab:ablation_ma}.
Initially, within ***VALL-E R***, we replaced the merged codec (first layer downsample 2x) with the tradition codec.
The results indicate that the merged codec does not adversely affect WER or Spk-sim while improving inference speed, thereby affirming the superiority of our proposed acceleration strategy.
Subsequently, upon removing the monotonic phoneme strategy, the model regresses completely to VALL-E, leading to a significant deterioration in WER performance.
This highlights the effectiveness of our proposed method in terms of robustness.

</td><td>

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

In this paper, we propose ***VALL-E R***, a robust and efficient zero-shot text-to-speech model based on neural codec language models.
We first employ codec-merging method to downsample the discrete codes to improve inference speed without affecting the sound quality.
More importantly, we incorporate phoneme prediction into the training process and utilize monotonic alignment strategy during the inference.
Experimental results demonstrate that ***VALL-E R*** can effectively model the correlation between phonemes and audio, significantly improve the robustness of speech synthesis, and endow the ability to flexibly control prosody.

</td><td>

</td></tr></table>

[^01]: achiam2023gpt
[^02]: touvron2023llama
[^03]: ramesh2021zero
[^04]: radford2021learning
[^05]: zeghidour2021soundstream
[^06]: defossez2022high
[^07]: borsos2023audiolm
[^08]: chen2018sample
[^09]: moss2020boffin
[^10]: arik2018neural
[^11]: cooper2020zero
[^12]: brown2020language
[^13]: [VALL-E](../CodecLM/2023.01.05_VALL-E.md5_VALL-E.md)
[^14]: [SPEAR-TTS](2023.02.07_SPEAR-TTS.md
[^15]: jiang2023mega
[^16]: kim2023clam
[^17]: hao2023boosting
[^18]: graves2012sequence
[^19]: gong2024advanced
[^20]: song2024ella
[^21]: du2024vall
[^22]: copet2024simple
[^23]: yu2024megabyte
[^24]: ding2021cogview
[^25]: kondratyuk2023videopoet
[^26]: zhao2023clap
[^27]: zhang2023speak
[^28]: wang2023viola
[^29]: xin2024rall
[^30]: he2019robust
[^31]: sotelo2017char2wav
[^32]: vasquez2019melnet
[^33]: battenberg2020location
[^34]: raffel2017online
[^35]: tachibana2018efficiently
[^36]: chiu2017monotonic
[^37]: chen2020multispeech
[^38]: tan2021survey
[^39]: panayotov2015librispeech
[^40]: mcauliffe2017montreal
[^41]: siuzdak2023vocos
[^42]: gulati2020conformer
[^43]: chen2022wavlm
[^44]: chen2022large
[^45]: chen2022v2c
