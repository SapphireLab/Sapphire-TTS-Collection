# ELLA-V

<details>
<summary>基本信息</summary>

- 标题: "ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering"
- 作者:
  - 01 Yakun Song
  - 02 Zhuo Chen
  - 03 Xiaofei Wang
  - 04 Ziyang Ma
  - 05 Chen Xie
- 链接:
  - [ArXiv](https://arxiv.org/abs/2401.07333)
  - [Publication]
  - [Github]
  - [Demo](https://ereboas.github.io/ELLAV/)
- 文件:
  - [ArXiv](../SpeechLM/_PDF/2401.07333v1__ELLA-V__Stable_Neural_Codec_Language_Modeling_with_Alignment-Guided_Sequence_Reordering.pdf)
  - [Publication] #TODO

</details>

## Abstract·摘要

## Abstract

<table><tr><td width="50%">

The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation.
However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy.
To alleviate these issues, we propose ***ELLA-V***, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level.
The key to ***ELLA-V*** is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens.
The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies.
The code of ***ELLA-V*** will be open-sourced after cleanups.
Audio samples are available at [URL](https://ereboas.github.io/ELLAV/).

> ***ELLA-V*** is the word "VALL-E" spelled backwards, to highlight that our model changes the sequence order of VALL-E input.
> VALL-E is not officially open source.
> We reproduced it and open-sourced it in this repository.

</td><td>

</td></tr></table>

## 1·Introduction

<table><tr><td width="50%">

Recently, deep generative AI has achieved remarkable results in various tasks, leading to the emergence of many transformative real-world applications ( **GPT-3**[^Brown2020GPT-3], [^Ramesh2022Hierarchical], **DDPM**[^Ho2020DDPM], **LDM**[^Rombach2021LDM], **AudioLM**[^Borsos2022AudioLM], **VITS**[^Kim2021VITS], [^Chiang2019Cluster-GCN]).
With the advancement of generative models, there have been rapid developments in the field of speech synthesis as well.
In particular, zero-shot TTS technology has gained increasing attention because it can synthesize high-quality target voices without the need of specified speaker's training data.
As a state-of-the-art generative model family, diffusion models ( [^Sohl-Dickstein2015Deep], **DDPM**[^Ho2020DDPM], [^Song2020Improved]) progressively add noise to the training data and then learn the reverse process to generate samples.
By leveraging diffusion models and their variants ( [^Sohl-Dickstein2015Deep], **DDPM**[^Ho2020DDPM], [^Song2020Improved], [^Song2020Score], **FM**[^Lipman2022FM]), many works have successfully applied them to the audio domain ( **Grad-TTS**[^Popov2021Grad-TTS], **ProDiff**[^Huang2022ProDiff], **Make-An-Audio**[^Huang2023Make-An-Audio], **NaturalSpeech2**[^Shen2023NaturalSpeech2]).
Another major class of generative models is language modeling based on  **Transformer**[^Vaswani2017Transformer].
[^Devlin2018BERT], **T5**[^Raffel2019T5], [^Lewis2020Bart] utilize encoder-only or encoder-decoder architectures to build masked language models so that they selectively focus on relevant segments and effectively model relationships in long sequences.
However, masked language model often requires fine-tuning to adapt to specific tasks, which can be inconvenient for practical usage and deployment.
On the other hand, AR language models use a decoder-only architecture to predict the next token in a sequence as the training objective, which has demonstrated extremely powerful few-shot and zero-shot capabilities in many generative tasks[^Brown2020Language], [^Thoppilan2022LaMDA], **PaLM**[^Chowdhery2022PaLM].
In light of this,  **VALL-E**[^Wang2023VALL-E] and subsequent works ( **SPEAR-TTS**[^Kharitonov2023SPEAR-TTS], **AudioPaLM**[^Rubenstein2023AudioPaLM], **LauraGPT**[^Du2023LauraGPT]) have successfully employed decoder-only language model for zero-shot TTS.
These approaches first quantize the speech signal into a series of discrete acoustic tokens.
Subsequently, they employ an AR language model to predict coarse-grained acoustic tokens, eliminating the necessity for explicit duration predictors or speaker encoders.
Once trained on a large-scale corpus, such as LibriLight[^Kahn2020Libri-Light], these approaches are capable of synthesizing speech with competitive fidelity and naturalness in a zero-shot manner.

</td><td>

</td></tr>
<tr><td>

While VALL-E and its variants have achieved numerous impressive milestones, they still possess certain limitations that impact practical deployment.
For instance, existing methods **VALL-E**[^Wang2023VALL-E], **SPEAR-TTS**[^Kharitonov2023SPEAR-TTS] directly concatenate phoneme tokens and acoustic tokens as a whole sequence to train language models.
In this way, the alignment between audio and phoneme sequences is completely learned through the self-attention in the transformer, making it potentially unstable as self-attention does not explicitly capture the monotonic alignment between audio and phoneme.
Additionally, the decoder-only language model architecture can lead to potential attention degradation issues[^Fu2023Decoder-Only], where the alignment quality between the target audio sequence and the source phoneme sequence deteriorates as the generated sequence increases, resulting in inaccurate or low-quality speech outputs.

</td><td>

</td></tr>
<tr><td>

Another limitation stems from the nature of AR language modeling.
Specifically, given a sequence $\mathbf{x}$, the standard AR language model factorizes the likelihood $p(\mathbf{x})$ over the dimensions of $\mathbf{x}$ via the chain rule $p(\mathbf{x})=\prod_{t=0}^{T}p(x_t|\mathbf{x}_{<t})$.
AR models predict the current tokens solely based on the historical tokens without users' control in the inference process, and sometimes generate semantic repetitions or incoherence in the generated output[^Yang2019XLNet], **GPT-3**[^Brown2020GPT-3].
In the TTS task, correspondingly, VALL-E cannot directly determine which segment of the output audio corresponds to which prompt phoneme, thus there is no trivial way to promptly detect and prevent issues occurring in the generation process.
These drawbacks can manifest as meaningless phoneme repetitions, transpositions, omissions, or even catastrophic **infinite silence**, \ie, during the process of generation, the model anomalously outputs silence or noise tokens for an extended period of time without stopping.
Specifically, [Tab.01](#tab:intro) demonstrates the word error rate (WER) and the probability of the **infinite silence** in VALL-E samples at different threshold top-$p$ for nuclear sampling[^Holtzman2019Curious].
The detailed experimental setup is described in Section [sec:exp](#sec:exp).
Notably, a shift in the decoding strategy of VALL-E from fully sampling-based to fully greedy-based leads to a marked decline in sample quality.
It should be emphasized that while sampling-based stochastic decoding strategies have advantages in terms of synthesis diversity, deterministic decoding strategies (e.g., beam search and its variants) are more suitable for cases where there is less tolerance for synthesis errors and more emphasis on fluency and coherence[^Ippolito2019Comparison].

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="tab:intro">![]()</a>

</td></tr>
<tr><td>

Tab.01: Comparison of VALL-E’s zero-shot TTS performance across various top-$p$ thresholds in nuclear sampling.

</td><td>

</td></tr>
<tr><td>

Faced with the pros and cons of the existing methods, we introduce ***ELLA-V***, a simple but effective language model approach for zero-shot TTS.
***ELLA-V*** proposes a generalized AR (GAR) language model to generate the first layer of residual vector quantizer (RVQ) codes of a neural codec model.
Then as with VALL-E, ***ELLA-V*** employs a non-autoregressive (NAR) language model to obtain codes of the other RVQs.
Our core innovation lies in 3 fold:
-  Firstly, ***ELLA-V*** inserts phone tokens into the corresponding positions of the acoustic sequence.
Unlike existing methods, Connecting phoneme tokens with their corresponding acoustic tokens can help the language model capture the alignment between phoneme and acoustic modalities in local dependencies.
-  Secondly, instead of maximizing the expected log-likelihood of the hybrid sequence under a conventional casual mask or a prefix mask like VALL-E and UniLM[^Bao2020UniLMv2], ***ELLA-V*** computes loss only on acoustic tokens and special tokens `EndOfPhone(EOP)` and `EndOfSentence(EOS)`.
This training objective not only reduces the redundant computation of cross-modal alignment in the output based on experimental results, but also provides a natural way to have fine-grained control in inference: the model predicts `EOP`, and then the user provides the next phone token.
Meanwhile, ***ELLA-V***'s GAR model always maintains awareness of the phoneme it is currently synthesizing, allowing it to promptly detect and truncate any abnormal phoneme to avoid any possible **infinite silence** issue.
-  Thirdly, we further propose an improvement to the input sequence.
We introduce **local advance**, which involves shifting the `EOP` token and the next-word phoneme token a few frames ahead.
Intuitively, the pronunciation of a phoneme, especially its ending, is not only influenced by the context in history but also by the upcoming phonemes.
By advancing these special tokens, the GAR model can better utilize local dependencies to predict the pronunciation of the current phoneme.

</td><td>

</td></tr>
<tr><td>

Experimental results, using comparable model configurations and 960 hours of speech data from  **LibriSpeech**[^Panayotov2015LibriSpeech] as a training set, demonstrate the superiority of ***ELLA-V***.
Compared to the state-of-the-art zero-shot TTS system VALL-E, ***ELLA-V*** significantly improves the accuracy of synthesized speech, and demonstrates comparable or superior speaker similarity and speech naturalness on a series of subjective and objective experiments.
***ELLA-V*** achieves a WER of 2.28\

</td><td>

</td></tr></table>

## 2·Related Work

<table><tr><td width="50%">

**Language Modeling**

Recently, language models have garnered increasing interest in both the academic and industrial communities.
Compared to models that are confined to specific tasks, language models have been proven to possess the capability to solve a wide array of tasks, shining across various domains such as text[^Brown2020Language], **PaLM**[^Chowdhery2022PaLM], [^Rae2021Scaling], [^Yu2022Scaling], images[^Alayrac2022Flamingo], [^Tsimpoukelli2021Multimodal], and videos[^Yang2022Zero-Shot], [^Wang2022Language].
In the audio domain, **AudioLM**[^Borsos2022AudioLM] trains language models on discretized audio tokens, achieving speech synthesis tasks through hierarchical prediction of these tokens.
**AudioGen**[^Kreuk2022AudioGen] employs an auto-encoding approach to extract discrete encodings of raw audio, and trains a language model conditioned on textual features for controlled audio generation.
**LM-VC**[^Wang2023LM-VC] employs three language models—a masked prefix language model, an external LM, and a prefix LM—to achieve zero-shot voice conversion.
[^Kakouros2023Investigating] investigates the role of word surprisal, extracted from language models, in influencing the prosody of speech synthesized by TTS systems.
For zero-shot TTS, **VALL-E**[^Wang2023VALL-E] approaches TTS as a conditional language modeling task rather than a continuous signal regression.
By employing discrete audio codes obtained from pre-trained neural codec, it trains a discrete audio language model, achieving improved naturalness in speech and preservation of speaker characteristics.
**VALL-E X**[^Zhang2023VALL-EX] extends VALL-E by utilizing source language speech and target language text as prompts when predicting the acoustic marker sequence of the target language speech.
This approach supports high-quality zero-shot cross-lingual voice synthesis.
These methods require only a single utterance of an unknown speaker as a prompt to generate high-quality, specified speech.

</td><td>

</td></tr>
<tr><td>

**Speech Synthesis**

Speech synthesis has long been a significant topic in the fields of artificial intelligence, natural language processing, and speech processing.
Early methods were based on Statistical Parametric Speech Synthesis (SPSS)[^Zen2009Statistical], typically involving complex components such as text analysis models, acoustic models, and vocoders (e.g., hidden Markov model(HMM)[^Yoshimura1999Simultaneous] based).
While cost-effective in terms of data, the generated speech of SPSS still exhibited noticeable differences from natural human speech.
With the advancement of modern neural networks, some work initially replaced HMMs with recurrent neural networks (RNNs) but still followed the SPSS paradigm ([^Fan2014Tts], [^Zen2015Unidirectional], [^Valentini-Botinhao2016Investigating]).
Later, end-to-end neural TTS models were introduced, which synthesize Mel spectrograms and employ a vocoder (**WaveNet**[^Oord2016WaveNet], **WaveGlow**[^Prenger2018WaveGlow]) for speech synthesis (**Tacotron**[^Wang2017Tacotron], [^Ar{\i}k2017Deep], **FastSpeech**[^Ren2019FastSpeech]).
Some methods, utilizing techniques such as VAE (**GMVAE-Tacotron**[^Hsu2018GMVAE-Tacotron], [^Lee2022Bidirectional]), flow (**Flow-TTS**[^Miao2020Flow-TTS], **Glow-TTS**[^Kim2020Glow-TTS]), diffusion (**Diff-TTS**[^Jeong2021Diff-TTS], **Guided-TTS**[^Kim2021Guided-TTS], **Grad-TTS**[^Popov2021Grad-TTS]), and others ( [^Wu2022It\^oWave]), have achieved promising performance in end-to-end speech synthesis.
On the other hand, models like **VALL-E**[^Wang2023VALL-E] and **AudioLM**[^Borsos2022AudioLM] utilize autoregressive Transformers to model discrete audio tokens, achieving great in-context learning performance.
When it comes to zero-shot speech synthesis, autoregressive Transformer-based models can predict and generate audio without the need for an additional duration model, which strikes a favorable balance between efficiency and performance, and has been garnering increasing attention.

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="fig:overall">![](Images/2024.01.14_ELLA-V_Fig.01.png)</a>

</td></tr>
<tr><td>

Fig.01: The overall architecture of ***ELLA-V***.
Input an audio prompts and text prompts, ***ELLA-V*** first changes sequence order -- sandwiching each phoneme's audio $\left<\mathbf{C}_k\right>_{:}$ between the $k$-th phoneme and a \texttt{EOP} token and prepending the phoneme sequence to the beginning.
By learning on the mixed sequence, ***ELLA-V*** can generate audio sequence of the text prompts while maintaining the acoustic and environmental conditions of the audio prompts.

</td><td>

</td></tr></table>

## 3·Method

### 3.1·Overview

<table><tr><td width="50%">

[Fig.01](#fig:overall) demonstrates the overall architecture of ***ELLA-V***.
***ELLA-V*** primarily follows a two-stage framework similar to VALL-E, considering zero-shot TTS as a conditional codec language modeling task.
***ELLA-V*** maps input text prompts and speech prompts into a unified vocabulary space with a text encoder and a neural codec, respectively.
Different from VALL-E, an additional sequence order rearranging step is performed to the text-audio token sequence, after which, ***ELLA-V*** utilizes a decoder-only language model to learn to perform conditional generation on the hybrid sequences of phoneme and audio tokens.
Detailed information about the language model will be presented in Section~[sec:training](#sec:training).

</td><td>

</td></tr>
<tr><td>

To obtain discrete audio representations, we employ a pre-trained neural audio codec model, **EnCodec**[^Defossez2022EnCodec], following **VALL-E**[^Wang2023VALL-E].
EnCodec transforms 24 kHz raw waveforms into 75 Hz discrete tokens using $L$ RVQ layers.
The discrete acoustic tokens have a hierarchical structure, where the first layer quantizer contains semantic information and coarse-grained acoustic contours, while subsequent $L-1$ quantizers learn fine-grained acoustic details.
In our experiments, we use the same settings as VALL-E, with $L=8$.
For each quantizer, we set the codebook size to 1024.
In this setting, each second of the waveform is represented by $75\times 8$ discrete tokens from RVQ.

</td><td>

</td></tr>
<tr><td>

To obtain phoneme sequences, we apply the Montreal Forced Aligner (MFA)[^McAuliffe2017Montreal] to the input audio and text transcriptions.
Notably, MFA not only serves as a text tokenizer but also extracts alignment relationships between phonemes and the corresponding speech.
The forced alignment information is essential for ***ELLA-V*** to **change sequence order**.
In Section~[sec:training](#sec:training), we will provide a detailed explanation of how this information is used to construct the target sequence.

</td><td>

</td></tr></table>

### 3.2·Training: Codec Language Model

<table><tr><td width="50%">

***ELLA-V*** employs a Generalized Autoregressive Codec language model for the prediction of the first quantization layer in the EnCodec, which corresponds to capturing semantic information and coarse-grained acoustic profiles.
Subsequently, a non-autoregressive language model is utilized to generate codes for the subsequent quantization layers, aimed at reconstructing fine-grained acoustic details.
Specifically, given a speech corpus $\mathcal{D} = \{\mathbf{x}_i , \mathbf{y}_i \}$, where $\mathbf{x}$ represents an audio sample, and $\mathbf{y}$ is its text transcription.
We utilize the EnCodec to extract the discrete representation of $\mathbf{x}$, formulated as

$$
\mathbf{C}^{T\times 8} = \text{EnCodec} (\mathbf{y})
$$

where $\mathbf{C}$ represents the two-dimensional acoustic code matrix, and $T$ is the downsampled utterance length.

</td><td>

</td></tr>
<tr><td>

We employ MFA to obtain the phoneme sequence $\mathbf{P}_{1:n}$ corresponding to the transcription $\mathbf{y}$, while also extracting forced alignment information between the audio $\mathbf{x}$ and the transcription $\mathbf{y}$:

$$
\mathbf{P}_{1:n}, \bm{l}_{1:n} = \text{MFA} (\mathbf{x}, \mathbf{y})
$$

where $n$ is the number of phonemes of the audio sample $\mathbf{x}$, and $l_i$ denotes the length of the $i$-th phoneme of the discrete audio sequence.
MFA treats silence also as a kind of phoneme, so that the original audio sequence is partitioned into $n$ consecutive intervals corresponding to $n$ phonemes.
Specifically, let $\left<\mathbf{C}_i\right>^{l_i\times 8}$ represent the audio sequence corresponding to the $i$-th phoneme:

$$
\mathbf{C} =
\begin{bmatrix} \left<\mathbf{C}_1\right>_{1:l_1} \\ \left<\mathbf{C}_2\right>_{1:l_2} \\ \vdots \\ \left<\mathbf{C}_n\right>_{1:l_n} \\ \end{bmatrix}

$$

and

$$
\left<\mathbf{C}_k\right>_{1:l_k} = \mathbf{C}_{\sum_{i=1}^{k-1} l_i+1 : \sum_{i=1}^{k} l_i}
$$

After quantization, we utilize the EnCodec decoder to reconstruct the audio waveform from the discrete acoustic sequence $\mathbf{C}$, formulated as $\hat{\mathbf{x}} \approx \text{DeCodec}(\mathbf{C})$.

</td><td>

</td></tr>
<tr><td>

For the zero-shot TTS task, the optimization objective is $\max p (\mathbf{C} |\mathbf{P}, \mathbf{\hat{C}})$, where $\mathbf{\hat{C}}$ is the acoustic prompt of the unseen speaker.
We use language modeling to generate acoustic tokens for the unseen speaker, by learning on the mixed sequence composed of phonemes and codec codes, consistent with previous works[^Wang2023VALL-E], **AudioPaLM**[^Rubenstein2023AudioPaLM].

</td><td>

</td></tr>
<tr><td>

Unlike existing approaches, ***ELLA-V*** does not concatenate phoneme tokens and acoustic tokens directly to form the target sequence for training the language model.
Instead, ***ELLA-V*** **interleaves phoneme and acoustic tokens** in order to make it easier for language models to learn the alignment between audio and text.
Specifically, we insert each phoneme token $P_i$ (except the **silence phoneme**) into the corresponding position of the audio sequence, so that each phoneme's audio $\left<\mathbf{C}_i\right>$ is sandwiched between $P_i$ and `EOP` tokens.
We also prepend the phoneme sequence to the beginning of the mixed sequence, which is referred to as **global advance**.
In Section [sec:local](#sec:local), we further propose a variant sequence order with higher generation stability, named **local advance**, which moves the non-acoustic tokens of the sequence several frames forward.

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="fig:ar">![](Images/2024.01.14_ELLA-V_Fig.02.png)</a>

</td></tr>
<tr><td>

Fig.02: The illustration of Generalized Autoregressive language model of ***ELLA-V***.

</td><td>

</td></tr></table>

#### 3.2.1·Generalized Autoregressive (GAR) Codec Language Model

<table><tr><td width="50%">

As shown in [Fig.02](#fig:ar), ***ELLA-V*** first constructs a hybrid sequence $\mathbf{H}_{:,1}$ of acoustic and phoneme tokens, structured as:
\begin{center} $P_1, P_2, \ldots{}, P_n, \texttt{BOS}, P_1, \left<\mathbf{C}_1\right>_{:,1}, \texttt{EOP},$\\ $P_2, \left<\mathbf{C}_2\right>_{:,1}, \texttt{EOP}, \ldots{}, P_n, \left<\mathbf{C}_n\right>_{:,1}, \texttt{EOP}, \texttt{EOS}$ \end{center}
It is worth noting that the MFA (Montreal Forced Aligner) treats silence as a distinct phoneme, whereas our phoneme sequence $\mathbf{P}$ exclusively comprises phonemes other than silence.
To clarify, we retain the acoustic component associated with silence but do not sandwich it with an `EOP` and a specific silence phoneme, nor do we use a silence phoneme in the **global advance** part.

</td><td>

</td></tr>
<tr><td>

We design a GAR language model to learn the continuation task on the aforementioned hybrid sequence, to generate the discrete acoustic code sequence $\mathbf{C}_{:,1}$.
The GAR model consists of multiple Transformer decoder layers[^Vaswani2017Attention].
After training, it can generate discrete audio codes for a specified text prompt and acoustic prompt.
GAR is also responsible for predicting `EOP` and `EOS` to indicate the conclusion of a phoneme and the entire sentence, respectively.

</td><td>

</td></tr>
<tr><td>

The optimization of GAR is achieved by maximizing the likelihood of the acoustic part $\mathbf{C}_{:,1}$ of the hybrid sequence $\mathbf{H}_{:,1}$, as well as the special `EOP` and `EOS` tokens.
Under forward factorization, this process is formulated as:

$$
\begin{aligned}
&\max\limits_{\theta_{GAR}} \ \; \log p ( \mathbf{\tilde{C}}_{:,1} \,|\, \mathbf{P} ; \theta_{GAR} ) \\ = &\sum_{i=1}^{n} \sum_{t=0}^{l_{i}} \log p \bigl( \bigl<\mathbf{\tilde{C}}_{i}\bigr>_{t,1} | \bigl<\mathbf{\tilde{C}}_{i}\bigr>_{<t,1}, \bigl<\mathbf{\tilde{C}}_{<i}\bigr>_{:,1}, \\ &\mathbf{P} ; \theta_{GAR} \bigr) \\ = &\sum_{\substack{t=0 \\ \mathbf{H}_{t,1} \neq \texttt{BOS} \\ \mathbf{H}_{t,1} \notin \{\mathbf{P}\}}}^{T_{\bm{H}}} \log p \left( \mathbf{H}_{t,1} \,|\, \mathbf{H}_{<t,1} ; \theta_{GAR} \right)
\end{aligned}
$$

where $\mathbf{H}$ has a size of $T_{\bm{H}} \times 8$, $\{\mathbf{P}\}$ denotes the phoneme set, $\bigl<\mathbf{\tilde{C}}_{i}\bigr>$ is the concatenation of $\left<\mathbf{C}_i\right>$ along with its broadcast trailing `EOP` and/or `EOS` tokens, $\mathbf{\tilde{C}}$ is then the concatenation of $\left<\mathbf{C}_i\right>$, and $\theta_{GAR}$ represents neural network parameters of GAR model.
The factorization of the training objective naturally encapsulates the core intuition of the GAR model: GAR generates the audio sequence phoneme-by-phoneme.
GAR produces maximum likelihood predictions for each phoneme token successively, indicating the end of generating a specified phoneme by predicting `EOP`.
Through **global advancement**, GAR can directly infer the next phoneme to be generated without relying on network predictions.
After the prediction for the last phoneme is completed, GAR stops the generation process by predicting `EOS`.
The generated sequence by GAR is **self-aligned**, as it can instantly know the corresponding position of any generated acoustic token in relation to the phoneme prompt.

</td><td>

</td></tr>
<tr><td>

During training, we apply a bidirectional mask to the phoneme sequence before the `BOS` in the hybrid sequence, while a unidirectional mask is used for the part after `BOS`.
We frame the training as a next-token-prediction language modeling task on the hybrid sequence.
However, it's important to note that the model does not predict phonemes (or `BOS`).
In other words, as shown in [Fig.02](#fig:ar), we only compute loss when the token to be predicted is not a phoneme (or `BOS`).
During inference, whenever the model predicts an `EOP` for a phoneme, the next phoneme token is directly appended to the end of the sequence, which will be further discussed in Section~[sec:exp](#sec:exp).

</td><td>

</td></tr></table>

#### 3.2.2·Non-Autoregressive (NAR) Codec Language Model

<table><tr><td width="50%">

In the second stage, the NAR language model is employed to predict the codes from the second to the last quantization layers in parallel.
The input-output sequence construction of the NAR model follows the same pattern as used in the GAR model discussed in Section~[sec:gar](#sec:gar).
Specifically, the $i$-th column $\mathbf{H}_{:, i}$ of the hybrid sequence matrix $\mathbf{H}$ is structured as:
\begin{center} $P_1, P_2, \ldots{}, P_n, \verb|BOS|, P_1, \left<\mathbf{C}_1\right>_{:,i}, \verb|EOP|, P_2, \left<\mathbf{C}_2\right>_{:,i},$ \\ $\verb|EOP|, \ldots{}, P_n, \left<\mathbf{C}_n\right>_{:,i}, \verb|EOP|, \verb|EOS|$ \end{center}
And in practice if $P_i$ represents the silence, $\mathbf{C}_{:,i}$ will not be sandwiched by $P_i$ and `EOP`.
The NAR model takes the previously generated hybrid sequence of the previous $j-1$ layers as input and predicts the codes of the $j$-th layer in parallel, formulated as:

$$
\begin{aligned}
&\max\limits_{\theta_{NAR}} \ \; \sum_{j=2}^{8} \log p ( \mathbf{C}_{:,j} \,|\, \mathbf{H}_{:,<j}, \mathbf{P} ; \theta_{NAR} ) \\ = &\sum_{j=2}^{8} \sum_{\substack{t=0 \\ \mathbf{H}_{t,j} \in \{\mathbf{C}_{:,j}\}}}^{T_{\bm{H}}} \log p ( \mathbf{H}_{t,j} \,|\, \mathbf{H}_{:,<j}, \mathbf{P} ; \theta_{NAR} )
\end{aligned}
$$

where $\{\mathbf{C}_{:,j}\}$ denotes the acoustic token set of the $j$-th quantizer.
In this formulation, The embeddings of tokens from the previous $j-1$ quantizers are summed up to feed the NAR model to predict the $j$-th layer.
Intuitively, both the GAR and NAR model of ***ELLA-V*** compute the loss on the acoustic tokens of the target sequence, and GAR additionally computes loss for `EOP` and `EOS`.

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="fig:inference">![](Images/2024.01.14_ELLA-V_Fig.03.png)</a>

</td></tr>
<tr><td>

Fig.03: The illustration of the inference process of ***ELLA-V***.

</td><td>

</td></tr></table>

### 3.3·Inference

<table><tr><td width="50%">

***ELLA-V*** can use a short clip of speech from an unseen speaker as an acoustic prompt to synthesize speech for a specified text prompt.
[Fig.03](#fig:inference) illustrates the inference process of the GAR model.
While VALL-E may get stuck in an infinite loop during inference, resulting in the synthesis of either **infinite silence** or repetitive pronunciation **VALL-E**[^Wang2023VALL-E], ***ELLA-V*** is capable of generating `EOP` and promptly truncating abnormally long phonemes.
Following an `EOP`, we can directly append the next phoneme token to the end of the generated sequence, ensuring the proper generation of speech without abnormal pauses or repetitions.
For the GAR model, we employ a sampling-based decoding strategy, whereas for the NAR model, we use a greedy decoding approach to strike a balance between efficiency and performance.

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="fig:advance">![](Images/2024.01.14_ELLA-V_Fig.04.png)</a>

</td></tr>
<tr><td>

Fig.04: **Local advance**.
A phoneme can locally have access to information about the next phoneme token advanced by $Adv$ frames, allowing it to anticipate the upcoming phoneme token's characteristics.

</td><td>

</td></tr></table>

### 3.4·Local Advance

<table><tr><td width="50%">

One intuition is that the pronunciation of a phoneme is strongly related to the pronunciation of the phonemes just before and after it.
However, due to the autoregressive nature of the GAR model, an acoustic token cannot attend to the following phoneme tokens, even though we can leverage the transformer's ability to model long-term dependencies through **global advance** to provide complete context for the acoustic token generation.
To further harness the powerful capability of the transformer in modeling local dependencies, ***ELLA-V*** introduces an additional change in the sequence order based on Section~[sec:training](#sec:training).
Specifically, we move the phoneme token and the `EOP` token ahead by a few frames, referred to as **local advance**.

</td><td>

</td></tr></table>

## 4·Experiments

### 4.1·Experimental Setup

<table><tr><td width="50%">

**Data \& Tasks:**

We trained ***ELLA-V*** using the Librispeech[^Panayotov2015LibriSpeech] 960h training dataset.
We utilized Montreal Forced Aligner (MFA)[^McAuliffe2017Montreal] to obtain forced alignment information for the audio-transcription pairs.
Sentences with unrecognized or unknown phones by MFA were excluded.
The open-source 24kHz [checkpoint](https://github.com/facebookresearch/encodec) of **EnCodec**[^Defossez2022EnCodec] was used as the codec to generate discrete acoustic tokens.
The LibriSpeech training data was upsampled to 24 kHz before feeding it into EnCodec.
In evaluating the model, two zero-shot TTS tasks were considered.
For the zero-shot TTS continuation task, we adhered to methodologies established by previous works (**VALL-E**[^Wang2023VALL-E], **VoiceBox**[^Le2023VoiceBox], **SpeechX**[^Wang2023SpeechX]), selecting examples ranging from 4 seconds to 10 seconds from the LibriSpeech test-clean dataset as our test set.
In this task, we used the complete phoneme transcription as the text prompt and the first 3 seconds of the test audio sample as the acoustic prompt.
The model was required to generate continuations.
For the zero-shot TTS cross-speaker task, we designed a hard case set comprising 100 hard sentences, as outlined in the demo page \url{}.
These sentences included challenging phonetic patterns, alliteration, and unusual (abnormal) combinations of words that might pose difficulties for a TTS system to generate natural-sounding speech.
In this case, we randomly picked 3-second sentences from the LibriSpeech test-clean subset as the acoustic prompt.
We then concatenated the transcription of this segment and the target phoneme sequence in the hard case set to form the text prompt.
The model was tasked with cloning the voice of the speaker to say the specified target text in the hard case set.

</td><td>

</td></tr>
<tr><td>

**Training configuration:**

For both GAR and NAR models, we stacked 12 Transformer decoder layers with an embedding dimension of 1024, a hidden state dimension of 1024, and a feed-forward layer dimension of 4096.
All models were trained in parallel using 8 NVIDIA Tesla V100 GPUs with a batch size of 16384 tokens for GAR and 12288 tokens for NAR per GPU, respectively, learning a total of 320k steps.
We used the AdamW optimizer with $\beta_1=0.9$, $\beta_2=0.999$, $\epsilon=10^{-9}$.
We employed an inverse-sqrt learning rate scheduler with warm-up.
For the first $32000$ updates, we linearly increased the learning rate from $10^{-7}$ to a peak of $5 \times 10^{-4}$.
The weight decay was 0.01.

</td><td>

</td></tr>
<tr><td>

**Baseline:**

In our research, we benchmarked the performance of zero-shot speech synthesis against **VALL-E**[^Wang2023VALL-E].
This system was originally trained on a substantial 60k hours of audio from the Librilight dataset **Libri-Light**[^Kahn2019Libri-Light].
To ensure a rigorous evaluation, we reproduced the VALL-E model and adapted it to train on the LibriSpeech 960h dataset.
We also adjusted the model dimensions and the number of layers to match the parameter settings of ***ELLA-V*** and VALL-E.
Both GAR (or AR) and NAR models of VALL-E and ***ELLA-V*** have 154.3M parameters.
Moreover, to mitigate any potential bias introduced by the audio codec, we pre-processed the authentic speech samples using EnCodec's encoder and decoder.
We also include the result for Encodec reconstructed speech for reference, denoted as Ground-Truth Encodec.

</td><td>

</td></tr>
<tr><td>

**Evaluation Metrics:**

We evaluated our system with several objective metrics.
Speaker similarity (SPK) and WER served as our primary measures.
SPK was assessed using the fine-tuned WavLM-TDNN model [URL](https://huggingface.co/microsoft/wavlm-base-plus-sv)[^Chen2021WavLM], scoring similarity on a scale of -1 to 1, with values above 0.86 indicate the same speaker identity (This value comes from the release model card page).
The WER was determined by comparing the synthesized speech to the original text using the Conformer-Transducer model [URL](https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge)[^Gulati2020Conformer].
In addition to these standard metrics, we introduced two novel measures: INF\In the practical implementation, INF\On the other hand, as discussed in the previous session, the design of ***ELLA-V*** enables the control of the duration for each phoneme during inference, thus avoiding the synthesis failure.
In our experiments, we forcibly truncate the synthesis of phonemes with a length greater than 0.4 seconds.
CUT\For each objective metric, we reported average values over three experimental runs with different random seeds.
For subjective analysis, we relied on the mean opinion score (MOS).
30 test samples were chosen for this purpose, with each sample being evaluated by at least 15 listeners for aspects like naturalness and speaker similarity.
The comparative mean option score (CMOS) and the similarity mean option score (SMOS) were the key subjective metrics used.
SMOS was rated on a 1 to 5 scale, in 0.5-point increments, to gauge speaker similarity, while CMOS, ranging from -1 to 1, assessed the overall naturalness and quality of the synthesized speech against the baseline.

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="tab:results">![]()</a>

</td></tr>
<tr><td>

Tab.02: Subjective and Objective performance comparison between ELLA-V and VALL-E on zero-shot TTS continuation task.
$^\dagger$ indicates that ground-truth audios were passed through the encoder and decoder of Encodec to evaluate the influence of neural audio codec.

</td><td>

</td></tr></table>

### 4.2·Results

<table><tr><td width="50%">

**Zero-Shot TTS continuation task.**

We present the evaluation results in [Tab.02](#tab:results), where a comparison between ***ELLA-V*** and VALL-E is shown.
First, regarding speaker similarity, both subjective (SMOS) and objective (SPK) results indicate that ***ELLA-V*** and VALL-E performed similarly, which can be attributed to their shared backbone approach, combining (G)AR and NAR.
Meanwhile, CMOS testing shows that ***ELLA-V*** achieved a +0.10 score, demonstrating a higher generation quality (i.e., naturalness) compared to VALL-E.
Additionally, WERs calculated between the recognized text of synthesized audio and the ground-truth text show that ***ELLA-V*** is significantly better than VALL-E (2.28 versus 5.00).
This underscores ***ELLA-V***'s enhanced capability in synthesizing higher-quality and more robust speech.
Overall, ***ELLA-V*** substantially improved the synthesis accuracy and robustness of the language model-based TTS framework without affecting the naturalness and speaker similarity.
This conclusion is not only corroborated by this easy continuation task, but also validated via the challenging synthesis sets in the subsequent section.

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="tab:hard">![]()</a>

</td></tr>
<tr><td>

Tab.03: WER comparison between ***ELLA-V*** and VALL-E on 100 particularly hard synthesis cases.
Sub, Del, and Ins refer to Substitution, Deletion, and Insertion error rates, respectively.

</td><td>

</td></tr>
<tr><td>

**Zero-shot TTS cross-speaker task on hard cases.**

**VALL-E** utilized a traditional AR model that frequently resulted in alignment errors, including repetitions, transpositions, and omissions, particularly in more challenging synthesis cases (see Section [sec:data](#sec:data) for details of the challenging synthesis set).
[Tab.03](#tab:hard) presents the WER comparison of VALL-E and ***ELLA-V*** on the 100 particularly hard synthesis sentences.
In contrast to VALL-E, ***ELLA-V*** demonstrates markedly lower WER, signifying its enhanced robustness.
This substantial reduction in errors translates to more accurate and reliable voice synthesis applications, significantly improving user experience in real-world scenarios.
Regarding VALL-E's tendency to fall into infinite silence, an intuitive explanation is that the silence patterns in the training data are relatively simple and many of them are repetitive.
In this case, a traditional language model is prone to overfitting to these patterns.
During testing, when the model encounters silence, it assigns a high probability to silence.
This leads to issues such as beam search, which is based on maximum likelihood, getting stuck in a loop.
However, ***ELLA-V*** does not face this problem.

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="fig:result2c">![](pngs/results/result2c.png)</a>

</td></tr>
<tr><td>

Fig.05: Ablations on decoding strategies.
The figures demonstrate the trends of three metrics, INF (for VALL-E), CUT (for ***ELLA-V***), and WER (for both), with respect to the variations in top\_p in nuclear sampling.

</td><td>

</td></tr>
<tr><td>

**Analysis of Decoding Strategies.**

To demonstrate the stability of ***ELLA-V*** under different decoding strategies, we conducted an ablation study, testing the decoding performance with different top-$p$ values for nuclear sampling, by varying $p \in \{1, 0.99, 0.95, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2,$ $0.1, 0.0(\text{greedy})\}$.
The results are shown in Figure [fig:results](#fig:results).
We can observe that as top\_$p$ decreases, the accuracy of VALL-E's synthesized speech significantly decreases.
At this point, VALL-E is more prone to generating a large number of overfit silence tokens, leading to a significant increase in INF\

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="tab:ablation">![]()</a>

</td></tr>
<tr><td>

Tab.04: The ablation study to investigate the impact of global and local phoneme information.

</td><td>

</td></tr>
<tr><td>

**Ablation Study.**

In this paragraph, we conduct ablation experiments.
(1) To investigate the impact of global phoneme information on synthesized speech, we removed the global phoneme sequence at the beginning of the trained sequence (abbr.
***ELLA-V***-noglobal).
(2) To investigate whether it is necessary to provide the specific phoneme token before its corresponding acoustic tokens during both training and inference, rather than just using the `EOP` separator, we removed all phoneme tokens following `BOS` in the mixed sequence (abbr.
***ELLA-V***-nophn).
The experimental results are shown in [Tab.04](#tab:ablation).
It is observed that the accuracy of synthesized speech significantly deteriorated either when global phoneme tokens were not used or when local phoneme tokens were disabled within the hybrid sequence.
It is also notable that even in the absence of global advance (i.e., in the ***ELLA-V***-noglobal configuration), the SPK and WER of the synthesized audio were comparable to those of VALL-E.
These findings indicate the importance of both local and global information in achieving more accurate synthesized audios, meanwhile, combining both of them potentially leads to further enhancements in accuracy.

</td><td>

</td></tr></table>

## 5·Conclusion

<table><tr><td width="50%">

In this paper, we introduce ***ELLA-V***, a simple and efficient two-stage zero-shot TTS framework based on language modeling.
By learning interleaved sequences of acoustic and text tokens, our proposed GAR model can provide fine-grained control over synthesized audio at the phoneme level and can better leverage local dependencies to predict the pronunciation of the current phoneme.
Experimental results demonstrate that ***ELLA-V*** achieves higher accuracy and more stable results under different threshold top-$p$ for nuclear sampling.
We aspire for this work to advance research in enhancing the robustness of speech generation.

</td><td>

</td></tr></table>

## References

[^Brown2020GPT-3]: [**GPT-3**: Language Models are Few-Shot Learners](../TextLM/2020.05.28_GPT-3.md). ArXiv:2005.14165v4/NeurIPS2020.
[^Ramesh2022Hierarchical]: Hierarchical Text-Conditional Image Generation With {CLIP} Latents. arXiv e-prints.
[^Ho2020DDPM]: [**DDPM**: Denoising Diffusion Probabilistic Models](../Diffusion/_2020.06.19_DDPM.md). ArXiv:2006.11239v2.
[^Rombach2021LDM]: [**LDM**: High-Resolution Image Synthesis with Latent Diffusion Models](../Diffusion/_2021.12.20_LDM.md). ArXiv:2112.10752v2/CVPR2022.
[^Borsos2022AudioLM]: [**AudioLM**: A Language Modeling Approach to Audio Generation](../SpeechLM/PureSpeechLM/2022.09.07_AudioLM.md). ArXiv:2209.03143v2/TASLP2023.
[^Kim2021VITS]: [**VITS**: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech](../E2E/2021.06.11_VITS.md). ArXiv:2106.06103/ICML2021.
[^Chiang2019Cluster-GCN]: {Cluster-Gcn}: {An} Efficient Algorithm for Training Deep and Large Graph Convolutional Networks. Proc. ACM SIGKDD 2019.
[^Sohl-Dickstein2015Deep]: Deep Unsupervised Learning Using Nonequilibrium Thermodynamics. Proc. ICML 2015.
[^Song2020Improved]: Improved Techniques for Training Score-Based Generative Models. Proc. NeurIPS 2020.
[^Song2020Score]: [Score-Based Generative Modeling through Stochastic Differential Equations](../Diffusion/_2020.11.26_Song2020Score.md). ArXiv:2011.13456v2/ICLR2021Oral.
[^Lipman2022FM]: [**FM**: Flow Matching for Generative Modeling](../FlowMatching/_2022.10.06_Flow_Matching.md). ArXiv:2210.02747v2.
[^Popov2021Grad-TTS]: [**Grad-TTS**: A Diffusion Probabilistic Model for Text-to-Speech](../Acoustic/2021.05.13_Grad-TTS.md). ArXiv:2105.06337v2/ICML2021.
[^Huang2022ProDiff]: [**ProDiff**: Progressive Fast Diffusion Model For High-Quality Text-to-Speech](../Diffusion/2022.07.13_ProDiff.md). ArXiv:2207.06389v1/ACM Multimedia 2022.
[^Huang2023Make-An-Audio]: [**Make-An-Audio**: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models](../Diffusion/2023.01.30_Make-An-Audio.md). ArXiv:2301.12661v1/ICML2023.
[^Shen2023NaturalSpeech2]: [**NaturalSpeech2**: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers](../Diffusion/2023.04.18_NaturalSpeech2.md). ArXiv:2304.09116v3/ICLR2024Spotlight.
[^Vaswani2017Transformer]: [**Transformer**: Attention Is All You Need](../_Basis/2017.06.12_Transformer.md). ArXiv:1706.03762/NeurIPS2017.
[^Devlin2018BERT]: [**BERT**: Pre-training of Deep Bidirectional Transformers for Language Understanding](../TextLM/2018.10.11_BERT.md). ArXiv:1810.04805v2/NAACL2019.
[^Raffel2019T5]: [**T5**: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](../TextLM/2019.10.23_T5.md). ArXiv:1910.10683/JMLR2020.
[^Lewis2020Bart]: {BART}: {Denoising} Sequence-to-Sequence Pre-Training for Natural Language Generation, Translation, and Comprehension. Proc. ACL 2020.
[^Thoppilan2022LaMDA]: {LaMDA}: {Language} Models for Dialog Applications. arXiv:2201.08239.
[^Chowdhery2022PaLM]: [**PaLM**: Scaling Language Modeling with Pathways](../TextLM/2022.04.05_PaLM.md). ArXiv:2204.02311v5.
[^Wang2023VALL-E]: [**VALL-E**: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers](2023.01.05_VALL-E.md). ArXiv:2301.02111/TASLP2025.
[^Kharitonov2023SPEAR-TTS]: [**SPEAR-TTS**: Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision](../SpeechLM_TTS/2023.02.07_SPEAR-TTS.md). ArXiv:2302.03540v1/TACL2023.
[^Rubenstein2023AudioPaLM]: [**AudioPaLM**: A Large Language Model That Can Speak and Listen](../SpeechLM/ST2ST/2023.06.22_AudioPaLM.md). ArXiv:2306.12925.
[^Du2023LauraGPT]: [**LauraGPT**: Listen, Attend, Understand, and Regenerate Audio with GPT](../SpeechLM/ST2ST/2023.10.07_LauraGPT.md). ArXiv:2310.04673.
[^Kahn2019Libri-Light]: [**Libri-Light**: A Benchmark for ASR with Limited or No Supervision](../../Datasets/2019.12.17_Libri-Light.md). ArXiv:1912.07875v1/ICASSP2020.
[^Fu2023Decoder-Only]: Decoder-Only or Encoder-Decoder? {Interpreting} Language Model as a Regularized Encoder-Decoder. arXiv:2304.04052.
[^Yang2019XLNet]: {XLNet}: {Generalized} Autoregressive Pretraining for Language Understanding. Proc. NeurIPS 2019.
[^Holtzman2019Curious]: The Curious Case of Neural Text Degeneration. Proc. ICLR 2019.
[^Ippolito2019Comparison]: Comparison of Diverse Decoding Methods From Conditional Language Models. Proc. ACL 2019.
[^Bao2020UniLMv2]: {UniLMv2}: {Pseudo}-Masked Language Models for Unified Language Model Pre-Training. Proc. ICML 2020.
[^Panayotov2015LibriSpeech]: [**LibriSpeech**: An ASR Corpus Based on Public Domain Audio Books](../../Datasets/2015.04.19_LibriSpeech.md). IEEE@ICASSP2015.
[^Rae2021Scaling]: {Scaling} Language Models: {Methods}, Analysis \& Insights From Training Gopher. arXiv:2112.11446.
[^Yu2022Scaling]: Scaling Autoregressive Models for Content-Rich {Text-to-Image} Generation. Transactions on Machine Learning Research 2022.
[^Alayrac2022Flamingo]: Flamingo: {A} Visual Language Model for Few-Shot Learning. Proc. NeurIPS 2022.
[^Tsimpoukelli2021Multimodal]: Multimodal Few-Shot Learning With Frozen Language Models. Proc. NeurIPS 2021.
[^Yang2022Zero-Shot]: Zero-Shot Video Question Answering via Frozen Bidirectional Language Models. Proc. NeurIPS 2022.
[^Wang2022Language]: Language Models With Image Descriptors Are Strong Few-Shot Video-Language Learners. Proc. NeurIPS 2022.
[^Kreuk2022AudioGen]: [**AudioGen**: Textually Guided Audio Generation](../SpeechLM/ST2S/2022.09.30_AudioGen.md). ArXiv:2209.15352v2/ICLR2023Poster.
[^Wang2023LM-VC]: [**LM-VC**: Zero-shot Voice Conversion via Speech Generation based on Language Models](../-VC/2023.06.18_LM-VC.md). ArXiv:2306.10521v2.
[^Kakouros2023Investigating]: Investigating the Utility of Surprisal From Large Language Models for Speech Synthesis Prosody. 12th Speech Synthesis Workshop (SSW) 2023.
[^Zhang2023VALL-EX]: [**VALL-E X**: Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling](2023.03.07_VALL-E_X.md). ArXiv:2303.03926.
[^Zen2009Statistical]: Statistical Parametric Speech Synthesis. Speech Communication 2009.
[^Yoshimura1999Simultaneous]: Simultaneous Modeling of Spectrum, Pitch and Duration in {HMM}-based Speech Synthesis. Sixth European Conference on Speech Communication and Technology 1999.
[^Fan2014Tts]: {TTS} Synthesis With Bidirectional {LSTM} Based Recurrent Neural Networks. Proc. INTERSPEECH 2014.
[^Zen2015Unidirectional]: Unidirectional Long Short-Term Memory Recurrent Neural Network With Recurrent Output Layer for Low-Latency Speech Synthesis. Proc. ICASSP 2015.
[^Valentini-Botinhao2016Investigating]: Investigating {RNN}-based Speech Enhancement Methods for Noise-Robust Text-to-Speech. Speech Synthesis Workshop (SSW) 2016.
[^Oord2016WaveNet]: [**WaveNet**: A Generative Model for Raw Audio](../Vocoder/2016.09.12_WaveNet.md). ArXiv:1609.03499v2.
[^Prenger2018WaveGlow]: [**WaveGlow**: A Flow-based Generative Network for Speech Synthesis](../Vocoder/2018.10.31_WaveGlow.md). ArXiv:1811.00002v1/ICASSP2019.
[^Wang2017Tacotron]: [**Tacotron**: Towards End-to-End Speech Synthesis](../Acoustic/2017.03.29_Tacotron.md). ArXiv:1703.10135v2/InterSpeech2017.
[^Ar{\i}k2017Deep]: {Deep Voice}: {Real}-Time Neural Text-to-Speech. Proc. ICML 2017.
[^Ren2019FastSpeech]: [**FastSpeech**: Fast Robust and Controllable Text to Speech](../Acoustic/2019.05.22_FastSpeech.md). ArXiv:1905.09263/NeurIPS2019.
[^Hsu2018GMVAE-Tacotron]: [**GMVAE-Tacotron**: Hierarchical Generative Modeling for Controllable Speech Synthesis](../Acoustic/2018.10.16_GMVAE-Tacotron.md). ArXiv:1810.07217v2/ICLR2019.
[^Lee2022Bidirectional]: Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech. Proc. ICLR 2022.
[^Miao2020Flow-TTS]: [**Flow-TTS**: A Non-Autoregressive Network for Text to Speech Based on Flow](../Acoustic/2020.04.09_Flow-TTS.md). ICASSP2020.
[^Kim2020Glow-TTS]: [**Glow-TTS**: A Generative Flow for Text-to-Speech via Monotonic Alignment Search](../Acoustic/2020.05.22_Glow-TTS.md). ArXiv:2005.11129v2/NeurIPS2020.
[^Jeong2021Diff-TTS]: [**Diff-TTS**: A Denoising Diffusion Model for Text-to-Speech](../Acoustic/2021.04.03_Diff-TTS.md). ArXiv:2104.01409v1/InterSpeech2021.
[^Kim2021Guided-TTS]: [**Guided-TTS**: A Diffusion Model for Text-to-Speech via Classifier Guidance](../Diffusion/2021.11.23_Guided-TTS.md). ArXiv:2111.11755v4/ICML2022.
[^Wu2022It\^oWave]: {It\^oWave}: {It\^o} Stochastic Differential Equation Is All You Need for Wave Generation. Proc. ICASSP 2022.
[^Defossez2022EnCodec]: [**EnCodec**: High Fidelity Neural Audio Compression](../Tokenizers/2022.10.24_EnCodec.md). ArXiv:2210.13438/TMLR2023.
[^McAuliffe2017Montreal]: {Montreal Forced Aligner}: {Trainable} Text-Speech Alignment Using {Kaldi}. Proc. Interspeech 2017.
[^Vaswani2017Transformer]: [**Transformer**: Attention Is All You Need](../_Basis/2017.06.12_Transformer.md). ArXiv:1706.03762/NeurIPS2017.
[^Le2023VoiceBox]: [**VoiceBox**: Text-Guided Multilingual Universal Speech Generation at Scale](../FlowMatching/2023.06.23_Voicebox.md). ArXiv:2306.15687/NeurIPS2023Poster.
[^Wang2023SpeechX]: [**SpeechX**: Neural Codec Language Model as a Versatile Speech Transformer](../SpeechLM/ST2S/2023.08.14_SpeechX.md). ArXiv:2308.06873/TASLP2024.
[^Chen2021WavLM]: [**WavLM**: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](../Tokenizers/2021.10.26_WavLM.md). ArXiv:2110.13900v5/JSTSP2022.
[^Gulati2020Conformer]: [**Conformer**: Convolution-augmented Transformer for Speech Recognition](../-ASR/2020.05.16_Conformer.md). ArXiv:2005.08100v1/InterSpeech2020.