# Speak Foreign Languages With Your Own Voice: Cross-Lingual Neural Codec Language Modeling

<details>
<summary>基本信息</summary>

- 标题: "Speak Foreign Languages With Your Own Voice: Cross-Lingual Neural Codec Language Modeling."
- 作者:
  - 01 Ziqiang Zhang
  - 02 Long Zhou
  - 03 Chengyi Wang
  - 04 Sanyuan Chen
  - 05 Yu Wu
  - 06 Shujie Liu
  - 07 Zhuo Chen
  - 08 Yanqing Liu
  - 09 Huaming Wang
  - 10 Jinyu Li
  - 11 Lei He
  - 12 Sheng Zhao
  - 13 Furu Wei
- 链接:
  - [ArXiv](https://arxiv.org/abs/2303.03926)
  - [Publication]()
  - [Github Reproduce](https://github.com/Plachtaa/VALL-E-X)
  - [Demo](https://aka.ms/vallex)
- 文件:
  - [ArXiv:2303.03926v1]()
  - [Publication] #TODO

</details>

## Abstract

<table><tr><td width="50%">

We propose a *cross-lingual neural codec language model*, ***VALL-E X***, for cross-lingual speech synthesis.
Specifically, we extend **VALL-E**[^Wang2023VALL-E] and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts.
***VALL-E X*** inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks.
Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment.
Moreover, ***VALL-E X*** effectively alleviates the foreign accent problems, which can be controlled by a language ID.
Audio samples are available at [URL](https://aka.ms/vallex).

</td><td>

我们提出了一种跨语言神经编解码语言模型, ***VALL-E X***, 用于跨语言语音合成.
具体来说, 我们扩展了 **VALL-E**, 并训练了一个多语言条件编解码语言模型, 利用源语言语音和目标语言文本作为提示, 预测目标语言语音的声学标记序列.

***VALL-E X*** 继承了强大的上下文学习能力, 能够应用于零样本跨语言文本到语音合成和零样本语音到语音翻译任务.

实验结果表明, 它能够仅通过源语言的一次语音输入作为提示, 生成目标语言的高质量语音, 同时保持未见说话人的声音、情感和声学环境.
此外, ***VALL-E X*** 有效缓解了外语口音问题, 且该问题可以通过语言ID进行控制.

音频样本可在 [此链接](https://aka.ms/vallex) 处获取.

</td></tr></table>

## 1·Introduction

<table><tr><td width="50%">

Recent years have witnessed significant advancements in end-to-end text-to-speech (TTS) synthesis, and the quality of synthesized speech is even close to human parity (**TransformerTTS**[^Li2019TransformerTTS], **FastSpeech**[^Ren2019FastSpeech], **NaturalSpeech**[^Tan2022NaturalSpeech]).
However, these models can only generate high-quality speech for a specific speaker in a specific language.
Cross-lingual speech synthesis is a new emerging task that aims to transfer the speaker's voice from one language to another.
The speech quality for cross-lingual speech synthesis, especially the speaker similarity, is far behind the monolingual TTS models due to two reasons, 1) data scarcity, as it is difficult to collect multi-lingual speech data for the same speaker, and 2) model capacity, as conventional cross-lingual TTS models are not powerful enough to transfer the speaker voice, speech background, and speaker emotion from the source language speech to the target language speech.

</td><td>

</td></tr>
<tr><td>

Previous methods to tackle these challenges typically augment end-to-end TTS models with specific subnets for speaker and language control ( [^Nachmani2019Unsupervised], [^Zhang2020Multilingual], [^Yang2020Towards], [^Ellinas2022Cross-Lingual], [^Cai2023Cross-Lingual]).
For example, based on the multi-speaker TTS model, [^Nachmani2019Unsupervised] introduce multiple encoders for each language and additional loss to keep the speaker's identity.
[^Zhang2020Multilingual] employs a phonemic representation to capture cross-language information and an adversarial network to disentangle speaker identities.
[^Yang2020Towards] incorporate speaker and language networks with speaker and language IDs as input to deal with the multi-speaker and cross-lingual problems respectively.
[^Yang2022Cross-Lingual] further propose a multi-task learning method with additional tasks of speaker similarity and language identification.
Moreover, [^Cai2023Cross-Lingual] investigates cross-lingual multi-speaker text-to-speech synthesis with sufficient or limited bilingual speech training data.
However, the above methods fail to effectively extend to zero-shot scenarios for synthesizing target speech from the unseen source speaker, and often suffer from low speaker similarity and L2 (second-language, or foreign) accent problems [^Zhang2019Learning], [^Lee2022Empirical].

</td><td>

</td></tr>
<tr><td>

In this work, we present a novel approach to address these issues by proposing a simple yet effective cross-lingual neural codec language model, ***VALL-E X***, which leverages strong in-context learning capacities to achieve high-quality zero-shot cross-lingual speech synthesis.
Based on the knowledge learned from large-scale multi-lingual speech data, ***VALL-E X*** is able to transfer the speech characteristics, including the speaker's voice, emotions, and the speech background, from the source language to the target language, and also alleviate the foreign accent problems.
More specifically, we first obtain the multi-lingual speech-transcription data from existing ASR data or pseudo-labeled speech data.
Then we convert the transcriptions to phoneme sequences with a rule-based converter (G2P tool) and the speech data to acoustic tokens with an offline neural codec encoder.
Finally, we concatenate the paired phoneme and acoustic token sequences of each language and train a multi-lingual conditional language model.

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="table_compare">![](Images/2023.03.07_VALL-E_X_Fig.01.png)</a>

</td></tr>
<tr><td>

Fig.01: The overall framework of VALL-E X, which can synthesize personalized speech in another
language for a monolingual speaker.
Taking the phoneme sequences derived from the source and
target text, and the source acoustic tokens derived from an audio codec model as prompts, VALL-E
X is able to produce the acoustic tokens in the target language, which can be then decompressed
to the target speech waveform.
Thanks to its powerful in-context learning capabilities, VALL-E X
does not require cross-lingual speech data of the same speakers for training, and can perform various
zero-shot cross-lingual speech generation tasks, such as cross-lingual text-to-speech synthesis and
speech-to-speech translation.

</td><td>

</td></tr>
<tr><td>

As illustrated in [Fig.01](#fig_vallex_framework), after training, ***VALL-E X*** can predict the acoustic tokens of the target language prompted by the phoneme sequences of both languages and the acoustic tokens of the source language.
The generated acoustic token sequence is decompressed to the target speech waveform by an offline audio codec decoder.
***VALL-E X*** is trained on two large-scale multi-speaker datasets, **Libri-Light**[^Kahn2019Libri-Light] and WenetSpeech [^Zhang2022Wenetspeech], containing about 60,000 hours of English audiobook speech data and 10,000+ hours of multi-domain Chinese ASR data, respectively.
To our knowledge, they are the largest publicly available speech datasets for English and Chinese.
The combination of LibriLight and WenetSpeech makes a large multi-lingual multi-speaker multi-domain unclean speech dataset, which significantly improves the coverage of different speakers and enhances ***VALL-E X***'s generalization capacity.
The comparison between ***VALL-E X*** and the previous cross-lingual TTS systems are listed in [Tab.01](#table_compare).

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="table_compare">![]()</a>

</td></tr>
<tr><td>

Tab.01: A comparison between ***VALL-E X*** and previous cross-lingual TTS systems.

</td><td>

</td></tr>
<tr><td>

We conduct experiments on two kinds of cross-lingual speech generation tasks, zero-shot cross-lingual text-to-speech synthesis (XTTS), and zero-shot speech-to-speech translation (S2ST).
For cross-lingual text-to-speech synthesis, the proposed ***VALL-E X*** is evaluated with **LibriSpeech**[^Panayotov2015LibriSpeech] and EMIME [^Wester2010Emime] for English and Chinese respectively, including English TTS prompted by Chinese speakers and Chinese TTS prompted by English speakers.
For zero-shot speech-to-speech translation, EMIME [^Wester2010Emime] dataset is used for the evaluation of ***VALL-E X*** on bidirectional Chinese$\leftrightarrow$English translation tasks, and it contains bilingual audio recordings by the same speakers.
We evaluate the proposed ***VALL-E X*** framework from several aspects, including speaker similarity, speech quality (ASR- WER or BLEU), speech naturalness, and human evaluation (e.g., SMOS, MOS, and CMOS).
Specifically, due to the strong in-context learning capability, ***VALL-E X*** achieves a higher speaker similarity score than the previous SOTA model for the unseen speaker.
By training on large-scale speech-transcription data, the proposed ***VALL-E X*** significantly reduces the word error rate from 8.53 to 4.07 in the cross-lingual English TTS task, obtains the substantial gain of 3.17 BLEU scores than the strong baseline in S2ST tasks, and achieves better speech naturalness.
Furthermore, the human evaluation shows that our ***VALL-E X*** outperforms strong baselines in terms of SMOS (4.00 vs. 3.42 in XTTS, 4.12 vs. 3.06 in S2ST), CMOS (+0.24 for ours vs. baseline in XTTS), and MOS (3.87 vs. 3.81 in S2ST).

</td><td>

</td></tr>
<tr><td>

Our contributions can be summarized as follows:
- We develop a cross-lingual neural codec language model ***VALL-E X*** with large multi-lingual multi-speaker multi-domain unclean speech data.
***VALL-E X*** is a conditional cross-lingual language model predicting the target language acoustic tokens with the source language speech and target language text as prompts.
- The multi-lingual in-context learning framework enables ***VALL-E X*** to generate cross-lingual speech maintaining the unseen speaker's voice, emotion, and speech background, prompted by only one sentence in the source language.
- Based on the learned cross-lingual speech modeling ability with the introduced language ID, ***VALL-E X*** can generate speech in a native tongue for any speaker and can significantly reduce the foreign accent problem, which is a well-known problem in cross-lingual speech synthesis tasks.
- We apply ***VALL-E X*** to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks.
Experiments show that the proposed ***VALL-E X*** can beat the strong baseline in terms of speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.

We encourage readers to listen to the audio samples on our [demo page](https://aka.ms/vallex).

</td><td>

</td></tr></table>

## 2·Related Work

<table><tr><td width="50%">

**Speech/Audio Synthesis**

With the rapid development and application of neural networks, speech and audio synthesis have made tremendous progress with different network frameworks, such as **WaveNet**[^Oord2016WaveNet], **HiFi-GAN**[^Kong2020HiFi-GAN], and **DiffWave**[^Kong2020DiffWave].
Academic and industrial communities also pay increasing attention to synthesizing speech or sound from text, namely text-to-speech (TTS) (**TransformerTTS**[^Li2019TransformerTTS], **FastSpeech**[^Ren2019FastSpeech]) or text-to-sound ( [^Yang2022Diffsound], **AudioGen**[^Kreuk2022AudioGen]).
Recently, it is emerging to apply discrete audio representation learning to audio synthesis, e.g., **AudioGen**[^Kreuk2022AudioGen] and **AudioLM**[^Borsos2022AudioLM].
AudioGen, consisting of an audio encoder, a text encoder, a Transformer encoder, and an audio decoder, is an autoregressive audio generation model with textual descriptions as inputs.
AudioLM reviews high-quality audio generation as unidirectional language modeling.
In AudioLM, the input audio is mapped to semantic tokens using **W2V-BERT**[^Chung2021W2V-BERT] and acoustic tokens using **SoundStream**[^Zeghidour2021SoundStream].
Through three subsequent stages, AudioLM can accomplish speech continuation, acoustic generation, unconditional generation tasks, and so on.
The most related work to ours is **VALL-E**[^Wang2023VALL-E], which was recently proposed to utilize a neural codec language model to achieve monolingual text-to-speech synthesis.
Trained on large-scale speech data, **VALL-E** shows a strong in-content learning capability and can synthesize high-quality personalized speech prompted by a short recording of an unseen speaker.
Different from the above work, this paper focuses on cross-lingual speech synthesis, and the goal is to retain the source language speaker's voice in the synthesized speech of the target language.

</td><td>

</td></tr>
<tr><td>

**Cross-Lingual TTS**

In cross-lingual speech synthesis, the goal is to synthesize the speech of another language for a monolingual speaker, which is more challenging than conventional monolingual TTS ( [^Nachmani2019Unsupervised], [^Zhang2020Multilingual], [^Yang2022Cross-Lingual], [^Cai2023Cross-Lingual]).
By using shared phonemic input representation across languages and incorporating an adversarial objective to disentangle the speaker's identity and speech content, [^Zhang2019Learning] is able to achieve cross-lingual voice cloning within limited speakers.
[^Liu2020Multi-Lingual] also investigate the cross-lingual speech synthesis with speakers' voices enrolled in their native language.
In this system, they achieve it using a Tacotron-based synthesizer with a speaker encoder module and introduce a shared phoneme set with IPA to enhance the cross-lingual capability.
Aiming at improving the speaker similarity between the synthesized speech and the recordings of the native speaker, authors in [^Yang2022Cross-Lingual] propose multi-task learning by jointly training speaker classification and cross-lingual TTS models.
[^Cai2023Cross-Lingual] explores cross-lingual multi-speaker speech synthesis under the scenarios of sufficient and limited bilingual training data.
In the data-limited scenario, they employ a series of modules including a linguistic feature classifier, a speaker representation extractor, a non-autoregressive multi-speaker voice conversion module, and a neural vocoder, to achieve cross-lingual synthesis.
Although previous work has made considerable achievements in cross-lingual TTS, they still suffer from the issue of low speaker similarity and the lack of zero-shot ability.
In contrast, leveraging large-scale multi-lingual multi-speaker ASR data, our proposed framework with a neural codec language model demonstrates a strong in-context learning ability to alleviate the above issues.

</td><td>

</td></tr>
<tr><td>

**Speech to Speech Translation (S2ST)**

S2ST aims to translate the speech of one language to the speech of another language.
The initial research and application mainly focus on cascaded S2ST systems ( [^Lavie1997Janus-Iii], [^Nakamura2006Atr], [^Wahlster2013Verbmobil]), consisting of speech recognition (ASR), machine translation (MT), and speech synthesis (TTS) models.
Recently, end-to-end S2ST models have been explored ( [^Jia2019Direct], [^Lee2021Direct], [^Jia2021Translatotron], [^Lee2021Textless], [^Wei2022Joint], [^Huang2022TranSpeech], [^Li2022Textless]), achieving the direct conversion from source speech to target speech.
However, there is still an unsolved problem to reserve the source sound characteristics (e.g.
speaker, emotion, and the speech background) in generated speech.
This challenge is largely due to the zero-shot nature as the bilingual speech data from the same speakers are hard to collect.
Though researchers have put much effort into constructing speech-to-speech translation corpora, such as Voxvopule [^Wang2021Voxpopuli], CVSS [^Jia2022CVSS], and SpeechMatrix [^Duquenne2022SpeechMatrix], they are either synthesized from text or mined from multilingual speech corpora thus can not meet the requirement that bilingual data come from the same speakers.
At the same time, Translatotron [^Jia2019Direct] tries to synthesize target speech conditioned by the speaker embedding extracted from the source speech, but it misses richer voice information due to the limitation of the speaker embedding.
Translatotron 2 [^Jia2021Translatotron] retrains the speaker voices relying on the pseudo bilingual speech data of the same speakers generated by multi-speaker TTS systems, while the synthetic speech does not completely simulate the speech of the real world.
To address these challenges, we propose to equip the cross-lingual neural codec language model with translation modules and show its zero-shot capability to reserve the sound characteristics in the S2ST task.

</td><td>

</td></tr></table>

## 3·Cross-Lingual Codec Language Model

<table><tr><td width="50%">

In this section, we will first present the background, namely conditional codec language model **VALL-E**, and then introduce the framework of ***VALL-E X***, followed by the multi-lingual training and cross-lingual inference approaches.

</td><td>

</td></tr></table>

### 3.1·Background

<table><tr><td width="50%">

Our ***VALL-E X*** is the cross-lingual version of text-to-speech synthesizer **VALL-E**[^Wang2023VALL-E], which was recently proposed to leverage a neural codec language model to achieve text-to-speech synthesis.
Unlike conventional TTS methods that adopt the continuous regression task, e.g., mel-spectrogram generation, **VALL-E** regards TTS as a conditional language modeling task with neural codec codes, i.e.
acoustic tokens, as an intermediate representation of speech.
VALL-E employs two-stage modeling, which first generates the codec codes of the first quantizer of **EnCodec**[^Defossez2022EnCodec] from the paired phoneme sequences using an autoregressive language model, and then generates the codes of the rest quantizers in parallel using a non-autoregressive model.
After training on the large-scale English speech-transcription dataset LibriLight, **VALL-E** shows strong in-context learning capabilities.
It can generate personalized speech by taking only a 3-second speech fragment as a prompt.
Based on **VALL-E**, our ***VALL-E X*** extend to train a cross-lingual neural codec language model, enabling zero-shot cross-lingual capability and supporting cross-lingual TTS or speech-to-speech translation tasks.

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="fig_vallex_training">![](Images/2023.03.07_VALL-E_X_Fig.02.png)</a>

</td></tr>
<tr><td>

Fig.02: Training illustration of the cross-lingual neural codec language model ***VALL-E X***, consisting of a multi-lingual autoregressive codec LM ($\phi_{\mathrm{MAR}}$) and a multi-lingual non-autoregressive codec LM ($\phi_{\mathrm{MNAR}}$).
Multi-lingual acoustic tokens ($\mathcal{A}$) and phoneme sequences ($\mathcal{S}$) are converted from speech and transcription using an audio codec encoder and G2P tool, respectively.
During training, we use paired $\mathcal{S}$ and $\mathcal{A}$ from different languages to optimize these two models.

</td><td>

</td></tr></table>

### 3.2·Model Framework

<table><tr><td width="50%">

Inspired by **VALL-E**, the cross-lingual codec language model ***VALL-E X*** (denoted as $\phi$) leverages a multi-lingual autoregressive codec LM and a multi-lingual non-autoregressive codec LM to generate acoustic tokens at different granularities, as shown in the left part of [Fig.02](#fig_vallex_training).
We also adopt the neural codec model **EnCodec**[^Defossez2022EnCodec] as the acoustic quantizer, which is an encoder-decoder model with $L$ quantization layers.
We choose $L=8$ in our experiments, for each layer it produces quantized codes of 1024 entries at 75Hz.

</td><td>

</td></tr>
<tr><td>

**Multi-lingual Autoregressive Codec LM**

The multi-lingual autoregressive codec LM $\phi_{\mathrm{MAR}}$ is a unidirectional Transformer decoder that autoregressively generates acoustic tokens based on the semantic tokens (phoneme sequence).
To make the sentence-level training efficient and accelerate the decoding during inference, similar to **VALL-E**, the cross-lingual autoregressive codec LM $\phi_{\mathrm{MAR}}$ is only used to predict the acoustic tokens from the first quantizer of EnCodec model.

Formally, based on paired speech-transcription data in any language, let $\mathcal{S}$ denote the transcribed phoneme sequence, and $\mathcal{A}_{:,1} \triangleq \{a_{i,1}|i=1,\ldots,N\}$ denotes the first-layer acoustic tokens extracted from the speech $\mathcal{X}$.
The decoder $\phi_{\mathrm{MAR}}$, modeling the concatenated sequence $\left \langle \mathcal{S},\mathcal{A}_{:,1}\right \rangle$, is trained to predict $\mathcal{A}_{:,1}$ autoregressively.
It is optimized by maximizing the log-likelihood,

$$
\mathcal{L}_{\mathrm{MAR}} =- \mathrm{log}~p_{\mathrm{AR}}\left(\mathcal{A}_{:,1}\mid\mathcal{S};\phi_{\mathrm{MAR}}\right) = - \mathrm{log}~ \prod_{i=1}^{N}p\left(a_{i,1}\mid\left \langle \mathcal{S},\mathcal{A}_{<i,1}\right \rangle;\phi_{\mathrm{MAR}} \right)
$$

where $\langle \rangle$ means sequence concatenation operation, and $p(.)$ is the softmax function.

</td><td>

</td></tr>
<tr><td>

**Multi-lingual Non-Autoregressive Codec LM**

Instead of the autoregressive generation pattern, multi-lingual non-autoregressive codec LM $\phi_{\mathrm{MNAR}}$ is a non-autoregressive Transformer language model aiming at iteratively generating the rest layers of acoustic tokens from the first layer.
It is prompted by the phoneme sequence of the current sentence ($\mathcal{S}$) and the acoustic token sequence of another sentence with the same speaker ($\mathcal{\tilde{A}}$).
Here $\mathcal{\tilde{A}}$ is taken from the previous sentence in the dataset where the adjusted sentences are usually segmented from the same paragraph.
It is expected to have the same characteristics of voice (speaker, speed, background, etc) as the current sentence and is used as an additional reference for cloning the target voice.
Like **VALL-E**, for generating acoustic tokens of each layer $l\in [2,8]$, the embeddings of $l-1$ layers' acoustic tokens ($\mathcal{A}_{:,1:l-1}$) are summed up layerwise as input.
The learning objective for the $l$-layer acoustic tokens $\mathcal{A}_{:,l}$ can be calculated as <a id="eqn:ate_loss"></a>

$$
\mathcal{L}_{\mathrm{MNAR}} = \sum_{l=2}^{8} \mathrm{log}~p_{\mathrm{NAR}}\left(\mathcal{A}_{:,l}\mid\left\langle\mathcal{S},\mathcal{\tilde{A}}_{:,1:8},\mathcal{A}_{:,1:l-1}\right\rangle;\phi_{\mathrm{MNAR}}\right)
$$

where $\langle \rangle$ means the sequence concatenation.
$p_{\mathrm{NAR}}(.)$ computes the pointwise probabilities of $\mathcal{A}_{:,l}$.

</td><td>

</td></tr></table>

### 3.3·Multi-Lingual Training

<a id="ssec:bilingual_training"></a>

<table><tr><td width="50%">

In order to learn cross-lingual acoustic conversion information for cross-lingual TTS and speech-to-speech translation tasks, we take advantage of bilingual speech-transcription (ASR) corpus, pairs of ($\mathcal{S}^{s}$, $\mathcal{A}^{s}$) and ($\mathcal{S}^{t}$, $\mathcal{A}^{t}$) to train our multi-lingual codec LMs $\phi_{\mathrm{MAR}}$ and $\phi_{\mathrm{MNAR}}$, where $s$ and $t$ represent two different (source and target) languages.

Current version of ***VALL-E X*** is trained on the speech-transcription of two languages, we leave exploring more languages for future work.

</td><td>

</td></tr>
<tr><td>

**Language ID Module**

Following multi-lingual TTS, we leverage a language ID to guide the speech generation for specific languages in ***VALL-E X***.
On the one hand, without language ID, ***VALL-E X*** may be confused to select suitable acoustic tokens for the specific language since it is trained with multi-lingual data.
On the other hand, some languages have very different characteristics, for example, Chinese is a tone language while English is a non-tone language, which increases the difficulty of adjusting the speaking style across languages.
Our experiments found that adding language information to the input of our multi-lingual autoregressive codec LM $\phi_{\mathrm{MAR}}$ is surprisingly effective in guiding the right speaking style and relieving the L2 accent problem, which will be introduced in [Sec.5.5](#analysis).
Concretely, we embed language IDs into dense vectors and add them to the embeddings of acoustic tokens.

</td><td>

</td></tr></table>

### 3.4·Cross-Lingual Inference

<a id="subsec_cross_lingual_inference"></a>

<table><tr><td width="50%">

After training, ***VALL-E X*** can perform cross-lingual speech synthesis, as shown in [Fig.03](#fig_vallex_inference).
In detail, we first concatenate source phonemes $\mathcal{S}^{s}$ and target phonemes $\mathcal{S}^{t}$ as prompts, and take the first-layer source acoustic tokens $\mathcal{A}^{s}_{:,1}$ as the decoding prefix, condition on which the multi-lingual autoregressive codec LM $\phi_{\mathrm{MAR}}$ generates the first-layer target acoustic tokens $\mathcal{A}^{t}_{:,1}$, <a id="equation_ar_infenrece"></a>

$$
\hat{a}^{t}_{i,1} \sim p_{\mathrm{AR}}\left(a^t_{i,1}\mid \left\langle \mathcal{S}^{s},\mathcal{S}^{t},\mathcal{A}^{s}_{:,1},\mathcal{A}^{t}_{<i,1}\right\rangle;\phi_{\mathrm{MAR}}\right), i=1,\ldots,
$$

where $\sim$ means probability-based sampling.
The sampling is stopped until the \texttt{<end-of-sentence>} token is sampled.
As mentioned in [Sec.3.3](#ssec:bilingual_training), language ID is used to control the speaking style of the final generated speech.
After obtaining the first-layer target acoustic tokens $\mathcal{A}^{t}_{:,1}$ from $\phi_{\mathrm{MAR}}$, multi-lingual non-autoregressive codec LM $\phi_{\mathrm{MNAR}}$ is used to predict the rest layers of acoustic tokens $\left\{\mathcal{A}^{t}_{:,l}\mid l=2,\ldots,8\right\}$ by greedy search, i.e., choosing the tokens with maximum probabilities, <a id="equation_nar_infenrece"></a>

$$
\mathcal{A}^t_{:,l} = \mathop{\mathrm{argmax}}\limits_{\mathcal{A}^t_{:,l}} p_{\mathrm{NAR}}\left(\mathcal{A}^t_{:,l}\mid \left\langle \mathcal{S}^{t},\mathcal{A}^{s}_{:,1:8},\mathcal{A}^t_{:,1:l-1}\right\rangle; \phi_{\mathrm{MNAR}} \right), l=2,\ldots,8.
$$

Finally, we use the decoder of EnCodec to synthesize the target speech from the complete target acoustic tokens $\mathcal{A}^{t}_{:,1:8}$.

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="fig_vallex_inference">![](Images/2023.03.07_VALL-E_X_Fig.03.png)</a>

</td></tr>
<tr><td>

Fig.03: Inference illustration of the cross-lingual neural codec language model ***VALL-E X***, with two-stage decoding strategies.
***VALL-E X*** can support zero-shot cross-lingual TTS and zero-shot speech-to-speech translation tasks.

</td><td>

</td></tr></table>

## 4·VALL-E X Application

<table><tr><td width="50%">

***VALL-E X*** can be applied to various cross-lingual speech generation tasks.
In this paper, we take zero-shot cross-lingual TTS and zero-shot speech-to-speech translation as two examples, as illustrated in [Fig.03](#fig_vallex_inference).

</td><td>

</td></tr></table>

### 4.1·Zero-Shot Cross-Lingual TTS

<table><tr><td width="50%">

The proposed ***VALL-E X*** is naturally suitable for zero-shot cross-lingual TTS tasks.
Cross-lingual TTS tries to synthesize the target speech from text with a foreign speaker's voice.
Conventional methods mainly employ additional speaker and language networks to model the speaker and language information respectively, without zero-shot synthesis capability.
Thanks to the in-context learning capability of large language models, ***VALL-E X*** surprisingly shows the ability to perform zero-shot cross-lingual speech synthesis.
More specifically, given the source speech, source transcript, and target text, we first convert source speech into source acoustic token $\mathcal{A}^{s}$ using the encoder of neural codec model EnCodec, and convert source transcript and target text into source phonemes $\mathcal{S}^{s}$ and target phonemes $\mathcal{S}^{t}$ using G2P tool.
More specifically, as introduced in [Sec.3.4](#subsec_cross_lingual_inference), we let $\mathcal{S}^{t}$ be the phonemes extracted from the target text, $\mathcal{S}^{s}$ and $\mathcal{A}^{s}$ be the phonemes and acoustic tokens extracted from the source speech.
Then ***VALL-E X*** generates the full-layer target acoustic tokens, which are finally decompressed into the target speech by EnCodec decoder.

</td><td>

</td></tr></table>

### 4.2·Zero-Shot Speech-to-Speech Translation

<table><tr><td width="50%">

We can also apply our ***VALL-E X*** to zero-shot speech-to-speech translation tasks with additional speech recognition & translation model, which is responsible for synchronously recognizing and translating the source speech to the source and target phoneme sequences.

</td><td>

</td></tr>
<tr><td>

**Speech Recognition & Translation Model**

We leverage the improved SpeechUT [^Zhang2022SpeechUT] as our speech recognition & translation model, which is a unified-modal speech-unit-text pre-training framework using hidden units as the modality bridge between speech and text.
It supports various speech-to-text tasks, including both ASR and speech-to-text translation (ST).
Inspired by SpeechLM [^Zhang2022Speechlm] which explores different choices of units, we improve SpeechUT by replacing the clustering-based hidden units with phonemes.
Specifically, it consists of a speech encoder, a phoneme encoder, and a phoneme decoder.
All these components are pre-trained on ASR corpus (source speech $\mathcal{X}^\mathrm{{s}}$, source phoneme $\mathcal{S}^\mathrm{{s}}$) and MT corpus (source phoneme $\mathcal{S}^\mathrm{{s}}$, target phoneme $\mathcal{S}^\mathrm{{t}}$), where the phoneme sequences are converted from the text.
Please see Appendix [subsec_Appendix_speechut_td](#subsec_Appendix_speechut_td) for more pre-training details about this model.
After pre-training, the model is fine-tuned with ($\mathcal{X}^s$, $\mathcal{S}^s$, $\mathcal{S}^t$) triplet data derived from the ST corpus.
Specifically, we perform multi-task learning with the CTC [^Graves2006Connectionist] loss added on the phoneme encoder predicting the source phonemes and the cross-entropy loss on the phoneme decoder predicting the target phonemes.

</td><td>

</td></tr>
<tr><td>

**Inference**

[Fig.03](#fig_vallex_inference) shows the inference process of speech-to-speech translation.
Given a source speech $\mathcal{X}^{s}$, the speech recognition & translation model first generates the source phonemes $\mathcal{S}^{s}$ from the semantic encoder and the target phonemes $\mathcal{S}^{t}$ from the semantic decoder.
Besides, we use the EnCodec encoder to compress $\mathcal{X}^{s}$ into source acoustic tokens $\mathcal{A}^{s}$.
Then, we concatenate $\mathcal{S}^{s}$, $\mathcal{S}^{t}$, and $\mathcal{A}^{s}$, as the input of ***VALL-E X***, to produce the acoustic token sequence for the target speech, as introduced in [Sec.3.4](#subsec_cross_lingual_inference).
The generated acoustic tokens are converted to the final target speech with the decoder of EnCodec.

</td><td>

</td></tr></table>

### 4.3·Evaluation

<a id="ssec:evaluation"></a>
<table><tr><td width="50%">

The proposed model is verified using various evaluation criteria, including speaker similarity (ASV-Score), speech quality (ASR-WER), translation quality (ASR-BLEU), naturalness, and human evaluation.
Specifically, we measure speaker similarity between synthesized target speech and groud-truth target speech or source speech as an automatic speaker verification (ASV) task, where a **WavLM**[^Chen2021WavLM] based ASV model is used to calculate the score.
To verify the quality of generated speech, we first utilize the ASR system from the released **HuBERT**-Large model[^Hsu2021HuBERT] to recognize it into text.
For TTS, speech quality is measured by ASR-WER between the recognized text and the original target text.
For S2ST, speech quality is measured by ASR-BLEU between the recognized text and the provided translation text.
Finally, to better verify our proposed ***VALL-E X*** systems, we adopt the open-source NISQA [URL](https://github.com/gabrielmittag/NISQA) [^Mittag2021Deep] (the NISQA-TTS model) to evaluate the naturalness of the synthetic speech and further perform the human evaluation with manual scoring on the generated speech, e.g., mean opinion score (MOS), comparative mean opinion score (CMOS) and similar mean opinion score (SMOS).

</td><td>

</td></tr></table>

## 5·Experiments

<table><tr><td width="50%">

We evaluate the proposed model on zero-shot cross-lingual TTS including English TTS prompted by Chinese speakers and Chinese TTS prompted by English speakers, and zero-shot S2ST including Chinese$\rightarrow$English and English$\rightarrow$Chinese directions.
We provide the synthesized audio samples on our demo page to better show the performance of ***VALL-E X***.

</td><td>

</td></tr></table>

### 5.1·Dataset

<a id="ssec:data"></a>
<table><tr><td width="50%">

Our ***VALL-E X*** is trained using bilingual speech-transcription (ASR) data.
The Chinese ASR data are from WenetSpeech [^Zhang2022Wenetspeech] containing 10,000+ hours of multi-domain labeled speech.
The English ASR data are from **Libri-Light**[^Kahn2019Libri-Light] containing about 60,000 hours of unlabeled speech, whose speech data are collected from audiobooks.
We train a Kaldi [URL](https://github.com/kaldi-asr/kaldi/tree/master/egs/librispeech) ASR model on the labeled Librispeech **LibriSpeech**[^Panayotov2015LibriSpeech] dataset to generate the pseudo transcripts for the unlabeled LibriLight speech.

</td><td>

</td></tr>
<tr><td>

To train the speech recognition & translation model for S2ST, we also use additional MT and ST data.
The MT data are from AI Challenger [URL](https://challenger.ai/competition/translation), OpenSubtitles2018 [URL](https://opus.nlpl.eu/OpenSubtitles2018.php) and WMT2020 [URL](https://www.statmt.org/wmt20/translation-task.html), which contain about 13M, 10M, and 50M sentence pairs in conversion, drama [URL](http://www.opensubtitles.org/), and news domains, respectively.
The English$\rightarrow$Chinese ST data is from GigaST [^Ye2022GigaST], which is created by translating the transcripts in GigaSpeech [^Chen2021Gigaspeech] using a strong machine translation system.
Similarly, we create the Chinese$\rightarrow$English ST data by translating the transcripts of WenetSpeech using an MT model trained by ourselves on the MT data mentioned above.

</td><td>

</td></tr>
<tr><td>

We evaluate zero-shot S2ST using the Effective Multilingual Interaction in Mobile Environments (EMIME) dataset [^Wester2010Emime], which contains bilingual Chinese/English speech recorded by the same speakers.
There are 25 pairs of bilingual sentences recorded by 7 female and 7 male native Chinese speakers, thus the total number of test examples is 350.
Zero-shot cross-lingual TTS is evaluated using Librispeech **LibriSpeech**[^Panayotov2015LibriSpeech] dev-clean set and EMIME dataset providing English and Chinese data, respectively.
We have two settings in the experiments: (1) Librispeech English TTS with EMIME Chinese speech as prompts; (2) EMIME Chinese TTS with Librispeech Engish speech as prompts.

</td><td>

</td></tr></table>

### 5.2·Experimental Setup

<table><tr><td width="50%">

**Phonemization & Quantization**

The right picture of [Fig.02](#fig_vallex_training) illustrates the phonemization & quantization processes for different languages.
All text data, including ASR transcripts and MT/ST translations, are converted by the lexicon provided in ASR datasets.
We use a unified phoneme set called BigCiDian [URL](https://github.com/speechio/BigCiDian) for two languages which are based on International Phonetic Alphabet (IPA).
The ASR transcripts (or pseudo transcripts) are also converted by Kaldi force-alignment tools [URL](https://github.com/kaldi-asr/kaldi/tree/master/) for additional alignment information used for the pre-training of speech recognition & translation model.
The speech is quantized into discrete codec codes as the acoustic tokens using the neural audio codec model **EnCodec** [URL](https://github.com/facebookresearch/encodec), which employs residual vector quantization to iteratively quantize speech to a codebook according to the residual after quantization, resulting in multi-layer codebooks.

</td><td>

</td></tr>
<tr><td>

**Model Architecture**

For the cross-lingual codec language models, $\phi_{\mathrm{MAR}}$ and $\phi_{\mathrm{MNAR}}$ are both 12-layer Transformer decoders with an attention dimension of 1024 and the FFN dimension of 4096.
The autoregression is implemented by attention masking in the $\phi_{\mathrm{MAR}}$ model.
Sinuous position embedding is separately computed for each prompt sequence in $\phi_{\mathrm{MAR}}$ and $\phi_{\mathrm{MNAR}}$ models.
Besides, the $\phi_{\mathrm{MNAR}}$ model uses individual layer normalization for generating each layer of acoustic tokens.
We also introduce the model architecture of speech recognition & translation for S2ST in Appendix [subsec_Appendix_speechut_ma](#subsec_Appendix_speechut_ma).
We call our cross-lingual TTS model and S2ST model as **\our{**} and **\our{** Trans} in the subsequent experiments, respectively.

</td><td>

</td></tr>
<tr><td>

**Training Details**

We optimize each module of ***VALL-E X*** individually, including $\phi_{\mathrm{MAR}}$ and $\phi_{\mathrm{MNAR}}$.
For both modules, The maximum sentence length is set to 20 seconds, so we re-segment the LibriLight data to an average utterance duration of 12 seconds by detecting the consecutive silence phonemes.
Fortunately, the WenetSpeech data has already been segmented into short utterances.
The maximum learning rate is 5e-4 with warm-up steps of 8,000.
The models are trained on 32 V100 GPUs for 800k steps.
$\phi_{\mathrm{MAR}}$ is trained with the batch size of 120 seconds per GPU, which is 66 seconds for $\phi_{\mathrm{MNAR}}$ due to the memory constraint.
When optimizing $\phi_{\mathrm{MNAR}}$, instead of accumulating all layer's loss in Eqn.
([eqn:ate_loss](#eqn:ate_loss)), we randomly select one layer at each optimization step for efficiency.
For speech recognition & translation model, the training details can be found in Appendix [subsec_Appendix_speechut_td](#subsec_Appendix_speechut_td).

</td><td>

</td></tr>
<tr><td>

**Baselines**

We adopt [**YourTTS** [URL]](https://github.com/Edresson/YourTTS)[^Casanova2021YourTTS] as our baseline for zero-shot cross-lingual TTS.
YourTTS is a zero-shot multi-speaker TTS model for everyone, whose speaker information is based on speaker embedding extracted from a reference speech.
Since previous work shows that current end-to-end S2ST systems underperform cascaded S2ST systems [^Jia2022Translatotron], [^Lee2021Textless], we also build an S2ST baseline which is cascaded by an ASR model, an MT model, and a multi-speaker YourTTS.
The source speech serves as the reference speech when synthesizing the target speech using YourTTS.
The ASR model is the released HuBERT model introduced in [Sec.4.3](#ssec:evaluation), and the MT model is a vanilla Transformer trained by ourselves on the MT data introduced in [Sec.5.1](#ssec:data).
Since YourTTS is built only for English, we don't get its performance for English$\rightarrow$Chinese translation direction.

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="Tab:x_tts">![]()</a>

</td></tr>
<tr><td>

Tab.02: Zero-shot cross-lingual TTS evaluation for English TTS with Chinese speech as prompts and Chinese TTS with English speech as prompts, using automatic evaluation matrices, including ASV-Score (hypothesis vs.
prompt), ASR-WER, and Naturalness.

</td><td>

</td></tr></table>

### 5.3·Zero-Shot Cross-Lingual TTS Evaluation

<table><tr><td width="50%">

We first select samples with a length between 4 and 10 seconds from LibriSpeech dev-clean set, resulting in 40 speakers and 1373 samples.
For English TTS, we randomly select one audio from EMIME set as the Chinese prompt for each target sentence in LibriSpeech dev-clean set.
For Chinese TTS, we use extra 149 Chinese text sentences provided by the EMIME set and repeat them to the total number of 1373 so that they can be prompted by the LibriSpeech audios one-by-one.
When synthesizing the target language speech, the whole sequence of the source language speech is used as the prompt.

</td><td>

</td></tr>
<tr><td>

**Automatic Evaluation**

[Tab.02](#Tab:x_tts) summarizes the results of cross-lingual zero-shot TTS tasks, including English TTS prompted by Chinese speech and Chinese TTS prompted by English speech.
We measure the speaker similarity using the automatic speaker verification (ASV) model, ranging from -1 to +1 given two speech utterances.
The larger the value, the more similar the speakers of the two utterances are.
The results show that: (1) for English TTS with Chinese as prompts, the speaker similarity between the hypothesis and prompts of ***VALL-E X*** is superior to that of the baseline (0.36 vs 0.30).
(2) ***VALL-E X*** reduces the WER significantly from the baseline (from 8.53 to 4.07), demonstrating the effectiveness of our method.
(3) ***VALL-E X*** has better speech naturalness than the baseline thanks to the large-scale training data and the large language model capacity.
The results of Chinese TTS with English prompts are also listed.

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="exp:libri_spk_human">![]()</a>

</td></tr>
<tr><td>

Tab.03: Human evaluation for zero-shot cross-lingual TTS.
SMOS means similarity MOS between generated speech and prompt, and CMOS means comparative MOS based on Baseline.

</td><td>

</td></tr>
<tr><td>

**Human Evaluation**

We further conduct the human evaluation on 50 randomly selected speech records for zero-shot cross-lingual English TTS with Chinese speech as prompts, including SMOS and CMOS.
Note that SMOS ranges from 1 to 5 where the larger the value, the higher the voice similarity, and CMOS ranges from -3 to 3 where the positive number means the new system is better than the baseline.
The results are listed in [Tab.03](#exp:libri_spk_human).
Baseline gets 3.42 SMOS scores between generated speech and prompts, while our ***VALL-E X*** achieves 4.00, which further demonstrates the model's superiority in keeping the speech characteristic in the cross-lingual setting.
Moreover, to directly compare the speech synthesis quality between the proposed ***VALL-E X*** and baseline, we calculate the CMOS score between them evaluated by native speakers on the 50 sentences.
The last column of [Tab.03](#exp:libri_spk_human) shows that ***VALL-E X*** obtains the gain of +0.24 CMOS scores than the baseline.

</td><td>

</td></tr></table>

### 5.4·Zero-Shot S2ST Evaluation

<table><tr><td width="50%">

S2ST is evaluated on bidirectional Chinese$\leftrightarrow$English data of EMIME dataset, measured by speaker similarity, translation quality, speech naturalness, and human evaluation.

</td><td>

</td></tr>
<tr><td>

**Speaker Similarity**

We first evaluate whether the speaker's voice is preserved in the generated target speech using speaker similarity (ASV-Score), whose results are listed in [Tab.04](#Tab:results).
Because the EMIME test set has paired speech utterances with Chinese and English, we are able to calculate the ASV score among the generated speech (hyp), the source speech (src), as well as the target speech (tgt), resulting in 3 settings (tgt vs.
src, hyp vs.
src, and hyp vs.
tgt).
From [Tab.04](#Tab:results) we can find that: (1) For Chinese$\rightarrow$English, the ASV score of ***VALL-E X*** Trans significantly outperforms that of the conventional speaker embedding based S2ST system (Baseline), demonstrating the superiority of our model in terms of maintaining the source speaker's voice.
(2) The ASV score has similar values when the generated speech (hyp) is compared with the source speech (src) and the target speech (tgt), and it is far away from the upper bound (tgt vs.
src) for the English$\rightarrow$Chinese direction, which suggests that the cross-lingual voice transferability still has the improvement space.
(3) When directly generating speech from the ground-truth (oracle) text which degrades into cross-lingual TTS, the ASV score does not increase notably, indicating that voice transferability is less affected by the quality of translation.

</td><td>

</td></tr>
<tr><td>

**Translation Quality**

[Tab.04](#Tab:results) also shows the translation performance of ***VALL-E X*** Trans.
Note that ASR-BLEU with oracle target text as the input of ***VALL-E X*** can be seen as the upper bound when translations are exactly correct.
With oracle target text as input, ***VALL-E X*** Trans can achieve the performance of about 84$\sim$87 BLEU scores, which also reflects the high performance of our neural codec language model.
For Chinese$\rightarrow$English, ***VALL-E X*** Trans achieves higher BLEU over the baseline (30.66 vs.
27.49), demonstrating the end-to-end speech-to-phoneme translation is more effective against the conventional cascaded speech-to-text translation when applying to S2ST task.

</td><td>

</td></tr>
<tr><td>

**Speech Naturalness**

We also evaluate the Naturalness with the open-source NISQA [^Mittag2021Deep] for S2ST outputs.
As shown in the last column of [Tab.04](#Tab:results), compared to the baseline, ***VALL-E X*** Trans achieves a better naturalness score (3.54 vs.
3.44), which shows that ***VALL-E X*** can generate more natural target language speech than the baseline.

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="Tab:results">![]()</a>

</td></tr>
<tr><td>

Tab.04: S2ST performance on EMIME dataset for Chinese$\leftrightarrow$English directions.
Baseline is a cascaded S2ST system based on speaker embedding.
Automatic evaluation matrices include ASV-Score, ASR-BLEU, and Naturalness.

</td><td>

</td></tr>
<tr><td>

**Human Evaluation**

We randomly sample 56 translation pairs to perform a human evaluation using SMOS and MOS matrices for both Chinese$\rightarrow$English and English$\rightarrow$Chinese directions.
There are 14 speakers in the bilingual Chinese/English dataset, and 4 sentence pairs are chosen for each speaker, resulting in 56 translation pairs in total.
[Tab.05](#Tab:subjection_evaluation) lists the results of ***VALL-E X*** Trans as well as the Chinese$\rightarrow$English baseline.
We use MOS (from 1 to 5 scores) instead of CMOS because the translated content may be different among models, which is not suitable for CMOS evaluation.
For speaker similarity evaluation, ***VALL-E X*** Trans outperforms the baseline with 1.06 SMOS scores (4.12 vs.
3.06), demonstrating its superior ability to model speaker property of the proposed ***VALL-E X***.
Note that this value still can be improved since it is still far from the SMOS between the source speech prompt and ground truth (4.91).
For speech quality, our ***VALL-E X*** slightly outperforms the baseline in Chinese$\rightarrow$English S2ST in terms of MOS score (3.87 vs.
3.81).

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="Tab:subjection_evaluation">![]()</a>

</td></tr>
<tr><td>

Tab.05: Subjection evaluation with SMOS and MOS scores on bidirectional Chinese$\leftrightarrow$English S2ST tasks.
SMOS is measured by comparing with the ground-truth target speech.
English$\rightarrow$Chinese S2ST baseline is not reported since it is not supported by the released YourTTS.

</td><td>

</td></tr>
<tr><td>

</td><td>

</td></tr></table>

### 5.5·Analysis

<a id="analysis"></a>
<table><tr><td width="50%">

In this section, we first analyze the effect of language ID, then explore the foreign accent problems, and qualitatively investigate the ability to maintain voice emotion and synthesize code-switch speech of our proposed model.

</td><td>

</td></tr>
<tr><td>

**Effect of Language ID**

Our ***VALL-E X*** is trained with multi-lingual ASR data, which might increase the modeling difficulty for each specific language.
We address it by adding language IDs to guide speech synthesis in the autoregressive language codec model.
Here, we verify the effectiveness by removing the language ID (LID) or adding the wrong LID (i.e.
the source LID).
The ASV-Score and ASR-BLEU are reported in [Tab.06](#Tab:lid_results).
Without LID or with the wrong language ID, the translation quality decreases, while the speaker similarity between the hypothesis and source speech increases.
These results demonstrate the importance of language ID for the accuracy of the content.
It also indicates that target LID reduces the transfer of information, which means the model without LID or with source LID will better maintain the sound of the original speaker.

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="Tab:lid_results">![]()</a>

</td></tr>
<tr><td>

Tab.06: Evaluation for the effect of language ID on Chinese$\leftrightarrow$English EMIME dataset.
ASV-Score is computed between synthesized speech and source prompt speech.
The last column lists the subjection evaluation score of the foreign accent (from 1 to 5 scores).

</td><td>

</td></tr>
<tr><td>

**Foreign Accent Control**

L2 (second-language, or foreign) accent problem, the synthesized speech sounds like the accents of a foreigner, has arisen in cross-lingual TTS systems [^Zhang2019Learning], [^Lee2022Empirical].
Automatic Evaluation has shown that adding LID can boost speech quality.
Besides, we conduct a subjection evaluation to label foreign accents from 1 to 5 on randomly selected 20 synthesized speech for both English and Chinese, where each sample is measured with a score from 1 to 5 denoting high-status foreign speakers, low-status foreign speakers, middle-status speakers, low-status native speakers, and high-status native speakers, respectively.
As summarized in the last column of [Tab.06](#Tab:lid_results), we observed that our ***VALL-E X*** can control the accent for the target speech by LID modules.
For example, in English$\rightarrow$Chinese, ***VALL-E X*** Trans with right LID and without LID get the score of 4.03 and 2.35, respectively.
This indicates that by using correct LID embedding, ***VALL-E X*** Trans is able to alleviate the foreign accent problem.
Please also see the demo for audio examples of ***VALL-E X*** Trans with or without language ID.

</td><td>

</td></tr>
<tr><td>

**Voice Emotion Maintenance**

Generating the speech with a specific emotion is a difficult task for speech synthesis because conventional TTS methods require TTS data with emotion labels to train [^Um2020Emotional].
Moreover, it is more tempting to reserve the source speaker's emotion in generated target speech for the S2ST task, which is not explored in previous S2ST work.
In these experiments, we adopt the source prompts from the emotional voices dataset EmoV-DB [^Um2020Emotional] as inputs of ***VALL-E X*** Trans to generate the translated target speech, whose samples are listed on our demo page.
We found that the proposed ***VALL-E X*** can maintain emotional consistency to a certain extent between the source prompt and the synthesized speech.
The underlying reasons are (1) our ***VALL-E X*** is trained with large-scale multi-lingual multi-speaker speech-transcription data, which contains various emotional speech records, and (2) the strong in-context learning ability of ***VALL-E X***, like **GPT-3**[^Brown2020GPT-3], promotes the generated speech to reserve the characteristic of the source prompt.

</td><td>

</td></tr>
<tr><td>

**Code-Switch Speech Synthesis**

It is a common phenomenon to use code-switch utterances in bilingual or multi-lingual communities [^Cao2020Code-Switched], [^Zhao2020Towards], [^Manghat2022Normalization].
Code-switch speech synthesis aims to produce a fluent and consistent voice for code-switch text.
Although our proposed ***VALL-E X*** is trained on multiple monolingual speech data, without special optimization for code-switch setting, ***VALL-E X*** provides a promising solution to code-switch speech synthesis.
We put the code-switch samples on our demo page, demonstrating that due to its strong in-context learning ability, ***VALL-E X*** can synthesize fluent code-switch speech with a consistent voice.

</td><td>

</td></tr></table>

## 6·Conclusion

<table><tr><td width="50%">

In this work, we propose ***VALL-E X***, a cross-lingual neural codec language model, which can retrain the source language speaker's voice in the generated target language speech.
***VALL-E X*** is free of the requirement for cross-lingual paired data from the same speakers.
By training on large-scale multi-lingual multi-speaker speech-transcription data, the proposed ***VALL-E X*** demonstrates strong in-context learning capabilities and can support zero-shot cross-lingual text-to-speech and zero-shot voice-retentive speech-to-speech translation tasks.
For future work, we plan to expand this method with more data and more languages.

</td><td>

</td></tr></table>

## 7·Appendix

### 7.1·Speech Recognition & Translation Model

#### 7.1.1·Model Pre-Training

<table><tr><td width="50%">

Specifically, speech recognition & translation model consists of a speech encoder ($\theta_{enc1}$), a semantic encoder ($\theta_{enc2}$), and a semantic decoder ($\theta_{dec}$).
Given a speech waveform $\mathcal{X}^s$ and the corresponding phonemes $\mathcal{S}^s \triangleq \{s^s_i|i=1,\ldots,N\}$ where $N$ is the sequence length, the speech-side pre-training objective is to predict the phonemes from the top of the speech encoder and semantic encoder, formalized as

$$
\mathcal{L}_\mathrm{speech} =- \sum_{i\in \mathcal{M}}\left(\mathrm{log}~p\left(s^s_i|\mathcal{X}^s; \theta_{enc1}\right)+\mathrm{log}~p\left(s^s_i|\mathcal{X}^s;\theta_{enc1}, \theta_{enc2} \right)\right)
$$

where $\mathcal{M}$ is a set of masked positions, and the $p(.)$ is parameterized as the same way with original SpeechUT.
Then, given bilingual phoneme sequences, $\mathcal{S}^s$ and $\mathcal{S}^t$, the text-side pre-training objective is to perform sequence-to-sequence translation autoregressively, formalized as

$$
\mathcal{L}_\mathrm{text} =- \sum_{i=1}^{|\mathcal{S}^t|}\mathrm{log}~p\left(s^t_i|\mathcal{S}^t_{<i},\mathcal{S}^s;\theta_{enc2}, \theta_{dec} \right)
$$

In this way, each of the three components can be pre-trained with one or two learning objectives.
The final pre-training objective is $\mathcal{L}_\mathrm{pt} = \mathcal{L}_\mathrm{speech} + \mathcal{L}_\mathrm{text}$.

</td><td>

</td></tr></table>

#### 7.1.2·Model Architecture

<table><tr><td width="50%">

For the speech recognition & translation model, we leverage the Base architecture of the SpeechUT model, where all encoder/decoders consist of 6 Transformer layers with relative position bias [^Shaw2018Self-Attention].
The FFN dimension is 3072 and the attention dimension is 768.
Besides, a speech pre-net is equipped before the speech encoder, which contains several 1-D convolutional layers with 512 channels and kernel sizes of [10,3,3,3,3,2,2].
It can downsample the speech waveform by 320 and convert it to fix-dimensional embeddings.

</td><td>

</td></tr></table>

#### 7.1.3·Training Details

<table><tr><td width="50%">

The speech recognition & translation model is pre-trained following the hyper-parameter setting of [^Zhang2022SpeechUT].
The speech mask probability is 8\The embedding mixing mechanism of the original SpeechUT is also performed.
The batch sizes of speech and phonemes on each GPU are 1,400,000 (87.5 seconds) and 3,000, respectively.
The maximum learning rate is 5e-4 with warm-up steps of 32,000.
The model is pre-trained on 32 V100 GPUs for 400K steps.
After pre-training, we perform ASR/ST joint fine-tuning, where the transcription phonemes are predicted on the top of the semantic encoder through a nonlinear CTC layer, and the translation phonemes are predicted through the semantic decoder.
the transcription phonemes are reduced by removing the repetitive phonemes.
During fine-tuning, we empirically set the weight of the CTC loss to 0.2.
The models are tuned on 32 GPUs with a batch size of 2,000,000 (125 seconds) per GPU for 200K steps.

</td><td>

</td></tr></table>

## References

[^Wang2023VALL-E]: [**VALL-E**: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers](2023.01.05_VALL-E.md). ArXiv:2301.02111/TASLP2025.
[^Li2019TransformerTTS]: [**TransformerTTS**: Neural Speech Synthesis with Transformer Network](../Acoustic/2018.09.19_TransformerTTS.md). ArXiv:1809.08895v3/AAAI2019.
[^Ren2019FastSpeech]: [**FastSpeech**: Fast Robust and Controllable Text to Speech](../Acoustic/2019.05.22_FastSpeech.md). ArXiv:1905.09263/NeurIPS2019.
[^Tan2022NaturalSpeech]: [**NaturalSpeech**: End-to-End Text to Speech Synthesis with Human-Level Quality](../E2E/2022.05.09_NaturalSpeech.md). ArXiv:2205.04421v2/TPAMI2024.
[^Nachmani2019Unsupervised]: Unsupervised Polyglot Text-to-Speech. ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2019.
[^Zhang2020Multilingual]: Multilingual Speech Synthesis and Cross-Language Voice Cloning.
[^Yang2020Towards]: Towards Universal Text-to-Speech.. Interspeech 2020.
[^Ellinas2022Cross-Lingual]: Cross-Lingual Text-to-Speech With Flow-Based Voice Conversion for Improved Pronunciation. arXiv:2210.17264.
[^Cai2023Cross-Lingual]: Cross-Lingual Multi-Speaker Speech Synthesis With Limited Bilingual Training Data. Computer Speech & Language 2023.
[^Yang2022Cross-Lingual]: Cross-Lingual Text-to-Speech Using Multi-Task Learning and Speaker Classifier Joint Training. arXiv:2201.08124.
[^Zhang2019Learning]: Learning to Speak Fluently in a Foreign Language: Multilingual Speech Synthesis and Cross-Language Voice Cloning. arXiv:1907.04448.
[^Lee2022Empirical]: An Empirical Study on L2 Accents of Cross-Lingual Text-to-Speech Systems via Vowel Space. arXiv:2211.03078.
[^Kahn2019Libri-Light]: [**Libri-Light**: A Benchmark for ASR with Limited or No Supervision](../../Datasets/2019.12.17_Libri-Light.md). ArXiv:1912.07875v1/ICASSP2020.
[^Zhang2022Wenetspeech]: WENETSPEECH: A 10000+ Hours Multi-Domain Mandarin Corpus for Speech Recognition. ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2022.
[^Panayotov2015LibriSpeech]: [**LibriSpeech**: An ASR Corpus Based on Public Domain Audio Books](../../Datasets/2015.04.19_LibriSpeech.md). IEEE@ICASSP2015.
[^Wester2010Emime]: The EMIME Bilingual Database.
[^Oord2016WaveNet]: [**WaveNet**: A Generative Model for Raw Audio](../Vocoder/2016.09.12_WaveNet.md). ArXiv:1609.03499v2.
[^Kong2020HiFi-GAN]: [**HiFi-GAN**: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis](../Vocoder/2020.10.12_HiFi-GAN.md). ArXiv:2010.05646/NeurIPS2020.
[^Kong2020DiffWave]: [**DiffWave**: A Versatile Diffusion Model for Audio Synthesis](../Vocoder/2020.09.21_DiffWave.md). ArXiv:2009.09761v3/ICLR2021Oral.
[^Yang2022Diffsound]: Diffsound: Discrete Diffusion Model for Text-to-Sound Generation. arXiv:2207.09983.
[^Kreuk2022AudioGen]: [**AudioGen**: Textually Guided Audio Generation](../SpeechLM/ST2S/2022.09.30_AudioGen.md). ArXiv:2209.15352v2/ICLR2023Poster.
[^Borsos2022AudioLM]: [**AudioLM**: A Language Modeling Approach to Audio Generation](../SpeechLM/PureSpeechLM/2022.09.07_AudioLM.md). ArXiv:2209.03143v2/TASLP2023.
[^Chung2021W2V-BERT]: [**W2V-BERT**: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training](../Tokenizers/2021.08.07_W2V-BERT.md). ArXiv:2108.06209/ASRU2021.
[^Zeghidour2021SoundStream]: [**SoundStream**: An End-to-End Neural Audio Codec](../Tokenizers/2021.07.07_SoundStream.md). ArXiv:2107.03312v1/TASLP2021.
[^Liu2020Multi-Lingual]: Multi-Lingual Multi-Speaker Text-to-Speech Synthesis for Voice Cloning With Online Speaker Enrollment.. Interspeech 2020.
[^Lavie1997Janus-Iii]: JANUS-III: Speech-to-Speech Translation in Multiple Languages. 1997 IEEE International Conference on Acoustics, Speech, and Signal Processing 1997.
[^Nakamura2006Atr]: The ATR Multilingual Speech-to-Speech Translation System. IEEE Transactions on Audio, Speech, and Language Processing 2006.
[^Wahlster2013Verbmobil]: Verbmobil: Foundations of Speech-to-Speech Translation.
[^Jia2019Direct]: Direct Speech-to-Speech Translation With a Sequence-to-Sequence Model. arXiv:1904.06037.
[^Lee2021Direct]: Direct Speech-to-Speech Translation With Discrete Units. arXiv:2107.05604.
[^Jia2021Translatotron]: Translatotron 2: Robust Direct Speech-to-Speech Translation. arXiv:2107.08661.
[^Lee2021Textless]: Textless Speech-to-Speech Translation on Real Data. arXiv:2112.08352.
[^Wei2022Joint]: Joint Pre-Training With Speech and Bilingual Text for Direct Speech to Speech Translation. arXiv:2210.17027.
[^Huang2022TranSpeech]: TranSpeech: Speech-to-Speech Translation With Bilateral Perturbation. arXiv:2205.12523.
[^Li2022Textless]: Textless Direct Speech-to-Speech Translation With Discrete Speech Representation. arXiv:2211.00115.
[^Wang2021Voxpopuli]: Voxpopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation. arXiv:2101.00390.
[^Jia2022CVSS]: CVSS Corpus and Massively Multilingual Speech-to-Speech Translation. arXiv:2201.03713.
[^Duquenne2022SpeechMatrix]: SpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations. arXiv:2211.04508.
[^Defossez2022EnCodec]: [**EnCodec**: High Fidelity Neural Audio Compression](../Tokenizers/2022.10.24_EnCodec.md). ArXiv:2210.13438/TMLR2023.
[^Zhang2022SpeechUT]: SpeechUT: Bridging Speech and Text With Hidden-Unit for Encoder-Decoder Based Speech-Text Pre-Training. arXiv:2210.03730.
[^Zhang2022Speechlm]: Speechlm: Enhanced Speech Pre-Training With Unpaired Textual Data. arXiv:2209.15329.
[^Graves2006Connectionist]: Connectionist Temporal Classification: Labelling Unsegmented Sequence Data With Recurrent Neural Networks. Proceedings of the 23rd International Conference on Machine Learning 2006.
[^Chen2021WavLM]: [**WavLM**: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](../Tokenizers/2021.10.26_WavLM.md). ArXiv:2110.13900v5/JSTSP2022.
[^Hsu2021HuBERT]: [**HuBERT**: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](../Tokenizers/2021.06.14_HuBERT.md). ArXiv:2106.07447/TASLP2021.
[^Mittag2021Deep]: Deep Learning Based Assessment of Synthetic Speech Naturalness. arXiv:2104.11673.
[^Ye2022GigaST]: GigaST: A 10,000-Hour Pseudo Speech Translation Corpus. arXiv:2204.03939.
[^Chen2021Gigaspeech]: Gigaspeech: An Evolving, Multi-Domain Asr Corpus With 10,000 Hours of Transcribed Audio. arXiv:2106.06909.
[^Casanova2021YourTTS]: [**YourTTS**: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for Everyone](../E2E/2021.12.04_YourTTS.md). ArXiv:2112.02418v4/ICML2022.
[^Jia2022Translatotron]: Translatotron 2: High-Quality Direct Speech-to-Speech Translation With Voice Preservation. International Conference on Machine Learning 2022.
[^Um2020Emotional]: Emotional Speech Synthesis With Rich and Granularized Control. ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2020.
[^Brown2020GPT-3]: [**GPT-3**: Language Models are Few-Shot Learners](../TextLM/2020.05.28_GPT-3.md). ArXiv:2005.14165v4/NeurIPS2020.
[^Cao2020Code-Switched]: Code-Switched Speech Synthesis Using Bilingual Phonetic Posteriorgram With Only Monolingual Corpora. ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2020.
[^Zhao2020Towards]: Towards Natural Bilingual and Code-Switched Speech Synthesis Based on Mix of Monolingual Recordings and Cross-Lingual Voice Conversion. arXiv:2010.08136.
[^Manghat2022Normalization]: Normalization of Code-Switched Text for Speech Synthesis. Proc. Interspeech 2022 2022.
[^Shaw2018Self-Attention]: Self-Attention With Relative Position Representations. Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers) 2018.