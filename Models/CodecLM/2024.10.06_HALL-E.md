# HALL-E

<details>
<summary>基本信息</summary>

- 标题: "HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis"
- 作者:
  - 01 Yuto Nishimura,
  - 02 Takumi Hirose,
  - 03 Masanari Ohi,
  - 04 Hideki Nakayama,
  - 05 Nakamasa Inoue
- 链接:
  - [ArXiv](https://arxiv.org/abs/2410.04380)
  - [Publication]
  - [Github]
  - [Demo](https://yutonishimura-v2.github.io/HALL-E_DEMO/)
- 文件:
  - [ArXiv](../SpeechLM/_PDF/2410.04380v1__HALL-E__Hierarchical_Neural_Codec_Language_Model_for_Minute-Long_Zero-Shot_Text-to-Speech_Synthesis.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

Recently, Text-to-speech (TTS) models based on large language models (LLMs) that translate natural language text into sequences of discrete audio tokens have gained great research attention, with advances in neural audio codec (NAC) models using residual vector quantization (RVQ).
However, long-form speech synthesis remains a significant challenge due to the high frame rate, which increases the length of audio tokens and makes it difficult for autoregressive language models to generate audio tokens for even a minute of speech.
To address this challenge, this paper introduces two novel post-training approaches: 1) ***Multi-Resolution Requantization (MReQ)*** and 2) ***HALL-E***.
***MReQ*** is a framework to reduce the frame rate of pre-trained NAC models.
Specifically, it incorporates multi-resolution residual vector quantization (MRVQ) module that hierarchically reorganizes discrete audio tokens through teacher-student distillation.
***HALL-E*** is an LLM-based TTS model designed to predict hierarchical tokens of ***MReQ***.
Specifically, it incorporates the technique of using MRVQ sub-modules and continues training from a pre-trained LLM-based TTS model.
Furthermore, to promote TTS research, we create ***MinutesSpeech***, a new benchmark dataset consisting of 40k hours of filtered speech data for training and evaluating speech synthesis ranging from 3s up to 180s.
In experiments, we demonstrated the effectiveness of our approaches by applying our post-training framework to VALL-E.
We achieved the frame rate down to as low as 8 Hz, enabling the stable minitue-long speech synthesis in a single inference step.
Audio samples, dataset, codes and pre-trained models are available at [this https URL](https://yutonishimura-v2.github.io/HALL-E_DEMO/).

</td><td>

近期, 随着使用残差向量量化的神经音频编解码器模型的进步, 将自然语言文本转换为离散音频标记序列的基于大语言模型的文本转语音模型引起了广泛的研究关注.
然而, 长语音合成仍然是一个重要的挑战, 因为高帧率会增加音频标记的长度, 使得自回归语言模型甚至很难为一分钟的语音生成音频标记.
为了应对这一挑战, 本文引入了两种新颖的后训练方法: 1. 多分辨率重量化 (***MReQ***) 和 2. ***HALL-E***.

- ***MReQ*** 是用于减少预先训练的神经音频编解码器模型帧率的框架.
具体来说, 它采用多分辨率残差向量量化 (MRVQ) 模块, 通过教师-学生蒸馏的方式重新组织离散音频标记.
- ***HALL-E*** 是基于 LLM 的 TTS 模型, 旨在预测 ***MReQ*** 的层次化标记.
具体来说, 它采用 MRVQ 子模块, 并从预训练的 LLM-based TTS 模型继续训练.

为了促进 TTS 研究, 我们创建了 ***MinutesSpeech*** 新的数据集, 其中包含 40k 小时的过滤语音数据, 用于训练和评估语音合成, 范围从 3s 到 180s.
在实验中, 我们展示了我们的方法的有效性, 通过将我们的后训练框架应用于 VALL-E.
我们实现了帧率低至 8 Hz 的稳定分钟长语音合成, 使得单次推断可以实现分钟长度的语音合成.
音频样本、数据集、代码和预训练模型可在 [此 https URL](https://yutonishimura-v2.github.io/HALL-E_DEMO/) 获得.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

Recent advances in large language models (LLMs) have enabled us to model complex linguistic structures and patterns with unprecedented precision in natural language processing tasks[^01], [^02], [^03].
Motivated by these developments, LLM-based text-to-speech (TTS) models have gained significant research interest for their ability to model complex speech structures and patterns, enabling the synthesis of more natural-sounding speech in a zero-shot manner[^04], [^05], [^06], [^07], [^08], [^09], [^10].
The core concept behind LLM-based TTS models is to translate natural language text into a sequence of audio tokens, typically using frozen natural audio codec (NAC) models that quantize audio signals into discrete tokens via vector quantization techniques[^11], [^12], [^13], [^14], [^15], [^16], [^17], [^18].

</td><td>

</td></tr>
<tr><td>

Since LLMs are becoming capable of capturing long context in text[^19], [^20], [^21], LLM-based TTS models are also expected to handle long context to synthesize speech over extended periods.
However, this presents significant challenges, mainly because the length of audio tokens per second is typically large.
More specifically, when using an autoregressive language model to predict audio tokens, as in the VALL-E architecture[^04], the high frame rate at the first layer of residual vector quantization (RVQ)[^11] in NAC models often becomes a major factor that hinders the synthesis of long speech.
We refer to the number of audio tokens per second as the frame rate (Hz).
As recently discussed and investigated by[^07], [^08], reducing the frame rate is essential, but not straightforward.
Simply reducing the frame rate in NAC models results in degraded audio quality and incorrect phoneme pronunciation.
Addressing this issue requires novel solutions that bridge NAC models and LLM-based TTS models.

</td><td>

</td></tr>
<tr><td>

In this paper, we propose a novel approach to tackle the challenge of minitue-long speech synthesis in LLM-based TTS models by introducing a hierarchical post-training framework that effectively manages the trade-off between reducing frame rate and producing high-quality speech.
Our contributions are summarized as follows:
(1) We propose ***Multi-Resolution Requantization (MReQ)***, a post-training framework for hierarchically reorganizing pre-trained RVQ module to reduce the frame rate at lower quantization layers.
***MReQ*** incorporates a multi-resolution residual vector quantization (MRVQ) module into a pre-trained NAC model and continues training in a teacher-student distillation manner.
This results in reducing the frame rate at the first quantization layer to 8 Hz.

(2) We propose HALL-E, a hierarchical LLM-based TTS model designed to predict hierarchical tokens of ***MReQ***.
The AR model is trained using 8Hz tokens, while the NAR model is trained by using sub-modules in MRVQ, and continues training from a pre-trained LLM-based TTS model.

(3) We introduce ***MinutesSpeech***, a new benchmark dataset to promote TTS research, particularly for minute-long speech synthesis.
The training set consists of 40k hours of automatically filtered and balanced speech data.
The test set consists of 8 hours of speech data with transcriptions created by professional transcribers.
(4) We thoroughly conducted experiments to provide best practices for managing the trade-off between reducing frame rate and producing high-quality speech, while demonstrating the effectiveness and efficiency of our approach.
We open-source dataset, codes and pre-trained models along with audio samples at [Demo Page](https://yutonishimura-v2.github.io/HALL-E_DEMO/).

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

**Neural Audio Codec Models**

NAC models produce discrete audio tokens by quantizing audio signals.
**SoundStream**[^11] and **Encodec**[^12] are pioneering NAC models, which significantly improved compression efficiency over traditional audio codecs.
Recent studies have proposed NAC models with a focus on maintaining performance in speech processing tasks.
Examples include **SpeechTokenizer**[^14], **Descript Audio Code**[^13], **AcademiCodec**[^18], **AudioDec**[^15], **RepCodec**[^16], and **FunCodec**[^17].
Many studies have focused on reducing bps, while few have explored lowering the frame rate.
**Mimi**[^22] achieved the lowest frame rate for a NAC model, reaching 12.5Hz by incorporating Transformer models and SpeechTokenizer.
We report achieving an even lower frame rate of 8Hz, which is about 1.5 times shorter than it.

</td><td>

</td></tr>
<tr><td>

**Zero-Shot TTS**

Zero-Shot TTS aims to synthesize speech from text in the voice of a target speaker using only a short reference audio segment from that speaker.
Early models relied on speaker embeddings or speaker adaptation[^23], [^24], [^25], while recent studies have focused on LLM-based models that use NAC models in conjunction with LLMs.
VALL-E[^04] was the first LLM-based model, demonstrating impressive capabilities in zero-shot TTS tasks.
Follow-up studies have explored various extensions such as VALL-E~X for cross-lingual TTS[^05], ELLA-V using Montreal forced aligner[^06], RALL-E using prosody features[^09], VALL-E R using monotonic alignment[^07], VALL-E 2 using grouped code modeling[^08], and MELLE using mel-spectrogram features[^10].
Prosody information can also be modeled by LLMs latent language mode Mega-TTS[^27] and Mega-TTS 2[^28] introduced prosody LLMs to generate more natural prosody.
Meanwhile, diffusion-based models ({\it e.g.,} NaturalSpeech2/3[^29], [^30] and Voicebox[^31]) and prompt-based models ({\it e.g.,} Prompt-TTS2[^32], [^33]) are also known to be effective to generate high-quality controllable speech.
Beyond speech synthesis, several studies proposed audio generation models such as
UniAudio[^34],
Audiobox[^35].
In contrast, this work explores post-training methods to reduce the frame rate of LLM-based models, aiming at minute-long speech synthesis given a pre-trained NAC model such as Encodec.

</td><td>

</td></tr></table>

### Preliminaries

<table><tr><td width="50%">

**Neural Audio Codec**

A NAC model typically consists of three components: an encoder $\mathrm{Enc}(\cdot)$, a vector quantizer $\mathrm{VQ}(\cdot)$, and a decoder $\mathrm{Dec}(\cdot)$.
The quantizer is the core component that produces discrete audio tokens.
This work assumes that an RVQ module[^11] is used as the quantizer, which is defined as follows:

$$
\bm{z}_{l} = \mathrm{VQ}_{l} (\bm{x}_{l - 1}),\quad \bm{x}_{l+1} = \bm{x}_{l} - \tilde{\bm{z}}_{l},\quad \bm{h} = \sum_{l=1}^{L} \tilde{\bm{z}}_{l},
$$

where $\bm{x}_{0} = \mathrm{Enc}(\bm{x}_{\mathrm{in}}) \in \mathbb{R}^{d \times n}$ is the encoder output for the input audio $\bm{x}_{\mathrm{in}}$,
$d$ is the latent dimension,
$n$ is the sequence length,
$\mathrm{VQ}_{l}$ is a vector quantizer,
$\bm{z}_{l} \in \mathbb{N}^{n}$ is a discrete token sequence,
$\tilde{\bm{z}}_{l} = \mathrm{Emb}_{l}(\bm{z}_{l}) \in \mathbb{R}^{d \times n}$ is a sequence of embeddings corresponding to $\bm{z}_{l}$ obtained through a learnable embedding layer $\mathrm{Emb}_{l}(\cdot)$\footnote{In this paper, the tilde symbol (~$\tilde{}$~) denotes the embeddings corresponding to a token sequence.},
$l \in \{1, 2, \cdots, L\}$ is the layer index, and $L$ is the number of layers.
The output $\bm{h} \in \mathbb{R}^{d \times n}$ is then fed into the decoder to reconstruct the input audio as $\bm{y} = \mathrm{Dec}(\bm{h})$.

</td><td>

</td></tr>
<tr><td>

**LLM-based TTS**

An LLM-based TTS typically consists of two decoder-only language models: an autoregressive (AR) model $T_{\mathrm{ar}}$ and a non-autoregressive (NAR) model $T_{\mathrm{nar}}$ [^04].
The speech synthesis procedure is given by the following equations:

$$
\hat{\bm{z}}_{1} = T_{\mathrm{ar}}(\bm{t}, \bm{z}_{1}^{prompt}),\quad \hat{\bm{z}}_{l+1} = T_{\mathrm{nar}} (\bm{t}, \bm{h}^{prompt}_{L}, \hat{\bm{h}}_{l}, l),\\
\hat{\bm{h}}_{l} = \sum_{l^{\prime}=1}^{l} \hat{\tilde{\bm{z}}}_{l^{\prime}},\quad \hat{\bm{y}} = \mathrm{Dec} ([\bm{h}^{prompt}_{L}, \hat{\bm{h}}_{L}]),
$$

where $[\bm{h}^{prompt}_{L}, \hat{\bm{h}}_{L}]$ denotes the concatenation of these two matrices along the time axis.
In Eq.~(\ref{eq:valle1}), $T_{\mathrm{ar}}$ generates an audio token sequence $\hat{\bm{z}}_{1} \in \mathbb{N}^{n^{\prime}}$ corresponding to the first layer of RVQ given two inputs: a text prompt $\bm{t}$ and {an audio prompt} $\bm{z}^{prompt}_{1} = \mathrm{VQ}_{1} (\mathrm{Enc}(\bm{x}_{prompt}))$ {extracted from an audio input} $\bm{x}_{prompt}$.
In Eq.~(\ref{eq:valle2}), $T_{\mathrm{nar}}$ iteratively generates token sequences $\hat{\bm{z}}_{l+1} \in \mathbb{N}^{n^{\prime}}$ from the accumulated hidden features $\hat{\bm{h}}_{l}$ in Eq.~(\ref{eq:valle3}) and the audio prompt's hidden features $\bm{h}^{prompt}_{L}$.
Finally, in Eq.~(\ref{eq:valle4}), speech $\hat{\bm{y}}$ is generated.
Note that $\mathrm{Enc}, \mathrm{VQ}_{1}$, and $\mathrm{Dec}$ are from a frozen NAC model.

</td><td>

</td></tr>
<tr><td>

**Preliminary Experiments**

LLM-based TTS models have a predefined context window size and are typically trained with speech data ranging from several seconds to several tens of seconds.
To generate long speech segments, a straightforward approach is to reduce the frame rate in the NAC model.
However, reducing the frame rate below 48 Hz significantly decreases speech reconstruction performance as shown in
Figure~\ref{fig:preliminary}, where we evaluated the performance of Encodec[^12] in terms of the Perceptual Evaluation of Speech Quality (PESQ) scores and word error rates (WERs) as functions of frame rates.
Specifically, it is confirmed that training becomes entirely difficult at 8Hz.
Therefore, in this study, we propose a NAC model that works even at an 8Hz, demonstrating a significant improvement over existing limitations.

</td><td>

</td></tr></table>

## 3·Methodology: 方法

### MReQ: Multi-Resolution Requantization

<table><tr><td width="50%">

This section introduces ***MReQ***, a post-training framework for hierarchically reorganizing a pre-trained RVQ module to reduce the frame rate.
Specifically, ***MReQ*** incorporates a multi-resolution residual vector quantization (MRVQ) module to a pre-trained NAC model as shown in Figure~\hyperref[fig:mreq]{\ref*{fig:mreq}}, and continues training the NAC model in a teacher-student distillation manner.
For a pre-trained 48Hz Encodec model, ***MReQ*** reduces the frame rate at the first quantization layer to 8 Hz.
This enables LLM-based TTS models to handle longer contexts.

</td><td>

</td></tr></table>

#### Architecture

<table><tr><td width="50%">

The MRVQ module is a nested structure of RVQ.
Specifically, it consists of a residual structure composed of multiple low frame-rate residual vector quantization (LRVQ) blocks, each of which is itself a residual structure operating at a different frame rate.
The definition is given as follows.

**Definition 1 (MRVQ module).**
Let $\bm{x}_{0} \in \mathbb{R}^{d \times n_{0}}$ be an encoder output, where $d$ is the latent dimension and $n_{0} = T s_{0}$ is the sequence length depending on the time length $T$ (sec) and the frame rate $s_{0}$ (Hz).
The MRVQ module is defined as follows:

$$
\begin{aligned}
  \bm{c}_{k} = \mathrm{LRVQ}^{(k)}_{\alpha-\beta-\gamma} (\bm{x}_{k - 1}),\\
  \bm{x}_{k+1} = \bm{x}_{k} - \tilde{\bm{c}}_{k},\\
  \bm{h} = \sum_{k=1}^{K} \tilde{\bm{c}}_{k},
\end{aligned}
$$

where $\mathrm{LRVQ}^{(k)}_{\alpha-\beta-\gamma}$ is an LRVQ block, $K$ is the number of blocks, $\alpha-\beta-\gamma$ is a triplet of hyperparameters to determine the block structure.

**Definition 2 (LRVQ block).**

Each LRVQ block $\mathrm{LRVQ}^{(k)}_{\alpha-\beta-\gamma}$ consists of five components:
a pre-quantizer $\mathrm{PreQ}^{(k)}_{\alpha}$,
a sub-encoder $E_{k}$ for down sampling,
a main quantizer $\mathrm{Quant}^{(k)}_{\beta}$,
a sub-decoder $D_{k}$ for upsampling, and a post-quantizer $\mathrm{PostQ}^{(k)}_{\gamma}$.

The quantization procedure is given by

$$
\begin{aligned}
  \bm{a}_{k} = \mathrm{PreQ}^{(k)}_{\alpha} (\bm{x}_{k-1}),\\
  \bm{b}_{k} = \mathrm{Quant}^{(k)}_{\beta}(E_{k} ( \tilde{\bm{a}}_{k} )),
  \bm{c}_{k} = \mathrm{PostQ}^{(k)}_{\gamma} (D_{k} (\tilde{\bm{b}}_{k})),
\end{aligned}
$$

where $\bm{a}_{k}, \bm{b}_{k}, \bm{c}_{k}$ are token sequences.
The three quantizers $\mathrm{PreQ}^{(k)}_{\alpha}, \mathrm{Quant}^{(k)}_{\beta}$ and $\mathrm{PostQ}^{(k)}_{\gamma}$ are implemented using RVQ with $\alpha$, $\beta$, and $\gamma$ layers, respectively.
Note that $\bm{b}_{k} \in \mathbb{N}^{\beta \times n_{k}}$ is the token sequence representing audio in a low frame rate.
Its length is given by $n_{k} = T s_{k}$, where $s_{k}$ is the frame rate satisfying $s_{1} < s_{2} < \cdots < s_{K}$ and $s_{K} = s_{0}$.
The other two sequences $\bm{a}_{k}$ and $\bm{c}_{k}$ are used only for facilitating training of NAR  models used in LLM-based TTS models.

**Implementation details.**

Figure~\hyperref[fig:mreq]{\ref*{fig:mreq}a} shows the MRVQ module applied to the Encodec model, where the frame rate is reduced from $s_{0} = 48$ Hz to $s_{1} = 8$ Hz using $4$ LRVQ blocks.
Figure~\hyperref[fig:mreq]{\ref*{fig:mreq}b} shows the LRVQ block.
Each sub-encoder $E_{k}$ consists of a convolution layer followed by two bi-LSTM layers, which reduces the frame rate from $s_{0}$ to $s_{k}$.
Each sub-decoder $D_{k}$ consists of two bi-LSTM layers followed by a transposed convolution layer, which is symmetric to $E_{k}$.
Table~\ref{tab:mrvq_arc} lists frame rates $s_{k}$, hyperparameter triplets $\alpha-\beta-\gamma$, and strides for the convolution and transposed convolution layers.
For $k=4$, only the pre-quantize is used,  and the other components are replaced with identical functions, reducing Eqs.~(\ref{eq:lrvq2}, \ref{eq:lrvq3}) to $\bm{b}_{4} = \bm{a}_{4}$ and $\bm{c}_{4} = \bm{b}_{4}$, respectively.

</td><td>

</td></tr></table>

#### Post-Training with Teacher-Student Distillation

<table><tr><td width="50%">

Training NAC models with the MRVQ module is challenging because quantization layers with lower frame rates are prone to be ignored.
To address this, we introduce a post-training technique based on teacher-student distillation, where a NAC model pre-trained with a high frame rate serves as the teacher model.
As shown in Figure~\hyperref[fig:mreq]{\ref*{fig:mreq}a}, teacher embeddings ({in green}) are extracted from the frozen RVQ module, while student embeddings ({in purple}) are extracted from the MRVQ module.
We then minimize the feature-level distillation (FLD) loss and the hidden-state reconstruction (HSR) loss.

</td><td>

</td></tr>
<tr><td>

**FLD Loss**
The FLD loss is a mean absolute error (MAE) loss between teacher and student embeddings, defined as follows:

$$
\begin{aligned}
  L_{\mathrm{FLD}} = \sum_{({s}, {t}) \in \mathcal{P}}\lambda^{\mathrm{FLD}}_{({s}, {t})}\|{\hat{\bm{h}}_{s}} - {\bm{h}_{t}}\|_{1},\\
  \hat{\bm{h}}_{s} = \sum_{k=1}^{{s}} \tilde{\bm{c}}_{k},\\
  \bm{h}_{t} = \sum_{l=1}^{{t}} \tilde{\bm{z}}_{l},
\end{aligned}
$$

where
${\bm{h}_{s}}$ is a student embedding,
${\bm{h}_{t}}$ is a teacher embedding,
$\mathcal{P}$ is a set of student-teacher index pairs,
and $\lambda^{\mathrm{FLD}}_{({s}, {t})}$ is a weight {coefficient}.
We use $\mathcal{P} = \{({1}, {1}), ({2}, {3}), ({3}, {5}), ({4}, {8})\}$ for RVQ with eight layers and MRVQ with four LRVQ blocks.
Note that $\tilde{\bm{c}}_{k}$ and $\tilde{\bm{z}}_{l}$ are obtained from Eqs.~(\ref{eq:mrvq1}) and Eqs.~(\ref{eq:rvq1}), respectively.
The student-teacher pairs in $\mathcal{P}$ are determined so that the cumulative number of student's post-quantization layers matches that of the teacher's quantization layers.

</td><td>

</td></tr>
<tr><td>

**HSR Loss**

The HSR loss is introduced to further facilitate training of each LRVQ block:

$$
L_{\mathrm{HSR}} =
\sum_{k=1}^{K}
\lambda^{\mathrm{HSR}}_{k}
\|
\tilde{\bm{a}}_{k} -
D_{k} (\tilde{\bm{b}}_{k})
\|_{1}
$$

where $\tilde{\bm{a}}_{k}, \tilde{\bm{b}}_{k}$ are from Eqs.~(\ref{eq:lrvq1}, \ref{eq:lrvq2}), and $\lambda^{\mathrm{HSR}}_{k}$ is a weight {coefficient}.

</td><td>

</td></tr>
<tr><td>

**Total Loss**

The total loss is given by
$L_{\mathrm{total}} = L_{\mathrm{NAC}} + L_{\mathrm{FLD}} + L_{\mathrm{HSR}}$, where $L_{\mathrm{NAC}}$ is the loss used to train the NAC model.
We continue to train the encoder and decoder with the MRVQ module using a copied weight from the NAC model, which is used as a teacher model.
Compared to training from scratch, this allows for more efficient and stable convergence.

</td><td>

</td></tr>
<tr><td>

**Discussion.**
Our post-training approach is designed to be independent of the encoder-decoder architecture of the original NAC model, as we only assumed the use of RVQ.
Consequently, by utilizing state-of-the-art NAC models such as SpeechTokenizer[^14] instead of Encodec[^12], it is possible to achieve higher performance at a lower frame rate.

</td><td>

</td></tr></table>

### MinutesSpeech Benchmark Dataset

<table><tr><td width="50%">

This section introduces ***MinutesSpeech***, a benchmark dataset for minutes-long  TTS synthesis.
Unlike previous datasets such as LibriSpeech, which are primarily designed for automatic speech recognition, our dataset is curated to advance TTS research from the following three perspectives.

(1) Benchmarking Minutes-long TTS Synthesis.

We provide two subsets for benchmarking: ***MinutesSpeech-90s*** and ***MinutesSpeech-180s***, consisting of speech segments ranging from 3 seconds to 90 seconds and 3 seconds to 180 seconds, respectively.
All test audio files are under Creative Commons licenses.
For each speech segment, we provide transcriptions created by two professional native transcribers.
Since LLM-based TTS models have been typically evaluated using audio segments ranging from 4 to 10 seconds from LibriSpeech in previous studies, our dataset promotes research on longer speech synthesis by providing substantially longer segments.

(2) Balanced Audio Length for Training.

We also provide a training set consisting of 40,000 hours of audio data.
To successfully train TTS models capable of stably generating audio ranging from a few seconds to over a minute in a single inference, we carefully designed the distribution of audio lengths in the training dataset, covering durations from 3 seconds up to 180 seconds.
Automatically generated transcriptions are provided for this subset to facilitate large-scale training.

(3) Variety of Speaking Styles.

To encompass a variety of conversational speech styles, we curated data from podcasts.
Compared to the audiobook speech in LibriSpeech, synthesizing conversational speech is more challenging due to its spontaneous nature.
We believe this is an important research direction that can help bridge the gap between LLMs and LLM-based TTS models.

**Data Curation Pipeline**

To create the training set, we first curated 296,464 podcast files, ranging from 10 to 90 minutes, totaling about 186k hours.
We evaluated these audio files using the PESQ score.
The audio was segmented every 30 seconds, and the score was calculated for each segment.
We then computed the mean and standard deviation for each audio file, retaining only those with a mean score higher than 2.5 and a standard deviation lower than 0.6.
As a result, approximately 25\% of the data remained.
We then applied automatic speech recognition and speaker diarization to extract speech segments, each associated with a single speaker.
Specifically, Whisper distil Large v3\footnote{\scriptsize \url{https://huggingface.co/distil-whisper/distil-large-v3}} was used for automatic transcriptions, and Pyannote\footnote{\scriptsize \url{https://huggingface.co/pyannote/speaker-diarization-3.1}} was employed for speaker diarization.
We then segmented each audio file into segments ranging from 3 to 90 or 180 seconds.
Lastly, we removed a certain proportion of utterances with text lengths that were either too long or too short, based on their respective distributions, resulting in a final dataset of approximately 40k hours.
For the test set, we manually collected audio files under
Creative Commons licenses and asked professional transcribers to select high-quality audio and create accurate transcriptions.
Note that filtering based on the maximum and minimum text lengths used in the training set was also applied to each test set.

**Dataset Statistics**

Figure~\ref{fig:duration_dist} shows the distributions of speech durations and the number of words per segment in the test sets.
As shown, our dataset provides a diverse range of speech lengths, facilitating the development and evaluation of TTS models capable of handling varying durations and linguistic content.

</td><td>

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

**Baseline Models**

As a competitive comparison, we chose VALL-E[^04] using Encodec[^12] at 48Hz as the baseline model, which is already lower than 75Hz used in previous studies, based on our preliminary experiments (see Figure~\ref{fig:preliminary}).
We also demonstrate the effectiveness of {our approach} on SpeechTokenizer[^14].

</td><td>
</td></tr>
<tr><td>

**Datasets**

For training, we used the ***MinutesSpeech*** training set, specifically train-90s and -180s for ***HALL-E***, and train-28s, -54s, -90s, and -180s for VALL-E.
The train-28s and -54s are designed for 48Hz to match the token length in train-90s and -180s at 8Hz (see Table~\ref{tab:dataset}).
For evaluation, ***MinutesSpeech*** test-90s, -180s, and LibriSpeech test\_clean set are used.
The minimum audio length was set to 4s, while the maximum audio lengths were set to 90s, 180s, and 35s, respectively.
The audio length for prompt was consistently set to 3s.

</td><td>

</td></tr>
<tr><td>

**Evaluation Metrics**

Two evaluation metrics are used for speech reconstruction experiments: WER and PESQ.
WER is calculated using the conformer-transducer\footnote{\scriptsize \url{https://huggingface.co/nvidia/stt_en_conformer_transducer_xlarge}}.
Six evaluation metrics are used for zero-shot TTS experiments: WER, speaker similarity (SIM), deep noise suppression mean opinion score (DNSMOS), Wasserstein distance (WD) with respect to the duration distribution, subjective evaluation of naturalness (QMOS), and subjective evaluation of speaker similarity (SMOS).
SIM is calculated using WavLM-TDNN
\footnote{\scriptsize \url{https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification}}.
DNSMOS is calculated using the model trained with ground truth human ratings obtained using ITU-T P.808[^36]\footnote{\scriptsize \url{https://github.com/microsoft/DNS-Challenge/tree/master/DNSMOS}}.
WD is calculated between the duration distributions of the generated speech and the ground truth speech.
For QMOS and SMOS, 40 utterances were randomly selected from each test set, and three native English speakers rated their naturalness on a scale from 1 to 5.
We then calculated the mean and confidence intervals.

</td><td>
</td></tr>
<tr><td>

**Training Details**

Encodec and SpeechTokenizer were pre-trained using the Adam optimizer for 100k iters with a batch size of 704 for Encodec and 674 for SpeechTokenizer on four H100 GPUs, and a learning rate of $9\times 10^{-4}$.
***MReQ*** post-training was performed on a single H100 GPU for 160k iters with a batch size of 160 and a learning rate of $3\times 10^{-4}$.
VALL-E was pre-trained using the AdamW optimizer for 100k iters on four H100 GPUs.
To fully utilize GPU memory, the batch size was adjusted based on the audio length of the training samples.
A cosine annealing learning rate schedule was employed with an initial learning rate of $1 \times 10^{-4}$. ***HALL-E*** was trained using the same settings.
More details are provided in Appendix~\ref{sec:model_detail}.

</td><td>

</td></tr></table>

## 5·Results: 结果

### Speech Reconstruction

<table><tr><td width="50%">

Table~\ref{tab:codec_result} shows the speech reconstruction performance of Encodec and SpeechTokenizer before and after applying ***MReQ***.
As shown, ***MReQ*** achieved WER and PESQ scores comparable to the original values while significantly reducing the frame rate at the first quantization layer to as low as 8 Hz.

</td><td>

</td></tr></table>

### Zero-Shot Speech Synthesis

<table><tr><td width="50%">

**MinutesSpeech Evaluation**

Table~\ref{tab:MinutesSpeech_results} compares performance of zero-shot TTS synthesis on the ***MinutesSpeech*** test sets.
Overall, ***HALL-E*** significantly outperformed the VALL-E, achieving comparable or even better WER and DNSMOS scores than the ground truth audio.
This demonstrates the effectiveness of our approach.
VALL-E resulted in a high WER and a low QMOS score when trained on train-28s because it cannot generate audio longer than 28 seconds.
When trained on train-90s, the WER decreased but still remained significantly higher than that of ground truth.
This is because training an AR model with a high frame rate is unstable, indicating that simply changing the training data is not sufficient for successful training.
Our approach addressed this issue by lowering the frame rate via ***MReQ***.
In terms of SIM, VALL-E outperformed ***HALL-E*** because lowering the frame rate sacrifices acoustic information.
However, ***HALL-E*** surpassed VALL-E in SMOS, as it generates the speaker's important style element, duration, much more naturally than VALL-E (see Figure~\ref{fig:sample_wav}).

</td><td>

</td></tr>
<tr><td>

**LibriSpeech Evaluation**

Table~\ref{tab:librispeech_results} shows results on LibriSpeech for short speech synthesis up to 35 seconds.
As shown, VALL-E's WER increased as the training audio length increases.
In contrast, ***HALL-E*** achieved a WER lower than 5\% even with train-180s.
The gap in WER between the model and the ground truth is due to the difference between LibriSpeech, which primarily contains read speech, and ***MinutesSpeech***, which includes a significant amount of spontaneous speech.
Balancing speech synthesis of both read and spontaneous speech remains a challenge for future work.

</td><td>

</td></tr>
<tr><td>

**NAC Models**

Table~\ref{tab:nacmodels} compares the results obtained by Encodec and SpeechTokenizer.
As shown, all metrics except SIM improve with the use of SpeechTokenizer.
Since SpeechTokenizer aims to preserve more linguistic information in the first VQ layer, it is likely that this aligns well with maintaining linguistic information better than acoustic information when the frame rate is reduced.
These results suggest the potential for further performance enhancement by employing more advanced NAC models in the future.

</td><td>

</td></tr>
<tr><td>

**Computational Efficiency**

Table~\ref{tab:rtf} compares the real-time factor (RTF) of {VALL-E and ***HALL-E***, measured} on an RTX 4090 GPU using the 4s to 10s segments from LibriSpeech.
The results show that ***HALL-E*** generates audio approximately \textbf{3.4 times faster} than VALL-E.
In LLM-based TTS, the {computational} bottleneck typically lies in the AR model.
{Significant speed improvements were achieved by ***HALL-E*** because it reduces the number of tokens required for generation by a factor of six.
This enhancement shows promising potential not only for LLM-based TTS but also for various applications, such as spoken language modeling, as part of future work.}

</td><td>

</td></tr></table>

### Analysis and ablation studies

<table><tr><td width="50%">

**Is the hierarchical structure essential?**

The results in Table \ref{tab:arc} demonstrate the impact of different frame rates adopted in ***MReQ*** on ***MinutesSpeech*** test-90s.
For further details, please refer to Appendix~\ref{subsec:mreq_ablation}.
From this table, it is evident that the proposed method achieves the best WER, indicating the importance of gradually increasing the frame rate in a hierarchical manner.
{We also observed a trade-off between WER and SIM, as the SIM deteriorates with a reduction in the number of 48Hz layers in the proposed method.}

</td><td>

</td></tr>
<tr><td>

**Can HALL-E handle long audio prompts?**

Table~\ref{tab:prompt} shows the SIM as a function of the audio length for prompt.
The SIM values were calculated using audio segments longer than 25 seconds from the ***MinutesSpeech*** {test-90s}.
As expected, longer audio length {resulted} in better performance.
In zero-shot TTS, due to the limited context, the voice-cloning results are typically inferior to standard fine-tuning [^37].
Our proposed method has the potential to push the limits of such zero-shot TTS capabilities.

</td><td>

</td></tr>
<tr><td>

**Ablation study for MReQ:**

Table~\ref{tab:ablation_codec} shows the ablation study on the codec in the proposed method, using the LibriSpeech test set.
The results indicate that both losses and post-training play important roles in the overall performance.
It is important to note that although the impact of the FLD loss may appear minor, this does not mean that its influence on TTS is negligible.
The codec model has a hierarchical structure with redundancy, meaning that information lost at {the first layer} can be compensated for in subsequent layers.
However, when training the AR {model}, it is crucial for sufficient information to be present in {the first layer}, and {the} FLD loss is essential for achieving this.
For further details, please refer to Appendix~\ref{subsec:mreq_ablation}.

</td><td>

</td></tr>
<tr><td>

**Ablation study for HALL-E:**

Table~\ref{tab:ablation_tts} {shows} the ablation study on ***HALL-E*** on ***MinutesSpeech*** test-90s.
As shown, all the proposed methods were found to be effective.
In particular, the results of {training only with PreQ or PostQ} highlight the importance of using the MRVQ sub-modules with NAR model as pointed out in Section~\ref{sec:halle}.
This underscores the necessity of designing the codec model with downstream tasks in mind, ensuring that it is optimized for the specific requirements of those tasks.

</td><td>

</td></tr>
<tr><td>

**Qualitative Results:**

Figure~\ref{fig:sample_wav} shows the ground truth audio from ***MinutesSpeech*** test-90s and audio generated by each models.
Both ***HALL-E*** and VALL-E models were trained on ***MinutesSpeech*** train-90s.
As can be observed, ***HALL-E*** is able to generate durations very similar to the ground truth.
On the other hand, VALL-E produces an unnatural waveform with almost no silence.
Training the model at a low frame rate not only enables the generation of long speech segments but also allows for capturing more natural long-term temporal dynamics.
For more examples, please refer to Appendix~\ref{subsec:duration_analysis}.

</td><td>

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

We introduced two novel approaches for minute-long zero-shot text-to-speech synthesis: ***MReQ*** and ***HALL-E***.
***MReQ*** reduced the frame rate of the Encodec model to 8 Hz by reorganizing audio tokens via MRVQ.
***HALL-E*** efficiently synthesized minute-long speech by using 8Hz tokens in the AR model and MRVQ sub-modules in the NAR model.
We demonstrated the effectiveness of these approaches on ***MinutesSpeech***, the newly introduced dataset consisting of 40,000 hours of speech data.
Our work contributed to promote zero-shot TTS research.

</td><td>

</td></tr></table>

### Limitations and Future Work


<table><tr><td width="50%">

By reducing the frame rate to 8 Hz, our AR model can utilize longer contexts, enhancing the naturalness of the synthesized speech.
We believe that to handle extended context is particularly advantageous for larger AR models such as AudioLM[^38], SpeechGPT[^39], and PSLM[^40].
Demonstrating the effectiveness of our approach not only in TTS but also with these models remains a future work.
Furthermore, as shown in Table~\ref{tab:dataset}, we have achieved shorter audio token length than  the corresponding text token length.
However, in our current AR model, we concatenate these tokens, which results in the text tokens becoming a bottleneck in terms of sequence length.
Small-E[^41] propose methods to mitigate this issue by processing each token individually and fusing them using cross-attention.
Exploring such architectural enhancements is an important direction for future work.
Lastly, as shown in Table~\ref{tab:nacmodels}, our method brought significant improvements with SpeechTokenizer, even more so than when applied to Encodec.
It means that our approach can further enhance the objective of preserving linguistic information.
This indicates that our method could serve as a replacement for traditional SSL models[^42], [^43] or ASR encoders[^44], [^45], marking an important direction for future research.

</td><td>

</td></tr></table>

## References: 参考文献

[^01]: [GPT-3](../TextLM/2020.05.28_GPT-3.md)
[^02]: [PaLM](../TextLM/2022.04.05_PaLM.md)
[^03]: [GLM-130B](../../TextLM/GLM-130B.md)
[^04]: [VALL-E](../../SpeechLM_TTS/2023.01.05_VALL-E.md023.01.05_VALL-E.md)
[^05]: [VALL-E X](../CodecLM/2023.03.07_VALL-E_X.mdVALL-E_X.md)
[^06]: [ELLA-V](../../SpeechLM_TTS/2024.01.14_ELLA-V.md024.01.14_ELLA-V.md)
[^07]: [VALL-E R](../../SpeechLM_TTS/2024.06.12_VALL-E_R.md4.06.12_VALL-E_R.md)
[^08]: [VALL-E2](../../SpeechLM_TTS/2024.06.08_VALL-E_2.md4.06.08_VALL-E_2.md)
[^09]: [RALL-E](2024.04.04_RALL-E.md
[^10]: [MELLE](../../SpeechLM_TTS/2024.07.11_MELLE.md2024.07.11_MELLE.md)
[^11]: [SoundStream](../../SpeechCodec/2021.07.07_SoundStream.md)
[^12]: [EnCodec](../../SpeechCodec/2022.10.24_EnCodec.md)
[^13]: [DAC](../../SpeechCodec/2023.06.11_Descript-Audio-Codec.md)
[^14]: [SpeechTokenizer](../Tokenizers/2023.08.31_SpeechTokenizer.md)
[^15]: [AudioDec](../../SpeechCodec/2023.05.26_AudioDec.md)
[^16]: [RepCodec](../Tokenizers/2023.08.31_RepCodec.md)
[^17]: [FunCodec](../Tokenizers/2023.09.14_FunCodec.md)
[^18]: [HiFi-Codec](../../SpeechCodec/2023.05.04_HiFi-Codec.md)
[^19]: Longt5: Efficient text-to-text transformer for long sequences.
[^20]: Longlora: Efficient fine-tuning of long-context large language models.
[^21]: Hyperattention: Long-context attention in near-linear time.
[^22]: [Moshi](../SpeechLM/Interaction/2024.09.17_Moshi.md)
[^23]: [YourTTS](../E2E/2021.12.04_YourTTS.md)
[^24]: Neural voice cloning with a few samples.
[^25]: Sample Efficient Adaptive Text-to-Speech.
[^27]: [Mega-TTS](../SpeechLM_TTS/2023.06.06_Mega-TTS.md)
[^28]: [Mega-TTS2](../SpeechLM/ST2S/2023.07.14_Mega-TTS2.md)
[^29]: [NaturalSpeech2](../Diffusion/2023.04.18_NaturalSpeech2.md)
[^30]: [NaturalSpeech3](../Diffusion/2024.03.05_NaturalSpeech3.md)
[^31]: [Voicebox](../../FlowMatching/2023.06.23_Voicebox.md.md)
[^32]: [PromptTTS](../Acoustic/2022.11.22_PromptTTS.md)
[^33]: [PromptTTS2](../Acoustic/2023.09.05_PromptTTS2.md)
[^34]: [UniAudio](../SpeechLM/ST2S/2023.10.01_UniAudio.md)
[^35]: [Audiobox](../SpeechLM/2023.12.25_Audiobox.md)
[^36]: [ITU. P.808](../../Evaluations/P.808_DCR.md)
[^37]: [Seed-TTS](2024.06.04_Seed-TTS.md
[^38]: [AudioLM](../SpeechLM/PureSpeechLM/2022.09.07_AudioLM.md)
[^39]: [SpeechGPT](../SpeechLM/Interaction/2023.05.18_SpeechGPT.md)
[^40]: [PSLM](../SpeechLM/Interaction/2024.06.18_PSLM.md)
[^41]: [Small-E](../SpeechLM_TTS/2024.06.06_Small-E.md)
[^42]: [HuBERT](../Tokenizers/2021.06.14_HuBERT.md)
[^43]: [WavLM](../Tokenizers/2021.10.26_WavLM.md)
[^44]: [GSLM](../SpeechLM/PureSpeechLM/2021.02.01_GSLM.md)
[^45]: [Qwen-Audio](../SpeechLM/ST2T/2023.11.14_Qwen-Audio.md)
