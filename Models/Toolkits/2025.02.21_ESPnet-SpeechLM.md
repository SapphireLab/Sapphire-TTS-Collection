# ESPnet-SpeechLM

<details>
<summary>基本信息</summary>

- 标题: "ESPnet-SpeechLM: An Open Speech Language Model Toolkit."
- 作者:
  - 01 Jinchuan Tian
  - 02 Jiatong Shi
  - 03 William Chen
  - 04 Siddhant Arora
  - 05 Yoshiki Masuyama
  - 06 Takashi Maekaku
  - 07 Yihan Wu
  - 08 Junyi Peng
  - 09 Shikhar Bharadwaj
  - 10 Yiwen Zhao
  - 11 Samuele Cornell
  - 12 Yifan Peng
  - 13 Xiang Yue
  - 14 Chao-Han Huck Yang
  - 15 Graham Neubig
  - 16 Shinji Watanabe
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.15218)
  - [Publication]()
  - [Github](https://github.com/espnet/espnet/tree/speechlm)
  - [Demo]()
- 文件:
  - [ArXiv](PDF/2025.02.24-2502.15218v2__ESPnet-SpeechLM__An_Open_Speech_Language_Model_Toolkit.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

We present ***ESPnet-SpeechLM***, an open toolkit designed to democratize the development of speech language models (SpeechLMs) and voice-driven agentic applications.
The toolkit standardizes speech processing tasks by framing them as universal sequential modeling problems, encompassing a cohesive workflow of data preprocessing, pre-training, inference, and task evaluation.
With ***ESPnet-SpeechLM***, users can easily define task templates and configure key settings, enabling seamless and streamlined SpeechLM development.
The toolkit ensures flexibility, efficiency, and scalability by offering highly configurable modules for every stage of the workflow.
To illustrate its capabilities, we provide multiple use cases demonstrating how competitive SpeechLMs can be constructed with ***ESPnet-SpeechLM***, including a 1.7B-parameter model pre-trained on both text and speech tasks, across diverse benchmarks.
The toolkit and its recipes are fully transparent and reproducible at: [this https URL](https://github.com/espnet/espnet/tree/speechlm).

</td><td>

我们展示了 ***ESPnet-SpeechLM***, 一个开放的工具包，设计用于民主化语音语言模型 (SpeechLMs) 和声音驱动的智能应用的发展.
该工具包将语音处理任务作为通用序列建模问题进行标准化, 涵盖了数据预处理, 预训练, 推理和任务评估的连贯工作流.
通过 ***ESPnet-SpeechLM***, 用户可以轻松定义任务模板并配置关键设置, 实现无缝和流畅的 SpeechLM 开发.
该工具包通过为工作流的每一个阶段提供高度可配置的模块以确保灵活性, 效率和可扩展性.

为了说明其能力, 我们提供了多个使用案例, 展示了如何使用 ***ESPnet-SpeechLM*** 构建具有竞争力的 SpeechLMs, 包括一个 1.7B 参数的模型, 在文本和语音任务上进行了预训练, 适用于多种基准.

该工具包及其配方的完整透明性和可重复性可在[此链接](https://github.com/espnet/espnet/tree/speechlm)上找到.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

The advent of large language models (LLMs) has significantly advanced machine intelligence, particularly in the text domain \cite{gpt4, llama}.
As research expands beyond text, LLMs are increasingly applied to multimodal scenarios \cite{mlm_survey, gpt4o, vita}, such as speech \cite{slm_survey1, slm_survey2} and vision \cite{vlm_survey}, with the aim of achieving higher-level intelligence and enhancing human-computer interactions.
Within this context, Speech Language Models (SpeechLMs) have emerged as a powerful paradigm addressing challenges unique to speech processing.

</td><td>

</td></tr>
<tr><td>

SpeechLMs have demonstrated remarkable progress across a variety of speech tasks, including zero-shot generalization \cite{valle}, low-resource modeling \cite{speartts}, multi-task learning \cite{voxtlm, uniaudio}, instruction following \cite{instruct_following}, real-time interaction \cite{moshi, mini_omni}, and emergent abilities \cite{uniaudio1.5}.
Similarly to text-based LLMs \cite{llm_scaling}, SpeechLMs benefit from scaling data volume, parameter size, and computational resources \cite{scaling}.
These advances have fueled a growing interest in SpeechLM research within the speech and language processing community.

</td><td>

</td></tr>
<tr><td>

However, despite these advances, the development of SpeechLMs remains a complex and resource-intensive endeavor \cite{moshi}.
Building such models requires significant expertise and effort across diverse tasks, from data preparation to training, inference, and evaluation.
To address these challenges and democratize SpeechLM research, we introduce ***ESPnet-SpeechLM***, an open-source toolkit designed to streamline and accelerate SpeechLM development.

</td><td>

</td></tr>
<tr><td>

***ESPnet-SpeechLM*** unifies speech tasks under a sequential modeling framework and organizes the SpeechLM development process into a standardized workflow.
As illustrated in Fig.\ref{fig:overall_workflow}, users begin by defining a custom task template, followed by configuring key parameters.
The toolkit then automates all phases of the pipeline: preprocessing, training, inference, and evaluation (Section.\ref{workflow}).
This modular workflow supports a wide range of design choices, including tokenization methods, model architectures, dynamic multi-tasking, etc.
In addition, ***ESPnet-SpeechLM*** provides a HuggingFace-compatible interface for sharing datasets and models (Section.\ref{feature}).
The toolkit is fully open-source, ensuring reproducibility and accessibility.

</td><td>

</td></tr>
<tr><td>

To showcase its versatility, we present several use cases demonstrating the scalability and efficiency of ***ESPnet-SpeechLM***.
These include building competitive SpeechLM-based automatic speech recognition (ASR) and text-to-speech (TTS) systems on datasets exceeding 200k hours of speech-text paired data (Section.\ref{asrtts_exp}).
We also detail the creation of a 1.7B-parameter multi-task SpeechLM, pre-trained on ASR, TTS, TextLM, and AudioLM tasks, by leveraging 240 billion \textit{text tokens or audio frames} (Section.\ref{multitask_exp}).

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

The ***ESPnet-SpeechLM*** toolkit builds upon prior works in two main directions:

</td><td>

</td></tr>
<tr><td>

**Text LLM ecosystem**

Some popular development tools in text LLM ecosystems can be generalized to any large-scale sequential modeling task, which means they are also suitable for SpeechLM training.
Examples of this include DeepSpeed \cite{deepspeed} and FlashAttention \cite{flashattention}.
To preserve text capability, it is common to initialize SpeechLMs from pre-trained text LLMs, which can rely on open-source platforms like HuggingFace Transformers\footnote{\url{https://github.com/huggingface/transformers}}.
These tools are integrated into ***ESPnet-SpeechLM***.
We also noticed that current text LLM training frameworks \cite{megatron, llamafactory} provide limited support for speech features.
Our toolkit is presented as a supplement in this direction.

</td><td>

</td></tr>
<tr><td>

**Open-Sourced SpeechLMs and Speech Toolkits**

Current research on SpeechLMs and their transparency has been significantly advanced by prior open-source SpeechLM research works \cite{glmvoice, mini_omni, moshi, uniaudio, voxtlm}.
SpeechLM research also greatly benefits from general speech processing and sequence-to-sequence modeling toolkits \cite{espnet, speechbrain, amphion, s3prl, nemo, fairseq}, as they provide a wide range of components applicable to SpeechLM development.
***ESPnet-SpeechLM*** is presented as a combination of cutting-edge SpeechLM research and well-established speech processing techniques within the open-sourced community.
More specifically, it is built upon the existing ESPnet \cite{espnet} codebase to better exploit prior community efforts and compare with existing non-SpeechLM works.
We summarize ***ESPnet-SpeechLM*** and related codebases in Tab.\ref{tab:comparison}.

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

This section outlines the hierarchical design of the ***ESPnet-SpeechLM*** toolkit.
We first introduce the fundamental concepts of SpeechLMs in Section.\ref{background} followed by a detailed description of the ***ESPnet-SpeechLM*** workflow in Section.\ref{workflow}.
Lastly, we highlight key features of our toolkit in Section.\ref{feature}.

</td><td>

</td></tr></table>

### Speech Language Model: 语音语言模型

<table><tr><td width="50%">

Speech tasks can be generically formulated as predicting target sequences $\mathbf{y} = [\mathbf{y}_1, ..., \mathbf{y}_N]$ given input conditions $\mathbf{x} = [\mathbf{x}_1, ..., \mathbf{x}_M]$, where each $\mathbf{x}_m$ and $\mathbf{y}_n$ represents individual data items.
$M$ and $N$ stand for the number of data items in conditions and targets, respectively.
E.g., for ASR, $\mathbf{x}_1$ is the input speech; $\mathbf{y}_1$ is the corresponding transcription.
Commonly, the training objective is to maximize the posterior $P(\mathbf{y}|\mathbf{x})$.

</td><td>

</td></tr>
<tr><td>

***ESPnet-SpeechLM*** uniformly frames speech tasks as sequential modeling problems using auto-regressive prediction over discrete token sequences within a decoder-only Transformer \cite{transformer}.
Specifically, all data items $\mathbf{x}_m$ and $\mathbf{y}_n$ are first tokenized into discrete token sequences $\mathbf{x}_m^{\text{d}}$ and $\mathbf{y}_n^{\text{d}}$.
Then, the spliced sequence $\mathbf{s}^{\text{d}} = [\mathbf{x}_1^{\text{d}}, ..., \mathbf{x}_M^{\text{d}}, \mathbf{y}_1^{\text{d}}, ..., \mathbf{y}_N^{\text{d}}]$ serves as the input for model training.
Cross-entropy loss optimization over $\mathbf{y}_1^{\text{d}}, ..., \mathbf{y}_N^{\text{d}}$ approximates the objective $P(\mathbf{y}|\mathbf{x})$.
Predicting $\hat{\mathbf{y}}_1^{\text{d}}, ..., \hat{\mathbf{y}}_N^{\text{d}}$ based on the conditions $\mathbf{x}_1^{\text{d}}, ..., \mathbf{x}_M^{\text{d}}$ and then detokenizing them into  $\hat{\mathbf{y}}_1, ..., \hat{\mathbf{y}}_N$ yield the final system prediction.

</td><td>

</td></tr>
<tr><td>

***ESPnet-SpeechLM*** specifically supports multi-stream language models, i.e., $\mathbf{s}^{\text{d}}\in\mathbb{N}^{T \times n_q}$ where $T$ stands for the sequence length and $n_q$ stands for the number of streams (See Fig.\ref{fig:template}).
This capability is especially critical for audio codec models \cite{encodec, soundstream}, which encode each audio frame into multiple tokens.
Padding tokens are added to align non-audio data when splicing $\mathbf{s}^{\text{d}} = [\mathbf{x}_1^{\text{d}}, ..., \mathbf{x}_M^{\text{d}}, \mathbf{y}_1^{\text{d}}, ..., \mathbf{y}_N^{\text{d}}]$.
These multi-stream models require specialized design considerations discussed in Section.\ref{feature}.

</td><td>

</td></tr></table>

### Workflow: 工作流

<table><tr><td width="50%">

The ***ESPnet-SpeechLM*** workflow begins with single-task scenarios and extends naturally to multitask training.
In the following, we introduce the concept of the task template (Section.\ref{template}) and describe the end-to-end pipeline from preprocessing to evaluation (Section.\ref{preprocessing}-\ref{evaluation}).
Multitasking is described in \ref{multi-task}.

</td><td>

</td></tr>
<tr><td>

#### Task Template: 任务模板

As in Section.\ref{background}, regardless of the exact sequence $\mathbf{s}^{\text{d}}$, the SpeechLM performs sequential modeling indiscriminately.
It is the definition of conditions $\mathbf{x}$, targets $\mathbf{y}$, and the corresponding tokenization methods that give the distinctive composition of $\mathbf{s}^{\text{d}}$ and thus the support of different tasks within SpeechLMs.

To handle different speech tasks uniformly, ***ESPnet-SpeechLM*** defines each task using a \textit{task template}, which specifies the composition of the training sequence $\mathbf{s}^{\text{d}}$.
As shown in Fig.\ref{fig:template}, the task template defines the name of the \textcolor{red}{task}, the conditions, and the targets.
For each data item $\mathbf{x}_m$ or $\mathbf{y}_n$, the \textcolor{blue}{item\_name} and \textcolor{ForestGreen}{tokenizer} are specified.
The training sequence starts from the \textcolor{red}{task} identifier, followed by the tokenized sequences from all the data items.
For each data item, the first token is a \textcolor{Mulberry}{tokenizer indicator}; the raw data is tokenized by the specified \textcolor{ForestGreen}{tokenizer}.
For single-stream data items and special tokens, \textcolor{Gray}{padding tokens} are added.
With this template, the training sequences can be assembled automatically from the given raw data.

</td><td>

</td></tr>
<tr><td>

#### Preprocessing: 数据预处理

Preprocessing primarily involves tokenization.
As some tokenizers are neural network-based and heavy, it is more efficient to conduct the tokenization offline.
The tokenization is handled automatically by ***ESPnet-SpeechLM*** after receiving a folder for each train/valid/test set as follows.
Specifically, an index file is provided for each data item, with the format of \texttt{example-id content} in each line.
The name of these index files should correspond to the \textcolor{blue}{item\_name} in the task template.

```
- (folder) train
  - (file) wav
    - (line) example-id1 path-to-wav1
    - (line) example-id2 path-to-wav2
  - (file) text
    - (line) example-id1 text1
    - (line) example-id2 text2
```

***ESPnet-SpeechLM*** processes these files to generate a unified \texttt{data.json} file for each dataset, which contains tokenized results and metadata.
\texttt{data.json} is the data format in both training and evaluation.
During preprocessing, all tokenizers in use are detected, and a joint vocabulary is constructed automatically.

</td><td>

</td></tr>
<tr><td>

#### Training: 训练

The training behavior of ***ESPnet-SpeechLM*** is specified by a configuration file.
Besides common training configurations like optimization, batch size, and distributed training setup, the toolkit also supports flexible model architecture configurations for SpeechLM development.

***ESPnet-SpeechLM*** provides multiple implementations of multi-stream language models \cite{valle, musicgen, uniaudio}.
The implementations of multi-stream language models all rely on the Transformer body implementation.
We provide the ESPnet built-in Transformer implementation to maximize flexibility;
alternatively, we support any \texttt{AutoModelForCausalLM} from HuggingFace Transformers to leverage pre-trained text LLMs
Also, following \citet{moshi}, custom weights can be provided during loss computing to balance the tokens from different tokenization methods.
This is usually to guarantee one audio frame has the same loss weight as one non-audio token.
Lastly, in addition to applying the cross-entropy loss, the toolkit also supports reinforcement learning from human feedback (RLHF) for SpeechLMs (see \cite{dpo} for details).

</td><td>

</td></tr>
<tr><td>

#### Inference: 推理

For each of the supported multi-stream language models, we provide multiple inference methods, such as greedy search, beam search, and top-k/top-p sampling.
Our implementation also allows multiple heuristics like the min/max generation length.
One important heuristic is essential to SpeechLM: unlike text LLMs that only predict text, SpeechLMs need to know the modality of the current predicting target, so that tokens from other modalities can be filtered out to avoid invalid predictions.
The current modality is known from the most recent tokenizer indicator (Section.\ref{template}), and will switch when a new tokenizer indicator is predicted.

</td><td>

</td></tr>
<tr><td>

#### Evaluation: 评估

We create an evaluation script for each supported task.
Within these scripts, we consistently adopt the VERSA \footnote{\url{https://github.com/shinjiwlab/versa}}, a comprehensive collection of >60 speech and audio evaluation metrics \cite{espnet_codec}.
Besides the existing evaluation scripts, a model in a new task can be evaluated simply by specifying the metrics, the inference results, and the reference (if needed).

</td><td>

</td></tr>
<tr><td>

#### Multitasking: 多任务学习

To build SpeechLMs with versatile functionalities, ***ESPnet-SpeechLM*** flexibly supports multitasking.
As the SpeechLMs have the same modeling procedure for all tasks, achieving multitasking training is to fuse the training sequences from different tasks in the mini-batches.
Similar to single-task training, for each task, the task template definition (Section.\ref{template}) and preprocessing (Section.\ref{preprocessing}) are completed separately, which gives multiple tokenized datasets and the corresponding \texttt{data.json} files\footnote{
  Especially, these preprocessing works are easy to distribute and are suitable for collaborative works.
}.
The data loader accepts a list of \texttt{data.json} and fuses these datasets before training, which allows the users to dynamically change the multitasking data setups.
Mini-batches are sampled from the fused datasets during training.
Additionally, the sampling ratio among these datasets is adjustable to emphasize some specific data portions.

</td><td>

</td></tr></table>

### Supported Features: 支持特性

<table><tr><td width="50%">

We summarize the core configurable features in ***ESPnet-SpeechLM*** workflow in Tab.\ref{tab:features} and highlight them as follows:

</td><td>

</td></tr>
<tr><td>

**Tokenization**

For text, we support subword models and grapheme-to-phoneme (G2P) tools, with an emphasis on HuggingFace tokenizers.
For audio tokenization, we support both audio codec models and self-supervised learning (SSL) tokens.
We provide multiple options for these two tokenization methods, with an emphasis on ESPnet-Codec \cite{espnet_codec} and XEUS \cite{xeus}.
Additionally, we find that concatenating codec and SSL tokens frame-by-frame behaves well in both speech understanding and generation.
Besides text and audio, these multi-modal models can leverage information from auxiliary modalities, such as music score \cite{muskits}, vision token \cite{avhubert}, classification labels (e.g., bool, time-stamp), speaker-identity and the continuous LLM embeddings.


</td><td>

</td></tr>
<tr><td>

**Training**

As in Section.\ref{training}, for the Transformer body, we provide the ESPnet built-in implementation as well as the HuggingFace Transformers implementation.
Upon the Transfomer, we support 4 distinctive multi-stream language model implementations \cite{valle, uniaudio, musicgen}.
For training efficiency, we leverage DeepSpeed \cite{deepspeed}, FlashAttention \cite{flashattention} and Liger-Kernel\footnote{\url{https://github.com/linkedin/Liger-Kernel}}.
These modules enable us to achieve model FLOPs utility (MFU) \cite{mfu} as high as 35\% with multi-node training using NVIDIA H100 GPUs.

</td><td>

</td></tr>
<tr><td>

**Inference, Evaluation, and Sharing**

For all supported architecture, we provide all 4 inference methods.
VERSA provides more than 60 speech-related evaluation metrics.
To ensure transparency and reproducibility, the code and task templates are released through the ESPnet GitHub repository; tokenized datasets and pre-trained models are released through ESPnet Huggingface Hub.

</td><td>

</td></tr></table>

## 4·User Cases: 用例

<table><tr><td width="50%">


This section provides several user cases to demonstrate the performance of SpeechLMs built from ***ESPnet-SpeechLM***.
We first build single-task SpeechLM-Style ASR and TTS models in Section.\ref{asrtts_exp}.
As a highlight of this demo, we present a 1.7B pre-trained SpeechLM that covers 4 tasks similar to \cite{voxtlm}: ASR, TTS, text auto-regressive prediction (TextLM), and speech auto-regressive prediction (AudioLM) (Section.\ref{multitask_exp}).
These models are released through ESPnet HuggingFace Hub\footnote{\url{https://huggingface.co/espnet}}.

</td><td>

</td></tr></table>

### Experimental Setup: 实验设置

<table><tr><td width="50%">

**Model and Tokenization**

We consistently leverage the pre-trained text LLM, SmolLM2 series\footnote{\url{https://huggingface.co/HuggingFaceTB}}, for SpeechLM initialization.
We adopt the 360M and 1.7B versions for single-task and multi-task models, respectively.
We adopt delay interleave \cite{musicgen} as the multi-stream language model architecture.
In terms of tokenization, we adopt the Codec\_SSL method
(Section.\ref{feature}) for speech representation.
To preserve full transparency and self-consistency, ESPnet-Codec\footnote{\url{https://huggingface.co/ftshijt/espnet\_codec\_dac\_large\_v1.4\_360epoch}}
and XEUS\footnote{\url{https://huggingface.co/espnet/xeus}; K-Means tokenizer trained on its last layer of representation using 5k clusters} are adopted for codec and SSL tokenizers respectively.

</td><td>

</td></tr>
<tr><td>

**Data, Training, and Inference**

We collect open-sourced data for all experiments.
Our data contains 200k hours of speech and 115B tokens of text, most in English.
When expanding speech data into ASR, TTS, and AudioLM tasks, this is equivalent to 240B \textit{text tokens or audio frames}.
Detailed data composition is in Appendix \ref{sec:appendix2}.
We balance the weights for text, SSL, and codec tokens as 1: 0.5: 0.0625\footnote{Each audio frame is represented by 1 SSL token and 8 codec tokens.
This ratio is to ensure (1) the text tokens have the same weight as the audio frames, and (2) SSL tokens have the same weight as 8 codec tokens combined.}.
The training used 8/24 H100 GPUs for single/multi-task training.
We use batch size as large as around 2M \textit{frames or tokens} and a constant learning rate of 2e-4, with 10k warmup steps.
We train the model for 2 data passes.
We use greedy search for ASR and top-k sampling for TTS ($k=30, temperature=0.8$).

</td><td>

</td></tr>
<tr><td>

**Evaluation**

Following \cite{voxtlm, dpo}, we test word error rate (WER) for ASR; ASR WER, Speaker Similarty and Proxy MOS for TTS; perplexity for AudioLM.
We measure the TextLM ability using popular metrics like MMLU \cite{mmlu}, ARC-Challenge (ARC-C) \cite{arc}, HellaSwag (HS)\cite{hellaswag}, and OpenBookQA (OBQA) \cite{openbookqa}.

</td><td>

</td></tr></table>

### ASR and TTS Experiments

<table><tr><td width="50%">

We evaluate our ASR system on multiple benchmarks and compare it with the popular open-sourced ASR models: whisper-v3-large \cite{whisper} and OWSM v3.1-medium \cite{owsm3.1}.
As suggested in Tab.\ref{tab:asr}, our SpeechLM-based ASR system achieves comparable results in English with these two popular speech recognizers even using much fewer parameters.
In Tab.\ref{tab:tts}, we compare the ***ESPnet-SpeechLM*** TTS system with other discrete-based TTS systems\footnote{For VallE-X and VallE 2, we use the third-party implementations: \url{https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt}, \url{https://huggingface.co/amphion/valle}}.
The results suggest our TTS system achieves decent performance on all evaluation metrics.

</td><td>

</td></tr></table>

### Multi-Task Experiments

<table><tr><td width="50%">

We demonstrate the performance of our multitask pre-trained SpeechLM in Tab.\ref{tab:lm}.
Compared with other SpeechLMs \cite{voxtlm, mini_omni, glmvoice, llamaomni} and multimodal LMs \cite{vita}, our pre-trained model still preserves decent ASR, TTS and AudioLM performance even with limited parameter budget.
In terms of text capability, the pre-trained model preserves close performance compared with the text-only LLM LLaMA-3.2-1B \cite{llama}.

</td><td>

</td></tr></table>

## 5·Future Work: 未来工作

<table><tr><td width="50%">

We will continue the development of the ***ESPnet-SpeechLM*** toolkit, such as supporting more tokenization methods, more task templates, more modeling options, and LLM inference engines \cite{vllm}.
We are also interested in applying this toolkit to our SpeechLM research.
For pre-training, we are interested in larger-scale models and models that can capture rich paralinguistic information in speech.
For post-training, we are interested in achieving conversational interactions, speech-based instruction following ability, and even agent-alike behaviors.
Our plan also includes real-time and duplex design, HFRL for SpeechLM and SpeechLMs that trained from flat start.

</td><td>

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

This demo presents ***ESPnet-SpeechLM***, a toolkit that covers the whole workflow of speech language model development, with comprehensive support in multiple design choices.
We also provide user cases for both single-task and multi-task training, showing competitive performance with other models in the market.
The toolkit promises to keep full transparency in data, code, recipes, and pre-trained models.

</td><td>

</td></tr></table>
