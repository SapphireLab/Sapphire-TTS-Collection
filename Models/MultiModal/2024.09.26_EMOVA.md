# EMOVA (EMotionally Omni-present Voice Assistant)

<details>
<summary>基本信息</summary>

- 标题: "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions"
- 作者: 
  - 01 Kai Chen
  - 02 Yunhao Gou
  - 03 Runhui Huang
  - 04 Zhili Liu
  - 05 Daxin Tan
  - 06 Jing Xu
  - 07 Chunwei Wang
  - 08 Yi Zhu
  - 09 Yihan Zeng
  - 10 Kuo Yang
  - 11 Dingdong Wang
  - 12 Kun Xiang
  - 13 Haoyuan Li
  - 14 Haoli Bai
  - 15 Jianhua Han
  - 16 Xiaohui Li
  - 17 Weike Jin
  - 18 Nian Xie
  - 19 Yu Zhang
  - 20 James T. Kwok
  - 21 Hengshuang Zhao
  - 22 Xiaodan Liang
  - 23 Dit-Yan Yeung
  - 24 Xiao Chen
  - 25 Zhenguo Li
  - 26 Wei Zhang
  - 27 Qun Liu
  - 28 Lanqing Hong
  - 29 Lu Hou
  - 30 Hang Xu
- 链接: 
  - [ArXiv](https://arxiv.org/abs/2409.18042)
  - [Publication]()
  - [Github]()
  - [Demo](https://emova-ollm.github.io)
- 文件: 
  - [ArXiv](_PDF/2409.18042v1__EMOVA__Empowering_Language_Models_to_See_Hear_and_Speak_with_Vivid_Emotions.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

GPT-4o, an omni-modal model that enables vocal conversations with diverse emotions and tones, marks a milestone for omni-modal foundation models.
However, empowering Large Language Models to perceive and generate images, texts, and speeches end-to-end with publicly available data remains challenging in the open-source community.
Existing vision-language models rely on external tools for the speech processing, while speech-language models still suffer from limited or even without vision-understanding abilities. 
To address this gap, we propose ***EMOVA (EMotionally Omni-present Voice Assistant)***, to enable Large Language Models 
with end-to-end speech capabilities while maintaining the leading vision-language performance.
With a semantic-acoustic disentangled speech tokenizer, we notice surprisingly that omni-modal alignment can further enhance vision-language and speech abilities compared with the corresponding bi-modal aligned counterparts.
Moreover, a lightweight style module is proposed for flexible speech style controls (\eg, emotions and pitches).
For the first time, ***EMOVA*** achieves state-of-the-art performance on both the vision-language and speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue with vivid emotions. 

## 1.Introduction: 引言

OpenAI GPT-4o~\citep{openai2024gpt4o}, a new milestone for omni-modal foundation models, has rekindled people's attentions on intelligent assistants that can \textit{see} (\ie, perceiving fine-grained visual inputs), \textit{hear} (\ie, understanding vocal instructions) and \textit{speak} (\ie, generating vocal responses) simultaneously.
Most existing Multi-modal Large Language Models (MLLMs) focus on two modalities only, either vision-language~\citep{bai2023qwen,li2024llavaonevisioneasyvisualtask} or speech-language~\citep{chu2024qwen2audiotechnicalreport,xie2024mini}, demonstrating severe demands for omni-modal models with visual, language and speech abilities.
How to effectively empower Large Language Models (LLMs) to process omni-modal data in an end-to-end manner remains an open question. 

Existing omni-modal LLMs~\citep{chen2024internvlscalingvisionfoundation,fu2024vitaopensourceinteractiveomni} generally build upon Vision LLMs and integrate the speech modality by adopting a speech encoder like Whisper~\citep{radford2022robustspeechrecognitionlargescale}, which extracts \textbf{continuous} features from speech, similar to how images are processed, and enables speech understanding. 
However, these models still rely on external Text-to-Speech (TTS) tools for generating speech responses, limiting their ability to support real-time interactions.
AnyGPT~\citep{zhan2024anygptunifiedmultimodalllm}, instead, opts for a fully \textbf{discretization} manner, which first discretizes all data modalities (\ie, images, texts, and speeches), followed by omni-modal auto-regressive modeling. 
This enables AnyGPT to handle multiple modalities with a \textbf{unified end-to-end} framework, facilitating \textbf{real-time interactions} with the help of \textbf{streaming decoding}.
However, the discrete vision tokenizer adopted by AnyGPT struggles to capture visual details, especially for high-resolution images, making it far behind its continuous counterparts on vision-language benchmarks.
Moreover, none of the existing works explore speech style controls (\eg, emotions and pitches) with LLMs.
Therefore, our question arises: \textit{How to build an end-to-end omni-modal LLM enabling spoken dialogue with vivid emotions while maintaining state-of-the-art vision-language performance?}

In this paper, we propose ***EMOVA (EMotionally Omni-present Voice Assistant)***, a novel end-to-end omni-modal LLM with state-of-the-art vision-language and speech capabilities while supporting emotional spoken dialogue.
Fig.~\ref{fig:framework} shows an overview of the model framework.
A \textit{continuous} vision encoder captures the fine-grained visual details, while the \textit{discrete} speech tokenizer and detokenizer enable the end-to-end speech understanding and generation. 
Specifically, the speech-to-unit (S2U) tokenizer converts the input speech waveforms into discrete speech units as LLM inputs, while the unit-to-speech (U2S) detokenizer reconstructs the speech waveforms from the LLM's output speech units.
To seamlessly integrate the speech modality with LLMs, we meticulously design a \textbf{semantic-acoustic disentangled} speech tokenizer to decouple the semantic contents and acoustic styles of the input speeches~\citep{tao2024toneunitspeechdiscretizationapproach}, where 
1) \textit{semantic content} (\ie, what it says) captures the semantic meanings of input speeches, which is finally discretized and aligned with LLMs, while 
2) \textit{acoustic style} (\ie, how it says) captures the diverse speech styles (\eg, emotions and pitches).

Utilizing the semantic-acoustic disentanglement of our speech tokenizer, we further introduce a lightweight style module to support spoken dialogue with vivid emotions and pitches.
As in Sec.~\ref{sec_speech_in}, this innovative disentanglement design better facilitates the modality alignment between texts and speeches while maintaining flexibility for diverse speech style controllability and personalization.

With the end-to-end omni-modal architecture of ***EMOVA***, we empirically demonstrate that publicly available bi-modal image-text and speech-text data are sufficient for omni-modal alignment, utilizing the text modality as a bridge. This eliminates the need for omni-modal data (\ie, image-text-speech), which is usually scarce. Surprisingly, we find that omni-modal alignment can further improve both vision-language and speech capabilities through joint optimization, even when compared with their bi-modal aligned counterparts.
Finally, only a small amount of mixed-modality samples are required to teach the model to respond in the desired format.
For the first time, ***EMOVA*** achieves state-of-the-art performance on both vision-language and speech benchmarks (see Table~\ref{tab:compare} for comparisons).
The main contributions of this work contain three parts:

- We propose ***EMOVA***, a novel end-to-end omni-modal LLM that can see, hear and speak. We use a continuous vision encoder and a semantic-acoustic disentangled speech tokenizer for seamless omni-modal alignment and diverse speech style controllability. 
- We introduce an efficient text-centric omni-modal alignment which can further improve the vision-language and speech capabilities, even compared with the corresponding bi-modal aligned counterparts (\ie, image-text only and speech-text only alignment).
- For the first time, our ***EMOVA*** achieve state-of-the-art comparable performance on both the vision-language and speech benchmarks simultaneously, while supporting flexible spoken dialogues with vivid emotions.

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

This work presents ***EMOVA***, an innovative end-to-end omni-modal large language model that effectively aligns vision, speech, and text simultaneously.
We employ a continuous vision encoder to capture fine-grained visual details, while a discrete, semantic-acoustic disentangled speech tokenizer and detokenizer enable end-to-end speech understanding and generation. 
A lightweight style module further supports spoken dialogue with vivid emotions. 
By using text as a bridge, we demonstrate that omni-modal alignment is achievable without relying on scarce omni-modal image-text-speech data, which not only enhances both vision-language and speech capabilities but also surpasses its bi-modal counterparts through joint optimization. 
For the first time, ***EMOVA*** achieves state-of-the-art performance on both vision-language and speech benchmarks, setting a novel standard for the omni-modal models for versatile and expressive omni-modal interactions.
