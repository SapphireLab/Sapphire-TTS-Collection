# EMOVA (EMotionally Omni-present Voice Assistant)

<details>
<summary>基本信息</summary>

- 标题: "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions"
- 作者: 
  - 01 Kai Chen
  - 02 Yunhao Gou
  - 03 Runhui Huang
  - 04 Zhili Liu
  - 05 Daxin Tan
  - 06 Jing Xu
  - 07 Chunwei Wang
  - 08 Yi Zhu
  - 09 Yihan Zeng
  - 10 Kuo Yang
  - 11 Dingdong Wang
  - 12 Kun Xiang
  - 13 Haoyuan Li
  - 14 Haoli Bai
  - 15 Jianhua Han
  - 16 Xiaohui Li
  - 17 Weike Jin
  - 18 Nian Xie
  - 19 Yu Zhang
  - 20 James T. Kwok
  - 21 Hengshuang Zhao
  - 22 Xiaodan Liang
  - 23 Dit-Yan Yeung
  - 24 Xiao Chen
  - 25 Zhenguo Li
  - 26 Wei Zhang
  - 27 Qun Liu
  - 28 Lanqing Hong
  - 29 Lu Hou
  - 30 Hang Xu
- 链接: 
  - [ArXiv](https://arxiv.org/abs/2409.18042)
  - [Publication]()
  - [Github]()
  - [Demo](https://emova-ollm.github.io)
- 文件: 
  - [ArXiv](_PDF/2409.18042v1__EMOVA__Empowering_Language_Models_to_See_Hear_and_Speak_with_Vivid_Emotions.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

GPT-4o, an omni-modal model that enables vocal conversations with diverse emotions and tones, marks a milestone for omni-modal foundation models.
However, empowering Large Language Models to perceive and generate images, texts, and speeches end-to-end with publicly available data remains challenging in the open-source community.
Existing vision-language models rely on external tools for the speech processing, while speech-language models still suffer from limited or even without vision-understanding abilities. 
To address this gap, we propose ***EMOVA (EMotionally Omni-present Voice Assistant)***, to enable Large Language Models 
with end-to-end speech capabilities while maintaining the leading vision-language performance.
With a semantic-acoustic disentangled speech tokenizer, we notice surprisingly that omni-modal alignment can further enhance vision-language and speech abilities compared with the corresponding bi-modal aligned counterparts.
Moreover, a lightweight style module is proposed for flexible speech style controls (\eg, emotions and pitches).
For the first time, ***EMOVA*** achieves state-of-the-art performance on both the vision-language and speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue with vivid emotions. 

## 1.Introduction: 引言

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

This work presents ***EMOVA***, an innovative end-to-end omni-modal large language model that effectively aligns vision, speech, and text simultaneously.
We employ a continuous vision encoder to capture fine-grained visual details, while a discrete, semantic-acoustic disentangled speech tokenizer and detokenizer enable end-to-end speech understanding and generation. 
A lightweight style module further supports spoken dialogue with vivid emotions. 
By using text as a bridge, we demonstrate that omni-modal alignment is achievable without relying on scarce omni-modal image-text-speech data, which not only enhances both vision-language and speech capabilities but also surpasses its bi-modal counterparts through joint optimization. 
For the first time, ***EMOVA*** achieves state-of-the-art performance on both vision-language and speech benchmarks, setting a novel standard for the omni-modal models for versatile and expressive omni-modal interactions.
