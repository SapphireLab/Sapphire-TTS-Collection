# EMOVA (EMotionally Omni-present Voice Assistant)

<details>
<summary>基本信息</summary>

- 标题: "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions"
- 作者: 
  - 01 Kai Chen
  - 02 Yunhao Gou
  - 03 Runhui Huang
  - 04 Zhili Liu
  - 05 Daxin Tan
  - 06 Jing Xu
  - 07 Chunwei Wang
  - 08 Yi Zhu
  - 09 Yihan Zeng
  - 10 Kuo Yang
  - 11 Dingdong Wang
  - 12 Kun Xiang
  - 13 Haoyuan Li
  - 14 Haoli Bai
  - 15 Jianhua Han
  - 16 Xiaohui Li
  - 17 Weike Jin
  - 18 Nian Xie
  - 19 Yu Zhang
  - 20 James T. Kwok
  - 21 Hengshuang Zhao
  - 22 Xiaodan Liang
  - 23 Dit-Yan Yeung
  - 24 Xiao Chen
  - 25 Zhenguo Li
  - 26 Wei Zhang
  - 27 Qun Liu
  - 28 Lanqing Hong
  - 29 Lu Hou
  - 30 Hang Xu
- 链接: 
  - [ArXiv](https://arxiv.org/abs/2409.18042)
  - [Publication]()
  - [Github]()
  - [Demo](https://emova-ollm.github.io)
- 文件: 
  - [ArXiv](_PDF/2409.18042v1__EMOVA__Empowering_Language_Models_to_See_Hear_and_Speak_with_Vivid_Emotions.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

GPT-4o, an omni-modal model that enables vocal conversations with diverse emotions and tones, marks a milestone for omni-modal foundation models.
However, empowering Large Language Models to perceive and generate images, texts, and speeches end-to-end with publicly available data remains challenging in the open-source community.
Existing vision-language models rely on external tools for the speech processing, while speech-language models still suffer from limited or even without vision-understanding abilities. 
To address this gap, we propose ***EMOVA (EMotionally Omni-present Voice Assistant)***, to enable Large Language Models 
with end-to-end speech capabilities while maintaining the leading vision-language performance.
With a semantic-acoustic disentangled speech tokenizer, we notice surprisingly that omni-modal alignment can further enhance vision-language and speech abilities compared with the corresponding bi-modal aligned counterparts.
Moreover, a lightweight style module is proposed for flexible speech style controls (\eg, emotions and pitches).
For the first time, ***EMOVA*** achieves state-of-the-art performance on both the vision-language and speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue with vivid emotions. 

## 1.Introduction: 引言

OpenAI GPT-4o~\citep{openai2024gpt4o}, a new milestone for omni-modal foundation models, has rekindled people's attentions on intelligent assistants that can \textit{see} (\ie, perceiving fine-grained visual inputs), \textit{hear} (\ie, understanding vocal instructions) and \textit{speak} (\ie, generating vocal responses) simultaneously.
Most existing Multi-modal Large Language Models (MLLMs) focus on two modalities only, either vision-language~\citep{bai2023qwen,li2024llavaonevisioneasyvisualtask} or speech-language~\citep{chu2024qwen2audiotechnicalreport,xie2024mini}, demonstrating severe demands for omni-modal models with visual, language and speech abilities.
How to effectively empower Large Language Models (LLMs) to process omni-modal data in an end-to-end manner remains an open question. 

Existing omni-modal LLMs~\citep{chen2024internvlscalingvisionfoundation,fu2024vitaopensourceinteractiveomni} generally build upon Vision LLMs and integrate the speech modality by adopting a speech encoder like Whisper~\citep{radford2022robustspeechrecognitionlargescale}, which extracts \textbf{continuous} features from speech, similar to how images are processed, and enables speech understanding. 
However, these models still rely on external Text-to-Speech (TTS) tools for generating speech responses, limiting their ability to support real-time interactions.
AnyGPT~\citep{zhan2024anygptunifiedmultimodalllm}, instead, opts for a fully \textbf{discretization} manner, which first discretizes all data modalities (\ie, images, texts, and speeches), followed by omni-modal auto-regressive modeling. 
This enables AnyGPT to handle multiple modalities with a \textbf{unified end-to-end} framework, facilitating \textbf{real-time interactions} with the help of \textbf{streaming decoding}.
However, the discrete vision tokenizer adopted by AnyGPT struggles to capture visual details, especially for high-resolution images, making it far behind its continuous counterparts on vision-language benchmarks.
Moreover, none of the existing works explore speech style controls (\eg, emotions and pitches) with LLMs.
Therefore, our question arises: \textit{How to build an end-to-end omni-modal LLM enabling spoken dialogue with vivid emotions while maintaining state-of-the-art vision-language performance?}

In this paper, we propose ***EMOVA (EMotionally Omni-present Voice Assistant)***, a novel end-to-end omni-modal LLM with state-of-the-art vision-language and speech capabilities while supporting emotional spoken dialogue.
Fig.~\ref{fig:framework} shows an overview of the model framework.
A \textit{continuous} vision encoder captures the fine-grained visual details, while the \textit{discrete} speech tokenizer and detokenizer enable the end-to-end speech understanding and generation. 
Specifically, the speech-to-unit (S2U) tokenizer converts the input speech waveforms into discrete speech units as LLM inputs, while the unit-to-speech (U2S) detokenizer reconstructs the speech waveforms from the LLM's output speech units.
To seamlessly integrate the speech modality with LLMs, we meticulously design a \textbf{semantic-acoustic disentangled} speech tokenizer to decouple the semantic contents and acoustic styles of the input speeches~\citep{tao2024toneunitspeechdiscretizationapproach}, where 
1) \textit{semantic content} (\ie, what it says) captures the semantic meanings of input speeches, which is finally discretized and aligned with LLMs, while 
2) \textit{acoustic style} (\ie, how it says) captures the diverse speech styles (\eg, emotions and pitches).

Utilizing the semantic-acoustic disentanglement of our speech tokenizer, we further introduce a lightweight style module to support spoken dialogue with vivid emotions and pitches.
As in Sec.~\ref{sec_speech_in}, this innovative disentanglement design better facilitates the modality alignment between texts and speeches while maintaining flexibility for diverse speech style controllability and personalization.

With the end-to-end omni-modal architecture of ***EMOVA***, we empirically demonstrate that publicly available bi-modal image-text and speech-text data are sufficient for omni-modal alignment, utilizing the text modality as a bridge. This eliminates the need for omni-modal data (\ie, image-text-speech), which is usually scarce. Surprisingly, we find that omni-modal alignment can further improve both vision-language and speech capabilities through joint optimization, even when compared with their bi-modal aligned counterparts.
Finally, only a small amount of mixed-modality samples are required to teach the model to respond in the desired format.
For the first time, ***EMOVA*** achieves state-of-the-art performance on both vision-language and speech benchmarks (see Table~\ref{tab:compare} for comparisons).
The main contributions of this work contain three parts:

- We propose ***EMOVA***, a novel end-to-end omni-modal LLM that can see, hear and speak. We use a continuous vision encoder and a semantic-acoustic disentangled speech tokenizer for seamless omni-modal alignment and diverse speech style controllability. 
- We introduce an efficient text-centric omni-modal alignment which can further improve the vision-language and speech capabilities, even compared with the corresponding bi-modal aligned counterparts (\ie, image-text only and speech-text only alignment).
- For the first time, our ***EMOVA*** achieve state-of-the-art comparable performance on both the vision-language and speech benchmarks simultaneously, while supporting flexible spoken dialogues with vivid emotions.

## 2.Related Works: 相关工作

### Vision Large Language Models (VLLMs) 

VLLMs integrate vision modality into Large Language Models (LLMs)~\citep{touvron2023llama,chen2023gaining}, enabling advanced understanding and reasoning over visual instructions~\citep{liu2023llava,bai2023qwen,gou2023mixture,gou2024eyes}.
Recent efforts in VLLMs can be broadly categorized into three directions, including 
1) \textit{Vision encoders}~\citep{oquab2023dinov2,chen2021multisiam,chen2023mixed} are enhanced and aggregated for robust representations~\citep{lin2023sphinx,li2024mini,tong2024cambrian}.
2) \textit{High-resolution} methods are proposed to overcome the fixed resolution of pre-trained vision encoders (e.g., $336 \times 336$ for CLIP~\citep{radford2021learningtransferablevisualmodels}), empowering LLMs to perceive fine-grained visual information~\citep{liu2024llavanext,dong2024xcomposer2-4khd,huang2024hires,luo2024feast}.
3) \textit{High-quality instruction data} is essential for the VLLMs to generate accurate and well-formed responses following instructions~\citep{laurenccon2024matters,li2024llavaonevisioneasyvisualtask,chen2024internvlscalingvisionfoundation}.

In this paper, besides achieving state-of-the-art vision-language performance, we further introduce speech understanding and generating capabilities into our ***EMOVA***.

### Speech Large Language Models (SLLMs) 

SLLMs empower speech interaction with LLMs.
\textit{Continuous SLLMs}~\citep{wu2023decoder,chu2024qwen2audiotechnicalreport} utilize the speech encoders~\citep{radford2022robustspeechrecognitionlargescale} to extract continuous speech embeddings for LLM, which, however, only support speech understanding, relying on external TTS modules for speech generation, and therefore, hampering real-time interaction.
\textit{Discrete SLLMs}~\citep{zhang2023speechgpt}, instead, first discretize speech signals with speech tokenizers, followed by auto-regressive modeling.
Recent works~\citep{fang2024llamaomniseamlessspeechinteraction,xie2024mini} further combine the continuous speech encoders with the discrete speech tokenizers for better performance.
Although effective, none of the existing works explore speech style controllability in SLLMs (\eg, genders, emotions, and pitches), which is essential for real-life spoken dialogue.

### Omni-modal Large Language Models

Omni-modal LLMs support visual, text, and speech capabilities with a unified architecture simultaneously.
Similar to the continuous SLLMs, InternOmni~\citep{chen2024internvlscalingvisionfoundation} and VITA~\citep{fu2024vitaopensourceinteractiveomni} connect a speech encoder with VLLMs, supporting speech understanding only.
Instead, AnyGPT~\citep{zhan2024anygptunifiedmultimodalllm} proposes a unified architecture to discretize and conduct auto-regressive modeling for image, text, and audio simultaneously, which, however, suffers from inevitable information loss brought by discretization, especially for the high-resolution visual inputs.
In this work, we propose ***EMOVA***, the very first unified Omni-modal Large Language Models with state-of-the-art vision-language and speech performance at the same time. 

## 3.Methodology: 方法

### Architecture

#### Formulation

Denote the Large Language Model (LLM) as $f(\cdot)$ and the text, visual and speech inputs as $\textbf{X}_T$, $\textbf{X}_V$ and $\textbf{X}_S$, respectively.
$\textbf{X}_T$ is converted to discrete tokens $\textbf{U}_T$ via a text tokenizer~\citep{gage1994new}, while the visual input $\textbf{X}_V$ is first encoded with a vision encoder $v(\cdot)$ as $\textbf{E}_V = v(\textbf{X}_V)$, and then projected into the text embedding space with a projector $p(\cdot)$ as $\textbf{H}_V = p(\textbf{E}_V)$.
As for the speech input $\textbf{X}_S$, a \textit{Speech-to-Unit} (S2U) procedure is required.
Specifically, $\textbf{X}_S$ first passes through a speech encoder $s(\cdot)$ as $\textbf{E}_S = s(\textbf{X}_S)$, which is then discretized by the quantizer $q(\cdot)$ as $\textbf{U}_S = q(\textbf{E}_S)$.
The LLM $f(\cdot)$ is then trained to compute the joint probability of the output text and speech units $\textbf{U}_T^{o},\textbf{U}_S^{o}$ as

$$
    \mathbb{P}(\textbf{U}_T^{o},\textbf{U}_S^{o} | \textbf{U}_T,\textbf{U}_S,\textbf{H}_V) = \prod_{i=1}^L \mathbb{P}(\boldsymbol{x}_i|\textbf{U}_{T,<i}^{o},\textbf{U}_{S,<i}^{o},\textbf{U}_T,\textbf{U}_S,\textbf{H}_V),
$$

where $\boldsymbol{x}_i\in \textbf{U}_T^{o}\cup\textbf{U}_S^{o}$ and $L=|\textbf{U}_T^{o}|+|\textbf{U}_S^{o}|$. 
The output response units $\textbf{U}_S^{o}$ are then recovered into the output speech waveform $\textbf{Y}_S^{o}$ via a \textit{Unit-to-Speech} (U2S) decoder $d(\cdot,\cdot)$ together with an emotion style embedding $\textbf{E}_{style}^o$ to realize vivid emotional spoken dialogue controllability (Sec.~\ref{sec:speech_tokenizer}).

##### LLM

We adopt the LLaMA-3.1-8B~\citep{dubey2024llama} as our base LLM $f(\cdot)$, due to its superior performance among publicly available checkpoints, which is equipped with a tiktoken text tokenizer and a vocabulary size of 128,256, supporting both multilingual textual inputs and outputs. 

##### Vision encoder and projector

We utilize InternViT-6B~\citep{chen2024internvlscalingvisionfoundation} as our visual encoder $v(\cdot)$ with $448\times 448$ base resolution, and C-Abstractor~\citep{cha2024honeybee} with two ResBlocks (both before and after the pooling layer) and $4\times$ downsample rate as vision projector $p(\cdot)$.
To process the high-resolution inputs, the high-resolution image-slicing~\citep{liu2024llavanext} is used, where visual tokens for one image are concatenation with a low-resolution thumbnail and the origin high-resolution image with separators in each line, allowing a maximum of nine tiles during training.

#### Speech Tokenization

##### Speech-to-unit (S2U) tokenizer.

Following \cite{tao2024toneunitspeechdiscretizationapproach}, we adopt the SPIRAL~\citep{huang2022spiral} architecture for the speech encoder $s(\cdot)$ to capture both phonetic and tonal information, which is then discretized by the quantizer $q(\cdot)$ utilizing the finite scalar quantization (FSQ) ~\citep{mentzer2023finite}.
The size of the speech codebook is 4,096, while the sample rate is 25 tokens per second.
Once discretized, the speech modality can be simply integrated into LLMs by concatenating the text vocabulary and speech codebook.

Our S2U tokenizer provides the following advantages: 
1) \textit{Data efficiency}: after pre-training on large-scale unlabeled speech data, it requires only a small amount of speech-text pair data for fine-tuning.
2) \textit{Bilingual}: the speech codebook is shared among different languages (\ie, English and Chinese), sharing unit modeling abilities across languages.
Check more training details and comparisons with other speech tokenizers~\citep{zhang2023speechtokenizer} in Appendix~\ref{app:s2u training}.

##### Semantic-acoustic disentanglement.

To seamlessly align speech units with the highly semantic embedding space of LLMs, we opt for decoupling the semantic contents and acoustic styles of input speeches.
Specifically, given input speechs $\textbf{X}_S$, both semantic embedding $\textbf{E}_{semantic}$ and style embeddings $\textbf{E}_{style}$ are extracted separately, while only the $\textbf{E}_{semantic}$ is quantified by $q(\cdot)$ to generate speech units $\textbf{U}_S$.
By changing $\textbf{E}_{style}$ while maintaining the same $\textbf{E}_{semantic}$, we can easily control speech styles without disturbing the semantic contents of recovered speeches.
Moreover, the disentanglement facilitates modality alignment among speeches and texts, as later shown in Sec.~\ref{sec_speech_in}.

##### Unit-to-speech (U2S) detokenizer with style control.

Building on VITS \citep{kim2021conditional}, our U2S detokenizer adopts a conditional VAE architecture (see Fig.~\ref{fig:sdecoder}). 
To achieve vivid style controls, we utilize the semantic-style disentanglement of our S2U tokenizer (as discussed above) and adopt a novel style embedding to control the speech styles (\eg, speaker identities, emotions, and pitches).
Specifically, the LLM $f(\cdot)$ is trained to generate both the output speech units $\textbf{U}_S^{o}$ and a style label.
The speech units $\textbf{U}_S^{o}$ are converted to unit embeddings $\textbf{E}_{semantic}^{o}$, while the style label is utilized to generate a unique style prototype $\textbf{E}_{style}^{o}$. 
Both $\textbf{E}_{semantic}^{o}$ and $\textbf{E}_{style}^{o}$ are taken as inputs to speech decoder $d(\cdot,\cdot)$ to synthesize the output speech waveform $\textbf{Y}_{S}^{o}$.
See Appendix~\ref{app: sec_u2s} for more details.

Our U2S detokenizer is pre-trained on LibriTTS~\citep{zen2019libritts} and AISHELL-1~\citep{bu2017aishell} and subsequently fine-tuned on synthetic style-rich speech data.
Specifically, due to the scarcity of real-life style-rich data, we utilize TTS tools~\citep{du2024cosyvoice} to synthesize speech samples diverse in genders, pitches, and emotions. 
As for style prototypes, Emotion2Vec~\citep{ma2023emotion2vec} is adopted to select the most representative samples with the highest confidence in conveying the desired style.
Our empirical results reveal that even one representative style reference speech has been sufficient to control the speech styles flexibly and precisely.

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

This work presents ***EMOVA***, an innovative end-to-end omni-modal large language model that effectively aligns vision, speech, and text simultaneously.
We employ a continuous vision encoder to capture fine-grained visual details, while a discrete, semantic-acoustic disentangled speech tokenizer and detokenizer enable end-to-end speech understanding and generation. 
A lightweight style module further supports spoken dialogue with vivid emotions. 
By using text as a bridge, we demonstrate that omni-modal alignment is achievable without relying on scarce omni-modal image-text-speech data, which not only enhances both vision-language and speech capabilities but also surpasses its bi-modal counterparts through joint optimization. 
For the first time, ***EMOVA*** achieves state-of-the-art performance on both vision-language and speech benchmarks, setting a novel standard for the omni-modal models for versatile and expressive omni-modal interactions.
