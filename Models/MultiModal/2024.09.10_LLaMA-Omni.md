# LLaMA-Omni

<details>
<summary>基本信息</summary>

- 标题: LLaMA-Omni: Seamless Speech Interaction with Large Language Models
- 作者:
  1. Qingkai Fang
  2. Shoutao Guo
  3. Yan Zhou 
  4. Zhengrui Ma 
  5. Shaolei Zhang 
  6. Yang Feng
- 机构:
  1.
- 时间:
  - 预印时间: 2024.09.10 ArXiv v1
  - 更新笔记: 2024.09.15
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2409.06666)
  - [DOI]()
  - [Github](https://github.com/ictnlp/LLaMA-Omni)
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: 16
- 引用: ?
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Models like GPT-4o enable real-time interaction with large language models (LLMs) through speech, significantly enhancing user experience compared to traditional text-based interaction. 
> However, there is still a lack of exploration on how to build speech interaction models based on open-source LLMs. 
> To address this, we propose LLaMA-Omni, a novel model architecture designed for low-latency and high-quality speech interaction with LLMs. 
> LLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM, and a streaming speech decoder. 
> It eliminates the need for speech transcription, and can simultaneously generate text and speech responses directly from speech instructions with extremely low latency. 
> We build our model based on the latest Llama-3.1-8B-Instruct model. 
> To align the model with speech interaction scenarios, we construct a dataset named InstructS2S-200K, which includes 200K speech instructions and corresponding speech responses. 
> Experimental results show that compared to previous speech-language models, LLaMA-Omni provides better responses in both content and style, with a response latency as low as 226ms. 
> Additionally, training LLaMA-Omni takes less than 3 days on just 4 GPUs, paving the way for the efficient development of speech-language models in the future.

像 GPT-4o 这样的模型通过语音实现了与大语言模型的实时交互, 与传统的基于文字的交互相比显著提升了用户体验.
然而, 如何基于开源大语言模型构建语音交互模型仍然缺乏探索.
为了解决这一问题, 我们提出了 ***LLaMA-Omni***, 一种为低延迟和高质量的语音交互设计的新模型架构.
***LLaMA-Omni*** 集成了预训练的语音编码器, 语音适配器, 大语言模型, 以及流式语音解码器.
它消除了语音转写的需求, 并能够以极低的延迟直接从语音指令同时生成文本和语音响应.
我们基于最新的 LLaMA-3.1-8B-Instruct 模型构建了我们的模型.
为了使得模型适应语音交互场景, 我们构建了一个名为 InstructS2S-200K 的数据集, 其中包含 200K 个语音指令和相应的语音响应.
实验结果表明, 与之前的语音-语言模型相比, ***LLaMA-Omni*** 提供了更好的响应内容和风格, 响应延迟仅为 226ms.
此外, 训练 ***LLaMA-Omni*** 只需要 4 个 GPU 训练不到 3 天时间, 这为将来更高效地开发语音-语言模型铺平了道路.

## 1.Introduction: 引言

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
