# Wav2Vec 2.0

<details>
<summary>基本信息</summary>

- 标题: wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations
- 作者:
  - 01 [Alexei Baevski](../../Authors/Alexei_Baevski.md)
  - 02 [Henry Zhou](../../Authors/Henry_Zhou.md)
  - 03 [Abdelrahman Mohamed](../../Authors/Abdelrahman_Mohamed.md)
  - 04 [Michael Auli](../../Authors/Michael_Auli.md)
- 机构:
  - [Facebook AI](../../Institutions/USA-Meta.AI.md)
- 时间:
  - 预印时间: 2020.06.20 ArXiv v1
  - 预印时间: 2020.09.22 ArXiv v2
  - 预印时间: 2020.10.22 ArXiv v3
  - 更新笔记: 2024.06.14
- 发表:
  - [NeurIPS 2020](../../Publications/NeurIPS.md)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2006.11477)
  - [DOI](https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html)
  - [Github](https://github.com/facebookresearch/fairseq/tree/main/fairseq/models/wav2vec)
- 标签:
  - [自监督学习](../../Tags/Learning_Self-Supervised.md)
  - [语音表示](../../Tags/SpeechRepresentation.md)
  - [开源](../../Tags/OpenSource.md)
- 页数: 12
- 引用: 60
- 被引: 4503
- 数据:
  - [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md)
  - [LibriVox](../../Datasets/LibriVox.md)
  - [TIMIT](../../Datasets/TIMIT.md)

</details>

## Abstract

> We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. 
> ***Wav2Vec 2.0*** masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned.
> Experiments using all labeled data of [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md) achieve 1.8/3.3 [WER](../../Evaluations/WER.md) on the clean/other test sets.
> When lowering the amount of labeled data to one hour, ***Wav2Vec 2.0*** outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data.
> Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 [WER](../../Evaluations/WER.md).
> This demonstrates the feasibility of speech recognition with limited amounts of labeled data.

## 1.Introduction

> Neural networks benefit from large quantities of labeled training data.
> However, in many settings labeled data is much harder to come by than unlabeled data: current speech recognition systems require thousands of hours of transcribed speech to reach acceptable performance which is not available for the vast majority of the nearly 7,000 languages spoken worldwide [31].
> Learning purely from labeled examples does not resemble language acquisition in humans: infants learn language by listening to adults around them - a process that requires learning good representations of speech.

> In machine learning, self-supervised learning has emerged as a paradigm to learn general data representations from unlabeled examples and to fine-tune the model on labeled data.
> This has been particularly successful for natural language processing ([43]; [45]; [BERT](../LLM/2018.10.11_BERT.md)) and is an active research area for computer vision [20], [2], [36], [19], [6].

> In this paper, we present a framework for self-supervised learning of representations from raw audio data.
> Our approach encodes speech audio via a multi-layer convolutional neural network and then masks spans of the resulting latent speech representations [26], [56], similar to masked language modeling ([BERT](../LLM/2018.10.11_BERT.md)).
> The latent representations are fed to a Transformer network to build contextualized representations and the model is trained via a contrastive task where the true latent is to be distinguished from distractors ([CPC](../../Models/Speech_Representaion/2018.07.10_CPC.md); [Wav2Vec](../../Models/Speech_Representaion/2019.04.11_Wav2Vec.md); [48]; [28]) (Sec.2).

> As part of training, we learn discrete speech units ([VQ-VAE](../../Modules/VQ/2017.11.02_VQ-VAE.md); [32]; [7]; [18]) via a gumbel softmax ([24]; [VQ-Wav2Vec](../../Models/Speech_Representaion/2019.10.12_VQ-Wav2Vec.md)) to represent the latent representations in the contrastive task (Figure 1) which we find to be more effective than non-quantized targets.
> After pre-training on unlabeled speech, the model is fine-tuned on labeled data with a Connectionist Temporal Classification (CTC) loss [14] [4] to be used for downstream speech recognition tasks (Sec.3)

> Previous work learned a quantization of the data followed by a contextualized representations with a self-attention model ([VQ-Wav2Vec](../../Models/Speech_Representaion/2019.10.12_VQ-Wav2Vec.md); [4]), whereas our approach solves both problems end-to-end.
> Masking parts of the input with Transformer networks for speech has been explored [4]; [26], but prior work relies either on a two-step pipeline or their model is trained by reconstructing the filter bank input features.

> Other related work includes learning representations from auto-encoding the input data [52]; [11] or directly predicting future timesteps [8].
> Our results show that jointly learning discrete speech units with contextualized representations achieves substantially better results than fixed units learned in a prior step [4].
> We also demonstrate the feasibility of ultra-low resource speech recognition: when using only 10 minutes of labeled data, our approach achieves [Word Error Rate (WER)](../../Evaluations/WER.md) 4.8/8.2 on the clean/other test sets of [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md).
> We set a new state of the art on [TIMIT](../../Datasets/TIMIT.md) phoneme recognition as well as the 100 hour clean subset of [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md).
> Moreover, when we lower the amount of labeled data to just one hour, we still outperform the previous state of the art self-training method of [42] while using 100 times less labeled data and the same amount of unlabeled data.
> When we use all 960 hours of labeled data from [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md), then our model achieves 1.8/3.3 [WER](../../Evaluations/WER.md) (Sec.4, Sec.5).

## 2.Methodology

> Our model is composed of a multi-layer convolutional feature encoder $f: \mathcal{X}\mapsto\mathcal{Z}$ which takes as input raw audio $\mathcal{X}$ and outputs latent speech representations $z_1,\cdots, z_T$ for $T$ time-steps.
> They are then fed to a Transformer $g :\mathcal{Z}\mapsto\mathcal{C}$ to build representations $c_1, \cdots, c_T$ capturing information from the entire sequence [9];[5];[4].
> The output of the feature encoder is discretized to $q_t$ with a quantization module $\mathcal{Z}\mapsto \mathcal{Q}$ to represent the targets (Figure 1) in the self-supervised objective (Sec.3.2).
> Compared to [VQ-Wav2Vec](../../Models/Speech_Representaion/2019.10.12_VQ-Wav2Vec.md), our model builds context representations over continuous speech representations and self-attention captures dependencies over the entire sequence of latent representations end-to-end.

> ### Feature Encoder
> The encoder consists of several blocks containing a temporal convolution followed by [Layer Normalization](../../Modules/Normalization/2016.07.21_LayerNorm.md) and a GELU activation function [21].
> The raw waveform input to the encoder is normalized to zero mean and unit variance.
> The total stride of the encoder determines the number of time-steps T which are input to the Transformer (Sec.4.2).

> ### Contextualized Representations with Transformers
> The output of the feature encoder is fed to a context network which follows the Transformer architecture ([Transformer](../_Transformer/2017.06.12_Transformer.md); [BERT](../LLM/2018.10.11_BERT.md); [RoBERTa](../LLM/2019.07.26_RoBERTa.md))
> Instead of fixed positional embeddings which encode absolute positional information, we use a convolutional layer similar to [37], [4], [57] which acts as relative positional embedding.
> We add the output of the convolution followed by a GELU to the inputs and then apply layer normalization.

> ### Quantization Module
> For self-supervised training we discretize the output of the feature encoder z to a finite set of speech representations via product quantization [25].
> This choice led to good results in prior work which learned discrete units in a first step followed by learning contextualized representations ([VQ-Wav2Vec](../../Models/Speech_Representaion/2019.10.12_VQ-Wav2Vec.md)).
> Product quantization amounts to choosing quantized representations from multiple codebooks and concatenating them.
> Given $G$ codebooks, or groups, with $V$ entries $e \in \mathbb{R}^{V\times d/G}$, we choose one entry from each codebook and concatenate the resulting vectors $e_1, \cdots, e_G$ and apply a linear transformation $\mathbb{R}^{d}\mapsto\mathbb{R}^f$ to obtain $q\in\mathbb{R}^f$.

> The Gumbel softmax enables choosing discrete codebook entries in a fully differentiable way [16], [24], [35].
> We use the straight-through estimator [26] and setup G hard Gumbel softmax operations [24].
> The feature encoder output $z$ is mapped to l 2 RG⇥Vlogits and the probabilities for choosing the v-th codebook entry for group g are 

$$
  p_{g,v} = \dfrac{\exp(l_{g,v}+n_v)/\tau}{\sum_{k=1}^{V}\exp(l_{g,k}+n_k)/\tau},\tag{01}
$$

> where $\tau$ is a non-negative temperature, $n = -\log(-\log(u))$ and $u$ are uniform samples from $\mathcal{U}(0, 1)$.
> During the forward pass, codeword $i$ is chosen by $i = \arg\max_{j} p_{g,j}$ and in the backward pass, the true gradient of the Gumbel softmax outputs is used.

## 3.Training

> To pre-train the model we mask a certain proportion of time steps in the latent feature encoder space (Sec.3.1), similar to masked language modeling in [BERT](../LLM/2018.10.11_BERT.md).
> The training objective requires identifying the correct quantized latent audio representation in a set of distractors for each masked time step (Sec.3.2) and the final model is fine-tuned on the labeled data (Sec.3.3).

### 3.1.Masking

> We mask a proportion of the feature encoder outputs, or time steps before feeding them to the context network and replace them with a trained feature vector shared between all masked time steps; we do not mask inputs to the quantization module.
> To mask the latent speech representations output by the encoder, we randomly sample without replacement a certain proportionpof all time steps to be starting indices and then mask the subsequentMconsecutive time steps from every sampled index; spans may overlap.

### 3.2.Objective

> During pre-training, we learn representations of speech audio by solving a contrastive taskLmwhich requires to identify the true quantized latent speech representation for a masked time step within a set of distractors.
> This is augmented by a codebook diversity lossLdto encourage the model to use the codebook entries equally often.

$$
  Loss = Loss_{m} + \alpha Loss_{d}\tag{02}
$$

> where $\alpha$ is a tuned hyperparameter.

> #### Contrastive Loss
> Given context network outputctcentered over masked time stept, the model needs to identify the true quantized latent speech representationqtin a set ofK + 1quantized candidate representations˜ q 2 Qtwhich includesqtandKdistractors [23], [54].
> Distractors are uniformly sampled from other masked time steps of the same utterance.
> The loss is defined as

$$
$$

> where we compute the cosine similaritysim(a, b) = aTb/kakkbkbetween context representations and quantized latent speech representations [19], [6].

> #### Diversity Loss
> The contrastive task depends on the codebook to represent both positive and negative examples and the diversity lossLdis designed to increase the use of the quantized codebook representations [10].
> We encourage the equal use of theVentries in each of theGcodebooks by maximizing the entropy of the averaged softmax distributionlover the codebook entries for each codebook¯ pgacross a batch of utterances; the softmax disribution does not contain the gumbel noise nor a temperature:2

### 3.3.Fine-tuning

> Pre-trained models are fine-tuned for speech recognition by adding a randomly initialized linear projection on top of the context network into $C$ classes representing the vocabulary of the task [4].
> For [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md), we have 29 tokens for character targets plus a word boundary token.
> Models are optimized by minimizing a CTC loss [14] and we apply a modified version of SpecAugment [41] by masking to time-steps and channels during training which delays overfitting and significantly improves the final error rates, especially on the Libri-light subsets with few labeled examples.

## 4.Experiments Setup

### 4.1.Datasets

> As unlabeled data we consider the [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md) corpus [40] without transcriptions containing 960 hours of audio (LS-960) or the audio data from [LibriVox](../../Datasets/LibriVox.md) (LV-60k).
> For the latter we follow the pre-processing of [27] resulting in 53.2k hours of audio.
> We fine-tune on five labeled data settings: 960 hours of transcribed [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md), the train-clean-100 subset comprising 100 hours (100 hours labeled), as well as the Libri-light limited resource training subsets originally extracted from [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md), these are train-10h (10 hours labeled), train-1h (1 hour labeled), train-10min (10 min labeled).
> We follow the evaluation protocol of Libri-light for these splits and evaluate on the standard [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md) dev-other/clean and test-clean/other sets.

> We fine-tune the pre-trained models for phoneme recognition on the [TIMIT dataset](../../Datasets/TIMIT.md).
> It contains five hours of audio recordings with detailed phoneme labels.
> We use the standard train, dev and test split and follow the standard protocol of collapsing phone labels to 39 classes.

### 4.2.Pre-training

> Models are implemented in fairseq [39].
> For masking, we sample p = 0.065of all time-steps to be starting indices and mask the subsequent M = 10 time-steps.
> This results in approximately 49% of all time steps to be masked with a mean span length of 14.7, or 299ms (see Appendix A for more details on masking).

> The feature encoder contains seven blocks and the temporal convolutions in each block have 512 channels with strides (5,2,2,2,2,2,2) and kernel widths (10,3,3,3,3,2,2).
> This results in an encoder output frequency of 49 hz with a stride of about 20ms between each sample, and a receptive field of 400 input samples or 25ms of audio.
> The convolutional layer modeling relative positional embeddings has kernel size 128 and 16 groups.

> We experiment with two model configurations which use the same encoder architecture but differ in the Transformer setup: BASEcontains 12 transformer blocks, model dimension 768, inner dimension (FFN) 3,072 and 8 attention heads.
> Batches are built by cropping 250k audio samples, or 15.6sec, from each example.
> Crops are batched together to not exceed 1.4m samples per GPU and we train on a total of 64 V100 GPUs for 1.6 days [38]; the total batch size is 1.6h.

> The LARGE model contains 24 transformer blocks with model dimension 1,024, inner dimension 4,096 and 16 attention heads.
> We crop 320k audio samples, or 20sec, with a limit of 1.2m samples per GPU and train on 128 V100 GPUs over 2.3 days for [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md) and 5.2 days for [LibriVox](../../Datasets/LibriVox.md); the total batch size is 2.7h.
> We use dropout 0.1 in the Transformer, at the output of the feature encoder and the input to the quantization module.
> Layers are dropped at a rate of 0.05 for BASEand 0.2 for LARGE [22], [12]; there is no layer drop for LV-60k.

> We optimize with Adam [29], warming up the learning rate for the first 8% of updates to a peak of $5\times 10^{-4}$ for BASEand3 ⇥ 10�4for LARGE , and then linearly decay it.
> LARGE trains for 250k updates, BASEfor 400k updates, and LARGE on LV-60k for 600k updates.
> We use weight↵ = 0.1 for the diversity loss Equation 2.
> For the quantization module we useG = 2andV = 320for both models, resulting in a theoretical maximum of 102.4k codewords.
> Entries are of sized/G = 128 for BASEamdd/G = 384for LARGE .
> The Gumbel softmax temperature⌧is annealed from 2 to a minimum of 0.5 for BASEand 0.1 for LARGE by a factor of 0.999995 at every update.
> The temperature in the contrastive loss (Equation 3) is set to = 0.1.
> For the smaller [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md) dataset, we regularize the model by applying an L2 penalty to the activations of the final layer of the feature encoder and scale down the gradients for the encoder by a factor of 10.
> We also use a slightly different encoder architecture where we do not use layer normalization, and instead of normalizing the raw waveform, the output of the first encoder layer is normalized.
> In the contrastive loss we useK = 100 distractors.
> We choose the training checkpoint with the lowest Lmon the validation set.

### 4.3.Fine-tuning

> After pre-training we fine-tune the learned representations on labeled data and add a randomly initialized output layer on top of the Transformer to predict characters ([LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md)/Libri-light) or phonemes ([TIMIT](../../Datasets/TIMIT.md)).
> For Libri-light, we train three seeds with two different learning rates (2e-5 and 3e-5) for all subsets and choose the configuration with lowest [WER](../../Evaluations/WER.md) on dev-other subset decoded with the official 4-gram language model (LM) with beam 50 and fixed model weights (LM weight 2, word insertion penalty -1).
> For BASEon the labeled 960h subset we use a learning rate of 1e-4.

> We optimize with Adam and a tri-state rate schedule where the learning rate is warmed up for the first 10% of updates, held constant for the next 40% and then linearly decayed for the remainder.
> BASE uses a batch size of 3.2m samples per GPU and we fine-tune on 8 GPUs, giving a total batch size of 1,600sec.
> LARGE batches 1.28m samples on each GPU and we fine-tune on 24 GPUs, resulting in an effective batch size of 1,920sec.
> For the first 10k updates only the output classifier is trained, after which the Transformer is also updated.
> The feature encoder is not trained during fine-tuning.
> We mask the feature encoder representations with a strategy similar to SpecAugment [41] detailed in Appendix B.

### 4.4.Language Models and Decoding

> We consider two types of language models (LM): a 4-gram model and a Transformer [3] trained on the [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md) LM corpus.
> The Transformer LM is identical to [51] and contains 20 blocks, model dimension 1,280, inner dimension 6,144 and 16 attention heads.
> We tune the weights of the language model (interval[0, 5]) and a word insertion penalty ([�5, 5]) via Bayesian optimization3: we run 128 trials with beam 500 for the 4-gram LM and beam 50 for the Transformer LM and choose the best set of weights according to performance on dev-other.
> Test performance is measured with beam 1,500 for the n-gram LM and beam 500 for the Transformer LM.
> We use the beam search decoder of [44].

## 5.Results

### 5.1.Low-Resource Labeled Data Evaluation

> We first evaluate our pre-trained models in settings where the amount of labeled data is limited to get a sense of how the representations learned on unlabeled data can improve low resource settings.
> If a pre-trained model captures the structure of speech, then it should require few labeled examples to fine-tune it for speech recognition.
> The models are pre-trained on the audio data of either [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md) (LS-960) or [LibriVox](../../Datasets/LibriVox.md) (LV-60k) and most results are obtained by decoding with a Transformer language model (Transf.); Appendix C shows results with no language model at all as well as with an n-gram language model.

> The LARGE model pre-trained on LV-60k and fine-tuned on only 10 minutes of labeled data achieves a word error rate of 5.2/8.6 on the [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md) clean/other test sets.
> Ten minutes of labeled data corresponds to just 48 recordings with an average length of 12.5 seconds.
> This demonstrates that ultra-low resource speech recognition is possible with self-supervised learning on unlabeled data.

> Our approach of jointly learning discrete units and contextualized representations clearly improves over previous work which learned quantized audio units in a separate step [4], reducing [WER](../../Evaluations/WER.md) by a about a third.

> A recent iterative self-training approach [42] represents the state of the art on the clean 100 hour subset of [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md) but it requires multiple iterations of labeling, filtering, and re-training.
> Our approach is simpler: we pre-train on the unlabeled data and fine-tune on the labeled data.
> On the 100 hour subset of [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md), their method achieves [WER](../../Evaluations/WER.md) 4.2/8.6 on test-clean/other which compares to [WER](../../Evaluations/WER.md) 2.3/5.0 with the LARGE model in a like for like setup, a relative [WER](../../Evaluations/WER.md) reduction of 45%/42%.

> When the LARGE model uses an order of magnitude less labeled data (10h labeled), then it still achieves [WER](../../Evaluations/WER.md) 3.2/6.1, an error reduction of 24%/29% relative to iterative self-training.
> Using only a single hour of labeled data, the same model achieves [WER](../../Evaluations/WER.md) 3.9/7.6 which improves on both test-clean and test-other by 7%/12% - with two orders of magnitude less labeled data.
> We note that the Libri-light data splits contain both clean and noisy data leading to better accuracy on test-other compared to test-clean.
> Increasing model size reduces [WER](../../Evaluations/WER.md) on all setups with the largest improvements on test-other (BASEvs.
> LARGE both on LS-960) and increasing the amount of unlabeled training data also leads to large improvements (LARGE LS-960 vs.
> LV-60k).

### 5.2.High-Resource Labeled Data Evaluation on [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md)

> In this section we evaluate the performance when large quantities of labeled speech are available to assess the effectiveness of our approach in a high resource setup.
> Specifically, we fine-tune the same models as before on the full 960 hours of labeled [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md): BASEand LARGE pre-trained on LS-960 as well as LARGE pre-trained on LV-60k.

> Table 2 shows that our approach achieves [WER](../../Evaluations/WER.md) 1.8/3.3 on test-clean/other on the full [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md) benchmark.
> This is despite a weaker baseline architecture: supervised training of our architecture achieves [WER](../../Evaluations/WER.md) 2.1/4.6 (LARGE - from scratch) compared to [WER](../../Evaluations/WER.md) 1.9/4.1 for ContextNet [17], the baseline architecture of the state of the art [42].
> We use a simple Transformer with CTC which does not perform as well as seq2seq models [51].

> Note that the vocabulary of our acoustic model (characters) does not match the vocabulary of the LM (words) which delays feedback from the LM and is likely to be detrimental.
> Most recent work [51], [58], [17], [42] uses the better performing word pieces [50] for both models.
> Moreover, our result is achieved without any data balancing such as [42].
> Finally, self-training is likely complimentary to pre-training and their combination may yield even better results.
> Appendix E presents a detailed error analysis of our pre-trained models in various labeled data setups.

### 5.3.Phoneme Recognition on TIMIT

> Next, we evaluate accuracy on [TIMIT](../../Datasets/TIMIT.md) phoneme recognition by fine-tuning the pre-trained models on the labeled [TIMIT](../../Datasets/TIMIT.md) training data.
> We fine-tune as for the 10 hour subset of Libri-light but do not use a language model.
> Table 3 shows that our approach can achieve a new state of the art on this dataset, reducing PER by a relative 23%/29% over the next best result on the dev/test sets.
> Appendix D shows an analysis of how the discrete latent speech representations related to phonemes.
> Other recent work on pre-training which evaluates on [TIMIT](../../Datasets/TIMIT.md) includes [47] who solve multiple tasks to learn good representations of speech.

### 5.4.Ablations

> A difference to previous work ([VQ-Wav2Vec](../../Models/Speech_Representaion/2019.10.12_VQ-Wav2Vec.md), [4] is that we quantize the latent audio representations only for the contrastive loss, i.e., when latents are used as targets, but not when the latents are input to the Transformer network.
> We motivate this choice by an ablating for which we adopt a reduced training setup to increase experimental turn around: we pre-train BASEon LS-960 for 250k updates with masking probabilityp = 0.075, fine-tune on train-10h for 60k updates on a single GPU with 640k samples per batch, or 40 sec of speech audio.
> We report the average [WER](../../Evaluations/WER.md) and standard deviation on the concatenation of dev-clean and dev-other (dev PER) for three seeds of fine-tuning.

> Table 4 shows that our strategy of continuous inputs with quantized targets (Baseline) performs best.
> Continuous latent speech representations retain more information to enable better context representations and quantizing the target representations leads to more robust training.
> Quantizing the latents both in the input and the targets performs least well, and explains the lower performance of prior work ([VQ-Wav2Vec](../../Models/Speech_Representaion/2019.10.12_VQ-Wav2Vec.md), [4]).
> Continuous targets reduce the effectiveness of self-supervised training since targets can capture detailed artifacts of the current sequence, e.g. speaker and background information, which make the task easier and prevent the model from learning general representations beneficial to speech recognition.
> The training accuracy of identifying the correct latent audio representation increases from 62% to 78.0% when switching from quantized to continuous targets.
> Continuous inputs and continuous targets perform second best but various attempts to improve it did not lead to better results (see Appendix F for this experiment and other ablations on various hyperparameters).

## 6.Conclusions

<details>
<summary>原文</summary>

> We presented ***Wav2Vec 2.0***, a framework for self-supervised learning of speech representations which masks latent representations of the raw waveform and solves a contrastive task over quantized speech representations.
> Our experiments show the large potential of pre-training on unlabeled data for speech processing: when using only 10 minutes of labeled training data, or 48 recordings of 12.5 seconds on average, we achieve a [WER](../../Evaluations/WER.md) of 4.8/8.2 on test-clean/other of [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md).

> Our model achieves results which achieve a new state of the art on the full [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md) benchmark for noisy speech.
> On the clean 100 hour [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md) setup, ***Wav2Vec 2.0*** outperforms the previous best result while using 100 times less labeled data.
> The approach is also effective when large amounts of labeled data are available.
> We expect performance gains by switching to a seq2seq architecture and a word piece vocabulary.

</details>
<br>

我们提出了 ***Wav2Vec 2.0***, 这是一个用于自监督学习语音表示的框架, 它对原始波形的潜在表示进行掩膜, 并在量化语音表示上解决了一个对比任务.

我们的实验显示了在语音处理中对未标记数据进行预训练的巨大潜力：
当仅使用 10 分钟的标记训练数据, 或平均 48 个 12.5 秒的录音时, 我们在 [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md) 的 test-clean/other 上达到了 4.8/8.2 的词错误率.

我们的模型在全噪声 [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md) 基准测试中达到了新的最先进水平.
在干净的 100 小时 [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md) 设置中, ***Wav2Vec 2.0*** 在使用比之前最佳结果少一百倍的标记数据的情况下, 超越了之前的最佳结果.

当有大量标记数据可用时, 这种方法也同样有效.
我们预计通过切换到 Seq2Seq 架构和词块词汇表, 性能将得到提升.