# SoCodec

<details>
<summary>基本信息</summary>

- 标题: SoCodec: A Semantic-Ordered Multi-Stream Speech Codec for Efficient Language Model Based Text-to-Speech Synthesis
- 作者:
  | 序号 | 作者 | 机构 |
  | :-: | --- | --- |
  | 01 | [郭浩翰 (Haohan Guo)](../../Authors/Haohan_Guo_(郭浩翰).md) | [香港中文大学](../../Institutions/CHN-CUHK_香港中文大学.md) |
  | 02 | [Fenglong Xie](../../Authors/Fenglong_Xie.md) | [小红书](../../Institutions/CHN-XiaohongshuInc_小红书.md) |
  | 03 | [Kun Xie](../../Authors/Kun_Xie.md) | [小红书](../../Institutions/CHN-XiaohongshuInc_小红书.md) |
  | 04 | [杨东超 (Dongchao Yang)](../../Authors/Dongchao_Yang_(杨东超).md) | [香港中文大学](../../Institutions/CHN-CUHK_香港中文大学.md) |
  | 05 | [郭大可 (Dake Guo)](../../Authors/Dake_Guo_(郭大可).md) | [西北工业大学](../../Institutions/CHN-NPU_西北工业大学.md) |
  | 06 | [Xixin Wu](../../Authors/Xixin_Wu.md) | [香港中文大学](../../Institutions/CHN-CUHK_香港中文大学.md) |
  | 07 | [蒙美玲 (Helen Meng)](../../Authors/Helen_Meng_(蒙美玲).md) | [香港中文大学](../../Institutions/CHN-CUHK_香港中文大学.md) |
- 机构:
  | 序号 | 机构 | 占比 |
  | :-: | --- | :-: |
  | 01 | [香港中文大学](../../Institutions/CHN-CUHK_香港中文大学.md) | 04/07 |
  | 02 | [小红书](../../Institutions/CHN-XiaohongshuInc_小红书.md) | 02/07 |
  | 03 | [西北工业大学](../../Institutions/CHN-NPU_西北工业大学.md) | 01/07 |
- 时间:
  - 预印时间: 2024.09.02 ArXiv v1
  - 更新笔记: 2024.09.04
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2409.00933)
  - [DOI]()
  - [Github]()
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: 8
- 引用: 31
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> The long speech sequence has been troubling language models (LM) based TTS approaches in terms of modeling complexity and efficiency. This work proposes SoCodec, a semantic-ordered multi-stream speech codec, to address this issue. It compresses speech into a shorter, multi-stream discrete semantic sequence with multiple tokens at each frame. Meanwhile, the ordered product quantization is proposed to constrain this sequence into an ordered representation. It can be applied with a multi-stream delayed LM to achieve better autoregressive generation along both time and stream axes in TTS. The experimental result strongly demonstrates the effectiveness of the proposed approach, achieving superior performance over baseline systems even if compressing the frameshift of speech from 20ms to 240ms (12x). The ablation studies further validate the importance of learning the proposed ordered multi-stream semantic representation in pursuing shorter speech sequences for efficient LM-based TTS.

## 1.Introduction: 引言

> Large language models (LLMs) have demonstrated powerful capability in text generation \cite{brown2020language,openai2023gpt4,touvron2023llama2}. This breakthrough inspires applications of LLMs in speech generation, especially for zero-shot text-to-speech (TTS) synthesis, e.g., VALL-E \cite{VALLEX}, Tortoise \cite{tortoise}, BASE-TTS \cite{lajszczak2024base}, etc. These models treat TTS as a next-token prediction task by auto-regressively generating discrete speech tokens (codes). Hence, such a language-model-based TTS system, i.e. LM-TTS, usually relies on an audio codec system \cite{encodec, hifi-codec, dac}. It encodes speech signals into discrete speech tokens and reconstructs them back. Then, we train an LM to predict speech tokens from the text to achieve TTS synthesis. However, unlike the text, speech signals contain abundant information, including phonetics, prosody, speaker identity, acoustic environment, etc., making it challenging to compress into a token sequence as short as the text. The long sequence seriously affects the LM in terms of modeling complexity and efficiency, hindering its development in the speech domain.

> In this work, we propose SoCodec, a semantic-ordered multi-stream speech codec, to provide a shorter token sequence for efficient LM-TTS. First, SoCodec leverages the self-supervised-learning-based model \cite{hsu2021hubert, wavlm} to compress speech signals into a multi-stream semantic sequence containing sufficient phonetic and prosodic information. Then, an utterance-level acoustic embedding is extracted from the Mel spectrogram to represent time-invariant acoustic information, including speaker identity, acoustic environment, etc. Meanwhile, we propose ordered product quantization (OPQ) for SoCodec to quantize speech into an ordered speech representation along the stream axis. It can be incorporated with a multi-stream LLM \cite{copet2024simple} based on a delayed pattern to achieve the high-quality and efficient zero-shot TTS, which is validated in both subjective and objective experiments.

> Our contributions are summarized as follows: 1) we propose a new speech codec, SoCodec, providing a shorter speech sequence for efficient LM-TTS; 2) we propose ordered product quantization (OPQ) to learn an ordered multi-stream sequence to adapt multi-stream LM better; 3) we propose an LM-TTS system based on SoCodec, achieving higher efficiency while keeping high synthesis quality in TTS, even with a frameshift of only 240ms, the shortest sequence across all LM-TTS approaches to the best of our knowledge.

## 2.Related Works: 相关工作

> To provide a short speech sequence for LMs, the codec is usually optimized from two aspects: 1) information reduction, compressing speech signals to represent speech with fewer tokens, and 2) representing speech with multiple streams, i.e., each frame consists of multiple tokens to increase the frameshift of the sequence. The mainstream audio codec, e.g. Encodec \cite{encodec}, Hifi-Codec \cite{hifi-codec}, DAC \cite{hifi-codec}, usually adopt residual vector quantization (RQ) based approaches to compress speech signals into a multi-stream sequence with a frameshift of around 20ms and more than 8 streams to cover sufficient acoustic information. However, the sequence is still too long to adapt LMs well. Hence, TiCodec \cite{ren2024fewer} and SingleCodec \cite{li2024single} are proposed to represent speech with fewer tokens by disentangling time-invariant acoustic information out of the discrete sequence. Meanwhile, some works \cite{ns3, zhang2023speechtokenizer, liu2024semanticodec} emphasize keeping only semantic information in speech tokens to compress the sequence further. Based on these works, we propose SoCodec to compress speech into a shorter multi-stream semantic representation.

> On the other hand, to better generate the multi-stream representation, an ordered generation process along the stream axis is also applied in LM-TTS. For example, VALL-E \cite{wang2023neural, VALLEX} predicts the first stream via one AR model, and predicts the following streams recursively by running one NAR model 7 times. Recently, a multi-stream LLM with a delayed pattern \cite{copet2024simple, kharitonov2022text} is proposed to generate tokens along both axes auto-regressively by only running one AR model once. However, this autoregressive generation from the low stream to the high stream makes it easy to deliver accumulated errors to high-stream prediction, degrading the generation quality. Hence, we intuitively aim to seek an ordered multi-stream speech representation to first generate principal speech information in low streams to ensure a stable generation process. Inspired by ordered representation learning \cite{rippel2014learning, xu2021anytime}, we propose ordered product quantization for SoCodec.

## 3.Methodology: 方法

> In this section, we will give a detailed introduction to Semantic-Ordered Speech Codec (SoCodec), including the proposed ordered product quantization, the model architecture, and the loss function.

### Ordered Product Quantization

> Ordered representation learning \cite{rippel2014learning, xu2021anytime} aims to encode the input into a PCA-like representation, where the first dimension represents the most principal information of the input, and the following dimensions represent the residual information recursively. Inspired by it, we propose ordered product quantization (OPQ) to encode speech into a representation with this order along the stream axis.

### Model Architecture

#### Semantic Encoder

> As shown in Fig. \ref{fig:socodec}, we first encode the speech signal into a semantic token sequence by employing a pre-trained self-supervised learning (SSL) model, HuBERT \cite{hsu2021hubert}. It encodes speech signals into embedding sequence $S$ with rich semantic information as the encoder input. The time-variant encoder based on ResNet blocks further processes this sequence and uses stridden convolutional layers for down-sampling to obtain a shorter encoding sequence. We then process this sequence with OPQ to obtain the ordered quantized sequence $Z$.

#### Acoustic Encoder

> Meanwhile, we also apply an ECAPA-TDNN-based \cite{dawalatabad2021ecapa} time-invariant encoder to extract an utterance-level global embedding $g$ from speech signals to preserve time-invariant information, e.g., speaker identity, global speaking style, acoustic environment, etc. This embedding can be used in speech reconstruction and imitate the target voice in zero-shot TTS. To avoid the leakage of content information into this embedding, we propose a simple pro-processing technique, ``Clip\&Shuffle", on the Mel spectrogram to remove short-time variant information from it. Specifically, we first sample a segment with a length of 25\% to 75\% of the utterance, and then chunk it into slices with the length of 1 second. Finally, these slices are shuffled randomly to form a new sequence. This simple approach effectively reduces content leakage, and avoids complicated disentangling techniques such as adversarial learning \cite{lajszczak2024base}.

#### Decoder

> In the decoder, the global embedding $g$ is duplicated and added with the quantized sequence $Z$ to form the decoder input. It is then processed by ResNet blocks with the transposed convolutional layers for up-sampling to reconstruct both SSL features and acoustic features. The discriminator proposed in Mega-TTS \cite{jiang2023mega} is applied in training to improve the generative quality of the Mel spectrogram. Finally. we employ a pre-trained Mel-spectrogram-based neural vocoder, BigVGAN \cite{bigvgan}, to generate the reconstructed audio. Notably, although the SSL feature is not applied to generate the audio, the training objective of minimizing the reconstruction loss of SSL features still matters in learning discrete semantic representations.

#### Loss Function

> The loss function of SoCodec is written as follows:

$$
\begin{aligned}
    \mathcal{L}_c &= \lambda_1 * \left \| Z - \tilde{Z} \right \|^2_2 + \lambda_2 * \left \| S - \hat{S} \right \|^2_2 \\
    & + \lambda_3 * \left \| A - \hat{A} \right \|^2_2 + \lambda_4 * \left \| 1 - D(\hat{A}) \right \|^2_2
\end{aligned}
$$

> where $\lambda_1, \lambda_2, \lambda_3, \lambda_4$ are weight coefficients. $\left \| Z - \tilde{Z} \right \|^2_2$ is the VQ loss between the quantized sequence $Z$ and the encoding sequence $\tilde{Z}$ before the quantization. $\left \| S - \hat{S} \right \|^2_2$ is the semantic loss between the SSL features $S$ and the reconstructed ones $\hat{S}$. Finally, the acoustic loss is composed of two terms: the L2 loss between the ground-truth Mel spectrogram $A$ and the reconstructed one $\hat{A}$, and an adversarial loss, where $D(*)$ denotes outputs of all discriminators. Meanwhile, discriminators are trained alternately with codec.

### Multi-Stream LM

#### Model Architecture

> In this work, we adopt a GPT-2-based \cite{radford2019language} decoder-only Transformer to construct a multi-stream LM, as shown in Fig. \ref{fig:lm}. In training, we first extract semantic tokens and global (reference) embedding from the target speech via the pre-trained SoCodec. Then, the reference embedding, text, and speech, are mapped into the embedding space with the same dimension. All streams of the speech sequence are processed respectively and then added together. The text sequence and speech sequence are added with different learnable positional embeddings. Then, we process this embedding sequence with a stack of causal Transformer layers, and predict probabilities of speech tokens with a group of linear layers. The loss function of multi-stream LM is the averaged cross-entropy loss across all streams of the speech sequence.

#### Delay Prediction

> An ideal LM for multi-stream representations is to predict the joint distribution of all streams in an auto-regressive manner:

$$
\begin{aligned}
    P(y_{t, 1:m}|y_{1: t-1, 1: m}, x)
\end{aligned}
$$

> where $m$ and $x$ are the number of streams and the input, i.e. the text and the reference embedding in our work. It requires integrating all tokens at the same frame into one token corresponding to a giant dictionary, which is impossible to achieve. Hence, we usually adopt the chain rule, i.e.:

$$
\begin{aligned}
    \prod^m_{j=1}P(y_{t, j}|y_{t, 1: j-1}, y_{1: t-1, 1: m}, x)
\end{aligned}
$$

> we can achieve this by flattening all streams into one sequence, but it also multiplies the length of the sequence, going against our intention of using multi-stream representations. The delayed prediction breaks the dilemma effectively and has shown superior performance in music and speech generation \cite{copet2024simple, kharitonov2022text}. It shifts the $j$-th stream with $d * (j-1)$ frames for all streams, where $d$ is the pre-designed number of delayed steps. In this way, the objective of LM is changed to:

$$
\begin{aligned}
\label{eq:delay}
    \prod^m_{j=1}P(y_{t, j}|&y_{1: t - 1 + d*(j - 1), 1}, y_{1: t - 1 + d*(j - 2), 2}, \\
    & ..., y_{1: t - 1 + d*(j - m), m}, x)
\end{aligned}
$$

> This approach allows us to predict multiple tokens in parallel while keeping chain-rule prediction from low to high streams. However, this pattern also causes missing high-stream information in low-stream prediction and more accumulated errors in high-stream prediction. Specifically, when generating low-stream tokens, the model cannot see full information on high streams from the past. For example, $y_{3,4}$ is unseen when generating $y_{4, 1}$. Moreover, the accumulated errors from the auto-regressive prediction lead to more noise in the high-stream prediction. Hence, an ordered representation is necessary for this multi-stream LM to ensure that principal speech information is preserved and predicted in low streams for a high-quality and stable generation.

## 4.Experiments: 实验

### Datasets & Features

> All codecs and LLMs in this work are trained with WenetSpeech4TTS (Basic) \cite{ma2024wenetspeech4tts}, an open-source large-scale Chinese multi-speaker speech dataset. It contains 7,226 hours of found data with high diversity in audio quality, speaking style, acoustic environment, etc., hence putting a huge challenge to TTS modeling. All audio files are normalized to the sample rate of 16kHz in our experiments.  SoCodec adopts a pre-trained Chinese HuBERT \cite{TencentGameMate} to extract 1024-dim SSL features with a frameshift of 20ms, and uses the 80-dim Mel-spectrogram with a frameshift of 10ms as the acoustic feature. The BigVGAN-base \cite{bigvgan} neural vocoder is pre-trained on WenetSpeech4TTS to synthesize audio from Mel spectrograms. We train all LMs to generate speech tokens from the normalized text directly. Hence, we train a byte-pair encoding (BPE) based text dictionary with 8192 tokens, and feed text tokens to our LMs.

### Models

> In SoCodec\footnote{The implementation is available at \url{https://github.com/hhguo/SoCodec}}, the time-variant encoder and the decoder are applied with 1024-dim ResNet blocks, each consisting of 4 convolutional layers. We apply the time-invariant encoder with a 256-dim ECAPA-TDNN based on a smaller ResNet block with only 2 convolutional layers. In OPQ, we fix the number of codewords of each codebook to 128, and set the dimension of input vectors of the OPQ module to the total number of codewords, e.g. 1024 for 8 codebooks. Two codebooks form a larger codebook with 16,384 codewords for LLM training and inference. In training, we set $\lambda_1=1, \lambda_2=10^3, \lambda_3=10, \lambda_4=1$ to balance these loss terms, and use EMA to update codebooks with the decay rate of 0.99.
>
> LMs use the GPT-2 module consisting of 12 Transformer layers with a feature dimension of 1024 to predict probability distributions with the dimension of 16386 (the extra two indices are for BOS and EOS tokens) for each stream. SoCodec and LMs are trained using AdamW \cite{loshchilov2017decoupled} for 100,000 iterations. By default, we train models with a batch size of 1600 seconds and run LM inference using a sampling strategy with a temperature of 0.8, a top-p of 0.8, a top-k of 10, and a repetition penalty of 2.0 for a stable generation process. In \ref{ssec:tts_comparison}, we increase the batch size to 3.5 hours in training to keep a similar training configuration with baseline systems.

### Evaluation Metrics

> In codec evaluation, we sample 860 utterances from the WenetSpeech \cite{zhang2022wenetspeech} test set, with high diversity in speaking style and audio quality, to evaluate the performance of codecs. We adopt the following objective metrics\footnote{We use FunASR, the open-source ASR tool, for transcribing, available at \url{https://github.com/modelscope/FunASR}. The tool for extracting speaker embedding is available at \url{https://huggingface.co/Wespeaker/wespeaker-cnceleb-resnet34}}: Mel-cepstrum distortion (MCD, dB), character error rate (CER, \%), and speaker similarity (SIM, $\times10^{-2}$) to measure reconstruction quality, intelligibility, and speaker similarity. 
>
> In TTS evaluation, we create a test set with 860 utterances, where each utterance is paired with a different out-of-training-set audio file as the reference audio for zero-shot TTS. In \ref{ssec:tts_comparison}, we use the subset with 100 utterances to conduct objective and subjective tests, i.e. the MOS tests. There are 10 native speakers invited to the test and asked to give scores ranging from 1 to 5 in terms of naturalness (NMOS) and speaker/style similarity (SMOS), respectively. We also calculate the real-time factor (RTF) to measure the efficiency of TTS systems.

## 5.Results: 结果

## 6.Conclusions: 结论
