# Single-Codec

<details>
<summary>基本信息</summary>

- 标题: Single-Codec: Single-Codebook Speech Codec towards High-Performance Speech Generation
- 作者:
  - 01 [Hanzhao Li](../../Authors/Hanzhao_Li_(李函昭).md)
  - 02 [Liumeng Xue](../../Authors/Liumeng_Xue_(薛浏蒙).md)
  - 03 [Haohan Guo](../../Authors/Haohan_Guo_(郭浩翰).md)
  - 04 [Xinfa Zhu](../../Authors/Xinfa_Zhu_(朱新发).md)
  - 05 [Yuanjun Lv](../../Authors/Yuanjun_Lv_(吕元骏).md)
  - 06 [Lei Xie](../../Authors/Lei_Xie_(谢磊).md)
  - 07 [Yunlin Chen](../../Authors/Yunlin_Chen_(陈云琳).md) 
  - 08 [Hao Yin](../../Authors/Hao_Yin_(殷昊).md)
  - 09 [Zhifei Li](../../Authors/Zhifei_Li_(李志飞).md)
- 机构:
  - [NPU](../../Institutions/NPU_西北工业大学.md)
  - [香港中文大学 深圳](../../Institutions/CUHK_香港中文大学.md)
  - [香港中文大学](../../Institutions/CUHK_香港中文大学.md)
  - [出门问问 Inc](../../Institutions/Mobvoi_墨百意.md)
- 时间:
  - 预印时间: 2024.06.11 ArXiv v1
  - 更新笔记: 2024.06.20
- 发表:
  - [InterSpeech 2024](../../Publications/InterSpeech.md)
- 链接:
  - [ArXiv](https://www.arxiv.org/abs/2406.07422)
  <!-- - [DOI]() -->
  <!-- - [Github]() -->
  - [Demo](https://kkksuper.github.io/Single-Codec)
  - [Scholar](https://scholar.google.com/scholar?cluster=6055535157486740073)
- 标签:
  - [编解码器](../../Tags/Codec.md)
- 页数: 5
- 引用: 33
- 被引: 0
- 数据:
  - ? 
- 对比:
  - [VQ-VAE](../../Models/_Basis/2017.11.02_VQ-VAE.md)
  - [EnCodec](../../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md)
  - [TiCodec](../../Models/Speech_Neural_Codec/TiCodec.md)
- 复现:
  - ?

</details>

## Abstract: 摘要

> The multi-codebook speech codec enables the application of large language models (LLM) in TTS but bottlenecks efficiency and robustness due to multi-sequence prediction.
> To avoid this obstacle, we propose ***Single-Codec***, a single-codebook single-sequence codec, which employs a disentangled [VQ-VAE](../_Basis/2017.11.02_VQ-VAE.md) to decouple speech into a time-invariant embedding and a phonetically-rich discrete sequence.
> Furthermore, the encoder is enhanced with (1) contextual modeling with a BLSTM module to exploit the temporal information, (2) a hybrid sampling module to alleviate distortion from upsampling and downsampling, and (3) a resampling module to encourage discrete units to carry more phonetic information.
> Compared with multi-codebook codecs, e.g., [EnCodec](../Speech_Neural_Codec/2022.10.24_EnCodec.md) and [TiCodec](../Speech_Neural_Codec/TiCodec.md), SingleCodec demonstrates higher reconstruction quality with a lower bandwidth of only 304bps.
> The effectiveness of Single-Code is further validated by LLM-TTS experiments, showing improved naturalness and intelligibility.

## 1.Introduction: 引言

> Large language models (LLMs) have attracted wide attention in the speech domain, particularly in text-to-speech synthesis (TTS) ([VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md); [SPEAR-TTS](../../Models/Speech_LLM/2023.02.07_SPEAR-TTS.md)).
> In such LLM-based TTS systems, to operate speech synthesis as a simple next-token prediction problem, the first thing is to seek an appropriate speech codec for speech tokenization and waveform reconstruction.
> Multi-codebook codecs ([EnCodec](../Speech_Neural_Codec/2022.10.24_EnCodec.md)), as the SOTA approaches, are widely adopted in LLM-based TTS to achieve superior reconstruction quality.
> However, they also require the LLM to predict multiple discrete sequences, affecting efficiency and stability seriously, although various designs of codec ([SpeechTokenizer](../../Models/Speech_Neural_Codec/2023.08.31_SpeechTokenizer.md); [Language-Codec](../../Models/Speech_Neural_Codec/2024.02.19_Language-Codec.md); [TiCodec](../Speech_Neural_Codec/TiCodec.md)) and LLM ([SoundStorm](../../Models/Speech_LLM/2023.05.16_SoundStorm.md); [UniAudio](../../Models/Speech_LLM/2023.10.01_UniAudio.md); [MAGNeT](../../Models/Speech_LLM/MAGNeT.md)) are proposed to adapt this multi-sequence discrete representation better.
> Hence, seeking an effective approach to obtain the single-sequence discrete speech representation is critical to bypass this limitation.

> However, unlike the text, it is impossible to completely represent the speech audio with abundant information in semantics and acoustics with only one discrete token sequence.
> Although [Tortoise-TTS](../../Models/_tmp/2023.05.12_TorToise-TTS.md) achieves the LLM with the single-sequence discrete speech representation, A diffusion model still needs to be trained to generate Mel Spectrograms from latent embeddings in the LLM of predicted speech units.
> These embeddings contain more information related to the input text and the target speaker to compensate for the compression loss.
> However, this operation introduces more training and inference costs.
> Recently, [TiCodec](../Speech_Neural_Codec/TiCodec.md) proposes introducing an additional global encoder to disentangle time-invariant information out of speech units, reducing the amount of frame-level information that needs encoding.
> It inspires us to re-think speech codec from the perspective of feature disentanglement.

> In this study, we propose a single-codebook neural audio codec, ***Single-Codec***, for high-performance speech generation.
> ***Single-Codec*** performs compression and reconstruction on Mel Spectrogram instead of the raw waveform, enabling efficient compression of speech information while preserving important details, as stated in [Tortoise-TTS](../../Models/_tmp/2023.05.12_TorToise-TTS.md).
> To further enhance the codec performance and applicability to speech synthesis, ***Single-Codec*** incorporates several key components: 
> - A global reference encoder to decouple time-invariant features.
> Specifically, we utilize continuous global representations rather than discrete representations and longer reference segments to capture more acoustic details, enabling embedding sufficient phonetic information into single-codebook discrete units.
> - A BLSTM module for contextual modeling to help discover the correlation between adjacent frames, enhancing speech content clustering efficiency.
> - A hybrid sampling module that uses both convolution and pooling to achieve downsampling, and transposed convolution and replication to achieve upsampling, alleviating upsampling and downsampling distortion.
> - A resampling module to encourage the encoder to extract more phonetics-relevant information with lower short-time variance from the acoustic sequence.

> To the best of our knowledge, ***Single-Codec*** is the first single-codebook codec dedicatedly designed for LLM-based speech generation.
> We comprehensively compare SingleCodec with SOTA multi-codebook codecs, including [EnCodec](../Speech_Neural_Codec/2022.10.24_EnCodec.md) and [TiCodec](../Speech_Neural_Codec/TiCodec.md), by conducting both objective and subjective tests in analysis-synthesis and TTS.
> The results show that ***Single-Codec*** with the lower bandwidth has better speech reconstruction quality, and significantly improves intelligibility, naturalness, and speaker similarity of synthesized speech in zero-shot LLM-TTS.
> Audio samples are available at https://kkksuper.github.io/Single-Codec.

## 2.Related Works: 相关工作

None

## 3.Methodology: 方法

### 3.1.Architecture of ***Single-Codec***: 单码本编解码器架构

![](Images/2024.06.11_Single-Codec_Fig.01.png)

> The architecture of ***Single-Codec*** is shown in [Figure.01](Images/2024.06.11_Single-Codec_Fig.01.png).
> It is built on [Vector Quantized-Variational AutoEncoder (VQ-VAE)](../_Basis/2017.11.02_VQ-VAE.md) with Mel Spectrogram input and reconstruction, similar to [Tortoise-TTS](../../Models/_tmp/2023.05.12_TorToise-TTS.md).
> We adopt a [Conformer](../../Models/_Basis/2020.05.16_Conformer.md)-based encoder to encode a Mel Spectrogram segment seg2 into a latent content representation c, which is then passed to the Vector Quantizer (VQ) for vector quantization.
> The convolution-based decoder reconstructs the Mel Spectrogram ˜seg2 from the quantized content representation c.
> Additionally, we apply a discriminator to improve generation quality ([VQGAN](../_Basis/2020.12.17_VQGAN.md)).
> Finally, we use a neural vocoder [BigVGAN](../../Models/TTS3_Vocoder/2022.06.09_BigVGAN.md) to reconstruct waveform from codec output, i.e., Mel Spectrogram.

> To achieve a high-quality single-codebook codec, we improve the codec architecture with four modules.
> Specifically, we add a reference encoder to decouple time-invariant information in speech from a Mel Spectrogram segment seg1, yielding a global representation g.
> A hybrid sampling module is adopted to alleviate sampling loss.
> Moreover, we introduce a [BLSTM](../../Models/_Basis/BLSTM.md) module and resampling module in both codec encoder and decoder to enhance contextual information and phonetics-relevant information, respectively.

### 3.2.Reference Encoder: 参考编码器

> Speech contains multiple aspects of information, such as time-variant content, time-invariant timbre, and acoustic environment.
> Multiple codebook in codec makes it easy to encode these various information.
> However, for a single-codebook codec, it is challenging to compress all information into a limited number of discrete units.
> To solve this problem, we decouple global information (such as timbre and acoustic environment) that is almost invariable in all frames of the utterance and discretize speech content into code.

> We introduce a reference encoder to derive global representation g that is mainly related to timbre and acoustic environment.
> The input of the reference encoder is a segment seg1 randomly selected from the input utterance.
> We set the length of the segment seg1 for reference input to 600 frames while the input segment seg2 for codec encoder to 200 frames, where the short segment seg2 can reduce the amount of calculation and memory overhead, while the longer reference segment seg1 can help to obtain more robust global features.
> The output g of the reference encoder is fed to the codec encoder and decoder after passing through different linear layers, where it subtracts with output of the encoder blocks and adds to the input of the decoder blocks.

### 3.3.BLSTM Module: BLSTM 模块

> Codecs are generally trained on large-scale speech data to ensure good generalization.
> The diversity of speech content creates challenges for single-codebook codecs with appropriate sizes.
> Unlike [EnCodec](../Speech_Neural_Codec/2022.10.24_EnCodec.md), which introduces the sequence modelling with [LSTM](../../Models/_Basis/2014.02.05_LSTM.md) and finds that it can improve the [Scale-Invariant Signal-to-Noise Ration (SI-SNR)](../../Evaluations/SI-SNR.md), we add BLSTM modules before and after the quantizer to enhance contextual information.
> We found this can improve the efficiency of speech content modelling and make it easier to form stable clustering centers.

### 3.4.Hybrid Sampling Module: 混合采样模块

> Neural codecs usually employ a sampling module to reduce the sequence length of the discrete representation.
> Currently, the up-sampling and down-sampling operations in codecs are usually implemented by convolution, transposed convolution, or pooling and repeat.
> The sampling process inevitably produces sampling loss, resulting in reduced encoding and decoding capabilities.
> Inspired by [MR-HuBERT](../Speech_Representaion/2023.10.04_MR-HuBERT.md), we introduce an improved hybrid sampling module that uses both convolution and pooling to achieve downsampling and transposed convolution and replication to achieve upsampling.
> The combination of different sampling methods can alleviate sampling distortion.

### 3.5.Resampling Module: 重采样模块

> The main goal of a single-codebook speech codec is to extract short-term invariant speech units from acoustic representations.
> The diversity of acoustic representations brings challenges to the learning of codebook vectors.
> To solve this problem, we propose a novel resampling module, which first downsamples the input feature for local modelling and then residual connect after upsampling.
> This bottlenecking operation along the time axis encourages the encoder to extract more phonetics-relevant information with lower short-time variance from the acoustic sequence.

## 4.Experiments: 实验

### 4.1.Dataset: 数据集

> We train speech codecs and a zero-shot TTS system, [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md), using five open-source datasets, including [LibriTTS](../../Datasets/2019.04.05_LibriTTS.md), [Hi-Fi TTS](../../Datasets/Hi-Fi_TTS.md), [VCTK](../../Datasets/2012.08.00_VCTK.md), [AISHELL-1](../../Datasets/AISHELL.md), and [AISHELL3](../../Datasets/AISHELL.md). 
> A total of 1165.3 hours of English and Chinese speech is used.

### 4.2.Comparison Models: 对比模型

> We adopt [EnCodec](../Speech_Neural_Codec/2022.10.24_EnCodec.md) with one codebook ([EnCodec](../Speech_Neural_Codec/2022.10.24_EnCodec.md)-1VQ) and [TiCodec](../Speech_Neural_Codec/TiCodec.md) with one codebook ([TiCodec](../Speech_Neural_Codec/TiCodec.md)-1VQ) and two codebooks ([TiCodec](../Speech_Neural_Codec/TiCodec.md)-2VQ) as the baselines to compare with our proposed ***Single-Codec***.
> For [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md), we use [EnCodec](../Speech_Neural_Codec/2022.10.24_EnCodec.md) with one, four, and eight codebooks, representing [EnCodec](../Speech_Neural_Codec/2022.10.24_EnCodec.md)-1VQ, [EnCodec](../Speech_Neural_Codec/2022.10.24_EnCodec.md)-4VQ, and [EnCodec](../Speech_Neural_Codec/2022.10.24_EnCodec.md)-8VQ, [TiCodec](../Speech_Neural_Codec/TiCodec.md) with one codebook ([TiCodec](../Speech_Neural_Codec/TiCodec.md)-1VQ) as the baselines to evaluate the performance of codecs on speech synthesis.

> To verify the effectiveness of our designed modules in ***Single-Codec***, we conduct ablation studies on the following models. 

> - [VQ-VAE](../_Basis/2017.11.02_VQ-VAE.md): A basic [VQ-VAE](../_Basis/2017.11.02_VQ-VAE.md) codec with a discriminator for perceptual loss, the structure and configuration of the [VQ-VAE](../_Basis/2017.11.02_VQ-VAE.md) codec is similar to that in [Tortoise-TTS](../../Models/_tmp/2023.05.12_TorToise-TTS.md).
> - Ref-short: [VQ-VAE](../_Basis/2017.11.02_VQ-VAE.md) with a reference encoder that consumes a short segment with 200 frames as input.
> - Ref-long: [VQ-VAE](../_Basis/2017.11.02_VQ-VAE.md) with a reference encoder that consumes a long segment with 600 frames as input.
> - Ref-BLSTM: Ref-long with the BLSTM module to verify the effectiveness of the BLSTM module.
> - Ref-HybSam: Ref-long with the hybrid sampling module to verify the effectiveness of the hybrid sampling module.
> - Ref-BLSTM-HybSam: Ref-long with the BLSTM and hybrid sampling modules to verify the effectiveness of the combination of BLSTM and hybrid sampling modules.
> - Ref-BLSTM-HybSam-Conf: Ref-BLSTM-HybSam with the [Conformer](../../Models/_Basis/2020.05.16_Conformer.md)-based encoder, excluding the resampling module.

### 4.3.Model Parameters and Training Details: 模型参数和训练细节

> The audio sample rate is 24khz, and the hop length and window length of the Mel Spectrogram are 256 and 1024, respectively.
> The downsample rate is 4, resulting in a total downsampling of 1024 times (about 23 discrete tokens per second).
> The codebook size is 8192.
> The model size of the codec is 256.
> The sizes of the intermediate hidden states in convolution blocks are 256, 512, and 1024, while the hidden size of the [Conformer](../../Models/_Basis/2020.05.16_Conformer.md) block is 1024.
> The reference encoder consists of 6 layers of 2D convolution with a kernel size of 3 and a [GRU](../_Basis/2014.09.03_GRU.md) layer.
> The [residual block](../../Models/_Basis/2015.12.10_ResNet.md) consists of two residual units.
> Each residual unit includes 2 one-dimensional convolutions with kernel sizes of 3 and 1 respectively.
> Discriminator consists of 4 layers of 2D convolution with a kernel size of 5 and 2 layers of 2D convolution with a kernel size of 3.
> The BLSTM module contains two LSTM layers with a hidden size of 128.

> During training, we conduct 300k iterations using a batch size of 1024 on a single V100 GPU for ***Single-Codec***.
> The baseline model [EnCodec](../Speech_Neural_Codec/2022.10.24_EnCodec.md) utilizes with the code reimplemented in [HiFi-Codec](../Speech_Neural_Codec/2023.05.04_HiFi-Codec.md), and is trained for 25 epochs.
> [TiCodec](../Speech_Neural_Codec/TiCodec.md) is trained for 300k steps with a batch size of 40 on two V100 GPUs.
> For TTS, we employ [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md), reimplemented in [Amphion](../../Toolkits/Amphion/2023.12.15_Amphion.md), with dynamic batch sizing and a maximum token limit of 4000 per batch.
> The single-codebook codec only utilizes the AR stage, while the multiple-codebook codec trains both the AR and NAR stages simultaneously.
> Eight A800 GPUs and 70 epochs are employed for training [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md).

### 4.4.Ablation Studies: 消融研究

> We calculate [STOI](../../Evaluations/STOI.md), [PESQ](../../Evaluations/PESQ.md), [Mel cepstral distortion (MCD)](../../Evaluations/MCD.md), [UTMOS](../../Evaluations/UTMOS.md) and [speaker cosine similarity (SPK)](../../Evaluations/SPK.md) to objectively evaluate the quality of speech reconstruction.
> The test set is composed of 100 randomly selected sentences from unseen speakers.
> The objective result is shown in Table 1.

> Compared with [VQ-VAE](../_Basis/2017.11.02_VQ-VAE.md), either Ref-short or Ref-long obtains better performance on all metrics.
> It indicates that it is effective to decouple global information from speech for the single-codebook codec.
> Moreover, Ref-long outperforms Ref-short in both reconstruction and speaker similarity, suggesting that longer reference segments help capture more accurate time-invariant information and enhance content modelling.
> Ref-BLSTM, Ref-HybSam, and Ref-BLSTM-HybSam get higher reconstruction quality, showing the effectiveness of the BLSTM and hybrid sampling modules.
> Moreover, Ref-BLSTM-HybSam-Con yields on-pair performance with Ref-BLSTM-HybSam but gets further improvement after adding the resampling module, i.e., our proposed Single-Code, achieving the best results.

### 4.5.Commitment Loss Analysis: 承诺损失分析

> We further analyze the commitment loss in the training to explore the impact of different designed modules on the single-codebook codec.
> Commitment loss is the difference between representations before and after quantization.
> The degree of convergence of the commitment loss can reflect the relationship between the encoder output and the cluster center in the codebook.
> As shown in Figure.02, the commitment loss of [VQ-VAE](../_Basis/2017.11.02_VQ-VAE.md) tends to diverge after model training, indicating that the entanglement of time-invariant global information and time-variant content information hinders forming a limited variety of content-related speech units.
> After considering time-invariant decoupled modelling, the loss of Ref-short increases slowly, indicating the effectiveness of global information disentanglement for speech unit learning.
> Ref-long further verifies this result, illustrating the effectiveness of a longer reference segment.
> The loss curve of Ref-HybSam is flat, indicating that the hybrid sampling module effectively improves codec performance.
> Moreover, the losses of the models with context modelling via the BLSTM module are all converged.
> It demonstrates that the models have learned stable phonetic units before quantization, indicating the effectiveness of context modelling in codecs.

> Furthermore, considering the results presented in Table 1, we observe that the commitment loss is not strictly inversely related to reconstruction quality.
> However, the convergence status of the commitment loss (divergence, flat, convergence) is indeed associated with reconstruction quality.
> Specifically, the converged codec surpasses the codec which is not converged.
> This result further highlights the significance of achieving a stable clustering center in the single codebook codec, which directly impacts the overall reconstruction quality.

### 4.6.Speech reconstruction Evaluation: 语音重建评估

> We compare the performance in speech reconstruction of the proposed ***Single-Codec*** with other codecs.
> The results, as presented in Table 1, demonstrate that despite lower bandwidth, the proposed ***Single-Codec*** surpasses other codecs with 1 codebook and is on par with the [TiCodec](../Speech_Neural_Codec/TiCodec.md) with 2 codebooks in terms of reconstruction quality and speaker similarity.
> [VQ-VAE](../_Basis/2017.11.02_VQ-VAE.md) performs better than [EnCodec](../Speech_Neural_Codec/2022.10.24_EnCodec.md) with 1 codebook, demonstrating the high quantization efficiency of codecs operated on Mel Spectrogram.
> Compared to [TiCodec](../Speech_Neural_Codec/TiCodec.md), which also quantized the decoupled time-invariant information, ***Single-Codec*** achieves higher speaker similarity and reconstruction quality, indicating the effectiveness of continuous time-invariant representations and longer reference length.

### 4.7.Zero-Shot TTS Evaluation: 零样本文本转语音评估

> To evaluate the performance of codecs applied in speech synthesis tasks, we train [VALL-E](../../Models/Speech_LLM/2023.01.05_VALL-E.md) using discrete tokens extracted from [EnCodec](../Speech_Neural_Codec/2022.10.24_EnCodec.md) in the number of 1,4,8 codebooks, [TiCodec](../Speech_Neural_Codec/TiCodec.md) in 1 codebook, and ***Single-Codec***.
> We conduct naturalness Mean Opinion Score (N-MOS) and speaker similarity MOS (S-MOS) for subjective evaluation of the synthesized speech.
> The test set consists of 30 sentences, including Chinese and English speech.
> The 20 listeners who are Chinese Mandarin native speakers and familiar to English are invited to participate in each MOS test.
> Meanwhile, we calculate the word error rate (WER) using an ASR model ([Whisper](../../Models/Speech_LLM/Whisper.md)) to measure speech intelligibility.
> We also use [WeSpeaker](../../Models/_tmp/WeSpeaker.md) to extract speaker embedding to calculate the speaker embedding cosine similarity.

> Table 2 shows the subjective and objective results.
> SingleCodec outperforms other models in terms of naturalness and speaker similarity.
> In single-codebook scenes, [TiCodec](../Speech_Neural_Codec/TiCodec.md)-1VQ and ***Single-Codec*** are significantly better than other codec models in speaker similarity, naturalness, and stability.
> This is because decoupling global information makes the frame-level codebook pay more attention to content modelling and enables more global information transmission.
> Meanwhile, SingleCodec performs better than [TiCodec](../Speech_Neural_Codec/TiCodec.md), indicating the effectiveness of continuous global representation and additional content modelling.
> In addition, ***Single-Codec*** exceeds multiple-codebook codecs regarding speaker similarity and naturalness while WER is slightly higher than [EnCodec](../Speech_Neural_Codec/2022.10.24_EnCodec.md)-8VQ.
> This is mainly because the higher bandwidth brings higher-resolution speech unit perception.

## 5.Conclusions: 结论

> In this paper, we propose ***Single-Codec***, the first single-codebook codec dedicatedly designed for LLM-based speech generation.
> ***Single-Codec*** employs a disentangled [VQ-VAE](../_Basis/2017.11.02_VQ-VAE.md) on Mel Spectrograms to decouple speech into time-invariant global embedding and one phonetically-rich discrete sequence quantized by one codebook.
> Furthermore, the encoder is enhanced with a BLSTM module for contextual modelling, a hybrid sampling module to alleviate distortion from upsampling and downsampling, and a resampling module to encourage discrete units to carry more phonetic-relevant information with lower short-time variance.
> In experiments, compared with multi-codebook codecs, e.g. [EnCodec](../Speech_Neural_Codec/2022.10.24_EnCodec.md) and [TiCodec](../Speech_Neural_Codec/TiCodec.md), ***Single-Codec*** demonstrates higher speech reconstruction quality with lower bandwidth of only 304bps, and enables a higher-quality LLM-TTS with better naturalness and intelligibility.
> In the future, we will focus on developing a more efficient single-codebook codec for speech reconstruction and speech synthesis.

