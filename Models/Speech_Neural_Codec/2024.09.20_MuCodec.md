# MuCodec

<details>
<summary>基本信息</summary>

- 标题: "MuCodec: Ultra Low-Bitrate Music Codec"
- 作者: 
  - 01 Yaoxun Xu - Tsinghua University, Shenzhen - xuyx22@mails.tsinghua.edu.cn
  - 02 Hangting Chen - Tencent AI Lab - erichtchen@tencent.com
  - 03 Jianwei Yu - tomasyu@foxmail.com
  - 04 Wei Tan - Tencent AI Lab
  - 05 Rongzhi Gu - Tencent AI Lab
  - 06 Shun Lei - Tsinghua University, Shenzhen
  - 07 Zhiwei Lin - Tsinghua University, Shenzhen
  - 08 Zhiyong Wu - Tsinghua University, Shenzhen/CUHK - zywu$@$sz.tsinghua.edu.cn
- 链接: 
  - [ArXiv](https://arxiv.org/abs/2409.13216)
  - [Publication]() #TODO
  - [Github](https://github.com/xuyaoxun/MuCodec)
  - [Demo](https://xuyaoxun.github.io/MuCodec_demo/)
- 文件: 
  - [ArXiv](_PDF/2409.13216v1__MuCodec__Ultra_Low-Bitrate_Music_Codec.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Music codecs are a vital aspect of audio codec research, and ultra low-bitrate compression holds significant importance for music transmission and generation. 
Due to the complexity of music backgrounds and the richness of vocals, solely relying on modeling semantic or acoustic information cannot effectively reconstruct music with both vocals and backgrounds. To address this issue, we propose MuCodec, specifically targeting music compression and reconstruction tasks at ultra low bitrates. 
MuCodec employs MuEncoder to extract both acoustic and semantic features, discretize them with RVQ, and obtains Mel-VAE features via flow-matching. 
The music is then reconstructed using a pre-trained MEL-VAE decoder and HiFi-GAN.  
MuCodec can reconstruct high-fidelity music at ultra low (0.35kbps) or high bitrates (1.35kbps), achieving the best results to date in both subjective and objective metrics. 
Code and Demo: https://xuyaoxun.github.io/MuCodec_demo/.

</details>
<br>

## 1.Introduction: 引言

Music codecs\cite{codec1,codec2,codec3} are a crucial component in the field of audio codec\cite{audiocodec1,audiocodec2,audiocodec3} research. The significance of ultra low-bitrate compression lies in its potential applications, such as music transmission, where the bitrate of MP3\cite{casas2021mp3} is considerably high, and music generation\cite{copet2024musicgen,gao2024endtoendapproachchordconditionedsong,lei2024songcreatorlyricsbaseduniversalsong}, where short sequences are highly effective for language model construction. Furthermore, considering the diversity of background, sound events, and vocals in music, achieving high-fidelity reconstruction at ultra low bitrates would signify a substantial advancement in the field of universal audio generation.

Recent music compression techniques based on neural codecs\cite{nerualcodec1,nerualcodec2,nerualcodec3,nerualcodec4,nerualcodec5} attempt to compress music directly into discrete tokens. While discrete representations often yield higher compression densities, they inherently suffer from substantial information loss. To reconstruct a more accurate approximation of the original features from discrete tokens, a more robust representation and a stronger decoder are necessary. Common codecs like Encodec\cite{encodec} and Generative Adversarial Networks(GAN)-based methods\cite{gan1,gan2,gan3} exhibit limitations in achieving particularly low bitrates.

In recent years, some research and works have focused on using semantic modeling to represent musical characteristics and utilizing diffusion\cite{diffusion} for reconstruction, such as SemantiCodec\cite{liu2024semanticodec} and SEED-TTS\cite{anastassiou2024seed}. However, these models are not specifically designed for music-related tasks. Compared to speech tasks, music has a rich background, including instruments like piano and bass, and vocals that should be clearly discernible from the background music. Therefore, it is essential to consider both semantic and acoustic information; focusing solely on one aspect would compromise the overall perceptual quality of the reconstructed audio.

To address these challenges, we propose a flow-matching-based\cite{lipman2022flow} music codec MuCodec. MuCodec uses a specialized feature extractor, MuEncoder, based on the two key aspects of music: vocals and background. The MuEncoder features are then discretized using RVQ and employed as conditions for reconstructing Mel-VAE features via flow-matching. We reconstruct the Mel spectrogram by passing the Mel-VAE features through a pre-trained Mel-VAE decoder\cite{liu2024audioldm2}, and ultimately, the music is reconstructed using HiFi-GAN\cite{kong2020hifi}. Our contributions can be summarized as follows:

- We propose MuCodec, which achieves the lowest bitrate to date while maintaining the highest-quality music reconstruction capabilities. 
- MuCodec employs MuEncoder as the feature extractor and Diffusion Transformer (DiT) \cite{dit} along with flow-matching-based method for fine-grained music modeling.
- Both subjective and objective experiments demonstrate that MuCodec achieves the best performance to date in music reconstruction tasks at both low and high bitrates.

## 2.Related Works: 相关工作

## 3.Methodology: 方法

As illustrated in Fig. 1, MuCodec comprises MuEncoder, RVQ, a reconstruction model using flow-matching, Mel-VAE decoder, and HiFi-GAN. MuEncoder is a music extractor, primarily responsible for extracting both acoustic and semantic representations that better capture the characteristics of music. RVQ compresses the representations obtained from MuEncoder. The objective of flow-matching is to reconstruct low-bitrate discrete representations to obtain Mel-VAE features. Subsequently, the pretrained Mel-VAE decoder restores these features into a Mel spectrogram. Finally, the reconstructed music is obtained through a pretrained HiFi-GAN.

### MuEncoder

Music reconstruction is more complex than speech or audio events, as it requires modeling both acoustic background and vocals. We design MuEncoder, composed of 13 stacked Conformer blocks, to extract acoustic and semantic features of background music and vocals.

To enable MuEncoder to extract both acoustic and semantic features, we implement a two-stage training process. In the first stage, we use the Mask Language Model constraint\cite{bestrq} to learn to predict masked regions based on unmasked speech signals, allowing MuEncoder to perceive contextual information and enhance representational capabilities. In the second stage, we introduce two constraints: reconstruction and lyrics recognition constraints. Reconstruction constraint aims to make extracted features closer to acoustic features, with two targets: restoring Mel spectrograms and predicting Constant-Q Transform (CQT)\cite{cqt} features. Lyrics recognition constraint ensures extracted features contain semantic information. These constraints enhance MuEncoder's feature extraction compatibility from both background music and vocal perspectives. 

### Residual Vector Quantization

In MuCodec, we opt to use Residual Vector Quantization (RVQ) to discretize the MuEncoder features for its ability to compress representations through the residual process and provide more refined approximations using cascaded codebooks.

### Flow-Matching

MuCodec employs a flow-matching-based method for reconstruction, as it offers more stable training compared to GAN-based method and requires fewer training steps to achieve better results in ultra low-bitrate reconstruction task. Specifically, we use the discretized MuEncoder representations as a condition and perform finer-grained reconstruction through flow-matching with a Diffusion Transformer.

Instead of choosing the music or its Mel spectrogram as the flow-matching target due to their abundant and complex information, we predict the more manageable and information-rich Mel-VAE features for reconstruction. A pretrained Mel-VAE decoder serves as our Mel spectrogram generator, while a pre-trained HiFi-GAN functions as the music generator.

### Discussion

#### Disentangle

In music reconstruction tasks, the two most important evaluation aspects are vocals and music background. To better verify the benefits of simultaneously focusing on these two features in music reconstruction tasks, we design comparative experiments to model these two aspects separately. Specifically, we choose pre-trained HuBERT\cite{hsu2021hubert} and MERT\cite{li2023mert} models to separately model vocals and music background. HuBERT typically contains richer semantic information, while MERT focuses more on acoustic features. 

#### Scalability

Although MuCodec is initially designed for music reconstruction tasks, it can also be easily applied to other types of audio without incorporating any additional training data, such as speech or acoustic events. MuCodec employs two constraints, one to enhance the background modeling of the audio itself and the other to strengthen the semantic modeling of vocals. As a result, MuCodec exhibits good performance in scenarios with pure vocals, pure background, or both vocals and background simultaneously. Our demo webpage exhibits the reconstruction results of different audio types and presents some other experimental outcomes.

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论