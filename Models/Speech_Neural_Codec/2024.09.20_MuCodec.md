# MuCodec

<details>
<summary>基本信息</summary>

- 标题: "MuCodec: Ultra Low-Bitrate Music Codec"
- 作者: 
  - 01 Yaoxun Xu - Tsinghua University, Shenzhen - xuyx22@mails.tsinghua.edu.cn
  - 02 Hangting Chen - Tencent AI Lab - erichtchen@tencent.com
  - 03 Jianwei Yu - tomasyu@foxmail.com
  - 04 Wei Tan - Tencent AI Lab
  - 05 Rongzhi Gu - Tencent AI Lab
  - 06 Shun Lei - Tsinghua University, Shenzhen
  - 07 Zhiwei Lin - Tsinghua University, Shenzhen
  - 08 Zhiyong Wu - Tsinghua University, Shenzhen/CUHK - zywu$@$sz.tsinghua.edu.cn
- 链接: 
  - [ArXiv](https://arxiv.org/abs/2409.13216)
  - [Publication]() #TODO
  - [Github](https://github.com/xuyaoxun/MuCodec)
  - [Demo](https://xuyaoxun.github.io/MuCodec_demo/)
- 文件: 
  - [ArXiv](_PDF/2409.13216v1__MuCodec__Ultra_Low-Bitrate_Music_Codec.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Music codecs are a vital aspect of audio codec research, and ultra low-bitrate compression holds significant importance for music transmission and generation. 
Due to the complexity of music backgrounds and the richness of vocals, solely relying on modeling semantic or acoustic information cannot effectively reconstruct music with both vocals and backgrounds. To address this issue, we propose MuCodec, specifically targeting music compression and reconstruction tasks at ultra low bitrates. 
MuCodec employs MuEncoder to extract both acoustic and semantic features, discretize them with RVQ, and obtains Mel-VAE features via flow-matching. 
The music is then reconstructed using a pre-trained MEL-VAE decoder and HiFi-GAN.  
MuCodec can reconstruct high-fidelity music at ultra low (0.35kbps) or high bitrates (1.35kbps), achieving the best results to date in both subjective and objective metrics. 
Code and Demo: https://xuyaoxun.github.io/MuCodec_demo/.

</details>
<br>

## 1.Introduction: 引言

Music codecs\cite{codec1,codec2,codec3} are a crucial component in the field of audio codec\cite{audiocodec1,audiocodec2,audiocodec3} research. The significance of ultra low-bitrate compression lies in its potential applications, such as music transmission, where the bitrate of MP3\cite{casas2021mp3} is considerably high, and music generation\cite{copet2024musicgen,gao2024endtoendapproachchordconditionedsong,lei2024songcreatorlyricsbaseduniversalsong}, where short sequences are highly effective for language model construction. Furthermore, considering the diversity of background, sound events, and vocals in music, achieving high-fidelity reconstruction at ultra low bitrates would signify a substantial advancement in the field of universal audio generation.

Recent music compression techniques based on neural codecs\cite{nerualcodec1,nerualcodec2,nerualcodec3,nerualcodec4,nerualcodec5} attempt to compress music directly into discrete tokens. While discrete representations often yield higher compression densities, they inherently suffer from substantial information loss. To reconstruct a more accurate approximation of the original features from discrete tokens, a more robust representation and a stronger decoder are necessary. Common codecs like Encodec\cite{encodec} and Generative Adversarial Networks(GAN)-based methods\cite{gan1,gan2,gan3} exhibit limitations in achieving particularly low bitrates.

In recent years, some research and works have focused on using semantic modeling to represent musical characteristics and utilizing diffusion\cite{diffusion} for reconstruction, such as SemantiCodec\cite{liu2024semanticodec} and SEED-TTS\cite{anastassiou2024seed}. However, these models are not specifically designed for music-related tasks. Compared to speech tasks, music has a rich background, including instruments like piano and bass, and vocals that should be clearly discernible from the background music. Therefore, it is essential to consider both semantic and acoustic information; focusing solely on one aspect would compromise the overall perceptual quality of the reconstructed audio.

To address these challenges, we propose a flow-matching-based\cite{lipman2022flow} music codec MuCodec. MuCodec uses a specialized feature extractor, MuEncoder, based on the two key aspects of music: vocals and background. The MuEncoder features are then discretized using RVQ and employed as conditions for reconstructing Mel-VAE features via flow-matching. We reconstruct the Mel spectrogram by passing the Mel-VAE features through a pre-trained Mel-VAE decoder\cite{liu2024audioldm2}, and ultimately, the music is reconstructed using HiFi-GAN\cite{kong2020hifi}. Our contributions can be summarized as follows:

- We propose MuCodec, which achieves the lowest bitrate to date while maintaining the highest-quality music reconstruction capabilities. 
- MuCodec employs MuEncoder as the feature extractor and Diffusion Transformer (DiT) \cite{dit} along with flow-matching-based method for fine-grained music modeling.
- Both subjective and objective experiments demonstrate that MuCodec achieves the best performance to date in music reconstruction tasks at both low and high bitrates.

## 2.Related Works: 相关工作

## 3.Methodology: 方法

As illustrated in Fig. 1, MuCodec comprises MuEncoder, RVQ, a reconstruction model using flow-matching, Mel-VAE decoder, and HiFi-GAN. MuEncoder is a music extractor, primarily responsible for extracting both acoustic and semantic representations that better capture the characteristics of music. RVQ compresses the representations obtained from MuEncoder. The objective of flow-matching is to reconstruct low-bitrate discrete representations to obtain Mel-VAE features. Subsequently, the pretrained Mel-VAE decoder restores these features into a Mel spectrogram. Finally, the reconstructed music is obtained through a pretrained HiFi-GAN.

### MuEncoder

Music reconstruction is more complex than speech or audio events, as it requires modeling both acoustic background and vocals. We design MuEncoder, composed of 13 stacked Conformer blocks, to extract acoustic and semantic features of background music and vocals.

To enable MuEncoder to extract both acoustic and semantic features, we implement a two-stage training process. In the first stage, we use the Mask Language Model constraint\cite{bestrq} to learn to predict masked regions based on unmasked speech signals, allowing MuEncoder to perceive contextual information and enhance representational capabilities. In the second stage, we introduce two constraints: reconstruction and lyrics recognition constraints. Reconstruction constraint aims to make extracted features closer to acoustic features, with two targets: restoring Mel spectrograms and predicting Constant-Q Transform (CQT)\cite{cqt} features. Lyrics recognition constraint ensures extracted features contain semantic information. These constraints enhance MuEncoder's feature extraction compatibility from both background music and vocal perspectives. 

### Residual Vector Quantization

In MuCodec, we opt to use Residual Vector Quantization (RVQ) to discretize the MuEncoder features for its ability to compress representations through the residual process and provide more refined approximations using cascaded codebooks.

### Flow-Matching

MuCodec employs a flow-matching-based method for reconstruction, as it offers more stable training compared to GAN-based method and requires fewer training steps to achieve better results in ultra low-bitrate reconstruction task. Specifically, we use the discretized MuEncoder representations as a condition and perform finer-grained reconstruction through flow-matching with a Diffusion Transformer.

Instead of choosing the music or its Mel spectrogram as the flow-matching target due to their abundant and complex information, we predict the more manageable and information-rich Mel-VAE features for reconstruction. A pretrained Mel-VAE decoder serves as our Mel spectrogram generator, while a pre-trained HiFi-GAN functions as the music generator.

### Discussion

#### Disentangle

In music reconstruction tasks, the two most important evaluation aspects are vocals and music background. To better verify the benefits of simultaneously focusing on these two features in music reconstruction tasks, we design comparative experiments to model these two aspects separately. Specifically, we choose pre-trained HuBERT\cite{hsu2021hubert} and MERT\cite{li2023mert} models to separately model vocals and music background. HuBERT typically contains richer semantic information, while MERT focuses more on acoustic features. 

#### Scalability

Although MuCodec is initially designed for music reconstruction tasks, it can also be easily applied to other types of audio without incorporating any additional training data, such as speech or acoustic events. MuCodec employs two constraints, one to enhance the background modeling of the audio itself and the other to strengthen the semantic modeling of vocals. As a result, MuCodec exhibits good performance in scenarios with pure vocals, pure background, or both vocals and background simultaneously. Our demo webpage exhibits the reconstruction results of different audio types and presents some other experimental outcomes.

## 4.Experiments: 实验

To train MuCodec, we utilize a large-scale internal music dataset of Chinese and English songs with a minimum 32kHz sampling rate. We segment the music into fixed 35.84-second lengths during training. For fairness, the test set comprises randomly selected 250 Chinese and 250 English song segments, each 20-30 seconds long with corresponding lyrics.

For the GAN-based method, we use a fully convolutional architecture following Descript Audio Codec(DAC)\cite{dac} encoder and decoder, changing the quantizer to RVQ. We match its model size to MuCodec. To further analyze, we also experiment with replacing its encoder directly with MuEncoder.

Considering GANs' weak reconstruction capabilities in low-bitrate scenarios, GAN-based method experiments are trained for 120k steps. In other cases, unless specifically stated, all test models train for 20k steps to demonstrate our approach's effectiveness within reasonable training time. 
Regarding SemantiCodec, we select two settings with bitrates similar to the high and low bitrates used in our experiments.

To better evaluate the performance of reconstructed music, we adopt both subjective and objective assessments. In subjective evaluations, we randomly select 5 Chinese and 5 English song clips as the test set and invite 10 professional participants to conduct a MUSHRA-inspired\cite{mushra} listening test. In objective evaluations, we choose two types of metrics corresponding to the two aspects of music: background and vocals. We use ViSQOL\cite{hines2015visqol} as an audio quality assessment metric.
Since the background can interfere with vocal evaluation, we separate the vocals and background of the generated music using a pre-trained separation model\cite{seperate}. We then calculate the similarity between the generated vocal part and the original vocal part with a pre-trained speaker similarity model\cite{speaker} and use Whisper-v2\cite{radford2023robust} to compute the Word Error Rate (WER) of the generated vocal part as the vocal clarity evaluation metric.

In the MuEncoder setting, we employ a 13-layer Conformer\cite{gulati2020conformer} model. We assign a weight of 1 to the music reconstruction loss and 0.2 to the lyrics recognition loss, which consists of both CTC Loss and RNN-T Loss.

In the RVQ setting, we design two configurations for high- and low-bitrate scenarios. For low-bitrate scenarios, we employ a single codebook with a size of 16,384 and a bitrate of 0.35kbps. Conversely, for the relatively high-bitrate scenarios, we use four codebooks, each with a size of 10,000, and achieve a bitrate of 1.33kbps. 

In the flow-matching setting, we employ a 24-layer Transformer2d model\cite{transformer2d} for reconstruction, featuring an attention head dimension of 72, a norm epsilon of 1e-06, and 32 norm groups. We use ada norm single as the norm type and set the number of ada norm embeds to 1000. 
During the generation process, we utilize sampling via classifier-free guidance\cite{cfg}, specifically setting the guidance scale value to 1.5. 

During inference, we choose a denoising step size of 50 for flow-matching to balance reconstruction quality and computational efficiency. We use a pre-trained open-source Mel-VAE decoder and HiFi-GAN for both training and inference. Our experiments run on 8 40G-A100 GPUs with a batch size of 4. 

## 5.Results: 结果

## 6.Conclusions: 结论