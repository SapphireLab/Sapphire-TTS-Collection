# Articulatory Encodec

<details>
<summary>基本信息</summary>

- 标题: Articulatory Encodec: Vocal Tract Kinematics as a Codec for Speech
- 作者:
  - Cheol Jun Cho
  - Peter Wu
  - Tejas S. Prabhune
  - Dhruv Agarwal
  - Gopala K. Anumanchipalli
- 机构:
  - 机构 
- 时间:
  - 预印时间: 2024.06.18 ArXiv v1
  - 更新笔记: 2024.08.07
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.12998)
  - [DOI]()
  - [Github]()
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: ?
- 引用: ?
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Vocal tract articulation is a natural, grounded control space of speech production. The spatiotemporal coordination of articulators combined with the vocal source shapes intelligible speech sounds to enable effective spoken communication. Based on this physiological grounding of speech, we propose a new framework of neural encoding-decoding of speech -- \textit{articulatory encodec}. The articulatory encodec comprises an articulatory analysis model that infers articulatory features from speech audio, and an articulatory synthesis model that synthesizes speech audio from articulatory features. The articulatory features are kinematic traces of vocal tract articulators and source features, which are intuitively interpretable and controllable, being the actual physical interface of speech production. An additional speaker identity encoder is jointly trained with the articulatory synthesizer to inform the voice texture of individual speakers. By training on large-scale speech data, we achieve a fully intelligible, high-quality articulatory synthesizer that generalizes to unseen speakers. Furthermore, the speaker embedding is effectively disentangled from articulations, which enables accent-perserving zero-shot voice conversion. To the best of our knowledge, this is the first demonstration of universal, high-performance articulatory inference and synthesis, suggesting the proposed framework as a powerful coding system of speech.

## 1.Introduction: 引言

\section{Introduction}
Humans naturally produce intelligible speech by controlling articulators on the vocal tract. Such vocal tract articulation has long been claimed to be the physiological ground of speech production in various aspects. The source-filter theory of speech describes articulation as shaping the vocal cavity to implement filters that are applied to glottal flow to shape speech sounds \cite{chiba1958vowel, fant1971acoustic}. Articulatory phonetics and phonology have explained the basis of speech in terms of the coordination of articulators, identifying some canonical articulators that can determine the phonetic properties \cite{maeda1990compensatory, browman1992articulatory, international1999handbook}. From deep down in the brain, the speech sensorimotor cortex has been proven to represent continuous, real-time vocal tract articulation while naturally speaking, suggesting the vocal tract articulation as a cognitive basis of speech production \cite{chartier2018encoding, anumanchipalli2019speech, cho2023neural}. 

Furthermore, the recent findings by Cho et al. \cite{cho2023evidence,cho2023self} suggest that articulatory inversion naturally emerges from self-supervised learning (SSL) of speech. When probed on articulatory kinematics measured by electromagnetic articulography (EMA), the features representation of the recent speech SSL models (e.g., HuBERT \cite{hsu2021hubert}) is highly correlated with EMA, where high-fidelity articulation can be reconstructed by a simple linear mapping from speech SSL features \cite{cho2023evidence}. This suggests that the articulatory inference is a natural solution of SSL of speech for abstracting the speech information. This emergent property is further shown to be universal to any speakers, dialects, and even languages \cite{cho2023self}. Together, these suggest that the biophysical, articulatory representation of speech is a shared coding principle in both biological and artificial intelligence of speech. 
\input{figures/intro_scheme}
However, despite the ubiquity of articulatory coding in speech science, an effective and scalable articulatory coding system for speech has not yet been demonstrated, which requires a generalizable articulatory synthesis model that synthesizes speech back from the articulatory inputs, along with a robust encoding model for articulatory features. Previous studies have demonstrated that intelligible speech can be synthesized from articulatory features \cite{birkholz2013prev_artsynth, krug2021vocaltractlab, wu2022artsynth, kim2023style, gao2024copysynthesis}. Combined with acoustics-to-articulatory inversion (AAI), resynthesis frameworks have shown the potential of articulatory features as viable intermediate for speech coding systems \cite{wu2023speakerind, gao2024copysynthesis}. However, the previous methods are limited to a fixed set of speakers and the qualities are still far behind the commercial speech synthesis models. This absence of a universal, generalizable framework has significantly limited the utility of articulatory-based speech coding as a practically usable system. 

Here, we first demonstrate a high-performance, universal articulatory encoder and decoder that can scale and generalize across an indefinite number of speakers. We leverage the universal articulatory inference by speech SSL \cite{cho2023self} to build a generalizable articulatory encoder that transforms speech into a template articulatory space. The template articulatory space is agnostic to individual anatomical differences which are compensated by a separate speaker identity encoder. By training a vocoder with a large-scale dataset, we achieve a universal articulatory vocoder that can generate fully intelligible, high-quality speech from any speaker's articulation. Furthermore, the speaker embedding learned by the speaker identity encoder enables a zero-shot, dialect-preserving voice conversion. By closing the loop of articulatory encoding and decoding, we propose a novel, speech science guided encoding-decoding (encodec) framework of speech -- \textit{articulatory encodec}.\footnote{We will open-source the code and model checkpoints upon publication.} The articulatory encodec shows a minimal loss of intelligibility and quality compared to the original speech audio.

Compared to existing neural coding of speech \cite{choi2021nansy, choi2022nansy++, zeghidour2021soundstream, defossez2022encodec, ju2024naturalspeech}, representing speech as articulatory features has following benefits:

\begin{itemize}
\item{\textbf{Low-dimensionality}: The articulatory features have only 14 channels with 50 Hz sampling rate. This is significantly lower than the previous acoustic features or neural embedding of speech. }
\item{\textbf{Interpretability}: Each channel corresponds to the actual physical articulator on the vocal tract, which can be intuitively interpretable by visualization on the vocal tract.}
\item{\textbf{Controllability}: The articulatory features can be naturally controlled by the same principle as speech production.}
\item{\textbf{Universality}: The articulatory encoding is universal across speakers despite and disentangled from individual anatomical variance.}
\end{itemize}

With these unique benefits, empirical evidence and demonstration show the promising potential of the proposed articulatory encodec as a valid, novel coding framework of speech.\footnote{Audio samples are available at https://articulatoryencodec.github.io.}

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
