# Articulatory Encodec

<details>
<summary>基本信息</summary>

- 标题: Articulatory Encodec: Vocal Tract Kinematics as a Codec for Speech
- 作者:
  - Cheol Jun Cho
  - Peter Wu
  - Tejas S. Prabhune
  - Dhruv Agarwal
  - Gopala K. Anumanchipalli
- 机构:
  - 机构 
- 时间:
  - 预印时间: 2024.06.18 ArXiv v1
  - 更新笔记: 2024.08.07
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.12998)
  - [DOI]()
  - [Github]()
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: ?
- 引用: ?
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Vocal tract articulation is a natural, grounded control space of speech production. The spatiotemporal coordination of articulators combined with the vocal source shapes intelligible speech sounds to enable effective spoken communication. Based on this physiological grounding of speech, we propose a new framework of neural encoding-decoding of speech -- \textit{articulatory encodec}. The articulatory encodec comprises an articulatory analysis model that infers articulatory features from speech audio, and an articulatory synthesis model that synthesizes speech audio from articulatory features. The articulatory features are kinematic traces of vocal tract articulators and source features, which are intuitively interpretable and controllable, being the actual physical interface of speech production. An additional speaker identity encoder is jointly trained with the articulatory synthesizer to inform the voice texture of individual speakers. By training on large-scale speech data, we achieve a fully intelligible, high-quality articulatory synthesizer that generalizes to unseen speakers. Furthermore, the speaker embedding is effectively disentangled from articulations, which enables accent-perserving zero-shot voice conversion. To the best of our knowledge, this is the first demonstration of universal, high-performance articulatory inference and synthesis, suggesting the proposed framework as a powerful coding system of speech.

## 1.Introduction: 引言

\section{Introduction}
Humans naturally produce intelligible speech by controlling articulators on the vocal tract. Such vocal tract articulation has long been claimed to be the physiological ground of speech production in various aspects. The source-filter theory of speech describes articulation as shaping the vocal cavity to implement filters that are applied to glottal flow to shape speech sounds \cite{chiba1958vowel, fant1971acoustic}. Articulatory phonetics and phonology have explained the basis of speech in terms of the coordination of articulators, identifying some canonical articulators that can determine the phonetic properties \cite{maeda1990compensatory, browman1992articulatory, international1999handbook}. From deep down in the brain, the speech sensorimotor cortex has been proven to represent continuous, real-time vocal tract articulation while naturally speaking, suggesting the vocal tract articulation as a cognitive basis of speech production \cite{chartier2018encoding, anumanchipalli2019speech, cho2023neural}. 

Furthermore, the recent findings by Cho et al. \cite{cho2023evidence,cho2023self} suggest that articulatory inversion naturally emerges from self-supervised learning (SSL) of speech. When probed on articulatory kinematics measured by electromagnetic articulography (EMA), the features representation of the recent speech SSL models (e.g., HuBERT \cite{hsu2021hubert}) is highly correlated with EMA, where high-fidelity articulation can be reconstructed by a simple linear mapping from speech SSL features \cite{cho2023evidence}. This suggests that the articulatory inference is a natural solution of SSL of speech for abstracting the speech information. This emergent property is further shown to be universal to any speakers, dialects, and even languages \cite{cho2023self}. Together, these suggest that the biophysical, articulatory representation of speech is a shared coding principle in both biological and artificial intelligence of speech. 
\input{figures/intro_scheme}
However, despite the ubiquity of articulatory coding in speech science, an effective and scalable articulatory coding system for speech has not yet been demonstrated, which requires a generalizable articulatory synthesis model that synthesizes speech back from the articulatory inputs, along with a robust encoding model for articulatory features. Previous studies have demonstrated that intelligible speech can be synthesized from articulatory features \cite{birkholz2013prev_artsynth, krug2021vocaltractlab, wu2022artsynth, kim2023style, gao2024copysynthesis}. Combined with acoustics-to-articulatory inversion (AAI), resynthesis frameworks have shown the potential of articulatory features as viable intermediate for speech coding systems \cite{wu2023speakerind, gao2024copysynthesis}. However, the previous methods are limited to a fixed set of speakers and the qualities are still far behind the commercial speech synthesis models. This absence of a universal, generalizable framework has significantly limited the utility of articulatory-based speech coding as a practically usable system. 

Here, we first demonstrate a high-performance, universal articulatory encoder and decoder that can scale and generalize across an indefinite number of speakers. We leverage the universal articulatory inference by speech SSL \cite{cho2023self} to build a generalizable articulatory encoder that transforms speech into a template articulatory space. The template articulatory space is agnostic to individual anatomical differences which are compensated by a separate speaker identity encoder. By training a vocoder with a large-scale dataset, we achieve a universal articulatory vocoder that can generate fully intelligible, high-quality speech from any speaker's articulation. Furthermore, the speaker embedding learned by the speaker identity encoder enables a zero-shot, dialect-preserving voice conversion. By closing the loop of articulatory encoding and decoding, we propose a novel, speech science guided encoding-decoding (encodec) framework of speech -- \textit{articulatory encodec}.\footnote{We will open-source the code and model checkpoints upon publication.} The articulatory encodec shows a minimal loss of intelligibility and quality compared to the original speech audio.

Compared to existing neural coding of speech \cite{choi2021nansy, choi2022nansy++, zeghidour2021soundstream, defossez2022encodec, ju2024naturalspeech}, representing speech as articulatory features has following benefits:

\begin{itemize}
\item{\textbf{Low-dimensionality}: The articulatory features have only 14 channels with 50 Hz sampling rate. This is significantly lower than the previous acoustic features or neural embedding of speech. }
\item{\textbf{Interpretability}: Each channel corresponds to the actual physical articulator on the vocal tract, which can be intuitively interpretable by visualization on the vocal tract.}
\item{\textbf{Controllability}: The articulatory features can be naturally controlled by the same principle as speech production.}
\item{\textbf{Universality}: The articulatory encoding is universal across speakers despite and disentangled from individual anatomical variance.}
\end{itemize}

With these unique benefits, empirical evidence and demonstration show the promising potential of the proposed articulatory encodec as a valid, novel coding framework of speech.\footnote{Audio samples are available at https://articulatoryencodec.github.io.}

## 2.Related Works: 相关工作

### Electromagnetic Articulography

%While a various recording device has been suggested to measure articulation \cite{hueber2010ultrasound, gaddy2020digital, gonzalez2020silent, lim2021rtmri, rebernik2021review}, the most common and straightforward measurement is 

Electromagnetic articulography (EMA) measures time-varying displacements of vocal tract articulators synchronously while speaking. Typically, sensors are placed on the upper lip (UL), lower lip (LL), lower incisor (LI), tongue tip (TT), tongue blade (TB), and tongue dorsum (TD) (Fig. \ref{fig:pipeline}) \cite{rebernik2021review}. A combination of displacements of these articulators on the midsagittal plane configures a place of articulation, which shapes phonetic content by combining with source information, or manner of articulation. As the traces are continuously collected in real-time, the EMA data naturally reflect phoneme contextualization, or coarticulation, and individual tendencies in pronunciations (dialects and accents). Given these properties, EMA has been widely accepted for studying articulatory bases of speech, providing biophysical evidence for many linguistic or cognitive theories of speech production \cite{ browman1992articulatory, rebernik2021review, chartier2018encoding}. However, EMA has been significantly limited to scale due to the complicated nature and high cost of the collection procedure. %many phonetics theories have been suggested based on suggested that the phonetic properties of speech can be configured by the midsagittal displacement of these 6 articulators \cite{}.
%However, articulatory data including EMA is significantly limited to scale due to the complicated recording procedure \cite{rebernik2021review}. 
%Note that velum is mostly excluded in EMA data since putting a sensor on the velum causes significant discomfort to participants, though velum is an important factor for nasal sounds \cite{rebernik2021review}. However, for normal speakers, velum functions as a gate for letting air flow while the oral cavity is blocked, which is acoustically reflected as a voice onset time (VOT). Therefore, we hypothesize that this missing information can be compensated by source vocalization, which is activated while the oral cavity is closed.


### Acoustics-to-Articulation Inversion

To replace the complicated data collection procedure, acoustic-to-articulatory inversion (AAI) models have been actively developed to predict EMA directly from speech audio \cite{ghosh2010generalized,  ghosh2011subject, liu2015deep, chartier2018encoding, anumanchipalli2019speech, wu2023speakerind, attia2023improving, siriwardena2023secret, gao2024copysynthesis}. However, the individual variance in vocal tract anatomy across speakers induces inconsistent placements of sensors, which has posed a significant barrier to developing a model that can apply to unseen speakers \cite{rahim1993annartsynth, wu2023speakerind, attia2023improving, siriwardena2023secret}. Despite such variability, a canonical basis of articulation is suggested to exist, which is agnostic to individual vocal tract anatomy \cite{cho2023self}. In fact, Cho et al. \cite{cho2023self} demonstrated that a linear affine transformation can geometrically align one speaker's articulatory system to another's. This suggests that individual articulatory spaces are lying on the same linear space so that an articulatory space of any speaker can be a hypothetical universal template space of articulation. We empirically prove this statement by leveraging a single-speaker AAI model as a universal articulatory encoder for our codec framework. %Therefore, one good AAI model fit to a single speaker's EMA sensor arrangement can be used as a universal AAI model. %Furthermore, an AAI model is implemented by a frozen speech SSL model with a linear head inserted in the later layer of Transformer encoder where speaker information is marginalized out.  

### Articulatory Synthesis

Articulatory synthesis aims to generate speech audio from articulatory features. A century of efforts have been made to build articulatory synthesizers for basic research of speech \cite{dudley1939synthetic, dudley1939remaking, dunn1950calculation, stevens1953electrical, rosen1958dynamic, mermelstein1973articulatory, rubin1981articulatory, maeda1982digital, scully1990articulatory}. Several methods have been proposed for improving intelligibility and quality%\cite{birkholz2013prev_artsynth, aryal2016data, anumanchipalli2019speech, kim2023style, wu2022artsynth}
, demonstrating broader use cases including text-to-speech (TTS) \cite{krug2021vocaltractlab}, prosody manipulation \cite{aryal2014accent, birkholz2017manipulation}, and speech brain-computer interfaces (BCIs) \cite{anumanchipalli2019speech}. Some of these works utilized deep learning models to map articulatory features to acoustic features, which are then converted to audio using pretrained acoustic synthesizers \cite{aryal2016data, anumanchipalli2019speech, kim2023style}. A recent study shows that a GAN-based generative model can directly synthesize speech waveform from articulatory features with high intelligibility \cite{wu2022artsynth}. However, to our knowledge, none of the existing approaches has achieved industrial-level performance, which requires high intelligibility, quality, and generalizability across unseen speakers. 


### Neural Codec of Speech

Many deep learning methods have been proposed to learn data-driven representations of speech. Various autoencoder methods have been suggested to jointly train encoders that compress audio into low-bitrate discrete units \cite{van2017neural, zeghidour2021soundstream, defossez2022encodec} or decompose speech into different factors \cite{choi2022nansy++, zhang2023speechtokenizer, ju2024naturalspeech}, and decoders that reconstruct speech from encoded features with minimal loss of information. Also, pretrained speech SSL models have been utilized to extract rich linguistic content of speech, and synthesizers are trained to restore speech audio from those features \cite{choi2021nansy, polyak2021resynthesis, choi2022nansy++,  lee2022hierspeech, lee2023hierspeech++, guo2023quickvc}. These SSL-based methods often utilize separate source modeling (e.g., pitch) and speaker encoding, since SSL model encoders tend to marginalize out acoustic and speaker information \cite{chen2022spkinfo, pasad2023comparative}. We categorize all these kinds of closed-loop frameworks utilizing neural networks for both encoding and decoding as \emph{neural codecs}. Though the existing methods achieve high fidelity in representing speech audio, they significantly lack interpretability and controllability. 

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
