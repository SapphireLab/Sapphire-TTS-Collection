# Articulatory Encodec

<details>
<summary>基本信息</summary>

- 标题: Articulatory Encodec: Vocal Tract Kinematics as a Codec for Speech
- 作者:
  - Cheol Jun Cho
  - Peter Wu
  - Tejas S. Prabhune
  - Dhruv Agarwal
  - Gopala K. Anumanchipalli
- 机构:
  - 机构 
- 时间:
  - 预印时间: 2024.06.18 ArXiv v1
  - 更新笔记: 2024.08.07
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.12998)
  - [DOI]()
  - [Github]()
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: ?
- 引用: ?
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Vocal tract articulation is a natural, grounded control space of speech production. The spatiotemporal coordination of articulators combined with the vocal source shapes intelligible speech sounds to enable effective spoken communication. Based on this physiological grounding of speech, we propose a new framework of neural encoding-decoding of speech -- \textit{articulatory encodec}. The articulatory encodec comprises an articulatory analysis model that infers articulatory features from speech audio, and an articulatory synthesis model that synthesizes speech audio from articulatory features. The articulatory features are kinematic traces of vocal tract articulators and source features, which are intuitively interpretable and controllable, being the actual physical interface of speech production. An additional speaker identity encoder is jointly trained with the articulatory synthesizer to inform the voice texture of individual speakers. By training on large-scale speech data, we achieve a fully intelligible, high-quality articulatory synthesizer that generalizes to unseen speakers. Furthermore, the speaker embedding is effectively disentangled from articulations, which enables accent-perserving zero-shot voice conversion. To the best of our knowledge, this is the first demonstration of universal, high-performance articulatory inference and synthesis, suggesting the proposed framework as a powerful coding system of speech.

## 1.Introduction: 引言

Humans naturally produce intelligible speech by controlling articulators on the vocal tract. Such vocal tract articulation has long been claimed to be the physiological ground of speech production in various aspects. The source-filter theory of speech describes articulation as shaping the vocal cavity to implement filters that are applied to glottal flow to shape speech sounds \cite{chiba1958vowel, fant1971acoustic}. Articulatory phonetics and phonology have explained the basis of speech in terms of the coordination of articulators, identifying some canonical articulators that can determine the phonetic properties \cite{maeda1990compensatory, browman1992articulatory, international1999handbook}. From deep down in the brain, the speech sensorimotor cortex has been proven to represent continuous, real-time vocal tract articulation while naturally speaking, suggesting the vocal tract articulation as a cognitive basis of speech production \cite{chartier2018encoding, anumanchipalli2019speech, cho2023neural}. 

Furthermore, the recent findings by Cho et al. \cite{cho2023evidence,cho2023self} suggest that articulatory inversion naturally emerges from self-supervised learning (SSL) of speech. When probed on articulatory kinematics measured by electromagnetic articulography (EMA), the features representation of the recent speech SSL models (e.g., HuBERT \cite{hsu2021hubert}) is highly correlated with EMA, where high-fidelity articulation can be reconstructed by a simple linear mapping from speech SSL features \cite{cho2023evidence}. This suggests that the articulatory inference is a natural solution of SSL of speech for abstracting the speech information. This emergent property is further shown to be universal to any speakers, dialects, and even languages \cite{cho2023self}. Together, these suggest that the biophysical, articulatory representation of speech is a shared coding principle in both biological and artificial intelligence of speech. 

However, despite the ubiquity of articulatory coding in speech science, an effective and scalable articulatory coding system for speech has not yet been demonstrated, which requires a generalizable articulatory synthesis model that synthesizes speech back from the articulatory inputs, along with a robust encoding model for articulatory features. Previous studies have demonstrated that intelligible speech can be synthesized from articulatory features \cite{birkholz2013prev_artsynth, krug2021vocaltractlab, wu2022artsynth, kim2023style, gao2024copysynthesis}. Combined with acoustics-to-articulatory inversion (AAI), resynthesis frameworks have shown the potential of articulatory features as viable intermediate for speech coding systems \cite{wu2023speakerind, gao2024copysynthesis}. However, the previous methods are limited to a fixed set of speakers and the qualities are still far behind the commercial speech synthesis models. This absence of a universal, generalizable framework has significantly limited the utility of articulatory-based speech coding as a practically usable system. 

Here, we first demonstrate a high-performance, universal articulatory encoder and decoder that can scale and generalize across an indefinite number of speakers. We leverage the universal articulatory inference by speech SSL \cite{cho2023self} to build a generalizable articulatory encoder that transforms speech into a template articulatory space. The template articulatory space is agnostic to individual anatomical differences which are compensated by a separate speaker identity encoder. By training a vocoder with a large-scale dataset, we achieve a universal articulatory vocoder that can generate fully intelligible, high-quality speech from any speaker's articulation. Furthermore, the speaker embedding learned by the speaker identity encoder enables a zero-shot, dialect-preserving voice conversion. By closing the loop of articulatory encoding and decoding, we propose a novel, speech science guided encoding-decoding (encodec) framework of speech -- \textit{articulatory encodec}.\footnote{We will open-source the code and model checkpoints upon publication.} The articulatory encodec shows a minimal loss of intelligibility and quality compared to the original speech audio.

Compared to existing neural coding of speech \cite{choi2021nansy, choi2022nansy++, zeghidour2021soundstream, defossez2022encodec, ju2024naturalspeech}, representing speech as articulatory features has following benefits:

> - \textbf{Low-dimensionality}: The articulatory features have only 14 channels with 50 Hz sampling rate. This is significantly lower than the previous acoustic features or neural embedding of speech. 
> - \textbf{Interpretability}: Each channel corresponds to the actual physical articulator on the vocal tract, which can be intuitively interpretable by visualization on the vocal tract.
> - \textbf{Controllability}: The articulatory features can be naturally controlled by the same principle as speech production.
> - \textbf{Universality}: The articulatory encoding is universal across speakers despite and disentangled from individual anatomical variance.

With these unique benefits, empirical evidence and demonstration show the promising potential of the proposed articulatory encodec as a valid, novel coding framework of speech.\footnote{Audio samples are available at https://articulatoryencodec.github.io.}

## 2.Related Works: 相关工作

### Electromagnetic Articulography

Electromagnetic articulography (EMA) measures time-varying displacements of vocal tract articulators synchronously while speaking. Typically, sensors are placed on the upper lip (UL), lower lip (LL), lower incisor (LI), tongue tip (TT), tongue blade (TB), and tongue dorsum (TD) (Fig. \ref{fig:pipeline}) \cite{rebernik2021review}. A combination of displacements of these articulators on the midsagittal plane configures a place of articulation, which shapes phonetic content by combining with source information, or manner of articulation. As the traces are continuously collected in real-time, the EMA data naturally reflect phoneme contextualization, or coarticulation, and individual tendencies in pronunciations (dialects and accents). Given these properties, EMA has been widely accepted for studying articulatory bases of speech, providing biophysical evidence for many linguistic or cognitive theories of speech production \cite{ browman1992articulatory, rebernik2021review, chartier2018encoding}. However, EMA has been significantly limited to scale due to the complicated nature and high cost of the collection procedure. %many phonetics theories have been suggested based on suggested that the phonetic properties of speech can be configured by the midsagittal displacement of these 6 articulators \cite{}.
%However, articulatory data including EMA is significantly limited to scale due to the complicated recording procedure \cite{rebernik2021review}. 
%Note that velum is mostly excluded in EMA data since putting a sensor on the velum causes significant discomfort to participants, though velum is an important factor for nasal sounds \cite{rebernik2021review}. However, for normal speakers, velum functions as a gate for letting air flow while the oral cavity is blocked, which is acoustically reflected as a voice onset time (VOT). Therefore, we hypothesize that this missing information can be compensated by source vocalization, which is activated while the oral cavity is closed.


### Acoustics-to-Articulation Inversion

To replace the complicated data collection procedure, acoustic-to-articulatory inversion (AAI) models have been actively developed to predict EMA directly from speech audio \cite{ghosh2010generalized,  ghosh2011subject, liu2015deep, chartier2018encoding, anumanchipalli2019speech, wu2023speakerind, attia2023improving, siriwardena2023secret, gao2024copysynthesis}. However, the individual variance in vocal tract anatomy across speakers induces inconsistent placements of sensors, which has posed a significant barrier to developing a model that can apply to unseen speakers \cite{rahim1993annartsynth, wu2023speakerind, attia2023improving, siriwardena2023secret}. Despite such variability, a canonical basis of articulation is suggested to exist, which is agnostic to individual vocal tract anatomy \cite{cho2023self}. In fact, Cho et al. \cite{cho2023self} demonstrated that a linear affine transformation can geometrically align one speaker's articulatory system to another's. This suggests that individual articulatory spaces are lying on the same linear space so that an articulatory space of any speaker can be a hypothetical universal template space of articulation. We empirically prove this statement by leveraging a single-speaker AAI model as a universal articulatory encoder for our codec framework. %Therefore, one good AAI model fit to a single speaker's EMA sensor arrangement can be used as a universal AAI model. %Furthermore, an AAI model is implemented by a frozen speech SSL model with a linear head inserted in the later layer of Transformer encoder where speaker information is marginalized out.  

### Articulatory Synthesis

Articulatory synthesis aims to generate speech audio from articulatory features. A century of efforts have been made to build articulatory synthesizers for basic research of speech \cite{dudley1939synthetic, dudley1939remaking, dunn1950calculation, stevens1953electrical, rosen1958dynamic, mermelstein1973articulatory, rubin1981articulatory, maeda1982digital, scully1990articulatory}. Several methods have been proposed for improving intelligibility and quality%\cite{birkholz2013prev_artsynth, aryal2016data, anumanchipalli2019speech, kim2023style, wu2022artsynth}
, demonstrating broader use cases including text-to-speech (TTS) \cite{krug2021vocaltractlab}, prosody manipulation \cite{aryal2014accent, birkholz2017manipulation}, and speech brain-computer interfaces (BCIs) \cite{anumanchipalli2019speech}. Some of these works utilized deep learning models to map articulatory features to acoustic features, which are then converted to audio using pretrained acoustic synthesizers \cite{aryal2016data, anumanchipalli2019speech, kim2023style}. A recent study shows that a GAN-based generative model can directly synthesize speech waveform from articulatory features with high intelligibility \cite{wu2022artsynth}. However, to our knowledge, none of the existing approaches has achieved industrial-level performance, which requires high intelligibility, quality, and generalizability across unseen speakers. 


### Neural Codec of Speech

Many deep learning methods have been proposed to learn data-driven representations of speech. Various autoencoder methods have been suggested to jointly train encoders that compress audio into low-bitrate discrete units \cite{van2017neural, zeghidour2021soundstream, defossez2022encodec} or decompose speech into different factors \cite{choi2022nansy++, zhang2023speechtokenizer, ju2024naturalspeech}, and decoders that reconstruct speech from encoded features with minimal loss of information. Also, pretrained speech SSL models have been utilized to extract rich linguistic content of speech, and synthesizers are trained to restore speech audio from those features \cite{choi2021nansy, polyak2021resynthesis, choi2022nansy++,  lee2022hierspeech, lee2023hierspeech++, guo2023quickvc}. These SSL-based methods often utilize separate source modeling (e.g., pitch) and speaker encoding, since SSL model encoders tend to marginalize out acoustic and speaker information \cite{chen2022spkinfo, pasad2023comparative}. We categorize all these kinds of closed-loop frameworks utilizing neural networks for both encoding and decoding as \emph{neural codecs}. Though the existing methods achieve high fidelity in representing speech audio, they significantly lack interpretability and controllability. 

## 3.Methodology: 方法

To bridge this interpretability and controllability gap, we propose neural articulatory inversion and synthesis as a new type of neural codec that can provide an interpretable and controllable coding of speech.

### Articulatory Analysis

The articulatory encoding is extracted by articulatory analysis of 3 different aspects of speech: vocal-tract articulation, source features, and speaker identity (Fig. \ref{fig:pipeline}). 

#### Vocal-tract articulation

Based on the finding by \cite{cho2023self} (explained in \S \ref{relatedwork:AAI}), we propose to use a single speaker's EMA as a template articulatory space to represent speaker-generic articulatory kinematics. We selected one of the largest single-speaker EMA datasets, MNGU0 \cite{mngu}, that includes 75 minutes of EMA collected while reading newspapers aloud. This dataset is widely accepted and verified in many studies, given a fine signal quality carefully controlled by the authors. We claim all speakers' articulations can be represented on this single-speaker EMA space without losing information that contributes to the intelligibility of speech. That is, EMA represents phonetic content in a way that can be detached from the variance of vocal tract anatomical structure across individuals. Our results show empirical evidence of this claim. %we empirically demonstrate this single-speaker AAI can be used to build speaker-agnostic articulatory encodec. 

We use the SSL-linear AAI approach proposed by \cite{cho2023evidence, cho2023self}. The SSL-linear model is built by training a linear mapping from SSL features to EMA, while keeping the SSL encoder weights frozen. This simple mapping can effectively find a linear subspace in the SSL feature space which is highly correlated with EMA, proven by previous probing studies \cite{cho2023evidence,cho2023self}. We use the WavLM Large model \cite{chen2022wavlm}, which shows the highest correlation amongst speech SSL models as reported in \cite{cho2023evidence}. Note that the linear head is the only fitted part here, thus, maintaining the generalization capacity of the WavLM encoder that is attained by pretraining on large-scale speech data and adversarial data augmentation \cite{chen2022wavlm}. Furthermore, the speaker information tends to diminish after a few early layers \cite{chen2022spkinfo}, which indicates the mapping can be speaker-agnostic, further contributing to multi-speaker generalizability. 

The original 200 Hz EMA data is downsampled to 50 Hz to match the sampling rate of the SSL features, and z-scored within utterances. A low-pass filter is applied to the features to remove high-frequency noise, where the frequency threshold is set as 10 Hz. We used the 9th layer of the WavLM Transformer encoder from which the features are extracted for inversion (see Appendix A.1 for the selection procedure). The resulting AAI model outputs 12 channels of EMA (X and Y axis of each of 6 articulators) with 50 Hz frequency. 

#### Source Features

Though EMA has a full descriptive capacity of the place of articulation, it lacks source information generated by the glottal excitation, which is crucial to implementing the manner of articulation and expressing the prosody of speech. Therefore, we include pitch (or fundamental frequency (f0)) and loudness features to represent the source features \cite{choi2021nansy, choi2022nansy++, yang2024streamvc}. The loudness feature also informs non-larynx constriction, which is important for voiced fricatives such as ``z" and ``v". We use CREPE \cite{kim2018crepe} to infer pitch from speech, and loudness is measured by the average of absolute magnitudes of waves for every 20 ms. %\footnote{We used the model from https://github.com/maxrmorrison/torchcrepe.} and the loudness is measured by average amplitude of every 20 ms window. %The periodicity outputs of the pitch tracker are not used for the source feature. 
Together with the EMA from AAI, we referred to these features as ``articulatory features" that have 14 channels (12 EMA + 2 source) and a 50 Hz sampling rate.

#### Speaker Identity

Though we rule out the individual variance in vocal tract anatomy by using a template space, such missing anatomical configuration is an important determinant of the voice texture, or timbre, of an individual speaker which is a crucial factor in defining the speaker's identity \cite{netzorg2024percetualqaulity}. Note that our definition of the speaker identity does not include information about dialect or accent which is actually aimed to be disentangled from the speaker identity. %\footnote{We prefer to use the term timbre over speaker embedding since the latter term subsumes dialect and pitch range which are separately encoded in our framework.} %The texture of the voice is a crucial factor in defining the speaker's identity. The timbre is shaped by the anatomy of vocal tract and physiological characteristics of the vocal fold. This feature varies As we rule out vocal tract anatomy from articulatory features,  
Here, we compensate for this missing information with a separate speaker identity encoder which is jointly trained with a vocoder to extract the speaker-specific speaker information.
%Therefore, timbre encoding is indispensable to fully map out multi-speaker speech. The most common approach to encode timbre is to utilize CNN based architecture to extract timbre information, which is either pretrained \cite{polyak2021resynthesis} or end-to-end trained \cite{choi2021nansy, choi2022nansy++, yang2024streamvc}. 

To this end, we propose a simple yet effective speaker encoder, which minimizes the trainable portion of the model. Based on the observation by \cite{chen2022spkinfo, fan2020exploring} that the speaker information is largely concentrated in the CNN outputs of speech SSL models, the encoder consists of the frozen CNN extractor from WavLM Large followed by a weighted pooling layer and a learnable feedforward network (FFN). The pooling layer weighted-averages the acoustic features from WavLM CNN across frames, where the weight is given by the periodicity inferred from CREPE. This allows more attention to the periodic signals which may encode voice texture better, and rules out non-speech frames. Then, the FFN projects the average feature to a speaker embedding with 64 channels. This speaker identity encoding is indispensable to fully map out multi-speaker speech.

### Articulatory Synthesis

We adopt HiFi-GAN as the vocoder for articulatory synthesis \cite{kong2020hifi, wu2022artsynth, polyak2021resynthesis}. The vocoder is trained to synthesize speech audio with a 16K Hz sampling rate from articulatory features defined in \S\ref{methods:inversion} and \S\ref{methods:source}. To condition the generation on the speaker embedding, we apply FiLM \cite{perez2018film} to each convolution module with different receptive fields in the HiFi-GAN architecture, which modulates the output channels of each module. We adopt the same loss functions as \cite{kong2020hifi}: Mel-spectrogram loss for reconstruction and multi-period and multi-scale discriminator loss for GAN training. 

### Dataset

For training the vocoder, we use LibriTTS-R \cite{koizumi2023librittsr}, an enhanced version of LibriTTS \cite{zen2019libritts}. The dataset is comprised of 585 hours of reading audiobooks ((555, 15, 15) hours for (train, dev, test) sets). The original 24K Hz audio is downsampled to 16K Hz. We use VCTK \cite{veaux2017vctk} to further evaluate the generalizability of the model to a broader range of speakers and accents. The entire VCTK dataset is not included in the training set and is only used for evaluation. 

Note that only the FFN in the speaker identity encoding and the vocoder are updated in the large-scale training (orange modules in Fig. \ref{fig:pipeline}), and the rest of the pipeline remains fixed while training. More details of implementation and training are in Appendix B.1-7.

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

We propose a novel encoding-decoding framework of speech, \textit{articulatory encodec}, which mimics the biophysical interface of speech production. By projecting speech to articulatory features, the speech can be expressed in a low-dimensional space, where each channel corresponds to an articulator on the vocal tract. Such physical embodiment of speech coding enables a natural, intuitive interpretation, which is further facilitated by a visual aid that displays the articulation in a 2D animation. With large-scale training, our proposed articulatory encodec achieves a high-performance articulatory resynthesis that can generalize to unseen speakers and novel dialects, with a minimal loss of information compared to ground truth speech. To our knowledge, this is the first demonstration of universal articulatory synthesis that can scale up to an indefinite number of speakers. This universality is aided by a novel speaker identity encoding, which embeds highly discriminable speaker-specific speaker. Furthermore, the encoded speaker embedding is effectively disentangled from articulatory features and can be used for accent-preserving voice conversion. 

In future work, we will scale the articulatory encodec to incorporate expressive speech and singing. Also, we will improve the robustness of the system under noisy environments. These efforts will maximize the promising utilities of the articulatory coding of speech.
