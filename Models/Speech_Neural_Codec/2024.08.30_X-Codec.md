# ***X-Codec***

<details>
<summary>基本信息</summary>

- 标题: Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model
- 作者:
  - 01 [Zhen Ye](../../Authors/Zhen_Ye.md)
  - 02 [Peiwen Sun](../../Authors/Peiwen_Sun.md)
  - 03 [Jiahe Lei](../../Authors/Jiahe_Lei.md)
  - 04 [Hongzhan Lin](../../Authors/Hongzhan_Lin.md)
  - 05 [Xu Tan](../../Authors/Xu_Tan_(谭旭).md)
  - 06 [Zheqi Dai](../../Authors/Zheqi_Dai.md)
  - 07 [Qiuqiang Kong](../../Authors/Qiuqiang_Kong.md)
  - 08 [Jianyi Chen](../../Authors/Jianyi_Chen.md)
  - 09 [Jiahao Pan](../../Authors/Jiahao_Pan.md)
  - 10 [Qifeng Liu](../../Authors/Qifeng_Liu_(柳崎峰).md)
  - 11 [Yike Guo](../../Authors/Yike_Guo_(郭毅可).md)
  - 12 [Wei Xue](../../Authors/Wei_Xue_(雪巍).md)
- 机构:
  - 机构 
- 时间:
  - 预印时间: 2024.08.30 ArXiv v1
  - 更新笔记: 2024.09.02
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2408.17175)
  - [DOI]()
  - [Github](https://github.com/zhenye234/xcodec)
  - [Demo](https://x-codec-audio.github.io/)
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: 10
- 引用: ?
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). 
> The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. 
> However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. 
> Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. 
> For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. 
> To overcome these issues, we propose a straightforward yet effective approach called ***X-Codec***. 
> ***X-Codec*** incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. 
> By enhancing the semantic ability of the codec, ***X-Codec*** significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. 
> Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. 
> Our code and demo are available.
> Demo: https://x-codec-audio.github.io
> Code: https://github.com/zhenye234/xcodec

## 1.Introduction: 引言

> In recent years, Large Language Models (LLMs) such as GPT \cite{brown2020language} have demonstrated remarkable capabilities in modeling complex, high-dimensional data across various domains, including text and image generation \cite{zhao2023survey,liu2024visual}. 
> Inspired by these successes, there has been significant interest ([MusicLM](../Speech_LLM/2023.01.26_MusicLM.md); [AudioLM](../Speech_LLM/2022.09.07_AudioLM.md); [VALL-E](../Speech_LLM/2023.01.05_VALL-E.md); [UniAudio](../Speech_LLM/2023.10.01_UniAudio.md)) in exploring the application of LLMs to audio generation.
>
> Audio codecs ([SoundStream](2021.07.07_SoundStream.md)) have emerged as a critical technique for audio LLMs, bridging the gap between continuous audio waveforms and token-based language models. 
> By discretizing high-rate audio signals into a finite set of tokens, these codecs enable the application of LLM architectures to audio data, leveraging the successes of textual LLMs. 
>
> However, prior research on audio codecs has primarily focused on achieving lower compression rates and higher reconstruction quality ([DAC](2023.06.11_Descript-Audio-Codec.md); [EnCodec](2022.10.24_EnCodec.md); [HiFi-Codec](2023.05.04_HiFi-Codec.md)). 
> Meanwhile, many efforts in audio generation have concentrated on enhancing model architecture, scaling, or leveraging larger datasets. 
> For instance, [AudioLM](../Speech_LLM/2022.09.07_AudioLM.md) adopts a two-stage pipeline that models the acoustic token in an autoregressive way conditioned on the semantic token. 
> [VALL-E](../Speech_LLM/2023.01.05_VALL-E.md), the first TTS framework to leverage large, diverse, and multi-speaker speech data, demonstrates strong in-context learning capabilities similar to GPT-3, treating TTS as a language modeling task on audio codecs. 
> [MusicGen](../Speech_LLM/2023.06.08_MusicGen.md) generates music using a single-stage transformer LM alongside efficient token interleaving patterns. 
> Similarly, [UniAudio](../Speech_LLM/2023.10.01_UniAudio.md) scaled up to 165K hours of audio and 1B parameters, utilizing LLM techniques to generate tokens for various types of audio, including speech, sounds, music, and singing, given different input conditions.
>
> While these works have shown success in developing audio language models, they all rely on the acoustic codecs such as [EnCodec](2022.10.24_EnCodec.md) or [SoundStream](2021.07.07_SoundStream.md) for audio tokenization and de-tokenization. 
> However, these acoustic codecs were originally designed for audio compression rather than for audio language models. 
> This misalignment means the design may not be optimal for audio language modeling.
>
> To design a better audio codec for Audio LLMs, we drew inspiration from the initial purpose of LLMs such as GPT, which were designed to process text. 
> These models focus on understanding and generating natural language, which is inherently rich in semantics. 
> Motivated by this, we assume that a better audio tokenizer should encapsulate rich semantic information to facilitate an easy understanding of audio content, thus reducing the language model's burden in interpreting tokens. 
> However, most audio codecs focus on acoustic reconstruction which ignores the semantic information. 
> As a result, LLM essentially tries to predict the local fluctuations of the audio signal, which is difficult, and methods like VALL-E, which condition acoustic token generation on text transcriptions, frequently result in content inaccuracies causing elevated word error rates (WER), stemming from the semantic misinterpretations of acoustic tokens, leading to word skipping and errors.
>
> To address this issue, approaches like [SpeechTokenizer](2023.08.31_SpeechTokenizer.md) have attempted to disentangle speech into separate tokens for content and timbre and perform distillation-based semantic and acoustic integration. 
> However, this method may not integrate smoothly with all audio LLMs, especially those requiring uniform token treatment across different layers, such as utilizing flattened codec tokens ([UniAudio](../Speech_LLM/2023.10.01_UniAudio.md); [MusicGen](../Speech_LLM/2023.06.08_MusicGen.md)). 
>
> In this paper, We propose a straightforward yet effective method termed `X-Codec`, which integrates both semantic and acoustic features into a unified tokenization framework. 
> The ***X-Codec*** architecture employs a distinctive `X-shaped` structure, characterized by two inputs and two outputs, unifying semantic and acoustic information within a single Residual Vector Quantizer (RVQ) structure. 
> This design enables simultaneous embedding learning of semantic richness and acoustic fidelity for every token, resulting in better performance for audio LLM. 
>
> We have conducted comprehensive evaluations of ***X-Codec*** across various applications, including text-to-speech, music continuation, and text-to-sound synthesis. 
> The results consistently demonstrate the effectiveness of the proposed method. 
> Furthermore, our comparative evaluation on VALL-E based TTS demonstrates that ***X-Codec*** outperforms existing disentanglement techniques, thereby highlighting its efficacy and versatility in advancing audio LLM technologies.

## 2.Related Works: 相关工作

### Audio Language Model

> The success of Large Language Models (LLMs) has sparked a significant trend in leveraging language foundation models for audio generation tasks ([AudioPaLM](../Speech_LLM/2023.06.22_AudioPaLM.md); zhang2024speechlm,wu2023decoder,wu2023speechgen,[UniAudio](../Speech_LLM/2023.10.01_UniAudio.md); [LauraGPT](../Speech_LLM/2023.10.07_LauraGPT.md)). 
> Audio, much like language, consists of variable-length sequences, making it well-suited for modeling with language foundation models. 
> One pioneering method, [AudioLM](../Speech_LLM/2022.09.07_AudioLM.md), employs a multi-stage strategy to harness the predictive capabilities of foundation models for generating tokens unconditionally. 
> This approach involves predicting semantic tokens from various conditions (e.g., phonemes, text descriptions, MIDI) in the initial stage, followed by transforming them into acoustic tokens through coarse-to-fine modeling, ultimately generating the waveform. 
> Representative systems such as [SPEAR-TTS](../Speech_LLM/2023.02.07_SPEAR-TTS.md) for speech synthesis and [MusicLM](../Speech_LLM/2023.01.26_MusicLM.md) for music generation have also been proposed. 
> However, the two-stage process can lead to complexity in training and suboptimal performance due to the separate development of semantic and acoustic tokens, leading to error accumulation.
>
> Conversely, recent advancements have shown that methods employing a single-stage language model outperform two-stage approaches. 
> For example, [VALL-E](../Speech_LLM/2023.01.05_VALL-E.md) utilizes an autoregressive (AR) model to predict the first token and a non-autoregressive (NAR) model to estimate the residual tokens, demonstrating superior performance compared to AudioLM. 
> Similarly, [MusicGen](../Speech_LLM/2023.06.08_MusicGen.md) employs a single-stage transformer language model and incorporates a delay pattern strategy for efficient token interleaving, achieving better results than MusicLM. 
> Other notable works include [CLAM-TTS](../Speech_LLM/2024.04.03_CLaM-TTS.md), [VoiceCraft](../Speech_LLM/2024.03.25_VoiceCraft.md), and [UniAudio](../Speech_LLM/2023.10.01_UniAudio.md).
>
> Despite recent advancements, directly modeling the intricate low-level acoustic fluctuations with an LLM poses challenges. 
> LLMs are primarily designed for processing natural language, which is inherently rich in semantics. 
> In order to overcome this limitation, we propose ***X-Codec***, a novel enhancement that aims to enrich semantic processing within acoustic codecs. 
> By doing so, we aim to improve the overall performance of audio LLMs.

### Audio Codec

> Recent advancements have seen a surge in deep learning methodologies employing vector quantization ([VQ-VAE](../../Modules/VQ/2017.11.02_VQ-VAE.md)) to reconstruct continuous signals into discrete representations for AR generation. 
> Notably, audio codecs based on the [VQ-GAN](../_Basis/2020.12.17_VQGAN.md) framework have gained prominence. 
> For example, [SoundStream](2021.07.07_SoundStream.md) introduces a versatile codec adaptable to various audio types, integrating Residual Vector Quantization (RVQ) and Generative Adversarial Network (GAN) to refine quantization and reconstruction. 
> Similarly, [EnCodec](2022.10.24_EnCodec.md) enhances compression through a multi-scale discriminator and a loss-balancing strategy alongside a language model. 
> [HiFi-Codec](2023.05.04_HiFi-Codec.md) employs Group-Residual Vector Quantization (GRVQ) to minimize the need for extensive codebooks while maintaining high reconstruction fidelity. 
> [DAC](2023.06.11_Descript-Audio-Codec.md) addresses codebook collapse, where some codes remain unused, by applying improved codebook learning to achieve higher compression rates.
>
> These codecs primarily focus on acoustic reconstruction and higher compression rates, often overlooking their potential as tokenizers for audio LLMs. 
> Some attempts have been made to develop more suitable tokenizers for audio LLMs. 
> For example, [SpeechTokenizer](2023.08.31_SpeechTokenizer.md) utilizes HuBERT to separate speech into distinct VQ components for content and timbre/acoustic details. 
> This separation improves the modeling of content in the AR stage of VALL-E, while the NAR stage enriches the acoustic details. 
> However, a distillation framework is exploited, this makes SpeechTokenizer may not be compatible with all LLM architectures, especially those that require uniform treatment of tokens, such as methods using flattened codec tokens ([UniAudio](../Speech_LLM/2023.10.01_UniAudio.md); [MusicGen](../Speech_LLM/2023.06.08_MusicGen.md)). 
> Another attempt is presented by [SemantiCodec](SemantiCodec.md), which employs a pre-trained [AudioMAE](../_tmp/AudioMAE.md) to generate distinct semantic and acoustic tokens from mel-spectrograms. 
> However, this method inherits the issues of SpeechTokenizer and introduces additional complexity in token modeling. 
> Moreover, since the audioMAE is performed on 2D time-frequency mel-spectrograms, LLMs must effectively handle dual scales (time and frequency), which may require significant modifications to existing LLM structures.
>
> In contrast, our proposed ***X-Codec*** provides a uniform and comprehensive enhancement of semantic information for all tokens, resulting in significant performance improvements for existing audio LLMs without requiring any structural modifications. 

## 3.Methodology: 方法

> In this section, we propose ***X-Codec***, a straightforward yet effective method to overcome the semantic shortcomings of the current acoustic codecs. 

### Acoustic Audio codec

> As illustrated in Figure \ref{fig_xcodec}, our model builds upon the framework established by existing acoustic codecs such as [EnCodec](2022.10.24_EnCodec.md) and [DAC](2023.06.11_Descript-Audio-Codec.md). 
> An acoustic audio codec is composed of three main components: an acoustic encoder, a quantizer, and an acoustic decoder. 
> The input of the codec is the raw waveform $\textbf{X} \in \mathbb{R}^{n}$, where $n$ represents the number of waveform samples. 
> This waveform is fed into the acoustic encoder, which consists of several convolutional layers and employs temporal downscaling to extract frame-level latent acoustic features $\textbf{A} \in \mathbb{R}^{H_a \times T}$, where $H_a$ denotes the hidden size of the acoustic features and $T$ is the number of frames. 
> These continuous features are then transformed into a series of discrete tokens $\textbf{Q} \in \mathbb{R}^{M \times T}$ using a Residual Vector Quantizer (RVQ) with $M$ quantizer layers. 
> During training, a specific codebook for the quantizer is learned, enabling the conversion of discrete tokens back to continuous features $\textbf{A}_q \in \mathbb{R}^{H_a \times T}$. 
> The acoustic decoder then reconstructs the waveform $\hat{\textbf{X}}$ from $\textbf{A}_q$ using several convolutional layers and temporal upsampling. 
> The training process is supervised using various losses, including mel loss, STFT loss, and GAN loss, to ensure high-quality acoustic reconstruction.

### Analysing Semantic Shortcoming

> In this section, we investigate the impact of acoustic codecs on the performance of audio LLMs, focusing specifically on VALL-E, a pioneering model that leverages language model principles for text-to-speech. 
> Our analysis reveals that training VALL-E using Encodec results in high word error rates (WER) and frequent inaccuracies in content generation. 
> For example, when the input text ``he passed through Henley Saint Albans and came so near to London as Harrow on the Hill'' is synthesized, it is erroneously produced as ``he passed through henley saint albeans and camsel knew to lunglan as herold the lor''. 
> This misinterpretation, which is beyond simply improving the audio quality, suggests a fundamental limitation in Encodec's ability to differentiate phonemes, possibly due to its inadequate semantic processing capabilities.
>
> To substantiate the above hypothesis, we conducted Phonetic Discriminability ABX Tests to evaluate the phonetic discriminability of Encodec's representations. 
> The details are provided in the experiment section. 
> Our findings reveal that Encodec's representations exhibit poor phonetic discriminability, which confirms the presence of semantic inadequacies in the codec. 
> Based on these results, we assert that these semantic shortcomings are a significant contributing factor to the observed inaccuracies of language model based audio generation. 
>
> To effectively address these semantic limitations, we introduce a novel approach that integrates more comprehensive semantic features into the codec's architecture. 
> This enhancement is designed to enrich the codec's understanding of audio content, thereby alleviating the interpreting load on the language model. 
> Detailed elaboration of this method is provided in the subsequent section.

### Designing Auxiliary Semantic Module

> Our approach employs a straightforward method that enhances audio codecs by directly concatenating semantic and acoustic features. 
> Initially, we extract the semantic feature vector $\textbf{S}^* \in \mathbb{R}^{H_s \times T}$ from the audio waveform $\textbf{x}$. 
> This extraction utilizes a self-supervised, pre-trained model such as [HuBERT](../Speech_Representaion/2021.06.14_HuBERT.md) or [wav2vec 2.0](../Speech_Representaion/2020.06.20_Wav2Vec2.0.md). 
> The extracted features are then processed through multiple convolutional layers within a semantic encoder to yield the refined semantic feature vector $\textbf{S}$. 
> Concurrently, the acoustic branch produces the feature $\textbf{A}$. 
> These outputs, $\textbf{S}$ and $\textbf{A}$, are subsequently concatenated using a linear projection $\phi$, formulated as:

$$
\textbf{U} =  concat(\phi_s(\textbf{A}), \phi_a(\textbf{S})) ,
$$

> where the concatenated feature $\textbf{U} \in \mathbb{R}^{H_u \times T}$ is designed to maximize information preservation from both semantic and acoustic sources. 
> This combined feature is then subject to RVQ using an $M$-layer quantizer, resulting in tokens that encapsulate a rich mixture of semantic and acoustic information.
>
> The quantized feature $\textbf{U}_q$ is designed to meet the decoder's objectives through two projectors, $\beta_s$ and $\beta_a$, which enable the decoders to reconstruct the original semantic feature $\hat{\textbf{S}}^*$ and the audio waveform $\hat{\textbf{x}}$. 
> We adhere to established acoustic reconstruction methods from previous works while introducing a Mean Squared Error (MSE) loss specifically for the reconstruction of semantic features. 
> Furthermore, a constant weight $\gamma$ is applied to the semantic loss to ensure that its scale is aligned with other losses, thus promoting a balanced training objective.

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

> In this paper, we introduced ***X-Codec***, an advanced audio codec that integrates semantic information through self-supervised learning models to enhance performance in large language models, specifically in text-to-speech synthesis, music continuation, and general audio classification tasks. 
> Our evaluations demonstrate that ***X-Codec*** significantly improves semantic understanding and audio generation quality across a variety of domains.  
