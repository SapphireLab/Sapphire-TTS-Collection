# Google USM

<details>
<summary>基本信息</summary>

- 标题: "Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages"
- 作者:
  - 01 Yu Zhang
  - 02 Wei Han
  - 03 James Qin
  - 04 Yongqiang Wang
  - 05 Ankur Bapna
  - 06 Zhehuai Chen
  - 07 Nanxin Chen
  - 08 Bo Li
  - 09 Vera Axelrod
  - 10 Gary Wang
  - 11 Zhong Meng
  - 12 Ke Hu
  - 13 Andrew Rosenberg
  - 14 Rohit Prabhavalkar
  - 15 Daniel S. Park
  - 16 Parisa Haghani
  - 17 Jason Riesa
  - 18 Ginger Perng
  - 19 Hagen Soltau
  - 20 Trevor Strohman
  - 21 Bhuvana Ramabhadran
  - 22 Tara Sainath
  - 23 Pedro Moreno
  - 24 Chung-Cheng Chiu
  - 25 Johan Schalkwyk
  - 26 Françoise Beaufays
  - 27 Yonghui Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2303.01037)
  - [Publication]
  - [Github]
  - [Demo]
- 文件:
  - [ArXiv](_PDF/2303.01037v3__Google_USM__Scaling_Automatic_Speech_Recognition_Beyond_100_Languages.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

We introduce the ***Universal Speech Model (USM)***, a single large model that performs automatic speech recognition (ASR) across 100+ languages.
This is achieved by pre-training the encoder of the model on a large unlabeled multilingual dataset of 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller labeled dataset.
We use multilingual pre-training with random-projection quantization and speech-text modality matching to achieve state-of-the-art performance on downstream multilingual ASR and speech-to-text translation tasks.
We also demonstrate that despite using a labeled training set 1/7-th the size of that used for the **Whisper** model, our model exhibits comparable or better performance on both in-domain and out-of-domain speech recognition tasks across many languages.

</td><td>

我们介绍了 ***Universal Speech Model (USM)***, 这是一个单一的大型模型, 能够执行超过 100 种语言的自动语音识别 (ASR).
这一目标通过在一个包含 300 多种语言的 1200 万小时大规模无标注多语言数据集上预训练模型的编码器, 并在较小的标注数据集上进行微调来实现.

我们使用多语言预训练、随机投影量化和语音-文本模态匹配技术, 在下游多语言 ASR 和语音到文本翻译任务上取得了最先进的性能.

我们还展示了, 尽管使用的标注训练集仅为 **Whisper** 模型训练集的 1/7 大小, 我们的模型在多个语言的领域内外的语音识别任务上, 表现出与之相当甚至更好的性能.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td></tr></table>
