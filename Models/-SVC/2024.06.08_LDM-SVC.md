# LDM-SVC: Latent Diffusion Model Based Zero-Shot Any-to-Any Singing Voice Conversion With Singer Guidance

<details>
<summary>基本信息</summary>

- 标题: "LDM-SVC: Latent Diffusion Model Based Zero-Shot Any-to-Any Singing Voice Conversion With Singer Guidance."
- 作者:
  - 01 Shihao Chen
  - 02 Yu Gu
  - 03 Jie Zhang
  - 04 Na Li
  - 05 Rilin Chen
  - 06 Liping Chen
  - 07 Lirong Dai
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.05325v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2406.05325v1](_PDF/2024.06.08_2406.05325v1_LDM-SVC__Latent_Diffusion_Model_Based_Zero-Shot_Any-to-Any_Singing_Voice_Conversion_With_Singer_Guidance.pdf)
  - [Publication] #TODO

</details>

## Abstract

% 1000 characters.
ASCII characters only.
No citations.
Any-to-any singing voice conversion (SVC) is an interesting audio editing technique, aiming to convert the singing voice of one singer into that of another, given only a few seconds of singing data.
However, during the conversion process, the issue of timbre leakage is inevitable: the converted singing voice still sounds like the original singer's voice.
To tackle this, we propose a latent diffusion model for SVC (LDM-SVC)  in this work, which attempts to perform SVC in the latent space using an LDM.
We pretrain a variational autoencoder structure using the noted open-source So-VITS-SVC project based on the VITS framework, which is then used for the LDM training.
Besides, we propose a singer guidance training method based on  classifier-free guidance to further suppress the timbre of the original singer.
Experimental results show the superiority of  the proposed method over previous works in both subjective and objective evaluations of timbre similarity. 

## 1·Introduction

Singing Voice Conversion (SVC) is a popular audio editing technique that aims to change the singing voice of one singer to mimic another.

Different from singing voice synthesis which requires well-designed musical note inputs [^Gu2021ByteSing], [^Cui2024Sifisinger], 
this technique allows users to customize their favorite singers performing any songs just given  corresponding recorded songs sung by other singers.
Unlike the many-to-many or many-to-one scenarios,
%which requires hours of singing data from a single singer, 
the any-to-any SVC is much more challenging, which demands the model to perform conversion for any target singer who were not included in the training set by solely a short snippet of reference singing voice that even lasts for few seconds.
%, can be , including those who were not encountered during the training phase in situations with limited resources. %This makes the any-to-any SVC a more challenging but also a more flexible and practical approach

The main challenge of SVC is to separate and reassemble the singer's unique vocal timbre from the content and melody of songs. 
Similarly to voice conversion,  mainstreaming SVC systems also follows a recognition-synthesis scheme as a typical two-stage process.

In the first stage, singer-independent features such as phonetic posteriorgrams (PPG) [^Sun2016Phonetic], [^Polyak2020Unsupervised], [^Liu2021Fastsvc], [^Liu2021Diffsvc], [^Li2021PPG-Based] from an ASR model and self-supervised learning (SSL) representations [^Jayashankar2023Self-Supervised], [^Zhou2023VITS}-Based] trained on large amounts of unlabeled speech data are used to encode audio. 
These representations serve as intermediary for SVC, which can effectively extract content and  semantic information from waveforms.

In the second stage, acoustic models are involved to generate the target audio or acoustic features 
from these immediate representations.
Various generative models have been employed for SVC decoding, including autoregressive models [^Nachmani2019Unsupervised], [^Deng2020Pitchnet], [^Zhang2020Durian-Sc], [^Takahashi2021Hierarchical], generative adversarial networks (GANs) [^Polyak2020Unsupervised], [^Liu2021Fastsvc], [^Zhou2022HiFi-SVC], variational autoencoder (VAE) [^Luo2020Singing] and diffusion models [^Liu2021Diffsvc].

Despite of naturalness, sound quality and intonation accuracy of converted singing voice have largely improved by above different SVC models, the timbre leakage problems remain serious, especially for the challenges of SVC cross different genders.

This is primarily due to PPG and SSL features containing not only content information but also some timbre information of the original singer.To alleviate the timbre leakage,  many works such as So-VITS-SVC\footnote{\url{https://github.com/PlayVoice/so-vits-svc-5.0/tree/bigvgan-mix-v2}} involved information perturbation by directly adding white noises on 
hidden features or acoustic features. 
However such white noises were totally independent with singer information and the information perturbation modules were not trainable and optimized in the network training stage and directly adding noise on acoustic features may also lead in pronunciation and quality distortion.

![](fig1.pdf)

<a id="ldm-svc">Left: Pre-training procedure of So-VITS-SVC; Right: Training procedure of LDM-SVC.</a>

Recently, Latent Diffusion Model (LDM) based systems have shown a great success in image generation from text such as Stable Diffusion [^Rombach2022High-Resolution] and high-quality sound generation from text such as Tango[^Ghosal2023Text-to-Audio] and  AudioLDM[^Liu2023Audioldm], [^Liu2023AudioLDM], which performed forward and denoising diffusion processes on the   hidden spaces rather 
than acoustic features like other models [^Liu2022Diffsinger], [^Popov2021Diffusion-Based].

Motivated by these models,  we present LDM-SVC, a novel any-to-any SVC method which reconstructs the waveform directly from the latent representation in an end-to-end latent diffusion manner.

Unlike DiffSVC [^Liu2021Diffsvc] employed on mel-spectrograms and requiring an additional vocoder, we conduct the diffusion model on the hidden space from a pre-trained VAE model using  So-VITS-SVC and directly
utilize the predicted latent representation to generate the waveforms by the VAE decoder.

To address the timbre leakage issue, we regard the LDM forward process as a information perturbation process in those both progressively adding noise to decouple the singer timbre from 
content and melody.

Different from the aforementioned methods which simply add white noise on waverforms, such information perturbation module is trainable and conditioned on singer information.

To better decouple singer information, a singer guidance training mechanism is explored, which is inspired by the  classifier-free method [^Ho2022Classifier-Free] in image generation when training the conditional and unconditional diffusion model at the same time.

Comparing with
many state-of-the-art SVC models, both subjective and objective experimental results indicate that our proposed method can achieve greater timbre similarity in any-to-any SVC tasks for both seen singer conversion and unseen singer conversion scenarios and better singing naturalness.

The rest of this paper is organized as follows.

Section 2 outlines the proposed LDM-SVC method.

Experiments are presented in Section 3.

Finally, Section 4 concludes this work.

## 2·Proposed Method

### VAE Pretraining

\label{vae pretrain}

We pretrain the VAE model using So-VITS-SVC, which is a VAE based any-to-any SVC model following the VITS framework [^Kim2021Conditional] and consists of three key components: posterior encoder, prior encoder and decoder as depicted in Figure 1. 

The posterior encoder $\mathcal{E}(\cdot)$ composed of non-causal WaveNet [^Oord2016Wavenet] residual blocks models the posterior distribution $p(z|y,e)$ of the hidden representation $z=\mathcal{E}(y,e)$ from the linear spectrograms  $y$ generated from the original singing waveforms  where singer embedding $e$ is extracted by an additional speaker verification model.

The prior encoder is implemented using a multi-layer Transformer [^Vaswani2017Attention].

Given the PPG and fundamental frequency (F0) denoted as $x$ and $f_0$ respectively, the prior encoder estimates the prior distribution $p(z|x,f_0,e)$ with the target singer's timbre and the flow. 
To bridge the distribution between the prior encoder and posterior encoder, normalizing flow with speaker-normalized affine couplin (SNAC) [^Choi2022Snac] layers is exploited to perform an invertible transformation of a simple distribution into a more complex one. 
The BigVGAN-based decoder [^Lee2022Bigvgan] $\mathcal{D}(\cdot)$ generates the singing waveform from the latent representation $z$ using a neural source filter (NSF) scheme [^Wang2019Neural] with F0 to enhance voice reconstruction quality. %Furthermore, a multi-period discriminator (MPD), a multi-resolution discriminator (MRD) and a multi-scale discriminator (MSD) are employed to constrain the waveform quality in an adversarial manner. 

After training the So-VITS-SVC system, we retain only the posterior encoder and decoder.

The posterior encoder compresses the linear spectrogram to generate the latent representation, used as the prediction target for the diffusion model in LDM-SVC training.

During inference, the latent representation is predicted from Gaussian White Noise by the denoising process, and the waveform is generated via the decoder.

### Latent Diffusion

LDM is  adopted as the probabilistic models that fit the hidden distribution by denoising on data latent space from pretrained VAE model.

We use the Denoising Diffusion Probabilistic Models (DDPM) method [^Ho2020Denoising] to train the diffusion model, which consists of forward and denoising processes.

Initially, we pretrain an SVC model using the So-VITS-SVC framework and combine its posterior encoder and decoder to form a VAE (see the right part of Figure 1).

During the LDM training, the singer's timbre $e$ and the linear spectrogram of the singing voice $y$ are used as inputs to the posterior encoder $\mathcal{E}(\cdot)$, yielding the latent variable $z_0=z=\mathcal{E}(y,e)$. 
In the forward process, the original data distribution is transformed into a standard Gaussian distribution by gradually adding noise according to a fixed schedule $\beta_1,\dots,\beta_T$.

Here, $T$ represents the total timesteps.

The transition from $z_{0}$ to $z_t$ follows a Markov chain, where the conditional distribution $q(z_t|z_{t-1})$ is defined as a Gaussian distribution: $q(z_t|z_{t-1}) = \mathcal{N}(z_t;\sqrt{1-\beta_t}z_{t-1}, {\beta_t}\mathbf{I})$.

The denoising process, parameterized by $\theta$, acts as a denoising function, eliminating the added noise and restoring the original data structure.

This denoising distribution, $p_{\theta}(z_{t-1}|z_t)$, is modeled as a conditional Gaussian distribution.

By using this parameterized denoising process, we iteratively sample the target data $z_0$ from a Gaussian noise for $t = T, T-1, \ldots, 1$.

In each iteration, $z_{t-1}$ is sampled according to $p_{\theta}(z_{t-1}|z_t)$.

The LDM model receives $z_t$ and $t$ as inputs, complemented by conditional inputs such as the singer's timbre $e$, fundamental frequency $f_0$, and PPG $x$.

To put it simply, in the denoising procedure the calculation of $z_{t-1}$ is given by

$$z_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( z_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_{\theta}(z_t, t, x, f_0, e) \right) + \sigma_t \epsilon,
$$

where $\epsilon \sim \mathcal{N}(0, I)$ represents Gaussian white noise, $\alpha_t = 1 - \beta_t$, and $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$.

For a more detailed derivation of (1), please refer to [^Ho2020Denoising].

The configuration of the diffusion model we use aligns with that in DiffSVC.  %based on the denoising diffusion probabilistic model (DDPM) [^Ho2020Denoising].

The difference is that we use the latent representation as the target. %\textcolor{green}{The probability space it resides in is more suitable for the prediction of the DDPM  compared to the original data distribution.}  Specifically, we pretrain an SVC model using the So-VITS-SVC framework, and then combine its posterior encoder and decoder to form a VAE, see the right part of Figure 1. %During the training process of the LDM model, we use the singer's timbre $e$ and the linear spectrogram of the singing voice $y$ as inputs to the posterior encoder $\mathcal{E}(\cdot)$ to obtain the latent variable $z_0=\mathcal{E}(y,e)$. 
The training loss of the latent diffusion model $\epsilon_\theta$ is defined as the
mean squared error (MSE) in the noise space:

$$
\mathcal{L}_{LDM}=||\epsilon-\epsilon_\theta(z_t, t, x, f_0, e)||_2^2.
$$

During inference, the singer's timbre and F0 are replaced with the target singer's attributes, defined as $e_{tar}$ and $f_0$, while the source singer's PPG $x_{src}$ is used as a condition.

We sample a random Gaussian White Noise in the denoising process to obtain latent variable $z_0$.

Consequently, $z_0$, $e_{tar}$, and $f_0$ are fed into the pretrained decoder $\mathcal{D}(\cdot)$ to generate the audio waveform.

The use of LDM can alleviate the inconsistency caused by training-testing mismatches in So-VITS-SVC.

\begin{algorithm}[t]
\caption{Training procedure of LDM-SVC.}

\begin{algorithmic}[1]
\label{alg:training}
\REQUIRE Conversion model $\epsilon_\theta(\cdot)$; training set $D_{train} = \{(x,{f_0},e,y)\}_{m=1}^M$; pretrained So-VITS-SVC posterior encoder $\mathcal{E(\cdot)}$; distortion probability $p_{uncond}$; $N_{iter}$ iterations.\FOR{$i=1,2,...,N_{iter}$}
\STATE Sample $(x, f_0, e, y)$ from $D_{train}$;
\STATE $z_0=\mathcal{E}(y,e)$;
\STATE $e,f_0 \leftarrow \varnothing$ with probability $p_{uncond}$;
\STATE $\epsilon\sim\mathcal{N}(0,I)$;
\STATE Sample $t\sim$ Uniform$(\{1, \cdots, T\})$;
\STATE Take gradient descent step on \\ \quad$\nabla_\theta||\epsilon-\epsilon_\theta(\sqrt{\bar\alpha_t}z_0+\sqrt{1-\bar\alpha_t}\epsilon, t, x, f_0, e)||_2^2$;
\ENDFOR
\RETURN $\epsilon_\theta(\cdot)$;
\end{algorithmic}

\end{algorithm}

\begin{algorithm}[t]
\caption{Inference procedure of LDM-SVC.}

\begin{algorithmic}[1]
\label{alg:infer}
\REQUIRE Trained conversion model $\epsilon_\theta(\cdot)$; source singer 
PPG $x_{src}$; target singer embedding $e_{tar}$; modified $f_0$; pretrained So-VITS-SVC decoder $\mathcal{D}(\cdot)$; guidance weight $w$.\STATE Sample $z_T\sim\mathcal{N}(0,I)$;
\FOR{$t=T,T-1,...,1$}
\STATE $\epsilon_t=(1+w)\epsilon_\theta(z_t, t, x, f_0, e_{tar})-w\epsilon_\theta(z_t, t, x, \varnothing, \varnothing)$;\STATE $\epsilon\sim\mathcal{N}(0,I)$ if $t>1$ else $\epsilon=0$;
\STATE $z_{t-1} = \frac{1}{\sqrt{\alpha_t}}(z_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_t)+\sigma_t \epsilon$;
\ENDFOR
\RETURN $\mathcal{D}(z_0,f_0,e_{tar})$;
\end{algorithmic}

\end{algorithm}

### Singer Guidance

As in Figure 2, we employ the speaker condition layer normalization (SCLN) [^Wu2021Cross-Speaker] to normalize the PPG feature.

Additionally, we use classifier-free guidance [^Ho2022Classifier-Free] to train the model.

Specifically, we expect the model to predict $p(z|x)$ via the score estimator $\epsilon_\theta(z_t, t, x)$ obtained from unconditional diffusion, while simultaneously predicting $p(z|x, e, f_0)$ through the score estimator $\epsilon_\theta(z_t, t, x, e, f_0)$ obtained from the conditional diffusion, as summarized in Algorithm [alg:training](#alg:training).

During model inference, we perform two inferences.

In the first inference, we input all conditions normally.

In the second, we set $e_{tar}$ and $f_0$ to an empty set.  Finally, we perform sampling using the following linear combination of the conditional and unconditional score estimates with a guidance weight $w$ as shown in Algorithm [alg:infer](#alg:infer):

$$
\epsilon_t=(1+w)\epsilon_\theta(z_t, t, x, f_0, e_{tar})-w\epsilon_\theta(z_t, t, x, \varnothing, \varnothing).
$$

Based on the linear combination of predictions from both the conditional and unconditional models, we can thus more effectively reduce the timbre information of the source singer.

![](c2.pdf)

<a id="guidance">Singer guidance using a latent diffusion model.</a>

## 3·Experiment

<a id="result_eer">Subjective indicators (SMOS, NMOS) and objective indicators (SSIM, FPC) under various method.</a>

![](new2.pdf)

<a id="gender">Detailed SMOS for seen and unseen scenarios, including M2M, M2F, F2M and F2F.</a>

### Experiment Setup

To better accomplish the any-to-any SVC task, we choose the OpenSinger [^Huang2021Multi-Singer] dataset, which contains a large number of Chinese singers.

The dataset comprises 74 singers, consisting of 27 males and 47 females, and amounts to a total of 52 hours of recordings. 
To evaluate the conversion efficacy of the proposed system, we test two cases separately on whether  the target singers are included in the training dataset (seen singers) or not (unseen singers).

In the test set, 4 male and 4 female singers are randomly selected as  the unseen singers.

For the case of seen singers, 8 sentences from each remaining 66 singers are chosen 
for testing and the rest of songs are used for training.

We train FastSVC, DiffSVC and So-VITS-SVC models for comparison, where the So-VITS-SVC model is also the VAE used in our LDM method.

To ensure a fair comparison, the PPG for each model is extracted using Whisper [^Radford2023Robust], while F0 is extracted using CREPE  [^Kim2018Crepe].

The singer embeddings are obtained from the pre-trained model of the CAM++ [^Wang2023Cam++],  a speaker verification open-source project on modelscope\footnote{\url{https://modelscope.cn/models/iic/speech_campplus_sv_zh-cn_16k-common/summary}}.

In experiments, all audio files are resampled to 32kHz.

For F0 embedding in the diffusion condition, we first quantize the Log-F0 features into 256 bins and then go through a melody embedding lookup table.

During inference, we re-edit the F0 of the original singer.

Specifically, we calculate the mean F0 values of the target singer and source singer in the voice segment, denoted as ${\rm mean}({f_0}^{src}_v)$ and ${\rm mean}({f_0}^{tar}_v)$ respectively.

Then, we multiply  ${f_0}^{src}$ with the ratio of these mean values to obtain the modified ${f_0}$ as ${f_0}^{src} \times {{\rm mean}({f_0}^{tar}_v)}/{{\rm mean}({f_0}^{src}_v)}$.

For the singer guidance method, we set the distortion probability $p_{uncond}$ during training to 0.1, and the guidance weight $w$ during inference to 0.3 similarly to [^Ho2022Classifier-Free].

The total number of diffusion steps is 100 (i.e., T = 100).

The noise schedule, represented as $\beta$, is configured to be linearly distributed, ranging from a very small value of $1 \times 10^{-4}$ up to $0.06$.

This setup follows the approach used in the DiffSVC model.

In our DiffSVC experiment, to ensure a fair comparison, we use BigVGAN with NSF as the unified vocoder in accordance with the structure adopted in So-VITS-SVC.

Audio samples are available at \url{https://sounddemos.github.io/ldm-svc}.

### Evaluation Metrics

To construct the conversion trials, we separately establish pairs of conversions for both seen and unseen scenarios and a cross-validation strategy  is employed  to conduct audio clips where each singer provides the vocal content information as source and other singers provide timbre information as targets.

We evaluate these converted audios using F0 Pearson correlation (FPC) and Singer Similarity (SSIM), with SSIM calculated via cosine similarity using speaker embedding from the Automatic Speaker Verification model.

For subjective evaluation, we randomly pick 40 audio samples from each model in both seen and unseen senarios.

These include  Male-to-Male (M2M), Male-to-Female (M2F), Female-to-Male (F2M) and Female-to-Female (F2F), and each has 10 conversions. 
We conduct a 5-point Mean Opinion Score (1-bad, 2-poor, 3-fair, 4-good, 5-excellent) test using these randomly selected audio clips.

We invite 10 volunteers to participate in the listening test, where both the Naturalness Mean Opinion Score (NMOS) and Similarity Mean Opinion Score (SMOS) are evaluated.

### Experimental Results

Results for seen and unseen singer cases are shown in Table [result_eer](#result_eer).

Overall, the SMOS, NMOS, SSIM and FPC in the seen scenario are generally better than those in the unseen scenario, which is consistent with common sense. %The FPC exceeds 0.94 on all models except for  FastSVC, aligning almost identically with the input F0.

This might be caused by the consistent use of the BigVGAN structure in the experiments.

In Table 1, the fluency and similarity results of FastSVC are at a relatively low level, indicating a poor applicability to any-to-any conversion, and the generated audio is of poor quality and has no conversion effect.

For the DiffSVC and So-VITS-SVC methods, the scores on NMOS are higher, but the SMOS is relatively low, implying that in case of only providing a few seconds of the target singer's audio, these SVC models may not be able to convert source inputs to the target singer's singing well.

Comparing DiffSVC with So-VITS-SVC, it can be observed that the end-to-end generative model So-VITS-SVC produces a better sound quality than the two-stage DiffSVC model.

However, the So-VITS-SVC model tends to have more severe timbre leakage.

The results of proposed system LDM-SVC and the ablation system without singer guidance mechanism (LDM-SVC-w/o-SD) are also illustrated in Table 1.

It is clear that both variants have higher SMOS than comparison methods and also have some improvement in NMOS.

This discrepancy may be due to the fact that we directly predict $z$ generated by the posterior encoder, whereas during inference with the So-VITS-SVC model, the prior encoder and flow are used to predict $z$ that is approximated by the posterior encoder during training, leading to a mismatch between training and testing.

Comparing the inclusion of singer guidance, the SMOS in the seen and unseen cases are at the same level.

However, in the unseen case, the similarity of the model trained with singer guidance is higher than that in the seen case, and the SMOS scores are closer.

This reveals that the proposed singer guidance method is effective in the zero-shot scenario. 

The SMOS results for the gender-specific SVC tests are shown in Figure 3.

We see that the SMOS scores for all `to male' conversions are significantly lower than those for `to female' cases, which may be due to the imbalance in the dataset between male and female singers.

Overall, the issue of timbre leakage in cross-gender conversion is more severe than in same-gender conversions.

For example, in the case of M2M the similarity of the model using the singer guidance method is lower than that only using the LDM method, which means that singer guidance in M2M conversions is less effective than others. %读起来好费劲、像绕口令一样，上面说effective，这里又是not effective

## 4·Conclusion

In this paper, we proposed the LDM-SVC system, which utilizes an LDM for any-to-any singing voice conversion.  We pre-trained a So-VITS-SVC model, whose posterior encoder and decoder were used to construct a VAE to handle the latent representation.

It was shown that the proposed LDM-SVC method outperforms existing approaches in terms of timbre similarity in  both seen and unseen singer conversion cases.

The designed singer guidance method is beneficial for improving the conversion similarity under zero-shot conditions for unseen singer conversions.

In the future, we will consider cross-domain SVC tasks, e.g., converting input speech into singing voice, as a necessity for low-resource scenarios without singing voice data.

\bibliographystyle{IEEEtran}
\bibliography{mybib}

\end{document}
