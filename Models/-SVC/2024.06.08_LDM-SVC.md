# LDM-SVC: Latent Diffusion Model Based Zero-Shot Any-to-Any Singing Voice Conversion With Singer Guidance

<details>
<summary>基本信息</summary>

- 标题: "LDM-SVC: Latent Diffusion Model Based Zero-Shot Any-to-Any Singing Voice Conversion With Singer Guidance."
- 作者:
  - 01 Shihao Chen
  - 02 Yu Gu
  - 03 Jie Zhang
  - 04 Na Li
  - 05 Rilin Chen
  - 06 Liping Chen
  - 07 Lirong Dai
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.05325v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2406.05325v1](_PDF/2024.06.08_2406.05325v1_LDM-SVC__Latent_Diffusion_Model_Based_Zero-Shot_Any-to-Any_Singing_Voice_Conversion_With_Singer_Guidance.pdf)
  - [Publication] #TODO

</details>

## Abstract

% 1000 characters.
ASCII characters only.
No citations.
Any-to-any singing voice conversion (SVC) is an interesting audio editing technique, aiming to convert the singing voice of one singer into that of another, given only a few seconds of singing data.
However, during the conversion process, the issue of timbre leakage is inevitable: the converted singing voice still sounds like the original singer's voice.
To tackle this, we propose a latent diffusion model for SVC (LDM-SVC)  in this work, which attempts to perform SVC in the latent space using an LDM.
We pretrain a variational autoencoder structure using the noted open-source So-VITS-SVC project based on the VITS framework, which is then used for the LDM training.
Besides, we propose a singer guidance training method based on  classifier-free guidance to further suppress the timbre of the original singer.
Experimental results show the superiority of  the proposed method over previous works in both subjective and objective evaluations of timbre similarity. 

## 1·Introduction

Singing Voice Conversion (SVC) is a popular audio editing technique that aims to change the singing voice of one singer to mimic another.

Different from singing voice synthesis which requires well-designed musical note inputs [^Gu2021ByteSing], [^Cui2024Sifisinger], 
this technique allows users to customize their favorite singers performing any songs just given  corresponding recorded songs sung by other singers.
Unlike the many-to-many or many-to-one scenarios,
%which requires hours of singing data from a single singer, 
the any-to-any SVC is much more challenging, which demands the model to perform conversion for any target singer who were not included in the training set by solely a short snippet of reference singing voice that even lasts for few seconds.
%, can be , including those who were not encountered during the training phase in situations with limited resources. %This makes the any-to-any SVC a more challenging but also a more flexible and practical approach

The main challenge of SVC is to separate and reassemble the singer's unique vocal timbre from the content and melody of songs. 
Similarly to voice conversion,  mainstreaming SVC systems also follows a recognition-synthesis scheme as a typical two-stage process.

In the first stage, singer-independent features such as phonetic posteriorgrams (PPG) [^Sun2016Phonetic], [^Polyak2020Unsupervised], [^Liu2021Fastsvc], [^Liu2021Diffsvc], [^Li2021PPG-Based] from an ASR model and self-supervised learning (SSL) representations [^Jayashankar2023Self-Supervised], [^Zhou2023VITS}-Based] trained on large amounts of unlabeled speech data are used to encode audio. 
These representations serve as intermediary for SVC, which can effectively extract content and  semantic information from waveforms.

In the second stage, acoustic models are involved to generate the target audio or acoustic features 
from these immediate representations.
Various generative models have been employed for SVC decoding, including autoregressive models [^Nachmani2019Unsupervised], [^Deng2020Pitchnet], [^Zhang2020Durian-Sc], [^Takahashi2021Hierarchical], generative adversarial networks (GANs) [^Polyak2020Unsupervised], [^Liu2021Fastsvc], [^Zhou2022HiFi-SVC], variational autoencoder (VAE) [^Luo2020Singing] and diffusion models [^Liu2021Diffsvc].

Despite of naturalness, sound quality and intonation accuracy of converted singing voice have largely improved by above different SVC models, the timbre leakage problems remain serious, especially for the challenges of SVC cross different genders.

This is primarily due to PPG and SSL features containing not only content information but also some timbre information of the original singer.To alleviate the timbre leakage,  many works such as So-VITS-SVC\footnote{\url{https://github.com/PlayVoice/so-vits-svc-5.0/tree/bigvgan-mix-v2}} involved information perturbation by directly adding white noises on 
hidden features or acoustic features. 
However such white noises were totally independent with singer information and the information perturbation modules were not trainable and optimized in the network training stage and directly adding noise on acoustic features may also lead in pronunciation and quality distortion.

![](fig1.pdf)

<a id="ldm-svc">Left: Pre-training procedure of So-VITS-SVC; Right: Training procedure of LDM-SVC.</a>

Recently, Latent Diffusion Model (LDM) based systems have shown a great success in image generation from text such as Stable Diffusion [^Rombach2022High-Resolution] and high-quality sound generation from text such as Tango[^Ghosal2023Text-to-Audio] and  AudioLDM[^Liu2023Audioldm], [^Liu2023AudioLDM], which performed forward and denoising diffusion processes on the   hidden spaces rather 
than acoustic features like other models [^Liu2022Diffsinger], [^Popov2021Diffusion-Based].

Motivated by these models,  we present LDM-SVC, a novel any-to-any SVC method which reconstructs the waveform directly from the latent representation in an end-to-end latent diffusion manner.

Unlike DiffSVC [^Liu2021Diffsvc] employed on mel-spectrograms and requiring an additional vocoder, we conduct the diffusion model on the hidden space from a pre-trained VAE model using  So-VITS-SVC and directly
utilize the predicted latent representation to generate the waveforms by the VAE decoder.

To address the timbre leakage issue, we regard the LDM forward process as a information perturbation process in those both progressively adding noise to decouple the singer timbre from 
content and melody.

Different from the aforementioned methods which simply add white noise on waverforms, such information perturbation module is trainable and conditioned on singer information.

To better decouple singer information, a singer guidance training mechanism is explored, which is inspired by the  classifier-free method [^Ho2022Classifier-Free] in image generation when training the conditional and unconditional diffusion model at the same time.

Comparing with
many state-of-the-art SVC models, both subjective and objective experimental results indicate that our proposed method can achieve greater timbre similarity in any-to-any SVC tasks for both seen singer conversion and unseen singer conversion scenarios and better singing naturalness.

The rest of this paper is organized as follows.

Section 2 outlines the proposed LDM-SVC method.

Experiments are presented in Section 3.

Finally, Section 4 concludes this work.

## 2·Proposed Method

### VAE Pretraining

\label{vae pretrain}

We pretrain the VAE model using So-VITS-SVC, which is a VAE based any-to-any SVC model following the VITS framework [^Kim2021Conditional] and consists of three key components: posterior encoder, prior encoder and decoder as depicted in Figure 1. 

The posterior encoder $\mathcal{E}(\cdot)$ composed of non-causal WaveNet [^Oord2016Wavenet] residual blocks models the posterior distribution $p(z|y,e)$ of the hidden representation $z=\mathcal{E}(y,e)$ from the linear spectrograms  $y$ generated from the original singing waveforms  where singer embedding $e$ is extracted by an additional speaker verification model.

The prior encoder is implemented using a multi-layer Transformer [^Vaswani2017Attention].

Given the PPG and fundamental frequency (F0) denoted as $x$ and $f_0$ respectively, the prior encoder estimates the prior distribution $p(z|x,f_0,e)$ with the target singer's timbre and the flow. 
To bridge the distribution between the prior encoder and posterior encoder, normalizing flow with speaker-normalized affine couplin (SNAC) [^Choi2022Snac] layers is exploited to perform an invertible transformation of a simple distribution into a more complex one. 
The BigVGAN-based decoder [^Lee2022Bigvgan] $\mathcal{D}(\cdot)$ generates the singing waveform from the latent representation $z$ using a neural source filter (NSF) scheme [^Wang2019Neural] with F0 to enhance voice reconstruction quality. %Furthermore, a multi-period discriminator (MPD), a multi-resolution discriminator (MRD) and a multi-scale discriminator (MSD) are employed to constrain the waveform quality in an adversarial manner. 

After training the So-VITS-SVC system, we retain only the posterior encoder and decoder.

The posterior encoder compresses the linear spectrogram to generate the latent representation, used as the prediction target for the diffusion model in LDM-SVC training.

During inference, the latent representation is predicted from Gaussian White Noise by the denoising process, and the waveform is generated via the decoder.

### Latent Diffusion

LDM is  adopted as the probabilistic models that fit the hidden distribution by denoising on data latent space from pretrained VAE model.

We use the Denoising Diffusion Probabilistic Models (DDPM) method [^Ho2020Denoising] to train the diffusion model, which consists of forward and denoising processes.

Initially, we pretrain an SVC model using the So-VITS-SVC framework and combine its posterior encoder and decoder to form a VAE (see the right part of Figure 1).

During the LDM training, the singer's timbre $e$ and the linear spectrogram of the singing voice $y$ are used as inputs to the posterior encoder $\mathcal{E}(\cdot)$, yielding the latent variable $z_0=z=\mathcal{E}(y,e)$. 
In the forward process, the original data distribution is transformed into a standard Gaussian distribution by gradually adding noise according to a fixed schedule $\beta_1,\dots,\beta_T$.

Here, $T$ represents the total timesteps.

The transition from $z_{0}$ to $z_t$ follows a Markov chain, where the conditional distribution $q(z_t|z_{t-1})$ is defined as a Gaussian distribution: $q(z_t|z_{t-1}) = \mathcal{N}(z_t;\sqrt{1-\beta_t}z_{t-1}, {\beta_t}\mathbf{I})$.

The denoising process, parameterized by $\theta$, acts as a denoising function, eliminating the added noise and restoring the original data structure.

This denoising distribution, $p_{\theta}(z_{t-1}|z_t)$, is modeled as a conditional Gaussian distribution.

By using this parameterized denoising process, we iteratively sample the target data $z_0$ from a Gaussian noise for $t = T, T-1, \ldots, 1$.

In each iteration, $z_{t-1}$ is sampled according to $p_{\theta}(z_{t-1}|z_t)$.

The LDM model receives $z_t$ and $t$ as inputs, complemented by conditional inputs such as the singer's timbre $e$, fundamental frequency $f_0$, and PPG $x$.

To put it simply, in the denoising procedure the calculation of $z_{t-1}$ is given by

$$z_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( z_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_{\theta}(z_t, t, x, f_0, e) \right) + \sigma_t \epsilon,
$$

where $\epsilon \sim \mathcal{N}(0, I)$ represents Gaussian white noise, $\alpha_t = 1 - \beta_t$, and $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$.

For a more detailed derivation of (1), please refer to [^Ho2020Denoising].

The configuration of the diffusion model we use aligns with that in DiffSVC.  %based on the denoising diffusion probabilistic model (DDPM) [^Ho2020Denoising].

The difference is that we use the latent representation as the target. %\textcolor{green}{The probability space it resides in is more suitable for the prediction of the DDPM  compared to the original data distribution.}  Specifically, we pretrain an SVC model using the So-VITS-SVC framework, and then combine its posterior encoder and decoder to form a VAE, see the right part of Figure 1. %During the training process of the LDM model, we use the singer's timbre $e$ and the linear spectrogram of the singing voice $y$ as inputs to the posterior encoder $\mathcal{E}(\cdot)$ to obtain the latent variable $z_0=\mathcal{E}(y,e)$. 
The training loss of the latent diffusion model $\epsilon_\theta$ is defined as the
mean squared error (MSE) in the noise space:

$$
\mathcal{L}_{LDM}=||\epsilon-\epsilon_\theta(z_t, t, x, f_0, e)||_2^2.
$$

During inference, the singer's timbre and F0 are replaced with the target singer's attributes, defined as $e_{tar}$ and $f_0$, while the source singer's PPG $x_{src}$ is used as a condition.

We sample a random Gaussian White Noise in the denoising process to obtain latent variable $z_0$.

Consequently, $z_0$, $e_{tar}$, and $f_0$ are fed into the pretrained decoder $\mathcal{D}(\cdot)$ to generate the audio waveform.

The use of LDM can alleviate the inconsistency caused by training-testing mismatches in So-VITS-SVC.

\begin{algorithm}[t]
\caption{Training procedure of LDM-SVC.}

\begin{algorithmic}[1]
\label{alg:training}
\REQUIRE Conversion model $\epsilon_\theta(\cdot)$; training set $D_{train} = \{(x,{f_0},e,y)\}_{m=1}^M$; pretrained So-VITS-SVC posterior encoder $\mathcal{E(\cdot)}$; distortion probability $p_{uncond}$; $N_{iter}$ iterations.\FOR{$i=1,2,...,N_{iter}$}
\STATE Sample $(x, f_0, e, y)$ from $D_{train}$;
\STATE $z_0=\mathcal{E}(y,e)$;
\STATE $e,f_0 \leftarrow \varnothing$ with probability $p_{uncond}$;
\STATE $\epsilon\sim\mathcal{N}(0,I)$;
\STATE Sample $t\sim$ Uniform$(\{1, \cdots, T\})$;
\STATE Take gradient descent step on \\ \quad$\nabla_\theta||\epsilon-\epsilon_\theta(\sqrt{\bar\alpha_t}z_0+\sqrt{1-\bar\alpha_t}\epsilon, t, x, f_0, e)||_2^2$;
\ENDFOR
\RETURN $\epsilon_\theta(\cdot)$;
\end{algorithmic}

\end{algorithm}

\begin{algorithm}[t]
\caption{Inference procedure of LDM-SVC.}

\begin{algorithmic}[1]
\label{alg:infer}
\REQUIRE Trained conversion model $\epsilon_\theta(\cdot)$; source singer 
PPG $x_{src}$; target singer embedding $e_{tar}$; modified $f_0$; pretrained So-VITS-SVC decoder $\mathcal{D}(\cdot)$; guidance weight $w$.\STATE Sample $z_T\sim\mathcal{N}(0,I)$;
\FOR{$t=T,T-1,...,1$}
\STATE $\epsilon_t=(1+w)\epsilon_\theta(z_t, t, x, f_0, e_{tar})-w\epsilon_\theta(z_t, t, x, \varnothing, \varnothing)$;\STATE $\epsilon\sim\mathcal{N}(0,I)$ if $t>1$ else $\epsilon=0$;
\STATE $z_{t-1} = \frac{1}{\sqrt{\alpha_t}}(z_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_t)+\sigma_t \epsilon$;
\ENDFOR
\RETURN $\mathcal{D}(z_0,f_0,e_{tar})$;
\end{algorithmic}

\end{algorithm}

### Singer Guidance

As in Figure 2, we employ the speaker condition layer normalization (SCLN) [^Wu2021Cross-Speaker] to normalize the PPG feature.

Additionally, we use classifier-free guidance [^Ho2022Classifier-Free] to train the model.

Specifically, we expect the model to predict $p(z|x)$ via the score estimator $\epsilon_\theta(z_t, t, x)$ obtained from unconditional diffusion, while simultaneously predicting $p(z|x, e, f_0)$ through the score estimator $\epsilon_\theta(z_t, t, x, e, f_0)$ obtained from the conditional diffusion, as summarized in Algorithm [alg:training](#alg:training).

During model inference, we perform two inferences.

In the first inference, we input all conditions normally.

In the second, we set $e_{tar}$ and $f_0$ to an empty set.  Finally, we perform sampling using the following linear combination of the conditional and unconditional score estimates with a guidance weight $w$ as shown in Algorithm [alg:infer](#alg:infer):

$$
\epsilon_t=(1+w)\epsilon_\theta(z_t, t, x, f_0, e_{tar})-w\epsilon_\theta(z_t, t, x, \varnothing, \varnothing).
$$

Based on the linear combination of predictions from both the conditional and unconditional models, we can thus more effectively reduce the timbre information of the source singer.

![](c2.pdf)

<a id="guidance">Singer guidance using a latent diffusion model.</a>
