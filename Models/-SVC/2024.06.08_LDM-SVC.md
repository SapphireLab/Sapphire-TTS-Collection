# LDM-SVC: Latent Diffusion Model Based Zero-Shot Any-to-Any Singing Voice Conversion With Singer Guidance

<details>
<summary>基本信息</summary>

- 标题: "LDM-SVC: Latent Diffusion Model Based Zero-Shot Any-to-Any Singing Voice Conversion With Singer Guidance."
- 作者:
  - 01 Shihao Chen
  - 02 Yu Gu
  - 03 Jie Zhang
  - 04 Na Li
  - 05 Rilin Chen
  - 06 Liping Chen
  - 07 Lirong Dai
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.05325v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2406.05325v1](_PDF/2024.06.08_2406.05325v1_LDM-SVC__Latent_Diffusion_Model_Based_Zero-Shot_Any-to-Any_Singing_Voice_Conversion_With_Singer_Guidance.pdf)
  - [Publication] #TODO

</details>

## Abstract

% 1000 characters.
ASCII characters only.
No citations.
Any-to-any singing voice conversion (SVC) is an interesting audio editing technique, aiming to convert the singing voice of one singer into that of another, given only a few seconds of singing data.
However, during the conversion process, the issue of timbre leakage is inevitable: the converted singing voice still sounds like the original singer's voice.
To tackle this, we propose a latent diffusion model for SVC (LDM-SVC)  in this work, which attempts to perform SVC in the latent space using an LDM.
We pretrain a variational autoencoder structure using the noted open-source So-VITS-SVC project based on the VITS framework, which is then used for the LDM training.
Besides, we propose a singer guidance training method based on  classifier-free guidance to further suppress the timbre of the original singer.
Experimental results show the superiority of  the proposed method over previous works in both subjective and objective evaluations of timbre similarity. 

## 1·Introduction

Singing Voice Conversion (SVC) is a popular audio editing technique that aims to change the singing voice of one singer to mimic another.

Different from singing voice synthesis which requires well-designed musical note inputs [^Gu2021ByteSing], [^Cui2024Sifisinger], 
this technique allows users to customize their favorite singers performing any songs just given  corresponding recorded songs sung by other singers.
Unlike the many-to-many or many-to-one scenarios,
%which requires hours of singing data from a single singer, 
the any-to-any SVC is much more challenging, which demands the model to perform conversion for any target singer who were not included in the training set by solely a short snippet of reference singing voice that even lasts for few seconds.
%, can be , including those who were not encountered during the training phase in situations with limited resources. %This makes the any-to-any SVC a more challenging but also a more flexible and practical approach

The main challenge of SVC is to separate and reassemble the singer's unique vocal timbre from the content and melody of songs. 
Similarly to voice conversion,  mainstreaming SVC systems also follows a recognition-synthesis scheme as a typical two-stage process.

In the first stage, singer-independent features such as phonetic posteriorgrams (PPG) [^Sun2016Phonetic], [^Polyak2020Unsupervised], [^Liu2021Fastsvc], [^Liu2021Diffsvc], [^Li2021PPG-Based] from an ASR model and self-supervised learning (SSL) representations [^Jayashankar2023Self-Supervised], [^Zhou2023VITS}-Based] trained on large amounts of unlabeled speech data are used to encode audio. 
These representations serve as intermediary for SVC, which can effectively extract content and  semantic information from waveforms.

In the second stage, acoustic models are involved to generate the target audio or acoustic features 
from these immediate representations.
Various generative models have been employed for SVC decoding, including autoregressive models [^Nachmani2019Unsupervised], [^Deng2020Pitchnet], [^Zhang2020Durian-Sc], [^Takahashi2021Hierarchical], generative adversarial networks (GANs) [^Polyak2020Unsupervised], [^Liu2021Fastsvc], [^Zhou2022HiFi-SVC], variational autoencoder (VAE) [^Luo2020Singing] and diffusion models [^Liu2021Diffsvc].

Despite of naturalness, sound quality and intonation accuracy of converted singing voice have largely improved by above different SVC models, the timbre leakage problems remain serious, especially for the challenges of SVC cross different genders.

This is primarily due to PPG and SSL features containing not only content information but also some timbre information of the original singer.To alleviate the timbre leakage,  many works such as So-VITS-SVC\footnote{\url{https://github.com/PlayVoice/so-vits-svc-5.0/tree/bigvgan-mix-v2}} involved information perturbation by directly adding white noises on 
hidden features or acoustic features. 
However such white noises were totally independent with singer information and the information perturbation modules were not trainable and optimized in the network training stage and directly adding noise on acoustic features may also lead in pronunciation and quality distortion.

![](fig1.pdf)

<a id="ldm-svc">Left: Pre-training procedure of So-VITS-SVC; Right: Training procedure of LDM-SVC.</a>

Recently, Latent Diffusion Model (LDM) based systems have shown a great success in image generation from text such as Stable Diffusion [^Rombach2022High-Resolution] and high-quality sound generation from text such as Tango[^Ghosal2023Text-to-Audio] and  AudioLDM[^Liu2023Audioldm], [^Liu2023AudioLDM], which performed forward and denoising diffusion processes on the   hidden spaces rather 
than acoustic features like other models [^Liu2022Diffsinger], [^Popov2021Diffusion-Based].

Motivated by these models,  we present LDM-SVC, a novel any-to-any SVC method which reconstructs the waveform directly from the latent representation in an end-to-end latent diffusion manner.

Unlike DiffSVC [^Liu2021Diffsvc] employed on mel-spectrograms and requiring an additional vocoder, we conduct the diffusion model on the hidden space from a pre-trained VAE model using  So-VITS-SVC and directly
utilize the predicted latent representation to generate the waveforms by the VAE decoder.

To address the timbre leakage issue, we regard the LDM forward process as a information perturbation process in those both progressively adding noise to decouple the singer timbre from 
content and melody.

Different from the aforementioned methods which simply add white noise on waverforms, such information perturbation module is trainable and conditioned on singer information.

To better decouple singer information, a singer guidance training mechanism is explored, which is inspired by the  classifier-free method [^Ho2022Classifier-Free] in image generation when training the conditional and unconditional diffusion model at the same time.

Comparing with
many state-of-the-art SVC models, both subjective and objective experimental results indicate that our proposed method can achieve greater timbre similarity in any-to-any SVC tasks for both seen singer conversion and unseen singer conversion scenarios and better singing naturalness.

The rest of this paper is organized as follows.

Section 2 outlines the proposed LDM-SVC method.

Experiments are presented in Section 3.

Finally, Section 4 concludes this work.
