# RobustSVC: HuBERT-based Melody Extractor and Adversarial Learning for Robust Singing Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "RobustSVC: HuBERT-based Melody Extractor and Adversarial Learning for Robust Singing Voice Conversion."
- 作者:
  - 01 Wei Chen
  - 02 Xintao Zhao
  - 03 Jun Chen
  - 04 Binzhu Sha
  - 05 Zhiwei Lin
  - 06 Zhiyong Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2409.06237v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2409.06237v1](_PDF\2024.09.10_2409.06237v1_RobustSVC__HuBERT-based_Melody_Extractor_and_Adversarial_Learning_for_Robust_Singing_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

Singing voice conversion (SVC) is hindered by noise sensitivity due to the use of non-robust methods for extracting pitch and energy during the inference. 
As clean signals are key for the source audio in SVC, music source separation preprocessing offers a viable solution for handling noisy audio, like singing with background music (BGM). 
However, current separating methods struggle to fully remove noise or excessively suppress signal components, affecting the naturalness and similarity of the processed audio. 
To tackle this, our study introduces RobustSVC, a novel any-to-one SVC framework that converts noisy vocals into clean vocals sung by the target singer. 
We replace the non-robust feature with a HuBERT-based melody extractor and use adversarial training mechanisms with three discriminators to reduce information leakage in self-supervised representations. 
Experimental results show that RobustSVC is noise-robust and achieves higher similarity and naturalness than baseline methods in both noisy and clean vocal conditions.

## 1·Introduction

SVC, a downstream research task of voice conversion (VC), aims to transform the timbre of the original singer's voice in a song to that of a target singer without altering aspects like melody and lyrics. 
Compared to conventional VC task, the SVC system requires a more sophisticated modeling of acoustic features. 
In the VC task, slight differences in prosody and tempo between the converted audio and input audio are acceptable. 

However, within the realm of SVC, musical features such as melody are closely tied to the song's essence, which means they are song-dependent and should be precisely preserved.

Traditional SVC methods [^Kobayashi2014Statistical], [^Villavicencio2010Applying], [^Kobayashi2015Statistical], [^Toda2007One-to-Many] generally use statistical models to build the mapping between the parallel samples which are expensive in practice. 

Non-parallel SVC is a more challenging but more practical undertaking in applications. 
A common way to achieve non-parallel SVC is extracting a singer-independent content representation and then producing the converted singing voices by utilizing the target singer embedding. 

To get content representations, some methods tried to disentangle content feature from speech in an unsupervised manner[^Luo2020Singing], [^Deng2020Pitchnet], [^Lu2020Vaw-Gan]. 

Nevertheless, due to the difficulty of unsupervised learning, its ability of feature disentanglement is not robust enough for redundant noise from input data[^Li2021PPG-Based].

![](11.pdf)

<a id="fig:baseline">Recognition-synthesis based SVC framework</a>

Recognition-synthesis based SVC is another representative approach of the non-parallel SVC, demonstrating significant performance enhancement[^Li2021PPG-Based], [^Chen2019Singing], [^Guo2020Phonetic], [^Zhang2023Leveraging], [^Ning2023Vits-Based], [^Wang2021Towards].

The framework is shown in Figure~[fig:baseline](#fig:baseline).

An intermediate embedding is extracted from a certain layer of the ASR model, either phonetic posteriorgrams (PPGs)[^Chen2019Singing] from the last layer regardless of preformed extractor structure, or bottleneck features (BNFs)[^Ning2023Vits-Based] from the penultimate layer with well-designed dimension, and then fed into a SVC system after combining with pitch and energy.

Since the singing voice contains much more characteristics like melody, which are difficult to model, several works have tried to extract melody feature while all of these efforts require clean source audio signals.

In [^Li2021PPG-Based], in addition to pitch, Mel-spectrogram is also inputted into the reference encoder to implicitly capture singing characteristics. 
Nonetheless, Mel-spectrogram contains a considerable amount of singer-specific information that is challenging to eliminate, ultimately leading to performance degradation. 
Moreover, in the process of dealing with noisy audio, such as singing alongside BGM, extracting accurate melody information from Mel-spectrogram presents significant challenges.

In [^Wang2021Towards], they utilized the HuBERT feature and pitch for melody feature modeling. 
However, as the pitch is not a robust feature, when confronted with noisy singing voice inputs, the effectiveness of melody extraction will significantly decrease, consequently affecting the quality of converted audio.

Despite using a music source separation model for preprocessing before conversion, it can only partially alleviate the noise sensitivity concern. 
Specifically, existing separating methods suffer from residual noise or over-suppression problems, leading to loss of voice information[^Dai2020Noise] and subsequently affecting the similarity and naturalness of the converted audio.

![](15v2.pdf)

<a id="fig:arc">RobustSVC framework.</a>

% HuBERT-based melody extractor
% Proposed model architecture.

In this work, to address this challenge, we propose RobustSVC, a high-quality noise-robust SVC system capable of converting noisy source audio into clean audio performed by the target singer.

Since the HuBERT feature contains melody information mentioned in [^Wang2021Towards], we design a HuBERT-based melody extractor with the purpose of modeling melody to further address the noise sensitivity issue. 
In order to provide representations with good disentangling properties[^Zhao2022Disentangling], RobustSVC adopts the BNFs extracted from the ASR model trained with Connectionist Temporal Classification loss (CTC-BNFs). 
Additionally, three auxiliary discriminators are implemented to generate Mel-spectrogram with high similarity as well as quality.

The contributions of our work are as follows:

-  We propose a HuBERT-based melody extractor that does not rely on non-robust methods for extracting pitch and energy during inference, achieving more robust and accurate melody modeling. 

-  We further develop a novel SVC framework, RobustSVC, which combines CTC-BNFs and an adversarial training strategy to reduce residual speaker information, enhance audio quality, and improve voice similarity.

-  Experimental results reveal that RobustSVC is not only noise-robust but also outperforms the baseline method in terms of subjective and objective evaluations for both noisy and clean vocal conditions, achieving higher similarity and naturalness.

## 2·Method

Our proposed RobustSVC system, shown in Figure~[fig:arc](#fig:arc), consists of six components: (i) A content encoder to encode CTC-BNFs into content embedding. 
(ii) A HuBERT-based melody extractor to accurately model the melody. 
(iii) A melody encoder followed by conditional instance normalization (CIN)[^Dumoulin2016Learned] to encode melody feature. 
(iv) A Feed Forward Transformer (FFT) based decoder. 
(v) three different discriminators which could help the framework generate Mel-spectrogram with high similarity as well as quality. 
(vi) A pre-trained HiFi-GAN[^Kong2020Hifi-Gan] vocoder which is finetinued by Ground Truth Aligned (GTA) mode. 
% Jun Chen：最后能加个后面具体介绍模中的细节的连接词

The HuBERT-based melody extractor and ASR system are initially trained and then frozen to extract melody feature and CTC-BNFs, respectively. 
Afterwards, they are input into the encoder-decoder model to be trained in generating high-quality Mel-spectrogram, which is then utilized by the HiFi-GAN vocoder for fine-tuning to produce the converted audio. 
The details of each part are as follows.

### ASR System

The ASR acoustic model based on Conformer[^Gulati2020Conformer] achieves excellent recognition performance by combining the global modeling ability of the attention mechanism with the local modeling ability of CNN. 

We train the ASR model using the Conformer encoder with the CTC loss function and extract 256-dims BNFs from the last hidden layer of the Conformer encoder to utilize its effective disentangling properties, which lead to high similarity[^Zhao2022Disentangling]. 

### Encoder-Decoder Model

As shown in Figure~[fig:arc](#fig:arc), the proposed conversion model is an encoder-decoder architecture based on the FFT, renowned for its efficient computational speed and high-quality output generation.

First, the input melody feature is processed by the melody encoder with 3 FFT blocks. 
The output of the melody encoder is concatenated with the content embedding extracted by the content encoder after eliminating the timbre information through the CIN module. 
The concatenated content and melody embedding are further input into the decoder based on the 6 FFT blocks to generate high-quality spectrograms. 
We compute the reconstruction loss between the ground truth and the reconstructed Mel-spectrogram, which is defined as follows:

$$

L_{rec} =  \Vert y^{f} - y^{g} \Vert_1

$$

where $y^{g}$ and $y^{f}$ are the Mel-spectrogram of the ground truth and reconstructed audio respectively.

### HuBERT-based Melody Extractor

To avoid relying on non-robust methods for extracting pitch and energy in obtaining melody information, we develop a melody extractor based on the HuBERT architecture, as depicted in Figure~[fig:hub](#fig:hub).

This extractor comprises a pre-trained HuBERT model augmented with three FFT blocks, which is fine-tuned for melody extraction tasks within limited vocal data scenarios. 

To strengthen the model's noise robustness, noisy training data is fed into the model. 
The output from the penultimate layer of the melody extractor is chosen as the melody feature input for the melody encoder. 
The loss function of the melody extractor is defined as follows:

$$

L_{melody} =  \Vert P^{f} - P^{g} \Vert_1 + \Vert E^{f} - E^{g} \Vert_1 +\Vert V^{f} - V^{g} \Vert_1

$$

where $ P^{f}$ and $P^{g}$ are the ground truth and predicted pitch, $ E^{f}$ and $E^{g}$ are the ground truth and predicted energy, $V^{f}$ and $V^{g}$ are the ground truth and predicted voice/unvoice flag (VUV) respectively.

![](16.pdf)

<a id="fig:hub">HuBERT-based melody extractor</a>

### Discriminator Group

Inspired by[^Zhao2023Adversarial], we introduce three different discriminators.

The real/fake discriminator aims to determine whether a sequence of Mel-spectrogram is extracted from the ground-truth audio or reconstructed by the SVC model, enhancing the quality of Mel-spectrogram generation.

Notably, the source audio used for reconstructed Mel comes from the target speaker’s corpora. 
The conversion discriminator is to distinguish whether a sequence of Mel-spectrogram has been converted by the SVC model, whose source audio comes from the external corpora. 
The embedding discriminator is tasked with judging whether a sequence of melody embeddings is extracted from the external corpora, which contain speaker information different from the target speaker. 
The latter two discriminators are able to minimize residual speaker information across different domains.

The final loss functions are given as follows:
\vspace{-1.2pt}

$$
\begin{aligned}

L(D)&=L_{sim}(D)+L_{rf}(D_{r}) \\
L(G)&=L_{sim}(G)+L_{rf}(G)+L_{rec}

\end{aligned}
$$

\vspace{-1.2pt}
where $ L_{sim}$ is the similarity-adversarial loss of conversion discriminator and embedding discriminator, $L_{rf}$ is the loss of real/fake discriminator aimed at enhancing the quality of the generated spectrograms.

### Vocoder

As the pre-trained HiFi-GAN[^Kong2020Hifi-Gan] vocoder is trained on pure speech data, its reconstruction performance on singing waveforms is limited. 
To address this limitation, we integrate the HiFi-GAN vocoder model into the training pipeline after a well-trained SVC model, a process known as the GTA mode[^Shen2018Natural]. 
During the finetuning process, the parameters of the SVC model are frozen, and the vocoder leverages the spectra predicted by the SVC as input to map them to real waveform data.

This strategy aims to mitigate any negative effects arising from differences in the training data domain while also smoothing out deficiencies in the predictions made by the SVC model.
