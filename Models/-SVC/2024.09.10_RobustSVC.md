# RobustSVC: HuBERT-based Melody Extractor and Adversarial Learning for Robust Singing Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "RobustSVC: HuBERT-based Melody Extractor and Adversarial Learning for Robust Singing Voice Conversion."
- 作者:
  - 01 Wei Chen
  - 02 Xintao Zhao
  - 03 Jun Chen
  - 04 Binzhu Sha
  - 05 Zhiwei Lin
  - 06 Zhiyong Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2409.06237v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2409.06237v1](_PDF\2024.09.10_2409.06237v1_RobustSVC__HuBERT-based_Melody_Extractor_and_Adversarial_Learning_for_Robust_Singing_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

Singing voice conversion (SVC) is hindered by noise sensitivity due to the use of non-robust methods for extracting pitch and energy during the inference. 
As clean signals are key for the source audio in SVC, music source separation preprocessing offers a viable solution for handling noisy audio, like singing with background music (BGM). 
However, current separating methods struggle to fully remove noise or excessively suppress signal components, affecting the naturalness and similarity of the processed audio. 
To tackle this, our study introduces RobustSVC, a novel any-to-one SVC framework that converts noisy vocals into clean vocals sung by the target singer. 
We replace the non-robust feature with a HuBERT-based melody extractor and use adversarial training mechanisms with three discriminators to reduce information leakage in self-supervised representations. 
Experimental results show that RobustSVC is noise-robust and achieves higher similarity and naturalness than baseline methods in both noisy and clean vocal conditions.

## 1·Introduction

SVC, a downstream research task of voice conversion (VC), aims to transform the timbre of the original singer's voice in a song to that of a target singer without altering aspects like melody and lyrics. 
Compared to conventional VC task, the SVC system requires a more sophisticated modeling of acoustic features. 
In the VC task, slight differences in prosody and tempo between the converted audio and input audio are acceptable. 

However, within the realm of SVC, musical features such as melody are closely tied to the song's essence, which means they are song-dependent and should be precisely preserved.

Traditional SVC methods [^Kobayashi2014Statistical], [^Villavicencio2010Applying], [^Kobayashi2015Statistical], [^Toda2007One-to-Many] generally use statistical models to build the mapping between the parallel samples which are expensive in practice. 

Non-parallel SVC is a more challenging but more practical undertaking in applications. 
A common way to achieve non-parallel SVC is extracting a singer-independent content representation and then producing the converted singing voices by utilizing the target singer embedding. 

To get content representations, some methods tried to disentangle content feature from speech in an unsupervised manner[^Luo2020Singing], [^Deng2020Pitchnet], [^Lu2020Vaw-Gan]. 

Nevertheless, due to the difficulty of unsupervised learning, its ability of feature disentanglement is not robust enough for redundant noise from input data[^Li2021PPG-Based].

![](11.pdf)

<a id="fig:baseline">Recognition-synthesis based SVC framework</a>

Recognition-synthesis based SVC is another representative approach of the non-parallel SVC, demonstrating significant performance enhancement[^Li2021PPG-Based], [^Chen2019Singing], [^Guo2020Phonetic], [^Zhang2023Leveraging], [^Ning2023Vits-Based], [^Wang2021Towards].

The framework is shown in Figure~[fig:baseline](#fig:baseline).

An intermediate embedding is extracted from a certain layer of the ASR model, either phonetic posteriorgrams (PPGs)[^Chen2019Singing] from the last layer regardless of preformed extractor structure, or bottleneck features (BNFs)[^Ning2023Vits-Based] from the penultimate layer with well-designed dimension, and then fed into a SVC system after combining with pitch and energy.

Since the singing voice contains much more characteristics like melody, which are difficult to model, several works have tried to extract melody feature while all of these efforts require clean source audio signals.

In [^Li2021PPG-Based], in addition to pitch, Mel-spectrogram is also inputted into the reference encoder to implicitly capture singing characteristics. 
Nonetheless, Mel-spectrogram contains a considerable amount of singer-specific information that is challenging to eliminate, ultimately leading to performance degradation. 
Moreover, in the process of dealing with noisy audio, such as singing alongside BGM, extracting accurate melody information from Mel-spectrogram presents significant challenges.

In [^Wang2021Towards], they utilized the HuBERT feature and pitch for melody feature modeling. 
However, as the pitch is not a robust feature, when confronted with noisy singing voice inputs, the effectiveness of melody extraction will significantly decrease, consequently affecting the quality of converted audio.

Despite using a music source separation model for preprocessing before conversion, it can only partially alleviate the noise sensitivity concern. 
Specifically, existing separating methods suffer from residual noise or over-suppression problems, leading to loss of voice information[^Dai2020Noise] and subsequently affecting the similarity and naturalness of the converted audio.

![](15v2.pdf)

<a id="fig:arc">RobustSVC framework.</a>

% HuBERT-based melody extractor
% Proposed model architecture.

In this work, to address this challenge, we propose RobustSVC, a high-quality noise-robust SVC system capable of converting noisy source audio into clean audio performed by the target singer.

Since the HuBERT feature contains melody information mentioned in [^Wang2021Towards], we design a HuBERT-based melody extractor with the purpose of modeling melody to further address the noise sensitivity issue. 
In order to provide representations with good disentangling properties[^Zhao2022Disentangling], RobustSVC adopts the BNFs extracted from the ASR model trained with Connectionist Temporal Classification loss (CTC-BNFs). 
Additionally, three auxiliary discriminators are implemented to generate Mel-spectrogram with high similarity as well as quality.

The contributions of our work are as follows:

-  We propose a HuBERT-based melody extractor that does not rely on non-robust methods for extracting pitch and energy during inference, achieving more robust and accurate melody modeling. 

-  We further develop a novel SVC framework, RobustSVC, which combines CTC-BNFs and an adversarial training strategy to reduce residual speaker information, enhance audio quality, and improve voice similarity.

-  Experimental results reveal that RobustSVC is not only noise-robust but also outperforms the baseline method in terms of subjective and objective evaluations for both noisy and clean vocal conditions, achieving higher similarity and naturalness.
