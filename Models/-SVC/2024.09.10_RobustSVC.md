# RobustSVC: HuBERT-based Melody Extractor and Adversarial Learning for Robust Singing Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "RobustSVC: HuBERT-based Melody Extractor and Adversarial Learning for Robust Singing Voice Conversion."
- 作者:
  - 01 Wei Chen
  - 02 Xintao Zhao
  - 03 Jun Chen
  - 04 Binzhu Sha
  - 05 Zhiwei Lin
  - 06 Zhiyong Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2409.06237v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2409.06237v1](_PDF\2024.09.10_2409.06237v1_RobustSVC__HuBERT-based_Melody_Extractor_and_Adversarial_Learning_for_Robust_Singing_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

Singing voice conversion (SVC) is hindered by noise sensitivity due to the use of non-robust methods for extracting pitch and energy during the inference. 
As clean signals are key for the source audio in SVC, music source separation preprocessing offers a viable solution for handling noisy audio, like singing with background music (BGM). 
However, current separating methods struggle to fully remove noise or excessively suppress signal components, affecting the naturalness and similarity of the processed audio. 
To tackle this, our study introduces RobustSVC, a novel any-to-one SVC framework that converts noisy vocals into clean vocals sung by the target singer. 
We replace the non-robust feature with a HuBERT-based melody extractor and use adversarial training mechanisms with three discriminators to reduce information leakage in self-supervised representations. 
Experimental results show that RobustSVC is noise-robust and achieves higher similarity and naturalness than baseline methods in both noisy and clean vocal conditions.

## 1·Introduction

SVC, a downstream research task of voice conversion (VC), aims to transform the timbre of the original singer's voice in a song to that of a target singer without altering aspects like melody and lyrics. 
Compared to conventional VC task, the SVC system requires a more sophisticated modeling of acoustic features. 
In the VC task, slight differences in prosody and tempo between the converted audio and input audio are acceptable. 

However, within the realm of SVC, musical features such as melody are closely tied to the song's essence, which means they are song-dependent and should be precisely preserved.

Traditional SVC methods [^Kobayashi2014Statistical], [^Villavicencio2010Applying], [^Kobayashi2015Statistical], [^Toda2007One-to-Many] generally use statistical models to build the mapping between the parallel samples which are expensive in practice. 

Non-parallel SVC is a more challenging but more practical undertaking in applications. 
A common way to achieve non-parallel SVC is extracting a singer-independent content representation and then producing the converted singing voices by utilizing the target singer embedding. 

To get content representations, some methods tried to disentangle content feature from speech in an unsupervised manner[^Luo2020Singing], [^Deng2020Pitchnet], [^Lu2020Vaw-Gan]. 

Nevertheless, due to the difficulty of unsupervised learning, its ability of feature disentanglement is not robust enough for redundant noise from input data[^Li2021PPG-Based].

![](11.pdf)

<a id="fig:baseline">Recognition-synthesis based SVC framework</a>

Recognition-synthesis based SVC is another representative approach of the non-parallel SVC, demonstrating significant performance enhancement[^Li2021PPG-Based], [^Chen2019Singing], [^Guo2020Phonetic], [^Zhang2023Leveraging], [^Ning2023Vits-Based], [^Wang2021Towards].

The framework is shown in Figure~[fig:baseline](#fig:baseline).

An intermediate embedding is extracted from a certain layer of the ASR model, either phonetic posteriorgrams (PPGs)[^Chen2019Singing] from the last layer regardless of preformed extractor structure, or bottleneck features (BNFs)[^Ning2023Vits-Based] from the penultimate layer with well-designed dimension, and then fed into a SVC system after combining with pitch and energy.

Since the singing voice contains much more characteristics like melody, which are difficult to model, several works have tried to extract melody feature while all of these efforts require clean source audio signals.

In [^Li2021PPG-Based], in addition to pitch, Mel-spectrogram is also inputted into the reference encoder to implicitly capture singing characteristics. 
Nonetheless, Mel-spectrogram contains a considerable amount of singer-specific information that is challenging to eliminate, ultimately leading to performance degradation. 
Moreover, in the process of dealing with noisy audio, such as singing alongside BGM, extracting accurate melody information from Mel-spectrogram presents significant challenges.

In [^Wang2021Towards], they utilized the HuBERT feature and pitch for melody feature modeling. 
However, as the pitch is not a robust feature, when confronted with noisy singing voice inputs, the effectiveness of melody extraction will significantly decrease, consequently affecting the quality of converted audio.

Despite using a music source separation model for preprocessing before conversion, it can only partially alleviate the noise sensitivity concern. 
Specifically, existing separating methods suffer from residual noise or over-suppression problems, leading to loss of voice information[^Dai2020Noise] and subsequently affecting the similarity and naturalness of the converted audio.

![](15v2.pdf)

<a id="fig:arc">RobustSVC framework.</a>

% HuBERT-based melody extractor
% Proposed model architecture.

In this work, to address this challenge, we propose RobustSVC, a high-quality noise-robust SVC system capable of converting noisy source audio into clean audio performed by the target singer.

Since the HuBERT feature contains melody information mentioned in [^Wang2021Towards], we design a HuBERT-based melody extractor with the purpose of modeling melody to further address the noise sensitivity issue. 
In order to provide representations with good disentangling properties[^Zhao2022Disentangling], RobustSVC adopts the BNFs extracted from the ASR model trained with Connectionist Temporal Classification loss (CTC-BNFs). 
Additionally, three auxiliary discriminators are implemented to generate Mel-spectrogram with high similarity as well as quality.

The contributions of our work are as follows:

-  We propose a HuBERT-based melody extractor that does not rely on non-robust methods for extracting pitch and energy during inference, achieving more robust and accurate melody modeling. 

-  We further develop a novel SVC framework, RobustSVC, which combines CTC-BNFs and an adversarial training strategy to reduce residual speaker information, enhance audio quality, and improve voice similarity.

-  Experimental results reveal that RobustSVC is not only noise-robust but also outperforms the baseline method in terms of subjective and objective evaluations for both noisy and clean vocal conditions, achieving higher similarity and naturalness.

## 2·Method

Our proposed RobustSVC system, shown in Figure~[fig:arc](#fig:arc), consists of six components: (i) A content encoder to encode CTC-BNFs into content embedding. 
(ii) A HuBERT-based melody extractor to accurately model the melody. 
(iii) A melody encoder followed by conditional instance normalization (CIN)[^Dumoulin2016Learned] to encode melody feature. 
(iv) A Feed Forward Transformer (FFT) based decoder. 
(v) three different discriminators which could help the framework generate Mel-spectrogram with high similarity as well as quality. 
(vi) A pre-trained HiFi-GAN[^Kong2020Hifi-Gan] vocoder which is finetinued by Ground Truth Aligned (GTA) mode. 
% Jun Chen：最后能加个后面具体介绍模中的细节的连接词

The HuBERT-based melody extractor and ASR system are initially trained and then frozen to extract melody feature and CTC-BNFs, respectively. 
Afterwards, they are input into the encoder-decoder model to be trained in generating high-quality Mel-spectrogram, which is then utilized by the HiFi-GAN vocoder for fine-tuning to produce the converted audio. 
The details of each part are as follows.

### ASR System

The ASR acoustic model based on Conformer[^Gulati2020Conformer] achieves excellent recognition performance by combining the global modeling ability of the attention mechanism with the local modeling ability of CNN. 

We train the ASR model using the Conformer encoder with the CTC loss function and extract 256-dims BNFs from the last hidden layer of the Conformer encoder to utilize its effective disentangling properties, which lead to high similarity[^Zhao2022Disentangling]. 

### Encoder-Decoder Model

As shown in Figure~[fig:arc](#fig:arc), the proposed conversion model is an encoder-decoder architecture based on the FFT, renowned for its efficient computational speed and high-quality output generation.

First, the input melody feature is processed by the melody encoder with 3 FFT blocks. 
The output of the melody encoder is concatenated with the content embedding extracted by the content encoder after eliminating the timbre information through the CIN module. 
The concatenated content and melody embedding are further input into the decoder based on the 6 FFT blocks to generate high-quality spectrograms. 
We compute the reconstruction loss between the ground truth and the reconstructed Mel-spectrogram, which is defined as follows:

$$

L_{rec} =  \Vert y^{f} - y^{g} \Vert_1

$$

where $y^{g}$ and $y^{f}$ are the Mel-spectrogram of the ground truth and reconstructed audio respectively.

### HuBERT-based Melody Extractor

To avoid relying on non-robust methods for extracting pitch and energy in obtaining melody information, we develop a melody extractor based on the HuBERT architecture, as depicted in Figure~[fig:hub](#fig:hub).

This extractor comprises a pre-trained HuBERT model augmented with three FFT blocks, which is fine-tuned for melody extraction tasks within limited vocal data scenarios. 

To strengthen the model's noise robustness, noisy training data is fed into the model. 
The output from the penultimate layer of the melody extractor is chosen as the melody feature input for the melody encoder. 
The loss function of the melody extractor is defined as follows:

$$

L_{melody} =  \Vert P^{f} - P^{g} \Vert_1 + \Vert E^{f} - E^{g} \Vert_1 +\Vert V^{f} - V^{g} \Vert_1

$$

where $ P^{f}$ and $P^{g}$ are the ground truth and predicted pitch, $ E^{f}$ and $E^{g}$ are the ground truth and predicted energy, $V^{f}$ and $V^{g}$ are the ground truth and predicted voice/unvoice flag (VUV) respectively.

![](16.pdf)

<a id="fig:hub">HuBERT-based melody extractor</a>

### Discriminator Group

Inspired by[^Zhao2023Adversarial], we introduce three different discriminators.

The real/fake discriminator aims to determine whether a sequence of Mel-spectrogram is extracted from the ground-truth audio or reconstructed by the SVC model, enhancing the quality of Mel-spectrogram generation.

Notably, the source audio used for reconstructed Mel comes from the target speaker’s corpora. 
The conversion discriminator is to distinguish whether a sequence of Mel-spectrogram has been converted by the SVC model, whose source audio comes from the external corpora. 
The embedding discriminator is tasked with judging whether a sequence of melody embeddings is extracted from the external corpora, which contain speaker information different from the target speaker. 
The latter two discriminators are able to minimize residual speaker information across different domains.

The final loss functions are given as follows:
\vspace{-1.2pt}

$$
\begin{aligned}

L(D)&=L_{sim}(D)+L_{rf}(D_{r}) \\
L(G)&=L_{sim}(G)+L_{rf}(G)+L_{rec}

\end{aligned}
$$

\vspace{-1.2pt}
where $ L_{sim}$ is the similarity-adversarial loss of conversion discriminator and embedding discriminator, $L_{rf}$ is the loss of real/fake discriminator aimed at enhancing the quality of the generated spectrograms.

### Vocoder

As the pre-trained HiFi-GAN[^Kong2020Hifi-Gan] vocoder is trained on pure speech data, its reconstruction performance on singing waveforms is limited. 
To address this limitation, we integrate the HiFi-GAN vocoder model into the training pipeline after a well-trained SVC model, a process known as the GTA mode[^Shen2018Natural]. 
During the finetuning process, the parameters of the SVC model are frozen, and the vocoder leverages the spectra predicted by the SVC as input to map them to real waveform data.

This strategy aims to mitigate any negative effects arising from differences in the training data domain while also smoothing out deficiencies in the predictions made by the SVC model.

## 3·Experiment

### Experiment Setup

The SVC experiments are conducted on an Opencpop[^Wang2022Opencpop] corpus performed by a professional female singer, totaling 5.2 hours in duration. 
The training data is augmented by varying the speaking rate from 0.9 to 1.5 times to enhance melody diversity. 
ASR system is trained with the same configuration as described in[^Zhao2022Disentangling], and 256-dimensional BNFs are extracted from the ASR model.

Additionally, we select a 20-hour subset of the OpenSinger[^Huang2021Multi-Singer] corpus as our external unannotated dataset for training. 
During the training steps, the weight of $L_{sim}$ and $L_{rf}$ is set to 0 until reaching 50,000 steps.

In the training of the melody extractor, we utilize an almost 30-hour subset of OpenSinger and MUSDB[^Rafii2017MUSDB18-A] for finetuning, resulting in a combined total duration of around 40 hours. 
The rest of the OpenSinger corpus is served as test data. 
Throughout the training process, random background music from MUSDB is overlaid onto clean vocal data to predict pitch and energy sequences. 
The HuBERT structure of the melody extractor is based on a pre-trained model[^Niekerk2022Comparison] using LibriSpeech-960[^Panayotov2015Librispeech] corpus and, after surpassing 5000 steps, the HuBERT structure is frozen and no longer update.

For comparison, we introduce baseline1, also known as **"Origin Pitch\&Energy"**. 
The model of the baseline1 method is the same as the proposed, except that it does not include a HuBERT-based melody extractor; instead, it directly inputs pitch and energy into the melody encoder.

Additionally, to assess the model's robustness to noise, we use the approach of known as **"Separated Origin Pitch\&Energy"**(Baseline2), where songs are separated and then converted.

The baseline2 method utilizes the same model as baseline1, with the distinction that the pitch and energy are extracted from audio processed by a state-of-the-art (SoTA) music source separation model[^Rouard2023Hybrid].

All the audio files are down-sampled to 16 kHz. 
Mel-spectrogram is generated using a short-time Fourier transform (STFT) with a 50 ms frame size, a 10 ms frame hop, and a Hanning window function. 
Baseline and proposed systems are assessed under identical settings unless explicitly stated otherwise. 
A demo page is accessible\footnote{https://wei-chan2022.github.io/RobustSVC/}.

\vspace{-1pt}

### Overall Evaluation

Table~[tab:overall](#tab:overall) presents the SVC capabilities of different models under noisy and clean vocal conditions. 
15 source audios that come from different source speakers and contents are used. 
To simulate noisy audio, random background music from MUSDB is overlaid onto the clean vocal data. 
An overall evaluation from both objective and subjective perspectives is conducted. 
The objective indicator includes F0 RMSE, while the subjective indicators encompass mean opinion scores (MOS) tests to assess naturalness and similarity.

F0 RMSE is used to measure the naturalness of the converted audio[^Huang2023Singing]. 
F0 sequences are normalized using min-max normalization before processing. 
We compute RMSE between the converted and source waveform without BGM since the source waveform is originally performed by real singers with fine-grained melody and high naturalness. 
A lower RMSE value typically signifies a higher level of naturalness and intonation in the converted waveform. 
In the noisy and clean vocal condition, RobustSVC demonstrates excellent performance in terms of F0 RMSE, outperforming baseline1 (Origin Pitch\&Energy) and even surpassing baseline2 (Separated Origin Pitch\&Energy).

In the clean vocal condition, baseline2 exhibits higher F0 RMSE compared to baseline1, implying lower naturalness. 
We speculate that this is due to the separated signal used directly by the baseline2 losing some voice information, which leads to the instability of SVC.

The standard 5-scale MOS tests results are also shown in Table~[tab:overall](#tab:overall). 
RobustSVC get the highest scores and significantly improved similarity and naturalness compared to the other two models in both conditions.

Overall, our method performs exceptionally well in both subjective and objective experiments conducted in noisy and noise-free environments.

<a id="tab:overall">Overall evaluation.

MOS results are reported with 95\% confidence intervals.</a>

<a id="tab:word_styles">Experiment on melody feature input.</a>

### Melody Feature Evaluation

To facilitate a comparative analysis of diverse melody features, we integrate several representations extracted from clean audio into the model framework illustrated in Figure~[fig:baseline](#fig:baseline). 
Specifically, pitch\&energy sequences, HuBERT Raw feature which is from the pre-trained model\footnote{https://github.com/bshall/hubert} published in[^Niekerk2022Comparison], and our novel melody feature are incorporated. 
The evaluation of F0 RMSE served as a metric to gauge the naturalness of the generated audio output.

As delineated in Table~[tab:word_styles](#tab:word_styles), the absence of any melody feature input results in a complete failure of melody modeling by the conversion model. 
Despite some rhythmic information preservation within the self-supervised representations, the efficacy of modeling song melody information using the HuBERT Raw feature is notably constrained. 

Conversely, Our proposed melody feature, while slightly compromised in sound quality due to timbre leakage[^Chen2022Large-Scale], contains richer melody information, performing comparably to pitch\&energy sequences input.

### Noise Inputs Evaluation

\vspace{3pt}

To further assess the model's noise robustness, we evaluate the accuracy of melody modeling under varying signal-to-noise ratios (SNRs). 
F0 RMSE is computed to gauge the naturalness and pitch accuracy of the generated audio output. 
In Table~[tab:noise](#tab:noise), with the increase in noise level, all models show a decline in performance. 
Baseline1 (Origin Pitch\&Energy) is sensitive to noise, with even slight increases in noise level leading to considerable performance deterioration.

However, RobustSVC only experiences a slight decrease in performance as the noise level increases, with this decline being smaller than that observed in the other two models.

At higher noise levels (SNR$\leq$10), the model directly utilizing pitch\&energy as melody feature is notably affected, resulting in substantial melody degradation. 

RobustSVC demonstrates superior noise robustness compared to baseline2 (Separated Origin Pitch\&Energy), underscoring the benefits of integrating robust feature extracted from a HuBERT-based melody extractor.

When SNR exceeds or is equal to 15, where background noise is significantly lower than the human voice, the modeling performance of the four models is relatively similar.

<a id="tab:noise">F0 RMSE in different model noise inputs.

P\&E means Pitch\&Energy.</a>

<a id="tab:ablation">Ablation study.</a>

### Ablation Study

\vspace{3pt}

To understand the contribution of each component to the model, F0 RMSE is computed between the converted and source waveform, and the speaker embeddings of the converted and real target audio are extracted using a pre-trained speaker verification model[^Wang2023Wespeaker] for COS-SIM, where a higher value signifies greater similarity. 
Results in Table~[tab:ablation](#tab:ablation) show that all components have a significant impact on our proposed model.

Model without $L_{sim}$ and CIN module leads to significant performance degradation in similarity and quality because both components could eliminate timbre information of the source speaker leaked from the HuBERT-based melody extractor.

In addition, the removal of the Real/Fake discriminator led to a degradation in speech quality.

\vspace{-4pt}
