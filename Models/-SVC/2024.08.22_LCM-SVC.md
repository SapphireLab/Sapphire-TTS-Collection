# LCM-SVC: Latent Diffusion Model Based Singing Voice Conversion With Inference Acceleration via Latent Consistency Distillation

<details>
<summary>基本信息</summary>

- 标题: "LCM-SVC: Latent Diffusion Model Based Singing Voice Conversion With Inference Acceleration via Latent Consistency Distillation."
- 作者:
  - 01 Shihao Chen
  - 02 Yu Gu
  - 03 Jianwei Cui
  - 04 Jie Zhang
  - 05 Rilin Chen
  - 06 Lirong Dai
- 链接:
  - [ArXiv](https://arxiv.org/abs/2408.12354v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2408.12354v1](_PDF/2024.08.22_2408.12354v1_LCM-SVC__Latent_Diffusion_Model_Based_Singing_Voice_Conversion_With_Inference_Acceleration_via_Latent_Consistency_Distillation.pdf)
  - [Publication] #TODO

</details>

## Abstract

Any-to-any singing voice conversion (SVC) aims to transfer a target singer's timbre to other songs using a short voice sample.
However many diffusion model based any-to-any SVC methods, which have achieved impressive results, usually suffered from low efficiency caused by a mass of inference steps.
In this paper, we propose LCM-SVC, a latent consistency distillation (LCD) based latent  diffusion model (LDM) to  accelerate inference speed. 
We  achieved one-step or few-step inference while maintaining the high performance by distilling a pre-trained LDM based SVC model, which had the advantages of  timbre decoupling and sound quality.
Experimental results show that our proposed method can significantly reduce the inference time and largely preserve the sound quality and timbre similarity comparing with other state-of-the-art SVC models.
Audio samples are available at \url{https://sounddemos.github.io/lcm-svc/}.

## 1·Introduction

Singing voice conversion (SVC) is an emerging audio editing application that aims to transfer the timbre of a target singer to another piece of singing, such as allowing a celebrity to sing a song we have composed ourselves.

Unlike singing voice synthesis
[^Gu2021ByteSing], [^Cui2024Sifisinger], SVC does not require the input of musical scores or lyrics and  it can be accomplished with just singing voice input.

Due to the scarcity of paired data, current SVC models primarily focus on  the task of decoupling information in singing, e.g., content, timbre, pitch.

By training a reconstruction model provided relevant clues, these features can be reassembled into singing and some can be replaced to achieve timbre conversion or pitch correction.  There were many works that have achieved good results on SVC task, such as models based on generative adversarial networks [^Polyak2020Unsupervised], [^Liu2021Fastsvc], [^Zhou2022HiFi-SVC], models based on Variational AutoEncoder (VAE) [^Luo2020Singing], models based on Diffusion [^Liu2021Diff{SVC], [^Lu2024CoMoSVC] and end-to-end model So-VITS-SVC\footnote{\url{https://github.com/PlayVoice/so-vits-svc-5.0/tree/bigvgan-mix-v2}}.

Among these systems, diffusion models exhibit a superiority in sound quality and timbre similarity, particularly using latent diffusion model (LDM) [^Chen2024Ldm-SVC].%等Interspeech, you can arXiv your interspeech paper and then cite。

The major drawback of the diffusion model based methods is the long required inference time.

The inference iterations of diffusion models can reach up to 100 or even more than 1000, which is impractical for real-world applications.

Some efforts were thus made to accelerate the inference, such as Denoising Diffusion Implicit Models (DDIM) [^Song2020Denoising], Diffusion Probabilistic Models Solver (DPM-Solver) [^Lu2022DPM-Solver], etc.

Recently, the emergence of consistency model (CM) provides a new proposal for accelerating the inference of diffusion, where the goal is to ensure that the output at each step of the diffusion's denoising process remains consistent [^Song2023Consistency].

By learning consistency mappings that maintain point consistency on Order Linear Equations (ODE)-trajectory, CM achieves one-step generation and thus avoids computationally intensive iterations.

Further, the advent of latent consistency model (LCM) [^Luo2023Latent] suggests that applying consistency distillation in the latent space can yield superior results, which can also improve the efficiency of diffusion  models, meanwhile maintaining high-quality outputs.

In this paper, we propose an SVC method (abbreviated by LCM-SVC) using latent consistency distillation (LCD) strategy on the basis of an LDM SVC model.

This method efficiently converts a pre-trained LDM into an LCM by solving an augmented Probability Flow ODE (PF-ODE).

Initially, we train a So-VITS-SVC model as the VAE structure to extract hidden latent variables and then a teacher model based on LDM following  a classifier guidance scheme.

We then utilize the LCD method to distill the model, facilitating few-step or even one-step inference while preserving audio quality similarly to the teacher model.

We further apply a skipping-step technique to accelerate the convergence of model training.

Experimental results indicate that LCM-SVC can only incur a slight loss in audio quality using one-step inference iteration.

More importantly, in case of using 2-step or 4-step inference, the obtained audio quality is comparable to that of the teacher model and the inference time is significantly reduced, which satisfies the efficiency request of practical applications. 
The rest of this paper is organized as follows.

Section 2 outlines the proposed LCM-SVC method.

Experiments are presented in Section 3.

Finally, Section 4 concludes this work.

## 2·Method

![](fig2.pdf)

<a id="ldm-svc">Left: Pre-training procedure of So-VITS-SVC; Right: Training procedure of LCM-SVC Teacher.</a>

### Latent Diffusion Model for SVC

The VAE model is firstly pre-trained to extract hidden latent vectors using So-VITS-SVC, which is a variant of VITS [^Kim2021Conditional], consisting of three key components: posterior encoder, prior encoder and decoder as depicted in Figure 1. 
The posterior encoder composed of non-causal WaveNet [^Oord2016Wavenet] residual blocks models the posterior distribution $p(z|y,e)$ of the hidden representation $z$ from the linear spectrograms  $y$ generated from the  singing waveforms,  where singer embedding $e$ is extracted by an additional speaker verification model.

The prior encoder, constructed with a multi-layer Transformer [^Vaswani2017Attention], estimates the prior distribution $p(z|x,f_0,e)$, where $x$ and $f_0$ represent the phonetic posteriorgrams (PPG) and fundamental frequency (F0) respectively.

A normalizing flow [^Papamakarios2021Normalizing] is utilized to transform the distributions of the prior and posterior encoders.

The BigVGAN-based [^Lee2022Bigvgan] decoder generates the singing waveform from the latent representation $z$, using a neural source filter (NSF) [^Wang2019Neural] scheme with F0s to enhance pitch accuracy.

After training, the posterior encoder and decoder are used to extract hidden latent variables for LDM and synthesize waveforms from the hidden latent vectors predicted by LDM.

The LDM is adopted as the probabilistic model that fit the hidden distribution $p(z_{0})$ by denoising in data latent space from the pre-trained VAE model.  Denoising diffusion probabilistic model (DDPM) [^Ho2020Denoising] is used, which consists of forward and denoising processes.

During the LDM training, the singer's timbre $e$ and the linear spectrogram of the singing voice $y$ are used as inputs to the posterior encoder $\mathcal{E}(\cdot)$, yielding the latent variable $z_0=\mathcal{E}(y,e)$. 
In the forward process, the original data distribution is transformed into a standard Gaussian distribution by gradually adding noise according to a fixed schedule $\beta_1,\dots,\beta_T$, where $T$ represents the total time steps.

This process includes a transition from $z_{0}$ to $z_t$ following a Markov chain, where the conditional distribution $q(z_t|z_{t-1})$ is defined as a Gaussian distribution, i.e., $q(z_t|z_{t-1}) = \mathcal{N}(z_t;\sqrt{1-\beta_t}z_{t-1}, {\beta_t}\mathbf{I})$.

The reverse process, denoted by $\theta$, functions as a denoising mechanism to suppress noise and recover the original data.

The denoising distribution, $p_{\theta}(z_{t-1}|z_t)$, is a conditional Gaussian distribution.

As such, we iteratively sample target data $z_0$ from Gaussian noise for $t = T, T-1, \ldots, 1$, with $z_{t-1}$ sampled based on $p_{\theta}(z_{t-1}|z_t)$.

The LDM uses $z_t$ and $t$ as inputs, together with conditional inputs like the singer's timbre $e$, fundamental frequency $f_0$, and PPG $x$.

We also use a singer guidance method, specifically Speaker Condition Layer Normalization (SCLN) [^Wu2021Cross-Speaker] for PPG feature normalization, and a classifier-free guidance method [^Ho2022Classifier-Free] for model training to better decouple timbre information.

The step-wise output $z_{t-1}$ of denoising  is calculated as:

$$z_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( z_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_{\theta}(z_t, t, x, f_0, e) \right) + \sigma_t \epsilon,
$$

where $\epsilon \sim \mathcal{N}(0, I)$, $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$.

For the detailed derivation of (1), please refer to [^Ho2020Denoising].

The configuration of the considered diffusion model keeps consistent with that of DiffSVC.

The training loss of the LDM $\epsilon_\theta$ is defined as the mean squared error (MSE) in the noise space: 

$$
\mathcal{L}_{\rm LDM}=||\epsilon-\epsilon_\theta(z_t, t, x, f_0, e)||_2^2.
$$

During inference, we input source PPG $x_{src}$, replace the singer timbre with $e_{tar}$ and adjust $f_0$ with the target attribute.

We sample random Gaussian white noise to obtain $\widetilde{z}_0$ using the denoising process.

Then, we input $\widetilde{z}_0$, $e_{tar}$, and $f_0$ into the pre-trained decoder $\mathcal{D}(\cdot)$ to generate the audio waveform.

### Latent Consistency Distillation

![](cm2.pdf)

<a id="cm">The Training and Inference procedure of LCD.</a>

Following [^Luo2023Latent], we use latent consistency distillation (LCD) method to accelerate the reverse process of LDM.

The Consistency Model (CM)[^Song2023Consistency] facilitates one-step or few-step generation.

The fundamental concept of the CM is to learn a function capable of mapping any points on a trajectory of the Probability Flow ODE (PF-ODE) back to the trajectory's origin, which is essentially the solution of the PF-ODE.

The general process of LCD is shown in Figure [cm](#cm).

As the purpose of LCD is to define a function such that the target is the same for each time step $t$, in order to maintain the accuracy of the prediction, the function $**F**_{\theta}$ is defined as:

$$
\resizebox{\linewidth}{!}{$**F**_{\theta}(z_t, x,f_0,e, t) = c_{\text{skip}}(t)z_t + c_{\text{out}}(t)\left(\frac{z_t - \sigma_{t}\epsilon_{\theta}(z_t, x,f_0,e, t)}{\alpha_t}\right)$}, \notag
$$

where $c_{\text{skip}}(t)$ and $c_{\text{out}}(t)$ are differentiable functions with $c_{\text{skip}}(0)=1$ and $c_{\text{out}}(0)=0$.

In order to facilitate consistency distillation, we apply a skipping-step method to expedite the convergence process.

Specifically, we use an ODE solver, denoted as $\Psi(\cdot)$, to predict $\hat{z}_{t}^{\Psi}$, which is based on $z_{t+k}$ with $k$ being the interval.

$$
\resizebox{\linewidth}{!}{$\hat{z}_{t}^{\Psi}= (1+\omega)\Psi(z_{t+k},t+k,t,x,f_0,e)-\omega\Psi(z_{t+k},t+k,t,x,\varnothing)$}, \notag
$$

where $\omega$ is the guidance weight and $\Psi$ the DDIM solver.

To enforce the self-consistency property, a target model $\theta^-$ is maintained.

This model is updated with the exponential moving average (EMA) of the parameter $\theta$ that we aim to learn.

This can be mathematically represented as:

$$
\theta^{-} \leftarrow\mu\theta^{-}+(1-\mu)\theta.
$$

To maintain the consistency of the model's output, we  define a loss function to constrain the consistency of the outputs at different steps, given by:

$$
\resizebox{\linewidth}{!}{$\mathcal{L}(\theta,\theta^{-};\Psi)= d(\mathbf{F}_\theta(z_{t+k},x,f_0,e,t+k),\mathbf{F}_{\theta^-}(\hat{z}_{t}^{\Psi},x,f_0,e,t))$}, \notag
$$

where $d(\cdot,\cdot)$ measures the distance, e.g., the often-used l2 norm.

During back propagation, only $\theta$ is updated, and note that $\theta^- $is updated through EMA.

The ODE solver $\Psi$ uses the frozen parameters obtained by the pre-trained LDM.

The LCD training procedure is summarized in Algorithm 1.

After the completion of training, the sample quality can be significantly improved via a sequence of denoising and noise injection steps.

In the $n$-th iteration, we initiate a noise-injecting forward process on the previously predicted sample $\widetilde{z}_0$ as $\widetilde{z}_{\tau_{n}}\sim\mathcal{N}(\alpha_{\tau_{n}}\widetilde{z}_0,\sigma_{\tau_{n}}I)$ with
$\tau_{n}$ denoting a decreasing sequence of time steps.

The key steps of the inference procedure are outlined in Algorithm 2.

For more detailed information, please refer to [^Luo2023Latent].

\begin{algorithm}[t]
\caption{Consistency Distillation of LCM-SVC.}

\begin{algorithmic}[1]
\label{alg:training}
\REQUIRE Initial model parameter $\theta$; training set $D_{train} = \{(x,{f_0},e,y)\}_{m=1}^M$; pretrained So-VITS-SVC posterior encoder $\mathcal{E(\cdot)}$; EMA rate $\mu$; noise schedule $\alpha_{t},\sigma_{t}$; guidance weight $\omega$; ODE solver $\Psi(\cdot)$; skipping interval $k$; distance metric $d(\cdot,\cdot)$; learning rate $\eta$.
\STATE $\theta^{-}\leftarrow\theta$
\REPEAT
\STATE Sample $(x, f_0, e, y)$ from $D_{train}$;
\STATE $z_0\leftarrow\mathcal{E}(y,e)$;
\STATE Sample $t\sim$ Uniform$(\{1, \cdots, T-k\})$;
\STATE Sample $z_{t+k}\sim\mathcal{N}(\alpha_{t+k}z_0;\sigma_{t+k}I)$;
\STATE $\hat{z}_{t}^{\Psi}\leftarrow  (1+\omega)\Psi(z_{t+k},t+k,t,x,f_0,e)$\\\qquad\qquad$-\omega\Psi(z_{t+k},t+k,t,x,\varnothing)$
\STATE $\mathcal{L}(\theta,\theta^{-};\Psi)\leftarrow d(\mathbf{F}_\theta(z_{t+k},x,f_0,e,t+k),$\\\qquad\qquad\qquad~~~~~~$\mathbf{F}_{\theta^-}(\hat{z}_{t}^{\Psi},x,f_0,e,t))$;
\STATE $\theta\leftarrow \theta - \eta\nabla_\theta(\theta,\theta^{-})$;
\STATE $\theta^{-} \leftarrow$ stopgrad$(\mu\theta^{-}+(1-\mu)\theta)$;
\UNTIL convergence.

\end{algorithmic}

\end{algorithm}

\begin{algorithm}[h]
\caption{Multi-step Inference of LCM-SVC.}

\begin{algorithmic}[1]
\label{alg:infer}
\REQUIRE Latent Consistency Model $\mathbf{F}_\theta(\cdot)$; source singer 
PPG $x_{src}$; target singer embedding $e_{tar}$; modified $f_0$; pretrained So-VITS-SVC decoder $\mathcal{D}(\cdot)$; noise schedule $\alpha_{t},\sigma_{t}$; sequence of timesteps $\tau_{1}>\tau_{2}>...>\tau_{N-1}$.\STATE Sample $\widetilde{z}_T\sim\mathcal{N}(0,I)$;
\STATE $\widetilde{z}_0\leftarrow \mathbf{F}_\theta(\widetilde{z}_T,x_{src},f_0,e_{tar},T)$;
\FOR{$n=1,2,...,N-1$}
\STATE $\widetilde{z}_{\tau_{n}}\sim\mathcal{N}(\alpha_{\tau_{n}}\widetilde{z}_0,\sigma_{\tau_{n}}\mathbf{I})$;
\STATE $\widetilde{z}_0\leftarrow \mathbf{F}_\theta(\widetilde{z}_{\tau_{n}},x_{src},f_0,e_{tar},\tau_{n})$;
\ENDFOR
\RETURN $\mathcal{D}(\widetilde{z}_0,f_0,e_{tar})$;
\end{algorithmic}

\end{algorithm}

## 3·Experiment

### Experimental Setup

<a id="result_eer">Subjective indicators (SMOS, NMOS) and objective indicators (SSIM, FPC) of comparison methods.</a>

<a id="lcm">Subjective evaluation results of LCM-SVC-$t$ inference using $t$ iterations.</a>

\vspace{-0.2cm}

For evaluation, we utilize the OpenSinger dataset[^Huang2021Multi-Singer], with a comprehensive collection of Chinese singers.

The dataset encompasses 74 singers, including 27 males and 47 females, and comprises a total of 52 hours of recordings.  To evaluate the conversion performances of different systems, two scenarios depending on if the target singers are included in the training dataset, i.e., seen and unseen singers are conducted.

In the test set, 4 male and 4 female singers are randomly selected as unseen singers.

For the seen singers' case, 8 sentences from each of the remaining 66 singers are chosen for testing, while the rest of the songs are used for training.

For comparative analysis, we trained So-VITS-SVC, DiffSVC, CoMoSVC and LCM-SVC teacher model (LCM-SVC-T).

To ensure a fair comparison, we extract the PPG for each model using Whisper[^Radford2023Robust], and the F0 using CREPE[^Kim2018Crepe].

Singer embeddings are obtained from the pre-trained model of CAM++[^Wang2023Cam++], an open-source project for speaker verification\footnote{\url{https://modelscope.cn/models/iic/speech_campplus_sv_zh-cn_16k-common/summary}}.

In experiments, all audio files are resampled to 32kHz.

Each model is trained for 400k steps  with a total batch size of 32.

For the F0 embedding in the diffusion condition, we first quantize the Log-F0 features into 256 bins, followed by a pass through a melody embedding lookup table.

During inference, we adjust the F0 of the original singer by calculating the mean F0 values of both the target and source singers within the voice segment, denoted as $\mathrm{mean}({f_0}^{src}_v)$ and $\mathrm{mean}({f_0}^{tar}_v)$, respectively.

We then scale ${f_0}^{src}$ by the ratio of these mean values to obtain the modified ${f_0}$ as ${f_0}^{src} \times \frac{\mathrm{mean}({f_0}^{tar}_v)}{\mathrm{mean}({f_0}^{src}_v)}$.

For the singer guidance method, we set the distortion probability $p_{\text{uncond}}$ to 0.1 during training, and the guidance weight $w$ to 0.3 during inference, in accordance with[^Ho2022Classifier-Free].

The total number of diffusion steps is set to 100 (i.e., $T = 100$).

The noise schedule $\beta$ is linearly distributed, starting from a minimal value of 1e-4 and reaching up to $0.06$, following the configuration used in the DiffSVC model.

For CoMoSVC, we use the same configuration as [^Lu2024CoMoSVC], that is, the number of steps for training the teacher model is 50.

As for the consistency distillation part, both LCM-SVC and CoMoSVC adopt the same hyper-parameters, where $\mu$ = 0.95 and the learning rate is set to 5e-5.

The interval $k$ of DDIM in LCD is set to 10.

For the CoMoSVC and DiffSVC models, it is necessary to train a separate vocoder that accepts mel-spectrogram as inputs.

Therefore, we exploit the same structure as the BigVGAN in So-VITS-SVC to train a vocoder with NSF with the number of iterations set to 400k.

### Evaluation Metrics

To construct the conversion trials, we separately establish pairs of conversions for both seen and unseen scenarios.

A cross-validation strategy is employed to conduct audio clips where each singer provides the vocal content information as the source and other singers provide timbre information as targets.

Consequently, 528 and 448 converted song clips are formed as the test trials for the seen and unseen cases respectively.

We evaluate these converted audios using objective metrics i.e.

Singer Similarity (SSIM), F0 Pearson correlation (FPC), Mel Cepstral Distortion (MCD) and  Real-Time Factor (RTF).

Specifically, we use the time ratio of the time taken to convert singing voices to the duration of audio as a representation of RTF.

All the inference processes are conducted on one NVIDIA A100 GPU.

For subjective evaluation, we randomly pick 20 audio samples from each model in both seen and unseen situations.

We employ a 5-point Mean Opinion Score (MOS) (1-bad, 2-poor, 3-fair, 4-good, 5-excellent) test and invite 20 volunteers to participate in the listening test, where both  Naturalness MOS (NMOS) and Similarity MOS (SMOS) are evaluated.

<a id="obj">Objective metrics of different methods.

The suffix 'S' represents the Seen Singer scenario, while the suffix 'U' represents the Unseen Singer scenario.</a>

### Experimental Results

Table 1 shows the different results for seen and unseen scenarios.

LCM-SVC-T significantly outperforms other models in terms of both NMOS and SMOS.

After LCD, the performance of LCM-SVC has a slight loss, but still surpasses other baseline models in terms of timbre similarity, and the NMOS maintains  a  comparable level to the other models.

Table 2 shows the performance of LCM-SVC under multi-step conditions.

It can be observed that in the seen case, an increase in the number of iterations significantly improves the sound quality, but has little effect on timbre.

Under the unseen condition, an increase in the number of iterations greatly enhances both sound quality and timbre.

Overall, these indicate that under the seen condition, the use of LCD tends to result in a reduction in sound quality, while in the unseen case it leads to a decrease in the timbre similarity.

Table 3 shows the SSIM, MCD and RTF of comparison methods under the unseen condition.

It can be seen that the SSIM of the LCM-SVC methods is significantly higher than that of other models, but the MCD is at a higher level.

This may be because both DiffSVC and CoMoSVC predict the Mel-spectrogram as the target, while So-VITS-SVC and LCM-SVC predict the latent representation.

However, they are expected to have a better listening experience.

As for the crucial RTF metric, the RTF of the diffusion-based methods DiffSVC and LCM-SVC-T is at a higher level, which is undoubtedly disastrous for real-time applications.

After LCD, the RTF of LCM-SVC-1 drops to 0.004, requiring only one-step inference, but its sound quality is consequently compromised.

LCM-SVC-2 and LCM-SVC-4, on the other hand, can achieve a quite good sound quality and timbre performance at an acceptable RTF.

This means that unless extreme RTF is required, using 2-step or 4-step inference can be a better choice in practice.

## 4·Conclusion

In this paper, we proposed the LCM-SVC, an any-to-any singing voice conversion method using the latent consistency model.

By using the LCD method to distill a pre-trained LDM, we can reduce inference steps to one or much less, significantly decreasing the inference time.

Tests on the OpenSinger dataset show that 1-step inference with LCM-SVC results in a performance drop, while the 2-step or 4-step inference yields performance comparable to the original LDM.

It would be better to use a multi-step inference in situations where extreme low latency isn't necessary to maintain a high performance in real-time SVC applications.

\newpage
\bibliographystyle{IEEEtran}

\bibliography{mybib}

\end{document}
