# LCM-SVC: Latent Diffusion Model Based Singing Voice Conversion With Inference Acceleration via Latent Consistency Distillation

<details>
<summary>基本信息</summary>

- 标题: "LCM-SVC: Latent Diffusion Model Based Singing Voice Conversion With Inference Acceleration via Latent Consistency Distillation."
- 作者:
  - 01 Shihao Chen
  - 02 Yu Gu
  - 03 Jianwei Cui
  - 04 Jie Zhang
  - 05 Rilin Chen
  - 06 Lirong Dai
- 链接:
  - [ArXiv](https://arxiv.org/abs/2408.12354v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2408.12354v1](_PDF/2024.08.22_2408.12354v1_LCM-SVC__Latent_Diffusion_Model_Based_Singing_Voice_Conversion_With_Inference_Acceleration_via_Latent_Consistency_Distillation.pdf)
  - [Publication] #TODO

</details>

## Abstract

Any-to-any singing voice conversion (SVC) aims to transfer a target singer's timbre to other songs using a short voice sample.
However many diffusion model based any-to-any SVC methods, which have achieved impressive results, usually suffered from low efficiency caused by a mass of inference steps.
In this paper, we propose LCM-SVC, a latent consistency distillation (LCD) based latent  diffusion model (LDM) to  accelerate inference speed. 
We  achieved one-step or few-step inference while maintaining the high performance by distilling a pre-trained LDM based SVC model, which had the advantages of  timbre decoupling and sound quality.
Experimental results show that our proposed method can significantly reduce the inference time and largely preserve the sound quality and timbre similarity comparing with other state-of-the-art SVC models.
Audio samples are available at \url{https://sounddemos.github.io/lcm-svc/}.

## 1·Introduction

Singing voice conversion (SVC) is an emerging audio editing application that aims to transfer the timbre of a target singer to another piece of singing, such as allowing a celebrity to sing a song we have composed ourselves.

Unlike singing voice synthesis
[^Gu2021ByteSing], [^Cui2024Sifisinger], SVC does not require the input of musical scores or lyrics and  it can be accomplished with just singing voice input.

Due to the scarcity of paired data, current SVC models primarily focus on  the task of decoupling information in singing, e.g., content, timbre, pitch.

By training a reconstruction model provided relevant clues, these features can be reassembled into singing and some can be replaced to achieve timbre conversion or pitch correction.  There were many works that have achieved good results on SVC task, such as models based on generative adversarial networks [^Polyak2020Unsupervised], [^Liu2021Fastsvc], [^Zhou2022HiFi-SVC], models based on Variational AutoEncoder (VAE) [^Luo2020Singing], models based on Diffusion [^Liu2021Diff{SVC], [^Lu2024CoMoSVC] and end-to-end model So-VITS-SVC\footnote{\url{https://github.com/PlayVoice/so-vits-svc-5.0/tree/bigvgan-mix-v2}}.

Among these systems, diffusion models exhibit a superiority in sound quality and timbre similarity, particularly using latent diffusion model (LDM) [^Chen2024Ldm-SVC].%等Interspeech, you can arXiv your interspeech paper and then cite。

The major drawback of the diffusion model based methods is the long required inference time.

The inference iterations of diffusion models can reach up to 100 or even more than 1000, which is impractical for real-world applications.

Some efforts were thus made to accelerate the inference, such as Denoising Diffusion Implicit Models (DDIM) [^Song2020Denoising], Diffusion Probabilistic Models Solver (DPM-Solver) [^Lu2022DPM-Solver], etc.

Recently, the emergence of consistency model (CM) provides a new proposal for accelerating the inference of diffusion, where the goal is to ensure that the output at each step of the diffusion's denoising process remains consistent [^Song2023Consistency].

By learning consistency mappings that maintain point consistency on Order Linear Equations (ODE)-trajectory, CM achieves one-step generation and thus avoids computationally intensive iterations.

Further, the advent of latent consistency model (LCM) [^Luo2023Latent] suggests that applying consistency distillation in the latent space can yield superior results, which can also improve the efficiency of diffusion  models, meanwhile maintaining high-quality outputs.

In this paper, we propose an SVC method (abbreviated by LCM-SVC) using latent consistency distillation (LCD) strategy on the basis of an LDM SVC model.

This method efficiently converts a pre-trained LDM into an LCM by solving an augmented Probability Flow ODE (PF-ODE).

Initially, we train a So-VITS-SVC model as the VAE structure to extract hidden latent variables and then a teacher model based on LDM following  a classifier guidance scheme.

We then utilize the LCD method to distill the model, facilitating few-step or even one-step inference while preserving audio quality similarly to the teacher model.

We further apply a skipping-step technique to accelerate the convergence of model training.

Experimental results indicate that LCM-SVC can only incur a slight loss in audio quality using one-step inference iteration.

More importantly, in case of using 2-step or 4-step inference, the obtained audio quality is comparable to that of the teacher model and the inference time is significantly reduced, which satisfies the efficiency request of practical applications. 
The rest of this paper is organized as follows.

Section 2 outlines the proposed LCM-SVC method.

Experiments are presented in Section 3.

Finally, Section 4 concludes this work.

## 2·Method

![](fig2.pdf)

<a id="ldm-svc">Left: Pre-training procedure of So-VITS-SVC; Right: Training procedure of LCM-SVC Teacher.</a>

### Latent Diffusion Model for SVC

The VAE model is firstly pre-trained to extract hidden latent vectors using So-VITS-SVC, which is a variant of VITS [^Kim2021Conditional], consisting of three key components: posterior encoder, prior encoder and decoder as depicted in Figure 1. 
The posterior encoder composed of non-causal WaveNet [^Oord2016Wavenet] residual blocks models the posterior distribution $p(z|y,e)$ of the hidden representation $z$ from the linear spectrograms  $y$ generated from the  singing waveforms,  where singer embedding $e$ is extracted by an additional speaker verification model.

The prior encoder, constructed with a multi-layer Transformer [^Vaswani2017Attention], estimates the prior distribution $p(z|x,f_0,e)$, where $x$ and $f_0$ represent the phonetic posteriorgrams (PPG) and fundamental frequency (F0) respectively.

A normalizing flow [^Papamakarios2021Normalizing] is utilized to transform the distributions of the prior and posterior encoders.

The BigVGAN-based [^Lee2022Bigvgan] decoder generates the singing waveform from the latent representation $z$, using a neural source filter (NSF) [^Wang2019Neural] scheme with F0s to enhance pitch accuracy.

After training, the posterior encoder and decoder are used to extract hidden latent variables for LDM and synthesize waveforms from the hidden latent vectors predicted by LDM.

The LDM is adopted as the probabilistic model that fit the hidden distribution $p(z_{0})$ by denoising in data latent space from the pre-trained VAE model.  Denoising diffusion probabilistic model (DDPM) [^Ho2020Denoising] is used, which consists of forward and denoising processes.

During the LDM training, the singer's timbre $e$ and the linear spectrogram of the singing voice $y$ are used as inputs to the posterior encoder $\mathcal{E}(\cdot)$, yielding the latent variable $z_0=\mathcal{E}(y,e)$. 
In the forward process, the original data distribution is transformed into a standard Gaussian distribution by gradually adding noise according to a fixed schedule $\beta_1,\dots,\beta_T$, where $T$ represents the total time steps.

This process includes a transition from $z_{0}$ to $z_t$ following a Markov chain, where the conditional distribution $q(z_t|z_{t-1})$ is defined as a Gaussian distribution, i.e., $q(z_t|z_{t-1}) = \mathcal{N}(z_t;\sqrt{1-\beta_t}z_{t-1}, {\beta_t}\mathbf{I})$.

The reverse process, denoted by $\theta$, functions as a denoising mechanism to suppress noise and recover the original data.

The denoising distribution, $p_{\theta}(z_{t-1}|z_t)$, is a conditional Gaussian distribution.

As such, we iteratively sample target data $z_0$ from Gaussian noise for $t = T, T-1, \ldots, 1$, with $z_{t-1}$ sampled based on $p_{\theta}(z_{t-1}|z_t)$.

The LDM uses $z_t$ and $t$ as inputs, together with conditional inputs like the singer's timbre $e$, fundamental frequency $f_0$, and PPG $x$.

We also use a singer guidance method, specifically Speaker Condition Layer Normalization (SCLN) [^Wu2021Cross-Speaker] for PPG feature normalization, and a classifier-free guidance method [^Ho2022Classifier-Free] for model training to better decouple timbre information.

The step-wise output $z_{t-1}$ of denoising  is calculated as:

$$z_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( z_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_{\theta}(z_t, t, x, f_0, e) \right) + \sigma_t \epsilon,
$$

where $\epsilon \sim \mathcal{N}(0, I)$, $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$.

For the detailed derivation of (1), please refer to [^Ho2020Denoising].

The configuration of the considered diffusion model keeps consistent with that of DiffSVC.

The training loss of the LDM $\epsilon_\theta$ is defined as the mean squared error (MSE) in the noise space: 

$$
\mathcal{L}_{\rm LDM}=||\epsilon-\epsilon_\theta(z_t, t, x, f_0, e)||_2^2.
$$

During inference, we input source PPG $x_{src}$, replace the singer timbre with $e_{tar}$ and adjust $f_0$ with the target attribute.

We sample random Gaussian white noise to obtain $\widetilde{z}_0$ using the denoising process.

Then, we input $\widetilde{z}_0$, $e_{tar}$, and $f_0$ into the pre-trained decoder $\mathcal{D}(\cdot)$ to generate the audio waveform.

### Latent Consistency Distillation

![](cm2.pdf)

<a id="cm">The Training and Inference procedure of LCD.</a>

Following [^Luo2023Latent], we use latent consistency distillation (LCD) method to accelerate the reverse process of LDM.

The Consistency Model (CM)[^Song2023Consistency] facilitates one-step or few-step generation.

The fundamental concept of the CM is to learn a function capable of mapping any points on a trajectory of the Probability Flow ODE (PF-ODE) back to the trajectory's origin, which is essentially the solution of the PF-ODE.

The general process of LCD is shown in Figure [cm](#cm).

As the purpose of LCD is to define a function such that the target is the same for each time step $t$, in order to maintain the accuracy of the prediction, the function $**F**_{\theta}$ is defined as:

$$
\resizebox{\linewidth}{!}{$**F**_{\theta}(z_t, x,f_0,e, t) = c_{\text{skip}}(t)z_t + c_{\text{out}}(t)\left(\frac{z_t - \sigma_{t}\epsilon_{\theta}(z_t, x,f_0,e, t)}{\alpha_t}\right)$}, \notag
$$

where $c_{\text{skip}}(t)$ and $c_{\text{out}}(t)$ are differentiable functions with $c_{\text{skip}}(0)=1$ and $c_{\text{out}}(0)=0$.

In order to facilitate consistency distillation, we apply a skipping-step method to expedite the convergence process.

Specifically, we use an ODE solver, denoted as $\Psi(\cdot)$, to predict $\hat{z}_{t}^{\Psi}$, which is based on $z_{t+k}$ with $k$ being the interval.

$$
\resizebox{\linewidth}{!}{$\hat{z}_{t}^{\Psi}= (1+\omega)\Psi(z_{t+k},t+k,t,x,f_0,e)-\omega\Psi(z_{t+k},t+k,t,x,\varnothing)$}, \notag
$$

where $\omega$ is the guidance weight and $\Psi$ the DDIM solver.

To enforce the self-consistency property, a target model $\theta^-$ is maintained.

This model is updated with the exponential moving average (EMA) of the parameter $\theta$ that we aim to learn.

This can be mathematically represented as:

$$
\theta^{-} \leftarrow\mu\theta^{-}+(1-\mu)\theta.
$$

To maintain the consistency of the model's output, we  define a loss function to constrain the consistency of the outputs at different steps, given by:

$$
\resizebox{\linewidth}{!}{$\mathcal{L}(\theta,\theta^{-};\Psi)= d(\mathbf{F}_\theta(z_{t+k},x,f_0,e,t+k),\mathbf{F}_{\theta^-}(\hat{z}_{t}^{\Psi},x,f_0,e,t))$}, \notag
$$

where $d(\cdot,\cdot)$ measures the distance, e.g., the often-used l2 norm.

During back propagation, only $\theta$ is updated, and note that $\theta^- $is updated through EMA.

The ODE solver $\Psi$ uses the frozen parameters obtained by the pre-trained LDM.

The LCD training procedure is summarized in Algorithm 1.

After the completion of training, the sample quality can be significantly improved via a sequence of denoising and noise injection steps.

In the $n$-th iteration, we initiate a noise-injecting forward process on the previously predicted sample $\widetilde{z}_0$ as $\widetilde{z}_{\tau_{n}}\sim\mathcal{N}(\alpha_{\tau_{n}}\widetilde{z}_0,\sigma_{\tau_{n}}I)$ with
$\tau_{n}$ denoting a decreasing sequence of time steps.

The key steps of the inference procedure are outlined in Algorithm 2.

For more detailed information, please refer to [^Luo2023Latent].

\begin{algorithm}[t]
\caption{Consistency Distillation of LCM-SVC.}

\begin{algorithmic}[1]
\label{alg:training}
\REQUIRE Initial model parameter $\theta$; training set $D_{train} = \{(x,{f_0},e,y)\}_{m=1}^M$; pretrained So-VITS-SVC posterior encoder $\mathcal{E(\cdot)}$; EMA rate $\mu$; noise schedule $\alpha_{t},\sigma_{t}$; guidance weight $\omega$; ODE solver $\Psi(\cdot)$; skipping interval $k$; distance metric $d(\cdot,\cdot)$; learning rate $\eta$.
\STATE $\theta^{-}\leftarrow\theta$
\REPEAT
\STATE Sample $(x, f_0, e, y)$ from $D_{train}$;
\STATE $z_0\leftarrow\mathcal{E}(y,e)$;
\STATE Sample $t\sim$ Uniform$(\{1, \cdots, T-k\})$;
\STATE Sample $z_{t+k}\sim\mathcal{N}(\alpha_{t+k}z_0;\sigma_{t+k}I)$;
\STATE $\hat{z}_{t}^{\Psi}\leftarrow  (1+\omega)\Psi(z_{t+k},t+k,t,x,f_0,e)$\\\qquad\qquad$-\omega\Psi(z_{t+k},t+k,t,x,\varnothing)$
\STATE $\mathcal{L}(\theta,\theta^{-};\Psi)\leftarrow d(\mathbf{F}_\theta(z_{t+k},x,f_0,e,t+k),$\\\qquad\qquad\qquad~~~~~~$\mathbf{F}_{\theta^-}(\hat{z}_{t}^{\Psi},x,f_0,e,t))$;
\STATE $\theta\leftarrow \theta - \eta\nabla_\theta(\theta,\theta^{-})$;
\STATE $\theta^{-} \leftarrow$ stopgrad$(\mu\theta^{-}+(1-\mu)\theta)$;
\UNTIL convergence.

\end{algorithmic}

\end{algorithm}

\begin{algorithm}[h]
\caption{Multi-step Inference of LCM-SVC.}

\begin{algorithmic}[1]
\label{alg:infer}
\REQUIRE Latent Consistency Model $\mathbf{F}_\theta(\cdot)$; source singer 
PPG $x_{src}$; target singer embedding $e_{tar}$; modified $f_0$; pretrained So-VITS-SVC decoder $\mathcal{D}(\cdot)$; noise schedule $\alpha_{t},\sigma_{t}$; sequence of timesteps $\tau_{1}>\tau_{2}>...>\tau_{N-1}$.\STATE Sample $\widetilde{z}_T\sim\mathcal{N}(0,I)$;
\STATE $\widetilde{z}_0\leftarrow \mathbf{F}_\theta(\widetilde{z}_T,x_{src},f_0,e_{tar},T)$;
\FOR{$n=1,2,...,N-1$}
\STATE $\widetilde{z}_{\tau_{n}}\sim\mathcal{N}(\alpha_{\tau_{n}}\widetilde{z}_0,\sigma_{\tau_{n}}\mathbf{I})$;
\STATE $\widetilde{z}_0\leftarrow \mathbf{F}_\theta(\widetilde{z}_{\tau_{n}},x_{src},f_0,e_{tar},\tau_{n})$;
\ENDFOR
\RETURN $\mathcal{D}(\widetilde{z}_0,f_0,e_{tar})$;
\end{algorithmic}

\end{algorithm}
