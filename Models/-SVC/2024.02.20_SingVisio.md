# SingVisio
# SingVisio: Visual Analytics of Diffusion Model for Singing Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "SingVisio: Visual Analytics of Diffusion Model for Singing Voice Conversion."
- 作者:
  - 01 Liumeng Xue
  - 02 Chaoren Wang
  - 03 Mingxuan Wang
  - 04 Xueyao Zhang
  - 05 Jun Han
  - 06 Zhizheng Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2402.12660v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2402.12660v1](_PDF/2024.02.20_2402.12660v1_SingVisio__Visual_Analytics_of_Diffusion_Model_for_Singing_Voice_Conversion.pdf)
  - [ArXiv:2402.12660v2](_PDF/2024.02.20_2402.12660v2_SingVisio__Visual_Analytics_of_Diffusion_Model_for_Singing_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

In this study, we present SingVisio, an interactive visual analysis system that aims to explain the diffusion model used in singing voice conversion.
SingVisio provides a visual display of the generation process in diffusion models, showcasing the step-by-step denoising of the noisy spectrum and its transformation into a clean spectrum that captures the desired singer's timbre.
The system also facilitates side-by-side comparisons of different conditions, such as source content, melody, and target timbre, highlighting the impact of these conditions on the diffusion generation process and resulting conversions.
Through comparative and comprehensive evaluations, SingVisio demonstrates its effectiveness in terms of system design, functionality, explainability, and user-friendliness.
It offers users of various backgrounds valuable learning experiences and insights into the diffusion model for singing voice conversion.

\begin{keyword}
Machine Learning, Explainable AI, Visual Analytics, Audio Processing
\end{keyword}

## 1·Introduction

Deep generative models have become increasingly prevalent in a myriad of data generation tasks, ranging from image generation to audio generation.

Among these, diffusion-based generative models have emerged as a cutting-edge research focus and the go-to methodology for such applications[^Yang2023Diffusion].

In the field of computer vision, diffusion models have gained significant popularity[^Zhang2023Text-to-Image], [^Xing2023Survey], particularly in applications such as text-to-image synthesis[^Xu2023Versatile], [^Rombach2022High-Resolution], [^Rombach2022High-Resolution], video generation[^Xing2023Survey] and editing[^Ceylan2023Pix2video].

In the audio community, there have been extensive studies of diffusion models in waveform synthesis[^Chen2021WaveGrad], [^Kong2020DiffWave], sound effects generation[^Liu2023AudioLDM], [^Huang2023Make-an-Audio], speech generation[^Popov2021Grad-TTS], [^Shen2024NaturalSpeech], and music generation[^Liu2022DiffSinger], [^Schneider2023Moûsai].

Given their wide-ranging utility and impressive performance, there is a burgeoning curiosity and necessity to unravel the intricacies of the diffusion process underpinning these generative tasks.

However, the complexity of the involved Markov chains and their complex mathematical formulations pose a significant hurdle to novices in the field.

In recent years, visual and interactive methodologies have proven instrumental in deciphering the structures and working mechanisms in various deep-learning models [^Kahng2018Gan], [^Lee2023Diffusion], [^Park2024Explaining].

This insight has spurred us to develop interactive visual tools aimed at broader audiences, facilitating a deeper comprehension of diffusion-based generative models.

The paper represents an attempt to demystify the diffusion-based generative paradigm.

Owing to its notable capabilities, the diffusion-based generative model has quickly risen as a formidable contender in singing voice conversion (SVC).

This advanced technique effectively alters one singer's voice to another's, meticulously preserving the song's original content and melody, as investigated in the studies[^Liu2021DiffSVC], [^Zhang2023Leveraging], [^Lu2024CoMoSVC].

When juxtaposed with other generative models, such as Generative Adversarial Networks (GANs)[^Goodfellow2020Generative] and Variational Auto-Encoders (VAEs)[^Kingma2014Auto-Encoding], diffusion-based models resolve the issue of unsatisfactory audio quality via incrementally introducing noise into the data and iteratively learning to eliminate noise.

Due to iterative noising and denoising processes in synthesizing high-quality data, comparing the changes in the diffusion process step-by-step is essential to learn about the diffusion model.

The current pedagogical approaches for beginners\footnote{In the context of this study, "beginners'' are defined as individuals who have less than one year of experience in both the field of machine learning and the field of music and singing processing.

This group primarily consists of users who are new to both the technical aspects of machine learning and the specific applications in music and singing processing.

We expect the beginners' main focus to be on gaining fundamental knowledge about the diffusion model applied in the SVC in this study.} learning about diffusion-based models is overly dependent on textual explanations and mathematical descriptions\footnote{\url{https://theaisummer.com/diffusion-models/}}.

This traditional learning method is neither intuitive nor efficient, often causing beginners to lose track among complex formulas without the ability to directly view and compare results at each step.

Moreover, understanding the impact of various conditions—such as the source voice's content, melody, and the target singer's unique timbre—on the generation process is crucial for experts to identify challenging samples for SVC and make informed decisions to enhance SVC performance.

Currently, comparing the effects of different conditions on SVC results is both time-consuming and cumbersome.

Researchers must generate and save each feature, such as Mel spectrograms and audio files, and then repeatedly open and compare these across various steps.

Methods involving visualization and exploratory interaction are less common, as evidenced by examples such as [^Sergios2022Diffusion] and [^O'Connor2024Diffusion], which do not offer users an immersive understanding of the diffusion process.

This highlights an urgent demand for comprehensive, interactive, and visually intuitive tools designed for diffusion-based generative models to fill this gap. 
In this paper, we propose SingVisio, a visual analytics system designed to interactively explain diffusion models in SVC.

To maintain anonymity during the review process, the code will be made publicly available upon the paper's acceptance.

SingVisio offers both a basic version to help beginners grasp the basic concepts of diffusion models, and an advanced version for experts by providing an efficient tool to further investigate diffusion-based SVC.

For visual representation, we extract Mel spectrograms and F0 contours from audio. 

Additionally, we demystify the diffusion process by extracting and rendering hidden features from different layers in the model over 1000 steps. 

Furthermore, we propose a novel interval clustering center sampling method, enabling users to flexibly specify the number of sample points and display the corresponding hidden features.

% basic version and advanced version
% both version
% Mel spectrogram and f0 extracted from audio
% metrics 

%% 其中 step comparison mode中，basic version 

The contributions of this work can be summarized as follows:

 

-  **A visual analytics system for understanding SVC.**
To the best of our knowledge, this is the first system supporting the exploration, visualization, and comparison of the diffusion model within the context of SVC.

It offers a versatile platform for comparing various aspects of the diffusion process, SVC modes, and evaluation metrics, allowing for a thorough exploration.

-  **Novel interactive exploration approach to understanding diffusion-based SVC.**
We have supported three core interactive exploration modes within our system: {\bf data-driven} exploration, which is steered by varying melodies, {\bf condition-driven} exploration that pivots on the specific inputs provided to the diffusion model, and {\bf evaluation-driven} exploration, which is based on the assessment metric.

Also, we propose a novel interval clustering center sampling method to efficiently sample and display hidden features at specified steps.
% These interactive modes are thoughtfully designed to aid users in comprehending and navigating the diffusion process integral to SVC. 

% \textcolor{blue}{
% 
-  **Novel sampling strategy to efficiently explore hidden features in the diffusion model**
% We introduce a novel diffusion sampling strategy called interval clustering center sampling.

This method enables us to sample and display hidden features at specified steps within the diffusion model.

By clustering features and selecting representative centers from each interval, users can efficiently explore and visualize the evolution of hidden features.
% }

-  **A comparative and comprehensive evaluation of SingVisio.**
We conducted a comparative and comprehensive evaluation of our system with the basic version and advanced version, including a case study involving two beginners, an expert study with two experts, and a formal user study encompassing both subjective and objective assessments for general users.

Such evaluation shows the effectiveness of our system.

## 2·Related Work

### Singing Voice Conversion

The early singing voice conversion research aims to design parametric statistical models such as HMM[^T{\"{u}}rk2009Application] or GMM[^Kobayashi2014Statistical], [^Kobayashi2015Statistical] to learn the spectral features mapping of the parallel data.

Since the parallel singing voice corpus is challenging to collect on a large scale, the non-parallel SVC[^Nachmani2019Unsupervised], [^Chen2019Singing], or recognition-synthesis SVC[^Huang2022Comparative], has been popular in recent years, whose pipeline is displayed in Fig.~[fig:svc-pipeline](#fig:svc-pipeline).

In the non-parallel SVC pipeline, the acoustic model conducts the feature conversion from source to target.

It can be various types of generative models, including autoregressive models[^Chen2019Singing], GAN-based models[^Liu2021FastSVC], [^Takahashi2022Robust], VAE-based models[^Luo2020Singing], [^{SVC-Develop-Team}2023SoftSVC], or Flow-based models[^{SVC-Develop-Team}2023SoftSVC].

Besides, adopting a diffusion-based acoustic model is also promising for VC[^Popov2022Diffusion-Based], [^Choi2023Diff-HierVC] and SVC[^Liu2021DiffSVC], [^Zhang2023Leveraging], [^Lu2024CoMoSVC].

Recently, more and more research has verified the strong performance of diffusion models in modeling audio areas[^Liu2022DiffSinger], [^Kong2020DiffWave], [^Shen2024NaturalSpeech], [^Wang2022Audit:]. 

Although the diffusion model has shown impressive quality and performance when applied to SVC, our understanding of its internal mechanisms is still limited.

Firstly, the existing diffusion models are still based on black-box neural networks.

Visualizing how it achieves singing voice conversion through step-by-step denoising would greatly deepen researchers' comprehension of the diffusion model's operating principles.

Secondly, the SVC conditions, which serve as inputs to the diffusion model, are crucial factors influencing the final conversion results.

However, we are still unclear about how different conditions affect the performance of the diffusion model.

Motivated by that, this paper will conduct a systematic analysis of diffusion-based SVC under different diffusion steps and diverse SVC conditions, like varied sources and targets.

### Visual Analysis for Explainable AI 

E**X**plainable **A**rtificial **I**ntelligence (XAI)[^Arrieta2020Explainable] has become increasingly important as machine learning models, especially deep learning models, grow in complexity and usage in critical applications[^Hohman2018Visual].

Visual analysis tools have been developed to make these models more interpretable and trustworthy to users.

CNN Explainer simplifies the understanding of Convolutional Neural Networks (CNNs) by visualizing their feature extraction process[^Wang2020Cnn].

LSTMVis[^Strobelt2017LSTMVis] and DQNViz[^Wang2018DQNViz] offer insights into the decision-making processes of LSTM networks and Deep Q-Networks, respectively.

M2Lens[^Wang2021M2Lens] and CNNVis[^Liu2016Towards] are designed to dissect the intricate layers of CNNs, providing a detailed examination of filter activations and network architectures.

AttentionViz focuses on the attention mechanisms in models, revealing how models prioritize different parts of the input data for decision-making~\cite {AttentionViz}.

Additionally, the interpretation of generative models through visualization addresses the challenge of understanding complex data generation processes.

Adversarial-Playground[^Norton2017Adversarial-Playground], GANLab[^Kahng2018Gan] and GANViz[^Wang2018GANViz] are interactive tools for exploring and interpreting Generative Adversarial Networks (GANs).

Research on analyzing the training processes of deep generative models uncovers the dynamics and stability issues inherent in these models.

Further, DrugExplorer[^Wang2022Extending] exemplifies the application of visualization techniques in domain-specific areas.

Recently, diffusion models have shown significant capabilities in generative tasks, and accordingly the visualization tool, aiming at making the diffusion process comprehensible to humans, is investigated[^Park2024Explaining].

Besides, Diffusion Explainer concentrates on demystifying the stable diffusion process, offering an understanding of the transformation from text prompts into images[^Lee2023Diffusion].

In our work, we design an interactive visual analysis system for the diffusion model applied in singing voice conversion.

It illustrates how the noisy spectrum is gradually denoised under the influence of conditions, ultimately converting the spectrum to the target singer's timbre.

## 3·Background: Diffusion-Based Singing Voice Conversion

\label{sec:background}

![](figs/svc-pipeline.png)

<a id="fig:svc-pipeline">The classic pipeline of SVC system, including three steps: (a) feature extraction that extracts content and melody features from the source and singer timbre from the target, (b) acoustic model mapping extracted features to acoustic features (e.g.

Mel spectrogram), (c) waveform synthesizer reconstructing singing voice from the converted acoustic feature.  In this study, we use "diffusion-based singing voice conversion" to refer that the acoustic model in the SVC is a diffusion model.
\vspace{-29pt}</a>

SVC aims to transform the voice in a singing signal to match that of a target singer while preserving the original lyrics and melody[^Huang2023Singing].

The classic pipeline of SVC typically involves three steps, as shown in Fig.~[fig:svc-pipeline](#fig:svc-pipeline). (a) Feature extraction: extract content (i.e., lyrics) and melody features from the source singing voice and the timbre feature from the target singing voice.

These features are then combined to form the conditions for SVC, which are fed into the following acoustic models. (b) Acoustic model: convert the source features to acoustic features (such as the Mel spectrogram) that match the target singer’s voice. (c) Waveform synthesizer: Reconstruct the singing voice waveform from the transformed acoustic features to produce the target singer timber while maintaining the source content.

In this study, the term `diffusion-based singing voice conversion' is used to denote that the acoustic model in the SVC system is a diffusion model. 

### Architecture and Workflow

In this study, we select DiffWaveNetSVC[^Zhang2023Amphion], [^Zhang2023Leveraging] as the SVC's acoustic model to visualize and analyze.

The internal module of the DiffWaveNetSVC is based on Bidirectional Non-Causal Dilated CNN[^Liu2021DiffSVC], [^Kong2020DiffWave], which is similar to WaveNet[^Oord2016WaveNet]. 

The architecture of DiffWaveNetSVC is shown in Fig.~[fig:diffwavenetsvc](#fig:diffwavenetsvc) of [app:svc_model](#app:svc_model).

It consists of multiple residual layers, within which it adopts Bidirectional Non-Causal Dilated CNN ("Bi-Dilated Conv" in Fig.~[fig:diffwavenetsvc](#fig:diffwavenetsvc)) of [app:svc_model](#app:svc_model) like[^Oord2016WaveNet], [^Kong2020DiffWave], [^Liu2021DiffSVC].

During training (i.e., the forward process of diffusion model), we extract the content, melody, and singer features from the same sample (which means the source and target in Fig.~[fig:svc-pipeline](#fig:svc-pipeline) are the same) and add them to obtain the SVC conditions  $\mathbf{c}$.

At the step $t \in [0, 1, 2, \cdots T]$, we sample a Gaussian noise $\mathbf{\epsilon}_t \sim N(\mathbf{0},~\mathbf{I})$ and obtain the noisy Mel spectrogram:

$$
\mathbf{y}_t = \sqrt{\alpha_t} \mathbf{y}_0 + \sqrt{1 - \alpha_t} \mathbf{\epsilon}_t,
$$

where $\alpha_t$ is the noise weight in diffusion model[^Ho2020Denoising].

And the training objective can be considered to predict the noise $\mathbf{\epsilon}_t$ using the neural network:

$$

\begin{split}
\hat{\mathbf{\epsilon}}_t &= \mathbf{DiffWaveNetSVC}(t, \mathbf{y}_t, \mathbf{c}), \\
\mathcal{L}_t &= \mathbf{MSE}(\hat{\mathbf{\epsilon}}_t, {\mathbf{\epsilon}}_t),
\end{split}

$$

where $\mathbf{DiffWaveNetSVC}$ represents the whole encoder based on the residual layers and $\mathbf{MSE}$ means the mean squared error loss function.

During inference/conversion (the reverse process of diffusion model), given the source and target, we extract the content and melody features from the source, extract the singer features from the target, and add them as the SVC conditions $\mathbf{c}$.

We feed a Gaussian noise $\hat{\mathbf{y}}_T \sim N(\mathbf{0},~\mathbf{I})$ to DiffWaveNetSVC and employ deep denoising implicit models[^Ho2020Denoising] with $T$ denoising steps to produce Mel spectrogram $\hat{\mathbf{y}_0}$.

### Implementation Details and Evaluation Metrics

In this paper, we follow the Amphion's implementation[^Zhang2023Amphion]\footnote{\href{https://github.com/open-mmlab/Amphion/tree/main/egs/svc/MultipleContentsSVC}{https://github.com/open-mmlab/Amphion/tree/main/egs/svc/MultipleContentsSVC}} for DiffWaveNetSVC.

Specifically, the layer number $N$ is 20, and the diffusion step number $T$ is 1000.

Following Zhang et al.[^Zhang2023Leveraging], we adopt both Whisper[^Radford2023Robust] and ContentVec[^Qian2022ContentVec] as the content features, we use Parselmouth\footnote{\href{https://parselmouth.readthedocs.io/en/stable/index.html}{https://parselmouth.readthedocs.io/en/stable/index.html}}[^Jadoul2018Introducing] to extract F0 as the melody features, and we adopt look-up table to obtain the one-hot singer ID as the singer features.

We utilize the DiffWaveNetSVC checkpoint of Zhang et al.[^Zhang2023Leveraging] to conduct the inference, conversion, and visualization analysis, which is pre-trained on 83.1 hours of speech (111 singer) and 87.2 hours of singing data (96 singers).

The detailed information about the dataset is described in~[app:dataset](#app:dataset).

For waveform synthesizer, we use the pre-trained Amphion Singing BigVGAN\footnote{\href{https://huggingface.co/amphion/BigVGAN_singing_bigdata}{https://huggingface.co/amphion/BigVGAN\_singing\_bigdata}} to produce waveform from Mel spectrogram.

Accurately and effectively assessing the results of SVC is significantly important[^Huang2023Singing].

Objective evaluation involves measuring performance at various aspects, such as spectrogram distortion, F0 modeling, intelligibility, and singer similarity.

To objectively evaluate synthesized samples, we adopt the evaluation methodology from Amphion[^Zhang2023Amphion]\footnote{\href{https://github.com/open-mmlab/Amphion/tree/main/egs/metrics}{https://github.com/open-mmlab/Amphion/tree/main/egs/metrics}} for our objective assessment.

This includes metrics such as **Singer Similarity (Dembed) with Resemblyzer~\footnote{\url{https://github.com/resemble-ai/Resemblyzer**}}, **F0 Pearson Correlation Coefficient (F0CORR)**, **Fréchet Audio Distance (FAD)**, **F0 Root Mean Square Error (F0RMSE)**,  and **Mel-cepstral Distortion (MCD)**.

Detailed definitions of these metrics are provided in ~[app:metircs](#app:metircs).
