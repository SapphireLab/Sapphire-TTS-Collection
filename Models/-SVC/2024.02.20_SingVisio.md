# SingVisio
# SingVisio: Visual Analytics of Diffusion Model for Singing Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "SingVisio: Visual Analytics of Diffusion Model for Singing Voice Conversion."
- 作者:
  - 01 Liumeng Xue
  - 02 Chaoren Wang
  - 03 Mingxuan Wang
  - 04 Xueyao Zhang
  - 05 Jun Han
  - 06 Zhizheng Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2402.12660v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2402.12660v1](_PDF/2024.02.20_2402.12660v1_SingVisio__Visual_Analytics_of_Diffusion_Model_for_Singing_Voice_Conversion.pdf)
  - [ArXiv:2402.12660v2](_PDF/2024.02.20_2402.12660v2_SingVisio__Visual_Analytics_of_Diffusion_Model_for_Singing_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

In this study, we present SingVisio, an interactive visual analysis system that aims to explain the diffusion model used in singing voice conversion.
SingVisio provides a visual display of the generation process in diffusion models, showcasing the step-by-step denoising of the noisy spectrum and its transformation into a clean spectrum that captures the desired singer's timbre.
The system also facilitates side-by-side comparisons of different conditions, such as source content, melody, and target timbre, highlighting the impact of these conditions on the diffusion generation process and resulting conversions.
Through comparative and comprehensive evaluations, SingVisio demonstrates its effectiveness in terms of system design, functionality, explainability, and user-friendliness.
It offers users of various backgrounds valuable learning experiences and insights into the diffusion model for singing voice conversion.

\begin{keyword}
Machine Learning, Explainable AI, Visual Analytics, Audio Processing
\end{keyword}

## 1·Introduction

Deep generative models have become increasingly prevalent in a myriad of data generation tasks, ranging from image generation to audio generation.

Among these, diffusion-based generative models have emerged as a cutting-edge research focus and the go-to methodology for such applications[^Yang2023Diffusion].

In the field of computer vision, diffusion models have gained significant popularity[^Zhang2023Text-to-Image], [^Xing2023Survey], particularly in applications such as text-to-image synthesis[^Xu2023Versatile], [^Rombach2022High-Resolution], [^Rombach2022High-Resolution], video generation[^Xing2023Survey] and editing[^Ceylan2023Pix2video].

In the audio community, there have been extensive studies of diffusion models in waveform synthesis[^Chen2021WaveGrad], [^Kong2020DiffWave], sound effects generation[^Liu2023AudioLDM], [^Huang2023Make-an-Audio], speech generation[^Popov2021Grad-TTS], [^Shen2024NaturalSpeech], and music generation[^Liu2022DiffSinger], [^Schneider2023Moûsai].

Given their wide-ranging utility and impressive performance, there is a burgeoning curiosity and necessity to unravel the intricacies of the diffusion process underpinning these generative tasks.

However, the complexity of the involved Markov chains and their complex mathematical formulations pose a significant hurdle to novices in the field.

In recent years, visual and interactive methodologies have proven instrumental in deciphering the structures and working mechanisms in various deep-learning models [^Kahng2018Gan], [^Lee2023Diffusion], [^Park2024Explaining].

This insight has spurred us to develop interactive visual tools aimed at broader audiences, facilitating a deeper comprehension of diffusion-based generative models.

The paper represents an attempt to demystify the diffusion-based generative paradigm.

Owing to its notable capabilities, the diffusion-based generative model has quickly risen as a formidable contender in singing voice conversion (SVC).

This advanced technique effectively alters one singer's voice to another's, meticulously preserving the song's original content and melody, as investigated in the studies[^Liu2021DiffSVC], [^Zhang2023Leveraging], [^Lu2024CoMoSVC].

When juxtaposed with other generative models, such as Generative Adversarial Networks (GANs)[^Goodfellow2020Generative] and Variational Auto-Encoders (VAEs)[^Kingma2014Auto-Encoding], diffusion-based models resolve the issue of unsatisfactory audio quality via incrementally introducing noise into the data and iteratively learning to eliminate noise.

Due to iterative noising and denoising processes in synthesizing high-quality data, comparing the changes in the diffusion process step-by-step is essential to learn about the diffusion model.

The current pedagogical approaches for beginners\footnote{In the context of this study, "beginners'' are defined as individuals who have less than one year of experience in both the field of machine learning and the field of music and singing processing.

This group primarily consists of users who are new to both the technical aspects of machine learning and the specific applications in music and singing processing.

We expect the beginners' main focus to be on gaining fundamental knowledge about the diffusion model applied in the SVC in this study.} learning about diffusion-based models is overly dependent on textual explanations and mathematical descriptions\footnote{\url{https://theaisummer.com/diffusion-models/}}.

This traditional learning method is neither intuitive nor efficient, often causing beginners to lose track among complex formulas without the ability to directly view and compare results at each step.

Moreover, understanding the impact of various conditions—such as the source voice's content, melody, and the target singer's unique timbre—on the generation process is crucial for experts to identify challenging samples for SVC and make informed decisions to enhance SVC performance.

Currently, comparing the effects of different conditions on SVC results is both time-consuming and cumbersome.

Researchers must generate and save each feature, such as Mel spectrograms and audio files, and then repeatedly open and compare these across various steps.

Methods involving visualization and exploratory interaction are less common, as evidenced by examples such as [^Sergios2022Diffusion] and [^O'Connor2024Diffusion], which do not offer users an immersive understanding of the diffusion process.

This highlights an urgent demand for comprehensive, interactive, and visually intuitive tools designed for diffusion-based generative models to fill this gap. 
In this paper, we propose SingVisio, a visual analytics system designed to interactively explain diffusion models in SVC.

To maintain anonymity during the review process, the code will be made publicly available upon the paper's acceptance.

SingVisio offers both a basic version to help beginners grasp the basic concepts of diffusion models, and an advanced version for experts by providing an efficient tool to further investigate diffusion-based SVC.

For visual representation, we extract Mel spectrograms and F0 contours from audio. 

Additionally, we demystify the diffusion process by extracting and rendering hidden features from different layers in the model over 1000 steps. 

Furthermore, we propose a novel interval clustering center sampling method, enabling users to flexibly specify the number of sample points and display the corresponding hidden features.

% basic version and advanced version
% both version
% Mel spectrogram and f0 extracted from audio
% metrics 

%% 其中 step comparison mode中，basic version 

The contributions of this work can be summarized as follows:

 

-  **A visual analytics system for understanding SVC.**
To the best of our knowledge, this is the first system supporting the exploration, visualization, and comparison of the diffusion model within the context of SVC.

It offers a versatile platform for comparing various aspects of the diffusion process, SVC modes, and evaluation metrics, allowing for a thorough exploration.

-  **Novel interactive exploration approach to understanding diffusion-based SVC.**
We have supported three core interactive exploration modes within our system: {\bf data-driven} exploration, which is steered by varying melodies, {\bf condition-driven} exploration that pivots on the specific inputs provided to the diffusion model, and {\bf evaluation-driven} exploration, which is based on the assessment metric.

Also, we propose a novel interval clustering center sampling method to efficiently sample and display hidden features at specified steps.
% These interactive modes are thoughtfully designed to aid users in comprehending and navigating the diffusion process integral to SVC. 

% \textcolor{blue}{
% 
-  **Novel sampling strategy to efficiently explore hidden features in the diffusion model**
% We introduce a novel diffusion sampling strategy called interval clustering center sampling.

This method enables us to sample and display hidden features at specified steps within the diffusion model.

By clustering features and selecting representative centers from each interval, users can efficiently explore and visualize the evolution of hidden features.
% }

-  **A comparative and comprehensive evaluation of SingVisio.**
We conducted a comparative and comprehensive evaluation of our system with the basic version and advanced version, including a case study involving two beginners, an expert study with two experts, and a formal user study encompassing both subjective and objective assessments for general users.

Such evaluation shows the effectiveness of our system.

## 2·Related Work

### Singing Voice Conversion

The early singing voice conversion research aims to design parametric statistical models such as HMM[^T{\"{u}}rk2009Application] or GMM[^Kobayashi2014Statistical], [^Kobayashi2015Statistical] to learn the spectral features mapping of the parallel data.

Since the parallel singing voice corpus is challenging to collect on a large scale, the non-parallel SVC[^Nachmani2019Unsupervised], [^Chen2019Singing], or recognition-synthesis SVC[^Huang2022Comparative], has been popular in recent years, whose pipeline is displayed in Fig.~[fig:svc-pipeline](#fig:svc-pipeline).

In the non-parallel SVC pipeline, the acoustic model conducts the feature conversion from source to target.

It can be various types of generative models, including autoregressive models[^Chen2019Singing], GAN-based models[^Liu2021FastSVC], [^Takahashi2022Robust], VAE-based models[^Luo2020Singing], [^{SVC-Develop-Team}2023SoftSVC], or Flow-based models[^{SVC-Develop-Team}2023SoftSVC].

Besides, adopting a diffusion-based acoustic model is also promising for VC[^Popov2022Diffusion-Based], [^Choi2023Diff-HierVC] and SVC[^Liu2021DiffSVC], [^Zhang2023Leveraging], [^Lu2024CoMoSVC].

Recently, more and more research has verified the strong performance of diffusion models in modeling audio areas[^Liu2022DiffSinger], [^Kong2020DiffWave], [^Shen2024NaturalSpeech], [^Wang2022Audit:]. 

Although the diffusion model has shown impressive quality and performance when applied to SVC, our understanding of its internal mechanisms is still limited.

Firstly, the existing diffusion models are still based on black-box neural networks.

Visualizing how it achieves singing voice conversion through step-by-step denoising would greatly deepen researchers' comprehension of the diffusion model's operating principles.

Secondly, the SVC conditions, which serve as inputs to the diffusion model, are crucial factors influencing the final conversion results.

However, we are still unclear about how different conditions affect the performance of the diffusion model.

Motivated by that, this paper will conduct a systematic analysis of diffusion-based SVC under different diffusion steps and diverse SVC conditions, like varied sources and targets.

### Visual Analysis for Explainable AI 

E**X**plainable **A**rtificial **I**ntelligence (XAI)[^Arrieta2020Explainable] has become increasingly important as machine learning models, especially deep learning models, grow in complexity and usage in critical applications[^Hohman2018Visual].

Visual analysis tools have been developed to make these models more interpretable and trustworthy to users.

CNN Explainer simplifies the understanding of Convolutional Neural Networks (CNNs) by visualizing their feature extraction process[^Wang2020Cnn].

LSTMVis[^Strobelt2017LSTMVis] and DQNViz[^Wang2018DQNViz] offer insights into the decision-making processes of LSTM networks and Deep Q-Networks, respectively.

M2Lens[^Wang2021M2Lens] and CNNVis[^Liu2016Towards] are designed to dissect the intricate layers of CNNs, providing a detailed examination of filter activations and network architectures.

AttentionViz focuses on the attention mechanisms in models, revealing how models prioritize different parts of the input data for decision-making~\cite {AttentionViz}.

Additionally, the interpretation of generative models through visualization addresses the challenge of understanding complex data generation processes.

Adversarial-Playground[^Norton2017Adversarial-Playground], GANLab[^Kahng2018Gan] and GANViz[^Wang2018GANViz] are interactive tools for exploring and interpreting Generative Adversarial Networks (GANs).

Research on analyzing the training processes of deep generative models uncovers the dynamics and stability issues inherent in these models.

Further, DrugExplorer[^Wang2022Extending] exemplifies the application of visualization techniques in domain-specific areas.

Recently, diffusion models have shown significant capabilities in generative tasks, and accordingly the visualization tool, aiming at making the diffusion process comprehensible to humans, is investigated[^Park2024Explaining].

Besides, Diffusion Explainer concentrates on demystifying the stable diffusion process, offering an understanding of the transformation from text prompts into images[^Lee2023Diffusion].

In our work, we design an interactive visual analysis system for the diffusion model applied in singing voice conversion.

It illustrates how the noisy spectrum is gradually denoised under the influence of conditions, ultimately converting the spectrum to the target singer's timbre.

## 3·Background: Diffusion-Based Singing Voice Conversion

\label{sec:background}

![](figs/svc-pipeline.png)

<a id="fig:svc-pipeline">The classic pipeline of SVC system, including three steps: (a) feature extraction that extracts content and melody features from the source and singer timbre from the target, (b) acoustic model mapping extracted features to acoustic features (e.g.

Mel spectrogram), (c) waveform synthesizer reconstructing singing voice from the converted acoustic feature.  In this study, we use "diffusion-based singing voice conversion" to refer that the acoustic model in the SVC is a diffusion model.
\vspace{-29pt}</a>

SVC aims to transform the voice in a singing signal to match that of a target singer while preserving the original lyrics and melody[^Huang2023Singing].

The classic pipeline of SVC typically involves three steps, as shown in Fig.~[fig:svc-pipeline](#fig:svc-pipeline). (a) Feature extraction: extract content (i.e., lyrics) and melody features from the source singing voice and the timbre feature from the target singing voice.

These features are then combined to form the conditions for SVC, which are fed into the following acoustic models. (b) Acoustic model: convert the source features to acoustic features (such as the Mel spectrogram) that match the target singer’s voice. (c) Waveform synthesizer: Reconstruct the singing voice waveform from the transformed acoustic features to produce the target singer timber while maintaining the source content.

In this study, the term `diffusion-based singing voice conversion' is used to denote that the acoustic model in the SVC system is a diffusion model. 

### Architecture and Workflow

In this study, we select DiffWaveNetSVC[^Zhang2023Amphion], [^Zhang2023Leveraging] as the SVC's acoustic model to visualize and analyze.

The internal module of the DiffWaveNetSVC is based on Bidirectional Non-Causal Dilated CNN[^Liu2021DiffSVC], [^Kong2020DiffWave], which is similar to WaveNet[^Oord2016WaveNet]. 

The architecture of DiffWaveNetSVC is shown in Fig.~[fig:diffwavenetsvc](#fig:diffwavenetsvc) of [app:svc_model](#app:svc_model).

It consists of multiple residual layers, within which it adopts Bidirectional Non-Causal Dilated CNN ("Bi-Dilated Conv" in Fig.~[fig:diffwavenetsvc](#fig:diffwavenetsvc)) of [app:svc_model](#app:svc_model) like[^Oord2016WaveNet], [^Kong2020DiffWave], [^Liu2021DiffSVC].

During training (i.e., the forward process of diffusion model), we extract the content, melody, and singer features from the same sample (which means the source and target in Fig.~[fig:svc-pipeline](#fig:svc-pipeline) are the same) and add them to obtain the SVC conditions  $\mathbf{c}$.

At the step $t \in [0, 1, 2, \cdots T]$, we sample a Gaussian noise $\mathbf{\epsilon}_t \sim N(\mathbf{0},~\mathbf{I})$ and obtain the noisy Mel spectrogram:

$$
\mathbf{y}_t = \sqrt{\alpha_t} \mathbf{y}_0 + \sqrt{1 - \alpha_t} \mathbf{\epsilon}_t,
$$

where $\alpha_t$ is the noise weight in diffusion model[^Ho2020Denoising].

And the training objective can be considered to predict the noise $\mathbf{\epsilon}_t$ using the neural network:

$$

\begin{split}
\hat{\mathbf{\epsilon}}_t &= \mathbf{DiffWaveNetSVC}(t, \mathbf{y}_t, \mathbf{c}), \\
\mathcal{L}_t &= \mathbf{MSE}(\hat{\mathbf{\epsilon}}_t, {\mathbf{\epsilon}}_t),
\end{split}

$$

where $\mathbf{DiffWaveNetSVC}$ represents the whole encoder based on the residual layers and $\mathbf{MSE}$ means the mean squared error loss function.

During inference/conversion (the reverse process of diffusion model), given the source and target, we extract the content and melody features from the source, extract the singer features from the target, and add them as the SVC conditions $\mathbf{c}$.

We feed a Gaussian noise $\hat{\mathbf{y}}_T \sim N(\mathbf{0},~\mathbf{I})$ to DiffWaveNetSVC and employ deep denoising implicit models[^Ho2020Denoising] with $T$ denoising steps to produce Mel spectrogram $\hat{\mathbf{y}_0}$.

### Implementation Details and Evaluation Metrics

In this paper, we follow the Amphion's implementation[^Zhang2023Amphion]\footnote{\href{https://github.com/open-mmlab/Amphion/tree/main/egs/svc/MultipleContentsSVC}{https://github.com/open-mmlab/Amphion/tree/main/egs/svc/MultipleContentsSVC}} for DiffWaveNetSVC.

Specifically, the layer number $N$ is 20, and the diffusion step number $T$ is 1000.

Following Zhang et al.[^Zhang2023Leveraging], we adopt both Whisper[^Radford2023Robust] and ContentVec[^Qian2022ContentVec] as the content features, we use Parselmouth\footnote{\href{https://parselmouth.readthedocs.io/en/stable/index.html}{https://parselmouth.readthedocs.io/en/stable/index.html}}[^Jadoul2018Introducing] to extract F0 as the melody features, and we adopt look-up table to obtain the one-hot singer ID as the singer features.

We utilize the DiffWaveNetSVC checkpoint of Zhang et al.[^Zhang2023Leveraging] to conduct the inference, conversion, and visualization analysis, which is pre-trained on 83.1 hours of speech (111 singer) and 87.2 hours of singing data (96 singers).

The detailed information about the dataset is described in~[app:dataset](#app:dataset).

For waveform synthesizer, we use the pre-trained Amphion Singing BigVGAN\footnote{\href{https://huggingface.co/amphion/BigVGAN_singing_bigdata}{https://huggingface.co/amphion/BigVGAN\_singing\_bigdata}} to produce waveform from Mel spectrogram.

Accurately and effectively assessing the results of SVC is significantly important[^Huang2023Singing].

Objective evaluation involves measuring performance at various aspects, such as spectrogram distortion, F0 modeling, intelligibility, and singer similarity.

To objectively evaluate synthesized samples, we adopt the evaluation methodology from Amphion[^Zhang2023Amphion]\footnote{\href{https://github.com/open-mmlab/Amphion/tree/main/egs/metrics}{https://github.com/open-mmlab/Amphion/tree/main/egs/metrics}} for our objective assessment.

This includes metrics such as **Singer Similarity (Dembed) with Resemblyzer~\footnote{\url{https://github.com/resemble-ai/Resemblyzer**}}, **F0 Pearson Correlation Coefficient (F0CORR)**, **Fréchet Audio Distance (FAD)**, **F0 Root Mean Square Error (F0RMSE)**,  and **Mel-cepstral Distortion (MCD)**.

Detailed definitions of these metrics are provided in ~[app:metircs](#app:metircs).

## 4·Design Requirements

### Requirement analysis

Through a series of interviews with experts in audio signal processing and machine learning, we identified three critical tasks that our system needs to support for effective analysis and interpretation of the diffusion model for SVC.

**C1: In-Depth Temporal Dynamics Analysis of Diffusion Generation Process.**

Experts highlighted the importance of visualizing the temporal dynamics of the diffusion generation process in singing voice conversion.

The objective is to create detailed visual representations that effectively trace the step-by-step evolution occurring in voice conversion at each diffusion stage.

This involves visualizing the progression of various acoustic parameters, such as frequency components and harmonics that evolve over time.

The visualizations are expected to provide users with an intuitive understanding of the voice transformation, highlighting the nuanced evolution from a noisy beginning to a structured and coherent output, thereby making the process more transparent and understandable to users without deep technical expertise in machine learning or signal processing.

**C2: Comprehensive Performance Metrics Evaluation in Singing Voice Synthesis.**

This task is centered on tracking evaluation metrics that gauge the quality of the converted voice at each step of the diffusion generation process.

These metrics include pitch accuracy, timbre consistency, naturalness, and speech quality.

The system should enable a detailed analysis of how each metric evolves with every diffusion step, offering insights into the conversion quality at different phases of the generation process.

This comprehensive evaluation is pivotal in identifying aspects where the conversion achieves optimal quality or, conversely, where improvements are needed.

This visual representation enhances the interpretability of the evaluation results and fosters insights into the underlying dynamics of the diffusion generation process.

**C3: Comparative Analysis of Different Source Singers, Songs, and Target Singers.**

Through visualization, we aim to systematically compare how different characteristics of source singers, such as vocal tone, pitch range, and singing style, influence the conversion outcome.

This will help in identifying specific attributes of source singers that are more amenable to conversion.

Additionally, the complexity and structure of the song itself are crucial variables.

Songs with intricate melodic lines or complex rhythms might pose more significant challenges in conversion processes.

Equally important is the analysis of the target singers' characteristics.

The system should visualize how well the model adapts the source singer's voice to match the timbre of the target singers.

This could lead to valuable insights, such as identifying particularly challenging source-target pairings or songs that consistently yield high-quality conversions.

Such analysis is not only crucial for understanding the current model's performance but also for guiding future improvements and applications in singing voice conversion technology.

C1 and C2 tasks are related to fundamental knowledge of the diffusion model, which is crucial and beneficial for beginners to understand diffusion models.

In contrast, C3 task focuses on exploring the impacts of different conditions on SVC, which is more suitable for experts or researchers seeking an in-depth understanding and analysis of diffusion-based SVC.

Accordingly, we design SingVisio in two versions: a basic version and an advanced version.

Both versions include C1 and C2 tasks.

Additionally, the advanced version encompasses C3 task, catering to the needs of experts and researchers.

![](figs/explainer_system_notes.pdf)

<a id="fig:explainer_system">%\jun{Display this figure in a teaser mode or move it into Section 5.} 
Visual system for diffusion-based singing voice conversion.

The system consists of five views. 
(A) ***Metric View*** shows objective evaluation results on the singing voice conversion model, allowing users to interactively explore the performance trend along diffusion steps. (B) ***Projection View*** aids users in tracking the data patterns of diffusion steps in the embedding space under different input conditions. (C) ***Step View*** provides users with the visualization of Mel spectrogram and pitch contour at one diffusion step. (D) ***Comparison View*** facilitates users to compare voice conversion results among different diffusion steps or singers. (E) ***Control Panel*** enables users to select various comparison modes and choose different source and target singers to visually understand and analyze the model behavior.

The red annotations provide explanations for the patterns or components. 
% \lmxue{upate figure and caption}
\vspace{-19pt}</a>

### Analytical Tasks

\label{sec:analytical-tasks}

Our system is a visualization system designed specifically for diffusion-based SVC tasks.

Diffusion-based SVC itself involves two aspects: in the realm of machine learning, it involves the diffusion generative model, and in the field of audio signal processing, it pertains to SVC.

Therefore, the analytical tasks supported by our system can be divided into two major categories.

In the aspect of machine learning, particularly in the diffusion model, to investigate the evolution and quality of the generated result from each step in the diffusion generation process, our system should support the following two tasks.

**T1: Step-wise Diffusion Generation Comparison.**

Examining the generated result of each step in the diffusion generation process helps in understanding the model's behavior.

Analyzing these early outputs can help us understand how the model initially handles noise.

As each step incrementally adds detail and structure to the output, by inspecting intermediate steps, we can observe the step-by-step improvement in content quality. (C1)

**T2: Step-wise Metric Comparison.**

As the diffusion steps progress, the generated content becomes clearer and more refined.

Analyzing the objective evaluation metrics and their corresponding curves along the diffusion steps serves as a useful tool for assessing both the quantitative and qualitative aspects of the generated content.

By tracking these metrics over the diffusion steps, we gain insights into how the model refines its output over time. (C2)

Regarding SVC, as described in Section~[sec:background](#sec:background), there are three factors (content, melody, singer timbre) that have a direct impact on the results of SVC and, therefore, should be considered during the conversion process.

To explore the impact of different factors on the converted results, the system needs to provide support for the following three tasks.

**T3: Pair-wise SVC Comparison with Different \uline{Target Singers**.}  
Pair-wisely comparing SVC under two different conditions of the target singer at different diffusion steps.

This task helps us to understand the impact of the timbre of the target singer that should be converted to the converted results of SVC, particularly in terms of singer similarity. (C1, C3)

**T4:  Pair-wise SVC Comparison with Different \uline{Source Singers**.}  
Pair-wisely comparing SVC under two different conditions of source singer at different diffusion steps.

This task benefits us in exploring the impact of the melody of the source that should be kept on the converted results of SVC, particularly in terms of F0CORR, F0RMSE. (C1, C3)

**T5: Pair-wise SVC Comparison with Different \uline{Songs**.}  
Pair-wisely comparing SVC under two different conditions of the song at different diffusion steps.

This task facilitates us to explore the impact of the content information conveyed in the song that should be maintained on the converted results of SVC, particularly in terms of MCD. (C1, C3)

## 5·Explainer System

Fig.~[fig:explainer_system](#fig:explainer_system) shows the overview of the explainer system which consists of five components: ***Control Panel*** allows users to modify mode and choose data for visual analysis; ***Step View*** provides users with an overview of the diffusion generation process; ***Comparison View*** makes it easy for users to compare converted results between different conditions; ***Projection View*** helps users observe the trajectory of diffusion steps with or without conditions; ***Metric View*** displays objective metrics evaluated on the diffusion-based SVC model, enabling users to interactively examine metric trends across diffusion steps.

### Control Panel

\label{sec:control_panel}

The control panel consists of six components, including two drop-down boxes to enable users to select display mode and projection embedding, three checkboxes to select source singer, source song, and target singer, and a step controller to enable users to control the diffusion step.

% 
-  
**Display Mode**

We design five types of display modes, including Step Comparison, Source Singer Comparison, Song Comparison, Target Singer Comparison, and Metric Comparison.

Users can click the drop-down box of "Display Mode " to choose a specific model.

-  **Step Comparison**

This mode primarily focuses on step-wise comparing the diffusion steps in the generation process.

It (1) provides an animation of random noise gradually refined for users to have an overview of the whole denoising process in ***Step View***, (2) enables users to adaptively select and compare the generated results from different diffusion steps in ***Comparison View***.

-  **Metric Comparison**

This mode presents five objective evaluation metric results of the diffusion-based SVC model represented by a bar chart.

It (1) enables users to click on a specific metric bar and then the system filters out an example that gains the best on the corresponding metric and displays metric curves along diffusion steps in the ***Comparison View***, (3) enables users to hover over and slide the mouse along the step axis of the metric curve, and then system will display the values of that metric at different steps in the ***Comparison View*** while synchronously showing the generated results at different steps in the ***Step View***.

-  **Source Singer Comparison**

This mode focuses on the pair-wise comparison of converting two different source singers' audio with the same song to the same target singer.

It (1) allows users to select \uline{two different source singers}, a source song and a target singer, (2) provides the details (including Mel spectrogram, pitch contour, and audible audio) of the two source audio and the target audio in the ***Comparison View***, (3) presents two conversion animations wherein random noise undergoes gradual refinement to transform into the singing voice of the target singer in the ***Step View***.

This mode is only available in the advanced version.

-  **Song Comparison**

This mode focuses on the pair-wise comparison of converting \uline{two different source audios} that are derived from the same singer but contain different songs to the same target singer.

It (1) allows users to select a source singer, a target singer but two songs, (2) provides the details (including Mel spectrogram, pitch contour, and audible audio) of the two source singers' audio and the target singer's audio in the ***Comparison View***, (3) supplies two conversion animations illustrating the progressive refinement of random noise into the singing voice of the target in the ***Step View***.

This mode is only available in the advanced version.

-  **Target Singer Comparison**

This mode focuses on the pair-wise comparison of converting the same source singing voice (also means the same song) to \uline{two different target singers}.

It (1) enables users to select a source singer and a source song, but two target singers, (2) provides the details (including Mel spectrogram, pitch contour, and audible audio) of the source singer's audio, and two target singers' audio in the ***Comparison View***, (3) provides the two corresponding conversion animations of random noise gradually refined to the target singer singing voice in the ***Step View***.

This mode is only available in the advanced version.

% 
-  
**Source Singer/Source Song/Target Singer**

Three drop-down boxes offer users options for source singer, source song, and target singer.

% 
-  
**Projection Embedding**

A drop-down box to enable users to choose different projection embeddings from different layers.

Then, the system displays 2D t-SNE visualization results of the high-dimensional diffusion steps in the ***Projection View***.

Specifically, the projection embedding can be the diffusion steps, the combined embeddings of the step and noise, or step, noise and conditions.

These embeddings can come from the first, middle, or final residual layer in the diffusion model, as illustrated in Fig.~[fig:diffwavenetsvc](#fig:diffwavenetsvc) of [app:svc_model](#app:svc_model).

% 
-  
**Components**

Two checkboxes, labeled 'F0 contour' and 'Frequency,' allow users to control the display of these components in the Mel spectrogram.

Additionally, the frequency bar lets users adjust the frequency range for display.

% 
-  
**Step Controller**

The Step Controller includes (1) a step slider to smoothly control the diffusion step, (2) a tool-tip to display or input a specific step number, and (3) a button named `Pin' that enables users to add a specific step's generated result in the ***Comparison View***.

### Step View

This view enables users to visualize the whole generation process of diffusion in the context of SVC tasks, which means users can observe how the spectral characteristics change over time as noise is subsequently removed, leading to the desired SVC.

Specifically, it can be observed that the Mel spectrogram transitions from being completely noisy to gradually becoming clearer, and the fundamental frequency curve also transforms from scattered points into a smooth curve.

The audio also undergoes a process of gradual optimization from being pure noise to having improved sound quality and intelligibility. 

The control panel, mentioned earlier, allows users to interact with the diffusion process by smoothly sliding the step slider.

Users can adjust the diffusion time step to observe the intermediate results of the generation process, enlarge the Mel spectrogram to observe detailed information through a brush operation, and restore it back to the original Mel spectrogram using the refresh button in the top right corner in the ***Step View***.

In the *Step Comparison* and *Condition Comparison* modes, the content presented in the ***Step View*** is slightly different.

In the *Step Comparison* mode, we focus on comparing and analyzing the converted results from different steps, so only one diffusion process animation is displayed in the view.

While, in the *Condition Comparison* mode, the main objective is to compare the conversion results under different conditions, e.g., source singer, song, and target singer.

At this time, the ***Step View*** shows pair-wise diffusion process animations for two different conditions.

![](figs/metrics_with_notes.pdf)

<a id="fig: metric_curve ">The left part is the ***Metric View*** with MCD metric selected.

The right part is the corresponding "Metric Curve over Diffusion Step" for the best-performing sample on the MCD metric.

The red annotation in the right part explains the tendencies of metric curves.
% \lmxue{update this figure}</a>

### Comparison View

To facilitate a more convenient and detailed observation of the intermediate results generated by the diffusion model, we introduce a ***Comparison View***.

Moreover, the comparison view differs between the basic and advanced versions.

In the basic version, the comparison view initially displays a step comparison matrix, highlighting differences in Mel spectrograms between pairs of steps in the diffusion model, as shown in Fig.~[fig:explainer_system](#fig:explainer_system).

Darker colors in the step comparison matrix indicate larger differences, while lighter colors represent smaller ones.

Users can add specific steps to the matrix using the pin feature in the control panel or by clicking data points in the projection view.

By clicking on the comparison matrix, Mel spectrograms and audio of the corresponding two steps can be displayed in the comparison view for detailed comparison.

In the advanced version, we directly display three Mel spectrograms from three steps by default.

Besides, users can select any step to replace the displayed three steps.

It enables users to compare differences among three steps, broadening the scope of comparison.

It is noted that along with the Mel spectrogram, the corresponding audible audio, and fundamental frequency (F0) contour are also displayed in the Comparison View.

All the information related to a clip of audio forms a basic block referred to as the "basic display unit'', as shown in the below two Mel spectrograms in the comparison view in Fig.~[fig:explainer_system](#fig:explainer_system)

On this basic display unit, we can observe the range of the F0 and the pattern of the F0 contour.

Through the brush operation, we can synchronously magnify all Mel spectrograms illustrated in this view, thus enabling a more detailed comparison and examination of the spectral differences.

When there is more than one basic display unit, users can select the checkboxes in the top left corner of any two basic display units.

The page will then pop up the visualization of the difference in the Mel spectrogram between these two basic display units, 

allowing for a clearer and more convenient comparison.

Specifically, the differences are represented by colors.

Warmer colors like reds and oranges signify larger differences, while cooler colors like blues and greens represent smaller differences between the two selected Mel spectrograms.

This visualization aids in identifying which parts of the Mel spectrogram are significantly refined during the step-by-step generation process, highlighting areas that may require further investigation by algorithm researchers.

Furthermore, the components displayed in the ***Comparison View*** differ between the *Step Comparison Mode* and the *Condition Comparison Mode*.

The *Step Comparison Mode* is primarily used to compare the results of different diffusion steps.

In this mode, the ***Comparison View*** will display basic display units from three different steps, based on the steps selected by the user.

The relative position of different basic display units (corresponding to different steps) can be directly adjusted by dragging.

On the other hand, the *Condition Comparison Mode* sports a similar layout but mainly compares the results of SVC under different conditions.

In this mode, the ***Comparison View*** primarily displays the basic display units corresponding to different audios of the source and target singers selected by users.

Additionally, in *Metric Comparison Mode*, the ***Comparison View*** illustrates the metric curve over the diffusion step, which is described in Section~[sec:metric_view](#sec:metric_view).

### Projection View

\label{sec:projection-view}

High-dimensional hidden features can be challenging to interpret directly. t-SNE reduces the dimensionality by projecting the hidden feature embedding into a lower-dimensional space, allowing researchers to gain insights into the intricate structure and relationships within the high-dimensional space.

By projecting high-dimensional step embeddings in the diffusion model into a lower-dimensional space, t-SNE reveals patterns and trajectories of the diffusion steps, enabling a visual exploration of the dynamic evolution of the diffusion process.

Consequently, we design ***Projection View*** to present the two-dimensional space obtained by projecting high-dimensional diffusion step embeddings (i.e., the step features in Fig.~[fig:diffwavenetsvc](#fig:diffwavenetsvc) of [app:svc_model](#app:svc_model)), as shown in Fig.~[fig:explainer_system](#fig:explainer_system).

Each point represents a diffusion step, and all 1000 diffusion steps together form a trajectory in space.

The boundary of this trajectory is highlighted with a gradient color scheme ranging from blue to red, reflecting the progression of the generation process.

Users can hover their mouse over the points and slide to inspect the step trajectory.

While sliding the mouse, users can simultaneously observe the SVC results transition from a coarse state to a fine state in ***Step View***. 

By scrolling the mouse wheel, they can zoom in or out on the points in the space to explore the distribution of the data points.

By clicking on a specific step point, a basic display unit corresponding to the step will be added into the ***Comparison View***. 

As described in Section~[sec:control_panel](#sec:control_panel), the drop-down menu of projection embedding in the control panel provides multiple projection embedding sources, including not only the vanilla diffusion step but also the combination of the diffusion step with noise and condition, as indicated by the red solid dots in Fig.~[fig:diffwavenetsvc](#fig:diffwavenetsvc) of [app:svc_model](#app:svc_model).

By examining the projection embedding results of combining diffusion step with noise and condition, users can compare the differences in diffusion step trajectories under different condition scenarios.

Additionally, we propose a novel sampling strategy called interval clustering center sampling.

The specific steps are as follows: (a) Set the number of steps to be sampled, denoted as $S$. (b) Divide the total interval into T/S sub-intervals, each sampling one sample. (c) Perform k-means clustering on all samples within each sub-interval to find the central point, and then calculate the distance from all samples to this center, and finally select the sample closest to the center as the representative sample.

The clustering approach ensures that each sub-interval selection considers global temporal embedding information.

Furthermore, by clustering and selecting the step closest to the center, the chosen steps are highly representative. 

### Metric View

\label{sec:metric_view}

***Metric View*** is designed to show the overall objective metrics evaluated on the model.

The five metrics, including Dembed, F0CORR, FAD, F0RMSE, and MCD, are divided into two groups based on whether the values of the metrics are positively or negatively correlated with model performance and drawn in histograms.

Here, the labels of the x-axis denote different metrics, and the labels of the y-axis are scores (the higher the better) and log scores (the lower the better).

Each bar in the histogram is labeled with the calculated result corresponding to the metric.

In the top right corner of this view, there is a button represented by a question mark.

When users click this button, a tip box will appear providing descriptions of the definitions of each metric.

The ***Metric View*** represents an average of all samples within the testing data pool, providing a comprehensive overview of the model performance with five objective metrics.

Upon hovering over a particular metric, the system automatically identifies and selects the best-performing sample.

This selection triggers a detailed visualization of the diffusion step for that sample within the ***Projection View***.

Additionally, for the chosen sample, the system dynamically computes and displays evaluation metrics, which are then used to plot the "Metric Curve over Diffusion Step" in ***Comparison View***, as shown in Fig.~[fig: metric_curve ](#fig: metric_curve ). 

At the top of the curve, five legends denoted as five different metrics are present with distinct colors.

The x-axis shows different steps ranging from 999 to 0 as diffusion generates data, and the y-axis displays scores for evaluation metrics.

The user can check the specific metric value for each step by hovering on the curve.

Also, the step preview will update as the cursor moves on the curve.

The interactive feature of ***Metric View*** allows users to not only see aggregate metric performance but also delve into the 
variation trend of metrics with diffusion step during the diffusion generation process.

### Implementation Details

The web application is designed to provide an interactive and user-friendly interface for visualizing spectrogram differences.

It uses D3.js and TailwindCSS for the front-end, ensuring a clean and dynamic user interface.

Specifically, D3.js handles the visualization, allowing for detailed and interactive spectrogram comparisons.

TailwindCSS ensures a responsive and aesthetic design, enhancing user experience.

The back-end is powered by Flask and Gunicorn, enabling efficient dynamic step sampling and efficient data retrieval with multiple workers.

Specifically, Flask serves as the core framework, managing API requests and data processing.

Gunicorn operates as the WSGI HTTP server, providing concurrency through multiple workers for fast data retrieval and processing.

This architecture ensures that the application is both robust and scalable, capable of handling real-time spectrogram analysis and visualization efficiently.

Mel spectrogram and Fundamental Frequency  Contour (F0 contour) are extracted from the audio using a signal processing algorithm.

Mel spectrogram is a 2D representation with the dimensions of Time*Channel, where the time axis captures the progression of the audio signal over time, and the channel axis represents the frequency components or Mel bins, providing a comprehensive view of the signal's spectral content.

The Mel spectrogram is color-coded to indicate the intensity or magnitude of different frequencies over time.

Bright colors, such as yellow and red, represent high energy or the presence of specific frequencies, while darker colors represent lower energy or the absence of those frequencies.

F0 contour is a key concept in the fields of speech processing and music analysis, especially in the study of prosody, intonation, and melody.

It refers to the variation in the pitch of a voice over time.

The fundamental frequency, or F0, is the lowest frequency of a periodic waveform and determines the pitch of the sound, which is one of the primary auditory attributes used to distinguish different sounds in speech and music.

This contour line may be drawn as a continuous curve that rises and falls to depict changes in F0.

The F0 contour line is colored red in this work, to distinguish it from the Mel spectrogram.

## 6·Case Study

We invited two beginners in machine learning and signal processing, E1 and E2, to participate in a case study to verify whether the system could make the model interpretable and help beginner users understand the working mechanism of the diffusion model applied in SVC tasks.

E1 focused on the step view, observing the transition of the Mel spectrogram from noisy to clean.

Initially, the Mel spectrogram appeared **chaotic with many random patterns, lacking clear, distinct features**.

As the process continued from step 999 to step 0, the spectrogram **gradually became clearer, displaying sharp lines representing fundamental frequencies and harmonics**.

Correspondingly, **the initial audio sounded indistinct and lacked clarity, presenting hissing and other unwanted sounds**.

Eventually, **the vocals became well-defined and easy to discern, with almost no unwanted sounds or interference**.
*E1 commented that this dynamic display intuitively demonstrated the entire process of SVC, making the generation process more interpretable and comprehensible.*

Moreover, *E1 mentioned that listening to the voice transition from one blurred timbre to another clear timbre was quite fascinating.*

E2 mainly interacted with the system by dragging the step axis to control the diffusion reverse step, observing the differences in the generated results at various steps.

E2 also focused on the metric view.

E2 clicked on the help button shaped like a question mark in the top right corner of the metric view (as shown in Fig.~[fig:explainer_system](#fig:explainer_system)) to learn about the definitions of metrics and their correlation with model performance.

E2 then clicked on the MCD metric bar, prompting the system to show five Metric Curves over Diffusion Steps in the Comparison View.

E2 moved the mouse over the MCD metric curve and the system displayed the corresponding MCD value,  Mel spectrogram and audio of the corresponding step.

Additionally, E2 listened to the corresponding audio at different steps, providing an audible perception of the changes.

E2 observed that all metric values changed from the starting point to a gradually stabilizing endpoint throughout the diffusion process. \textit{E2 mentioned that this was the first time they directly observed the fluctuations of metrics throughout the diffusion process.

E2 described the system as a comprehensive and user-friendly visualization tool for diffusion models in SVC tasks that allows for both an overview and a detailed study. 

}
