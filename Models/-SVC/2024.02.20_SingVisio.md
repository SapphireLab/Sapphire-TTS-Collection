# SingVisio
# SingVisio: Visual Analytics of Diffusion Model for Singing Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "SingVisio: Visual Analytics of Diffusion Model for Singing Voice Conversion."
- 作者:
  - 01 Liumeng Xue
  - 02 Chaoren Wang
  - 03 Mingxuan Wang
  - 04 Xueyao Zhang
  - 05 Jun Han
  - 06 Zhizheng Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2402.12660v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2402.12660v1](_PDF/2024.02.20_2402.12660v1_SingVisio__Visual_Analytics_of_Diffusion_Model_for_Singing_Voice_Conversion.pdf)
  - [ArXiv:2402.12660v2](_PDF/2024.02.20_2402.12660v2_SingVisio__Visual_Analytics_of_Diffusion_Model_for_Singing_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

In this study, we present SingVisio, an interactive visual analysis system that aims to explain the diffusion model used in singing voice conversion.
SingVisio provides a visual display of the generation process in diffusion models, showcasing the step-by-step denoising of the noisy spectrum and its transformation into a clean spectrum that captures the desired singer's timbre.
The system also facilitates side-by-side comparisons of different conditions, such as source content, melody, and target timbre, highlighting the impact of these conditions on the diffusion generation process and resulting conversions.
Through comparative and comprehensive evaluations, SingVisio demonstrates its effectiveness in terms of system design, functionality, explainability, and user-friendliness.
It offers users of various backgrounds valuable learning experiences and insights into the diffusion model for singing voice conversion.

\begin{keyword}
Machine Learning, Explainable AI, Visual Analytics, Audio Processing
\end{keyword}

## 1·Introduction

Deep generative models have become increasingly prevalent in a myriad of data generation tasks, ranging from image generation to audio generation.

Among these, diffusion-based generative models have emerged as a cutting-edge research focus and the go-to methodology for such applications[^Yang2023Diffusion].

In the field of computer vision, diffusion models have gained significant popularity[^Zhang2023Text-to-Image], [^Xing2023Survey], particularly in applications such as text-to-image synthesis[^Xu2023Versatile], [^Rombach2022High-Resolution], [^Rombach2022High-Resolution], video generation[^Xing2023Survey] and editing[^Ceylan2023Pix2video].

In the audio community, there have been extensive studies of diffusion models in waveform synthesis[^Chen2021WaveGrad], [^Kong2020DiffWave], sound effects generation[^Liu2023AudioLDM], [^Huang2023Make-an-Audio], speech generation[^Popov2021Grad-TTS], [^Shen2024NaturalSpeech], and music generation[^Liu2022DiffSinger], [^Schneider2023Moûsai].

Given their wide-ranging utility and impressive performance, there is a burgeoning curiosity and necessity to unravel the intricacies of the diffusion process underpinning these generative tasks.

However, the complexity of the involved Markov chains and their complex mathematical formulations pose a significant hurdle to novices in the field.

In recent years, visual and interactive methodologies have proven instrumental in deciphering the structures and working mechanisms in various deep-learning models [^Kahng2018Gan], [^Lee2023Diffusion], [^Park2024Explaining].

This insight has spurred us to develop interactive visual tools aimed at broader audiences, facilitating a deeper comprehension of diffusion-based generative models.

The paper represents an attempt to demystify the diffusion-based generative paradigm.

Owing to its notable capabilities, the diffusion-based generative model has quickly risen as a formidable contender in singing voice conversion (SVC).

This advanced technique effectively alters one singer's voice to another's, meticulously preserving the song's original content and melody, as investigated in the studies[^Liu2021DiffSVC], [^Zhang2023Leveraging], [^Lu2024CoMoSVC].

When juxtaposed with other generative models, such as Generative Adversarial Networks (GANs)[^Goodfellow2020Generative] and Variational Auto-Encoders (VAEs)[^Kingma2014Auto-Encoding], diffusion-based models resolve the issue of unsatisfactory audio quality via incrementally introducing noise into the data and iteratively learning to eliminate noise.

Due to iterative noising and denoising processes in synthesizing high-quality data, comparing the changes in the diffusion process step-by-step is essential to learn about the diffusion model.

The current pedagogical approaches for beginners\footnote{In the context of this study, "beginners'' are defined as individuals who have less than one year of experience in both the field of machine learning and the field of music and singing processing.

This group primarily consists of users who are new to both the technical aspects of machine learning and the specific applications in music and singing processing.

We expect the beginners' main focus to be on gaining fundamental knowledge about the diffusion model applied in the SVC in this study.} learning about diffusion-based models is overly dependent on textual explanations and mathematical descriptions\footnote{\url{https://theaisummer.com/diffusion-models/}}.

This traditional learning method is neither intuitive nor efficient, often causing beginners to lose track among complex formulas without the ability to directly view and compare results at each step.

Moreover, understanding the impact of various conditions—such as the source voice's content, melody, and the target singer's unique timbre—on the generation process is crucial for experts to identify challenging samples for SVC and make informed decisions to enhance SVC performance.

Currently, comparing the effects of different conditions on SVC results is both time-consuming and cumbersome.

Researchers must generate and save each feature, such as Mel spectrograms and audio files, and then repeatedly open and compare these across various steps.

Methods involving visualization and exploratory interaction are less common, as evidenced by examples such as [^Sergios2022Diffusion] and [^O'Connor2024Diffusion], which do not offer users an immersive understanding of the diffusion process.

This highlights an urgent demand for comprehensive, interactive, and visually intuitive tools designed for diffusion-based generative models to fill this gap. 
In this paper, we propose SingVisio, a visual analytics system designed to interactively explain diffusion models in SVC.

To maintain anonymity during the review process, the code will be made publicly available upon the paper's acceptance.

SingVisio offers both a basic version to help beginners grasp the basic concepts of diffusion models, and an advanced version for experts by providing an efficient tool to further investigate diffusion-based SVC.

For visual representation, we extract Mel spectrograms and F0 contours from audio. 

Additionally, we demystify the diffusion process by extracting and rendering hidden features from different layers in the model over 1000 steps. 

Furthermore, we propose a novel interval clustering center sampling method, enabling users to flexibly specify the number of sample points and display the corresponding hidden features.

% basic version and advanced version
% both version
% Mel spectrogram and f0 extracted from audio
% metrics 

%% 其中 step comparison mode中，basic version 

The contributions of this work can be summarized as follows:

 

-  **A visual analytics system for understanding SVC.**
To the best of our knowledge, this is the first system supporting the exploration, visualization, and comparison of the diffusion model within the context of SVC.

It offers a versatile platform for comparing various aspects of the diffusion process, SVC modes, and evaluation metrics, allowing for a thorough exploration.

-  **Novel interactive exploration approach to understanding diffusion-based SVC.**
We have supported three core interactive exploration modes within our system: {\bf data-driven} exploration, which is steered by varying melodies, {\bf condition-driven} exploration that pivots on the specific inputs provided to the diffusion model, and {\bf evaluation-driven} exploration, which is based on the assessment metric.

Also, we propose a novel interval clustering center sampling method to efficiently sample and display hidden features at specified steps.
% These interactive modes are thoughtfully designed to aid users in comprehending and navigating the diffusion process integral to SVC. 

% \textcolor{blue}{
% 
-  **Novel sampling strategy to efficiently explore hidden features in the diffusion model**
% We introduce a novel diffusion sampling strategy called interval clustering center sampling.

This method enables us to sample and display hidden features at specified steps within the diffusion model.

By clustering features and selecting representative centers from each interval, users can efficiently explore and visualize the evolution of hidden features.
% }

-  **A comparative and comprehensive evaluation of SingVisio.**
We conducted a comparative and comprehensive evaluation of our system with the basic version and advanced version, including a case study involving two beginners, an expert study with two experts, and a formal user study encompassing both subjective and objective assessments for general users.

Such evaluation shows the effectiveness of our system.
