# VibE-SVC: Vibrato Extraction With High-Frequency F0 Contour for Singing Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "VibE-SVC: Vibrato Extraction With High-Frequency F0 Contour for Singing Voice Conversion."
- 作者:
  - 01 Joon-Seung Choi
  - 02 Dong-Min Byun
  - 03 Hyung-Seok Oh
  - 04 Seong-Whan Lee
- 链接:
  - [ArXiv](https://arxiv.org/abs/2505.20794v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2505.20794v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.05.27_2505.20794v1_VibE-SVC__Vibrato_Extraction_With_High-Frequency_F0_Contour_for_Singing_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

Controlling singing style is crucial for achieving an expressive and natural singing voice.
Among the various style factors, vibrato plays a key role in conveying emotions and enhancing musical depth.
However, modeling vibrato remains challenging due to its dynamic nature, making it difficult to control in singing voice conversion.
To address this, we propose VibE-SVC, a controllable singing voice conversion model that explicitly extracts and manipulates vibrato using discrete wavelet transform.
Unlike previous methods that model vibrato implicitly, our approach decomposes the F0 contour into frequency components, enabling precise transfer.
This allows vibrato control for enhanced flexibility.
Experimental results show that VibE-SVC effectively transforms singing styles while preserving speaker similarity.
Both subjective and objective evaluations confirm high-quality conversion. 

## 1·Introduction

Singing voice conversion (SVC) is a technique that converts a source singer voice into a target singer voice while retaining the source lyrics, melody, and styles.

Recently, many singing voice models have been developed using various generative models [^Nachmani2019Unsupervised], [^Zhang2022VISinger], [^Liu2022Diffsinger], [^Liu2021Diffsvc], [^Byun2024Midi-Voice].

Although SVC has improved significantly, several challenges remain.

One of the most important challenges is effectively handling pitch.

Since the singing voice is more expressive than speech, accurate pitch modeling is essential.

For this reason, several studies [^Ning2023Vits-Based], [^Bai2024Spa-SVC] have been proposed to improve pitch-related performance.

SVCC-T23 [^Ning2023Vits-Based] extracts multi-scale F0 as an auxiliary input to better capture pitch variation in singing.

SPA-SVC [^Bai2024Spa-SVC] introduces a cycle pitch shifting strategy to mitigate voiceless regions and hoarse artifacts caused by narrow pitch range of input dataset. 

Some works [^Liu2021Vibrato], [^Zhang2024TCS}inger], [^Zhao2024SinTechSVS] have focused on handling style characteristics of singing, including pitch styles.

Vibrato control is achieved in [^Liu2021Vibrato] by extracting vibrato extent from the power spectrogram of the first-order difference.

To synthesize and control singing styles, SinTechSVS [^Zhao2024SinTechSVS] proposes style recommender and singing technique local score module.

TCSinger [^Zhang2024TCS}inger] introduces clustering style encoder to capture singing styles. 
Unlike these methods which focus on synthesizing singing styles, other approaches [^Hsu2025Many-to-Many], [^Luo2020Singing], [^Chen2023Few], [^Wang2022Towards] focus on transferring singing styles. 
Since style features are not explicitly defined, these methods disentangle style information implicitly.

Extracting information via signal decomposition is a key focus in various deep learning studies [^Lim2000Text], [^Kim2015Abstract].

Discrete wavelet transform is a method for decomposing an arbitrary signal into functions defined by discretely sampled wavelets.

Wavelet transform has been utilized in various methods [^Lee1996Multiresolution], [^Ren2021FastSpeech], [^Noyum2021Boosting], [^Lee2022Fre-Gan] such as pitch representation [^Ren2021FastSpeech], singer identification [^Noyum2021Boosting], or a downsampling method [^Lee2022Fre-Gan]. 
We assume that while the overall F0 contour remains consistent across singing styles, style-related variations are reflected in the high-frequency contour.

To capture these differences, we use DWT to decompose the F0 contour into low- and high-frequency bands.

Most existing methods aim to smooth the F0 contour to eliminate minor fluctuations or singing styles using filters such as median filter [^Kim2024Period] or band-pass filter [^Song2022Singing]. 
In contrast, our method employs DWT as a pass filter to disentangle and control singing styles.

In this work, we propose VibE-SVC, which disentangles vibrato style from singing voices and enables style transfer using DWT.

By predicting the high-frequency F0 contour, VibE-SVC achieves transfer between straight and vibrato styles.

Our approach demonstrates that DWT effectively separates singing styles and allows vibrato extent control without explicitly modeling the vibrato extent feature.

Experimental results show that VibE-SVC successfully transfers singing styles.

Audio samples are available at \url{https://castlechoi.github.io/VibE-SVC-demo}.

![](Figure/architecture_svc.pdf)

<a id="fig:svc_architecture">VibE-SVC</a>

## 2·Method

We propose a controllable SVC model that enables style conversion between straight and vibrato.

To separate style-related variations, we employ a DWT-based method to decompose the F0 contour into low- and high-frequency contours.

The high-frequency contour is then predicted by a pitch style converter to perform singing style conversion.

An overview of the VibE-SVC framework is shown in Figure [fig:overall_architecture](#fig:overall_architecture).

### Vibrato disentanglement

We use DWT to decompose a signal into approximation and detail coefficients, corresponding to the low- and high-frequency components.

Approximation coefficient $A$ and detail coefficient $D$ are defined as follows:

$$
\begin{aligned}

A_j[k] &= \sum_nx[n]\phi_{j,k}[n], \label{equation:dwt_approx} \\
D_j[k] &= \sum_nx[n]\psi_{j,k}[n],\label{equation:dwt_detail}

\end{aligned}
$$

where $x[n]$ denotes source signal. 
$j$, $k$, and $n$ denote the decomposition level of DWT, the position of wavelet function, and the position of signal $x$, respectively. 
$\phi$ and $\psi$ denote the scaling function and the wavelet function, respectively.

To reconstruct the low- and high-frequency signals from the corresponding coefficients, we use the inverse discrete wavelet transform (iDWT) as follows:

$$
\begin{aligned}

x[n] &= \sum_kA_{L}[k]\phi_{L,k}[n]
+ \sum_{j=1}^{L}\sum_kD_j[k]\psi_{j,k}[n], 
\label{equation:iDWT}

\end{aligned}
$$

where $L$ denotes the lowest frequency DWT level. 
Based on Equation [equation:iDWT](#equation:iDWT), we reconstruct low- and high-frequency F0 contours from the approximation coefficient and detail coefficients.

As shown in Figure [fig:dwt_analysis](#fig:dwt_analysis), high-frequency F0 contour $x_{high}$ and low-frequency F0 contour $x_{low}$ are defined as follows:

$$
\begin{aligned}

x_{high}[n] &= \sum_kA_{L}[k]\phi_{L,k}[n], \label{equation:iDWT_high}\\
x_{low}[n] &= \sum_{j=1}^L\sum_kD_j[k]\psi_{j,k}[n].\label{equation:iDWT_low}

\end{aligned}
$$

Based on Equations [equation:iDWT_high](#equation:iDWT_high) and [equation:iDWT_low](#equation:iDWT_low), the source F0 contour is reconstructed by low- and high-frequency F0 contours as follows:

$$
\begin{aligned}

x[n] &= x_{low}[n] + x_{high}[n]. \label{equation:iDWT_recon}

\end{aligned}
$$

We adopt the Daubechies1 (db1) wavelet function [^Daubechies1992Ten] to enhance the extraction of consistent vibrato extent and facilitate model training.

In DWT, the db1 wavelet function produces a rectangular function that aligns well with the characteristics of the musical instrument digital interface (MIDI).

### Pitch style converter

As shown in Figure [fig:converter_architecture](#fig:converter_architecture), we use log F0 contour to normalize the vibrato scale across frequencies, and decompose it into frequency components using DWT.

The low-frequency F0 contour, voice flag vector, and style embedding obtained from the style lookup table are concatenated and used as input to the style encoder to predict the high-frequency F0 contour as the target.

After prediction, the log F0 contour converted to the target style is reconstructed by Equation [equation:iDWT_recon](#equation:iDWT_recon).

The predicted log F0 contour is then denormalized and fed into the diffusion decoder.

Since the vibrato rate is typically characterized by a frequency range of 5 to 8 Hz [^Liu2021Vibrato], [^Nakano2006Automatic], we adopt a multi-period discriminator (MPD) [^Kong2020HiFi-Gan] to capture the periodic characteristics in the high-frequency F0 contour.

The predicted and ground-truth high-frequency log F0 contour are used as the input to the MPD.

We train the model using reconstruction loss $L_{recon}$ for log F0 contour, adversarial loss $L_{adv}$ [^Mao2017Least],
and feature matching loss $L_{fm}$.

The training loss functions are defined as follows: 

$$
\begin{aligned}

L(G) &= L_{recon}(G)+\mathcal L_{fm}(G)+\mathcal L_{adv}(G), \label{equation:generator_loss}\\
L(D) &= L_{adv}(D), \label{equation:discriminator_loss}

\end{aligned}
$$

where $G$ denotes the generator and $D$ denotes the discriminator.

![](Figure/dwt_orig_f0.pdf)

<a id="fig:orig_pitch">Source F0 contour</a>

### Overall architecture

As shown in Figure [fig:overall_architecture](#fig:overall_architecture), VibE-SVC consists of two main components: the SVC model and the pitch style converter, which are trained separately.

The SVC model generates Mel-spectrograms conditioned on HuBERT [^Hsu2021HuBERT] units, speaker embeddings, the F0 contour, and volume.

The HuBERT units are interpolated to align with the F0 contour, while the volume is computed as the squared magnitude of the input audio.

The SVC model is optimized with a simple diffusion loss, whereas the pitch style converter is trained according to the method described in Section 2.2.

For training, the source speaker embedding and source F0 contour are used as inputs.

During inference, we provide the target style embedding to predict the style-transferred F0 contour.

Then, we provide the target speaker embedding and the predicted F0 contour as input to the SVC model.

To adjust the pitch range, we scale the F0 contour by the ratio of the mean F0 values of the source and target speakers.

The mean F0 value for each speaker is computed in advance.

<a id="tab:main_table">Comparison results of style transfer.

The MOS and SMOS are presented with 95\% confidence intervals.</a>

![](Figure/exp_corr_style_only.pdf)

<a id="fig:style_only_conversion">Style-Only conversion</a>
