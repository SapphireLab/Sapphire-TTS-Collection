# CoMoSVC: Consistency Model-Based Singing Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "CoMoSVC: Consistency Model-Based Singing Voice Conversion."
- 作者:
  - 01 Yiwen Lu
  - 02 Zhen Ye
  - 03 Wei Xue
  - 04 Xu Tan
  - 05 Qifeng Liu
  - 06 Yike Guo
- 链接:
  - [ArXiv](https://arxiv.org/abs/2401.01792v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2401.01792v1](_PDF/2024.01.03_2401.01792v1_CoMoSVC__Consistency_Model-Based_Singing_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

The diffusion-based Singing Voice Conversion (SVC) methods have achieved remarkable performances, producing natural audios with high similarity to the target timbre.
However, the iterative sampling process results in slow inference speed, and acceleration thus becomes crucial.
In this paper, we propose CoMoSVC, a consistency model-based SVC method, which aims to achieve both high-quality generation and high-speed sampling.
A diffusion-based teacher model is first specially designed for SVC, and a student model is further distilled under self-consistency properties to achieve one-step sampling.
Experiments on a single NVIDIA GTX4090 GPU reveal that although CoMoSVC has a significantly faster inference speed than the state-of-the-art (SOTA) diffusion-based SVC system, it still achieves comparable or superior conversion performance based on both subjective and objective metrics.
Audio samples and codes are available at \href{https://comosvc.github.io/}{https://comosvc.github.io/}.

## 1·Introduction

\label{sec:intro}

Singing Voice Conversion(SVC) aims to convert one singer's voice to another one's, while preserving the content and melody.

It has wide applications in music entertainment, singing voice beautification, and art creation  [^Zhang2023Leveraging]. 

Statistical methods  [^Kobayashi2014Statistical], [^Kobayashi2015Statistical], [^Kobayashi2015Statistical] are applied to the SVC tasks with parallel training data from both the source and target singers, which is usually infeasible, and thus the non-parallel SVC methods have become the mainstream.

Two-stage methods are generally used for SVC, the first stage disentangles and encodes singer-independent and singer-dependent features from the audio.

Then the second decoding stage generates the converted audio by replacing the singer-dependent feature with the target one.

Since the substantial impact of the second stage on the quality of the converted audios, it has become crucial to design and optimize this stage.

Therefore, many generative models have been used for the SVC decoding, including the autoregressive (AR) models, generative adversarial network (GAN), Normalizing Flow, and diffusion models.

AR models are firstly used to develop USVC  [^Nachmani2019Unsupervised], and PitchNet  [^Deng2020Pitchnet] further improves USVC by adding a pitch adversarial network to learn the joint phonetic and pitch representation.

However, AR models are slow due to the recursive nature, then non-AR GAN-based UCD-SVC  [^Polyak2020Unsupervised] and FastSVC  [^Liu2021Fast{SVC] are later proposed.

Since the unstable training of GAN, a flow-based end-to-end SVC system, named SoVITS-SVC  [^SVC-Develop-Team2023SoftVC] received widespread attention for its excellent converted results in fast speed.

Recently, it has been shown that the conversion performance can be substantially improved by the diffusion-based SVC methods such as DiffSVC  [^Liu2021Diffsvc] and the diffusion version of SoVITS-SVC.

However, the iterative sampling process results to the slow inference of the diffusion-based SVC methods.

A new generative model named consistency model  [^Song2023Consistency] has been proposed to realize one-step generation.

Subsequently for speech synthesis, CoMoSpeech  [^Ye2023CoMoSpeech] exploits the consistency model to achieve both high-quality synthesis and fast inference speed.

Inspired by this, a consistency model-based SVC method, named CoMoSVC, is further developed in this paper to achieve {\em high-quality, high-similarity and high-speed}

SVC.

Based on the structure of EDM  [^Karras2022Elucidating], a diffusion-based teacher model with outstanding generative capability is firstly designed, and a student model is further distilled from it to achieve one-step sampling.

Experiments reveal that while the sampling speed of CoMoSVC is approximately 500 and 50 times faster than that of the diffusion-based SoVITS-SVC and DiffSVC respectively, the comparable performance is still retained and some improvements can even be achieved in both quality and similarity.

## 2·Background

The diffusion model generates samples by first adding noise to data during the forward process and then reconstructing the data structure in the reverse process.

We assume that the original data distribution is $p_{data}(\mathbf{x})$, and the forward process can be represented by a stochastic differential equation (SDE)  [^Song2021Score-Based], [^Karras2022Elucidating]:

$$
\mathrm{d}\mathbf{x}_t=\mathbf{f}(\mathbf{x}_t ,t)\mathrm{d}t+g(t)\mathrm{d}\mathbf{w}_t ,
$$

where $\mathbf{w}_t$ is the standard wiener process, $\mathbf{f}(\mathbf{x}_t ,t)$ and $g(t)$ are drift and diffusion coefficients respectively.

With setting $\mathbf{f}(\mathbf{x}_t,t)=0$ and $g(t)=\sqrt{2t}$, the same as the choice in  [^Karras2022Elucidating], the SDE can be defined by:

$$
\mathrm{d}\mathbf{x}_{t}=\sqrt{2t}\,\mathrm{d}\mathbf{w} _{t}. 
$$

The reverse process can also be expressed by a reverse-time SDE  [^Song2021Score-Based]:

$$
\mathrm{d}\mathbf{x}_{t}=-2t\nabla\mathrm{log}\,p_t(\mathbf{x}_t)dt+\sqrt{2t}\,\mathrm{d}\bar{\mathbf{w}}_{t} ,
$$

where $p_t(\mathbf{x}_t)$ is the distribution of $\mathbf{x}_t$, $\nabla\mathrm{log}\,p_t(\mathbf{x}_t)$ is the score function, and $\bar{\mathbf{w}}_{t}$ is the reverse-time standard wiener process.  [^Song2021Score-Based] found that there exists a probability flow (PF) ordinary differential equation (ODE), whose solution trajectories' distribution at time $t$ is the same as $p_t(\mathbf{x}_t)$.

The PF ODE with such property can be represented by

$$
\frac{\mathrm{d} \mathbf{x}_t}{\mathrm{d} t}=-t \nabla \log p_t\left(\mathbf{x}_t\right)=\frac{\mathbf{x}_t-D_\phi\left(\mathbf{x}_t, t\right)}{t},
\label{4}
$$

where $D_\phi$ is the neural network with $\phi$ as parameters to approximate the denoiser function.

Then for sampling, the PF ODE is solved by initiating from $\mathbf{x}_T$, as

$$    \mathbf{x}_0=\mathbf{x}_T+\int_T^0 \frac{\mathbf{x}_t-D_\phi\left(\mathbf{x}_t, t\right)}{t} \mathrm{~d} t.
\label{5}
$$

However, the diffusion model generally needs a large number of iterations to solve the PF ODE, making the sampling slow.

A consistency model [^Song2023Consistency] is proposed for one-step sampling based on the self-consistency property, making any point from the same PF ODE trajectory be mapped to the same initial point.

The self-consistency properties have two constraints: firstly, any pair of points $\mathbf{x}_{t_m}$ and $\mathbf{x}_{t_n}$ will be mapped to the same point, which can be represented by:

$$    D_\phi(\mathbf{x}_{t_m},t_m)=D_\phi(\mathbf{x}_{t_n},t_n).
$$

Secondly, the initial point should also be mapped to itself and this constraint is called the boundary condition.

To avoid numerical instability, it can be given by

$$
D_\phi(\mathbf{x}_\epsilon,t_\epsilon)=\mathbf{x}_\epsilon,
\label{7}
$$

where $\epsilon$ is a fixed small positive number and set as 0.002.
, all the singers have their identification number, which will be encoded as a singer embedding. 

## 3·Proposed Method

CoMoSVC is a two-stage model, where the first stage encodes the extracted features and the singer identity into embeddings.

These embeddings are concatenated and serve as the conditional input for the second stage to generate mel-spectrogram, which can be further rendered to audio by using a pre-trained vocoder.

The training process depicted in Fig.~[a_training](#a_training) takes the waveform and its singer identity as the input to reconstruct the mel-spectrogram, while the inference process illustrated in Fig.~[a_como](#a_como)  replaces the singer identity with the target one to generate the converted mel-spectrogram.

![](training.pdf)

<a id="fig1">The Training Process.</a>

![](training.pdf)

<a id="fig2">The inference process.</a>

### Encoding

This section encodes both singer-independent and singer-dependent features, which can be shown in the upper part of both Fig.~[fig1](#fig1) and Fig.~[fig2](#fig2).

We extract content, pitch, and loudness features to capture singer-independent information in audio, while the singer ID is used as the singer-dependent information.

The content features are extracted by using the pre-trained acoustic model ContentVec  [^Qian2022Content{Vec] and the large dimensionality of these features allows for enhancing the clarity of lyrics in the converted audio.

To represent pitch information, we use the widely-used and classical F0 estimator DIO  [^Morise2009Fast].

The squared magnitude of the audio signal is calculated as the loudness feature.

After feature extraction, we applied a linear layer to all the embeddings to unify the dimensions and concatenate them to form the conditional input for the decoding stage.

### Decoding

This stage is the key component of CoMoSVC, during which the mel-spectrograms can be generated from the conditional input.

A teacher model is first trained and then a student model is distilled from it, which will be introduced in section.~3.2.1 and section.~3.2.2 respectively.

The sampling process of both the teacher model and student model will be explained in section.~3.2.3.

#### Teacher Model

We use the architecture of EDM  [^Karras2022Elucidating] as the teacher model to train the denoiser function $D_\phi$ due to its high generative ability.

Moreover, the structure of $D_\phi$ used here is the non-causal Wavenet  [^Rethage2018Wavenet].

We use $\mathbf{x}_0\sim p_{data}(\mathbf{x})$ and $\mathit{cond}$ to denote the ground truth mel-spectrogram and the conditional input.

According to \eqref{4}, the empirical ODE is 

$$
\frac{\mathrm{d} \mathbf{x}_t}{\mathrm{d} t}=\frac{\mathbf{x}_t-D_\phi\left(\mathbf{x}_t, t,\mathit{cond}\right)}{t},
$$

where $\mathbf{x}_t=\mathbf{x}_0+t*\mathcal{N}(0,\boldsymbol{I})$, represents the result after adding noise.

Similar to  [^Karras2022Elucidating], we use a different network $F_\phi$ instead of directly approximating the denoiser function by $D_\phi$.

The network is preconditioned with a skip connection to make the estimation more flexible, it can be given by

$$
D_\phi(\mathbf{x}_t,t,\mathit{cond})=c_{\text {skip }}(t) \mathbf{x}_t+c_{\text {out}}(t) F_\phi\left(c_{\text {in }}(t) \mathbf{x}_t,t, c_{\text {noise }}(t)\right).
$$

$c_{\text {skip}}(t)$ modulates the skip connection, $c_{\text {in }}(t)$ and $c_{\text {out }}(t)$ scale the magnitudes of $\mathbf{x}_t$ and $F_\phi$ respectively, and $c_{\text {noise }}(t)$ maps noise level $t$ into a conditioning input for $F_\phi$.

To satisfy the boundary condition mentioned in \eqref{7} and ensure $c_{\text {skip }}(t)$ and $c_{\text {out }}(t)$ differential, we choose

$$
c_{\text {skip }}(t)=\frac{\sigma_{\text {data }}^2}{(t-\epsilon)^2+\sigma_{\text {data }}^2}, \quad c_{\text {out }}(t)=\frac{\sigma_{\text {data }}(t-\epsilon)}{\sqrt{\sigma_{\text {data }}^2+t^2}} ,
$$

where $\sigma_{\text {data }}$ is the standard deviation of $p_{data}(\mathbf{x})$.

The loss function $\mathcal{L}_\phi$ used to train the $D_\phi$ can be designed by

$$
\mathcal{L}_\phi=\mathbb{E}[\lambda(t)\| D_\phi\left(\mathbf{x}_t, t, cond\right)-\mathbf{x}_0 \|^2] ,
$$

where $\lambda(t)=(t^2+\sigma_{\text {data }}^2)/(t\cdot\sigma_{\text {data }} )^2$, denotes the weight corresponding to different noise level $t$.

The entire procedure is depicted in the lower left section in Fig.~[fig1](#fig1).

\begin{algorithm}[t]
\caption{Training procedure}\label{a_training}

\begin{algorithmic}[1]
\Statex **Input:**  The denoiser function $D_\phi$ of the teacher model; the conditional input $cond$; the original data distribution $p_{data}$ and $\mu$
\State **repeat**
\State Sample $n \sim \mathcal{U}(1,N-1)$ and $\mathbf{x}_0 \sim p_{data}$
\State Sample $\mathbf{x}_{t_{n+1}} \sim \mathcal{N}(\mathbf{x}_0,(t_{n+1})^2*I)$ 
\State $\hat{\mathbf{x}}_{t_n}^\phi \leftarrow \frac{t_n}{t_{n+1}}\mathbf{x}_{t_{n+1}}+ \frac{t_{n+1}-t_n}{t_{n+1}}D_\phi\left(\mathbf{x}_{t_{n+1}}, t_{n+1},cond\right)$
\State $\mathcal{L}_\theta\leftarrow d\left(D_\theta\left(\mathbf{x}_{t_{n+1}}, t_{n+1},cond\right),D_{\theta^{-}}\left(\hat{\mathbf{x}}_{t_n}^\phi, t_n, cond\right)\right)$
\State $\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}-\eta \nabla_{\boldsymbol{\theta}} \mathcal{L}\left(\boldsymbol{\theta},\boldsymbol{\theta}-;\boldsymbol{\phi}\right)$
\State $\boldsymbol{\theta}^{-} \leftarrow \operatorname{stopgrad}\left(\mu \boldsymbol{\theta}^{-}+(1-\mu) \boldsymbol{\theta}\right)$
\State **until** convergence

\end{algorithmic}

\end{algorithm}

\begin{algorithm}[t]
\caption{Sampling procedure}\label{a_como}

\begin{algorithmic}[1]
\Statex **Input:**  The denoiser function $D_\theta$ of the consistency model; the conditional input $cond$ ; a set of time points $t_{i\in\{0,\ldots,N\}}$
\State Sample $\mathbf{x}_{N} \sim \mathcal{N}(0,\sigma(t_N)^{2}*I)$
\State  $\mathbf{x} \leftarrow D_{\theta}(\mathbf{x}_{N},t_{N},cond)$ 

\State  **if**  one-step synthesis
\State \hskip1.0em **Output:** $\mathbf{x}$
\State  **else**            multi-step synthesis
\State \hskip1.0em **for** $i = N-1$ **to** $1$ **do**                
\State \hskip1.0em  \hskip1.0em  Sample $**z** \sim \mathcal{N}(0,\boldsymbol{I})$
\State \hskip1.0em \hskip1.0em  $\mathbf{x}_{i} \leftarrow  \mathbf{x} +\sqrt{t_{i}^{2}-\epsilon^{2}}{\bf z} $
\State \hskip1.0em \hskip1.0em  $\mathbf{x} \leftarrow D_{\theta}(\mathbf{x}_{i},t_{i},cond)$                       
\State \hskip1.0em **end for**
\Statex \hskip1.0em **Output:** $\mathbf{x}$

\end{algorithmic}

\end{algorithm}

#### Consistency Distillation

A student model can be further distilled from the pre-trained denoiser function $D_\phi$ to ultimately achieve one-step sampling, the process is illustrated in Algorithm.~[a_training](#a_training) and the lower right section of Fig.~[a_training](#a_training).

First, we randomly sample $n$ from the uniform distribution $\mathcal{U}(1,N-1)$ and obtain $\mathbf{x}_{t_{n+1}}$ by adding $t_{n+1}*\mathcal{N}(0,\boldsymbol{I})$ to $\mathbf{x}_0$, then we use the $D_\phi$ to get the one-step estimation $\hat{\mathbf{x}}_{t_n}^{\phi }$ from $\mathbf{x}_{t_{n+1}}$.

According to \eqref{4}, since first-order Euler Solver is used here, it can be given by

$$
\hat{\mathbf{x}}_{t_n}^{\phi } =\frac{t_n}{t_{n+1}}\mathbf{x}_{t_{n+1}}+ \frac{t_{n+1}-t_n}{t_{n+1}}D_\phi\left(\mathbf{x}_{t_{n+1}}, t_{n+1},cond\right) .
$$

The structure of the student model is inherited from the teacher model's denoiser function $D_\phi$, resulting in $D_\theta$ and $D_{\theta^{-}}$.

The parameters $\theta$ and $\theta^{-}$ are initialized with $\phi$, $\theta^{-}$ is a running average of the past values of $\theta$.

Afterwards, we use $D_{\theta^{-}}\left(\hat{\mathbf{x}}_{t_n}^\phi, t_n, cond\right)$ and $D_\theta\left(\mathbf{x}_{t_{n+1}}, t_{n+1},cond\right)$ to obtain different outputs of the pair of adjacent points $\hat{\mathbf{x}}_{t_n}^{\phi }$ and $\mathbf{x}_{t_n}$.

The consistency distillation is trained by minimizing the $L_{2}$  distance between the two outputs: 

$$

$$\begin{aligned}
&d\left(D_\theta\left(\mathbf{x}_{t_{n+1}}, t_{n+1},cond\right),D_{\theta^{-}}\left(\hat{\mathbf{x}}_{t_n}^\phi, t_n, cond\right)\right)\\
&=\| D_\theta\left(\mathbf{x}_{t_{n+1}}, t_{n+1},cond\right)-D_{\theta^{-}}\left(\hat{\mathbf{x}}_{t_n}^\phi, t_n, cond\right) \|^2.
\end{aligned}$$

$$

The parameter $\theta$ is updated by:

$$
\boldsymbol{\theta} \leftarrow\boldsymbol{\theta}-\eta \nabla_{\boldsymbol{\theta}} \mathcal{L}\left(\boldsymbol{\theta},\boldsymbol{\theta}-;\boldsymbol{\phi}\right).
$$

To stabilize the training, the exponential moving average (EMA) update and stop grad are adopted to $\theta^{-}$, as:

$$
\boldsymbol{\theta}^{-} \leftarrow \operatorname{stopgrad}\left(\mu \boldsymbol{\theta}^{-}+(1-\mu) \boldsymbol{\theta}\right),
$$

where $\mu$ is a momentum coefficient, empirically set as 0.95. 

#### Sampling Process

The sampling processes of both the two models are depicted in the lower part of Fig.~[fig2](#fig2).

The teacher model takes a number of iterations for sampling, while the student model can achieve one-step sampling as summarized in Algorithm.~[a_como](#a_como).

We first sample the noise that has the same shape as the mel-spectrogram by $\mathbf{x}_{t_N}=t_N*\mathcal{N}(0,\boldsymbol{I})$, and the output of $D_\theta(\mathbf{x}_{t_{N}},t_N,cond)$ is the sampling result.

The proposed CoMoSVC also supports multi-step sampling by chaining the outputs at multiple time steps.

However, there will be a trade-off between the number of iterations and sampling quality.
