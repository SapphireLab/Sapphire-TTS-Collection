# MeLoDy

<details>
<summary>基本信息</summary>

- 标题: "Efficient Neural Music Generation"
- 作者:
  - 01 Max W.Y.Lam
  - 02 Qiao Tian
  - 03 Tang Li
  - 04 Zongyu Yin
  - 05 Siyuan Feng
  - 06 Ming Tu
  - 07 Yuliang Ji
  - 08 Rui Xia
  - 09 Mingbo Ma
  - 10 Xuchen Song
  - 11 Jitong Chen
  - 12 Yuping Wang
  - 13 Yuxuan Wang
- 链接:
  - [ArXiv](https://arxiv.org/abs/2305.15719)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv]()
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

Recent progress in music generation has been remarkably advanced by the state-of-the-art **MusicLM**, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings.
Yet, sampling with the **MusicLM** requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation.
Efficient music generation with a quality on par with **MusicLM** remains a significant challenge.
In this paper, we present ***MeLoDy (M for music; L for LM; D for diffusion)***, an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in **MusicLM**, respectively, for sampling 10s or 30s music.
***MeLoDy*** inherits the highest-level LM from **MusicLM** for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform.
DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step.
Our experimental results suggest the superiority of ***MeLoDy***, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation.
Our samples are available at this https URL.

</td><td>

最近, 音乐生成领域取得了显著进展, 特别是由最先进的 **MusicLM** 推动的, 该模型由三个层次的语言模型（LM）组成, 分别用于语义建模、粗糙音频建模和细粒度音频建模.
然而, 使用 **MusicLM** 进行采样时, 需要依次通过这些语言模型来获得细粒度的音频标记, 这使得计算开销非常大, 难以支持实时生成.
要实现与 **MusicLM** 相媲美的高质量高效音乐生成仍然是一个重要挑战.

在本文中, 我们提出了 ***MeLoDy (M代表音乐；L代表语言模型；D代表扩散)***, 这是一种基于语言模型引导的扩散模型, 能够生成与最先进技术 **MusicLM** 相媲美的音乐音频, 同时在生成10秒或30秒音乐时分别减少95.7%或99.6%的前向传递计算量.
***MeLoDy*** 继承了 **MusicLM** 中的最高级别语言模型进行语义建模, 并应用了一种新颖的**双路径扩散 (Dual-Path Diffusion, DPD) 模型**和音频 VAE-GAN 来高效地将语义标记解码为波形.
DPD 模型通过在每个去噪步骤中通过交叉注意机制有效地将语义信息融入潜在变量的各个片段, 提出了同时建模粗糙和精细音频的策略.

我们的实验结果表明, ***MeLoDy***在采样速度、无限连续生成的实际优势方面具有明显的优势, 同时在音乐性、音频质量和文本相关性方面也达到了最先进的水平.
我们的样本可以在此网址查看.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td></tr></table>
