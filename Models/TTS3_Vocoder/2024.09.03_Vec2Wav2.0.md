# Vec2Wav 2.0

<details>
<summary>基本信息</summary>

- 标题: ***vec2wav 2.0***: Advancing Voice Conversion via Discrete Token Vocoders
- 作者:
  | 序号 | 作者 | 机构 |
  | :-: | --- | --- |
  | 01 | [郭奕玮 (Yiwei Guo)](../../Authors/Yiwei_Guo_(郭奕玮).md) | [X-LANCE 实验室](../../Institutions/CHN-SJTU_上海交通大学.md) |
  | 02 | [李之涵 (Zhihan Li)](../../Authors/Zhihan_Li_(李之涵).md) | [X-LANCE 实验室](../../Institutions/CHN-SJTU_上海交通大学.md) |
  | 03 | [李俊杰 (Junjie Li)](../../Authors/Junjie_Li_(李俊杰).md) | [X-LANCE 实验室](../../Institutions/CHN-SJTU_上海交通大学.md) |
  | 04 | [杜晨鹏 (Chenpeng Du)](../../Authors/Chenpeng_Du_(杜晨鹏).md) | [X-LANCE 实验室](../../Institutions/CHN-SJTU_上海交通大学.md) |
  | 05 | [王翰坤 (Hankun Wang)](../../Authors/Hankun_Wang_(王翰坤).md) | [X-LANCE 实验室](../../Institutions/CHN-SJTU_上海交通大学.md) |
  | 06 | [王帅 (Shuai Wang)](../../Authors/Shuai_Wang_(王帅).md) | [深圳大数据研究院](../../Institutions/CHN-SRIBD_深圳大数据研究院.md) |
  | 07 | [陈谐 (Xie Chen)](../../Authors/Xie_Chen_(陈谐).md) | [X-LANCE 实验室](../../Institutions/CHN-SJTU_上海交通大学.md) |
  | 08 | [俞凯 (Kai Yu)](../../Authors/Kai_Yu_(俞凯).md) | [X-LANCE 实验室](../../Institutions/CHN-SJTU_上海交通大学.md) |
- 机构:
  | 序号 | 机构 | 占比 |
  | :-: | :-: | :-: |
  | 01 | [上海交通大学 X-LANCE 实验室](../../Institutions/CHN-SJTU_上海交通大学.md) | 07/08 |
  | 02 | [深圳大数据研究院](../../Institutions/CHN-SRIBD_深圳大数据研究院.md) | 01/08 |
- 时间:
  - 预印时间: 2024.09.03 ArXiv v1
  - 更新笔记: 2024.09.04
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2409.01995)
  - [DOI]()
  - [Github](https://github.com/cantabile-kwok/vec2wav2.0)
  - [Demo](https://cantabile-kwok.github.io/vec2wav2/)
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: 5
- 引用: 40
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> We propose a new speech discrete token vocoder, ***vec2wav 2.0***, which advances voice conversion (VC).
> We use discrete tokens from speech self-supervised models as the content features of source speech, and treat VC as a prompted vocoding task.
> To amend the loss of speaker timbre in the content tokens, ***vec2wav 2.0*** utilizes the WavLM features to provide strong timbre-dependent information.
> A novel adaptive Snake activation function is proposed to better incorporate timbre into the waveform reconstruction process.
> In this way, ***vec2wav 2.0*** learns to alter the speaker timbre appropriately given different reference prompts.
> Also, no supervised data is required for ***vec2wav 2.0*** to be effectively trained.
> Experimental results demonstrate that ***vec2wav 2.0*** outperforms all other baselines to a considerable margin in terms of audio quality and speaker similarity in any-to-any VC.
> Ablation studies verify the effects made by the proposed techniques.
> Moreover, ***vec2wav 2.0*** achieves competitive cross-lingual VC even only trained on monolingual corpus.
> Thus, ***vec2wav 2.0*** shows timbre can potentially be manipulated only by speech token vocoders, pushing the frontiers of VC and speech synthesis.

## 1.Introduction: 引言

> Discretizing speech into `tokens` has prevailed in speech generative tasks, such as text-to-speech (TTS)~\cite{VQTTS,valle,du2024unicats,facodec}, in the era of large language models (LLMs). 
> However, the potential of discrete speech tokens in voice conversion (VC) has not been fully mined, which typically aims to convert source speech into target timbre from reference speech.
> Speech discrete tokens can be roughly divided into acoustic tokens and semantic tokens~\cite{yang2024towards}.
> Although acoustic tokens~\cite{encodec,kumar2024high} reconstruct speech signals well, they lack the ability of VC because all aspects of information in speech are mixed and retained together. 
> Semantic tokens usually come from speech self-supervised (SSL) models~\cite{vq-wav2vec,hsu2021hubert,baevski2020wav2vec,chen2022wavlm} that emphasize on content-related information.
> Whether timbre is intentionally or unintentionally removed in these tokens, they can act as content representations and thus be utilized in the recognition-synthesis VC paradigm~\cite{huang2022s3prl}.
>
> Throughout the history, VC methods with a continuous feature space have been researched with depth.
> The AutoVC series of work~\cite{qian2019autovc,qian2020unsupervised,chan2022speechsplit2} attempt to decouple speech attributes via autoencoder bottlenecks.
> VC with advanced generative algorithms has then achieved remarkable results, such as normalizing flows~\cite{casanova2022yourtts,merritt2022text,li2023freevc} and diffusion models~\cite{diffvc, choi2023diff,choi2024dddm}.
> After the rise of speech SSL methods, researchers begin to apply SSL features in VC ~\cite{huang2022s3prl,hussain2023ace,knnvc,neekhara2023selfvc} where the rich phonetic content information from SSL features are utilized.
>
> But continuous features make it hard to integrate VC with LLMs and thus an isolated step with other speech-related tasks.
> Discrete speech tokens can serve as content representations, so VC can be treated as a speech re-synthesis task then~\cite{polyak21}.
> Starting from \cite{polyak21}, discrete SSL features are increasingly investigated in VC for keeping phonetic content while discarding most of the acoustic details, such as UUVC~\cite{uuvc} and Vec-Tok-VC+~\cite{vectokvc+}.
> ContentVec~\cite{qian2022contentvec} introduces speaker disentanglement to SSL features that benefit VC.
> There also exist researches on decoupled speech codecs that also facilitate VC, such as SSVC~\cite{SSVC} and FACodec~\cite{facodec}.
> Nevertheless, the performance of those VC methods is still limited compared to continuous state-of-the-arts.
> Also, excessive design of speaker disentanglement in the discrete tokens may lead to a negative impact on other paralinguistic information that needs to be preserved, such as prosody.
> 
> Instead of pursuing perfect disentanglement in tokens, a different approach is to enhance the timbre controllability in discrete token vocoders.
> A typical instance is the idea of ``prompted vocoders'' proposed by CTX-vec2wav~\cite{du2024unicats} which is later verified in VC~\cite{li2024sef}.
> In CTX-vec2wav, timbre information is injected using a reference prompt.
> By its position-agnostic cross-attention mechanism, timbre in the mel-spectrogram prompts can be effectively incorporated into the process of speech re-synthesis than only using a time-invariant speaker embedding vector~\cite{li2024sef}. 
> This indicates the larger potential of performing VC through discrete token vocoders.
>
> In this study, we make key improvements upon this framework that significantly boost the effect of acoustic prompts as the source of timbre information.
> Advanced SSL features are utilized for providing discriminative timbre representation.
> Most notably, we propose a novel adaptive Snake activation function where the magnitude and frequency of the sinusoidal functions are both controlled by the target speaker's timbre features.
> This makes the intrinsic periodical properties in the generated signal highly sensitive to the provided timbre features.
> The resulting model, ***vec2wav 2.0***, is then a discrete token vocoder with strong timbre controlling abilities while retaining the content and styles from the content discrete tokens.
> In general, ***vec2wav 2.0*** has the following advantages:
> - **Unity**. ***vec2wav 2.0*** unifies speech discrete token re-synthesis and VC into the same framework of prompted vocoders.
> - **Simplicity**. ***vec2wav 2.0*** does not need any labeled data to train. The only data assumption is utterances are segmented into single-speaker ones. 
The training criterion is also simple enough, without additional losses for decoupling.
> - **Competitiveness**. ***vec2wav 2.0*** achieves superior any-to-any VC performance even compared to continuous VC methods. 
Furthermore, though only trained on English corpus, ***vec2wav 2.0*** shows remarkable cross-lingual VC performance.
> - **New Paradigm**. ***vec2wav 2.0*** proves that speaker timbre can be almost manipulated solely by vocoders even if the speech tokens are not perfectly speaker-decoupled, which may simplify the paradigm in the LLM-based zero-shot TTS world nowadays.
>
> Audio demos and source code are available online at [Github](https://cantabile-kwok.github.io/vec2wav2/).

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

> We present a novel VC method, ***vec2wav 2.0***, based on the re-synthesis of speech discrete tokens.
> It takes advantage of SSL features in both content and timbre representations and enhances CTX-vec2wav in architectural designs.
> The adaptive Snake activation technique is proposed to better incorporate timbre into waveform reconstruction.
> The resulting model achieves remarkable performance on intra and cross-lingual VC tasks.
> We believe ***vec2wav 2.0*** has promising impacts on the future LLM-based speech generation paradigm.
> Future efforts are needed in improving the intelligibility and prosody preservation of the proposed method.
