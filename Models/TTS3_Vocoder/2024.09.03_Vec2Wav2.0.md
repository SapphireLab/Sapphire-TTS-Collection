# Vec2Wav 2.0

<details>
<summary>基本信息</summary>

- 标题: ***vec2wav 2.0***: Advancing Voice Conversion via Discrete Token Vocoders
- 作者:
  | 序号 | 作者 | 机构 |
  | :-: | --- | --- |
  | 01 | [郭奕玮 (Yiwei Guo)](../../Authors/Yiwei_Guo_(郭奕玮).md) | [X-LANCE 实验室](../../Institutions/CHN-SJTU_上海交通大学.md) |
  | 02 | [李之涵 (Zhihan Li)](../../Authors/Zhihan_Li_(李之涵).md) | [X-LANCE 实验室](../../Institutions/CHN-SJTU_上海交通大学.md) |
  | 03 | [李俊杰 (Junjie Li)](../../Authors/Junjie_Li_(李俊杰).md) | [X-LANCE 实验室](../../Institutions/CHN-SJTU_上海交通大学.md) |
  | 04 | [杜晨鹏 (Chenpeng Du)](../../Authors/Chenpeng_Du_(杜晨鹏).md) | [X-LANCE 实验室](../../Institutions/CHN-SJTU_上海交通大学.md) |
  | 05 | [王翰坤 (Hankun Wang)](../../Authors/Hankun_Wang_(王翰坤).md) | [X-LANCE 实验室](../../Institutions/CHN-SJTU_上海交通大学.md) |
  | 06 | [王帅 (Shuai Wang)](../../Authors/Shuai_Wang_(王帅).md) | [深圳大数据研究院](../../Institutions/CHN-SRIBD_深圳大数据研究院.md) |
  | 07 | [陈谐 (Xie Chen)](../../Authors/Xie_Chen_(陈谐).md) | [X-LANCE 实验室](../../Institutions/CHN-SJTU_上海交通大学.md) |
  | 08 | [俞凯 (Kai Yu)](../../Authors/Kai_Yu_(俞凯).md) | [X-LANCE 实验室](../../Institutions/CHN-SJTU_上海交通大学.md) |
- 机构:
  | 序号 | 机构 | 占比 |
  | :-: | :-: | :-: |
  | 01 | [上海交通大学 X-LANCE 实验室](../../Institutions/CHN-SJTU_上海交通大学.md) | 07/08 |
  | 02 | [深圳大数据研究院](../../Institutions/CHN-SRIBD_深圳大数据研究院.md) | 01/08 |
- 时间:
  - 预印时间: 2024.09.03 ArXiv v1
  - 更新笔记: 2024.09.04
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2409.01995)
  - [DOI]()
  - [Github](https://github.com/cantabile-kwok/vec2wav2.0)
  - [Demo](https://cantabile-kwok.github.io/vec2wav2/)
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: 5
- 引用: 40
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> We propose a new speech discrete token vocoder, ***vec2wav 2.0***, which advances voice conversion (VC).
> We use discrete tokens from speech self-supervised models as the content features of source speech, and treat VC as a prompted vocoding task.
> To amend the loss of speaker timbre in the content tokens, ***vec2wav 2.0*** utilizes the [WavLM](../Speech_Representaion/2021.10.26_WavLM.md) features to provide strong timbre-dependent information.
> A novel adaptive Snake activation function is proposed to better incorporate timbre into the waveform reconstruction process.
> In this way, ***vec2wav 2.0*** learns to alter the speaker timbre appropriately given different reference prompts.
> Also, no supervised data is required for ***vec2wav 2.0*** to be effectively trained.
> Experimental results demonstrate that ***vec2wav 2.0*** outperforms all other baselines to a considerable margin in terms of audio quality and speaker similarity in any-to-any VC.
> Ablation studies verify the effects made by the proposed techniques.
> Moreover, ***vec2wav 2.0*** achieves competitive cross-lingual VC even only trained on monolingual corpus.
> Thus, ***vec2wav 2.0*** shows timbre can potentially be manipulated only by speech token vocoders, pushing the frontiers of VC and speech synthesis.

## 1.Introduction: 引言

> Discretizing speech into `tokens` has prevailed in speech generative tasks, such as text-to-speech (TTS) (VQTTS; [VALL-E [2]](../Speech_LLM/2023.01.05_VALL-E.md); [UniCATS [3]](../_tmp/UniCATS.md); [FACodec [4]](../Speech_Neural_Codec/2024.03.05_FACodec.md)), in the era of large language models (LLMs). 
> However, the potential of discrete speech tokens in voice conversion (VC) has not been fully mined, which typically aims to convert source speech into target timbre from reference speech.
> Speech discrete tokens can be roughly divided into acoustic tokens and semantic tokens~\cite{yang2024towards}.
> Although acoustic tokens ([EnCodec [6]](../Speech_Neural_Codec/2022.10.24_EnCodec.md); [DAC [7]](../Speech_Neural_Codec/2023.06.11_Descript-Audio-Codec.md)) reconstruct speech signals well, they lack the ability of VC because all aspects of information in speech are mixed and retained together. 
> Semantic tokens usually come from speech self-supervised (SSL) models ([VQ-Wav2Vec [8]](../Speech_Representaion/2019.10.12_VQ-Wav2Vec.md); [HuBERT [9]](../Speech_Representaion/2021.06.14_HuBERT.md); [Wav2Vec 2.0 [10]](../Speech_Representaion/2020.06.20_Wav2Vec2.0.md); [WavLM [11]](../Speech_Representaion/2021.10.26_WavLM.md)) that emphasize on content-related information.
> Whether timbre is intentionally or unintentionally removed in these tokens, they can act as content representations and thus be utilized in the recognition-synthesis VC paradigm~\cite{huang2022s3prl}.
>
> Throughout the history, VC methods with a continuous feature space have been researched with depth.
> The AutoVC series of work~\cite{qian2019autovc,qian2020unsupervised,chan2022speechsplit2} attempt to decouple speech attributes via autoencoder bottlenecks.
> VC with advanced generative algorithms has then achieved remarkable results, such as normalizing flows ([YourTTS [16]](../E2E/2021.12.04_YourTTS.md); merritt2022text,li2023freevc) and diffusion models (diffvc, choi2023diff,choi2024dddm).
> After the rise of speech SSL methods, researchers begin to apply SSL features in VC ~\cite{huang2022s3prl,hussain2023ace,knnvc,neekhara2023selfvc} where the rich phonetic content information from SSL features are utilized.
>
> But continuous features make it hard to integrate VC with LLMs and thus an isolated step with other speech-related tasks.
> Discrete speech tokens can serve as content representations, so VC can be treated as a speech re-synthesis task then~\cite{polyak21}.
> Starting from \cite{polyak21}, discrete SSL features are increasingly investigated in VC for keeping phonetic content while discarding most of the acoustic details, such as UUVC~\cite{uuvc} and Vec-Tok-VC+~\cite{vectokvc+}.
> ContentVec~\cite{qian2022contentvec} introduces speaker disentanglement to SSL features that benefit VC.
> There also exist researches on decoupled speech codecs that also facilitate VC, such as SSVC~\cite{SSVC} and [FACodec [4]](../Speech_Neural_Codec/2024.03.05_FACodec.md).
> Nevertheless, the performance of those VC methods is still limited compared to continuous state-of-the-arts.
> Also, excessive design of speaker disentanglement in the discrete tokens may lead to a negative impact on other paralinguistic information that needs to be preserved, such as prosody.
> 
> Instead of pursuing perfect disentanglement in tokens, a different approach is to enhance the timbre controllability in discrete token vocoders.
> A typical instance is the idea of `prompted vocoders` proposed by CTX-vec2wav ([UniCATS [3]](../_tmp/UniCATS.md)) which is later verified in VC~\cite{li2024sef}.
> In CTX-vec2wav, timbre information is injected using a reference prompt.
> By its position-agnostic cross-attention mechanism, timbre in the mel-spectrogram prompts can be effectively incorporated into the process of speech re-synthesis than only using a time-invariant speaker embedding vector~\cite{li2024sef}. 
> This indicates the larger potential of performing VC through discrete token vocoders.
>
> In this study, we make key improvements upon this framework that significantly boost the effect of acoustic prompts as the source of timbre information.
> Advanced SSL features are utilized for providing discriminative timbre representation.
> Most notably, we propose a novel adaptive Snake activation function where the magnitude and frequency of the sinusoidal functions are both controlled by the target speaker's timbre features.
> This makes the intrinsic periodical properties in the generated signal highly sensitive to the provided timbre features.
> The resulting model, ***vec2wav 2.0***, is then a discrete token vocoder with strong timbre controlling abilities while retaining the content and styles from the content discrete tokens.
> In general, ***vec2wav 2.0*** has the following advantages:
> - **Unity**. ***vec2wav 2.0*** unifies speech discrete token re-synthesis and VC into the same framework of prompted vocoders.
> - **Simplicity**. ***vec2wav 2.0*** does not need any labeled data to train. The only data assumption is utterances are segmented into single-speaker ones. 
The training criterion is also simple enough, without additional losses for decoupling.
> - **Competitiveness**. ***vec2wav 2.0*** achieves superior any-to-any VC performance even compared to continuous VC methods. 
Furthermore, though only trained on English corpus, ***vec2wav 2.0*** shows remarkable cross-lingual VC performance.
> - **New Paradigm**. ***vec2wav 2.0*** proves that speaker timbre can be almost manipulated solely by vocoders even if the speech tokens are not perfectly speaker-decoupled, which may simplify the paradigm in the LLM-based zero-shot TTS world nowadays.
>
> Audio demos and source code are available online at [Github](https://cantabile-kwok.github.io/vec2wav2/).

## 2.Related Works: 相关工作

## 3.Methodology: 方法

### 3.1.System Overview: 系统概览

> We design ***vec2wav 2.0*** to be a prompted discrete token vocoder as shown in Fig.\ref{fig:main}.
> The overall architecture inherits the frontend-generator framework of CTX-vec2wav ([UniCATS [3]](../_tmp/UniCATS.md)), where the input discrete speech tokens are first fed to a Conformer-based frontend module to soften the discreteness, before a vocoder generator that finally outputs the realistic waveforms.
> The acoustic prompt brings sufficient timbre information into the process of speech re-synthesis.
> We first extract prompt embeddings through a pretrained [WavLM](../Speech_Representaion/2021.10.26_WavLM.md) model, then use a CNN pre-net to process the hidden embeddings.
> In the frontend module, the prompt embeddings are utilized by the position-agnostic cross-attention mechanism~\cite{[UniCATS [3]](../_tmp/UniCATS.md); li2024sef}, which does not apply positional encoding to the query sequence.
> This special cross attention mechanism simulates shuffling the query sequence and inherently breaks the local patterns in the reference prompt, e.g. linguistic and prosodic features, which enables more accurate learning of target timbre as some global information.
>
> After timbre is preliminarily merged into the frontend, we design an adaptive [BigVGAN [31]](2022.06.09_BigVGAN.md) generator to further incorporate the timbre embedding in waveform generation. 
> The core component of this adaptive generator is a novel adaptive Snake activation function, which will be illustrated in Section \ref{sec:snake}.

### 3.2.Adaptive Snake Activation: 自适应蛇形激活函数

> The Snake activation function is proposed in \cite{snake} for modeling periodical inductive bias, which is then adopted in the BigVGAN vocoder to achieve state-of-the-art performance.
> This activation function can be represented as $f_\theta(x)=x+\frac1\beta\sin^2(\alpha x)$. The learnable parameters $\theta=\{\alpha,\beta\}$ are designed to control the frequency and magnitude respectively, and $f_\theta$ can operate on each input channel independently, i.e. different $\theta$ for each input channel.
>
> As this Snake activation can subtly capture the periodical pattern in the speech signals, we propose to inject more information from the target speaker timbre. Let $\bm s\in\mathbb R^d$ be some representative speaker embedding extracted from the target speaker, we design an adaptive Snake activation where the frequency and magnitude of sinusoidal function are both affected by $\bm s$:

$$
\begin{aligned}
    T(\bm s)&=\tanh(W\bm s+\bm b)\\
    f_{\theta}(\bm x, \bm s)&=\bm x+\frac1{\bm\beta+\frac12 T(\bm s)}\sin^2\left[(\bm\alpha + T(\bm s))\bm x\right]
\end{aligned}
$$

> where $T$ is a linear transform followed by $\tanh$ activation, and operations in \eqref{eq:adaptive-snake} are all element-wise. $T(\bm s)$ is discounted by $1/2$ on the magnitude part for numerical stability. To save parameters, we apply the same $T$ transformation to both magnitude and frequency. 
>
> In this way, the learnable parameter for each adaptive Snake is $\theta=\{\bm\alpha,\bm\beta,W,\bm b\}$, and the target timbre information can be effectively injected in every layer of the vocoder via adaptive activations, with strengthens the timbre controllability to a considerable extent.
>
> Here in ***vec2wav 2.0***, the prompt embeddings are first mean-pooled to form a single vector that averages out linguistic details and preserves global timbre, then inserted to every adaptive activation layer in the BigVGAN generator.
> Fig.\ref{fig:bigvgan-detail} illustrates the detailed architecture of adaptive BigVGAN generator.
> The input hidden states are iteratively upsampled by transposed convolutions and transformed by anti-aliased multi-periodicity composition (AMP) blocks. 
> Each AMP block receives an additional prompt embedding that is fed to the adaptive Snake activation layer for timbre control.
> Low-pass (LP) filters are applied after each upsampling and downsampling operation to prevent aliasing ([BigVGAN [31]](2022.06.09_BigVGAN.md)).
> The hidden states are recovered to sampling points after a final adaptive Snake and convolution block. 

### 3.3.Content and Prompt Features

> Both the content and prompt inputs to ***vec2wav 2.0*** are SSL features with different goals.
> For input discrete tokens, there should be as less timbre as possible; for prompt features, there should be enough and apparent timbre information to help reconstruction.

#### Content Features

> We use the off-the-shelf [VQ-Wav2Vec [8]](../Speech_Representaion/2019.10.12_VQ-Wav2Vec.md) SSL model for extracting the discrete content representation to be re-synthesized.
> The discrete tokens are extracted from the quantizer output before the feature aggregator, which is a two-group integer index sequence. 
> We favor this representation because a lot of speaker timbre information is removed due to the information bottleneck, while most of the phonetic pronunciation and prosody are retained~\cite{DSETTS}.
> Also, compared to [HuBERT [9]](../Speech_Representaion/2021.06.14_HuBERT.md)-style Transformer SSL models, [VQ-Wav2Vec](../Speech_Representaion/2019.10.12_VQ-Wav2Vec.md) is free of manual clustering and is also fully convolutional with a certain receptive field.
> This produces a representation that is unaware of the total sequence length, keeping consistent results for a given window.
> This consistency also shows potential for cross-lingual conversion, as its language-agnostic property has been successfully applied in multilingual TTS~\cite{limmits23}.
> Although there exists measurable speaker timbre leakage in the discrete tokens~\cite{superb,TNVQTTS,DSETTS}, the ***vec2wav 2.0*** vocoder exhibits strong timbre controllability, so that competitive VC can still be achieved.

#### Prompt Features

> Following CTX-vec2wav, the reference prompt segment is randomly cut from the current utterance, to maintain the same speaker identity without labeled data. Instead of using mel-spectrogram to provide timbre information from the reference prompt, we use a pretrained [WavLM [11]](../Speech_Representaion/2021.10.26_WavLM.md) model as a timbre feature extractor owing to its widely-verified advantage on speaker verification~\cite{jung2024espnet,superb}. We freeze the [WavLM](../Speech_Representaion/2021.10.26_WavLM.md) model in training and only use the output feature at a certain location of its Transformer blocks.
> In practice, we use the 6th layer of [WavLM](../Speech_Representaion/2021.10.26_WavLM.md)-Large model as early layers are proven to contain rich timbre information~\cite{knnvc}.

### 3.4.Discriminators and Training Criterion

> We inherit the multi-scale discriminators (MSD) and multi-period discriminators (MPD) from [HiFi-GAN [38]](2020.10.12_HiFi-GAN.md). These discriminators are jointly trained with the generator to distinguish fake signals from real ones in multiple scales and periods. 
> With the generator adversarially trained to fool the discriminators, we achieve high-fidelity speech re-synthesis and VC results.
> Different from some current VC models that often suffer from audio quality issues, ***vec2wav 2.0*** ensures the audio quality of speech signals by GAN training.
>
> The training criteria include the auxiliary mel prediction loss and all the other GAN losses from [HiFi-GAN](2020.10.12_HiFi-GAN.md).
> The auxiliary mel prediction loss is an L1 loss between the ground truth mel-spectrograms and predicted ones that come from linear projections after the Conformer frontend, to warm up the whole model.
> This loss is added with a certain coefficient, and following vec2wav~\cite{VQTTS} and CTX-vec2wav ([UniCATS [3]](../_tmp/UniCATS.md)), we cancel it after warming up.

### 3.4.Any-to-Any Voice Conversion

> Although not directly optimized for VC, ***vec2wav 2.0*** still has strong conversion ability due to its effectiveness on incorporating target speaker timbre.
> The content features retain most of the phonetic and prosodic information while losing much speaker identity, while the speaker timbre is controlled by the reference prompt.
> Therefore, we can achieve VC simply by using the target speaker's reference speech as the prompt input.
> This method naturally supports any-to-any VC because the content and prompt features are both acquired by SSL models trained on data with enough speaker variations.
>
> Moreover, as both the cross attention mechanism and the adaptive Snake activation are position agnostic, the ordering of the prompt features plays minimal role in timbre control.
> This allows cross-lingual VC where target speakers may come from unseen languages, since almost all linguistic-relevant patterns are broken by these position-agnostic operations.
> As long as the global traits are apparent enough in the [WavLM](../Speech_Representaion/2021.10.26_WavLM.md) features, speaker timbre can be successfully transferred, even if the model is not trained on multilingual data.

## 4.Experiments: 实验

### Data & Model Setup

> We use all the train splits of LibriTTS~\cite{libritts}, an English corpus with 585 hours of 24kHz speech data spoken by around 2500 speakers, to train ***vec2wav 2.0***.
> We only keep utterances from 6s to 30s to ensure proper prompt lengths.
> The resulting training set has around 360 hours.
> The prompt segment is cut starting from a random point within 1 second of either the beginning or the end of an utterance, extending inward towards the middle, with its length randomly sampled between one third and one half of the original utterance's duration.
> In this way, a reasonable range of prompt lengths is covered in training, and ***vec2wav 2.0*** learns to handle short reference lengths well.
>
> We use the k-means version of official [VQ-Wav2Vec](../Speech_Representaion/2019.10.12_VQ-Wav2Vec.md) model ([Github](https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec)) to extract content tokens from source speech.
> As this model adopts grouped vector quantization, we concatenate the code-vectors corresponding to each group before feeding the Conformer frontend.
> The input to the frontend is thus a 512-dimensional sequence in 10ms strides.
> The prompt embeddings are extracted from official [WavLM](../Speech_Representaion/2021.10.26_WavLM.md)-Large ([Github](https://github.com/microsoft/unilm/tree/master/wavlm)) at the 6th layer.
>
> The Conformer frontend of ***vec2wav 2.0*** contains 2 Conformer blocks, where each of the self and cross attention modules has 2 heads and 184 attention dimensions.
> The prompt prenet has four CNN blocks with scaled residual connections, where the hidden dimensions are 128, 256 and 512 before being fed to cross attentions.
> The resulting generator model has 40.3M parameters.
>
> The whole model is trained for 1 million steps on 4 NVIDIA A10 GPUs with a maximum batch capacity of 36s speech data per device. Other hyper-parameters follow CTX-vec2wav ([UniCATS [3]](../_tmp/UniCATS.md)).

### Evaluation

> We conduct English any-to-any VC comparisons using the unseen speakers in the LibriTTS test-clean split.
> We randomly select 10 speakers, from each of whom 2 utterances are chosen to be the source utterances. Another 10 speakers are selected as target speakers with one 3-second reference utterance for each.
> This yields a test set of 200 any-to-any VC cases.
>
> To measure the performance of VC systems, we adopt multi-faceted metrics that contain both objective and subjective ones:
> 1. Quality and intelligibility: We use the subjective naturalness MOS (NMOS) and word error rate (WER percentage) between recognized hypotheses and ground truth texts.
> The NMOS tests require listeners to rate the utterances by quality and naturalness ranging from 1 to 5.
> WERs are computed using NeMo ASR ([HuggingFace](https://huggingface.co/nvidia/stt_en_fastConformer_transducer_large)).
> 2. Speaker similarity: We conduct similarity MOS (SMOS) tests and compute speaker embedding cosine similarity (SECS).
> Listeners in SMOS tests are asked to rate timbre similarity between reference and synthesized items in 1-5 scale.
> SECS is computed via Resemblyzer ([Github](https://github.com/resemble-ai/Resemblyzer)) where speaker embeddings are extracted by a verification model for computing cosine similarity in range of -1 to 1.
> 3. Prosody preservation: We additionally measure the correlation coefficient of pitch contours (P.Corr) between the source speech and converted speech.
> This is also an important metric in VC because ideal VC systems should preserve prosodic variations in source speech while transferring timbre attributes.
> The value range is -1 to 1, with higher values indicating better preservation.
>
> We compare ***vec2wav 2.0*** with some famous VC models. 
> [YourTTS [16]](../E2E/2021.12.04_YourTTS.md) is a famous flow-based end-to-end VC model.
> DiffVC~\cite{diffvc} and Diff-HierVC~\cite{choi2023diff} promote convertibility via diffusion models.
> UUVC~\cite{uuvc} also performs VC by discrete token reconstruction, but incorporates HuBERT tokens and additional prosody predictions.
> [FACodec [4]](../Speech_Neural_Codec/2024.03.05_FACodec.md) is a state-of-the-art speech codec based on supervised decoupling of content, prosody, timbre and detail information.
> FACodec is capable of converting voices by simply replacing the speaker embedding into the target speaker and then decoding into waveform.
> We discard the detail tokens in FACodec for VC since we find these tokens still contain considerable speaker information that harms VC performance.
> We use the official implementation and checkpoints for all baselines.
> Note that the training data in all baselines either includes LibriTTS or is magnitudes larger (e.g. FACodec), so the comparisons are fair enough.

## 5.Results: 结果

### English Any-to-Any VC

> Table \ref{tab:en} presents the comparison results.
> `Source GT` means the ground truth source utterances, and MOS values are reported with 95\% confidence intervals.
> It is thus clear that ***vec2wav 2.0*** achieves significantly better synthesis quality and speaker similarity than all the baselines.
> The pitch correlation of ***vec2wav 2.0*** is also at a high level\footnote{ Note that pitch correlation is less meaningful if speaker similarity is low.}.
> The WER of ***vec2wav 2.0*** is still acceptable although not the best. 
> This is mostly due to the quantization errors that occur in the [VQ-Wav2Vec](../Speech_Representaion/2019.10.12_VQ-Wav2Vec.md) model itself.

### Cross-Lingual Any-to-Any VC

> To verify the cross-lingual VC ability of ***vec2wav 2.0***, we use the same set of English source utterances in Table \ref{tab:en}, but convert to target speakers in other languages.
> We collect reference utterances from five languages\footnote{ Spanish, German, Dutch, Italian, French.} in MLS~\cite{MLS}.
> The test set is the full combination of source and target utterances.
> For each of those languages, one male and one female speaker are randomly chosen as target speakers, and one reference utterance for each target speaker is sampled.
>
> Table \ref{tab:cross} shows the results.
> Although not trained on multilingual data, ***vec2wav 2.0*** consistently outperforms YourTTS and Diff-HierVC in speaker similarity and quality with a significant margin.
> The WER and P.Corr comparisons show a similar conclusion with Table \ref{tab:en} that ***vec2wav 2.0*** possesses a decent level of intelligibility and prosody preservation, although not the best.
> Therefore, it is demonstrated that ***vec2wav 2.0*** performs competitive conversions, regardless of the languages of references.

### Ablation Study

> We also conduct ablation studies on different input SSL discrete tokens and vocoder architectures.
> Apart from [VQ-Wav2Vec](../Speech_Representaion/2019.10.12_VQ-Wav2Vec.md), we train CTX-vec2wav (our predecessor) and ***vec2wav 2.0*** on HuBERT tokens and [Wav2Vec 2.0 [10]](../Speech_Representaion/2020.06.20_Wav2Vec2.0.md) tokens.
> The HuBERT tokens are obtained by 2048-centroid clustering on the output of the last layer.
> The [Wav2Vec 2.0](../Speech_Representaion/2020.06.20_Wav2Vec2.0.md) tokens are considered the quantizer output before the Transformer, with 2 codebook groups each with 320 codes.
>
> To compare architectures, we additionally train two variants of ***vec2wav 2.0*** on [VQ-Wav2Vec](../Speech_Representaion/2019.10.12_VQ-Wav2Vec.md) inputs: ***vec2wav 2.0-ab1*** that replaces the adaptive Snake activations in BigVGAN by the original Snakes; and ***vec2wav 2.0-ab2*** that further replaces BigVGAN with [HiFi-GAN](2020.10.12_HiFi-GAN.md).
> Thus the comparison between ***vec2wav 2.0*** and `ab1` indicates the effect of adaptive Snake activation, while that between CTX-vec2wav and `ab2` shows the difference made by prompt feature and modules.
> We present the ablation studies in terms of SECS and P.Corr in Fig.\ref{fig:ablation}, together with the baselines in Section \ref{sec:en}.
> It can be found that ***vec2wav 2.0*** obtains consistently large improvements in speaker similarity compared to the predecessor CTX-vec2wav in all the three input SSL tokens, while maintaining comparable pitch preservation.
> From the ablation of model architectures, it is obvious that the prompt-related improvements of ***vec2wav 2.0*** make a substantial contribution to speaker similarity, while the adaptive Snake activations further advance the VC performance.
> The proposed ***vec2wav 2.0*** with [VQ-Wav2Vec](../Speech_Representaion/2019.10.12_VQ-Wav2Vec.md) tokens is finally nearest to the top right corner of Fig.\ref{fig:ablation}, pushing the frontier of modern VC methods towards ideal voice converters.

### Analysis of Pitch Contours: Case Study

> Pitch plays an important role in both the perception of global timbre and local prosody variations.
> An ideal VC system should retain pitch variations but shift the global ranges according to target speakers.
> Fig.\ref{fig:pitch} shows the pitch contours of a sample converted by ***vec2wav 2.0***.
> It can be seen that the average pitch of the converted utterance closely matches that of the reference, while the source and converted ones share the similar variations.
> This intuitively shows ***vec2wav 2.0*** properly manages the global pitch range while keeping the local traits.

## 6.Conclusions: 结论

> We present a novel VC method, ***vec2wav 2.0***, based on the re-synthesis of speech discrete tokens.
> It takes advantage of SSL features in both content and timbre representations and enhances CTX-vec2wav in architectural designs.
> The adaptive Snake activation technique is proposed to better incorporate timbre into waveform reconstruction.
> The resulting model achieves remarkable performance on intra and cross-lingual VC tasks.
> We believe ***vec2wav 2.0*** has promising impacts on the future LLM-based speech generation paradigm.
> Future efforts are needed in improving the intelligibility and prosody preservation of the proposed method.
