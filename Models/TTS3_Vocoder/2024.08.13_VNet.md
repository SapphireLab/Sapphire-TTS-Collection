# VNet

<details>
<summary>基本信息</summary>

- 标题:
- 作者:
  - ??
- 机构:
  - 机构 
- 时间:
  - 预印时间: 20??.??.?? ArXiv v1
  - 更新笔记: 20??.??.??
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv]()
  - [DOI]()
  - [Github]()
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: ?
- 引用: ?
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

Since the introduction of Generative Adversarial Networks (GANs) in speech synthesis, remarkable achievements have been attained. In a thorough exploration of vocoders, it has been discovered that audio waveforms can be generated at speeds exceeding real-time while maintaining high fidelity, achieved through the utilization of GAN-based models. Typically, the inputs to the vocoder consist of band-limited spectral information, which inevitably sacrifices high-frequency details. To address this, we adopt the full-band Mel spectrogram information as input, aiming to provide the vocoder with the most comprehensive information possible.
However, previous studies have revealed that the use of full-band spectral information as input can result in the issue of over-smoothing, compromising the naturalness of the synthesized speech. To tackle this challenge, we propose VNet, a GAN-based neural vocoder network that incorporates full-band spectral information and introduces a Multi-Tier Discriminator (MTD) comprising multiple sub-discriminators to generate high-resolution signals. Additionally, we introduce an asymptotically constrained method that modifies the adversarial loss of the generator and discriminator, enhancing the stability of the training process.
Through rigorous experiments, we demonstrate that the VNet model is capable of generating high-fidelity speech and significantly improving the performance of the vocoder.

## 1.Introduction: 引言

Speech synthesis is crucial across various domains, including accessibility, education, entertainment, and customer service \cite{c1}. However, conventional systems often encounter challenges with timbre, speech rate variation, and vocal coherence \cite{c2,c3,c4}. Recent advancements in deep learning and neural network techniques have significantly improved the quality of speech synthesis \cite{c5,c6}. The neural network and deep learning-based speech synthesis now being introduced are broadly divided into two steps: 1) Acoustic modeling: taking characters (text) or phonemes as input and creating a model of the acoustic features. (The acoustic features used in most of the work are Mel Spectrograms); 2) Vocoder: a model that takes Mel Spectrograms (or similar spectrograms) as input and generates real audio \cite{c7}. As an important step in speech synthesis, the study of the vocoder has received extensive attention. This paper focuses on the vocoder part of the study. Vocoder models can be broadly categorized into autoregressive-based (e.g., WaveNet \cite{c8}, WaveRNN \cite{c9}), flow-based (e.g., WaveGlow \cite{c10}, Parallel WaveGAN \cite{c11}), GAN-based \cite{c12} (e.g., MelGAN \cite{c13}, HiFiGAN \cite{c14}, BigVGAN \cite{c15}) and diffusion model-based (e.g., WaveGrad \cite{c16}, Grad-tts \cite{c17}, FastDiff \cite{c18}, ProDiff \cite{c19}) approaches. These advancements promise more natural and coherent speech, enhancing user experience across various applications.

GANs employ an adversarial training approach, where the generator and discriminator engage in a competitive process. This competition fosters improved generator performance and enhances the ability to generate features resembling real data, making GANs widely utilized in vocoder tasks. While the GAN-based generative model can synthesize high-fidelity audio waveforms faster than real-time, most vocoders operate on band-limited Mel spectrogram as input. For instance, HiFi-GAN utilizes band-limited Mel spectrograms as input. Other similar models include LVCNet \cite{c20}, StyleMelGAN \cite{c21} and WaveGlow \cite{c10}. However, speech signals generated with band-limited Mel spectrograms lack high-frequency information, leading to fidelity issues in the resulting waveforms. Thus, considering full-band Mel spectrogram information as vocoder input is crucial. Despite attempts by Parallel WaveGAN to use full-band Mel spectrograms, it faces challenges such as excessive smoothing, resulting in the generation of non-sharp spectrograms and unnatural speech output \cite{c11}.

The loss function of a GAN typically encompasses both the generator and discriminator loss functions. However, various vocoder models employ distinct loss function designs and exhibit differences in the selection of similar loss terms, leading to training instability. For instance, Parallel WaveGAN incorporates cross-entropy loss into the generator loss to address instability issues, albeit without complete resolution \cite{c11}. MelGAN endeavors to enhance stability by replacing the cross-entropy loss with hinge loss and augmenting feature matching loss, yet gradient loss persists \cite{c13}. HiFiGAN introduces feature matching loss and Mel spectrogram loss to mitigate training instability \cite{c14}. Despite the inclusion of these additional loss functions, training may still encounter challenges such as gradient loss and pattern collapse, resulting in an unstable training process.

This paper introduces VNet, a novel vocoder model capable of synthesizing high-fidelity speech in real time. A new discriminator module, named MTD, is proposed, which utilizes multiple linear spectrogram magnitudes computed with distinct sets of parameters. Operating on full-band Mel spectrogram data, MTD facilitates the generation of full-band and high-resolution signals. The overall discriminator integrates a Multi-Period Discriminator (MPD), leveraging multiple scales of waveforms to enhance speech synthesis performance by capturing both time and frequency domain characteristics \cite{c14}. To mitigate model training instability, an asymptotically constrained approach is proposed to modify the adversarial training loss function. This entails constraining the adversarial training loss within a defined range, ensuring stable training of the entire model. Our contributions can be summarized in three main aspects:

- We propose VNet, a neural vocoder network for GAN-based speech synthesis that incorporates an MTD module to capture the features of speech signals from both time and frequency domains.
- We propose an asymptotically constrained approach to modify the adversarial training loss of the generator and discriminator of the vocoder.
- We demonstrate the effectiveness of the VNet model, as well as the effectiveness of the newly added MTD module and asymptotic constraints against training loss.

## 2.Related Works: 相关工作

GANs have emerged as powerful generative models \cite{c12}. Initially applied to image generation tasks, GANs have garnered significant success and attention. Similarly, in the domain of speech synthesis, where traditional approaches primarily rely on rule-based or statistical models, GAN technology has gradually gained traction. By leveraging the adversarial framework of GANs, speech synthesis models can better capture the complexity and realism of speech signals, thereby producing more natural, high-quality synthesized speech. 

WaveGAN simplifies speech synthesis by directly generating raw audio waveforms, producing high-quality and naturalistic speech segments. However, its training requires substantial data and computational resources. In contrast, Parallel WaveGAN extends single short-time Fourier transform (STFT) loss to multi-resolution, integrating it as an auxiliary loss for GAN training\cite{c11}. It may suffer from excessive smoothing. MelGAN achieves high-quality synthesis without additional distortion or perceptual losses by introducing a multi-scale discriminator (MSD) and incorporating hinge loss, feature matching loss, and discriminator loss\cite{c13}. HiFiGAN enhances the discriminator's ability to differentiate between generated and real audio and introduces a multi-receptive field fusion (MRF) module in the generator. Its loss functions include least squares loss, feature matching loss, Mel spectrogram loss, and discriminator loss\cite{c14,c22}. BigVGAN builds upon HiFiGAN by replacing the MSD with a multi-resolution discriminator (MRD) and introducing periodic activation into the generator. It proposes an anti-aliasing multi-periodicity composition (AMP) module for modeling complex audio waveforms. BigVGAN's loss functions comprise least squares adversarial loss, feature matching loss, and Mel spectrogram loss\cite{c15}.VNet distinguishes itself from these methods by simultaneously addressing the challenges of matching features at various resolutions and scales while also resolving the issue of poor fidelity results that arise from using full-band Mel spectrograms as input.

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

This study demonstrates the capabilities of the VNet model, a GAN-based vocoder, in enhancing speech synthesis. By utilizing full-band Mel spectrogram inputs, the model effectively addresses over-smoothing issues. Furthermore, the introduction of a Multi-Tier Discriminator (MTD) and refined adversarial loss functions has significantly improved speech quality and fidelity.

Future research should prioritize further reducing over-smoothing and exploring the model's potential in multilingual and diverse speech styles. Such advancements could greatly enhance the practical usability of GAN-based vocoders, resulting in more natural and expressive synthesized speech.
