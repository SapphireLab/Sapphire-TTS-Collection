# FreeV

<details>
<summary>基本信息</summary>

- 标题: FreeV: Free Lunch For Vocoders Through Pseudo Inversed Mel Filter
- 作者:
  - 01 [Yuanjun Lv](../../Authors/Yuanjun_Lv_(吕元骏).md)
  - 02 [Hai Li](../../Authors/Hai_Li.md)
  - 03 [Ying Yan](../../Authors/Ying_Yan.md)
  - 04 [Junhui Liu](../../Authors/Junhui_Liu.md)
  - 05 [Danming Xie](../../Authors/Danming_Xie.md)
  - 06 [Lei Xie](../../Authors/Lei_Xie_(谢磊).md)
- 机构:
  - 机构 
- 时间:
  - 预印时间: 2024.06.12 ArXiv v1
  - 更新笔记: 2024.08.05
- 发表:
  - [InterSpeech 2024](../../Publications/InterSpeech.md) 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.08196)
  - [DOI]()
  - [Github](https://github.com/bakerbunker/freev)
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=13731432507109093105)
- 标签:
  - ?
- 页数: ?
- 引用: ?
- 被引: 0
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Vocoders reconstruct speech waveforms from acoustic features and play a pivotal role in modern TTS systems. Frequent-domain GAN vocoders like Vocos and APNet2 have recently seen rapid advancements, outperforming time-domain models in inference speed while achieving comparable audio quality. However, these frequency-domain vocoders suffer from large parameter sizes, thus introducing extra memory burden. Inspired by PriorGrad and SpecGrad, we employ pseudo-inverse to estimate the amplitude spectrum as the initialization roughly. This simple initialization significantly mitigates the parameter demand for vocoder. Based on APNet2 and our streamlined Amplitude prediction branch, we propose our FreeV, compared with its counterpart APNet2, our FreeV achieves \textbf{1.8$\times$ inference speed improvement} with nearly \textbf{half parameters}. Meanwhile, our FreeV outperforms APNet2 in resynthesis quality, marking a step forward in pursuing real-time, high-fidelity speech synthesis. Code and checkpoints is available at: \url{https://github.com/BakerBunker/FreeV}

## 1.Introduction: 引言

Recently, there has been a rapid advancement in the field of neural vocoders, which transform speech acoustic features into waveforms. These vocoders play a crucial role in text-to-speech synthesis, voice conversion, and audio enhancement applications. Within these contexts, the process typically involves a model that predicts a mel-spectrogram from the source text or speech, followed by a vocoder that produces the waveform from the predicted mel-spectrogram. Consequently, the quality of the synthesized speech, the speed of inference, and the parameter size of the model constitute the three primary metrics for assessing the performance of neural vocoders.

Recent advancements in vocoders, including iSTFTNet~\cite{kaneko2022istftnet}, Vocos~\cite{siuzdak2023vocos}, and APNet~\cite{ai2023apnet}, have shifted from the prediction of waveforms in the time domain to the estimation of amplitude and phase spectra in the frequency domain, followed by waveform reconstruction via inverse short-time Fourier transform (ISTFT). This method circumvents the need to predict extensive time-domain waveforms, thus reducing the models' computational burden. ISTFTNet, for example, minimizes the computational complexity by decreasing the upsampling stages and focusing on frequency-domain spectra predictions before employing ISTFT for time-domain signal reconstruction. Vocos extends these advancements by removing all upsampling layers and utilizing the ConvNeXtV2~\cite{woo2023convnext} Block as its foundational layer. APNet~\cite{ai2023apnet} and APNet2~\cite{du2023apnet} further refine this approach by independently predicting amplitude and phase spectra and incorporating innovative supervision to guide phase spectra estimation. Nonetheless, with comparable parameter counts, these models often underperform their time-domain counterparts, highlighting potential avenues for optimization in the parameter efficiency of frequency-domain vocoders.

Several diffusion-based vocoders have integrated signal-processing insights to reduce inference steps and improve reconstruction quality. PriorGrad~\cite{lee2021priorgrad} initially refines the model's priors by aligning the covariance matrix diagonals with the energy of each frame of the Mel spectrogram. Extending this innovation, SpecGrad~\cite{koizumi2022specgrad} proposed to adjust the diffusion noise to align its dynamic spectral characteristics with those of the conditioning mel spectrogram. Moreover, GLA-Grad~\cite{liu2024glagrad} enhances the perceived audio quality by embedding the estimated amplitude spectrum into each diffusion step's post-processing stage. Nevertheless, the reliance on diffusion models results in slower inference speeds, posing challenges for their real-world application.

In this work, we introduce \textit{FreeV}, a streamlined GAN vocoder enhanced with prior knowledge from signal processing, and tested on the LJSpeech dataset~\cite{ljspeech17}. The empirical outcomes highlight FreeV's superior performance characterized by faster convergence in training, a near 50\% reduction in parameter size, and a notable boost in inference speed. Our contributions can be summarized as follows:
\begin{itemize}
    \item We innovated by using the product of the Mel spectrogram and the pseudo-inverse of the Mel filter, referred to as the pseudo-amplitude spectrum, as the model's input, effectively easing the model's complexity.
    \item Drawing on our initial insight, we substantially diminished the spectral prediction branch's parameters and the time required for inference without compromising the quality achieved by the original model.
\end{itemize}

## 2.Related Works: 相关工作

### PriorGrad & SpecGrad

Based on diffusion-based vocoder WaveGrad~\cite{chen2021wavegrad}, which direct reconstruct the waveform through a DDPM process, Lee \textit{et al.} proposed PriorGrad~\cite{lee2021priorgrad} by introducing an adaptive prior $\mathcal{N}(\mathbf{0},\mathbf{\Sigma})$, where $\mathbf{\Sigma}$ is computed from input mel spectrogram $X$. The covariance matrix $\mathbf{\Sigma}$ is given by: $\mathbf{\Sigma}=\mathrm{diag} [(\sigma_1^2,\sigma_2^2,\cdots,\sigma_D^2,)]$, where $\sigma_d^2,$ denotes the signal power at $d$th sample, which is calculated by interpolating the frame energy.  Compared to conventional DDPM-based vocoders, PriorGrad utilizes signal before making the source distribution closer to the target distribution, which simplifies the reconstruction task.

Based on PriorGrad, SpecGrad~\cite{koizumi2022specgrad} proposed adjusting the diffusion noise in a way that aligns its dynamic spectral characteristics with those of the conditioning mel spectrogram. SpecGrad introduced a decomposed covariance matrix and its approximate inverse using the idea from T-F domain filtering, which is conditioned on the mel spectrogram. This method enhances audio fidelity, especially in high-frequency regions. We denote the STFT by a matrix $G$, and the ISTFT by a matrix $G^+$, then the time-varying filter $L$ can be expressed as:
\begin{equation}
    L=G^+DG,
\end{equation}
where $D$ is a diagonal matrix that defines the filter, and it is obtained from the spectral envelope. Then we can obtain covariance matrix $\Sigma=LL^T$ of the standard Gaussian noise $\mathcal{N}(0,\Sigma)$ in the diffusion process. By introducing more accurate prior to the model, SpecGrad achieves higher reconstruction quality and inference speech than PriorGrad.

### APNet & APNet2

As illustrated in Figure~\ref{fig:overall}, APNet2~\cite{du2023apnet} consists of two components: amplitude spectra predictor (ASP) and phase spectra predictor (PSP). These two components predict the amplitude and phase spectra separately, which are then employed to reconstruct the waveform through ISTFT. The backbone of APNet2 is ConvNeXtV2~\cite{woo2023convnext} block, which is proved has strong modeling capability. In the PSP branch, APNet~\cite{ai2023apnet} proposed the parallel phase estimation architecture at the output end. The parallel phase estimation takes the output of two convolution layers as the pseudo imaginary part $I$ and real part $R$, then obtains the phase spectra by:
\begin{equation}
    \arctan(\frac{I}{R})-\frac{\pi}{2}\cdot sgn(I)\cdot[sgn(R)-1]
\end{equation}
where $sgn$ is the sign function.

A series of losses are defined in APNet to supervise the generated spectra and waveform. In addition to the losses used in HiFiGAN~\cite{kong2020hifigan}, which include Mel loss $\loss{mel}$, generator loss $\loss{g}$, discriminator loss $\loss{d}$, feature matching loss $\loss{fm}$, APNet proposed: 
\begin{itemize}
    \item amplitude spectrum loss $\loss{A}$, which is the L2 distance of the predicted and real amplitude;
    \item  phase spectrogram loss $\loss{P}$, which is the sum of instantaneous phase loss, group delay loss, and phase time difference loss, all phase spectrograms are anti-wrapped;
    \item STFT spectrogram loss $\loss{S}$, which includes the STFT consistency loss and L1 loss between predicted and real reconstructed STFT spectrogram.
\end{itemize}

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
