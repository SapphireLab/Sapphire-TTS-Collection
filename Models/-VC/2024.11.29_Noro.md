# Noro: A Noise-Robust One-Shot Voice Conversion System With Hidden Speaker Representation Capabilities

<details>
<summary>基本信息</summary>

- 标题: "Noro: A Noise-Robust One-Shot Voice Conversion System With Hidden Speaker Representation Capabilities."
- 作者:
  - 01 Haorui He
  - 02 Yuchen Song
  - 03 Yuancheng Wang
  - 04 Haoyang Li
  - 05 Xueyao Zhang
  - 06 Li Wang
  - 07 Gongping Huang
  - 08 Eng Siong Chng
  - 09 Zhizheng Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2411.19770v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2411.19770v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2024.11.29_2411.19770v1_Noro__A_Noise-Robust_One-Shot_Voice_Conversion_System_With_Hidden_Speaker_Representation_Capabilities.pdf)
  - [Publication] #TODO

</details>

## Abstract

One-shot voice conversion (VC) aims to alter the timbre of speech from a source speaker to match that of a target speaker using just a single reference speech from the target, while preserving the semantic content of the original source speech.
Despite advancements in one-shot VC, its effectiveness decreases in real-world scenarios where reference speeches, often sourced from the internet, contain various disturbances like background noise.
To address this issue, we introduce Noro, a Noise Robust One-shot VC system.
Noro features innovative components tailored for VC using noisy reference speeches, including a dual-branch reference encoding module and a noise-agnostic contrastive speaker loss.
Experimental results demonstrate that Noro outperforms our baseline system in both clean and noisy scenarios, highlighting its efficacy for real-world applications. 
Additionally, we investigate the hidden speaker representation capabilities of our baseline system by repurposing its reference encoder as a speaker encoder.
The results shows that it is competitive with several advanced self-supervised learning models for speaker representation under the SUPERB settings, highlighting the potential for advancing speaker representation learning through one-shot VC task.

## 1·Introduction

\IEEEPARstart{O}{ne-shot} voice conversion (VC) changes the timbre of speech from a source speaker to that of a target speaker using just one reference speech sample from the target, while maintaining the original speech's semantic content.

This task has been extensively researched in the deep learning era, employing various methods and producing promising results[^Popov2022Diffusion-Based], [^Li2024Sef-Vc], [^Zhang2024Amphion], [^He2024Emilia], [^Liu2024SpMis], [^Huang2024Debatts].

However, the strong performance of these one-shot VC methods is mainly demonstrated in controlled academic settings, where high-quality data is used for both training and evaluation.

As noted in[^Huang2021How], [^Huang2022Toward], [^Xie2023Noisy-to-Noisy], [^Xue2021Learning], their effectiveness significantly decreases in more challenging real-world scenarios, where reference speeches often collected from the internet are characterized by various interferences, such as background noise, which leads to a drop in the quality and similarity of the converted speech. 

Few previous works tried to address this challenge to achieve noise-robust VC.

Some researchers[^Valentini-Botinhao2016Investigating], [^Chan2021Speech] attempt to integrate a pre-trained or jointly trained speech enhancement (SE) module into standard VC systems to improve robustness.

However, these methods are not end-to-end and inevitably increase computational costs during inference.

Other researchers[^Huang2022Toward], [^Du2022Noise-Robust], [^Xue2021Learning] suggest strategies during training phase, such as direct data augmentation[^Huang2022Toward], to mitigate the impact of noise.

Despite these efforts, there is still room for improvement, especially in extremely noisy conditions where the Signal-to-Noise Ratio (SNR) is below 5 dB[^Du2022Noise-Robust].

To address these challenges, we introduce a novel Noise-Robust One-shot VC system named Noro.

Noro is built on a one-shot VC baseline system based on diffusion[^Ho2020Denoising], which generates speech from pitch and semantic representations obtained through a source encoder and speaker timbre representations via a reference encoder.
*To achieve noise-robustness of the system, we focus on learning speaker timbre representations that are agnostic to noise.*

We hypothesize that maintaining noise-agnostic input representations for the acoustic model will prevent its performance from deteriorating due to noise interference.

Therefore, we propose the following designs:
First, we introduce a novel dual-branch reference encoding module.

This module consists of a dual-branch transformer encoder structure with shared weights: one branch encodes clean reference speech, while the other encodes its noisy counterpart generated through data augmentation.

Next, we devise a noise-agnostic contrastive speaker loss to maximize the similarity between samples (whether clean or noisy) from the same speaker while minimizing it for those from different speakers.

This loss ensures that the dual-branch reference encoding module learns to represent speaker timbre independent of the acoustic environment of the reference speeches.

Experimental results demonstrate that Noro significantly improves the robustness of our baseline system in both clean and noisy environments, making it an ideal choice for real-world applications that encounter noisy reference speeches.

Additionally, inspired by[^Cho2020Learning], [^Cho2021Improving], we further investigate the speaker representation capabilities of our baseline system.

The motivation stems from the fact that the reference encoder within the VC systems, trained to encode the vocal timbre of various speakers, including unseen ones, for one-shot VC, inherently functions as a speaker encoder.

This encoder develops its capability to represent speakers in a self-supervised learning (SSL) manner without needing explicit speaker labels.

Thus, we argue that the reference encoder might possess hidden abilities to function effectively as an SSL speaker encoder.

To validate this hypothesis, we employ a pre-trained reference encoder from our baseline one-shot VC system as a speaker encoder, denoted as VC-SPK2VEC, and assess its speaker representation ability through a speaker verification task under the SUPERB[^Yang2021Superb] setting.

We compare its performance against several state-of-the-art (SOTA) SSL models, including Wav2vec[^Schneider2019Wav2vec], Wav2vec 2.0[^Baevski2020Wav2vec], HuBERT[^Hsu2021Hubert], and WavLM[^Chen2021WavLM].

Surprisingly, VC-SPK2VEC achieves a competitive Equal Error Rate (EER) of 5.32\%, comparable to these SOTA SSL models.

This result confirms the effectiveness of VC-SPK2VEC as an SSL speaker encoder and underscores the potential for leveraging one-shot VC tasks in advancing speaker representation learning.

Our contributions in this work can be summarized as follows.

-  We proposed Noro, a Noise-Robust One-shot VC system.

It achieves noise robustness through innovative designs, including a dual-branch reference encoding module, a noise-agnostic contrastive speaker loss.

Experimental results demonstrate that Noro significantly outperforms state-of-the-art one-shot VC systems in noisy scenarios, establishing it as the preferred choice for real-world applications requiring reliable, high-quality VC in diverse and challenging acoustic environments.

-  We investigated the hidden speaker representation capabilities of the proposed one-shot VC system by repurposing its reference encoder as a speaker encoder (VC-SPK2VEC).

VC-SPK2VEC exhibits comparable performance with several SOTA SSL models in speaker representation under the SUPERB setting, highlighting the overlooked potential for speaker representation learning via the one-shot VC task.

The paper is organized as follows: Section~[sec:related](#sec:related) discusses the related works.

Section~[sec:method](#sec:method) introduces the methodology, including a detailed description of our baseline one-shot voice conversion (VC) system and the proposed Noro system.

Section~[sec:exp](#sec:exp) first presents the experimental setup and results, evaluating the performance of Noro under various acoustic conditions and comparing it to baseline systems.

We then explore the impact of different source encoder models on the performance of Noro.

Subsequently, we explore the potential of re-purposing the reference encoder from our VC system as a self-supervised speaker encoder.

Finally, Section~[sec:conclude](#sec:conclude) provides concluding remarks, summarizing the findings and highlighting future research directions.

## 2·Related Work

\label{sec:related}

### Noise-robust Voice Conversion

#### One-shot Voice Conversion

Numerous studies have sought high-quality one-shot voice conversion using methods such as auto-encoders[^Hsu2016Voice], [^Kameoka2019Acvae-Vc], [^Qian2019Autovc], [^Qian2020Unsupervised], [^Wang2022DrVC], [^Popov2022Diffusion-Based], [^Wang2023Lm-Vc], [^Li2023FreeVC], [^Ju2024Naturalspeech], normalizing flows[^Kobyzev2020Normalizing], and self-supervised learning (SSL) models[^{Casanova, Edresson and Weber, Julian and Shulby, Christopher D and Junior, Arnaldo Candido and G{\"o}}lge2022YourTTS], [^Lin2021S2vc], [^Li2024Sef-Vc], [^Lim2024Wav2vec-Vc], [^Du2024UniCATS].

For example, Diff-VC[^Popov2022Diffusion-Based] uses an encoder that predicts an “average voice,” transforming mel features of each phoneme into averaged mel features across a large multi-speaker dataset, followed by decoding the mel for converted speech.

Similarly, FreeVC[^Li2023FreeVC] performs voice conversion by extracting disentangled content and target speaker information, then reconstructs the waveform using an end-to-end VITS framework.

Most recently, FaCodec-VC[^Ju2024Naturalspeech] employs a factorized codec approach to achieve state-of-the-art one-shot voice conversion performance.

It effectively disentangles speech attributes into content, prosody, and timbre representations using an information bottleneck, enabling strong one-shot voice conversion by altering the timbre of the source speech to match the target speaker.
*Despite these advances, the effectiveness of such models decreases in challenging real-world scenarios, where reference speech samples from the internet often contain interferences like background noise, reducing the quality and similarity of the converted speech.*

To address the issue of noise robustness, several studies have been conducted.

For instance, Valentini-Botinhao et al.[^Valentini-Botinhao2016Investigating] proposed an RNN-based speech enhancement method to improve noise robustness, utilizing LSTM networks for two-stage processing of input signals to effectively suppress noise.

Similarly, Chan et al.[^Chan2021Speech] developed a speech enhancement (SE)-assisted VC system, where the VC and SE components are jointly optimized to produce high-quality converted speech signals.

Additionally, Miao et al.[^Miao2019Noise-Robust] introduced a noise-robust voice conversion method that enhances high-quefrency components through sub-band cepstrum conversion and fusion.

Their approach leverages bidirectional long short-term memory networks to convert vocal tract parameters from a source to a target speaker in noisy conditions.

However, these approaches are not end-to-end, requiring separate stages for enhancement and synthesis, which adds complexity and increases computational overhead during inference.

To address this, some researchers have proposed training strategies to improve robustness.

Huang et al.[^Huang2022Toward] proposed applying random degraded data augmentations during training to force the model to filter out degradations and perform VC accurately.

Xue et al.[^Xue2021Learning] introduced a noise-controllable WaveGAN that learns noise-independent acoustic representations to improve conversion quality in noisy environments.

Du et al.[^Du2022Noise-Robust] employed domain adversarial training to ensure robustness against various noise conditions during conversion.

Despite these advancements, significant challenges remain, especially in extremely noisy environments where the Signal-to-Noise Ratio (SNR) is below 5 dB.
*This paper proposes the Noro system to address this challenge, introducing a dual-branch reference encoding module and a noise-agnostic contrastive speaker loss, which significantly enhance noise robustness even in challenging acoustic environments.*

### Self-supervised Learning for Speaker Representation Learning

Various self-supervised learning (SSL) models have been proposed for speech processing tasks.

These models are generally categorized into three types: generative, discriminative, and multi-task models[^Yang2021Superb].

Generative models, such as APC[^Chung2019Unsupervised], aim to reconstruct masked or future frames, focusing on predicting the missing or upcoming parts of the speech sequence.

Discriminative models like wav2vec[^Schneider2019Wav2vec] and HuBERT[^Hsu2021Hubert] use contrastive learning to distinguish between different segments of speech.

Multi-task models, such as WavLM+[^Chen2021WavLM], incorporate multiple pretraining objectives to learn versatile representations for downstream tasks.

These models have shown their effectiveness in various aspects of speech processing, including content, speaker, semantics, and paralinguistic information[^Yang2021Superb].

Inspired by[^Cho2020Learning], [^Cho2021Improving], \textit{this study further explores the speaker representation capabilities of one-shot VC systems.

The motivation arises from the observation that the reference encoder within one-shot VC systems, designed to capture the vocal timbre of diverse speakers—including previously unseen ones—naturally functions as a speaker encoder.

This aligns well with self-supervised speaker representation learning. }

## 3·Methodology

\label{sec:method}

In this section, we first introduce our baseline one-shot VC system.

As depicted in Fig.~[fig:noro](#fig:noro) (a), this system comprises a source encoder to simultaneously encode semantic and pitch representations (Sec.~[method-source](#method-source)), a reference encoder to extract speaker timbre representation from the reference speech (Sec.~[method-reference](#method-reference)), and a diffusion model that utilizes these representations as conditions to predict the mel-spectrogram of the target speech (Sec.~[method-diffusion](#method-diffusion)).

To achieve noise-robustness against noisy reference speeches, we introduce Noro.

Fig.~[fig:noro](#fig:noro) (b) provides an overview of Noro.

Noro builds upon the baseline system, but replaces its original reference encoders with a dual-branch reference encoding module and a noise-agnostic contrastive speaker loss.

![](figures/Noro.png)

<a id="fig:noro">Overview of the model architecture of our baseline system and Noro.</a>

### Our Baseline System

#### Source Encoder

\label{method-source}

To derive semantic representations from the source speech, our baseline system uses a pre-trained and frozen HuBERT model[^Hsu2021Hubert] as the semantic extractor.

This model encodes the source speech into continuous embeddings.

We then apply K-Means quantization to these embeddings.

Instead of using K-Means cluster IDs (discrete semantic tokens), we use the quantized continuous embeddings, which preserve more information, for semantic representation.

The resulting semantic representation for the source speech is denoted as $s_{src}$.

For pitch representations, we use the open-source software PyWorld\footnote{\url{https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder}} to extract frame-level F0 values from the source speech.

We normalize these values and use them for pitch representation, denoted as $p_{src}$.

After extracting the semantic and pitch representations, we employ a Conformer[^Gulati2020Conformer] encoder to simultaneously encode $s_{src}$ and $p_{src}$, obtaining the final source representation $h_{src}$.

#### Reference Encoder

\label{method-reference}

To encode the speaker timbre in the reference speech, we use a cross-attention scheme similar to the speech prompting mechanism in NaturalSpeech 2[^Shen2023Naturalspeech].

Specifically, we first employ a Transformer based reference encoder to convert the melspectrogram of the reference speech into a hidden sequence.

Instead of allowing the diffusion model to directly attend to this hidden sequence, we employ two attention blocks. 

In the first attention block, we use $m$ randomly initialized query embeddings to attend to the hidden sequence, resulting in $h_{ref}$, a hidden sequence of length $m$ that represents the reference speaker's timbre at the utterance level.

In the second attention block, the output hidden sequence of the WaveNet Layer serves as the query, while $h_{ref}$ functions as both the key and value.

The attention results of the second attention block are then used as conditional information for a FiLM layer in the final acoustic modeling process.

#### Diffusion Model

\label{method-diffusion}

To enhance in-context learning for improved one-shot generation, each training sample is randomly segmented during the training phase.

A segment comprising 25-45\% of the total sample is designated as the reference speech, while the remaining portion functions as both the source and target speech.

We utilize a WaveNet-based diffusion model[^Shen2023Naturalspeech], denoted as $s_\theta$, to predict the melspectrogram $z_{tgt}$ of the target speech.

We describe both the diffusion (forward) process and the denoising (reverse) process using the stochastic differential equation. 
In the forward process, Gaussian noise is added to the melspectrogram $z_{tgt}$:

$$

z_{tgt}^{t} = e^{-\frac{1}{2} \int_{0}^{t} \beta_s ds} z_{tgt} + (I - e^{- \int_{0}^{t} \beta_s ds}) \epsilon,

$$

where $\beta_t$ represents the noise schedule function, $\epsilon$ denotes randomly sampled Gaussian noise, and $t \in [0, 1]$.

The reverse process can be formulated as:

$$

d z_{tgt}^t = -\left(\frac{1}{2} z_{tgt}^t + \nabla \log p_t(z_{tgt}^t)\right) \beta_t \, dt + \sqrt{\beta_t} \epsilon.

$$

$s_\theta$ takes the noised melspectrogram $z_{tgt}^t$, the time step $t$, and the conditions of the source representation $h_{src}$, containing both semantic and pitch information, and the reference representation $h_{ref}$, containing speaker timbre information, to estimate $\nabla \log p_t(z_{tgt}^t)$.

The loss function of the diffusion model can be written as:

$$

L_{diff} = \left\|s_\theta(z_{tgt}^t, t, h_{src}, h_{ref}) - \nabla \log p_t(z_{tgt}^t)\right\|_{1}.

$$

During the inference stage, we gradually denoise $z_{tgt}^t$ utilizing the estimated score $s_\theta(z_{tgt}^t, t, s_{src}, p_{src}, h_{ref})$.

This process initiates with Gaussian noise, initially sampled as $z_{tgt}^1$.

### Noro

#### Dual-branch Reference Encoding Module

\label{method-ref-source}

To improve the noise robustness of the baseline system, we replaced its original reference encoder with a dual-branch reference encoding module.

Unlike the baseline system, which uses a single reference encoder, this module incorporates two reference encoders sharing identical model weights to form a dual-branch structure.

For each clean training sample, we randomly mix clean reference speech with eight types of noise from the DEMAND database[^Valentini-Botinhao2016Investigating] at SNRs following a normal distribution (0,20) dB to create noisy reference speeches.

Subsequently, the clean and noisy reference speeches are processed by separate reference encoders using the same query embeddings.

This process generates clean and noisy reference representations $h_{\text{ref}}$ and $h'_{\text{ref}}$, respectively. 

Considering the weight-sharing scheme used in this dual-branch reference encoding module, the method described here qualifies as a "training strategy" for enhancing our baseline system.

Initially, we load the Noro system with weights from a pre-trained baseline system.

During the training phase, the average of $h_{\text{ref}}$ and $h'_{\text{ref}}$ is fed into the diffusion model for acoustic modeling and acts as the keys and values in the second attention block.

During the inference phase, only one reference encoder is utilized, and the Noro system's structure mirrors that of the baseline system.

#### Noise-agnostic Contrastive Speaker Loss

After obtaining the representations of the clean speeches $h_{\text{ref}}$ and their noisy counterparts $h'_{\text{ref}}$, we introduce a noise-agnostic contrastive speaker loss.

This loss function aims to maximize the similarity between samples (clean or noisy) from the same speaker while minimizing it for samples from different speakers.

First, as $h_{\text{ref}}$ and $h'_{\text{ref}}$ are hidden sequences of length $m$, we perform average pooling over the length dimension to form a comprehensive reference representation that captures the reference speaker's timbre at the utterance level.

We then concatenate $h_{\text{ref}}$ and $h'_{\text{ref}}$ along the batch size dimension:

$$

h_{\text{all}} = \left[ h_{\text{ref}}; h'_{\text{ref}} \right].

$$

Given that the noisy speeches share the same speakers as the clean ones, we can express $y_{\text{all}}$ as:

$$

y_{\text{all}} = \left[ y_{\text{spk}}; y_{\text{spk}} \right].

$$

Finally, our proposed noise-agnostic contrastive speaker loss is formulated as:

$$

\mathcal{L}_{\text{ref}} = \frac{1}{2N} \sum_{i=1}^{2N} \text{CrossEntropy} \left( \frac{h_{i} \cdot h_{j}^T}{\tau}, M_{i,j} \right),

$$

where $h_{i}$ and $h_{j}$ are the $i$-th and $j$-th reference representations in $h_{\text{all}}$, $\tau$ is the temperature parameter adjusting the scaling of the logits, $N$ is the batch size, and the mask matrix $M_{i,j}$ is defined as follows:

$$

M_{i,j} =

\begin{cases} 
1 & \text{if } y_{i} = y_{j}, \\ 
0 & \text{otherwise},
\end{cases}

$$

where $y_{i}$ and $y_{j}$ are the speaker labels for $h_{i}$ and $h_{j}$ respectively.

This loss ensures that the reference encoders represent the vocal timbre of different speakers regardless of noise interference, thus enhancing the robustness of the system.

The total loss function for the Noro system is defined as follows:

$$

L_{total} = \alpha L_{diff} + \beta L_{ref},

$$

where \(\alpha\) and \(\beta\) are the weights assigned to each loss component. 

## 4·Experiments

\label{sec:exp}

In this section, we conduct experiments to evaluate the performance of our proposed system, Noro.

Specifically, we aim to answer the following evaluation questions (EQs):

- [EQ1:] How does Noro perform in one-shot VC under different acoustic environments?

- [EQ2:] How does the choice of source encoder (Linear, CNN, Conformer) affect the performance of the Noro system in terms of similarity and intelligibility?

- [EQ3:] How does the reference encoder in one-shot VC systems, which is naturally aligned with self-supervised speaker representation learning tasks, compare to SOTA SSL systems in terms of speaker representation ability?

To address EQ1 and EQ2, we adhere to the methodology outlined in[^Li2024Sef-Vc] by employing speaker embedding cosine similarity (SECS) and character error rate (CER).

SECS is determined using a cutting-edge speaker representation model,\footnote{\url{https://huggingface.co/microsoft/wavlm-base-plus-sv}} with the results reflecting similarity to the original voice prompt.

CER is calculated using an ASR model\footnote{\url{https://huggingface.co/facebook/hubert-large-ls960-ft}} to assess the intelligibility of the generated speech.

For subjective evaluation, we use the Comparative Mean Opinion Score (CMOS) and the Similarity Mean Opinion Score (SMOS) to evaluate naturalness and similarity, respectively.

We use 10 randomly selected pairs from the clean test set and another 10 from the noisy test set.

Twelve proficient english users conducted the assessments.

The naturalness scores range from one ("Bad") to five ("Excellent"), and the similarity scores range from one ("Different speaker, sure") to four ("Same speaker, sure").

### One-shot VC Under Different Acoustic Environments (EQ.~1)

\label{sec:exp_main}

#### Experimental Setups

To thoroughly assess the one-shot VC capabilities of the Noro system, we use two acoustic settings: clean and noisy.

In the clean setting, the high-quality VCTK Corpus, recorded in a studio, serves as the evaluation dataset.

VCTK includes recordings from 110 English speakers with diverse accents, characterized by minimal noise interference.

The test set comprises 150 randomly selected pairs of source and reference speech from VCTK.

For the noisy setting, we introduce noise of unseen types from the training stage to the reference speeches in the test set at SNR conditions ranging from 0-5 dB, creating challenging conditions to evaluate the robustness of VC systems.

Under these two settings, we compare Noro with our baseline system.

For both systems, we employ the Libri-Light[^Kahn2020Libri-Light] dataset for training, which consists of 60,000 hours of 16 kHz speech data from over 8,000 distinct speakers.

We leverage the official segmentation scripts\footnote{\url{https://github.com/facebookresearch/libri-light}} to segment the speech data into approximately 15-second sequences by concatenating consecutive chunks with voice activity.

Utterances longer than 30 seconds or shorter than 3 seconds are discarded.

Both systems are trained with four NVIDIA GeForce RTX 4090 GPUs, each handling a batch size of 60 seconds of speech data, over a total of 800,000 steps to ensure convergence.

The systems are optimized using the AdamW optimizer with a learning rate of 5e-5, incorporating 5,000 warmup steps followed by a cosine annealing learning rate schedule.

The hyperparameters $\tau$ and $\alpha$ are set to 1.0, and $\beta$ is set at 0.25.

For inferencing, we employ the Euler ODE solver, set the diffusion steps to 200, and the temperature to 1.2.

To generate waveforms from melspectrograms, we utilize a pre-trained BigVGAN vocoder.

Noro and the baseline system use the same model architecture, detailed hyper-parameters of are provided in Table~[tab:hyper](#tab:hyper).

<a id="tab:hyper">The detailed hyper-parameters of Noro and the baseline system.</a>

#### Experimental Results

We use both objective and subjective metrics to evaluate the systems.

For objective evaluation, we compare Noro with our baseline system and the baseline models.

<a id="tab:vc">The experimental results for objective evaluation.</a>

<a id="tab:vc-sub">The experimental results for subjective evaluation.</a>

Table~[tab:vc](#tab:vc) summarizes the objective evaluation results for Noro and the baseline systems under both clean and noisy conditions.

In clean settings, although FreeVC and FaCodec-VC marginally outperformed Noro in terms of CER, Noro still performed competitively with a CER of 4.74 and a strong SECS of 82.38.

This demonstrates Noro’s ability to effectively perform one-shot voice conversion, accurately preserving semantic content while adapting to the speaker’s timbre.

In noisy environments, the performance of baseline systems dropped significantly, with DiffVC’s SECS falling to 69.09, FreeVC to 73.83, and FaCodec-VC to 74.78—indicating a considerable reduction in their ability to preserve speaker characteristics.

In contrast, Noro demonstrated significantly better noise robustness, achieving a CER of 4.66 and an SECS of 80.09, highlighting its reliability in noisy, real-world scenarios.

Table~[tab:vc-sub](#tab:vc-sub) presents the subjective evaluation results of Noro and the baseline systems under both clean and noisy conditions.

In the clean setting, both systems perform well in preserving the quality of converted speech and maintaining speaker identity. 
However, under the noisy setting, the baseline system's performance declines significantly, with a CMOS of 2.09 and a SMOS of 2.75.

This drop highlights the baseline system's vulnerability to noise in reference speech, affecting both perceived quality and speaker similarity of the converted speech.

In contrast, Noro demonstrates remarkable robustness to noise, achieving a CMOS of 2.95 and a SMOS of 2.97.

The minimal performance degradation from clean to noisy conditions underscores Noro's ability to maintain high subjective quality and speaker similarity even in the presence of background noise.

Overall, the subjective evaluation confirms the objective results, reinforcing Noro's advantage in noise robustness and its potential applicability in real-world noisy environments.

#### Qualitative Analysis

Both the objective and subjective evaluations validate the robustness of Noro against noisy reference speech.

To interpret the improvement, we visualize the reference representation $h_{\text{ref}}$ produced by the baseline system and Noro, respectively, via t-SNE.

As observed from Fig.~[fig:emb](#fig:emb), the reference representation for clean and noisy reference speech in the baseline system shows a significant difference.

This discrepancy leads to a substantial decline in the baseline system's performance when the reference speech is noisy.

However, the reference representations for clean and noisy reference speech from Noro are mixed together.

This is because, during the training stage, Noro utilizes our proposed dual-branch reference encoding module and noise-agnostic contrastive speaker loss, effectively learning a noise-agnostic reference representation.

This capability explains Noro's superior performance under noisy settings.

#### Visualization Insights

![](figures/clean_embeddings.pdf)

<a id="fig:clean_embeddings">Our Baseline System</a>

### Evaluation of Different Source Encoders (Eq.~2)

#### Experimental Setup

This section describes the experimental setup used to evaluate various architectures for the source encoder in the Noro system, with the aim of identifying the most effective configuration.

The source encoder fuses content and pitch features into a unified representation, which the Noro system then leverages to predict target speech representations.

To optimize accuracy and computational efficiency, we evaluated several architectures for the source encoder:

-  **Linear**: The linear encoder processes a 769-dimensional input vector (768 dimensions for content features and 1 for the pitch feature), projecting it to a 512-dimensional target representation.

This basic approach serves as a baseline, focusing on dimensionality reduction with minimal computational complexity.

-  **CNN**: The CNN-based encoder processes the 769-dimensional input using a 1-D convolutional layer with a kernel size of 3 and a hidden dimension of 512.

This design is intended to capture local temporal patterns efficiently, using convolution to balance complexity and computational demand.

-  **Conformer**: The Conformer encoder is designed to extract both local and global features, combining a self-attention mechanism with an attention dimension of 512 and 8 attention heads.

This architecture enables effective modeling of intricate dependencies over both short and long time scales, potentially enhancing the quality of speech feature representations.

This experiment follows the same setup as detailed in Sec.~[sec:exp_main](#sec:exp_main), with the sole modification being the choice of source encoder.

#### Experimental Results

<a id="tab:encoder-comparison">Performance Comparison of Different Source Encoders.</a>

Table [tab:encoder-comparison](#tab:encoder-comparison) shows the performance comparison across encoder architectures.

Although the Linear and CNN encoders yielded slightly higher similarity scores, their significantly higher CER values indicate reduced intelligibility.

The Conformer encoder, however, achieved a balanced performance with competitive similarity scores and substantially lower CER, especially in noisy conditions, making it the optimal choice for the Noro system.

The Conformer encoder’s superior performance likely stems from its hybrid architecture that combines convolutional and transformer-based components.

The convolutional layers effectively capture fine-grained local features, while the self-attention mechanism adeptly models long-range global interactions.

This combination allows the Conformer to achieve significantly reduced character error rates (CER), enhancing overall intelligibility in varied acoustic environments.

### One-shot VC as Self-supervised Speaker Representation Learning (EQ.~3)

#### Experimental Setups

In previous sections, we demonstrated the strong one-shot VC capabilities of the VC systems, noting that the reference encoder effectively captures speaker timbre features.

In this section, we repurpose the reference encoder, originally designed for one-shot VC, as a self-supervised learning (SSL) speaker encoder—designated VC-SPK2VEC.

We evaluate its effectiveness in speaker representation against several popular pre-trained SSL models known for their superior performance in speaker-related downstream tasks such as speaker verification (SV) and speaker identification (SID)[^Chen2021WavLM], [^Yang2021Superb], under the SUPERB setups.

Following the SUPERB[^Yang2021Superb] setups, we use SSL features in speaker verification.

An input waveform is processed through a frozen pre-trained SSL model to extract frame-level features.

A simple x-vector model[^Snyder2018X-Vectors] is then employed as the downstream model to determine if two utterances belong to the same speaker, forming a binary classification challenge.

The training set for all models is VoxCeleb2[^Chung2018VoxCeleb2], augmented with noise, and testing is conducted on VoxCeleb1[^Nagrani2020VoxCeleb].

For conventional SSL models, the final representation is obtained by computing the weighted sum of hidden states from each layer to enhance speaker representation: $\hat{o}t = \sum{l=1}^{L} w_l \cdot h_{l}^{t}$, where $h_{l}^{t}$ represents the hidden states from layer $l$ at time $t$, and $w_l$ denotes the normalized weight for each layer.

For VC-SPK2VEC, the final representation is derived from the hidden states of the transformer encoder's last layer in the reference encoder.

The x-vector downstream model employs an additive-margin softmax loss, with the same hyperparameters as described in[^Nagrani2020VoxCeleb].

We calculate the cosine similarity between speaker embeddings for each pair of enrollment and test utterances to produce a score.

To evaluate the speaker representation capabilities of VC-SPK2VEC, we compare it against several baseline models: log mel filterbank (FBANK), Modified CPC[^Rivière2020Unsupervised], wav2vec[^Schneider2019Wav2vec], vq-wav2vec[^Baevski2019Vq-Wav2vec], wav2vec 2.0[^Baevski2020Wav2vec], HuBERT[^Hsu2021Hubert], and WavLM[^Chen2021WavLM].

The results for these baseline models are sourced directly from[^Yang2021Superb] and[^Chen2021WavLM].

\vspace{-5pt}

#### Experimental Results

<a id="speaker">Comparison of VC-SPK2VEC with Baseline Models on the SV Task, where "Params." denotes the number of parameters in the model, and "Data" denotes the hours of data used for training the model.</a>

Table~[speaker](#speaker) compares VC-SPK2VEC with the baseline models on the speaker verification (SV) task.

VC-SPK2VEC achieves a competitive EER of 5.32\%.

While it trails behind HuBERT Base (5.11\%) and the three WavLM model variants (4.69\%, 4.07\%, and 3.77\% for Base, Base+, and Large, respectively), it outperforms other baseline models such as wav2vec 2.0 Base, wav2vec 2.0 Large, and HuBERT Large.

Notably, VC-SPK2VEC uses a comparatively smaller model size of 72.4M parameters.

These results underscore VC-SPK2VEC's effectiveness as a self-supervised speaker encoder and highlight its comparable performance to SOTA SSL models.

These findings highlight the great potential of leveraging speaker representation learning in one-shot speech generation tasks, as initially explored in[^Cho2020Learning], [^Cho2021Improving].

With the advent of speech generation models trained on extensive datasets exceeding 100k hours of speech data, unifying speaker representation learning with generative speech tasks like voice conversion (VC) may lead to further advancements in both fields.

Future research could benefit from investigating the relationship between the capability of speaker representation and factors such as the number of speakers included in the training set of one-shot VC systems or the model size of the reference encoder in these systems.

## 5·Conclusions

\label{sec:conclude}

In this paper, we introduced Noro, a noise-robust one-shot voice conversion system featuring a dual-branch reference encoding module and a noise-agnostic contrastive speaker loss to enhance noise robustness in real-world scenarios.

Our experimental results show that Noro significantly outperforms our baseline system under both clean and noisy conditions, providing a practical solution for real-world applications where reference speeches are often contaminated with noise.

Future work could further improve the robustness of VC systems against noisy source speeches.

Additionally, we explored the hidden speaker representation capabilities of VC systems by repurposing the reference encoder as an SSL speaker encoder, termed VC-SPK2VEC.

Extensive evaluations on the SUPERB benchmark demonstrated that VC-SPK2VEC performs competitively against several advanced SSL models, highlighting its potential for advancing speaker representation learning through one-shot VC tasks.

This work underscore an interesting research direction for unifying speaker representation learning and VC.

Exploring the integration of large-scale voice conversion systems and extensive unlabeled data for speaker representation learning presents promising directions for advancing both voice conversion and speaker representation learning.

\bibliographystyle{IEEEtran}
\bibliography{main}

\end{document}

## References

[^Popov2022Diffusion-Based]: {Diffusion-Based Voice Conversion With Fast Maximum Likelihood Sampling Scheme}. {Iclr} 2022.
[^Li2024Sef-Vc]: {SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion With Cross Attention}. {Icassp} 2024.
[^Zhang2024Amphion]: Amphion: An Open-Source Audio, Music and Speech Generation Toolkit. SLT 2024.
[^He2024Emilia]: Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for Large-Scale Speech Generation. SLT 2024.
[^Liu2024SpMis]: SpMis: An Investigation of Synthetic Spoken Misinformation Detection. {Slt} 2024.
[^Huang2024Debatts]: Debatts: Zero-Shot Debating Text-to-Speech Synthesis. arXiv:2411.06540.
[^Huang2021How]: {How Far Are We From Robust Voice Conversion: A Survey}. {Slt} 2021.
[^Huang2022Toward]: {Toward Degradation-Robust Voice Conversion}. {Icassp} 2022.
[^Xie2023Noisy-to-Noisy]: {Noisy-to-Noisy Voice Conversion Under Variations of Noisy Condition}. IEEE/ACM Transactions on Audio, Speech, and Language Processing 2023.
[^Xue2021Learning]: {Learning Noise-Independent Speech Representation for High-Quality Voice Conversion for Noisy Target Speakers}. {Interspeech} 2021.
[^Valentini-Botinhao2016Investigating]: {Investigating RNN-Based Speech Enhancement Methods for Noise-Robust Text-to-Speech}. {Ssw} 2016.
[^Chan2021Speech]: {Speech Enhancement-Assisted Stargan Voice Conversion in Noisy Environments}. arXiv:2110.09923.
[^Du2022Noise-Robust]: {Noise-Robust Voice Conversion With Domain Adversarial Training}. Neural Networks 2022.
[^Ho2020Denoising]: {Denoising Diffusion Probabilistic Models}. Advances in Neural Information Processing Systems 2020.
[^Cho2020Learning]: {Learning Speaker Embedding From Text-to-Speech}. {Interspeech} 2020.
[^Cho2021Improving]: {Improving Reconstruction Loss Based Speaker Embedding in Unsupervised and Semi-Supervised Scenarios}. {Icassp} 2021.
[^Yang2021Superb]: {SUPERB: Speech Processing Universal Performance Benchmark}. Interspeech 2021.
[^Schneider2019Wav2vec]: {Wav2vec: Unsupervised Pre-Training for Speech Recognition}. {Interspeech} 2019.
[^Baevski2020Wav2vec]: {Wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations}. Advances in Neural Information Processing Systems 2020.
[^Hsu2021Hubert]: {Hubert: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units}. IEEE/ACM Transactions on Audio, Speech, and Language Processing 2021.
[^Chen2021WavLM]: WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing. arXiv:2110.13900.
[^Hsu2016Voice]: {Voice Conversion From Non-Parallel Corpora Using Variational Auto-Encoder}. {Apsipa} 2016.
[^Kameoka2019Acvae-Vc]: {ACVAE-VC: Non-Parallel Voice Conversion With Auxiliary Classifier Variational Autoencoder}. IEEE/ACM Transactions on Audio, Speech, and Language Processing 2019.
[^Qian2019Autovc]: {AUTOVC: Zero-Shot Voice Style Transfer With Only Autoencoder Loss}. {Icml} 2019.
[^Qian2020Unsupervised]: {Unsupervised Speech Decomposition via Triple Information Bottleneck}. {Icml} 2020.
[^Wang2022DrVC]: {DrVC: A Framework of Any-to-Any Voice Conversion With Self-Supervised Learning}. {Icassp} 2022.
[^Wang2023Lm-Vc]: {LM-VC: Zero-Shot Voice Conversion via Speech Generation Based on Language Models}. IEEE Signal Processing Letters 2023.
[^Li2023FreeVC]: {FreeVC: Towards High-Quality Text-Free One-Shot Voice Conversion}. {Icassp} 2023.
[^Ju2024Naturalspeech]: {Naturalspeech 3: Zero-Shot Speech Synthesis With Factorized Codec and Diffusion Models}. Icml 2024.
[^Kobyzev2020Normalizing]: {Normalizing Flows: An Introduction and Review of Current Methods}. IEEE Transactions on Pattern Analysis and Machine Intelligence 2020.
[^{Casanova, Edresson and Weber, Julian and Shulby, Christopher D and Junior, Arnaldo Candido and G{\"o}}lge2022YourTTS]: {YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for Everyone}. {Icml} 2022.
[^Lin2021S2vc]: {S2VC: A Framework for Any-to-Any Voice Conversion With Self-Supervised Pretrained Representations}. {Interspeech} 2021.
[^Lim2024Wav2vec-Vc]: {Wav2vec-Vc: Voice Conversion via Hidden Representations of Wav2vec 2.0}. {Icassp} 2024.
[^Du2024UniCATS]: {UniCATS: A Unified Context-Aware Text-to-Speech Framework With Contextual VQ-Diffusion and Vocoding}. {Aaai} 2024.
[^Miao2019Noise-Robust]: {Noise-Robust Voice Conversion Using High-Quefrency Boosting via Sub-Band Cepstrum Conversion and Fusion}. Applied Sciences 2019.
[^Chung2019Unsupervised]: Unsupervised Learning of Speech Representations via Autoregressive Predictive Coding. arXiv:1904.03240.
[^Gulati2020Conformer]: Conformer: Convolution-Augmented Transformer for Speech Recognition. Interspeech 2020.
[^Shen2023Naturalspeech]: {Naturalspeech 2: Latent Diffusion Models Are Natural and Zero-Shot Speech and Singing Synthesizers}. {Iclr} 2023.
[^Valentini-Botinhao2016Investigating]: {Investigating RNN-Based Speech Enhancement Methods for Noise-Robust Text-to-Speech}. {Ssw} 2016.
[^Kahn2020Libri-Light]: {Libri-Light: A Benchmark for ASR With Limited or No Supervision}. {Icassp} 2020.
[^Snyder2018X-Vectors]: {X-Vectors: Robust DNN Embeddings for Speaker Recognition}. {Icassp} 2018.
[^Chung2018VoxCeleb2]: {VoxCeleb2: Deep Speaker Recognition}. {Interspeech} 2018.
[^Nagrani2020VoxCeleb]: {VoxCeleb: Large-Scale Speaker Verification in the Wild}. Computer Speech \& Language 2020.
[^Rivière2020Unsupervised]: {Unsupervised Pretraining Transfers Well Across Languages}. {Icassp} 2020.
[^Baevski2019Vq-Wav2vec]: {Vq-Wav2vec: Self-Supervised Learning of Discrete Speech Representations}. {Iclr} 2019.