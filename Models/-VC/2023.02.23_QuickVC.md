# QuickVC: Any-to-Many Voice Conversion Using Inverse Short-Time Fourier Transform for Faster Conversion

<details>
<summary>基本信息</summary>

- 标题: "QuickVC: Any-to-Many Voice Conversion Using Inverse Short-Time Fourier Transform for Faster Conversion."
- 作者:
  - 01 Houjian Guo
  - 02 Chaoran Liu
  - 03 Carlos Toshinori Ishi
  - 04 Hiroshi Ishiguro
- 链接:
  - [ArXiv](https://arxiv.org/abs/2302.08296v4)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2302.08296v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2023.02.16_2302.08296v1_QuickVC__Many-to-Any_Voice_Conversion_Using_Inverse_Short-Time_Fourier_Transform_for_Faster_Conversion.pdf)
  - [ArXiv:2302.08296v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2023.02.16_2302.08296v2_QuickVC__Many-to-Any_Voice_Conversion_Using_Inverse_Short-Time_Fourier_Transform_for_Faster_Conversion.pdf)
  - [ArXiv:2302.08296v3](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2023.02.16_2302.08296v3_QuickVC__Many-to-Any_Voice_Conversion_Using_Inverse_Short-Time_Fourier_Transform_for_Faster_Conversion.pdf)
  - [ArXiv:2302.08296v4](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2023.02.16_2302.08296v4_QuickVC__Any-to-Many_Voice_Conversion_Using_Inverse_Short-Time_Fourier_Transform_for_Faster_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

With the development of automatic speech recognition (ASR) and text-to-speech (TTS) technology, high-quality voice conversion (VC) can be achieved by extracting source content information and target speaker information to reconstruct waveforms.
However, current methods still require improvement in terms of inference speed.
In this study, we propose a lightweight VITS-based VC model that uses the HuBERT-Soft model to extract content information features without speaker information.
Through subjective and objective experiments on synthesized speech, the proposed model demonstrates competitive results in terms of naturalness and similarity.
Importantly, unlike the original VITS model, we use the inverse short-time Fourier transform (iSTFT) to replace the most computationally expensive part.
Experimental results show that our model can generate samples at over 5000 KHz on the 3090 GPU and over 250 KHz on the i9-10900K CPU, achieving competitive speed for the same hardware configuration.\footnote{The audio samples are available at \url{https://quickvc.github.io/quickvc-demo}}

## 1·Introduction

Voice conversion (VC) is a technique that modifies a speaker's voice to sound like that of another speaker while preserving the original meaning and content of the speech [^Kaneko2018Cyclegan-Vc].

In this article, we aim at any-to-many voice conversion, which is the conversion of an arbitrary speaker's source speech signal to a target speaker appearing in the training set.

A typical approach of any-to-many VC is feature disentangling.

The content feature information and the speaker feature information in the speech are extracted separately and used to reconstruct the speech [^Qian2019Autovc].

The results of this method depend on whether the obtained content features do not contain the original speaker feature information, while not losing information about the speech content.%The results of this method depend on the following things:

%

Techniques from the ASR domain are widely used to extract linguistic content from speech while ignoring speaker-specific details.

VC based on phonetic posteriorgrams (PPGs) draws much attention [^Liu2021Any-to-Many], [^Li2021PPG-Based].

PPGs are intermediate results from a speaker-independent ASR system, representing the posterior probability of phonetic classes at the frame level [^Hazen2009Query-by-Example].

PPGs are independent of speaker and language, making them suitable for VC.

However, the accuracy of the PPG-based VC model largely depends on the precision of the ASR model used to extract the PPGs.

In addition to ASR, transfer learning from TTS methods has been employed to obtain linguistic representations for VC [^Zhang2021Transfer].

However, these methods require a large amount of annotated data containing text to train the ASR or TTS model.

There are also studies that do not require text annotated data, such as Cyclegan-VC [^Kaneko2018Cyclegan-Vc] and StarGAN [^Kaneko2019Stargan-Vc2], which are GAN-based models, and AutoVC [^Qian2019Autovc], which is an AutoEncoder.

But the speech they generate is relatively poor in terms of quality [^Zhao2020Voice].

Recently there has been a lot of new research on VC that attempts to obtain feature vectors of speech by means of self-supervised learning (SSL) models [^Huang2021Any-to-One], [^Niekerk2022Comparison], [^Li2022FreeVC].

By obtaining the content representation of speech from these feature vectors, speech can be reconstructed to achieve VC or singing voice conversion (SVC)\footnote{https://github.com/innnky/so-vits-svc/tree/32k}.

These studies achieve very good results in terms of quality and are close to the TTS models.

The development of TTS models has helped to produce high-quality speech based on content features.

TTS models such as Tacotron2 [^Shen2018Natural] and Fastspeech [^Ren2019Fastspeech] have the ability to synthesize naturalistic speech.

They have been applied in the field of VC [^Zhao2021Towards], [^Niekerk2022Comparison].

However, these TTS methods are two-stage, synthesizing acoustic features first and then using a vocoder to synthesize waveforms from the predicted acoustic features.

VITS is an end-to-end TTS model that enhances the quality of the reconstructed waveform through adversarial training [^Kim2021Conditional].

By applying VITS to VC, separate training of the VC model and the vocoder can be avoided.

Although there are VC models that are capable of producing speech of good perceptual quality, there is still a lack of research on high-quality and fast VC.

In practice, if you want to achieve real-time VC, you need to process the input speech frame by frame [^Saeki2020Real-Time,], so the inference speed of the models is important for real-time VC.

The faster the model performs this, the less delay will be experienced when converting speech.

However, previous research in real-time VC has not been good enough in terms of the naturalness and similarity of the synthesized speech [^Saeki2021Real-Time].

Now that high-quality VC is possible, there is a need for VC models that can maintain high quality while allowing fast inference.

In this study, we implement a fast and high-quality voice conversion model.

The main contributions of this paper can be summarized as follows

-  We propose the QuickVC model, which combines the high-quality speech synthesis model VITS with the speech content feature extraction model HuBERT-Soft to achieve high-quality any-to-many speech conversion.

-  In the VITS-based speech reconstruction part, the decoder structure is lightened to speed up the model.

The inference speed of the model on the CPU is up to 280KHz.

-  To make the model pay more attention to the content information in the input features, a data augmentation method is used during the training process, which improves the naturalness and similarity of the results.
