# QuickVC: Any-to-Many Voice Conversion Using Inverse Short-Time Fourier Transform for Faster Conversion

<details>
<summary>基本信息</summary>

- 标题: "QuickVC: Any-to-Many Voice Conversion Using Inverse Short-Time Fourier Transform for Faster Conversion."
- 作者:
  - 01 Houjian Guo
  - 02 Chaoran Liu
  - 03 Carlos Toshinori Ishi
  - 04 Hiroshi Ishiguro
- 链接:
  - [ArXiv](https://arxiv.org/abs/2302.08296v4)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2302.08296v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2023.02.16_2302.08296v1_QuickVC__Many-to-Any_Voice_Conversion_Using_Inverse_Short-Time_Fourier_Transform_for_Faster_Conversion.pdf)
  - [ArXiv:2302.08296v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2023.02.16_2302.08296v2_QuickVC__Many-to-Any_Voice_Conversion_Using_Inverse_Short-Time_Fourier_Transform_for_Faster_Conversion.pdf)
  - [ArXiv:2302.08296v3](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2023.02.16_2302.08296v3_QuickVC__Many-to-Any_Voice_Conversion_Using_Inverse_Short-Time_Fourier_Transform_for_Faster_Conversion.pdf)
  - [ArXiv:2302.08296v4](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2023.02.16_2302.08296v4_QuickVC__Any-to-Many_Voice_Conversion_Using_Inverse_Short-Time_Fourier_Transform_for_Faster_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

With the development of automatic speech recognition (ASR) and text-to-speech (TTS) technology, high-quality voice conversion (VC) can be achieved by extracting source content information and target speaker information to reconstruct waveforms.
However, current methods still require improvement in terms of inference speed.
In this study, we propose a lightweight VITS-based VC model that uses the HuBERT-Soft model to extract content information features without speaker information.
Through subjective and objective experiments on synthesized speech, the proposed model demonstrates competitive results in terms of naturalness and similarity.
Importantly, unlike the original VITS model, we use the inverse short-time Fourier transform (iSTFT) to replace the most computationally expensive part.
Experimental results show that our model can generate samples at over 5000 KHz on the 3090 GPU and over 250 KHz on the i9-10900K CPU, achieving competitive speed for the same hardware configuration.\footnote{The audio samples are available at \url{https://quickvc.github.io/quickvc-demo}}

## 1·Introduction

Voice conversion (VC) is a technique that modifies a speaker's voice to sound like that of another speaker while preserving the original meaning and content of the speech [^Kaneko2018Cyclegan-Vc].

In this article, we aim at any-to-many voice conversion, which is the conversion of an arbitrary speaker's source speech signal to a target speaker appearing in the training set.

A typical approach of any-to-many VC is feature disentangling.

The content feature information and the speaker feature information in the speech are extracted separately and used to reconstruct the speech [^Qian2019Autovc].

The results of this method depend on whether the obtained content features do not contain the original speaker feature information, while not losing information about the speech content.%The results of this method depend on the following things:

%

Techniques from the ASR domain are widely used to extract linguistic content from speech while ignoring speaker-specific details.

VC based on phonetic posteriorgrams (PPGs) draws much attention [^Liu2021Any-to-Many], [^Li2021PPG-Based].

PPGs are intermediate results from a speaker-independent ASR system, representing the posterior probability of phonetic classes at the frame level [^Hazen2009Query-by-Example].

PPGs are independent of speaker and language, making them suitable for VC.

However, the accuracy of the PPG-based VC model largely depends on the precision of the ASR model used to extract the PPGs.

In addition to ASR, transfer learning from TTS methods has been employed to obtain linguistic representations for VC [^Zhang2021Transfer].

However, these methods require a large amount of annotated data containing text to train the ASR or TTS model.

There are also studies that do not require text annotated data, such as Cyclegan-VC [^Kaneko2018Cyclegan-Vc] and StarGAN [^Kaneko2019Stargan-Vc2], which are GAN-based models, and AutoVC [^Qian2019Autovc], which is an AutoEncoder.

But the speech they generate is relatively poor in terms of quality [^Zhao2020Voice].

Recently there has been a lot of new research on VC that attempts to obtain feature vectors of speech by means of self-supervised learning (SSL) models [^Huang2021Any-to-One], [^Niekerk2022Comparison], [^Li2022FreeVC].

By obtaining the content representation of speech from these feature vectors, speech can be reconstructed to achieve VC or singing voice conversion (SVC)\footnote{https://github.com/innnky/so-vits-svc/tree/32k}.

These studies achieve very good results in terms of quality and are close to the TTS models.

The development of TTS models has helped to produce high-quality speech based on content features.

TTS models such as Tacotron2 [^Shen2018Natural] and Fastspeech [^Ren2019Fastspeech] have the ability to synthesize naturalistic speech.

They have been applied in the field of VC [^Zhao2021Towards], [^Niekerk2022Comparison].

However, these TTS methods are two-stage, synthesizing acoustic features first and then using a vocoder to synthesize waveforms from the predicted acoustic features.

VITS is an end-to-end TTS model that enhances the quality of the reconstructed waveform through adversarial training [^Kim2021Conditional].

By applying VITS to VC, separate training of the VC model and the vocoder can be avoided.

Although there are VC models that are capable of producing speech of good perceptual quality, there is still a lack of research on high-quality and fast VC.

In practice, if you want to achieve real-time VC, you need to process the input speech frame by frame [^Saeki2020Real-Time,], so the inference speed of the models is important for real-time VC.

The faster the model performs this, the less delay will be experienced when converting speech.

However, previous research in real-time VC has not been good enough in terms of the naturalness and similarity of the synthesized speech [^Saeki2021Real-Time].

Now that high-quality VC is possible, there is a need for VC models that can maintain high quality while allowing fast inference.

In this study, we implement a fast and high-quality voice conversion model.

The main contributions of this paper can be summarized as follows

-  We propose the QuickVC model, which combines the high-quality speech synthesis model VITS with the speech content feature extraction model HuBERT-Soft to achieve high-quality any-to-many speech conversion.

-  In the VITS-based speech reconstruction part, the decoder structure is lightened to speed up the model.

The inference speed of the model on the CPU is up to 280KHz.

-  To make the model pay more attention to the content information in the input features, a data augmentation method is used during the training process, which improves the naturalness and similarity of the results.

## 2·Method

### Motivation and strategy

QuickVC is inspired by VITS [^Kim2021Conditional], Soft-VC [^Niekerk2022Comparison] and MS-iSTFT-VITS [^Kawamura2022Lightweight] respectively.

The backbone of QuickVC is inherited from VITS, which adopts variational inference, augmented with normalizing flows and an adversarial training process.

We chose VITS as the basis for our VC system because of its ability to produce excellent speech synthesis and its non-autoregressive design, which is optimal for fast inference.

However, unlike VITS, QuickVC's prior encoder takes the raw waveforms as input rather than the phonemes.

The prior encoder refers to HuBERT-Soft in Soft-VC to obtain speaker-independent content feature information.

Speaker embeddings are extracted by a speaker encoder to perform multi-speaker VC.

In addition, the decoders of QuickVC and VITS are different.

To achieve faster inference, we refer to the MS-iSTFT-VITS, which is a single-speaker TTS model that replaces the original Hifigan vocoder-based decoder [^Kong2020Hifi-Gan] with multi-stream iSTFTNet [^Kaneko2022iSTFTNet].

### Model architecture

QuickVC model consists of a speaker encoder, a prior encoder, a posterior encoder, an MS-iSTFT-Decoder, and a discriminator, where the architecture of the posterior encoder and discriminator follow VITS.

We will focus on describing the prior encoder, speaker encoder, and  MS-iSTFT-Decoder in the following subsections.

#### Prior encoder

The prior encoder consists of HuBERT-Soft, a content encoder, and a  normalizing flow.

As the input is no longer text but speech, the text encoder in VITS becomes HuBERT-Soft and the content encoder.

HuBERT-Soft is a kind of feature extractor using HuBERT-Base as a backbone [^Hsu2021Hubert].

HuBERT-Soft takes the raw waveform as input and produces a 256-dimensional content feature.  The content encoder is based on the posterior encoder in VITS.

The content encoder can produce the mean and variance of the normal posterior distribution.  The normalizing flow follows VITS and is a stack of affine coupling layers.

The flow is used to improve the complexity of the prior distribution, conditioned on the speaker embedding $g$, to achieve any-to-many VC.

#### Speaker encoder

The speaker encoder generates an encoded speaker representation from an utterance.

It is trained from scratch together with the rest of the model.

The network structure of the speaker encoder contains one layer of LSTM structure and one layer of fully connected layers, which is referenced from  [^Liu2021Any-to-Many].

We extract mel-spectrograms from the audio signal and use them as input to the speaker encoder.

We assume that the output of the content encoder does not contain any speaker information.

The model then replaces the missing speaker information based on the input from the speaker encoder to synthesize speech.

#### MS-iSTFT-Decoder

The decoder module is considered to be the biggest bottleneck in VITS according to the previous study  [^Kawamura2022Lightweight].

The decoder architecture in VITS is based on the HiFi-GAN vocoder, which uses a repeated convolution-based network to upsample the input acoustic features.

We refer to the decoder architecture in MS-iSTFT-VITS, the decoder performs the following steps in sequence.

First, the VAE latent variable $z$ is conditioned on the speaker embedding $g$, then upsampled by a sequence of upsample convolutional residual blocks (ResBlock)  [^He2016Deep].

The upsampled $z$ is then projected to the magnitude and phase variables for each sub-band signal.

Then, using the magnitude and phase variables, the iSTFT operation is performed to generate each sub-band signal.

Finally, these sub-band signals are upsampled by inserting zeros between samples to match the sampling rate of the original signal and then integrated into full-band waveforms using a trainable synthesis filter.

### Training strategy

#### Data augmentation

In the original Soft-VC, HuBERT-Soft was applied to the any-to-one VC task.

To investigate whether the self-supervised features obtained by HuBERT-Soft still contain the features of the original speaker, we performed spectrogram-resize (SR) based data augmentation on the speech in the training dataset using the method in  [^Li2022FreeVC].

After data augmentation, we obtained speech with the same content and speed, but with a different speaker tone.

We hope that by training with augmented speech, the content encoder in the model will learn better to extract the unchanged content information. 

#### Speaker encoder input

In the training process, the speaker encoder is first fed with a different utterance from the same target speaker.

This helps the speaker encoder in the model to follow the speaker-related information in the speech at the start of training.

In the final steps of training, the target speech input is used to fine-tune the speaker encoder so that the model output is more similar to the reference speech.

#### Training loss

Following VITS, the QuickVC model combines VAE and adversarial training in the training process.

For the generator part, the loss can be expressed as:

$$

{\cal L}_{v a e}={\cal L}_{r e c o n}+{\cal L}_{k l}+{\cal L}_{a d v}(G)+{\cal L}_{f m}(G),
\label{eq0}

$$

where the ${\cal L}_{r e c o n}$ is reconstruction loss, which is the L1 distance between target mel-spectrogram $x_{m e l}$ and predicted mel-spectrogram ${\hat{x}}_{m e l}$:

$$

L_{r e c o n}=\Vert x_{m e l}-{\hat{x}}_{m e l}\Vert_{1} .
\label{eq1}

$$

The ${\cal L}_{k l}$ is KL loss, which is the KL divergence between the prior distribution $p_{\theta}(z|c)$ and posterior distribution $ q_{\phi}(z|x_{l i n})$, where

$$

{{q_{\phi}(z|x_{l i n})=N(z;\mu_{\phi}(x_{l i n}),\sigma_{\phi}(x_{l i n})),}} 
\label{eq2}

$$

$$

p_{\theta}(z|c)=N(f_{\theta}(z);\mu_{\theta}(c),\sigma_{\theta}(c))|d e t\frac{\partial f_{\theta}(z)}{\partial z}|.
\label{eq3}

$$

The $x_{l i n}$ is the linear spectrogram of the source speech.  $f_{\theta}$ means the normalizing flow.

The condition $c$ is the content information extracted from the source waveform by HuBERT-Soft.

The adversarial loss ${\cal L}_{a d v}(G)$ and the feature matching loss ${\cal L}_{f m}(G)$ is the adversarial training loss [^Mao2017Least], which is same as the VITS.

To adopt adversarial training in our learning system, we add a discriminator that discriminates between the output generated by the decoder and the ground truth waveform, the loss for the discriminator is ${\cal L}_{a d v}(D)$, which is also same as the VITS.

## 3·Experiment

### Dataset

We perform experiments on VCTK  [^Yamagishi2019CSTR], LibriSpeech  [^Panayotov2015Librispeech], and LJ Speech [^Ito2017Lj].

Only the VCTK corpus is used for training, which contains 44 hours of utterances from 107 English speakers with different accents.

We use the LibriSpeech and the LJ Speech dataset as source speech to test the ability of the model in the any-to-many VC scenario.% Most of the sentences in the VCTK corpus are extracted from newspapers and Rainbow Passage.

In particular, each speaker reads a different set of sentences. 

### Experimental setups

For our experiments, a sampling rate of 16,000 Hz is used.

We randomly split the utterances of a speaker into training and test sets in a 9:1 ratio.

Both linear spectrograms and 80-band mel-spectrograms are computed using short-time Fourier transform, with FFT, window, and hop size set to 1280, 1280, and 320, respectively.

Our models are trained on a single NVIDIA 3090 GPU for up to 600k steps, with a batch size of 64 and a maximum segment length of 512 frames.

The MS-iSTFT-Decoder uses the same FFT size, hop length, window length, number of sub-bands, and kernel size as those in MS-iSTFT-VITS.

The upsampling scale of the two residual blocks is [5,4].  With the same weight decay and learning rate as VITS, the AdamW optimizer  [^LoshchilovDecoupled] is used.

### Baselines

We select three baseline VC models to compare with our proposed model.

-  Diff-VCTK: a high-quality VC model that uses the diffusion model to generate high-quality speech [^Popov2022Diffusion-Based].

-  BNE-PPG-VC: a model that uses PPGs to obtain speech content features that do not include speaker features [^Liu2021Any-to-Many].

-  VQMIVC: a model that uses vector quantization for content encoding and introduces mutual information as a correlation metric [^Wang2021Vqmivc:].

In addition to the baseline model, our experiments also compare two versions of QuickVC.

-  QuickVC-nosr does not employ any data augmentation.

-  QuickVC-sr uses SR-based data augmentation when preparing the training dataset.

### Evaluation metrics

To obtain a comprehensive evaluation of the model, subjective and objective experiments were conducted separately. 

For the subjective experiments, following Wester et al.  [^Wester2016Analysis], we adopted Mean Opinion Score (MOS) to compute the naturalness and similarity scores of the converted utterances as subjective metrics.

We invited 80 subjects on Amazon Mechanical Turk to participate in all the experiments.

Forty subjects individually evaluated the naturalness of 6 original utterances from the dataset and 40 converted utterances.

Forty subjects individually evaluated the similarity of 2 utterances from each of the 6 speakers in the dataset and the similarity of the 30 converted utterances to the utterances of the target speakers. %We employed 30 participants on Amazon Mechanical Turk and evaluated the naturalness of the speech and the similarity of the speaker in terms of a 5-point Mean Opinion Score (MOS) and a Mean Opinion Score of Similarity (SMOS). 

For the objective experiments, we test speaker similarity,  intelligibility and inference speed.

We used the trained model Resemblyzer\footnote{https://github.com/resemble-ai/Resemblyzer} to score the speaker similarity between the converted speech and the target speech.

We randomly select 400 utterances (200 from VCTK, 200 from LibriSpeech) as source speech and 6 speakers from VCTK as target speakers.  Intelligibility is scored by the word error rate (WER) and character error rate (CER) between the source speech and the converted speech [^Zhao2021Improving].

WER and CER were obtained using the ASR model HuBERT-Large\footnote{https://huggingface.co/facebook/hubert-large-ls960-ft}.

The ASR model is trained with LibriSpeech, so only the utterances converted from LibriSpeech are used to score  intelligibility.

The inference speed of the models was measured separately for Intel(R) Core(TM) i9-10900K CPU @ 3.70GHz and NVIDIA GeForce RTX 3090 GPU.

The measurement of VC time refers to the time taken from the model's input of the source voice to the model's output of the converted voice.

### Results

#### Naturalness and similarity of speech

The experimental results of naturalness and similarity are shown in Table~[tab:mos](#tab:mos).

As can be seen from the N-MOS data of the subjective experiments, it is clear that QuickVC-nosr and QuickVC-sr outperform all baseline models in terms of speech naturalness.

In particular, QuickVC-sr achieves the best performance, proving that data augmentation helps the models to better extract content features for speech generation.

In terms of similarity, QuickVC-nosr and QuickVC-sr achieved similar scores to real speech in both S-MOS and Resemblyzer results, with QuickVC-sr scoring higher.

Both QuickVC and Diff-VCTK achieved competitive similarity scores, but QuickVC was better in naturalness, and the results of subsequent intelligibility and inference speed experiments showed that QuickVC was much better than Diff-VCTK.

<a id="tab:mos">Comparison of naturalness MOS (N-MOS), similarity MOS (S-MOS) with 95\% confidence intervals, and objective speaker similarity score obtained by Resemblyzer.</a>

#### Speech intelligibility

Word error rate (WER) and character error rate (CER) between source and converted speech represent the intelligibility of the generated speech.

It can be seen in Table~[tab:wer](#tab:wer) that our proposed models achieve lower WER and CER than all baseline models.

This indicates that the proposed method can preserve the linguistic content of source speech well.

Besides, we observe that training with data augmentation improves speech intelligence slightly.

<a id="tab:wer">Comparison of  intelligibility (WER, CER).</a>

#### Speed of voice conversion

Table~[tab:speed](#tab:speed) shows that the inference speed of our approach is the most competitive.

Our model is able to generate samples at over 5000 KHz on the 3090 GPU and over 250 KHz on the i9-10900K CPU.

Considering that our method is fast while maintaining a high level of naturalness, our methods have an obvious advantage when used in real-time VC systems.

<a id="tab:speed">Comparison of  inference speed(in KHz).</a>

#### Latent embedding visualization

The speaker embedding $g$ is the output of the speaker encoder in the QuickVC model.

In Figure~[fig:speaker_embedding](#fig:speaker_embedding) we plot the speaker embedding $g$ of 107 speakers in the VCTK dataset using the Barnes-Hut t-SNE method [^Van2014Accelerating].

Each speaker has about 10 audio samples.

Based on the speaker embeddings, utterances of the same speaker are clustered together, while those of different speakers are well separated.

The 2D speaker embedding demonstrates that the speaker encoder of QuickVC-sr and QuickVC-nosr successfully extracts speaker information.

![](pca6nosr.png)

<a id="fig:speaker_embedding">The Barnes-Hut t-SNE visualization of the speaker embeddings.</a>

## 4·Conclusion

In this study, we propose a VITS-based voice conversion model called QuickVC.

To achieve fast and high-quality voice conversion, we choose to extract speech content features using HuBERT-Soft and reconstruct the speech using VITS.

At the same time, to make the model capable of fast speech synthesis for practical on-device applications, we refer to MS-iSTFT-VITS, reduce the redundancy of decoder computation in the VITS backbone by iSTFT, and adopt a multi-band parallel strategy.

The experimental results show a competitive quality of speech synthesized by the model and a competitive level of synthesis speed.

However, in the case of zero-shot VC, the model shows a significant decrease in speaker similarity.

Future work will mainly focus on this part.

\bibliographystyle{IEEEtran}
\bibliography{mybib}

\end{document}

## References

[^Kaneko2018Cyclegan-Vc]: Cyclegan-Vc: Non-Parallel Voice Conversion Using Cycle-Consistent Adversarial Networks. 2018 26th European Signal Processing Conference (EUSIPCO) 2018.
[^Qian2019Autovc]: Autovc: Zero-Shot Voice Style Transfer With Only Autoencoder Loss. International Conference on Machine Learning 2019.
[^Liu2021Any-to-Many]: Any-to-Many Voice Conversion With Location-Relative Sequence-to-Sequence Modeling. IEEE/ACM Transactions on Audio, Speech, and Language Processing 2021.
[^Li2021PPG-Based]: PPG-Based Singing Voice Conversion With Adversarial Representation Learning. ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2021.
[^Hazen2009Query-by-Example]: Query-by-Example Spoken Term Detection Using Phonetic Posteriorgram Templates. 2009 IEEE Workshop on Automatic Speech Recognition \& Understanding 2009.
[^Zhang2021Transfer]: Transfer Learning From Speech Synthesis to Voice Conversion With Non-Parallel Training Data. IEEE/ACM Transactions on Audio, Speech, and Language Processing 2021.
[^Kaneko2019Stargan-Vc2]: Stargan-Vc2: Rethinking Conditional Methods for Stargan-Based Voice Conversion. arXiv:1907.12279.
[^Zhao2020Voice]: Voice Conversion Challenge 2020: Intra-Lingual Semi-Parallel and Cross-Lingual Voice Conversion. arXiv:2008.12527.
[^Huang2021Any-to-One]: Any-to-One Sequence-to-Sequence Voice Conversion Using Self-Supervised Discrete Speech Representations. ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2021.
[^Niekerk2022Comparison]: A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion. ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2022.
[^Li2022FreeVC]: FreeVC: Towards High-Quality Text-Free One-Shot Voice Conversion. 
[^Shen2018Natural]: Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions. 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2018.
[^Ren2019Fastspeech]: Fastspeech: Fast, Robust and Controllable Text to Speech. Advances in Neural Information Processing Systems 2019.
[^Zhao2021Towards]: Towards Natural and Controllable Cross-Lingual Voice Conversion Based on Neural TTS Model and Phonetic Posteriorgram. ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2021.
[^Kim2021Conditional]: Conditional Variational Autoencoder With Adversarial Learning for End-to-End Text-to-Speech. International Conference on Machine Learning 2021.
[^Saeki2020Real-Time,]: Real-Time, Full-Band, Online DNN-Based Voice Conversion System Using a Single CPU.. Interspeech 2020.
[^Saeki2021Real-Time]: Real-Time Full-Band Voice Conversion With Sub-Band Modeling and Data-Driven Phase Estimation of Spectral Differentials. IEICE TRANSACTIONS on Information and Systems 2021.
[^Kawamura2022Lightweight]: Lightweight and High-Fidelity End-to-End Text-to-Speech With Multi-Band Generation and Inverse Short-Time Fourier Transform. 
[^Kong2020Hifi-Gan]: Hifi-Gan: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis. Advances in Neural Information Processing Systems 2020.
[^Kaneko2022iSTFTNet]: iSTFTNet: Fast and Lightweight Mel-Spectrogram Vocoder Incorporating Inverse Short-Time Fourier Transform. ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2022.
[^Hsu2021Hubert]: Hubert: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units. IEEE/ACM Transactions on Audio, Speech, and Language Processing 2021.
[^He2016Deep]: Deep Residual Learning for Image Recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2016.
[^Mao2017Least]: Least Squares Generative Adversarial Networks. Proceedings of the IEEE International Conference on Computer Vision 2017.
[^Yamagishi2019CSTR]: CSTR VCTK Corpus: English Multi-Speaker Corpus for CSTR Voice Cloning Toolkit (Version 0.92). University of Edinburgh. The Centre for Speech Technology Research (CSTR) 2019.
[^Panayotov2015Librispeech]: Librispeech: An Asr Corpus Based on Public Domain Audio Books. 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2015.
[^Ito2017Lj]: The LJ Speech Dataset. 
[^LoshchilovDecoupled]: Decoupled Weight Decay Regularization. International Conference on Learning Representations .
[^Popov2022Diffusion-Based]: Diffusion-Based Voice Conversion With Fast Maximum Likelihood Sampling Scheme. The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022 2022.
[^Wang2021Vqmivc:]: {VQMIVC:} Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-Shot Voice Conversion. Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 30 August - 3 September 2021 2021.
[^Wester2016Analysis]: Analysis of the Voice Conversion Challenge 2016 Evaluation Results.. Interspeech 2016.
[^Zhao2021Improving]: Improving Model Stability and Training Efficiency in Fast, High Quality Expressive Voice Conversion System. Companion Publication of the 2021 International Conference on Multimodal Interaction 2021.
[^Van2014Accelerating]: Accelerating T-Sne Using Tree-Based Algorithms. The Journal of Machine Learning Research 2014.