# Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion."
- 作者:
  - 01 Yu Zhang
  - 02 Baotong Tian
  - 03 Zhiyao Duan
- 链接:
  - [ArXiv](https://arxiv.org/abs/2507.14534v3)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2507.14534v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.07.19_2507.14534v1_Conan__A_Chunkwise_Online_Network_for_Zero-Shot_Adaptive_Voice_Conversion.pdf)
  - [ArXiv:2507.14534v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.07.19_2507.14534v2_Conan__A_Chunkwise_Online_Network_for_Zero-Shot_Adaptive_Voice_Conversion.pdf)
  - [ArXiv:2507.14534v3](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.07.19_2507.14534v3_Conan__A_Chunkwise_Online_Network_for_Zero-Shot_Adaptive_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

Zero-shot online voice conversion (VC) holds significant promise for real-time communications and entertainment. 
However, current VC models struggle to preserve semantic fidelity under real-time constraints, deliver natural-sounding conversions, and adapt effectively to unseen speaker characteristics.
To address these challenges, we introduce Conan, a chunkwise online zero-shot voice conversion model that preserves the content of the source while matching the speaker identity of reference speech.
Conan comprises three core components: 
1) a Stream Content Extractor that leverages Emformer for low-latency streaming content encoding; 
2) an Adaptive Style Encoder that extracts fine-grained stylistic features from reference speech for enhanced style adaptation; 
3) a Causal Shuffle Vocoder that implements a fully causal HiFiGAN using a pixel-shuffle mechanism. 
Experimental evaluations demonstrate that Conan outperforms baseline models in subjective and objective metrics.
Audio samples can be found at \url{https://aaronz345.github.io/ConanDemo}.

## 1·Introduction

Zero-shot online voice conversion (VC) aims to extract the linguistic content from source speech and, in real time, modify its speaker identity to match that of a reference speech [^Wang2024Streamvoice]. 
This field has shown considerable potential for applications in real-time communication, interactive entertainment, virtual humans, and multimedia production [^Chen2022Controlvc].

Existing VC approaches typically employ pre-trained feature-extraction networks to extract content information from the source speech and combine it with speaker identity embeddings from the reference speech to perform chunkwise online conversion [^Yang2024Streamvc]. 

The field of VC has seen substantial advances [^Wang2021Vqmivc]. 
In particular, zero‐shot VC, which aims to transfer unseen speaker identity from reference speech to generated speech, has been explored in recent years [^Qian2019Autovc]. 
Moreover, real-time VC systems capable of operating online have been successfully deployed in low-latency scenarios using chunkwise streaming methods [^Yang2022Streamable].

However, these methods still fall short of achieving the semantic fidelity, style transfer, and naturalness required to meet the growing demand for high‑quality, customizable audio experiences [^Zhang2025TCSinger].

For chunkwise online source speech input, it is essential not only to maintain both content extraction and speech synthesis quality under strict online latency constraints but also to transfer the unseen speaker identity, which encompasses timbre and styles (like emotion and prosody) [^Zhang2025ISDrama].

Currently, zero-shot online VC faces three major challenges:

-  **Content extraction via chunkwise streaming often compromises quality.** 
Existing VC methods typically employ pretrained feature extraction models to derive linguistic representations, often leveraging self-supervised models such as HuBERT [^Van2022Comparison] or WavLM [^Chen2022Wavlm].

However, the large receptive fields of these pretrained encoders make them unsuitable for online scenarios with minimal or zero lookahead. 
Attempts to train a lightweight causal content encoder via distillation may have fallen short in efficiency and accuracy [^Yang2024Streamvc]. 
Alternatively, some approaches rely on phonetic extraction based on automatic speech recognition [^Sun2016Phonetic], which incurs additional latency.

-  **Achieving style transfer in zero-shot scenarios is still challenging.**

Most VC models rely on the strong assumption that the reference voice is accessible [^Chen2021Adaspeech] for model adaptation, which does not always hold in practice.

Moreover, many VC approaches focus solely on timbre transfer by directly applying the source speech’s F0 and energy, which contradicts the goal of capturing the reference speaker identity [^Yang2024Streamvc]. 
The handful of zero-shot VC methods that do attempt to generalize to unseen speakers still struggle to balance high speaker similarity with naturalness.

This shortfall arises from insufficient modeling of speaker identity, which prevents effective alignment of speaker identity with source content [^Yang2022Streamable].

-  **Maintaining high quality and naturalness in real-time VC remains difficult.**

In online VC, the system must process source speech chunk by chunk and synthesize generated speech incrementally, which prevents the direct use of conventional VC models and leads to poor naturalness at chunk boundaries [^Qian2019Autovc]. 
Leveraging causality with a constrained look-ahead window is a common approach to address this issue.

Many prior methods employ autoregressive decoders to enforce temporal causality; however, a frame‑by‑frame autoregressive approach often degrades the quality of early synthesized segments and incurs additional computational overhead for the model [^Wang2024Streamvoice].

Additionally, streaming models often convert non-causal vocoders into causal ones by zero-padding, introducing spectral artifacts that degrade quality and efficiency [^Quamer2024End-to-End].

To address these challenges above, we introduce **Conan**, a **c**hunkwise **o**nline **n**etwork for zero-shot
**a**daptive voice conversio**n**.

In zero‐shot online scenarios, Conan preserves the high‐fidelity content of the source speech while matching the unseen speaker identity of reference speech, generating highly natural speech on a chunkwise basis.

To achieve high-quality chunkwise streaming content extraction, we design the **Stream Content Extractor**, built on an Emformer architecture and trained using content representations extracted offline by HuBERT.

To enable adaptive style transfer, we introduce the **Adaptive Style Encoder**, which employs clustering-based vector quantization to capture detailed speaker attributes from reference speech and uses an alignment attention mechanism to fuse them with content and timbre information.

For high-quality natural streaming speech synthesis, we adopt a causal-convolution mel decoder and propose the **Causal Shuffle Vocoder**, a fully causal HiFiGAN that leverages a pixel-shuffle mechanism to eliminate checkerboard artifacts while improving computational efficiency.

Experimental evaluations show that Conan outperforms baseline models in real-time content accuracy, quality, and speaker similarity.

Conan can achieve a latency as low as 37 ms for the conversion on a single A100 GPU without any engineering optimizations.

## 2·Related Work

### Zero-Shot Voice Conversion

In practical scenarios, obtaining reference speech during training is often impractical. 
Therefore, zero-shot VC models are essential to capture and control speaker identity by disentangling content from speaker characteristics.

Chou et al. employ instance normalization to separate speaker and content [^Chou2019One-Shot], while Wang et al. retrieve variable‐length speaker embeddings along both temporal and channel dimensions under the guidance of a pre‐trained speaker‐verification model [^Wang2023Multi-Level].

Ebbers et al. leverage adversarial contrastive predictive coding for fully unsupervised separation of content and speaker [^Ebbers2021Contrastive].

AutoVC squeezes out speaker timbre from content embeddings using an information bottleneck [^Qian2019Autovc], and VQMIVC encodes content via vector quantization, applying mutual information constraints to decorrelate speech components [^Wang2021Vqmivc]. 
Some approaches obtain speaker representations through a speaker‐verification model and extract content from Automatic Speech Recognition (ASR) posteriorgrams [^Sun2016Phonetic]. 
ControlVC [^Chen2022Controlvc] uses pretrained encoders to get content and speaker embeddings and applies TD-PSOLA and pitch contour manipulation for time-varying speed and pitch control. 
NANSY trains in a fully self‐supervised manner using wav2vec features alongside Yingram [^Choi2021Neural], and LM-VC tokenizes speech into semantic tokens via HuBERT and acoustic tokens via SoundStream [^Wang2023Lm-Vc]. 
Although StreamVoice further advances real‐time separation and recombination of speaker and content using a language model and ASR [^Wang2024Streamvoice], current online VC systems still exhibit considerable room for improvement in transferring unseen reference speaker identity and in accurately extracting linguistic content.

### Online Voice Conversion

In online VC, the system must process source speech chunk by chunk and synthesize output incrementally, without access to future context.

This often degrades naturalness at chunk boundaries. 
For streaming applications, causal processing is a critical design consideration.

Hayashi et al. [^Hayashi2022Investigation] propose a streamable version of the non‐autoregressive sequence‐to‐sequence VC model based on FastSpeech2 [^Ren2020Fastspeech] and NAR‐S2S‐VC [^Hayashi2021Non-Autoregressive], incorporating causal convolutions and self‐attention with causal masks. 
FastS2S‐VC [^Kameoka2021FastS2S-Vc] learns to predict attention distributions from source speech and reference speaker indices alone, guided by a teacher model. 
IBF-VC [^Chen2023Streaming] uses the Intermediate Bottleneck Features (IBFs) to replace Phonetic Posteriorgrams (PPGs) in the ASR encoder to capture more fine-grained prosody information, and applies non-streaming teacher guidance for the timbre leakage problem. 
DualVC [^Ning2024Dualvc] leverages dynamic masked convolution to use the within-chunk future information better.

Yang et al. [^Yang2022Streamable] adapt the originally offline VQMIVC model [^Wang2021Vqmivc] for real‐time, chunk‐by‐chunk processing. 
ALO‐VC [^Wang2023Alo-Vc] assembles a streaming system composed of a speaker‐verification model, a streamable phonetic‐posteriorgram extractor, and an F0 extractor. 
StreamVC [^Yang2024Streamvc] demonstrates that a lightweight causal convolutional network can effectively capture soft speech‐unit representations, while StreamVoice [^Wang2024Streamvoice] introduces autoregressive decoders to enforce temporal causality. 

Quamer [^Quamer2024End-to-End] convert non-causal vocoders into causal ones by zero-padding, [^Quamer2024End-to-End].

However, methods relying on frame-by-frame autoregressive structures degrade the quality of early segments and incur extra computational overhead, and zero-padding causal vocoders suffer from artifacts, limiting overall naturalness. 

### Style Modeling

Modeling speaking identity remains a central challenge in speech research.

Prior approaches have largely relied on pre-trained models to capture only a limited set of styles, like wav2vec 2.0 [^Baevski2020Wav2vec], HuBERT [^Hsu2021Hubert], and WavLM [^Chen2022Wavlm]. 
Attentron [^Choi2020Attentron] introduces an attention mechanism to extract speaker identity from reference samples. 
ZSM-SS [^Kumar2021Normalization] proposes a Transformer-based architecture with an external speaker encoder based on wav2vec 2.0. 
Daft-Exprt [^Za{\i}di2021Daft-Exprt] employs a gradient reversal layer to improve reference speaker fidelity in style transfer. 
GenerSpeech introduces both global and local style adapters to capture diverse speaking identity [^Huang2022Generspeech], while Styler decomposes style into multiple levels of supervision [^Lee2021Styler]. 
Yang et al. [^Yang2022Streamable] jointly model speaker identity and global prosody using a GST-based style token network.

Mega-TTS 2 employs vector quantization for prosody encoding combined with a language model for prosody transfer [^Jiang2024Mega-TTS], and NaturalSpeech 3 uses factorized vector quantization to disentangle prosodic features [^Ju2024Naturalspeech]. 
CosyVoice integrates x-vectors into a large language model to both disentangle and model prosody [^Du2024Cosyvoice]. 
Most of these methods focus on offline speech synthesis, while our work addresses the challenge of zero-shot online voice conversion by modeling rich speaking identity and maintaining precise content alignment.

![](figures/arch.pdf)

<a id="fig: arch">The overall architecture of Conan (a). 
Online source speech is fed into the system in chunks, and the synthesized output is likewise produced on a chunkwise basis.

In (b), the size of the right‐context chunk R is configurable; when it is set to zero, the model operates in a chunkwise causal mode.

Here, $C_i^n$ is the content feature of the $i$-th chunk at layer $n$, $R_i^n$ is the right‐context block, $s_i^n$ is the chunk summary, and $M_{i}^n$ is the memory bank from the previous chunks.

Attention is computed using $(Q_i^n,K_i^n,V_i^n)$.

The output summary $m_{i}^{\,n+1}$ is also obtained and input to the next chunk in the upper layer.</a>
