# Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion."
- 作者:
  - 01 Yu Zhang
  - 02 Baotong Tian
  - 03 Zhiyao Duan
- 链接:
  - [ArXiv](https://arxiv.org/abs/2507.14534v3)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2507.14534v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.07.19_2507.14534v1_Conan__A_Chunkwise_Online_Network_for_Zero-Shot_Adaptive_Voice_Conversion.pdf)
  - [ArXiv:2507.14534v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.07.19_2507.14534v2_Conan__A_Chunkwise_Online_Network_for_Zero-Shot_Adaptive_Voice_Conversion.pdf)
  - [ArXiv:2507.14534v3](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.07.19_2507.14534v3_Conan__A_Chunkwise_Online_Network_for_Zero-Shot_Adaptive_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

Zero-shot online voice conversion (VC) holds significant promise for real-time communications and entertainment. 
However, current VC models struggle to preserve semantic fidelity under real-time constraints, deliver natural-sounding conversions, and adapt effectively to unseen speaker characteristics.
To address these challenges, we introduce Conan, a chunkwise online zero-shot voice conversion model that preserves the content of the source while matching the speaker identity of reference speech.
Conan comprises three core components: 
1) a Stream Content Extractor that leverages Emformer for low-latency streaming content encoding; 
2) an Adaptive Style Encoder that extracts fine-grained stylistic features from reference speech for enhanced style adaptation; 
3) a Causal Shuffle Vocoder that implements a fully causal HiFiGAN using a pixel-shuffle mechanism. 
Experimental evaluations demonstrate that Conan outperforms baseline models in subjective and objective metrics.
Audio samples can be found at \url{https://aaronz345.github.io/ConanDemo}.

## 1·Introduction

Zero-shot online voice conversion (VC) aims to extract the linguistic content from source speech and, in real time, modify its speaker identity to match that of a reference speech [^Wang2024Streamvoice]. 
This field has shown considerable potential for applications in real-time communication, interactive entertainment, virtual humans, and multimedia production [^Chen2022Controlvc].

Existing VC approaches typically employ pre-trained feature-extraction networks to extract content information from the source speech and combine it with speaker identity embeddings from the reference speech to perform chunkwise online conversion [^Yang2024Streamvc]. 

The field of VC has seen substantial advances [^Wang2021Vqmivc]. 
In particular, zero‐shot VC, which aims to transfer unseen speaker identity from reference speech to generated speech, has been explored in recent years [^Qian2019Autovc]. 
Moreover, real-time VC systems capable of operating online have been successfully deployed in low-latency scenarios using chunkwise streaming methods [^Yang2022Streamable].

However, these methods still fall short of achieving the semantic fidelity, style transfer, and naturalness required to meet the growing demand for high‑quality, customizable audio experiences [^Zhang2025TCSinger].

For chunkwise online source speech input, it is essential not only to maintain both content extraction and speech synthesis quality under strict online latency constraints but also to transfer the unseen speaker identity, which encompasses timbre and styles (like emotion and prosody) [^Zhang2025ISDrama].

Currently, zero-shot online VC faces three major challenges:

-  **Content extraction via chunkwise streaming often compromises quality.** 
Existing VC methods typically employ pretrained feature extraction models to derive linguistic representations, often leveraging self-supervised models such as HuBERT [^Van2022Comparison] or WavLM [^Chen2022Wavlm].

However, the large receptive fields of these pretrained encoders make them unsuitable for online scenarios with minimal or zero lookahead. 
Attempts to train a lightweight causal content encoder via distillation may have fallen short in efficiency and accuracy [^Yang2024Streamvc]. 
Alternatively, some approaches rely on phonetic extraction based on automatic speech recognition [^Sun2016Phonetic], which incurs additional latency.

-  **Achieving style transfer in zero-shot scenarios is still challenging.**

Most VC models rely on the strong assumption that the reference voice is accessible [^Chen2021Adaspeech] for model adaptation, which does not always hold in practice.

Moreover, many VC approaches focus solely on timbre transfer by directly applying the source speech’s F0 and energy, which contradicts the goal of capturing the reference speaker identity [^Yang2024Streamvc]. 
The handful of zero-shot VC methods that do attempt to generalize to unseen speakers still struggle to balance high speaker similarity with naturalness.

This shortfall arises from insufficient modeling of speaker identity, which prevents effective alignment of speaker identity with source content [^Yang2022Streamable].

-  **Maintaining high quality and naturalness in real-time VC remains difficult.**

In online VC, the system must process source speech chunk by chunk and synthesize generated speech incrementally, which prevents the direct use of conventional VC models and leads to poor naturalness at chunk boundaries [^Qian2019Autovc]. 
Leveraging causality with a constrained look-ahead window is a common approach to address this issue.

Many prior methods employ autoregressive decoders to enforce temporal causality; however, a frame‑by‑frame autoregressive approach often degrades the quality of early synthesized segments and incurs additional computational overhead for the model [^Wang2024Streamvoice].

Additionally, streaming models often convert non-causal vocoders into causal ones by zero-padding, introducing spectral artifacts that degrade quality and efficiency [^Quamer2024End-to-End].

To address these challenges above, we introduce **Conan**, a **c**hunkwise **o**nline **n**etwork for zero-shot
**a**daptive voice conversio**n**.

In zero‐shot online scenarios, Conan preserves the high‐fidelity content of the source speech while matching the unseen speaker identity of reference speech, generating highly natural speech on a chunkwise basis.

To achieve high-quality chunkwise streaming content extraction, we design the **Stream Content Extractor**, built on an Emformer architecture and trained using content representations extracted offline by HuBERT.

To enable adaptive style transfer, we introduce the **Adaptive Style Encoder**, which employs clustering-based vector quantization to capture detailed speaker attributes from reference speech and uses an alignment attention mechanism to fuse them with content and timbre information.

For high-quality natural streaming speech synthesis, we adopt a causal-convolution mel decoder and propose the **Causal Shuffle Vocoder**, a fully causal HiFiGAN that leverages a pixel-shuffle mechanism to eliminate checkerboard artifacts while improving computational efficiency.

Experimental evaluations show that Conan outperforms baseline models in real-time content accuracy, quality, and speaker similarity.

Conan can achieve a latency as low as 37 ms for the conversion on a single A100 GPU without any engineering optimizations.
