# Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion."
- 作者:
  - 01 Yu Zhang
  - 02 Baotong Tian
  - 03 Zhiyao Duan
- 链接:
  - [ArXiv](https://arxiv.org/abs/2507.14534v3)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2507.14534v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.07.19_2507.14534v1_Conan__A_Chunkwise_Online_Network_for_Zero-Shot_Adaptive_Voice_Conversion.pdf)
  - [ArXiv:2507.14534v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.07.19_2507.14534v2_Conan__A_Chunkwise_Online_Network_for_Zero-Shot_Adaptive_Voice_Conversion.pdf)
  - [ArXiv:2507.14534v3](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.07.19_2507.14534v3_Conan__A_Chunkwise_Online_Network_for_Zero-Shot_Adaptive_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

Zero-shot online voice conversion (VC) holds significant promise for real-time communications and entertainment. 
However, current VC models struggle to preserve semantic fidelity under real-time constraints, deliver natural-sounding conversions, and adapt effectively to unseen speaker characteristics.
To address these challenges, we introduce Conan, a chunkwise online zero-shot voice conversion model that preserves the content of the source while matching the speaker identity of reference speech.
Conan comprises three core components: 
1) a Stream Content Extractor that leverages Emformer for low-latency streaming content encoding; 
2) an Adaptive Style Encoder that extracts fine-grained stylistic features from reference speech for enhanced style adaptation; 
3) a Causal Shuffle Vocoder that implements a fully causal HiFiGAN using a pixel-shuffle mechanism. 
Experimental evaluations demonstrate that Conan outperforms baseline models in subjective and objective metrics.
Audio samples can be found at \url{https://aaronz345.github.io/ConanDemo}.

## 1·Introduction

Zero-shot online voice conversion (VC) aims to extract the linguistic content from source speech and, in real time, modify its speaker identity to match that of a reference speech [^Wang2024Streamvoice]. 
This field has shown considerable potential for applications in real-time communication, interactive entertainment, virtual humans, and multimedia production [^Chen2022Controlvc].

Existing VC approaches typically employ pre-trained feature-extraction networks to extract content information from the source speech and combine it with speaker identity embeddings from the reference speech to perform chunkwise online conversion [^Yang2024Streamvc]. 

The field of VC has seen substantial advances [^Wang2021Vqmivc]. 
In particular, zero‐shot VC, which aims to transfer unseen speaker identity from reference speech to generated speech, has been explored in recent years [^Qian2019Autovc]. 
Moreover, real-time VC systems capable of operating online have been successfully deployed in low-latency scenarios using chunkwise streaming methods [^Yang2022Streamable].

However, these methods still fall short of achieving the semantic fidelity, style transfer, and naturalness required to meet the growing demand for high‑quality, customizable audio experiences [^Zhang2025TCSinger].

For chunkwise online source speech input, it is essential not only to maintain both content extraction and speech synthesis quality under strict online latency constraints but also to transfer the unseen speaker identity, which encompasses timbre and styles (like emotion and prosody) [^Zhang2025ISDrama].

Currently, zero-shot online VC faces three major challenges:

-  **Content extraction via chunkwise streaming often compromises quality.** 
Existing VC methods typically employ pretrained feature extraction models to derive linguistic representations, often leveraging self-supervised models such as HuBERT [^Van2022Comparison] or WavLM [^Chen2022Wavlm].

However, the large receptive fields of these pretrained encoders make them unsuitable for online scenarios with minimal or zero lookahead. 
Attempts to train a lightweight causal content encoder via distillation may have fallen short in efficiency and accuracy [^Yang2024Streamvc]. 
Alternatively, some approaches rely on phonetic extraction based on automatic speech recognition [^Sun2016Phonetic], which incurs additional latency.

-  **Achieving style transfer in zero-shot scenarios is still challenging.**

Most VC models rely on the strong assumption that the reference voice is accessible [^Chen2021Adaspeech] for model adaptation, which does not always hold in practice.

Moreover, many VC approaches focus solely on timbre transfer by directly applying the source speech’s F0 and energy, which contradicts the goal of capturing the reference speaker identity [^Yang2024Streamvc]. 
The handful of zero-shot VC methods that do attempt to generalize to unseen speakers still struggle to balance high speaker similarity with naturalness.

This shortfall arises from insufficient modeling of speaker identity, which prevents effective alignment of speaker identity with source content [^Yang2022Streamable].

-  **Maintaining high quality and naturalness in real-time VC remains difficult.**

In online VC, the system must process source speech chunk by chunk and synthesize generated speech incrementally, which prevents the direct use of conventional VC models and leads to poor naturalness at chunk boundaries [^Qian2019Autovc]. 
Leveraging causality with a constrained look-ahead window is a common approach to address this issue.

Many prior methods employ autoregressive decoders to enforce temporal causality; however, a frame‑by‑frame autoregressive approach often degrades the quality of early synthesized segments and incurs additional computational overhead for the model [^Wang2024Streamvoice].

Additionally, streaming models often convert non-causal vocoders into causal ones by zero-padding, introducing spectral artifacts that degrade quality and efficiency [^Quamer2024End-to-End].

To address these challenges above, we introduce **Conan**, a **c**hunkwise **o**nline **n**etwork for zero-shot
**a**daptive voice conversio**n**.

In zero‐shot online scenarios, Conan preserves the high‐fidelity content of the source speech while matching the unseen speaker identity of reference speech, generating highly natural speech on a chunkwise basis.

To achieve high-quality chunkwise streaming content extraction, we design the **Stream Content Extractor**, built on an Emformer architecture and trained using content representations extracted offline by HuBERT.

To enable adaptive style transfer, we introduce the **Adaptive Style Encoder**, which employs clustering-based vector quantization to capture detailed speaker attributes from reference speech and uses an alignment attention mechanism to fuse them with content and timbre information.

For high-quality natural streaming speech synthesis, we adopt a causal-convolution mel decoder and propose the **Causal Shuffle Vocoder**, a fully causal HiFiGAN that leverages a pixel-shuffle mechanism to eliminate checkerboard artifacts while improving computational efficiency.

Experimental evaluations show that Conan outperforms baseline models in real-time content accuracy, quality, and speaker similarity.

Conan can achieve a latency as low as 37 ms for the conversion on a single A100 GPU without any engineering optimizations.

## 2·Related Work

### Zero-Shot Voice Conversion

In practical scenarios, obtaining reference speech during training is often impractical. 
Therefore, zero-shot VC models are essential to capture and control speaker identity by disentangling content from speaker characteristics.

Chou et al. employ instance normalization to separate speaker and content [^Chou2019One-Shot], while Wang et al. retrieve variable‐length speaker embeddings along both temporal and channel dimensions under the guidance of a pre‐trained speaker‐verification model [^Wang2023Multi-Level].

Ebbers et al. leverage adversarial contrastive predictive coding for fully unsupervised separation of content and speaker [^Ebbers2021Contrastive].

AutoVC squeezes out speaker timbre from content embeddings using an information bottleneck [^Qian2019Autovc], and VQMIVC encodes content via vector quantization, applying mutual information constraints to decorrelate speech components [^Wang2021Vqmivc]. 
Some approaches obtain speaker representations through a speaker‐verification model and extract content from Automatic Speech Recognition (ASR) posteriorgrams [^Sun2016Phonetic]. 
ControlVC [^Chen2022Controlvc] uses pretrained encoders to get content and speaker embeddings and applies TD-PSOLA and pitch contour manipulation for time-varying speed and pitch control. 
NANSY trains in a fully self‐supervised manner using wav2vec features alongside Yingram [^Choi2021Neural], and LM-VC tokenizes speech into semantic tokens via HuBERT and acoustic tokens via SoundStream [^Wang2023Lm-Vc]. 
Although StreamVoice further advances real‐time separation and recombination of speaker and content using a language model and ASR [^Wang2024Streamvoice], current online VC systems still exhibit considerable room for improvement in transferring unseen reference speaker identity and in accurately extracting linguistic content.

### Online Voice Conversion

In online VC, the system must process source speech chunk by chunk and synthesize output incrementally, without access to future context.

This often degrades naturalness at chunk boundaries. 
For streaming applications, causal processing is a critical design consideration.

Hayashi et al. [^Hayashi2022Investigation] propose a streamable version of the non‐autoregressive sequence‐to‐sequence VC model based on FastSpeech2 [^Ren2020Fastspeech] and NAR‐S2S‐VC [^Hayashi2021Non-Autoregressive], incorporating causal convolutions and self‐attention with causal masks. 
FastS2S‐VC [^Kameoka2021FastS2S-Vc] learns to predict attention distributions from source speech and reference speaker indices alone, guided by a teacher model. 
IBF-VC [^Chen2023Streaming] uses the Intermediate Bottleneck Features (IBFs) to replace Phonetic Posteriorgrams (PPGs) in the ASR encoder to capture more fine-grained prosody information, and applies non-streaming teacher guidance for the timbre leakage problem. 
DualVC [^Ning2024Dualvc] leverages dynamic masked convolution to use the within-chunk future information better.

Yang et al. [^Yang2022Streamable] adapt the originally offline VQMIVC model [^Wang2021Vqmivc] for real‐time, chunk‐by‐chunk processing. 
ALO‐VC [^Wang2023Alo-Vc] assembles a streaming system composed of a speaker‐verification model, a streamable phonetic‐posteriorgram extractor, and an F0 extractor. 
StreamVC [^Yang2024Streamvc] demonstrates that a lightweight causal convolutional network can effectively capture soft speech‐unit representations, while StreamVoice [^Wang2024Streamvoice] introduces autoregressive decoders to enforce temporal causality. 

Quamer [^Quamer2024End-to-End] convert non-causal vocoders into causal ones by zero-padding, [^Quamer2024End-to-End].

However, methods relying on frame-by-frame autoregressive structures degrade the quality of early segments and incur extra computational overhead, and zero-padding causal vocoders suffer from artifacts, limiting overall naturalness. 

### Style Modeling

Modeling speaking identity remains a central challenge in speech research.

Prior approaches have largely relied on pre-trained models to capture only a limited set of styles, like wav2vec 2.0 [^Baevski2020Wav2vec], HuBERT [^Hsu2021Hubert], and WavLM [^Chen2022Wavlm]. 
Attentron [^Choi2020Attentron] introduces an attention mechanism to extract speaker identity from reference samples. 
ZSM-SS [^Kumar2021Normalization] proposes a Transformer-based architecture with an external speaker encoder based on wav2vec 2.0. 
Daft-Exprt [^Za{\i}di2021Daft-Exprt] employs a gradient reversal layer to improve reference speaker fidelity in style transfer. 
GenerSpeech introduces both global and local style adapters to capture diverse speaking identity [^Huang2022Generspeech], while Styler decomposes style into multiple levels of supervision [^Lee2021Styler]. 
Yang et al. [^Yang2022Streamable] jointly model speaker identity and global prosody using a GST-based style token network.

Mega-TTS 2 employs vector quantization for prosody encoding combined with a language model for prosody transfer [^Jiang2024Mega-TTS], and NaturalSpeech 3 uses factorized vector quantization to disentangle prosodic features [^Ju2024Naturalspeech]. 
CosyVoice integrates x-vectors into a large language model to both disentangle and model prosody [^Du2024Cosyvoice]. 
Most of these methods focus on offline speech synthesis, while our work addresses the challenge of zero-shot online voice conversion by modeling rich speaking identity and maintaining precise content alignment.

![](figures/arch.pdf)

<a id="fig: arch">The overall architecture of Conan (a). 
Online source speech is fed into the system in chunks, and the synthesized output is likewise produced on a chunkwise basis.

In (b), the size of the right‐context chunk R is configurable; when it is set to zero, the model operates in a chunkwise causal mode.

Here, $C_i^n$ is the content feature of the $i$-th chunk at layer $n$, $R_i^n$ is the right‐context block, $s_i^n$ is the chunk summary, and $M_{i}^n$ is the memory bank from the previous chunks.

Attention is computed using $(Q_i^n,K_i^n,V_i^n)$.

The output summary $m_{i}^{\,n+1}$ is also obtained and input to the next chunk in the upper layer.</a>

## 3·Method

### Overview

The architecture of Conan is shown in Figure [fig: arch](#fig: arch) (a).

Source speech is input online and fed into the system in chunks. 
In the Stream Content Extractor, it is extracted as content label $z_{c-i}$ for the $i$-th chunk. 
Since the offline HuBERT content representation format is used, each content label corresponds to a 20 ms WAV segment.

The complete unseen reference speech is passed through a mel extractor to obtain reference mel $m_{rf}$, which is then input to the timbre encoder to produce a global speaker timbre embedding $z_t$.

Additionally, $m_{rf}$ is also fed into the Adaptive Style Encoder to extract chunk-level style representations that may include emotions and prosody.

By using attention to align with $z_t$ and $z_{c-i}$, we obtain a style representation $z_{s-i}$ that aligns with the $i$-th content chunk.

Next, the causal pitch predictor estimates F0, as explicit pitch modeling greatly aids in conveying speech style [^{\L}a{\'n}cucki2021Fastpitch].

The causal mel decoder then decodes the content, timbre, style, and pitch embeddings to generate the mel spectrogram $m_{gn-i}$, which is then converted to the generated speech $y_{gn-i}$ with high quality and stability via a causal shuffle vocoder. 
This generated speech contains the content of the source speech, the speaker identity of the reference speech.

With sufficient hardware, the processing time for each chunk is shorter than the chunk’s duration, enabling chunkwise, streaming, low-latency, zero-shot online voice conversion.

Note that, except for the Stream Content Extractor’s chunk-input-based design, all other components are frame-level causal models, allowing great flexibility in handling diverse chunk size settings.

### Stream Content Extractor

Content accuracy is a critical determinant of VC system quality.

However, existing approaches based on pre-trained models or ASR often rely on offline batch processing or incur prohibitive computational costs in chunkwise processing, rendering them unsuitable for streaming VC.

Thus, content extractors of online VC must balance efficiency and accuracy.

Emformer’s chunk‐based incremental attention mechanism uses cached contextual information from previous chunks to maintain context continuity and capture long‐range dependencies.

It preserves high content accuracy while substantially reducing latency, offering an efficient and practical solution for real‐time content extraction from the source utterance.

Our training objective is to distill the content representations extracted by HuBERT [^Hsu2021Hubert] from the source speech at 20 ms intervals—representations that have been widely adopted and validated in prior work [^Yang2024Streamvc].

As shown in Figure [fig: arch](#fig: arch) (b), we base our design on Emformer by partitioning the input into chunks for both attention computation and subsequent network processing.

During attention computation, we supply context keys, values, and memory‐bank embeddings extracted from the preceding chunk.

The memory bank functions provide prior contextual information, with each encapsulating information from one chunk. 

At layer $n$ and chunk $i$, we compute:

\begin{gather}

Q_i^n = \bigl[\,W_q\,C_i^n,W_q\,R_i^n,W_q\,s_i^n\bigr],
\label{eq:Q}\\[0.5em]
K_i^n = \bigl[\,W_k\,M_{i}^n,K_{L,i}^n,W_k\,C_i^n,W_k\,R_i^n\bigr], 
\label{eq:K}\\[0.5em]
V_i^n = \bigl[\,W_v\,M_{i}^n,V_{L,i}^n,W_v\,C_i^n,W_v\,R_i^n\bigr], 
\label{eq:V}
\end{gather}

\begin{gather}

C_i^{\,n+1} =
\mathrm{FFN}\Bigl(\mathrm{Attn}\bigl(W_q\,C_i^n,K_i^n,V_i^n\bigr)+C_i^n\Bigr),
\label{eq:Cupdate}\\[0.5em]
m_{i}^{\,n+1} = \mathrm{Attn}\bigl(W_q\,s_i^n,K_i^n,V_i^n\bigr),
\label{eq:mupdate}\\[0.5em]
z_{c\text{-}i} = 
\arg\max_{1 \le j \le J}\Bigl[\,
\mathrm{Softmax}\bigl(U\,C_i^{(N)}\bigr)
\Bigr]_{(j,t)}
.
\label{eq:zci}
\end{gather}

Here, $C_i^n$ is the content feature of the $i$-th chunk at layer $n, 1<n<N$, $R_i^n$ is the right‐context block, $s_i^n$ is the chunk summary computed by mean pooling, and $M_{i}^n$ is the memory bank from the previous chunk. 
Attention is computed using $(W_q C_i^n,K_i^n, V_i^n)$, the output is added residually to $C_i^n$, and fed into a feedforward network $\mathrm{FFN}$.

The summary $m_{i}^{\,n+1}$ is obtained by a similar Attention.

Finally, $z_{c\text{-}i}$ is produced by projecting $C_i^{(N)}$ with $U$, applying Softmax over $J$ classes per frame $t$, and selecting the highest‐probability label for each frame.

All layers remain causal by caching $M_{i}^n$, $K_{L,i}^n$, and $V_{L,i}^n$, enabling low‐latency, zero‐shot online content extraction.

In this attention mechanism, keys, values, and memory‐bank vectors are employed to capture contextual dependencies between data chunks.

By flexibly adjusting the size of the right context, one can trade off performance against latency, setting it to zero enforces a strictly causal behavior.

The sizes of the left context and memory buffer determine the overall receptive field.

Leveraging our Stream Content Extractor, we achieve efficient, high‐quality streaming content extraction.

### Adaptive Style Encoder

To comprehensively capture and transfer speaker identity from reference speech, we introduce the Adaptive Style Encoder. 
As shown in Figure [fig: arch2](#fig: arch2), reference mel-spectrogram $m_{rf}$ is initially refined through convolutional blocks. 
Subsequently, a downsampling layer pools the output into chunk-level representations.

In our experiments, we set the chunk size to 80 ms to incorporate styles, which may include emotion and prosody. 

Next, we employ a linear projection to map the outputs $z$ into a low-dimensional latent space for efficient code index lookup [^Zhang2024TCSinger].

The Clustering Vector Quantization (CVQ) layer [^Zheng2023Online] extracts style representations from these latent inputs, establishing an information bottleneck that effectively filters out non-style information [^Zhang2025Versatile]. 
By applying linear projection and the CVQ mechanism, we capture fine-grained speaker identity features except global timbre and content [^Huang2022Generspeech].

Compared to traditional VQ [^Van2017Neural], CVQ employs a dynamic initialization strategy during training. 
This ensures that less frequently or unused code vectors are also updated as frequently used ones do, effectively addressing the codebook collapse issue [^Zheng2023Online].

We train the CVQ layer using a combination of the VQ loss and contrastive loss, defined as:

$$

$$
\begin{aligned}

\mathcal{L}_{CVQ} = &\|sg[z] - e\|^2_2 + \beta \|z - sg[e]\|^2_2+\mathcal{L}_{Contras},

\end{aligned}
$$

\label{eq: vq}

$$

where $e$ is the selected code from the codebook, $\text{sg}(\cdot)$ is the stop-gradient operator, and $\beta$ is commitment loss hyperparameter. 
For each code $e$ in the codebook, we directly select the closest $z^+$ as the positive pair and sample other farther $z^-$ as negative pairs. 
Therefore, the contrastive loss is defined as:

$$

$$
\begin{aligned}

&\mathcal{L}_{Contras}=-\log\frac{e^{\text{sim}(e,z^+)}}{e^{\text{sim}(e,z^+)}+\sum_{i=1}^N e^{\text{sim}(e,z_i^-)}}.    

\end{aligned}
$$

$$

After generating the style embedding, we adaptively align it with the content, taking into account the influence of global timbre on style selection, by matching it to \(z_{ct-i}\), the concatenation of \(z_{c-i}\) and \(z_t\) [^Zhang2024StyleSinger].

For this purpose, we introduce the Align Attention module, incorporating the Scaled Dot-Product Attention mechanism [^Vaswani2017Attention]. 
Before inputting the detailed style embedding into the attention module, we also add positional encoding embeddings. 
Within this module, $z_{ct-i}$ serves as the query, while the style embedding functions as both key and value.

Ultimately, we get the detailed speaker identity $z_{s-i}$.

![](figures/arch2.pdf)

<a id="fig: arch2">The architecture of Adaptive Style Encoder.</a>

### Causal Shuffle Vocoder

Enforcing online VC causality through frame-by-frame autoregressive methods degrades the quality of early generated segments and incurs extra computational overhead, while using zero-padding in vocoders leads to spectral artifacts.

To address this, after generating the mel-spectrogram $m_{gn-i}$ using a causal mel decoder composed of causal convolutions, we build Causal Shuffle Vocoder. 
Based on high-quality HiFi-GAN [^Kong2020Hifi-Gan], we replace standard convolutions with causal convolutions and implement upsampling via pixel shuffle, resulting in a strictly causal vocoder with high audio quality.

As illustrated in Figure [fig: arch](#fig: arch) (c), $m_{gn-i}$ first passes through a single causal convolutional layer, producing an initial feature map of shape $(C, T_0)$, where $C$ and $T_0$ represent the channel and time dimensionality, respectively.

The network then comprises $N$ upsampling stages.

At the $n$-th stage, a causal convolution projects the channel dimension from $C$ up to $r_{n}C$, yielding an intermediate feature map of shape $(r_{n}C, T_{n-1})$.

Next, pixel shuffling rearranges these $r_{n}C$ channels into a new tensor of shape $(C, r_{n}T_{n-1})$, where $r_{n}T_{n-1}=T_{n}$, to achieve temporal upsampling.

Because pixel shuffling only reorders channels without introducing information from future frames, the network’s output is frame-level causal.

By choosing the upsampling factors, the final temporal resolution becomes $T_{0}\prod_{n=1}^{N}r_{n}$, recovering the original audio sampling rate.

Immediately following each upsampling stage is a parallel residual block, which refines harmonic and transient details while preserving causality.

Within each block, the input and its output are combined via a skip connection, allowing the network to learn only the incremental details rather than the entire spectral envelope.

This design ensures fine-grained correction of the spectrum without ever accessing future frames.

The outputs of all parallel residual branches at a given scale are averaged before being forwarded to the next upsampling layer.

After the last upsampling layer and its associated residual block, a final causal convolution then maps the channel dimension to 1, producing the generated speech.

By combining causal convolution with pixel shuffling, the waveform produced at each frame depends exclusively on past mel-spectrogram samples.

Causal convolutions pad solely on the left, avoiding spectral distortions from symmetric or right‐side padding.

Pixel shuffling achieves temporal upsampling by simply reorganizing feature‑map channels into a finer time grid, rather than by learning interpolation kernels that span future frames.

Because it never blends or spreads information unevenly across time, it inherently avoids the checkerboard artifacts typical of transposed convolutions [^Odena2016Deconvolution]. 
In our strictly causal setup, this means we can match the audio fidelity of HiFi‑GAN while operating online with minimal latency.

### Training and Inference Procedures

![](figures/intro.pdf)

<a id="fig: intro">The chunk-wise online inference procedure of Conan. 
Here, we illustrate the causal setting without utilizing any right context.</a>

**Training Procedure.** 
First, for Stream Content Extractor, we use the cross-entropy between the predicted content label and the label extracted by HuBERT. 
Then, the total loss for training the main model comprises $\mathcal{L}_{\mathrm{CVQ}}$ for the adaptive style encoder; $\mathcal{L}_{\mathrm{pitch}}$, the MSE between the ground truth and the predicted $F_{0}$; $\mathcal{L}_{\mathrm{mae}}$ and $\mathcal{L}_{\mathrm{ssim}}$, the MAE and SSIM losses between the predicted and ground truth mel‐spectrograms; and $\mathcal{L}_{\mathrm{GAN}}$, the LSGAN‐style adversarial loss [^Mao2017Least], whose objective is to minimize the distribution distance between predicted and ground truth mel‐spectrograms. 
Finally, the loss for Causal Shuffle Vocoder follows HiFiGAN [^Kong2020Hifi-Gan], including the GAN loss, mel loss, and feature‐map loss.

**Inference Procedure.** 
As shown in Figure~[fig: intro](#fig: intro), we first feed the entire reference speech into the model to provide timbre and stylistic information.

During chunk‑wise online inference, we wait until the input reaches a predefined chunk size before passing it to the model.

Because our generation speed for each chunk is faster than the chunk’s duration, online generation becomes possible.

To ensure temporal continuity, we employ a sliding context window strategy.

At each generation step, we not only input the source speech of the current chunk but also include the preceding context.

From the model’s output, we extract only the segment for this chunk. 
As the context covers the receptive field, consistent overlapping segments can be generated, ensuring smooth transitions at chunk boundaries.
