# PseudoVC: Improving One-Shot Voice Conversion With Pseudo Paired Data

<details>
<summary>基本信息</summary>

- 标题: "PseudoVC: Improving One-Shot Voice Conversion With Pseudo Paired Data."
- 作者:
  - 01 Songjun Cao
  - 02 Qinghua Wu
  - 03 Jie Chen
  - 04 Jin Li
  - 05 Long Ma
- 链接:
  - [ArXiv](https://arxiv.org/abs/2506.01039v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2506.01039v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.06.01_2506.01039v1_PseudoVC__Improving_One-Shot_Voice_Conversion_With_Pseudo_Paired_Data.pdf)
  - [Publication] #TODO

</details>

## Abstract

As parallel training data is scarce for one-shot voice conversion (VC) tasks, waveform reconstruction is typically performed by various VC systems.
A typical one-shot VC system comprises a content encoder and a speaker encoder.
However, two types of mismatches arise: one for the inputs to the content encoder during training and inference, and another for the inputs to the speaker encoder.
To address these mismatches, we propose a novel VC training method called *PseudoVC* in this paper.
First, we introduce an innovative information perturbation approach named *Pseudo Conversion* to tackle the first mismatch problem.
This approach leverages pretrained VC models to convert the source utterance into a perturbed utterance, which is fed into the content encoder during training.
Second, we propose an approach termed *Speaker Sampling* to resolve the second mismatch problem, which will substitute the input to the speaker encoder by another utterance from the same speaker during training.
Experimental results demonstrate that our proposed *Pseudo Conversion* outperforms previous information perturbation methods, and the overall *PseudoVC* method surpasses publicly available VC models.
Audio examples are available \footnote{https://songjuncao.github.io/pseudovc/}.

## 1·Introduction

\label{sec:introduction}

Voice conversion (VC) is a task that transfers the voice of a source speaker to a target speaker, while maintaining the linguistic information.

One-shot voice conversion is a special case where only one utterance of the target speaker is available.

Recently, many works [^Choi2024DDDM-Vc], [^Li2023Freevc], [^Shan2024Phoneme], [^Baas2023Voice], [^Casanova2022Yourtts], [^Wang2024Gr0] has been proposed to advance the development of the one-shot VC task.

As parallel training data is hard to achieve [^Kaneko2017Parallel-Data-Free], [^Tian2018Average], [^Lorenzo-Trueba2018Voice], many VC models are trained by reconstructing the source speech.

Those systems [^Choi2024DDDM-Vc], [^Li2023Freevc], [^Wang2024Gr0] typically consist of a content encoder, a speaker encoder and a decoder \footnote{Here we simply the model structure, as other information (pitch) can be modeled independently in some works.}, as depicted in Fig.~[fig:mismatch](#fig:mismatch).

In order to illustrate the mismatch problem more intuitively, we assume the outputs of both training and inference to be $x(c_i, s_m)$, where $c_i$ and $s_m$ denote content information and speaker information, respectively.

The content encoder extracts linguistic information from the source utterance $x(c_i, s_m)$, where $c_i$ and $s_m$ denote content information and speaker information, respectively.

The speaker encoder extracts speaker-related information from the source utterance $x(c_i, s_m)$.

The decoder then reconstructs the source utterance $x(c_i, s_m)$ based on the extracted content and speaker information.

However, there are two mismatches between training and inference phases: 1.

The inputs to the content encoder during training and inference are $x(c_i, s_m)$ and $x(c_i, s_n)$. 2.

The inputs of the speaker encoder during training and inference are $x(c_i, s_m)$ and $x(c_j, s_m)$.

To alleviate the first mismatch problem, some information perturbation approaches [^Li2023Freevc], [^Qian2020Unsupervised], [^Chan2022Speechsplit2.], [^Choi2021Neural] are proposed to transform the input of the content encoder from $x(c_i, s_m)$ to $x'(c_i, s_n)$ during training.

This transformation aims to modify the speaker-related information while preserving the content information $c_i$.

Although these perturbation methods can mitigate the mismatch problem to some extent, they still exhibit limitations in the diversity and naturalness of the generated $x'(c_i, s_n)$, which can adversely affect the performance of voice conversion.

To overcome this problem, we propose an effective method termed *Pseudo Conversion* in this paper.

This method utilizes pretrained VC models to convert the source utterance $x(c_i, s_m)$ into a pseudo utterance $x'(c_i, s_n)$, which will be fed into the content encoder.

The timbre of generated pseudo utterances $x'(c_i, s_n)$ can be diverse and natural, thereby reducing the gap between training and inference.

Regarding the second mismatch problem, there has been relatively little exploration in the literature [^Choi2024DDDM-Vc].

Inspired by scheduled sampling [^Bengio2015Scheduled], we propose a simple method called *Speaker Sampling*.

During training, we replace the input of the speaker encoder from $x(c_i, s_m)$ to  another utterance spoken by the same speaker with a certain probability.

![](mismatch.pdf)

<a id="fig:mismatch">Mismatch between training and inference</a>
