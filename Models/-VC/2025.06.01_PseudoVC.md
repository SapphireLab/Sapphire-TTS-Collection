# PseudoVC: Improving One-Shot Voice Conversion With Pseudo Paired Data

<details>
<summary>基本信息</summary>

- 标题: "PseudoVC: Improving One-Shot Voice Conversion With Pseudo Paired Data."
- 作者:
  - 01 Songjun Cao
  - 02 Qinghua Wu
  - 03 Jie Chen
  - 04 Jin Li
  - 05 Long Ma
- 链接:
  - [ArXiv](https://arxiv.org/abs/2506.01039v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2506.01039v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.06.01_2506.01039v1_PseudoVC__Improving_One-Shot_Voice_Conversion_With_Pseudo_Paired_Data.pdf)
  - [Publication] #TODO

</details>

## Abstract

As parallel training data is scarce for one-shot voice conversion (VC) tasks, waveform reconstruction is typically performed by various VC systems.
A typical one-shot VC system comprises a content encoder and a speaker encoder.
However, two types of mismatches arise: one for the inputs to the content encoder during training and inference, and another for the inputs to the speaker encoder.
To address these mismatches, we propose a novel VC training method called *PseudoVC* in this paper.
First, we introduce an innovative information perturbation approach named *Pseudo Conversion* to tackle the first mismatch problem.
This approach leverages pretrained VC models to convert the source utterance into a perturbed utterance, which is fed into the content encoder during training.
Second, we propose an approach termed *Speaker Sampling* to resolve the second mismatch problem, which will substitute the input to the speaker encoder by another utterance from the same speaker during training.
Experimental results demonstrate that our proposed *Pseudo Conversion* outperforms previous information perturbation methods, and the overall *PseudoVC* method surpasses publicly available VC models.
Audio examples are available \footnote{https://songjuncao.github.io/pseudovc/}.

## 1·Introduction

\label{sec:introduction}

Voice conversion (VC) is a task that transfers the voice of a source speaker to a target speaker, while maintaining the linguistic information.

One-shot voice conversion is a special case where only one utterance of the target speaker is available.

Recently, many works [^Choi2024DDDM-Vc], [^Li2023Freevc], [^Shan2024Phoneme], [^Baas2023Voice], [^Casanova2022Yourtts], [^Wang2024Gr0] has been proposed to advance the development of the one-shot VC task.

As parallel training data is hard to achieve [^Kaneko2017Parallel-Data-Free], [^Tian2018Average], [^Lorenzo-Trueba2018Voice], many VC models are trained by reconstructing the source speech.

Those systems [^Choi2024DDDM-Vc], [^Li2023Freevc], [^Wang2024Gr0] typically consist of a content encoder, a speaker encoder and a decoder \footnote{Here we simply the model structure, as other information (pitch) can be modeled independently in some works.}, as depicted in Fig.~[fig:mismatch](#fig:mismatch).

In order to illustrate the mismatch problem more intuitively, we assume the outputs of both training and inference to be $x(c_i, s_m)$, where $c_i$ and $s_m$ denote content information and speaker information, respectively.

The content encoder extracts linguistic information from the source utterance $x(c_i, s_m)$, where $c_i$ and $s_m$ denote content information and speaker information, respectively.

The speaker encoder extracts speaker-related information from the source utterance $x(c_i, s_m)$.

The decoder then reconstructs the source utterance $x(c_i, s_m)$ based on the extracted content and speaker information.

However, there are two mismatches between training and inference phases: 1.

The inputs to the content encoder during training and inference are $x(c_i, s_m)$ and $x(c_i, s_n)$. 2.

The inputs of the speaker encoder during training and inference are $x(c_i, s_m)$ and $x(c_j, s_m)$.

To alleviate the first mismatch problem, some information perturbation approaches [^Li2023Freevc], [^Qian2020Unsupervised], [^Chan2022Speechsplit2.], [^Choi2021Neural] are proposed to transform the input of the content encoder from $x(c_i, s_m)$ to $x'(c_i, s_n)$ during training.

This transformation aims to modify the speaker-related information while preserving the content information $c_i$.

Although these perturbation methods can mitigate the mismatch problem to some extent, they still exhibit limitations in the diversity and naturalness of the generated $x'(c_i, s_n)$, which can adversely affect the performance of voice conversion.

To overcome this problem, we propose an effective method termed *Pseudo Conversion* in this paper.

This method utilizes pretrained VC models to convert the source utterance $x(c_i, s_m)$ into a pseudo utterance $x'(c_i, s_n)$, which will be fed into the content encoder.

The timbre of generated pseudo utterances $x'(c_i, s_n)$ can be diverse and natural, thereby reducing the gap between training and inference.

Regarding the second mismatch problem, there has been relatively little exploration in the literature [^Choi2024DDDM-Vc].

Inspired by scheduled sampling [^Bengio2015Scheduled], we propose a simple method called *Speaker Sampling*.

During training, we replace the input of the speaker encoder from $x(c_i, s_m)$ to  another utterance spoken by the same speaker with a certain probability.

![](mismatch.pdf)

<a id="fig:mismatch">Mismatch between training and inference</a>

## 2·Related Work

\label{sec:related}

For the perturbation on the input of content encoder, SPEECHSPLIT2.0 [^Chan2022Speechsplit2.] uses Vocal Tract Length Perturbation (VTLP) [^Jaitly2013Vocal] to modify the timbre by warping the frequency.

NANSY [^Choi2021Neural] introduces a method that perturbs the information of the source utterance through three signal processing techniques.

Additionally, [^Li2023Freevc] utilizes an SR-based data augmentation approach to distort speaker information in the source utterance, thereby assisting the model in learning to extract clean content information.

All these methods aims to modify the timbre of the source waveform, while preserving the linguistic information.

However, despite these efforts, there remains a gap in terms of naturalness and speaker diversity between the perturbed utterances and real utterances.

DDDM-VC [^Choi2024DDDM-Vc] mixes the speaker representation by using a binary selection between the original and shuffled representations in the same batch.

So the speaker of selected representation may be different from the source utterance.

In contrast, our approach for the speaker encoder input involves using other utterances from the same speaker during training. 

## 3·Proposed Method

### Overall architecture

\label{sec:overall}

As illustrated in Fig.~[fig:overall](#fig:overall), our model is based on FreeVC [^Li2023Freevc].

The content encoder of our model contains a WavLM model [^Chen2022Wavlm], a bottleneck extractor and a normalizing flow [^Rezende2015Variational].

For the speaker encoder, we utilize a pretrained speaker verification model that has been trained on a large-scale corpus [^Liu2021Any-to-Many].

The posterior encoder is composed of non-causal WaveNet residual blocks used in WaveGlow [^Prenger2019Waveglow].

The decoder is the HiFIi-GAN V1 generator [^Kong2020Hifi-Gan].

\begin{gather}
L_{total}=L_{rec}+L_{kl}+L_{adv}(D)+L_{adv}(G)+L_{fm}(G)
\label{equation:total}
\end{gather}

Following [^Li2023Freevc], the training loss is defined by Equation [equation:total](#equation:total).

This loss function comprises several components: the KL divergence loss $L_{kl}$, the reconstruction loss $L_{rec}$, the adversarial loss $L_{adv}(D)$ for the discriminator $D$, the adversarial loss $L_{adv}(G)$ for the generator $G$ [^Mao2017Least], and the feature-matching loss $L_{fm}(G)$ [^Larsen2016Autoencoding].

### Training strategy

![](overall.pdf)

<a id="fig:overall">Top: The model architecture during training.

Bottom: The model architecture during inference.</a>

#### Pseudo Conversion

Inspired by semi-supervised learning [^Sohn2020Fixmatch], [^Zhang2022Censer], [^Arazo2020Pseudo-Labeling], we propose a novel information perturbation method to alleviate the first mismatch problem defined in Section [sec:introduction](#sec:introduction).

Firstly, we train a one-shot VC model $\mathcal{M}_t$ following [^Li2023Freevc], which is named as the VC teacher model.

Then, we generate pseudo utterances using $\mathcal{M}_t$ through the following process.

\begin{gather}
x'(c_i, s_n) = \mathcal{M}_t(x(c_i, s_m), x(c_j, s_n))
\label{equation:convert} \\
\mathcal{S}(x(c_i, s_m))=\{x'(c_i, s_n)\}, n=1,...,N \label{equation:set}
\end{gather}

Assuming the source utterance is $x(c_i, s_m)$, where $c_i$ and $s_m$ denotes the content and speaker.

We can convert the source utterance to a pseudo utterance $x'(c_i, s_n)$ using Equation [equation:convert](#equation:convert).

Here, $x(c_j, s_n)$ denotes a reference utterance with speaker $s_n$, which is randomly selected from the training dataset.
$\mathcal{S}(x(c_i, s_m))$ is the set of pseudo utterances corresponding to the source utterance $x(c_i, s_m)$, and $N$ indicates the total number of generated pseudo utterances.

The constructed $(x'(c_i, s_n), x(c_i, s_m))$ are referred to as *pseudo paired data*.

During training, one pseudo utterance $x'(c_i, s_n)$ 
will be randomly selected from $\mathcal{S}(x(c_i, s_m))$ and fed into the WavLM model, while the source utterance $x(c_i, s_m)$ serves as the target for prediction.

The above process of generating pseudo utterances is termed *Pseudo Conversion*.

While it draws inspiration from traditional semi-supervised learning, there are notable differences in the details.

In the semi-supervised learning, pseudo labels generated by the teacher model are used as the model's outputs.

In our method, pseudo utterances generated by the teacher model are used as the model's inputs.

This approach aims to enhance the robustness of the model by providing it with diverse pseudo paired data during training.

#### Speaker Sampling

As shown in Fig.~[fig:overall](#fig:overall), the speaker embedding $g$ extracted by the speaker encoder plays a vital role in our model.

It will be fed into the Flow, Decoder and Posterior Encoder components.

However the second mismatch problem defined in Section [sec:introduction](#sec:introduction) will affect the performance.

Similar mismatch problem exists in sequence prediction task [^Bengio2015Scheduled], which can be alleviated by scheduled sampling.

Similarly, we propose a simple training strategy to alleviate this problem, which is called *Speaker Sampling*.

In this approach, we modify the input to the speaker encoder from $x(c_i, s_m)$ to $x(c_j, s_m)$ during training with a certain probability $\alpha$. 
Here, $x(c_j, s_m)$ is randomly selected from other utterances of the same speaker $s_m$.

The overall training process combining *Pseudo Conversion* and *Speaker Sampling* is formulated in the Algorithm [alg:strategy](#alg:strategy).

\begin{algorithm}[h]
\caption{Training Process of PseudoVC} \label{alg:strategy}

\begin{algorithmic}[1] % The number tells LaTeX to number each line
\State **Input:**

Training utterance $x(c_i, s_m) \in \mathcal{X}$.

Generated pseudo utterance number $N$.

Training iteration number $Q$.

The probability of speaker sampling $\alpha$.
\State **Output:**

Trained VC model $\mathcal{M}$
\State Train $\mathcal{M}_t$ on $\mathcal{X}$ following [^Li2023Freevc]
\For{each utterance $x(c_i, s_m)$ in $\mathcal{X}$}
\For{$n=1$ to $N$}
\State Randomly select utterance $x(c_j, s_n)$ from $\mathcal{X}$
\State Generate pseudo utterance $x'(c_i, s_n)$ via Eq. [equation:convert](#equation:convert) \label{alg:pseudo}
\EndFor
\State Combine pseudo utterances into set $\mathcal{S}(x(c_i, s_m))$
\EndFor

\For{$q = 1$ to $Q$}
\State Randomly draw a batch $\mathcal{B}$ from $\mathcal{X}$
\For{each utterance $x(c_i, s_m)$ in $\mathcal{B}$}
\State Randomly select $x'(c_i, s_n)$ from $\mathcal{S}(x(c_i, s_m))$
\State $x'(c_i, s_n)$ will be fed into the content encoder
\State Generate a random number $r$ in the range $[0, 1]$
\If{$r < \alpha$}
\State $x(c_j, s_m)$ will be fed into the speaker encoder
\Else
\State $x(c_i, s_m)$ will be fed into the speaker encoder
\EndIf
\EndFor
\State Train VC model $\mathcal{M}$ with loss of Equation [equation:total](#equation:total)
\EndFor
\State \Return trained model $\mathcal{M}$
\end{algorithmic}

\end{algorithm}

\iffalse

### Training loss

\begin{gather}
q_{\phi}(z|x_{lin},g)=N(z;\mu_{\phi}(x_{lin}, g), \sigma_{\phi}(x_{lin}, g))
\label{equation:posterior} \\
p_{\theta}(z|c,g)=N(f_{\theta}(z,g);\mu_{\theta}(c),\sigma_{\theta}(c)) \left| \det{\frac{\partial f_{\theta}(z,g)}{\partial z}} \right|
\label{equation:prior} \\
L_{kl}=\log{q_{\phi}(z|x_{lin}, g)}-\log{p_{\theta}(z|c, g)}
\label{equation:kl} \\
L_{rec}=\Vert x_{mel}-\hat{x}_{mel} \Vert
\label{equation:rec}
\end{gather}

The posterior distribution and the prior distribution are defined by Equation [equation:posterior](#equation:posterior) and Equation [equation:prior](#equation:prior), where $x_{lin}$ denotes the linear-scale spectrogram of target waveform and $c$ is the content information contained in the input waveform.

The KL divergence $L_{kl}$ is defined by Equation [equation:kl](#equation:kl).
$x_{mel}$ and $\hat{x}_{mel}$ are the target and predicted mel-spectrogram.

The reconstruction loss $L_{rec}$ is defined by Equation [equation:rec](#equation:rec).

Besides, adversarial loss $L_{adv}(D)$ for discriminator D, adversarial loss $L_{adv}(G)$ for generator G [^Mao2017Least], and feature-matching loss $L_{fm}(G)$ [^Larsen2016Autoencoding] are used during training.

The total loss for training can be expressed as follows:

\begin{gather}
L_{total}=L_{rec}+L_{kl}+L_{adv}(D)+L_{adv}(G)+L_{fm}(G)
\label{equation:total}
\end{gather}

\fi
