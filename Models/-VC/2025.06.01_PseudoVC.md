# PseudoVC: Improving One-Shot Voice Conversion With Pseudo Paired Data

<details>
<summary>基本信息</summary>

- 标题: "PseudoVC: Improving One-Shot Voice Conversion With Pseudo Paired Data."
- 作者:
  - 01 Songjun Cao
  - 02 Qinghua Wu
  - 03 Jie Chen
  - 04 Jin Li
  - 05 Long Ma
- 链接:
  - [ArXiv](https://arxiv.org/abs/2506.01039v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2506.01039v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.06.01_2506.01039v1_PseudoVC__Improving_One-Shot_Voice_Conversion_With_Pseudo_Paired_Data.pdf)
  - [Publication] #TODO

</details>

## Abstract

As parallel training data is scarce for one-shot voice conversion (VC) tasks, waveform reconstruction is typically performed by various VC systems.
A typical one-shot VC system comprises a content encoder and a speaker encoder.
However, two types of mismatches arise: one for the inputs to the content encoder during training and inference, and another for the inputs to the speaker encoder.
To address these mismatches, we propose a novel VC training method called *PseudoVC* in this paper.
First, we introduce an innovative information perturbation approach named *Pseudo Conversion* to tackle the first mismatch problem.
This approach leverages pretrained VC models to convert the source utterance into a perturbed utterance, which is fed into the content encoder during training.
Second, we propose an approach termed *Speaker Sampling* to resolve the second mismatch problem, which will substitute the input to the speaker encoder by another utterance from the same speaker during training.
Experimental results demonstrate that our proposed *Pseudo Conversion* outperforms previous information perturbation methods, and the overall *PseudoVC* method surpasses publicly available VC models.
Audio examples are available \footnote{https://songjuncao.github.io/pseudovc/}.

## 1·Introduction

\label{sec:introduction}

Voice conversion (VC) is a task that transfers the voice of a source speaker to a target speaker, while maintaining the linguistic information.

One-shot voice conversion is a special case where only one utterance of the target speaker is available.

Recently, many works [^Choi2024DDDM-Vc], [^Li2023Freevc], [^Shan2024Phoneme], [^Baas2023Voice], [^Casanova2022Yourtts], [^Wang2024Gr0] has been proposed to advance the development of the one-shot VC task.

As parallel training data is hard to achieve [^Kaneko2017Parallel-Data-Free], [^Tian2018Average], [^Lorenzo-Trueba2018Voice], many VC models are trained by reconstructing the source speech.

Those systems [^Choi2024DDDM-Vc], [^Li2023Freevc], [^Wang2024Gr0] typically consist of a content encoder, a speaker encoder and a decoder \footnote{Here we simply the model structure, as other information (pitch) can be modeled independently in some works.}, as depicted in Fig.~[fig:mismatch](#fig:mismatch).

In order to illustrate the mismatch problem more intuitively, we assume the outputs of both training and inference to be $x(c_i, s_m)$, where $c_i$ and $s_m$ denote content information and speaker information, respectively.

The content encoder extracts linguistic information from the source utterance $x(c_i, s_m)$, where $c_i$ and $s_m$ denote content information and speaker information, respectively.

The speaker encoder extracts speaker-related information from the source utterance $x(c_i, s_m)$.

The decoder then reconstructs the source utterance $x(c_i, s_m)$ based on the extracted content and speaker information.

However, there are two mismatches between training and inference phases: 1.

The inputs to the content encoder during training and inference are $x(c_i, s_m)$ and $x(c_i, s_n)$. 2.

The inputs of the speaker encoder during training and inference are $x(c_i, s_m)$ and $x(c_j, s_m)$.

To alleviate the first mismatch problem, some information perturbation approaches [^Li2023Freevc], [^Qian2020Unsupervised], [^Chan2022Speechsplit2.], [^Choi2021Neural] are proposed to transform the input of the content encoder from $x(c_i, s_m)$ to $x'(c_i, s_n)$ during training.

This transformation aims to modify the speaker-related information while preserving the content information $c_i$.

Although these perturbation methods can mitigate the mismatch problem to some extent, they still exhibit limitations in the diversity and naturalness of the generated $x'(c_i, s_n)$, which can adversely affect the performance of voice conversion.

To overcome this problem, we propose an effective method termed *Pseudo Conversion* in this paper.

This method utilizes pretrained VC models to convert the source utterance $x(c_i, s_m)$ into a pseudo utterance $x'(c_i, s_n)$, which will be fed into the content encoder.

The timbre of generated pseudo utterances $x'(c_i, s_n)$ can be diverse and natural, thereby reducing the gap between training and inference.

Regarding the second mismatch problem, there has been relatively little exploration in the literature [^Choi2024DDDM-Vc].

Inspired by scheduled sampling [^Bengio2015Scheduled], we propose a simple method called *Speaker Sampling*.

During training, we replace the input of the speaker encoder from $x(c_i, s_m)$ to  another utterance spoken by the same speaker with a certain probability.

![](mismatch.pdf)

<a id="fig:mismatch">Mismatch between training and inference</a>

## 2·Related Work

\label{sec:related}

For the perturbation on the input of content encoder, SPEECHSPLIT2.0 [^Chan2022Speechsplit2.] uses Vocal Tract Length Perturbation (VTLP) [^Jaitly2013Vocal] to modify the timbre by warping the frequency.

NANSY [^Choi2021Neural] introduces a method that perturbs the information of the source utterance through three signal processing techniques.

Additionally, [^Li2023Freevc] utilizes an SR-based data augmentation approach to distort speaker information in the source utterance, thereby assisting the model in learning to extract clean content information.

All these methods aims to modify the timbre of the source waveform, while preserving the linguistic information.

However, despite these efforts, there remains a gap in terms of naturalness and speaker diversity between the perturbed utterances and real utterances.

DDDM-VC [^Choi2024DDDM-Vc] mixes the speaker representation by using a binary selection between the original and shuffled representations in the same batch.

So the speaker of selected representation may be different from the source utterance.

In contrast, our approach for the speaker encoder input involves using other utterances from the same speaker during training. 

## 3·Proposed Method

### Overall architecture

\label{sec:overall}

As illustrated in Fig.~[fig:overall](#fig:overall), our model is based on FreeVC [^Li2023Freevc].

The content encoder of our model contains a WavLM model [^Chen2022Wavlm], a bottleneck extractor and a normalizing flow [^Rezende2015Variational].

For the speaker encoder, we utilize a pretrained speaker verification model that has been trained on a large-scale corpus [^Liu2021Any-to-Many].

The posterior encoder is composed of non-causal WaveNet residual blocks used in WaveGlow [^Prenger2019Waveglow].

The decoder is the HiFIi-GAN V1 generator [^Kong2020Hifi-Gan].

\begin{gather}
L_{total}=L_{rec}+L_{kl}+L_{adv}(D)+L_{adv}(G)+L_{fm}(G)
\label{equation:total}
\end{gather}

Following [^Li2023Freevc], the training loss is defined by Equation [equation:total](#equation:total).

This loss function comprises several components: the KL divergence loss $L_{kl}$, the reconstruction loss $L_{rec}$, the adversarial loss $L_{adv}(D)$ for the discriminator $D$, the adversarial loss $L_{adv}(G)$ for the generator $G$ [^Mao2017Least], and the feature-matching loss $L_{fm}(G)$ [^Larsen2016Autoencoding].

### Training strategy

![](overall.pdf)

<a id="fig:overall">Top: The model architecture during training.

Bottom: The model architecture during inference.</a>

#### Pseudo Conversion

Inspired by semi-supervised learning [^Sohn2020Fixmatch], [^Zhang2022Censer], [^Arazo2020Pseudo-Labeling], we propose a novel information perturbation method to alleviate the first mismatch problem defined in Section [sec:introduction](#sec:introduction).

Firstly, we train a one-shot VC model $\mathcal{M}_t$ following [^Li2023Freevc], which is named as the VC teacher model.

Then, we generate pseudo utterances using $\mathcal{M}_t$ through the following process.

\begin{gather}
x'(c_i, s_n) = \mathcal{M}_t(x(c_i, s_m), x(c_j, s_n))
\label{equation:convert} \\
\mathcal{S}(x(c_i, s_m))=\{x'(c_i, s_n)\}, n=1,...,N \label{equation:set}
\end{gather}

Assuming the source utterance is $x(c_i, s_m)$, where $c_i$ and $s_m$ denotes the content and speaker.

We can convert the source utterance to a pseudo utterance $x'(c_i, s_n)$ using Equation [equation:convert](#equation:convert).

Here, $x(c_j, s_n)$ denotes a reference utterance with speaker $s_n$, which is randomly selected from the training dataset.
$\mathcal{S}(x(c_i, s_m))$ is the set of pseudo utterances corresponding to the source utterance $x(c_i, s_m)$, and $N$ indicates the total number of generated pseudo utterances.

The constructed $(x'(c_i, s_n), x(c_i, s_m))$ are referred to as *pseudo paired data*.

During training, one pseudo utterance $x'(c_i, s_n)$ 
will be randomly selected from $\mathcal{S}(x(c_i, s_m))$ and fed into the WavLM model, while the source utterance $x(c_i, s_m)$ serves as the target for prediction.

The above process of generating pseudo utterances is termed *Pseudo Conversion*.

While it draws inspiration from traditional semi-supervised learning, there are notable differences in the details.

In the semi-supervised learning, pseudo labels generated by the teacher model are used as the model's outputs.

In our method, pseudo utterances generated by the teacher model are used as the model's inputs.

This approach aims to enhance the robustness of the model by providing it with diverse pseudo paired data during training.

#### Speaker Sampling

As shown in Fig.~[fig:overall](#fig:overall), the speaker embedding $g$ extracted by the speaker encoder plays a vital role in our model.

It will be fed into the Flow, Decoder and Posterior Encoder components.

However the second mismatch problem defined in Section [sec:introduction](#sec:introduction) will affect the performance.

Similar mismatch problem exists in sequence prediction task [^Bengio2015Scheduled], which can be alleviated by scheduled sampling.

Similarly, we propose a simple training strategy to alleviate this problem, which is called *Speaker Sampling*.

In this approach, we modify the input to the speaker encoder from $x(c_i, s_m)$ to $x(c_j, s_m)$ during training with a certain probability $\alpha$. 
Here, $x(c_j, s_m)$ is randomly selected from other utterances of the same speaker $s_m$.

The overall training process combining *Pseudo Conversion* and *Speaker Sampling* is formulated in the Algorithm [alg:strategy](#alg:strategy).

\begin{algorithm}[h]
\caption{Training Process of PseudoVC} \label{alg:strategy}

\begin{algorithmic}[1] % The number tells LaTeX to number each line
\State **Input:**

Training utterance $x(c_i, s_m) \in \mathcal{X}$.

Generated pseudo utterance number $N$.

Training iteration number $Q$.

The probability of speaker sampling $\alpha$.
\State **Output:**

Trained VC model $\mathcal{M}$
\State Train $\mathcal{M}_t$ on $\mathcal{X}$ following [^Li2023Freevc]
\For{each utterance $x(c_i, s_m)$ in $\mathcal{X}$}
\For{$n=1$ to $N$}
\State Randomly select utterance $x(c_j, s_n)$ from $\mathcal{X}$
\State Generate pseudo utterance $x'(c_i, s_n)$ via Eq. [equation:convert](#equation:convert) \label{alg:pseudo}
\EndFor
\State Combine pseudo utterances into set $\mathcal{S}(x(c_i, s_m))$
\EndFor

\For{$q = 1$ to $Q$}
\State Randomly draw a batch $\mathcal{B}$ from $\mathcal{X}$
\For{each utterance $x(c_i, s_m)$ in $\mathcal{B}$}
\State Randomly select $x'(c_i, s_n)$ from $\mathcal{S}(x(c_i, s_m))$
\State $x'(c_i, s_n)$ will be fed into the content encoder
\State Generate a random number $r$ in the range $[0, 1]$
\If{$r < \alpha$}
\State $x(c_j, s_m)$ will be fed into the speaker encoder
\Else
\State $x(c_i, s_m)$ will be fed into the speaker encoder
\EndIf
\EndFor
\State Train VC model $\mathcal{M}$ with loss of Equation [equation:total](#equation:total)
\EndFor
\State \Return trained model $\mathcal{M}$
\end{algorithmic}

\end{algorithm}

\iffalse

### Training loss

\begin{gather}
q_{\phi}(z|x_{lin},g)=N(z;\mu_{\phi}(x_{lin}, g), \sigma_{\phi}(x_{lin}, g))
\label{equation:posterior} \\
p_{\theta}(z|c,g)=N(f_{\theta}(z,g);\mu_{\theta}(c),\sigma_{\theta}(c)) \left| \det{\frac{\partial f_{\theta}(z,g)}{\partial z}} \right|
\label{equation:prior} \\
L_{kl}=\log{q_{\phi}(z|x_{lin}, g)}-\log{p_{\theta}(z|c, g)}
\label{equation:kl} \\
L_{rec}=\Vert x_{mel}-\hat{x}_{mel} \Vert
\label{equation:rec}
\end{gather}

The posterior distribution and the prior distribution are defined by Equation [equation:posterior](#equation:posterior) and Equation [equation:prior](#equation:prior), where $x_{lin}$ denotes the linear-scale spectrogram of target waveform and $c$ is the content information contained in the input waveform.

The KL divergence $L_{kl}$ is defined by Equation [equation:kl](#equation:kl).
$x_{mel}$ and $\hat{x}_{mel}$ are the target and predicted mel-spectrogram.

The reconstruction loss $L_{rec}$ is defined by Equation [equation:rec](#equation:rec).

Besides, adversarial loss $L_{adv}(D)$ for discriminator D, adversarial loss $L_{adv}(G)$ for generator G [^Mao2017Least], and feature-matching loss $L_{fm}(G)$ [^Larsen2016Autoencoding] are used during training.

The total loss for training can be expressed as follows:

\begin{gather}
L_{total}=L_{rec}+L_{kl}+L_{adv}(D)+L_{adv}(G)+L_{fm}(G)
\label{equation:total}
\end{gather}

\fi

## 4·Experiment

### Experimental Setup

Our experiments are conducted on VCTK [^Veaux2017CSTR] and LibriTTS [^Zen2019Libritts].

We use VCTK corpus for training.

The split of training validation and testing follows [^Li2023Freevc] exactly.

The test-clean subset of LibriTTS is used for test.

We resample the audio samples from 24 kHz to 16 kHz.

We use the pre-trained WavLM-large encoder to extract 1024-dimensional vectors for every 20ms of 16 kHz audio.

We adopt the pretrained speaker model of [^Liu2021Any-to-Many] as speaker encoder, which is fixed during training. 
Our model is trained up to 200k steps using 8 NVIDIA V100 GPUs.

The batch size is set to 64, with a maximum segment length of 128 frames.

We set $N$ of Equation [equation:set](#equation:set) to 25,  meaning that 25 pseudo utterances will be generated for each source utterance.

### Baseline models

For the overall performance comparison, we choose three public models, which are: 

\begin{enumerate}

-  **PH** [^Shan2024Phoneme], a one-shot VC model named Phoneme Hallucinator.

It is based on set expansion.

The number of hallucinated features is set to 30000.

-  **DDDM-VC** [^Choi2024DDDM-Vc], a recently proposed diffusion-based generative model.

-  **FreeVC** [^Li2023Freevc], a widely-used voice conversion framework.

It is the exact baseline of our PseudoVC, as the two methods adopt similar model framework and training data. 
\end{enumerate}

For the ablation study of our proposed *Pseudo Conversion* method, we select three information perturbation methods for comparison.

Each of those methods generates 25 perturbed utterances:

\begin{enumerate}

-  **VTLP** is a signal processing method used by [^Jaitly2013Vocal].

The warping factor is set to $\alpha \sim U(0.9,1.1)$.

We use the implementation \footnote{https://github.com/biggytruck/SpeechSplit2.git}.

-  **NANSY** is proposed in [^Choi2021Neural].

The method has three functions, which are random frequency shaping, pitch randomization and formant shifting.

The hyperparameters of those functions are consistent with those in [^Choi2021Neural].

We refer to the project \footnote{https://github.com/dhchoi99/NANSY.git}.

-  **SR** is used by [^Li2023Freevc].

This conversion is based on vertical spectrogram-resize operation.

As described by [^Li2023Freevc], the resize ratio ranges from 0.85 to 1.15.

The details can be found in the official FreeVC project \footnote{https://github.com/OlaWod/FreeVC.git}.

\end{enumerate}

### Evaluation Metrics

#### Objective metrics

We utilize Amphion [^Zhang2024Amphion] to calculate the word error rate (WER) using the Whisper large-v3 model [^Radford2023Robust].

Additionally, we measure speaker similarity through speaker encoder cosine similarity (SECS) using the Resemblyzer [^Wan2018Generalized] model.

Following [^Li2023Freevc], we randomly select 400 utterances (200 from VCTK, 200 from LibriTTS) as source speech, and we choose 12 speakers as target speakers, which results in 4800 utterances.

#### Subjective metrics

For the subjective metrics, we measure the 5-scale mean opinion score (MOS) and similarity mean opinion score (SMOS) for the speech naturalness and speaker similarity. 1 is the lowest perceived quality and 5 is the highest perceived quality.
15 participants evaluate the scores of 300 utterances which are randomly selected from the objective test dataset.

### Main results

<a id="tab:main">Subjective evaluation results in terms of 5-scale MOS and SMOS with 95\% confidence intervals.

Objective evaluation results in terms of  WER and SECS.

GT denotes the source speech.</a>

The subjective and objective results in Table [tab:main](#tab:main) show that our proposed PseudoVC model outperforms all the baseline models.

When compared to the exact baseline FreeVC, our proposed model demonstrates superior naturalness and speaker similarity across both subjective and objective metrics.

We attribute these improvements to our two effective training strategies, which aims to reduce the mismatch between training and inference.

In the next section, we will conduct an ablation study to further investigate the effectiveness of the two training strategies.

### Ablation study of Pseudo Conversion

<a id="tab:ablation1">Ablation study of Pseudo Conversion. **Information Perturbation** stands for information perturbation methods. $\mathcal{M}_t$ denotes the VC teacher model which is used by *Pseudo Conversion* to generate pseudo utterances.</a>

#### Comparison with previous work

In Table [tab:ablation1](#tab:ablation1), we compare our proposed *Pseudo Conversion* with other information perturbation methods.

All the models of Table [tab:ablation1](#tab:ablation1) only differ in the input of WavLM.
**a0** is the baseline model, which feeds source utterances into the WavLM without any perturbation.

Results show that all the information perturbation methods achieve better results than the baseline model **a0**.

Model **b3** using *Pseudo Conversion* achieves the best speaker similarity among those methods.

We argue that both the diversity and naturalness of generated utterances' timbre will affect the final performance.

To explain our method's effectiveness, we use t-SNE [^Maaten2008Visualizing] to visualize speaker embeddings of perturbed utterances in Fig. [fig:tsne](#fig:tsne).

All the perturbed utterances are based on the same source utterance.

The results reveal that the utterances generated by our method exhibit the greatest speaker diversity.

Additionally, we plot the speaker embeddings of real utterances randomly selected from the test set.

The distribution of speaker embeddings from our method closely resembles that of the real data.

In contrast, the timbre of utterances generated by the **NANSY** method appears abnormal, with its distribution significantly deviating from that of the real data.

Perturbed utterances of different methods can be found in our demo page too.

#### Impact of teacher models

In this section, we study the impact of different teacher models.

Results for models **b1/b2/b3** indicate that they yield similar performance levels.

In traditional semi-supervised learning tasks, pseudo labels are typically treated as training targets, and it is generally observed that higher-quality pseudo labels lead to improved results.

However, in our approach, we utilize pseudo utterances as input to the model.

Consequently, the minor differences among the teacher models have a limited effect on the final performance.

### Ablation study of Speaker Sampling

![](tsne.pdf)

<a id="fig:tsne">Visualizations of speaker embeddings for generated utterances using different perturbation methods.

All the generated utterances are perturbed from the same source utterance. **Real** denotes speaker embedding of real utterances sampled from VCTK test set.</a>

<a id="tab:ablation2">Ablation study of Speaker Sampling. $\boldsymbol{\alpha}$ denotes the probability of speaker sampling.</a>

We explore the impact of *Speaker Sampling* in Table [tab:ablation2](#tab:ablation2).

Results show that our model with speaker sampling **c2** can get a better WER compared to the baseline model **b3**.

It can be attributed to the alleviation of mismatch between training and inference.

However, the results of model **c1** reveal that speaker similarity will degrade if the probability $\alpha$ is big.

We argue that aggressive speaker sampling may increase the difficulty of model training.

In conclusion, the objective results demonstrate that *Pseudo Conversion* enhances speaker similarity while *Speaker Sampling* contributes to improved intelligibility.
