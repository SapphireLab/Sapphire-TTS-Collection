# EZ-VC: Easy Zero-Shot Any-to-Any Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "EZ-VC: Easy Zero-Shot Any-to-Any Voice Conversion."
- 作者:
  - 01 Advait Joglekar
  - 02 Divyanshu Singh
  - 03 Rooshil Rohit Bhatia
  - 04 S. Umesh
- 链接:
  - [ArXiv](https://arxiv.org/abs/2505.16691v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2505.16691v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.05.22_2505.16691v1_EZ-VC__Easy_Zero-Shot_Any-to-Any_Voice_Conversion.pdf)
  - [ArXiv:2505.16691v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.05.22_2505.16691v2_EZ-VC__Easy_Zero-Shot_Any-to-Any_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

Voice Conversion research in recent times has increasingly focused on improving the zero-shot capabilities of existing methods.
Despite remarkable advancements, current architectures still tend to struggle in zero-shot cross-lingual settings.
They are also often unable to generalize for speakers of unseen languages and accents.
In this paper, we adopt a simple yet effective approach that combines discrete speech representations from self-supervised models with a non-autoregressive Diffusion-Transformer based conditional flow matching speech decoder.
We show that this architecture allows us to train a voice-conversion model in a purely textless, self-supervised fashion.
Our technique works without requiring multiple encoders to disentangle speech features.
Our model also manages to excel in zero-shot cross-lingual settings even for unseen languages.
We provide demo samples for our model here: \href{https://ez-vc.github.io/EZ-VC-Demo/}{https://ez-vc.github.io/EZ-VC-Demo/}

## 1·Introduction

Zero-shot Voice Conversion (VC) is the task of transforming a source speaker's voice characteristics into that of a target speaker while preserving linguistic content and prosodic attributes, even for speakers unseen during training.

Over the years with the advancement of modern deep learning techniques and substantial improvements in speech encoders and speech generation systems, numerous and vastly different approaches have been proposed to address this challenge. 

Textless VC architectures have become the primary area of research in this domain since cascaded ASR+TTS systems  are known to lose the non-verbal characteristics of the source speech such as laughs, whispers and other filler sounds.

They also lead to cascaded errors.

To overcome this, many textless VC systems these days employ either self-supervised speech encoders (SSL) or neural audio codecs (NAC) to extract speaker features or linguistic content before feeding them to a speech generation decoder.

These speech representations are also often disentangled to obtain certain composite characteristics such as timbre or style.

Sometimes quantized speech representations are used which form as the input for a speech generation or language model.

Speech synthesis systems, which are a key component of VC architectures, have of late greatly benefited from the advancements in diffusion and continuous normalizing flow (CNF) based techniques.

Voicebox[^Le2023Voicebox] and its successors that use these methods are able to produce high quality audio outputs that are almost undistinguishable from real speech.

These models thus show great promise for zero-shot VC tasks and yet architectures based on these methods remain under-explored. 

In this work we contribute the following,

-  We propose EZ-VC, a simple self-supervised any-to-any zero-shot voice conversion architecture that generalizes for unseen speakers, accents and languages while still producing highly natural and fluent speech.

-  We demonstrate that zero-shot VC is possible without requiring multiple encoders for feature disentanglement of speaker and speech attributes.

-  We show that combining quantized features from a self-supervised speech encoder and a flow matching speech generation decoder is sufficient to achieve state-of-the-art results.
