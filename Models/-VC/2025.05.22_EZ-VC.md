# EZ-VC: Easy Zero-Shot Any-to-Any Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "EZ-VC: Easy Zero-Shot Any-to-Any Voice Conversion."
- 作者:
  - 01 Advait Joglekar
  - 02 Divyanshu Singh
  - 03 Rooshil Rohit Bhatia
  - 04 S. Umesh
- 链接:
  - [ArXiv](https://arxiv.org/abs/2505.16691v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2505.16691v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.05.22_2505.16691v1_EZ-VC__Easy_Zero-Shot_Any-to-Any_Voice_Conversion.pdf)
  - [ArXiv:2505.16691v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.05.22_2505.16691v2_EZ-VC__Easy_Zero-Shot_Any-to-Any_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

Voice Conversion research in recent times has increasingly focused on improving the zero-shot capabilities of existing methods.
Despite remarkable advancements, current architectures still tend to struggle in zero-shot cross-lingual settings.
They are also often unable to generalize for speakers of unseen languages and accents.
In this paper, we adopt a simple yet effective approach that combines discrete speech representations from self-supervised models with a non-autoregressive Diffusion-Transformer based conditional flow matching speech decoder.
We show that this architecture allows us to train a voice-conversion model in a purely textless, self-supervised fashion.
Our technique works without requiring multiple encoders to disentangle speech features.
Our model also manages to excel in zero-shot cross-lingual settings even for unseen languages.
We provide demo samples for our model here: \href{https://ez-vc.github.io/EZ-VC-Demo/}{https://ez-vc.github.io/EZ-VC-Demo/}

## 1·Introduction

Zero-shot Voice Conversion (VC) is the task of transforming a source speaker's voice characteristics into that of a target speaker while preserving linguistic content and prosodic attributes, even for speakers unseen during training.

Over the years with the advancement of modern deep learning techniques and substantial improvements in speech encoders and speech generation systems, numerous and vastly different approaches have been proposed to address this challenge. 

Textless VC architectures have become the primary area of research in this domain since cascaded ASR+TTS systems  are known to lose the non-verbal characteristics of the source speech such as laughs, whispers and other filler sounds.

They also lead to cascaded errors.

To overcome this, many textless VC systems these days employ either self-supervised speech encoders (SSL) or neural audio codecs (NAC) to extract speaker features or linguistic content before feeding them to a speech generation decoder.

These speech representations are also often disentangled to obtain certain composite characteristics such as timbre or style.

Sometimes quantized speech representations are used which form as the input for a speech generation or language model.

Speech synthesis systems, which are a key component of VC architectures, have of late greatly benefited from the advancements in diffusion and continuous normalizing flow (CNF) based techniques.

Voicebox[^Le2023Voicebox] and its successors that use these methods are able to produce high quality audio outputs that are almost undistinguishable from real speech.

These models thus show great promise for zero-shot VC tasks and yet architectures based on these methods remain under-explored. 

In this work we contribute the following,

-  We propose EZ-VC, a simple self-supervised any-to-any zero-shot voice conversion architecture that generalizes for unseen speakers, accents and languages while still producing highly natural and fluent speech.

-  We demonstrate that zero-shot VC is possible without requiring multiple encoders for feature disentanglement of speaker and speech attributes.

-  We show that combining quantized features from a self-supervised speech encoder and a flow matching speech generation decoder is sufficient to achieve state-of-the-art results.

## 2·Related Work

Early research in VC focused on disentangling speaker and content information.

Works like YourTTS[^Casanova2023YourTTS] focused on using speaker embeddings to extract speaker features from target speech but usually required reference text to be provided as well.

Recent works like SEF-VC[^Li2024Sef-Vc] now prefer textless, speaker-embedding free VC which is also able to perform better.

Since the advent of SSL speech models like Hubert[^Hsu2021HuBERT] and WavLM[^Chen2022WavLM], VC research has quickly learned to leverage them for their high correlation with both acoustic and linguistic content. kNN-VC[^Baas2023Voice] works by replacing representations of source speech with the nearest neighbour from the reference speech.

Vec2wav 2.0 on the other hand, uses a combination of discrete representations from vq-wav2vec for source content and WavLM features for capturing the timbre of the target speaker.

At the same time, another school of approach has emerged that utilizes neural audio encoders and combines them with language models for high quality VC.

Unfortunately, these systems suffer from slow inference speeds due to their auto-regressive nature.

Diffusion based techniques also have been explored by DiffVC[^Popov2022Diffusion-Based] and similar works.

These models are able to demonstrate natural and robust outputs.

Conditional flow-matching based speech generation methods have also begun to appear in voice conversion literature.

Latest works such as AdaptVC[^Kim2025AdaptVC], StableVC[^Yao2024StableVC], Seed-VC[^Liu2024Zero-Shot] and PFlow-VC[^Zuo2025Enhancing] employ this technique for their speech decoders and generally couple them with SSL encoders. 

AdaptVC uses speaker and content encoder adapters on top of Hubert while StableVC includes three feature extractors for style, linguistic content, and mel-spectrograms.

Seed-VC on the other hand requires a timber shifter module and speaker-embeddings besides a semantic feature extractor.

PFlow-VC proposes a slightly different approach by using a timbre encoder for target speaker and semantic encoder for source speech.

In contrast, with our architecture we wish to eliminate the need for multiple encoders or adapters for voice conversion while still being able to achieve state-of-the-art results for any-to-any VC.

![](train.pdf)

<a id="fig:arch">An overview of EZ-VC</a>

## 3·Ez-Vc

EZ-VC is a simple architecture that only requires one pre-trained speech encoder and a trainable speech decoder.

Unlike most other works, we do not need multiple encoders for disentanglement of speech features.

Our architecture also benefits from using an off-the-shelf encoder.

Other than training a simple k-means model, we do not train our speech encoding module.

This helps reduce the compute and training time requirements compared to existing methods that usually ask for training both the encoder and decoder modules. 

Figure [fig:arch](#fig:arch) provides a description of our model's architecture for both training and inference.

At the time of training, our model does not require any supervised or labeled data.

To prepare our training set, we extract the mel-spectogram for every speech sample.

These are then passed through the speech encoder first and then the resultant speech features from the 14th layer are taken and quantized using a k-means clustering model.

The features are extracted at 75\% of the model depth consistent with previous works[^Maiti2024Voxtlm], [^Communication2023SeamlessM4T].

We also de-duplicate adjacent discrete units for all samples.

The mel and the corresponding discrete units become the input for our speech decoder training.

With this, the model is able to learn to produce mel-spectogram from these given discrete representations and is also able to condition them based on the provided speech prompt.

During inference, we pass both the source and target speech through our speech encoder system.

The mel-spectogram of the target speech and its discrete units form the reference for our CFM model and the source discrete units form the prompt to generate the corresponding mel.

The target and source units are concatenated and given as input to the model.

The target mel is then discarded upon inference.

This generated mel inherits the speaker attributes from the reference target mel while the content and style is obtained from the source units.

### Speech-to-Units

To extract high-quality speech representations, we employ Xeus [^Chen2024Towards], a self-supervised learning (SSL) encoder trained on an extensive multilingual dataset encompassing 4,000 languages.

Given its exposure to such linguistic diversity, we expect Xeus to provide robust, language-agnostic representations, enabling our model to generalize effectively to unseen languages.

Similar to WavLM, Xeus processes speech by generating frame-level embeddings.

Each output embedding corresponds to a 25ms window size with a 20ms stride, effectively producing 50 embeddings per second of speech.

For the purpose of enabling speech reconstruction, we apply a quantization step using k-means clustering.

Specifically, we train a 500-cluster k-means model using embeddings extracted from the 14th layer of Xeus.

This clustering process provides us discrete speech units that can be used to train a units-to-speech model for resynthesis.

Our k-means training dataset comprises 100 hours of English speech and 50 hours each from five Indian languages, ensuring a balanced and representative distribution of phonetic variations.

This dataset is a subset of the one used for training EZ-VC.

### Units-to-Speech

We choose the F5-TTS[^Chen2024F5-TTS] architecture for our speech generation system.

Building upon the work of E2-TTS[^Eskimez2024E2] and Voicebox, F5-TTS manages to alleviate several of their shortcomings such as duration modelling, phoneme alignment and slow convergence.

We train our model for speech generation with discrete units as input.

The model learns to reconstruct speech from these condensed speech representations via an infilling task.

The speaker attributes are derived from the unmasked mel-spectogram and the speech content comes from the input units.

This disentangles the speaker and speech, allowing us to achieve zero-shot voice conversion. 

## 4·Experiment

### Datasets

We select a wide variety of publicly available datasets for English and 5 Indian languages comprising of a total 12840 hours of speech.

We hope that using a diverse set of languages and accents will help the model to generalize in unseen settings.

For English, we use 3060 hours of speech which includes a range of American, European and Indian accents.

American accents come from Librispeech while European accents appear in Vox Populi[^Wang2021VoxPopuli] dataset.

For Indian English accent we use 1100 of speech from NPTEL\footnote{https://nptel.ac.in/} lectures.

We also select 5 Indian languages, namely Bengali, Hindi, Tamil, Telugu and Kannada to introduce diversity to our training set.

We obtain in total 9780 hours of data from these languages.

We procure unlabeled speech from several sources including Vaani[^Bhogale2022Effectiveness], Commonvoice[^Ardila2020Common] and datasets from IIIT-H and IIT-M.

Table [tab:indic](#tab:indic) contains a full breakdown.

We downsample all data, wherever neccessary to 16KHz.

We further pass this data through our speech decoder combination of Xeus and k-means model to obtain discrete speech representations of each audio sample.

<a id="tab:eval">Performance metrics comparison of different VC baselines</a>

### Training setup

We adopt the original implementation of F5-TTS for training our model.

We use the base model configuration(300M params) which consists of 22 layers, 16 attention heads.

For the audio samples we set sampling rate to 16KHz and use 80-dimensional log mel-filterbank features with hop length of 160.

We also train a base BigVGAN[^Lee2023BigVGAN] model on Libri-TTS[^Zen2019LibriTTS] with the same configuration for a million steps.

For our tokenizer, we use character level tokens with a vocabulary which includes all the 500 different discrete units.

We train this F5-TTS model from scratch with a batch size of 64 samples for 1.35 million updates on 4 NVIDIA RTX 6000 ADA GPUs.

We use a peak learning rate of 5e-5 with 100k warmup steps.

The rest remains the same as the original F5-TTS configuration.

## 5·Evaluation

Subjective and objective measures are equally important for evaluating voice conversion systems.

In our test we use Naturalness Mean Opinion Score (NMOS) and Similarity Mean Opinion Score (SMOS) as our subjective evaluations.

For objectivity, we utilize Speaker Similarity (SSim) and UTMOS[^Saeki2022Utmos] scores for comparing our models.

We measure speaker similarity by using cosine similarity scores between our target speech and that of our output speech by using embeddings from a speaker verification model called ECAPA-TDNN[^Desplanques2020Ecapa-TDNN].

For our baselines, we select few of the most recent and best performing open-source voice conversion models.

This makes sure that we evaluate our model against the current state-of-the-art architectures available.

We select SeedVC, vec2wav 2.0, Diff-HierVC[^Choi2023Diff-HierVC] and kNN-VC as our baselines.

Vec2wav and kNN-VC use primarily units-to-speech vocoders, while Diff-HierVC employs diffusion based methods.

SeedVC and our work meanwhile uses CFM based speech models. 

We choose 10 samples for our evaluations.

These samples are selected from various languages and accents.

We prepare a variety of source and target speech combinations based on gender, inter-lingual and cross-lingual speech.

We also include combinations of seen and unseen languages to test the robustness and generalization capabilities of these models.

All audios are resasmpled to 16KHz to ensure fair comparison.

For our subjective evaluation, we provided these 10 samples to 20 student volunteers for comparison.

Each volunteer was asked to evaluate each sample based on it's naturalness which evaluates for mainly intelligibility, style preservence, and sound quality of the output speech in comparison to the source speech.

In contrast, the similarity mean opinion score judges the similarity of the speaker in the output speech to that of the target speaker.

We take the average of all the samples from all the volunteers which becomes the results of our NMOS and SMOS scores.

We further objectively compare our model with Seed-VC on a seen language(English) and 2 unseen languages(German and Spanish).

The results, as shown in Table [tab:ssim_utmos_2](#tab:ssim_utmos_2), demonstrate that EZ-VC provides better naturalness according to UTMOS, while having comparable or better speaker similarity scores. 

Analyzing the naturalness and similarity MOS scores from Table [tab:eval](#tab:eval), we see that EZ-VC convincingly beats the latest state-of-the-art approaches for voice conversion.

We find that Vec2wav 2.0, which uses discrete units coupled with a vocoder competes very well for naturalness but lags behind when it comes to imitating the target speaker.

This shows that having a CFM based speech decoder is a major benefit for voice conversion systems as they are better able to capture speech styles.

They also seem to generalize very well for unseen languages and accents.

## 6·Conclusion

EZ-VC hopes to make a substantial advancement in the field of zero-shot voice conversion, demonstrating that high-quality voice transformation can be achieved with a minimal architecture.

By leveraging discrete speech representations from self-supervised models and a non-autoregressive speech decoder, EZ-VC balances both naturalness and speaker similarity without the need for complex feature disentanglement or multiple encoders.

The model's ability to generalize across diverse linguistic settings highlights its robustness in cross-lingual contexts.

Our findings may also suggest that discrete representations capture deeper, more universal representations of speech.

Our comprehensive evaluations show that EZ-VC achieves significantly improved capabilities for zero-shot voice conversion.

We hope that our work inspires further efforts to simplify voice conversion techniques.

## 7·Potential Risks

Given the highly realistic quality of voice synthesis and the ability to achieve cross-lingual voice conversion for even unseen languages, our model carries the risk of enabling dangerous deepfakes. 

\section*{Limitations}

Despite the benifits of our approach, it has a few limitations,

-  The EZ-VC architecture is reliant on the quality of the pretrained speech encoder.

It is likely that using an encoder trained on only one language may not achieve the level of generalization that our model does.

-  Although our approach introduces a much simpler architecture than previous works, the computational requirements are still comparable or higher.

\bibliography{custom}

\appendix

\section*{Appendix}
\label{sec:appendix}

<a id="tab:my_label">English Datasets</a>

<a id="tab:ssim_utmos_1">EZ-VC Vs Seed-VC on seen languages</a>

<a id="tab:ssim_utmos_2">EZ-VC Vs Seed-VC on unseen languages</a>

<a id="tab:indic">Indian Language Datasets</a>

\end{document}
