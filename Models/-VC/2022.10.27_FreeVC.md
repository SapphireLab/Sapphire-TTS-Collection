---
PDF: 
标题: "FreeVC: Towards High-Quality Text-Free One-Shot Voice Conversion"
作者:
  - Jingyi Li
  - Weiping Tu
  - Li Xiao
机构:
  - 武汉大学
代码: https://github.com/olawod/freevc
ArXiv: https://arxiv.org/abs/2210.15418
提出时间: 2022-10-27
出版社:
  - IEEE
发表期刊:
  - International Conference on Acoustics, Speech and Signal Processing (ICASSP)
发表时间: 2023-06-04
引文数量: 28
被引次数: 29
tags:
  - 单样本_One-Shot
  - 声音克隆_VC
  - 开源_OpenSource
DOI: https://doi.org/10.1109/ICASSP49357.2023.10095191
aliases:
  - FreeVC
ArXiv最新版本: "1"
ArXiv最新时间: 2022-10-27
PageNum: "5"
Demo: https://doi.org/10.1109/ICASSP49357.2023.10095191
---
# FreeVC: Towards High-Quality Text-Free One-Shot Voice Conversion

## Abstract

## 1.Introduction

## 2.Related Works

//TODO# FreeVC: Towards High-Quality Text-Free One-Shot Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "FreeVC: Towards High-Quality Text-Free One-Shot Voice Conversion."
- 作者:
  - 01 Jingyi li
  - 02 Weiping tu
  - 03 Li xiao
- 链接:
  - [ArXiv](https://arxiv.org/abs/2210.15418v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2210.15418v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2022.10.27_2210.15418v1_FreeVC__Towards_High-Quality_Text-Free_One-Shot_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

Voice conversion (VC) can be achieved by first extracting source content information and target speaker information, and then reconstructing waveform with these information.
However, current approaches normally either extract dirty content information with speaker information leaked in, or demand a large amount of annotated data for training.
Besides, the quality of reconstructed waveform can be degraded by the mismatch between conversion model and vocoder.
In this paper, we adopt the end-to-end framework of VITS for high-quality waveform reconstruction, and propose strategies for clean content information extraction without text annotation.
We disentangle content information by imposing an information bottleneck to WavLM features, and propose the spectrogram-resize based data augmentation to improve the purity of extracted content information.
Experimental results show that the proposed method outperforms the latest VC models trained with annotated data and has greater robustness.

## 1·Introduction

\label{sec:intro}

Voice conversion (VC) is a technique that alters the voice of a source speaker to a target style, such as speaker identity[^Mohammadi2017Overview], prosody[^Wang2018Style] and emotion[^Zhou2021Seen], while keeping the linguistic content unchanged.

In this paper, we focus on the speaker identity conversion under one-shot setting, i.e., given only one utterance of target speaker as reference.

A typical approach of one-shot voice conversion is to disentangle content information and speaker information from source and target speech, respectively, and then use them to reconstruct the converted speech[^Qian2019Autovc].

As a result, the quality of converted speech relys on (1) the disentanglement ability of VC model, and (2) the reconstruction ability of VC model.

Based on how a VC system disentangles content information, we can categorize current VC approaches into text-based VC and text-free VC.

A popular text-based VC approach is to use an automatic speech recognition (ASR) model to extract phonetic posteriorgram (PPG) as content representation[^Sun2016Phonetic][^Liu2021Any-to-Many].

Some researchers have also resolved to leveraging shared linguistic knowledge from a text-to-speech (TTS) model[^Zhang2021Transfer][^Park2020Cotatron].

However, these approaches require an extensive amount of annotated data for training the ASR or TTS model.

Data annotation is costly, and the accuracy and granularity, e.g. phoneme level and grapheme level, of annotation affects the model performance.

To avoid the concerns of text-based approaches, text-free approaches that learn to extract content information without the guidance of text annotation have been explored.

Typical text-free approaches include information bottleneck[^Qian2019Autovc], vector quantization[^Wu2020Vqvc+], instance normalization[^Chen2021Again-Vc], etc.

However, their performance generally lags behind text-based approaches[^Zhao2020Voice].

This can be attributed to the fact that the content information they extract is more easily to have source speaker information leaked in.

Many VC systems adopt a two-stage reconstruction pipe-line[^Liu2021Any-to-Many][^Qian2019Autovc].

A conversion model converts the source acoustic features into target speaker’s voice in the first stage, while a vocoder transforms the converted features into waveform in the second stage.

The two models are usually trained separately.

However, the acoustic feature predicted by conversion model has a different distribution from that the vocoder uses during training, which is from the real speech.

This feature mismatch problem, which also exists in TTS, can degrade the quality of reconstructed waveform[^Wu2018Collapsed].

VITS[^Kim2021Conditional] is a one-stage model that can do both TTS and VC.

By connecting models of the two stages through latent variables of a conditional variational autoencoder (CVAE), the feature mismatch is reduced.

By adopting adversarial training, the quality of reconstructed waveform is further improved.

However, VITS is a text-based model and is limited to many-to-many VC, i.e. the source and target speakers are all seen speakers.

In this paper, we propose a text-free one-shot VC system named FreeVC, which adopts the framework of VITS for its brilliant reconstruction ability, but learns to disentangle content information without the need of text annotation.

The recent success of speech self-supervised learning (SSL) in downstream tasks such as speech recognition[^Conneau2020Unsupervised], speaker verification[^Chen2022Large-Scale] and voice conversion[^Huang2022S3prl-Vc] has demonstrated the potential power of SSL features over traditional acoustic features like mel-spectrograms.

We use WavLM[^Chen2022Wavlm] to extract SSL features from waveform, and introduce a bottleneck extractor to extract content information from SSL features.

We also propose spectrogram-resize (SR) based data augmentation, which distorts speaker information without changing content information, to strengthen the disentanglement ability of the model.

To achieve one-shot VC, we use a speaker encoder for speaker information extraction.

Our code~\footnote{\url{https://github.com/OlaWod/FreeVC}} and demo page~\footnote{\url{https://olawod.github.io/FreeVC-demo}} are publicly available.

## 2·Methods

\label{sec:architecture}

As illustrated in Fig.[fig:model](#fig:model), the backbone of FreeVC is inherited from VITS, which is a CVAE augmented with GAN training.

Different from VITS, the prior encoder of FreeVC takes raw waveform as input instead of text annotation, and has a different structure.

The speaker embedding is extracted by a speaker encoder to perform one-shot VC.

In addition, FreeVC adopts a different training strategy and inference procedure.

We will present the details in the following subsections.

%\centerline{\epsfig{fig1,width=8.5cm}}

%\vspace{4cm}

![](figs/train.pdf)

<a id="fig:aug1">Training procedure</a>

### Model Architecture

\label{sssec:bottleneck}

FreeVC contains a prior encoder, a posterior encoder, a decoder, a discriminator and a speaker encoder, where the architecture of the posterior encoder, decoder and discriminator follows VITS.

We will focus on describing the prior encoder and speaker encoder in the following.

#### Prior Encoder

\label{sssec:bottleneck}

The prior encoder contains a WavLM model, a bottleneck extractor and a normalizing flow.

The WavLM model and bottleneck extractor are in charge of extracting content information in the form of modeling distribution $N(z'; \mu_{\theta}, \sigma_{\theta}^2)$.

The WavLM model takes raw waveform as input and produces 1024-dimensional SSL feature $x_{ssl}$ containing both content information and speaker information.

In order to remove the unwanted speaker information contained in $x_{ssl}$, the 1024-dim $x_{ssl}$ is put into the bottleneck extractor and converted into $d$-dim representation, where $d$ is much smaller than 1024.

This huge dimension gap imposes an information bottleneck, forcing the resulted low-dimentional representation to discard content-irrelevant information like noise or speaker information.

Next, the $d$-dim hidden representation is projected into $2d$-dim hidden representation, which is latter split into $d$-dim $\mu_{\theta}$ and $d$-dim $\sigma_{\theta}$.  The normalizing flow, which conditions on speaker embedding $g$, is adopted to improve the complexity of prior distribution.

Following VITS, it is composed of multiple affine coupling layers[^Dinh2016Density] and is made to be volume-preserving with the Jacobian determinant $|det\frac{\partial z'}{\partial z}|$ of 1.

#### Speaker Encoder

\label{sssec:bottleneck}

We use two types of speaker encoder: pretrained speaker encoder and non-pretrained speaker encoder.

The pretrained speaker encoder is a speaker verification model trained on datasets with large amounts of speakers.

It is widely used in VC and is considered to be superior to non-pretrained speaker encoder.

We adopt the one employed in [^Liu2021Any-to-Many].

The non-pretrained speaker encoder is jointly trained with the rest of the model from scratch.

We use a simple LSTM-based architecture and believe that if the extracted content representation is clean enough, the speaker encoder will learn to model the missing speaker information.

### Training Strategy

\label{sssec:sr}

![](figs/68.pdf)

<a id="fig:aug1">Resize ratio $r<1$</a>

#### SR-based Data Augmentation

\label{sssec:sr}

A too narrow bottleneck will lose some content information, while a too wide bottleneck will contain some speaker information[^Qian2019Autovc].

Instead of carefully tuning the bottleneck size, we resolve to SR-based data augmentation to help the model to learn to extract clean content information by distorting speaker information in the source waveform.

Unlike works[^Chan2022SpeechSplit2.][^Choi2021Neural] that use various signal processing techniques to corrupt speaker information, our method is much easier to implement, and does not require complicated signal processing knowledge. 

Our proposed SR-based data augmentation includes three steps: (1) get mel-spectrogram $x_{mel}$ from waveform $y$; (2) conduct vertical SR operation to $x_{mel}$, resulting in modified mel-spectrogram $x_{mel}'$; (3) reconstruct waveform $y'$ from $x_{mel}'$ with a neural vocoder.

The vertical SR operation is depicted in Fig.[fig:vsr](#fig:vsr).

A mel-spectrogram can be seen as an image with horizontal time axis and vertical frequency bin axis.

Vertical SR operation first resizes mel-spectrogram to a certain ratio $r$ vertically using bilinear interpolation, and then pads or cuts the resized mel-spectrogram to the original shape.

If the ratio $r$ is less than $1$, we pad the squeezed mel-spectrogram at the top with the sum of highest frequency bin value and Gaussian noise, resulting in speech with lower pitch and closer formant distance; else, we cut redundant frequency bins at the top of the stretched mel-spectrogram, resulting in speech with higher pitch and farther formant distance.

By training with augmented speech, the model will better learn to extract the unchanged content information shared in each ratio $r$.

In addition to vertical SR, we can also use horizontal SR to produce time-scale modified waveforms.

#### Training Loss

\label{ssec:backbone}

The training loss is divided into  CVAE-related loss and GAN-related loss.

The CVAE-related loss consists of reconstruction loss $L_{rec}$, which is the $L_1$ distance between target and predicted mel-spectrogram, and KL loss $L_{kl}$, which is the KL divergence between prior distribution $p_{\theta}(z|c)$ and posterior distribution $q_{\phi}(z|x_{lin})$, where

$$
\begin{aligned}

&q_{\phi}(z|x_{lin}) = N(z; \mu_{\phi}, \sigma_{\phi}^2),\\
&p_{\theta}(z|c) = N(z'; \mu_{\theta}, \sigma_{\theta}^2)|det\frac{\partial z'}{\partial z}|.

\end{aligned}
$$

Here the condition $c$ is content information contained in waveform $y/y’$.

By minimizing $L_{kl}$, the feature mismatch problem can be reduced.

The GAN-related loss consists of adversarial loss[^Mao2017Least] $L_{adv}(D)$ and $L_{adv}(G)$ for discriminator $D$ and generator $G$, and feature matching loss[^Larsen2016Autoencoding] $L_{fm}(G)$ for generator $G$.

Finally, the training loss of FreeVC can be expressed as:

$$
\begin{aligned}

L(G) &= L_{rec} + L_{kl} + L_{adv}(G) + L_{fm}(G),\\
L(D) &= L_{adv}(D).

\end{aligned}
$$

### Inference Procedure

\label{sssec:sr}

Different from VITS, which extracts content information through posterior encoder and normalizing flow in prior encoder during VC inference, FreeVC extracts content information through WavLM and bottleneck extractor in prior encoder during inference as with in training.

Such that the extracted content representation will not be affected by the quality of source speaker embedding.

## 3·Experiments

\label{sec:exp}

<a id="tab:sbj">Subjective evaluation results in terms of 5-scale MOS and SMOS with 95\% confidence intervals under seen-to-seen, unseen-to-seen and unseen-to-unseen scenarios.

For reference, we also report scores of source utterances.</a>

### Experimental Setups

\label{ssec:datapre}

We conduct experiments on VCTK[^Yamagishi2019CSTR] and LibriTTS[^Zen2019LibriTTS].

Only VCTK corpus is used for training.

For VCTK, we use data from 107 speakers, in which 314 utterances  (2 sentences per speaker) are randomly selected for validation, 10700 utterances (10 sentences per speaker) for test, and the rest for training.

For LibriTTS, we use the test-clean subset for test. 

All audio samples are downsampled to 16 kHz.

Linear spectrograms and 80-band mel-spectrograms are calculated using  short-time Fourier transform.

The FFT, window, and hop size are set to 1280, 1280, and 320, respectively.

We set the dimension $d$ of bottleneck extractor to 192.

For the SR-based data augmentation, the resize ratio $r$ ranges from 0.85 to 1.15.

And a HiFi-GAN v1 vocoder[^Kong2020Hifi-Gan] is used to transform the modified mel-spectrogram into waveform.

Our models are trained up to 900k steps on a single NVIDIA 3090 GPU.

The batch size is set to 64 with a maximum segment length of 128 frames.

Three baseline models are selected to be compared with the proposed method: (1) VQMIVC[^Wang2021Vqmivc], a text-free model that uses non-pretrained speaker encoder; (2) BNE-PPG-VC[^Liu2021Any-to-Many], a text-based model that uses pretrained speaker encoder; (3) YourTTS[^Casanova2022Yourtts], a text-based model that extends VITS to one-shot setting by introducing a pretrained speaker encoder.

Three versions of the proposed method are tested: (1) FreeVC-s, the proposed model that uses non-pretrained speaker encoder. (2) FreeVC, the proposed model that uses pretrained speaker encoder. (3) FreeVC (w/o SR), the proposed model that uses pretrained speaker encoder, but is trained without SR-based data augmentation.

### Evaluation Metrics

\label{sec:result}

We conduct evaluations both subjectively and objectively.

For subjective evaluation, 15 participants are invited to evaluate the naturalness and speaker similarity of the speech in terms of 5-scale mean opinion score (MOS) and similarity mean opinion score (SMOS), respectively.

We randomly select 6 seen speakers (3 male, 3 female) from VCTK, 6 unseen speakers (3 male, 3 female) from LibriTTS, and conduct evaluation in seen-to-seen, unseen-to-seen, and unseen-to-unseen scenarios separately.

For objective evaluation, we use three metrics: WER, CER and F0-PCC.

Word error rate (WER) and character error rate (CER) between source and converted speech are obtained by an ASR model~\footnote{\href{https://huggingface.co/facebook/hubert-large-ls960-ft}{https://huggingface.co/facebook/hubert-large-ls960-ft}}.

F0-PCC is the Pearson correlation coefficient[^Benesty2009Pearson] between F0 of source and converted speech.

We randomly select 400 utterances (200 from VCTK, 200 from LibriTTS) as source speech, and 12 speakers (6 seen, 6 unseen) as target speaker.

### Results and Analysis

\label{ssec:ablation}

#### Speech Naturalness and Speaker Similarity

\label{ssec:ablation}

The MOS and SMOS results in Table~[tab:sbj](#tab:sbj) demonstrate that the proposed models outperform all baseline models in all scenarios in terms of both speech naturalness and speaker similarity.

In addition, we observe that all baselines suffer from quality degradation when the quality of source speech is low, for example, with a low recording quality or an unclear pronunciation, while our proposed models are barely affected, which shows the robustness of proposed content extraction method.

Within the three versions of the proposed method, FreeVC (w/o SR) achieves lower speech naturalness and speaker similarity than FreeVC.

This indicates that model trained without SR-based data augmentation has some source speaker information leaked to the bottleneck, making it harder to reconstruct satisfactory waveform.

FreeVC-s has a similar performance with FreeVC, demonstrating that a pretrained speaker encoder is not a dominating factor of our method's performance, and a simple non-pretrained speaker encoder is able to match the performance with a pretrained speaker encoder.

FreeVC performs better than FreeVC-s in unseen-to-unseen scenario, which indicates that a speaker encoder pretrained on large amounts of speakers can improve the performance for unseen targets. 

#### Speech Intelligence and F0 Variation Consistency

\label{ssec:ablation}

It can be seen in Table~[tab:obj](#tab:obj) that our proposed models achieve lower WER and CER than all baseline models, even the text-based ones.

This indicates that the proposed method can preserve the linguistic content of source speech well.

The F0-PCC results show that our proposed method has higher F0 variation consistency with the source speech, which demonstrates that the proposed method can effectively maintain the prosody of source speech.

Besides, we observe that training with SR-based data augmentation improves both speech intelligence and F0 variation consistency slightly.

<a id="tab:obj">Objective evaluation results.

For WER and CER, the smaller the better.

F0-PCC ranges from -1 to 1 and the higher the better.</a>

## 4·Conclusion

\label{sec:conclusion}

This paper proposes FreeVC, a text-free one-shot voice conversion system.

We adopt the framework of VITS for high-quality waveform reconstruction.

The content information is extracted from the bottleneck of WavLM features.

We also propose SR-based data augmentation to improve the disentanglement ability of the model.

Experimental results demonstrate the superiority of proposed methods.

In the future, we will investigate speaker adaptation methods to improve similarity for unseen target speakers with little data.

\vfill\pagebreak

\bibliographystyle{IEEEbib}
\bibliography{strings}

\end{document}

## References

[^Mohammadi2017Overview]: An Overview of Voice Conversion Systems. Speech Communication 2017.
[^Wang2018Style]: Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis. International Conference on Machine Learning 2018.
[^Zhou2021Seen]: Seen and Unseen Emotional Style Transfer for Voice Conversion With a New Emotional Speech Dataset. Proc. ICASSP 2021 2021.
[^Qian2019Autovc]: Autovc: Zero-Shot Voice Style Transfer With Only Autoencoder Loss. International Conference on Machine Learning 2019.
[^Sun2016Phonetic]: Phonetic Posteriorgrams for Many-to-One Voice Conversion Without Parallel Data Training. 2016 IEEE International Conference on Multimedia and Expo (ICME) 2016.
[^Liu2021Any-to-Many]: Any-to-Many Voice Conversion With Location-Relative Sequence-to-Sequence Modeling. IEEE/ACM Transactions on Audio, Speech, and Language Processing 2021.
[^Zhang2021Transfer]: Transfer Learning From Speech Synthesis to Voice Conversion With Non-Parallel Training Data. IEEE/ACM Transactions on Audio, Speech, and Language Processing 2021.
[^Park2020Cotatron]: Cotatron: Transcription-Guided Speech Encoder for Any-to-Many Voice Conversion Without Parallel Data. Proc. Interspeech 2020 2020.
[^Wu2020Vqvc+]: VQVC+: One-Shot Voice Conversion by Vector Quantization and U-Net Architecture. Proc. Interspeech 2020 2020.
[^Chen2021Again-Vc]: Again-Vc: A One-Shot Voice Conversion Using Activation Guidance and Adaptive Instance Normalization. Proc. ICASSP 2021 2021.
[^Zhao2020Voice]: Voice Conversion Challenge 2020: Intra-Lingual Semi-Parallel and Cross-Lingual Voice Conversion. arXiv:2008.12527.
[^Wu2018Collapsed]: Collapsed Speech Segment Detection and Suppression for WaveNet Vocoder. Proc. Interspeech 2018 2018.
[^Kim2021Conditional]: Conditional Variational Autoencoder With Adversarial Learning for End-to-End Text-to-Speech. arXiv:2106.06103.
[^Conneau2020Unsupervised]: Unsupervised Cross-Lingual Representation Learning for Speech Recognition. arXiv:2006.13979.
[^Chen2022Large-Scale]: Large-Scale Self-Supervised Speech Representation Learning for Automatic Speaker Verification. Proc. ICASSP 2022 2022.
[^Huang2022S3prl-Vc]: S3prl-Vc: Open-Source Voice Conversion Framework With Self-Supervised Speech Representations. Proc. ICASSP 2022 2022.
[^Chen2022Wavlm]: Wavlm: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing. IEEE Journal of Selected Topics in Signal Processing 2022.
[^Dinh2016Density]: Density Estimation Using Real NVP. arXiv:1605.08803.
[^Chan2022SpeechSplit2.]: SpeechSplit2. 0: Unsupervised Speech Disentanglement for Voice Conversion Without Tuning Autoencoder Bottlenecks. Proc. ICASSP 2022 2022.
[^Choi2021Neural]: Neural Analysis and Synthesis: Reconstructing Speech From Self-Supervised Representations. Advances in Neural Information Processing Systems 2021.
[^Mao2017Least]: Least Squares Generative Adversarial Networks. Proceedings of the IEEE International Conference on Computer Vision 2017.
[^Larsen2016Autoencoding]: Autoencoding Beyond Pixels Using a Learned Similarity Metric. International Conference on Machine Learning 2016.
[^Yamagishi2019CSTR]: CSTR VCTK Corpus: English Multi-Speaker Corpus for CSTR Voice Cloning Toolkit (Version 0.92). 
[^Zen2019LibriTTS]: LibriTTS: A Corpus Derived From LibriSpeech for Text-to-Speech. arXiv:1904.02882.
[^Kong2020Hifi-Gan]: Hifi-Gan: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis. arXiv:2010.05646.
[^Wang2021Vqmivc]: Vqmivc: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-Shot Voice Conversion. arXiv:2106.10132.
[^Casanova2022Yourtts]: Yourtts: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for Everyone. International Conference on Machine Learning 2022.
[^Benesty2009Pearson]: Pearson Correlation Coefficient. Noise Reduction in Speech Processing 2009.