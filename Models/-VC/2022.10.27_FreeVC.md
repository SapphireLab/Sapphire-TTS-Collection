---
PDF: 
标题: "FreeVC: Towards High-Quality Text-Free One-Shot Voice Conversion"
作者:
  - Jingyi Li
  - Weiping Tu
  - Li Xiao
机构:
  - 武汉大学
代码: https://github.com/olawod/freevc
ArXiv: https://arxiv.org/abs/2210.15418
提出时间: 2022-10-27
出版社:
  - IEEE
发表期刊:
  - International Conference on Acoustics, Speech and Signal Processing (ICASSP)
发表时间: 2023-06-04
引文数量: 28
被引次数: 29
tags:
  - 单样本_One-Shot
  - 声音克隆_VC
  - 开源_OpenSource
DOI: https://doi.org/10.1109/ICASSP49357.2023.10095191
aliases:
  - FreeVC
ArXiv最新版本: "1"
ArXiv最新时间: 2022-10-27
PageNum: "5"
Demo: https://doi.org/10.1109/ICASSP49357.2023.10095191
---
# FreeVC: Towards High-Quality Text-Free One-Shot Voice Conversion

## Abstract

## 1.Introduction

## 2.Related Works

//TODO# FreeVC: Towards High-Quality Text-Free One-Shot Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "FreeVC: Towards High-Quality Text-Free One-Shot Voice Conversion."
- 作者:
  - 01 Jingyi li
  - 02 Weiping tu
  - 03 Li xiao
- 链接:
  - [ArXiv](https://arxiv.org/abs/2210.15418v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2210.15418v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2022.10.27_2210.15418v1_FreeVC__Towards_High-Quality_Text-Free_One-Shot_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

Voice conversion (VC) can be achieved by first extracting source content information and target speaker information, and then reconstructing waveform with these information.
However, current approaches normally either extract dirty content information with speaker information leaked in, or demand a large amount of annotated data for training.
Besides, the quality of reconstructed waveform can be degraded by the mismatch between conversion model and vocoder.
In this paper, we adopt the end-to-end framework of VITS for high-quality waveform reconstruction, and propose strategies for clean content information extraction without text annotation.
We disentangle content information by imposing an information bottleneck to WavLM features, and propose the spectrogram-resize based data augmentation to improve the purity of extracted content information.
Experimental results show that the proposed method outperforms the latest VC models trained with annotated data and has greater robustness.

## 1·Introduction

\label{sec:intro}

Voice conversion (VC) is a technique that alters the voice of a source speaker to a target style, such as speaker identity[^Mohammadi2017Overview], prosody[^Wang2018Style] and emotion[^Zhou2021Seen], while keeping the linguistic content unchanged.

In this paper, we focus on the speaker identity conversion under one-shot setting, i.e., given only one utterance of target speaker as reference.

A typical approach of one-shot voice conversion is to disentangle content information and speaker information from source and target speech, respectively, and then use them to reconstruct the converted speech[^Qian2019Autovc].

As a result, the quality of converted speech relys on (1) the disentanglement ability of VC model, and (2) the reconstruction ability of VC model.

Based on how a VC system disentangles content information, we can categorize current VC approaches into text-based VC and text-free VC.

A popular text-based VC approach is to use an automatic speech recognition (ASR) model to extract phonetic posteriorgram (PPG) as content representation[^Sun2016Phonetic][^Liu2021Any-to-Many].

Some researchers have also resolved to leveraging shared linguistic knowledge from a text-to-speech (TTS) model[^Zhang2021Transfer][^Park2020Cotatron].

However, these approaches require an extensive amount of annotated data for training the ASR or TTS model.

Data annotation is costly, and the accuracy and granularity, e.g. phoneme level and grapheme level, of annotation affects the model performance.

To avoid the concerns of text-based approaches, text-free approaches that learn to extract content information without the guidance of text annotation have been explored.

Typical text-free approaches include information bottleneck[^Qian2019Autovc], vector quantization[^Wu2020Vqvc+], instance normalization[^Chen2021Again-Vc], etc.

However, their performance generally lags behind text-based approaches[^Zhao2020Voice].

This can be attributed to the fact that the content information they extract is more easily to have source speaker information leaked in.

Many VC systems adopt a two-stage reconstruction pipe-line[^Liu2021Any-to-Many][^Qian2019Autovc].

A conversion model converts the source acoustic features into target speaker’s voice in the first stage, while a vocoder transforms the converted features into waveform in the second stage.

The two models are usually trained separately.

However, the acoustic feature predicted by conversion model has a different distribution from that the vocoder uses during training, which is from the real speech.

This feature mismatch problem, which also exists in TTS, can degrade the quality of reconstructed waveform[^Wu2018Collapsed].

VITS[^Kim2021Conditional] is a one-stage model that can do both TTS and VC.

By connecting models of the two stages through latent variables of a conditional variational autoencoder (CVAE), the feature mismatch is reduced.

By adopting adversarial training, the quality of reconstructed waveform is further improved.

However, VITS is a text-based model and is limited to many-to-many VC, i.e. the source and target speakers are all seen speakers.

In this paper, we propose a text-free one-shot VC system named FreeVC, which adopts the framework of VITS for its brilliant reconstruction ability, but learns to disentangle content information without the need of text annotation.

The recent success of speech self-supervised learning (SSL) in downstream tasks such as speech recognition[^Conneau2020Unsupervised], speaker verification[^Chen2022Large-Scale] and voice conversion[^Huang2022S3prl-Vc] has demonstrated the potential power of SSL features over traditional acoustic features like mel-spectrograms.

We use WavLM[^Chen2022Wavlm] to extract SSL features from waveform, and introduce a bottleneck extractor to extract content information from SSL features.

We also propose spectrogram-resize (SR) based data augmentation, which distorts speaker information without changing content information, to strengthen the disentanglement ability of the model.

To achieve one-shot VC, we use a speaker encoder for speaker information extraction.

Our code~\footnote{\url{https://github.com/OlaWod/FreeVC}} and demo page~\footnote{\url{https://olawod.github.io/FreeVC-demo}} are publicly available.
