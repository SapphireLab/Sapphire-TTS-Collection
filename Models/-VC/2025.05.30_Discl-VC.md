# Discl-Vc: Disentangled Discrete Tokens and in-Context Learning for Controllable Zero-Shot Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "Discl-Vc: Disentangled Discrete Tokens and in-Context Learning for Controllable Zero-Shot Voice Conversion."
- 作者:
  - 01 Kaidi Wang
  - 02 Wenhao Guan
  - 03 Ziyue Jiang
  - 04 Hukai Huang
  - 05 Peijie Chen
  - 06 Weijie Wu
  - 07 Qingyang Hong
  - 08 Lin Li
- 链接:
  - [ArXiv](https://arxiv.org/abs/2505.24291v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2505.24291v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.05.30_2505.24291v1_Discl-Vc__Disentangled_Discrete_Tokens_and_in-Context_Learning_for_Controllable_Zero-Shot_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

Currently, zero-shot voice conversion systems are capable of synthesizing the voice of unseen speakers.
However, most existing approaches struggle to accurately replicate the speaking style of the source speaker or mimic the distinctive speaking style of the target speaker, thereby limiting the controllability of voice conversion.
In this work, we propose Discl-VC, a novel voice conversion framework that disentangles content and prosody information from self-supervised speech representations and synthesizes the target speaker's voice through in-context learning with a flow matching transformer.
To enable precise control over the prosody of generated speech, we introduce a mask generative transformer that predicts discrete prosody tokens in a non-autoregressive manner based on prompts.
Experimental results demonstrate the superior performance of Discl-VC in zero-shot voice conversion and its remarkable accuracy in prosody control for synthesized speech.

## 1·Introduction

Voice conversion is the task of transforming the voice of a source speaker into that of a target speaker, while preserving the linguistic content information of the source speaker. 
Zero-shot voice conversion \cite{Li2023SEFVCSE, ma24e_interspeech} generates target speech given the voice of an unseen speaker during training, making the task even more challenging.

A widely adopted approach to address this challenge is to decouple linguistic content information and speaker information from the speech.

The AutoVC series \cite{pmlr-v97-qian19c,pmlr-v119-qian20a, 9747763}, based on a simple autoencoder architecture, utilizes a carefully designed bottleneck to achieve attribute disentanglement, enabling zero-shot conversions.

With the rise of self-supervised models, some researchers have explored disentangling content feature by filtering out timbre information through a bottleneck mechanism within self-supervised representations, such as K-means clustering \cite{Polyak2021SpeechRF}.

ContentVec \cite{pmlr-v162-qian22b} directly proposes a new self-supervised representation that effectively filters out speaker information, while minimizing content loss caused by discretization, thereby performing speech conversion using its continuous representations \cite{huang24_interspeech}.

Additionally, some studies have introduced generative models such as normalizing flows \cite{casanova2022yourtts} and diffusion models \cite{popov2022diffusionbased,choi2024dddm} into voice conversion, enhancing the quality of the converted speech.

However, in addition to linguistic content information and speaker information, speaking style is also a crucial component in speech \cite{Jiang2023MegaTTSZT,jiang2024megatts, 10889108}.

ACE-VC \cite{Hussain2023ACEVCAA} predicts speaking rate and pitch contour through speaker and content embeddings, achieving a controllable and adaptive voice conversion model.

Diff-HierVC \cite{choi23d_interspeech} proposes utilizing the diffusion process to generate F0 (fundamental frequency) to achieve more accurate pronunciation and natural intonation in the converted speech.

StableVC \cite{Yao2024StableVCSC} introduces a method to explicitly disentangle content, style, and timbre, enabling separate control over the speaking style and timbre of the generated speech.

Recently, Vevo \cite{zhang2025vevo} has also introduced a controllable voice conversion model by progressively incorporating style and timbre information using a popular hybrid autoregressive and non-autoregressive architecture.

Despite these advancements, there is still room for improvement in the naturalness and prosody similarity of the converted speech in controllable voice conversion.

In this paper, we propose Discl-VC, a controllable zero-shot voice conversion system that explicitly disentangles and separately models the various attributes of speech.

Specifically, we decompose speech into three parts: content, prosody, and timbre.

We leverage different self-supervised representations and discretization methods to disentangle prosody and content.

Given the impressive capabilities of flow matching \cite{lipman2023flow} in the field of audio \cite{NEURIPS2023_2d8911db,Guan2023ReflowTTSAR,Guan2024LAFMAAL,10889258}, we propose a flow matching transformer that leverages in-context learning to achieve fine-grained timbre modeling \cite{10832320, chen-etal-2024-f5tts}.

At the same time, we introduce a fully non-autoregressive prosody mask transformer, following the mask-and-predict learning paradigm \cite{chang2022maskgit,ju2024naturalspeech,wang2025maskgct}, which predicts prosody tokens of the generated speech based on a reference speech.

Our contributions are summarized as follows:

![](new_discl-vc.pdf)

<a id="fig:discl-vc">The overall architecture of our proposed system.</a>

-  We propose a method that disentangles content and prosody information from self-supervised representations, obtaining discrete content tokens and prosody tokens separately to facilitate prosody control.

-  We introduce two non-autoregressive modules: the prosody mask transformer and the flow matching transformer, which leverage in-context learning to perform prosody token prediction and acoustic representation prediction, respectively.

-  Experimental results demonstrate that our model outperforms the baseline in zero-shot voice conversion and prosody related controllable voice conversion tasks, with improved speech quality and more accurate prosody control.

Our demos are available at https://wkd12345.github.io/disclvc/.
