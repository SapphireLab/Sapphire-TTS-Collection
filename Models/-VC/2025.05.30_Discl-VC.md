# Discl-Vc: Disentangled Discrete Tokens and in-Context Learning for Controllable Zero-Shot Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "Discl-Vc: Disentangled Discrete Tokens and in-Context Learning for Controllable Zero-Shot Voice Conversion."
- 作者:
  - 01 Kaidi Wang
  - 02 Wenhao Guan
  - 03 Ziyue Jiang
  - 04 Hukai Huang
  - 05 Peijie Chen
  - 06 Weijie Wu
  - 07 Qingyang Hong
  - 08 Lin Li
- 链接:
  - [ArXiv](https://arxiv.org/abs/2505.24291v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2505.24291v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.05.30_2505.24291v1_Discl-Vc__Disentangled_Discrete_Tokens_and_in-Context_Learning_for_Controllable_Zero-Shot_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

Currently, zero-shot voice conversion systems are capable of synthesizing the voice of unseen speakers.
However, most existing approaches struggle to accurately replicate the speaking style of the source speaker or mimic the distinctive speaking style of the target speaker, thereby limiting the controllability of voice conversion.
In this work, we propose Discl-VC, a novel voice conversion framework that disentangles content and prosody information from self-supervised speech representations and synthesizes the target speaker's voice through in-context learning with a flow matching transformer.
To enable precise control over the prosody of generated speech, we introduce a mask generative transformer that predicts discrete prosody tokens in a non-autoregressive manner based on prompts.
Experimental results demonstrate the superior performance of Discl-VC in zero-shot voice conversion and its remarkable accuracy in prosody control for synthesized speech.

## 1·Introduction

Voice conversion is the task of transforming the voice of a source speaker into that of a target speaker, while preserving the linguistic content information of the source speaker. 
Zero-shot voice conversion \cite{Li2023SEFVCSE, ma24e_interspeech} generates target speech given the voice of an unseen speaker during training, making the task even more challenging.

A widely adopted approach to address this challenge is to decouple linguistic content information and speaker information from the speech.

The AutoVC series \cite{pmlr-v97-qian19c,pmlr-v119-qian20a, 9747763}, based on a simple autoencoder architecture, utilizes a carefully designed bottleneck to achieve attribute disentanglement, enabling zero-shot conversions.

With the rise of self-supervised models, some researchers have explored disentangling content feature by filtering out timbre information through a bottleneck mechanism within self-supervised representations, such as K-means clustering \cite{Polyak2021SpeechRF}.

ContentVec \cite{pmlr-v162-qian22b} directly proposes a new self-supervised representation that effectively filters out speaker information, while minimizing content loss caused by discretization, thereby performing speech conversion using its continuous representations \cite{huang24_interspeech}.

Additionally, some studies have introduced generative models such as normalizing flows \cite{casanova2022yourtts} and diffusion models \cite{popov2022diffusionbased,choi2024dddm} into voice conversion, enhancing the quality of the converted speech.

However, in addition to linguistic content information and speaker information, speaking style is also a crucial component in speech \cite{Jiang2023MegaTTSZT,jiang2024megatts, 10889108}.

ACE-VC \cite{Hussain2023ACEVCAA} predicts speaking rate and pitch contour through speaker and content embeddings, achieving a controllable and adaptive voice conversion model.

Diff-HierVC \cite{choi23d_interspeech} proposes utilizing the diffusion process to generate F0 (fundamental frequency) to achieve more accurate pronunciation and natural intonation in the converted speech.

StableVC \cite{Yao2024StableVCSC} introduces a method to explicitly disentangle content, style, and timbre, enabling separate control over the speaking style and timbre of the generated speech.

Recently, Vevo \cite{zhang2025vevo} has also introduced a controllable voice conversion model by progressively incorporating style and timbre information using a popular hybrid autoregressive and non-autoregressive architecture.

Despite these advancements, there is still room for improvement in the naturalness and prosody similarity of the converted speech in controllable voice conversion.

In this paper, we propose Discl-VC, a controllable zero-shot voice conversion system that explicitly disentangles and separately models the various attributes of speech.

Specifically, we decompose speech into three parts: content, prosody, and timbre.

We leverage different self-supervised representations and discretization methods to disentangle prosody and content.

Given the impressive capabilities of flow matching \cite{lipman2023flow} in the field of audio \cite{NEURIPS2023_2d8911db,Guan2023ReflowTTSAR,Guan2024LAFMAAL,10889258}, we propose a flow matching transformer that leverages in-context learning to achieve fine-grained timbre modeling \cite{10832320, chen-etal-2024-f5tts}.

At the same time, we introduce a fully non-autoregressive prosody mask transformer, following the mask-and-predict learning paradigm \cite{chang2022maskgit,ju2024naturalspeech,wang2025maskgct}, which predicts prosody tokens of the generated speech based on a reference speech.

Our contributions are summarized as follows:

![](new_discl-vc.pdf)

<a id="fig:discl-vc">The overall architecture of our proposed system.</a>

-  We propose a method that disentangles content and prosody information from self-supervised representations, obtaining discrete content tokens and prosody tokens separately to facilitate prosody control.

-  We introduce two non-autoregressive modules: the prosody mask transformer and the flow matching transformer, which leverage in-context learning to perform prosody token prediction and acoustic representation prediction, respectively.

-  Experimental results demonstrate that our model outperforms the baseline in zero-shot voice conversion and prosody related controllable voice conversion tasks, with improved speech quality and more accurate prosody control.

Our demos are available at https://wkd12345.github.io/disclvc/.

## 2·Discl-Vc

### Speech disentanglement

The overall architecture of our system is shown in Figure [fig:discl-vc](#fig:discl-vc) (1), where the Content Extractor and Content-Prosody Extractor are pre-trained self-supervised models.

In this work, we use HuBERT large \cite{9585401} as the Content Extractor and apply K-means clustering on the continuous representations from the 24th layer.

The number of clusters is set to 1024.

The content tokens obtained from the Content Extractor can be viewed as containing only semantic information while filtering out most timbre and prosody information.

We also perform a deduplication process by removing adjacent duplicate tokens, further eliminating duration-related prosody information.

This approach allows for the re-prediction of speech token durations, thereby improving the prosody of the generated speech.

For the Content-Prosody Extractor, we directly use ContentVec \cite{pmlr-v162-qian22b}.

Since its output continuous representations have already disentangled speaker information and contain almost all of the content and prosody information, this reduces the complexity of the disentangling process.

We introduce a Vector Quantization (VQ) Prosody Encoder to extract the prosody information, as shown in Figure [fig:discl-vc](#fig:discl-vc) (2), which consists of two convolution stacks, a Inversed length regulator (Inversed LR), and a VQ layer.

The Inversed LR adjusts the sequence length based on the duration of the tokens.

We propose a VQ layer as a bottleneck to filter content information.

We also adopt SimVQ \cite{Zhu2024AddressingRC} to avoid the codebook collapse issue.

Specifically, we randomly initialize the codebook vectors and do not update them during training.

Instead, we introduce a linear layer \(W\) that applies to the codebook vector \(q\) to generate the quantized result \(z_q\).

The vector quantization loss is formulated as follows:

$$

\mathcal{L}_{SimVQ} = \lambda \lvert\lvert qW-sg[z]\rvert\rvert^2 + \lvert\lvert z-sg[qW]\rvert\rvert^2

$$

where \(\lambda\) is the hyper-parameter.

Additionally, since F0 is a crucial component of prosody, we introduce a \(\mathcal{L}_{F0}\) to effectively supervise the model's extraction of prosody information.

Specifically, we compute the mean and variance for each speaker and apply Z-score normalization to the ground truth F0 values to obtain speaker-independent prosody.

We then use Smooth L1 loss to compute the difference between the predicted and ground truth values \cite{ju2024naturalspeech}.

Subsequently, we introduce a duration predictor and a length regulator (DP \& LR) to expand the token lengths based on the predicted durations.

The structure of these components follows that of \cite{ren2021fastspeech}.

### In-context learning modeling

#### Flow matching transformer

Flow Matching \cite{lipman2023flow} is a generative approach that learns to map a simple distribution to a target data distribution  by learning an ordinary differential equation (ODE). 
The core idea is to define a time-dependent vector field 
\(v_t(x;\theta)\) that gradually transforms the initial distribution into the data distribution, with the objective of minimizing the discrepancy between the true vector field 
\(u_t(x)\) and the learned vector field \(v_t(x;\theta)\).

In this work, we adopt the optimal transport flow-matching objective \cite{lipman2023flow}, where the flow between the initial and target distributions is modeled as a straight line, leading to the OT-CFM loss:

$$

$$
\begin{aligned}

\mathcal{L}_{OT-CFM}(\theta) = 
\mathbb{E}_{t, q(x_1),p(x_0)} \Big[\lvert\lvert v_t((1-t)x_0+tx_1)\ (x_1-x_0)\rvert\rvert^2 \Big]

\end{aligned}
$$

$$

VoiceBox \cite{NEURIPS2023_2d8911db} was the first to introduce integrating flow matching with transformer for the text-guided speech-infilling task.

In this work, we also adopt the mask-and-predict approach, using in-context learning to generate the masked acoustic features.

Specifically, during training, we sample a time step \(t\), add a certain level of Gaussian noise to the real mel spectrogram to obtain a noisy version, and then feed this noisy mel spectrogram, along with the input prosody and content features and the masked mel spectrogram, into the flow matching transformer.

This module uses the complete prosody and content information, along with the surrounding mel spectrograms, to predict the masked mel spectrogram.

During training, we focus on optimizing only the masked portion.

For the flow matching transformer, we use DiT blocks with the adaLN-zero structure \cite{chen-etal-2024-f5tts}.

We apply classifier-free guidance to generate higher quality speech samples, specifically by setting a 0.2 probability to remove prosody and content tokens, as well as the masked mel spectrogram.

#### Prosody mask transformer

In controllable voice conversion, we need to mimic the speaking style of a reference audio.

To achieve this, we introduce a non-autoregressive prosody mask transformer, which predicts the prosody tokens of the source speech based on the reference audio, thereby generating speech with the same semantic content but different speaking styles.

This masked token modeling approach was first introduced in image synthesis \cite{chang2022maskgit} and has since been widely applied to audio fields \cite{Borsos2023SoundStormEP, wang2025maskgct}.

Unlike autoregressive models that generate tokens sequentially, this method predicts all tokens in parallel and iteratively refines low-confidence outputs.

The training process of the module is shown in Figure [fig:discl-vc](#fig:discl-vc) (3), similar to that in \cite{wang2025maskgct}.

Specifically, given a sequence of prosody tokens, we introduce a special token and replace selected tokens with this token according to a sine schedule.

The module predicts the masked tokens based on the full content token sequence and the unmasked prosody tokens.

The module optimizes the masked portions using cross-entropy loss.

In the inference phase, the module uses the full reference audio's prompt tokens and the source speech's content tokens, along with completely masked prosody tokens, to iteratively unmask the prosody tokens.

At each step, low-confidence tokens are re-masked based on time step until all prosody tokens are generated.

The module architecture is consistent with the flow matching transformer.

We also apply classifier-free guidance with a 0.2 probability of removing the conditions.

### Model training

Our model requires a two-stage training process.

First, we jointly train the VQ prosody encoder, DP\&LR, and flow matching transformer.

Through this training, the model learns to decouple speech and is capable of performing high-quality zero-shot voice conversion tasks.

The loss function for this stage is:

$$

\mathcal{L}_{stage1} = 
\mathcal{L}_{Dur} + \mathcal{L}_{SimVQ} + \mathcal{L}_{FMT} + \mathcal{L}_{F0}

$$

where \(\mathcal{L}_{Dur}\) represents the MSE loss for duration, and \(\mathcal{L}_{FMT}\) represents the flow matching loss.

Afterwards, the trained VQ prosody encoder is used to extract the ground true prosody tokens.

These tokens, along with the content tokens, are then used to continue training the prosody mask transformer.

The training objective is:

$$

\mathcal{L}_{stage2} = \mathcal{L}_{PMT}

$$

where \(\mathcal{L}_{PMT}\) represents the cross-entropy loss for prosody mask transformer.

After training, the module is capable of predicting prosody tokens based on reference audio, enabling controllable voice conversion.
