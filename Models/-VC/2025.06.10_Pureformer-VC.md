# Pureformer-Vc: Non-Parallel Voice Conversion With Pure Stylized Transformer Blocks and Triplet Discriminative Training

<details>
<summary>基本信息</summary>

- 标题: "Pureformer-Vc: Non-Parallel Voice Conversion With Pure Stylized Transformer Blocks and Triplet Discriminative Training."
- 作者:
  - 01 Wenhan Yao
  - 02 Fen Xiao
  - 03 Xiarun Chen
  - 04 Jia Liu
  - 05 YongQiang He
  - 06 Weiping Wen
- 链接:
  - [ArXiv](https://arxiv.org/abs/2506.08348v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2506.08348v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.06.10_2506.08348v1_Pureformer-Vc__Non-Parallel_Voice_Conversion_With_Pure_Stylized_Transformer_Blocks_and_Triplet_Discriminative_Training.pdf)
  - [Publication] #TODO

</details>

## Abstract

As a foundational technology for intelligent human-computer interaction, voice conversion (VC) seeks to transform speech from any source timbre into any target timbre.
Traditional voice conversion methods based on Generative Adversarial Networks (GANs) encounter significant challenges in precisely encoding diverse speech elements and effectively synthesising these elements into natural-sounding converted speech.
To overcome these limitations, we introduce Pureformer-VC, an encoder-decoder framework that utilizes Conformer blocks to build a disentangled encoder and employs Zipformer blocks to create a style transfer decoder.
We adopt a variational decoupled training approach to isolate speech components using a Variational Autoencoder (VAE), complemented by triplet discriminative training to enhance the speaker's discriminative capabilities.
Furthermore, we incorporate the Attention Style Transfer Mechanism (ASTM) with Zipformer's shared weights to improve the style transfer performance in the decoder.
We conducted experiments on two multi-speaker datasets.
The experimental results demonstrate that the proposed model achieves comparable subjective evaluation scores while significantly enhancing objective metrics compared to existing approaches in many-to-many and many-to-one VC scenarios. 

