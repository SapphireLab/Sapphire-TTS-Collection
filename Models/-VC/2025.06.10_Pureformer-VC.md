# Pureformer-Vc: Non-Parallel Voice Conversion With Pure Stylized Transformer Blocks and Triplet Discriminative Training

<details>
<summary>基本信息</summary>

- 标题: "Pureformer-Vc: Non-Parallel Voice Conversion With Pure Stylized Transformer Blocks and Triplet Discriminative Training."
- 作者:
  - 01 Wenhan Yao
  - 02 Fen Xiao
  - 03 Xiarun Chen
  - 04 Jia Liu
  - 05 YongQiang He
  - 06 Weiping Wen
- 链接:
  - [ArXiv](https://arxiv.org/abs/2506.08348v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2506.08348v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.06.10_2506.08348v1_Pureformer-Vc__Non-Parallel_Voice_Conversion_With_Pure_Stylized_Transformer_Blocks_and_Triplet_Discriminative_Training.pdf)
  - [Publication] #TODO

</details>

## Abstract

As a foundational technology for intelligent human-computer interaction, voice conversion (VC) seeks to transform speech from any source timbre into any target timbre.
Traditional voice conversion methods based on Generative Adversarial Networks (GANs) encounter significant challenges in precisely encoding diverse speech elements and effectively synthesising these elements into natural-sounding converted speech.
To overcome these limitations, we introduce Pureformer-VC, an encoder-decoder framework that utilizes Conformer blocks to build a disentangled encoder and employs Zipformer blocks to create a style transfer decoder.
We adopt a variational decoupled training approach to isolate speech components using a Variational Autoencoder (VAE), complemented by triplet discriminative training to enhance the speaker's discriminative capabilities.
Furthermore, we incorporate the Attention Style Transfer Mechanism (ASTM) with Zipformer's shared weights to improve the style transfer performance in the decoder.
We conducted experiments on two multi-speaker datasets.
The experimental results demonstrate that the proposed model achieves comparable subjective evaluation scores while significantly enhancing objective metrics compared to existing approaches in many-to-many and many-to-one VC scenarios. 

## 1·Introduction

Voice conversion (VC) seeks to transform the speaker's timbre in speech to match that of a target speaker while preserving the original content.

The task is typically text-independent.

Traditional parallel VC research has primarily focused on feature matching methods [^Toda2007Voice], [^Helander2010Voice], [^Wu2020One-Shot], [^Erro2009Voice], [^Aihara2014Exemplar-Based], with the corpus consisting of paired utterances that have identical linguistic content.

Consequently, these methods struggle to address the challenge of converting between a wide range of different timbres and poor speech quality.

Recently, researchers have increasingly concentrated on non-parallel VC trained on multi-speaker datasets featuring randomly spoken utterances.

Non-parallel VC encompasses many-to-many and one-to-many VC tasks, designed to generate diverse timbres or a specific target timbre from various source timbres.

In this scenario, the target timbre only minimally participates or does not engage in the VC model training.

Drawing inspiration from the concept of image style transfer in computer vision, generative adversarial networks (GANs) have surfaced as a formidable tool for achieving non-parallel voice conversion (VC).

Several GAN-based VC methods have been proposed[^Kaneko2020Cyclegan-Vc3], [^Kaneko2021Maskcyclegan-Vc], [^Kameoka2018Stargan-Vc], [^Kaneko2019Stargan-Vc2], [^Li2021Starganv2-Vc], [^Chen2021Towards], which do not require explicit parallel target utterances for training.

Instead, a discriminator assesses whether a GAN-based VC model produces speech that embodies the target voice characteristics.

Consequently, GAN-based VC models learn to convert voice across trained timbres, resulting in limited timbre targets.

Within these methods, speaker encoders and style transfer functions play an essential role.

They assist the generator in understanding transformation relationships between various speaker domains, which is achieved through the integration of style transfer modules into the generators, such as Adaptive Instance Normalization (AdaIN) [^Huang2017Arbitrary] and Weight Adaptive Instance Normalization (WadaIN) [^Karras2020Analyzing]. 

However, training GAN models remains challenging due to issues with convergence and sensitivity to dataset imbalances.

In recent years, flow-based VC methods (such as SoftVC-VITS [^Van2022Comparison] and YourTTS [^Casanova2022Yourtts]), KNN-based VC approaches (like KNN-VC [^Baas2023Voice] and its derivative project RVC\footnote{https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI}), and Generative Large Language-based VC (GLL-VC), including GPT-SoVITS\footnote{https://github.com/RVC-Boss/GPT-SoVITS}, have significantly improved audio quality and training stability.

Flow-matching VC and some GLL-VC techniques utilize invertible flow architectures to transform the timbre in either the frequency domain of speech or the discrete speech unit.

The KNN-VC method substitutes the source content units with the nearest content units from the target timbre match set.

These high-quality VC methods require deep speech unit learning with a multi-speaker corpus pre-training, which can be time-consuming.

Considering that speech can be broken down into multiple components 
[^Qian2020Unsupervised] (e.g., timbre, pitch, content, and rhythm), disentanglement-based VC appears to be a promising approach.

This framework enables neural networks to develop distinct representations of each speech component using several encoders and a decoder 
[^Qian2020Unsupervised], [^Chan2022Speechsplit2.].

During training, each encoder analyzes the corresponding spectrogram to create independent representations of the speech components.

The decoder then combines these components to reconstruct the original speech.

However, current methods—such as the forced decomposition in SpeechSplit 
[^Qian2020Unsupervised], [^Chan2022Speechsplit2.], INVC 
[^Chou2019One-Shot], and the information bottleneck strategy in AutoVC 
[^Qian2019Autovc]—do not ensure perfect disentanglement or high-quality reconstruction.

*So, how should an efficient encoder-decoder framework be constructed for VC tasks?*

We propose that an effective and practical disentangled voice conversion framework, based on an encoder-decoder architecture, must be fundamentally grounded in three essential principles: (1) encoders and decoders with distinct roles, (2) an optimization objective that enhances representational discriminability, and (3) an efficient style transfer module within the decoder to merge speech components and enable precise speech reconstruction.

In recent years, the most effective architectures in the field of speech model backbones have been several enhanced transformer-based networks, such as Conformer[^Gulati2020Conformer], Paraformer[^Gao2022Paraformer], and Zipformer[^Yao2023Zipformer].

These models have shown exceptional sequence modeling capabilities and have achieved significant success in applications such as automatic speech recognition[^An2024Paraformer-V2], speaker verification[^Zhang2022Mfa-Conformer], [^Liu2022Mfa], and speech enhancement[^Kim2021SE-Conformer], [^Abdulatif2024Cmgan].

Consequently, we believe that constructing a VC framework utilizing Transformer-based networks is feasible.

Building on the previous discussion, we present **Pureformer-VC**\footnote{https://github.com/ywh-my/PureformerVC} as a comprehensive solution for a practical VC framework with three technical approaches.

For the first approach, we design a specialized content encoder that combines Conformer blocks with IN operations.

This structure enhances the model's ability to represent linguistic information through normalized distributions while filtering out speaker characteristics.

For the decoder, we utilize Zipformer blocks, which have shown exceptional performance in speech acoustic modeling tasks, to ensure high-quality synthesis output.

For the second approach, we integrate the Attention Style Transfer Mechanism (ASTM)
ootnote{wu2021styleformer} within the Zipformer blocks, effectively incorporating speaker information into generated speech.

The speaker encoder is likewise constructed using Conformer blocks but omits IN to prevent the potential filtering of speaker information.

For the final approach, we introduce a triplet loss [^Hermans2017In] alongside the reconstruction loss, allowing the model to learn and maintain distinct distances between utterances of different timbres.

In summary, this paper's main contributions are as follows.

-  We proposed a one-shot, many-to-many VC framework called Pureformer-VC.

The key modules are constructed using advanced speech encoding blocks, which assist in preserving the reconstruction quality.

-  To enhance style transfer, the shared weights in Zipformer are implemented using the ASTM in Styleformer.

-  We conducted one-shot and many-to-many voice conversion experiments on the VCTK and AISHELL-3 datasets.

The evaluation results indicate that our proposed method achieves comparable or even superior results in various voice conversion scenarios compared to existing methods.

