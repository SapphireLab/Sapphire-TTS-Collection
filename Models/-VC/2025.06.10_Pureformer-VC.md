# Pureformer-Vc: Non-Parallel Voice Conversion With Pure Stylized Transformer Blocks and Triplet Discriminative Training

<details>
<summary>基本信息</summary>

- 标题: "Pureformer-Vc: Non-Parallel Voice Conversion With Pure Stylized Transformer Blocks and Triplet Discriminative Training."
- 作者:
  - 01 Wenhan Yao
  - 02 Fen Xiao
  - 03 Xiarun Chen
  - 04 Jia Liu
  - 05 YongQiang He
  - 06 Weiping Wen
- 链接:
  - [ArXiv](https://arxiv.org/abs/2506.08348v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2506.08348v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.06.10_2506.08348v1_Pureformer-Vc__Non-Parallel_Voice_Conversion_With_Pure_Stylized_Transformer_Blocks_and_Triplet_Discriminative_Training.pdf)
  - [Publication] #TODO

</details>

## Abstract

As a foundational technology for intelligent human-computer interaction, voice conversion (VC) seeks to transform speech from any source timbre into any target timbre.
Traditional voice conversion methods based on Generative Adversarial Networks (GANs) encounter significant challenges in precisely encoding diverse speech elements and effectively synthesising these elements into natural-sounding converted speech.
To overcome these limitations, we introduce Pureformer-VC, an encoder-decoder framework that utilizes Conformer blocks to build a disentangled encoder and employs Zipformer blocks to create a style transfer decoder.
We adopt a variational decoupled training approach to isolate speech components using a Variational Autoencoder (VAE), complemented by triplet discriminative training to enhance the speaker's discriminative capabilities.
Furthermore, we incorporate the Attention Style Transfer Mechanism (ASTM) with Zipformer's shared weights to improve the style transfer performance in the decoder.
We conducted experiments on two multi-speaker datasets.
The experimental results demonstrate that the proposed model achieves comparable subjective evaluation scores while significantly enhancing objective metrics compared to existing approaches in many-to-many and many-to-one VC scenarios. 

## 1·Introduction

Voice conversion (VC) seeks to transform the speaker's timbre in speech to match that of a target speaker while preserving the original content.

The task is typically text-independent.

Traditional parallel VC research has primarily focused on feature matching methods [^Toda2007Voice], [^Helander2010Voice], [^Wu2020One-Shot], [^Erro2009Voice], [^Aihara2014Exemplar-Based], with the corpus consisting of paired utterances that have identical linguistic content.

Consequently, these methods struggle to address the challenge of converting between a wide range of different timbres and poor speech quality.

Recently, researchers have increasingly concentrated on non-parallel VC trained on multi-speaker datasets featuring randomly spoken utterances.

Non-parallel VC encompasses many-to-many and one-to-many VC tasks, designed to generate diverse timbres or a specific target timbre from various source timbres.

In this scenario, the target timbre only minimally participates or does not engage in the VC model training.

Drawing inspiration from the concept of image style transfer in computer vision, generative adversarial networks (GANs) have surfaced as a formidable tool for achieving non-parallel voice conversion (VC).

Several GAN-based VC methods have been proposed[^Kaneko2020Cyclegan-Vc3], [^Kaneko2021Maskcyclegan-Vc], [^Kameoka2018Stargan-Vc], [^Kaneko2019Stargan-Vc2], [^Li2021Starganv2-Vc], [^Chen2021Towards], which do not require explicit parallel target utterances for training.

Instead, a discriminator assesses whether a GAN-based VC model produces speech that embodies the target voice characteristics.

Consequently, GAN-based VC models learn to convert voice across trained timbres, resulting in limited timbre targets.

Within these methods, speaker encoders and style transfer functions play an essential role.

They assist the generator in understanding transformation relationships between various speaker domains, which is achieved through the integration of style transfer modules into the generators, such as Adaptive Instance Normalization (AdaIN) [^Huang2017Arbitrary] and Weight Adaptive Instance Normalization (WadaIN) [^Karras2020Analyzing]. 

However, training GAN models remains challenging due to issues with convergence and sensitivity to dataset imbalances.

In recent years, flow-based VC methods (such as SoftVC-VITS [^Van2022Comparison] and YourTTS [^Casanova2022Yourtts]), KNN-based VC approaches (like KNN-VC [^Baas2023Voice] and its derivative project RVC\footnote{https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI}), and Generative Large Language-based VC (GLL-VC), including GPT-SoVITS\footnote{https://github.com/RVC-Boss/GPT-SoVITS}, have significantly improved audio quality and training stability.

Flow-matching VC and some GLL-VC techniques utilize invertible flow architectures to transform the timbre in either the frequency domain of speech or the discrete speech unit.

The KNN-VC method substitutes the source content units with the nearest content units from the target timbre match set.

These high-quality VC methods require deep speech unit learning with a multi-speaker corpus pre-training, which can be time-consuming.

Considering that speech can be broken down into multiple components 
[^Qian2020Unsupervised] (e.g., timbre, pitch, content, and rhythm), disentanglement-based VC appears to be a promising approach.

This framework enables neural networks to develop distinct representations of each speech component using several encoders and a decoder 
[^Qian2020Unsupervised], [^Chan2022Speechsplit2.].

During training, each encoder analyzes the corresponding spectrogram to create independent representations of the speech components.

The decoder then combines these components to reconstruct the original speech.

However, current methods—such as the forced decomposition in SpeechSplit 
[^Qian2020Unsupervised], [^Chan2022Speechsplit2.], INVC 
[^Chou2019One-Shot], and the information bottleneck strategy in AutoVC 
[^Qian2019Autovc]—do not ensure perfect disentanglement or high-quality reconstruction.

*So, how should an efficient encoder-decoder framework be constructed for VC tasks?*

We propose that an effective and practical disentangled voice conversion framework, based on an encoder-decoder architecture, must be fundamentally grounded in three essential principles: (1) encoders and decoders with distinct roles, (2) an optimization objective that enhances representational discriminability, and (3) an efficient style transfer module within the decoder to merge speech components and enable precise speech reconstruction.

In recent years, the most effective architectures in the field of speech model backbones have been several enhanced transformer-based networks, such as Conformer[^Gulati2020Conformer], Paraformer[^Gao2022Paraformer], and Zipformer[^Yao2023Zipformer].

These models have shown exceptional sequence modeling capabilities and have achieved significant success in applications such as automatic speech recognition[^An2024Paraformer-V2], speaker verification[^Zhang2022Mfa-Conformer], [^Liu2022Mfa], and speech enhancement[^Kim2021SE-Conformer], [^Abdulatif2024Cmgan].

Consequently, we believe that constructing a VC framework utilizing Transformer-based networks is feasible.

Building on the previous discussion, we present **Pureformer-VC**\footnote{https://github.com/ywh-my/PureformerVC} as a comprehensive solution for a practical VC framework with three technical approaches.

For the first approach, we design a specialized content encoder that combines Conformer blocks with IN operations.

This structure enhances the model's ability to represent linguistic information through normalized distributions while filtering out speaker characteristics.

For the decoder, we utilize Zipformer blocks, which have shown exceptional performance in speech acoustic modeling tasks, to ensure high-quality synthesis output.

For the second approach, we integrate the Attention Style Transfer Mechanism (ASTM)
ootnote{wu2021styleformer} within the Zipformer blocks, effectively incorporating speaker information into generated speech.

The speaker encoder is likewise constructed using Conformer blocks but omits IN to prevent the potential filtering of speaker information.

For the final approach, we introduce a triplet loss [^Hermans2017In] alongside the reconstruction loss, allowing the model to learn and maintain distinct distances between utterances of different timbres.

In summary, this paper's main contributions are as follows.

-  We proposed a one-shot, many-to-many VC framework called Pureformer-VC.

The key modules are constructed using advanced speech encoding blocks, which assist in preserving the reconstruction quality.

-  To enhance style transfer, the shared weights in Zipformer are implemented using the ASTM in Styleformer.

-  We conducted one-shot and many-to-many voice conversion experiments on the VCTK and AISHELL-3 datasets.

The evaluation results indicate that our proposed method achieves comparable or even superior results in various voice conversion scenarios compared to existing methods.

## 2·Related Work

### Voice Conversion

Voice conversion (VC) model training can be broadly categorized into parallel and non-parallel approaches.

Early parallel methods relied on utterances with identical content but varying timbres to map features.

Techniques such as Gaussian Mixture Models (GMM-VC)[^Toda2007Voice], Directional Kernel Partial Least Squares (DKPLS)[^Helander2010Voice], Vector Quantization-based VC (VQ-VC)[^Wu2020One-Shot], frequency warping[^Erro2009Voice], and Non-Negative Matrix Factorization (NMF)[^Aihara2014Exemplar-Based] were frequently employed.

However, these approaches often yielded overly smooth outputs and demonstrated weak generative performance.

Recent technological advancements have triggered a paradigm shift toward non-parallel voice conversion, which can be systematically classified into two primary research directions:

**Domain Transfer.**

Domain transfer-based VC models, such as the series models of StarGAN-VC and CycleGAN-VC, treat each timbre as a domain and employ cyclic adversarial training to transform features across domains.

This method facilitates the generation of more realistic and diverse speech outputs.

A framework based on generative adversarial networks (GANs) has proven effective for non-parallel conversion.

**Information Disentanglement.**

Information disentanglement seeks to break down speech into distinct components through an encoder, including content, timbre, pitch, and rhythm, thereby allowing flexible recombination by a decoder.

Representative models, such as INVC, SpeechSplit, and MAIN-VC [^Li2024Main-Vc], utilize techniques like IN and mutual information estimation to achieve effective disentanglement and reconstruction.

**Generative VC.**

We consider recent VC methods based on generative theory to be generative VC approaches, which include flow-based VC [^Van2022Comparison], [^Casanova2022Yourtts], KNN-based VC [^Baas2023Voice], [^Shao2025kNN-SVC], and GLL-VC (including GPT-SoVITS, etc.).

In the voice conversion stage, flow-based models can invert the semantics of speech into textual information and then convert it back into speech with a new timbre.

The KNN-based VC replaces the source speech's deep speech unit vectors with the given target speech.

The GLL-VC models pretrain the discrete speech semantic representations using regression training and predict different speech timbres based on reference speech prompts.

This evolution from parallel to non-parallel methods emphasizes the shift toward models with enhanced flexibility, robustness, and generative capabilities in VC tasks.

Formally, most non-parallel VC models can be represented as $y=G(x_{s},x_{y})$, where $x_{s}$ represents the speech providing the content and $x_{y}$ represents the speech supplying the speaker's voice characteristics.

### Style Transfer Learning in VC

Style transfer learning teaches VC models to merge various speech representations.

Accordingly, the style transfer function accepts both source speaker-independent and target speaker-dependent representations.

Chou et al. [^Chou2019One-Shot] were the first to discover that IN can filter out speaker information while retaining the source content from original utterances in INVC.

Subsequently, the IN function found widespread application in GANs-based VC. [^Kaneko2019Stargan-Vc2], [^Li2021Starganv2-Vc].

Furthermore, the WadaIN method implements affine operations on the convolutional kernel in Convolutional Neural Networks (CNNs), modifying the style of source data in WadaIN-VC [^Chen2021Towards] by convolving the source data.

However, these models depend on CNN architectures and exhibit limited reconstruction capabilities.

To leverage the self-attention mechanism in Transformers, the Attention-AdaIN-VC [^Ke2022New] incorporated the styleformer block within the CNN blocks, achieving improved VC performance.

In styleformer [^Wu2021Styleformer], self-attention weights are stylized using speaker representations, successfully training an image style transfer model.

In the styleformer's blocks, ASTM is commonly used to integrate individual embedding with self-attention layers.

We typically incorporate ASTM into our decoder as a style transfer module. 

![](figs/vaeformer.pdf)

<a id='fig:1'>The architecture of Pureformer-VC.</a>

## 3·Methodology

### Overall Architecture

The overall architecture of Pureformer-VC is depicted in Figure [fig:1](#fig:1)(a).

Pureformer-VC consists of a content encoder, a decoder, a speaker encoder, and a vocoder.

We utilized a pre-trained Hifi-GAN generator [^Kong2020Hifi-Gan] as the vocoder, which remains frozen during the training stage. 

We set the input feature mel-spectrogram as $x \in X^{[L, D]}$, where L denotes the frame number and D represents the number of mel filters.

The content encoder $E_{c}$ extracts the posterior variances of the content representation $r_{m}, r_{s}=E_{c}(x)$.

The speaker encoder $E_{s}$ generates speaker embeddings as the timbre representation $s = E_{s}(x)$ from mel-spectrograms.

The decoder $E_{d}$ takes the reparameterization variance and outputs the converted spectrogram $x_{dec}$, incorporating the style embedding for transfer.

It is important to note that *e* is a random variable that follows the standard normal distribution $N(0,1)$. 

### Content Encoder with VAE Training

The content encoder parameterizes and approximates the variational distribution of $q_{\phi}(z|x)$.

Each Conformer block was constructed **with an IN function** and an AveragePooling1D layer following the convolution module to reduce the time dimension by half.

There are four continuous blocks as shown in Figure [fig:1](#fig:1) (a); thus, the output spectrogram's frame length is decreased by 16.

Finally, the content encoder outputs the reparameterization of the content representation by two convolution layers:

$$
\begin{aligned}

r_m &= Mean\_Conv(h_{enc}) \\
r_{s} &= Std\_Conv(h_{enc}) \\
r_{c} &= r_{m} + *e**r_{s}

\end{aligned}
$$

The $Mean\_Conv$ and $ Std\_Conv$ denote the two convolution layers after the output $h_{enc}$ of Conformer blocks.

Thus, the variable $r_{c}$ follows a normal distribution with mean $ r_{m}$ and variance $r_{s}$.

The reparameterization operation ensures that the speech latent variables input into the decoder remain typically distributed while also preserving the gradient propagation in the model.

It turns out to optimize the Evidence Lower Bound (ELBO)[^Zhu2020Batch] of $log (p(x))$:

$$

L_{elbo} = E[logP_{\theta}(x|z)] - KL(q_{\phi}(z|x) || p(z))

$$

where $\phi$ denotes the encoder network and $\theta$ represents the decoder.

The first term above is the
reconstruction loss, while the second is the KullbackLeibler divergence between the approximate posterior and the prior.

Thus, the VAE training loss can be summarized as:

$$
\begin{aligned}

L_{vae}(x,x_{dec}) = &E\left[ |x-x_{dec}| \right] + \\
0.5 \cdot &E[r_{c} + r_{m}^2 - log(r_{m}^2) - 1 ] 

\end{aligned}
$$

The $r_{c}$ is derived from the input mel-spectrogram $x$.

### ASTM in Decoder

ASTM aids the decoder in learning the timbral characteristics of the target speech.

We built the decoder using 4 Zipformer blocks with ASTM, as illustrated in Figure [fig:1](#fig:1)(b).

The decoder merges the content and timbre representations of the speech. 

To generate speech with diverse voice styles, we apply the ASTM to the weights in the self-attention mechanism of Zipformer blocks.

During the model initialization phase, the ASTM initially sets some attention weights $w_{q}, w_{k}, w_{v}, w_{u}$.

These weights are infused with the style characteristics of the split speaker embedding vector $s_{1}, s_{2} = split(E_{s}(x))$ as follows:

$$
\begin{aligned}

w_{q} = w_{q} \cdot s_{1} + s_{2}, w_{k} = w_{k} \cdot s_{1} +s_{2}\\
w_{v} = w_{v} \cdot s_{1} + s_{2}, w_{u} = w_{u} \cdot s_{1} +s_{2}

\end{aligned}
$$

Then, we apply weight normalization (WN) 
[^Salimans2016Weight] to the weights to achieve improved convergence performance.

WN takes the weight $w$ and normalizes it at the output dimension $i,j$ as follows:

$$
\begin{aligned}

w_{ij}' = w_{ij} \cdot \frac{1}{\sqrt{w_{ij}^2}}

\end{aligned}
$$

Using the WN operation, we scale the output of each weight $w \in \{ w_{q},w_{k},w_{v},w_{u} \}$ back to a unit standard deviation.

The WN aids the model in accelerating training convergence following the attention calculation with stylized weights.

Consequently, in the self-attention layers, the stylized attention is as follows:

$$
\begin{aligned}

x' &= norm(s_{1} \cdot x + s_{2}) \\
out &= \frac{wn(w_{q})x' \cdot (wn(w_{k})x')^{t}}{\sqrt{d}} \cdot wn(w_{k})x' + wn(w_{u})x'

\end{aligned}
$$

The $norm$ represents a non-parameterized layer normalization function.

Additionally, $d$ indicates the output dimension for each weight, and $wn$ refers to the WN.

The stylized attention output is connected residually.

### Speaker Encoder with AAM-Softmax Loss

The speaker encoder is designed to extract timbre representations from mel-spectrograms, allowing the model to capture speaker-specific features that are critical for VC.

For its backbone, we use a structure made up of several Conformer blocks from MFA-Conformer[^Zhang2022Mfa-Conformer].

Importantly, these Conformer blocks are configured **without the IN functions** to ensure the preservation of speaker-related information.

To further enhance the quality of the extracted embeddings, we incorporate the Additive Angular Margin Softmax (AAM-softmax) layer.

This parameterized loss function optimizes the learning of compact and well-separated clusters in the embedding space for different speakers.

By introducing a fixed angular margin between classes, the AAM-softmax layer encourages the encoder to produce discriminative and robust embeddings, thus improving the overall performance of speaker representation in the VC process.

### Triplet loss and Data Sample Strategy

Considering the previous disentanglement-based VC models, both the source and target mel-spectrograms were identical during the training stage but differed during the inference stage.

This discrepancy between training and inference diminishes the model's generalization capability and results in poor speech quality when the target spectrograms lack speaker information.

To address this issue, we utilize the triplet loss, as shown in Figure [fig:2](#fig:2), which is an unsupervised learning technique featuring discriminative training that allows the speaker encoder to discern the differences in timbre among various voices.

The triplet loss training necessitates a unique data sampling strategy. 

During the training stage, we sample three utterance segments of equal length from the dataset: an anchor sample $x_{anc}$, a positive sample $x_{pos}$, and a negative sample $x_{neg}$.

As illustrated in Figure [fig:2](#fig:2), the anchor sample and the positive sample share the same timbre, while the negative sample is from a different speaker than the anchor.

Therefore, let $nm$ represent the L2 normalization.

We can use the speaker encoder outputs of the three samples to calculate a triplet loss:

$$
\begin{aligned}

e_{anc},e_{pos},e_{neg} = E_{s}(x_{anc}),E_{s}(x_{pos}),E_{s}(x_{neg})

\end{aligned}
$$

$$
\begin{aligned}

L_{tri} = E[nm(e_{anc}) * nm(e_{pos})^t] - \\\notag   E[nm(e_{anc}) * nm(e_{neg})^t]  + \delta

\end{aligned}
$$

The $\delta$ is a hyper-parameter to control the speaker similarity.

Denoting the VC model as $G_{vc}$, the total model's output can be described as follows:

$$
\begin{aligned}

y_{1} = G_{vc}(x_{anc},x_{neg})\\
y_{2} = G_{vc}(x_{anc},x_{pos})

\end{aligned}
$$

### Training Objective

The training objective of the Pureformer-VC model includes VAE loss, AAM-softmax loss, and triplet loss.

The total VAE loss is based on two outputs $y_{1},y_{2}$ as shown in Figure [fig:2](#fig:2) and can be denoted as:

$$
\begin{aligned}

L_{t-vae} = \lambda_{1}(L_{vae}(x_{anc},y_{1}) + \lambda_{2}L_{vae}(x_{anc},y_{2}))

\end{aligned}
$$

The AAM-softmax loss can be computed using the three true labels of samples $C$ and the predictions of the speaker encoder:

$$

L_{t-aam} = \sum_{c_{i}\in C}

L_{aam}(c_{i},x_{i})

$$

Finally, the triplet loss helps the speaker encoder to distinguish the embeddings.

The total training objective is as follows:

$$

L_{total} = L_{t-vae} + \lambda_{3}L_{t-aam} + \lambda_{4}L_{tri} 

$$

![](figs/tripletloss.pdf)

<a id='fig:2'>The illustration of training objective.</a>

### Vocoder

The vocoder has the same structure as the HiFi-GAN generator.

In our study, it was pre-trained on the same dataset used for the VC training.

## 4·Experiments and Results

### Experimental Setup

**Datasets and Feature Setup.**

To evaluate the effectiveness of Pureformer-VC, we conducted a comparative experiment and an ablation study on VCTK[^Veaux2020VCTK] and AISHELL-3[^Shi2020Aishell-3] datasets.

The VCTK corpus includes 109 English speakers, each reading about 400 utterances.

The AISHELL-3 corpus contains roughly 85 hours of emotion-neutral recordings spoken by 218 native Chinese Mandarin speakers and 88035 utterances. 

The mel-spectrogram extraction process must align with the pre-trained vocoder and adhere to the algorithm outlined in the HiFi-GAN framework[^Kong2020Hifi-Gan].

The signal hyperparameters are defined as follows: 80 Mel frequency filters, 1024 FFT bins, a window length of 1024, a hop length of 256, a sampling rate of 22,050 Hz, and a maximum frequency of 8 kHz.

The final input features are log-mel spectrograms, obtained by applying the logarithm to the extracted mel-spectrograms.

Considering batch sampling during the training stage, we randomly selected an utterance from one speaker.

We then sampled two utterances from another speaker to create a training sample: $\{ x_{anc},x_{pos},x_{neg} \}$.

Five speakers are randomly chosen as unseen speakers for each corpus. 

<a id=table:1>Comparison of baseline and proposed methods for many-to-many and one-shot VC on the VCTK dataset (with a $95\%$ confidence interval).</a>

<a id=table:2>Comparison of baseline and proposed methods for many-to-many and one-shot VC on the AISHELL-3 dataset (with a $95\%$ confidence interval).</a>

**Training Setup.**

In the training stage, the batch size is 16.

The learning rate is constant at $2 \times 10^{-4}$.

The Pureformer-VC is trained by Adam optimizer[^Kingma2014Adam] with $ \beta_{1} = 0.9, \beta_{2} = 0.99, \epsilon = 1 \times 10^{-6}$.

The $\lambda_{1}$ is set to 10 and $\lambda_{2}$ ranges from $1 \times 10-4 $ to 1.

Both the $\lambda_{3},\lambda_{4}$ are set to 1.

The $\delta$ is 0.3. 

**Baseline Setup.**

We compared Pureformer-VC with recent VC frameworks, such as AdaIN-VC[^Chou2019One-Shot], AutoVC[^Qian2019Autovc], VQMIVC[^Wang2021Vqmivc], MAIN-VC[^Li2024Main-Vc], RVC$^{1}$, and GPT-SoVITS$^{2}$.

The experiments are conducted in a many-to-many, one-shot (any-to-any) setup.

We further evaluate the performance of the proposed model in cross-lingual VC, where the source utterance's language differs from that of the target language.

### Metrics and Evaluation

We assess the naturalness and intelligibility of the generated speech using subjective metrics, such as the Mean Opinion Score (MOS).

Additionally, we utilize objective metrics to evaluate timbre similarity with the target speech, including the Voice Similarity Score (VSS) and Mel-Cepstral Distortion (MCD).

Higher scores indicate greater effectiveness of the voice conversion (VC) system. 

**Mean Opinion Score (MOS)**.

The Mean Opinion Score (MOS) is a widely used metric for assessing the subjective quality of speech or audio.

It is based on ratings from human listeners, who are asked to evaluate the quality of speech samples using a scale that typically ranges from 1 to 5.

A higher MOS signifies better reconstruction quality.

**Voice similarity score (VSS)**.

The Voice Similarity Score (VSS) is an objective metric that quantifies the degree of resemblance between generated speech and authentic or target speech in terms of timbre, tone, and voice quality.

VSS is calculated based on embedding similarity derived from a pre-trained speaker verification model (Resemblyzer)
[^Desplanques2020Ecapa-TDNN].

Higher scores represent greater similarity, indicating improved voice conversion (VC) performance.  

**Mel-cepstral distortion (MCD)**.

The MCD is an objective quantitative measure that evaluates the Mel-cepstral divergence between the source and generated utterances.

The lower the MCD, the better the reconstruction effect.

During the testing phase, both many-to-many and one-shot (any-to-any) VC are conducted using non-parallel data.

For evaluation, 10 source/target speech pairs are fed into each VC model under two scenarios.

After this, 5 participants are invited to rate the speech samples.

### Experimental Results

Table [table:1](#table:1) and [table:2](#table:2) present a comparative analysis of the Pureformer-VC against baseline methods in both many-to-many and one-shot VC settings across both datasets. 

**MCD and MOS metrics.**

The MOS and MCD can evaluate the effectiveness of speech reconstruction, collectively referred to as reconstruction metrics.

Since the original sampling rate of the AISHELL-3 dataset is higher than that of the VCTK dataset, the quality of speech reconstruction is superior, resulting in better reconstruction metrics for the AISHELL-3 dataset.

The Pureformer-VC outperforms the encoder-decoder baseline models in both many-to-many and one-shot experimental configurations with respect to reconstruction metrics.

However, when compared to state-of-the-art methods like RVC and GPT-SoVITS, the Pureformer-VC still exhibits a slight performance gap.

These results showcase the effectiveness of the pure transformer architecture.

**VSS metric.**

The VSS evaluation measures the similarity between the generated target timbre and the actual target timbre using a resemblyzer.

Compared to the four classic encoder-decoder-based VC baselines, the Pureformer-VC surpasses them in the VSS metric.

However, a slight gap remains in the VSS performance between PVC and the state-of-the-art models RVC and GPT-SoVITS.

As shown in the results, we further investigated the impact of removing either the AAMSoftmax loss or the triplet loss from the training objectives to assess the model's ability to represent timbre in the embedded vectors during training.

It is evident that without these two losses, the proposed model's VSS stays relatively consistent with that of the baseline model.

Therefore, incorporating these losses helps enhance the model's VC expressiveness. 

![](figs/tsne.pdf)

<a id='fig:4'>The visualization of speaker representations extracted from 6 unseen
speakers’ utterances.</a>

### Ablation Study

We conduct ablation experiments to validate the effects of triplet loss and AAM-softmax loss on disentanglement.

We set up the following models: (a) the Pureformer-VC model, (b) the Pureformer-VC model without triplet loss (w/o triplet), and (c) the Pureformer-VC model without AAM-softmax loss (w/o AAM-softmax).

We used the resemblyzer to detect synthetic speech and evaluate conversion quality.

It assigns detection scores to fake (i.e., the VC model's experimental outputs) and authentic utterances from the target speaker after learning the target's characteristics from ten additional genuine utterances.

A higher score signifies a closer resemblance in timbre and superior speech quality.

The results are detailed in Table [table:3](#table:3).

We found that the decision scores of our method are higher than those of the best baseline model, MAIN-VC.

Furthermore, the experiments demonstrate that both triplet loss and AAM-Softmax contribute to improving the accuracy of timbre generation.

For a visual evaluation of each model's disentanglement capability, the t-SNE scatter plots of the speaker representations are shown in Figure [fig:4](#fig:4).

The AAM-softmax loss has a significant impact on the clustering of speaker embedding vectors, while the triplet loss helps create more distinct boundaries between categories.

<a id=table:3>Fake Detection score comparison for ablation study ablation</a>

### Cross-lingual VC

Pureformer VC is also capable of performing cross-lingual voice conversion (CVC), where the source and target utterances are in different languages.

We trained Pureformer-VC on a mixture of the AISHELL-3 and VCTK datasets.

Due to varying pronunciation habits, the reconstruction metrics ($MOS=3.15, MCD=5.12$) and VSS ($2.95$) scores for cross-lingual voice conversion are lower than those for monolingual VC experiments.

The bilingual timbre experiments suggest that additional latent variables may be necessary to decouple the languages using special encoders for improved conversion performance.

