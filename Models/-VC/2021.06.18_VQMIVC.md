# VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-Shot Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-Shot Voice Conversion."
- 作者:
  - 01 Disong Wang
  - 02 Liqun Deng
  - 03 Yu Ting Yeung
  - 04 Xiao Chen
  - 05 Xunying Liu
  - 06 Helen Meng
- 链接:
  - [ArXiv](https://arxiv.org/abs/2106.10132v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2106.10132v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2021.06.18_2106.10132v1_VQMIVC__Vector_Quantization_and_Mutual_Information-Based_Unsupervised_Speech_Representation_Disentanglement_for_One-Shot_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

One-shot voice conversion (VC), which performs conversion across arbitrary speakers with only a single target-speaker utterance for reference, can be effectively achieved by speech representation disentanglement.
Existing work generally ignores the correlation between different speech representations during training, which causes leakage of content information into the speaker representation and thus degrades VC performance.
To alleviate this issue, we employ vector quantization (VQ) for content encoding and introduce mutual information (MI) as the correlation metric during training, to achieve proper disentanglement of content, speaker and pitch representations, by reducing their inter-dependencies in an unsupervised manner.
Experimental results reflect the superiority of the proposed method in learning effective disentangled speech representations for retaining source linguistic content and intonation variations, while capturing target speaker characteristics.
In doing so, the proposed approach achieves higher speech naturalness and speaker similarity than current state-of-the-art one-shot VC systems.
Our code, pre-trained models and demo are available at \color{blue}\url{https://github.com/Wendison/VQMIVC}.

## 1·Introduction

Voice conversion (VC) is a technique used to modify para-linguistic factors of an utterance from a source speaker to sound like a target speaker.

Para-linguistic factors include speaker identity [^Mohammadi2017Overview], prosody [^Rentzos2003Transformation] and accent [^Oyamada2017Non-Native], etc.

In this paper, we focus on the conversion of speaker identity across arbitrary speakers under a one-shot scenario [^Liu2018Voice], [^Lu2019One-Shot], i.e., given only one target speaker's utterance for reference. 

Previous work that use methods based on speech representation disentanglement (SRD) [^Qian2019Autovc], [^Chou2019One-Shot], [^Wu2020Vqvc+] attempted to address one-shot VC by decomposing the speech into speaker and content representations, and then the speaker identity can be converted by changing the source speaker's representation to that of the target speaker.

However, it is difficult to measure the degree of SRD.

Besides, previous approaches generally do not impose correlation constraints between speaker and content representations during training, which results in leakage of content information into the speaker representation, leading to VC performance degradation.

To alleviate these issues, this paper proposes the vector quantization and mutual information-based VC (VQMIVC) approach, where mutual information (MI) measures the dependencies between different representations and can be effectively integrated into the training process to achieve SRD in an *unsupervised manner*.

Specifically, we first decompose an utterance into three factors: content, speaker and pitch, and then propose a VC system consisting of four components: (1) A content encoder using vector quantization with contrastive predictive coding (VQCPC) [^Niekerk2020Vector-Quantized], [^Baevski2019Vq-Wav2vec] to extract frame-level content representations from acoustic features; (2) A speaker encoder that takes in acoustic features to generate a single fixed-dimensional vector as the speaker representation; (3) A pitch extractor that is used to compute normalized fundamental frequency ($*F*_0$) at the utterance level as the pitch representation; and (4) A decoder that maps content, speaker and pitch representations to acoustic features.

During training, the VC system is optimized by minimizing VQCPC, reconstruction and MI losses.

VQCPC aims to explore local structures of speech, and MI reduces the inter-dependencies of different speech representations.

During inference, one-shot VC is achieved by only replacing the source speaker representation with the target speaker representation derived from a single target utterance.

The main contribution of this work lies in applying the combination of VQCPC and MI to achieve SRD, without any requirements of supervision information such as text transcriptions or speaker labels.

Extensive experiments have been conducted to thoroughly analyze the importance of MI, where information leakage issues can be significantly alleviated for enhanced SRD.

## 2·Related Work

VC performance is critically dependent on the availability of the target speaker’s voice data for training [^Toda2007Voice], [^Helander2010Voice], [^Erro2009Inca], [^Sun2016Phonetic], [^Hsu2017Voice], [^Kaneko2017Parallel-Data-Free], [^Kameoka2018Stargan-Vc].

Hence, the challenge of one-shot VC is in performing conversion across arbitrary speakers that may be unseen during training, and with only one single target-speaker utterance for reference.

Previous approaches for one-shot VC are based on SRD, which aims to separate speaker information from spoken content as far as possible.

Related work include: tunable information-constraining bottleneck [^Qian2019Autovc], [^Qian2020F0-Consistent], [^Qian2020Unsupervised], instance normalization techniques [^Chou2019One-Shot], [^Chen2021Again-Vc] and vector quantization (VQ) [^Wu2020Vqvc+], [^Chorowski2019Unsupervised].

We adopt VQCPC [^Niekerk2020Vector-Quantized], [^Baevski2019Vq-Wav2vec], which is an improved version of VQ, to extract accurate content representations.

Without explicit constraints between different speech representations, information leakage tends to occur, which degrades VC performance.

We draw inspirations from information theory [^Gierlichs2008Mutual] in using MI as a regularizer to constrain the dependency between variables.

As MI computation is challenging for variables with unknown distribution, various methods have been explored to estimate MI lower bound [^Nguyen2010Estimating], [^Gutmann2010Noise-Contrastive], [^Belghazi2018Mutual] in SRD-based speech tasks [^Ravanelli2019Learning], [^Kwon2020Intra-Class], [^Hu2020Unsupervised] .

To guarantee the reduction of MI values, we propose to use variational contrastive log-ratio upper bound (vCLUB) [^Cheng2020Club].

While a recent effort [^Yuan2021Improving] employs MI for VC by using speaker labels as the supervision for learning speaker representations, our proposed approach differs from [^Yuan2021Improving] in terms of the combination of VQCPC and MI for fully unsupervised training, and the incorporation of pitch representations to maintain source intonation variations.

![](architecture.png)

<a id="arc">Diagram of the proposed VQMIVC system.</a>

## 3·Proposed Approach

This section first describes the system architecture of the VQMIVC approach, then elaborates on the integration of MI minimization into the training process, and finally shows how one-shot VC is achieved. 

### Architecture of the VQMIVC system

As shown in Figure [arc](#arc), the proposed VQMIVC system includes four modules: content encoder, speaker encoder, pitch extractor and decoder.

The first three modules respectively extract content, speaker and pitch representations from the input voice; and the fourth module, the decoder, maps these representations back to acoustic features.

Assuming that there are *K* utterances, we use mel-spectrograms as acoustic features and randomly select *T* frames from each utterance for training.

The $*k*^{th}$ mel-spectrogram is denoted as $**X**_k=\{**x**_{k,1},**x**_{k,2},...,**x**_{k,T}\}$.

\vspace{0.1em}

**Content encoder** ${\theta}_c$: The content encoder strives to extract linguistic content information from $**X**_k$ by using VQCPC as shown in Figure [ce](#ce), which contains two networks *h*-net: $**X**_k$ $\rightarrow$ $**Z**_k$ and *g*-net: $\hat{**Z**}_k$ $\rightarrow$ $**R**_k$, and VQ operation *q*: $**Z**_k$ $\rightarrow$ $\hat{**Z**}_k$. *h*-net takes in $**X**_k$ to derive a sequence of dense features $**Z**_k=\{**z**_{k,1},**z**_{k,2},...,**z**_{k,T/2}\}$, where the length is reduced from *T* to *T/2*.

Then the quantizer *q* discretizes $**Z**_k$ with a trainable codebook *B* into $\hat{**Z**}_k$=\{$\hat{**z**}_{k,1}$,$\hat{**z**}_{k,2}$,...,$\hat{**z**}_{k,T/2}$\}, where $\hat{**z**}_{k,t}$ $\in *B*$ is the vector closest to $**z**_{k,t}$.

VQ imposes an information bottleneck to remove non-essential details in $**Z**_k$, making $\hat{**Z**}_k$ to be related with underlying linguistic information.

Then the content encoder ${\theta}_c$ is trained by minimizing the VQ loss [^Niekerk2020Vector-Quantized]:

\begin{footnotesize}

$$

{{L}_{VQ}}=\frac{2}{KT}\sum\limits_{k=1}^{K}{\sum\limits_{t=1}^{T/2} \left\| {{**z**}_{k,t}}-sg({{{\hat{**z**}}}_{k,t}}) \right\|_{2}^{2}} \label{vq-loss}

$$

\end{footnotesize}
where *sg*(·) denotes the stop-gradient operator.

To further encourage $\hat{**Z**}_k$ to capture local structures, contrastive predictive coding (CPC) is employed by adding RNN based *g*-net taking in $\hat{**Z**}_k$ to obtain aggregation $**R**_k=\{**r**_{k,1},**r**_{k,2},...,**r**_{k,T/2}\}$.

Given $**r**_{k,t}$, the model is trained to distinguish a positive sample $\hat{**z**}_{k,t+m}$ that is *m* steps in the future from negative samples drawn from the set ${\Omega}_{k,t,m}$ by minimizing the InfoNCE loss [^Oord2018Representation]:

\begin{footnotesize}

$$

{{L}_{CPC}}=-\frac{1}{KT'M}\!\sum\limits_{k=1}^{K}{\sum\limits_{t=1}^{T'}{\sum\limits_{m=1}^{M}{\!\log\! \left[\! \frac{\exp (\hat{**z**}_{k,t+m}^{T}{{\mathbf{W}}_{m}}{{**r**}_{k,t}})}{\sum\nolimits_{\tilde{**z**}\in {{\Omega }_{k,t,m}}}{\exp ({{{\tilde{**z**}}}^{T}}{{\mathbf{W}}_{m}}{{**r**}_{k,t}})}} \!\right]}}} \label{cpc-loss} 

$$

\end{footnotesize}
where ${T'}=T/2-M$, $**W**_m$ (*m*=1,2,...,*M*) is trainable projection matrix.

By predicting future samples with probabilistic contrastive loss ([cpc-loss](#cpc-loss)), local features (e.g., phonemes) spanning many time steps are encoded into $\hat{**Z**}_k$=*f*($**X**_k$;${\theta}_c$), which is the content representation used to accurately reconstruct the linguistic content.

During training, the negative samples set ${\Omega}_{k,t,m}$ is formed by randomly selecting samples from the current utterance.

![](content-encoder.png)

<a id="ce">Details of the VQCPC based content encoder.</a>

**Speaker encoder** ${\theta}_s$: The speaker encoder takes in $**X**_k$ to generate a vector $**s**_k$=*f*($**X**_k$;${\theta}_s$), which is used as the speaker representation. $**s**_k$ captures global speech characteristics to control the speaker identity of the generated speech. 

**Pitch extractor**: The pitch representation is expected to contain intonation variations but exclude content and speaker information, so we extract $*F*_0$ from the waveform and perform z-normalization for each utterance independently.

In our experiments, we adopt log-normalized $*F*_0$ (log-$*F*_0$) as $**p**_k$=($*p*_{k,1}$, $*p*_{k,2}$, ..., $*p*_{k,T}$), which is speaker-independent so that speaker encoder is forced to provide the speaker information, e.g., vocal ranges.

**Decoder** ${\theta}_d$: The decoder is used to map the content, speaker and pitch representations to mel-spectrograms.

Linear interpolation based upsampling ($\times$2) and repetition ($\times$*T*) are performed on $\hat{**Z**}_k$ and $**s**_k$ respectively to align with $**p**_k$ as inputs of decoder to generate mel-spectrograms $\hat{**X**}_k$=\{$\hat{**x**}_{k,1}$,$\hat{**x**}_{k,2}$,...,$\hat{**x**}_{k,T}$\}.

Decoder is jointly trained with content and speaker encoders by minimizing a reconstruction loss:

\begin{footnotesize}

$$

{{L}_{REC}}=\frac{1}{KT}\sum\limits_{k=1}^{K}{\sum\limits_{t=1}^{T}{\left[{{\left\| {{{\hat{**x**}}}_{t}}-{{**x**}_{t}} \right\|}_{1}}+{{\left\| {{{\hat{**x**}}}_{t}}-{{**x**}_{t}} \right\|}_{2}}\right]}} \label{rec-loss}
\vspace{-0.5em}

$$

\end{footnotesize}

\vspace{-1em}

### MI minimization integrated into VQMIVC training

Given the random variables **u** and **v**, the MI is Kullback-Leibler (KL) divergence between their joint and marginal distributions as $\mathbf{I}(\mathbf{u},\mathbf{v})={{D}_{KL}}(P(\mathbf{u},\mathbf{v});P(\mathbf{u})P(\mathbf{v}))$.

We adopt vCLUB [^Cheng2020Club] to compute the upper bound of MI as:

\begin{footnotesize}

$$

{**I**}\!\left( **u**,**v** \right)\!=\!{{E}_{P(**u**,**v**)}}\!\left[\! \log {{Q}_{{{\theta }_{**u**,**v**}}}}\!\left( **u**|**v** \right)\! \right]\!-\!{{E}_{P(**u**)}}{{E}_{P(**v**)}}\!\left[\! \log {{Q}_{{{\theta }_{**u**,**v**}}}}\!\left( **u**|**v** \right)\! \right] \label{club}

$$

\end{footnotesize}
where **u**,**v** $\in$ \{$\hat{**Z**}$, **s**, **p**\}, $\hat{**Z**}$, **s** and **p** are content, speaker and pitch representations respectively, ${{Q}_{{{\theta }_{**u**,**v**}}}}(**u**|**v**)$ is the variational approximation of ground-truth posterior of **u** given **v** and can be parameterized by a network ${{\theta }_{**u**,**v**}}$.

The unbiased estimation for vCLUB between different speech representations is given by:

\begin{footnotesize}

$$

\!{{\hat{**I**}}({\hat{**Z**},**s**})}\!=\!\frac{2}{{{K}^{2}}T}\sum\limits_{k=1}^{K}{\sum\limits_{l=1}^{K}{\sum\limits_{t=1}^{T/2}{\!\left[\! \log\!{{Q}_{{{\theta }_{\hat{**Z**},**s**}}}}\!\left( {{{\hat{**z**}}}_{k,t}}|{{**s**}_{k}}\! \right)\!-\!\log\! {{Q}_{{{\theta }_{\hat{**Z**},**s**}}}}\!\left( {{{\hat{**z**}}}_{l,t}}|{{**s**}_{k}}\! \right)\! \right]}}} \label{mi-1}

$$

\end{footnotesize}

\begin{footnotesize}

$$

\!{{\hat{**I**}}({{**p**},**s**})}\!=\!\frac{1}{{{K}^{2}}T}\sum\limits_{k=1}^{K}{\sum\limits_{l=1}^{K}{\sum\limits_{t=1}^{T}{\left[\! \log\!{{Q}_{{{\theta }_{{**p**},**s**}}}}\!\left( {{{{**p**}}}_{k,t}}|{{**s**}_{k}}\! \right)\!-\!\log\! {{Q}_{{{\theta }_{{**p**},**s**}}}}\!\left( {{{{**p**}}}_{l,t}}|{{**s**}_{k}}\! \right)\! \right]}}} \label{mi-2}

$$

\end{footnotesize}

\begin{footnotesize}

$$

\!\!\hat{**I**}(\!\hat{**Z**},\!**p**\!)\!=\!\frac{2}{{{K}^{2}}T}\!\sum\limits_{k=1}^{K}{\!\sum\limits_{l=1}^{K}{\!\sum\limits_{t=1}^{T/2}{\!\left[\! \log\! {{Q}_{{{\theta }_{\hat{**Z**},\!**p**}}}}\!\!\left( {{{\hat{**z**}}}_{k,t}}\!|{{{\hat{**p**}}}_{k,t}} \right)\!-\!\log\! {{Q}_{{{\theta }_{\hat{**Z**},\!**p**}}}}\!\!\left( {{{\hat{**z**}}}_{l,t}}\!|{{{\hat{**p**}}}_{k,t}} \!\right)\! \right]}}} \label{mi-3}

$$

\end{footnotesize}
where $\hat{**p**}_{k,t}=(**p**_{k,{2t-1}}+**p**_{k,{2t}})/2$.

With a good variational approximation, ([club](#club)) provides a reliable MI upper bound.

Therefore, we can decrease the correlation among different speech representations by minimizing ([mi-1](#mi-1))-([mi-3](#mi-3)), and the total MI loss is:
\vspace{-0.2em}

\begin{footnotesize}

$$

% {{L}_{MI}}=\hat{**I**}\left( \hat{**Z**},\mathbf{s} \right)\text{+}\hat{**I**}\left( \hat{**Z**},\mathbf{p} \right)\text{+}\hat{**I**}\left( \mathbf{p},\mathbf{s} \right) \label{mi-loss}
{{L}_{MI}}=\hat{**I**}(\hat{**Z**},\mathbf{s})\text{+}\hat{**I**}(\hat{**Z**},\mathbf{p})\text{+}\hat{**I**}(\mathbf{p},\mathbf{s}) \label{mi-loss}

$$

\end{footnotesize}
During training, variational approximation networks and VC network are optimized alternatively.

The variational approximation networks are trained to maximize the log-likelihood:
\vspace{-0.2em}

\begin{footnotesize}

$$

\begin{matrix}
{{L}_{**u**,**v**}}=\log {{Q}_{{{\theta }_{**u**,**v**}}}}\left( **u**|**v** \right), & \mathbf{u},\mathbf{v}\in \{\hat{**Z**},\mathbf{s},\mathbf{p}\}  \\
\end{matrix}
\label{ll-loss}

$$

\end{footnotesize}
while the VC network is trained to minimize VC loss:
\vspace{-0.1em}

\begin{footnotesize}

$$

{{L}_{VC}}={{L}_{VQ}}+{{L}_{CPC}}+{{L}_{REC}}+{\lambda}_{MI}{{L}_{MI}}
\label{vc-loss}

$$

\end{footnotesize}
where ${\lambda}_{MI}$ is a constant weight to control how MI loss enhances the disentanglement.

The final training process is summarized in Algorithm 1.

We note that no text transcriptions or speaker labels are used during training, so the proposed approach achieves disentanglement in a fully unsupervised way.

<a id="alo">Pseudocode for the proposed VQMIVC training</a>

### One-shot VC

During conversion, the content and pitch representations are first extracted from source speaker's utterance $**X**_*src*$ as $\hat{**Z**}_{src}=*f*({**X**_*src*};{{\theta }_{c}})$ and $**p**_{src}$ respectively, the speaker representation is extracted from only one target speaker's utterance $**X**_*tgt*$ as $**s**_{tgt}=*f*({**X**_{tgt}};{{\theta }_{s}})$, then the decoder generates the converted mel-spectrograms as *f*$(\hat{**Z**}_{src},**s**_{tgt},**p**_{src};{\theta}_d)$.

## 4·Experiments

### Experimental setup

All experiments are conducted on the VCTK corpus [^Veaux2016Superseded-CSTR] with 110 English speakers, which are randomly split into 90 and 20 speakers as training and testing sets respectively.

The testing speakers are treated as unseen speakers that are used to perform one-shot VC.

For acoustic features extraction, all audio recordings are downsampled to 16kHz, 80-dim mel-spectrograms and $*F*_0$ are both calculated with 25ms Hanning window, 10ms frame shift and 400-point fast Fourier transform.

The proposed VC network consists of the content encoder, speaker encoder and decoder.

The content encoder contains a *h*-net, a quantizer *q* and a *g*-net.

The *h*-net is composed of a convolutional layer with stride of 2, four blocks with layer normalization, 512-dim linear layer and ReLU activation function for each block.

The quantizer contains a codebook with 512 64-dim learnable vectors.

The *g*-net is a 256-dim uni-directional RNN layer.

For CPC, the future prediction step *M* is 6 and the number of negative samples $|{\Omega}_{k,t,m}|$ is 10.

The speaker encoder follows [^Chou2019One-Shot], which contains 8 ConvBank layers to encode the long-term information, 12 convolutional layers with 1 average-pooling layer, and 4 linear layers to derive the 256-dim speaker representation.

The decoder follows [^Qian2019Autovc] with a 1024-dim LSTM layer, three convolutional layers, two 1024-dim LSTM layers and a 80-dim linear layer.

Besides, a 5-layer convolutional based Postnet is added to refine predicted mel-spectrograms, which are converted to waveform by Parallel WaveGAN vocoder [^Yamamoto2020Parallel] that is trained by VCTK corpus.

The variational approximation ${{Q}_{{{\theta }_{**u**,**v**}}}}(**u**|**v**)$ for all MI is parameterized in Gaussian distribution as ${{Q}_{{{\theta }_{**u**,**v**}}}}(**u**|**v**)=\mathcal{N} (**u**|\mu (**v**),diag({{\sigma }^{2}}(**v**)))$ with mean $\mu (**v**)$ and variance ${\sigma }^{2}(**v**)$ inferred by a two-way fully-connected network ${{\theta }_{**u**,**v**}}$ that is composed of four 256-dim hidden layers.

The VC network is trained using Adam optimizer [^Kingma2014Adam] with 15-epoch warmup increasing the learning rate from 1e-6 to 1e-3, which is halved every 100 epochs after 200 epochs until 500 epochs in total.

Batch size is 256 and 128 frames are randomly selected from each utterance for training per iteration.

Variational approximation networks are also trained with the Adam optimizer with a learning rate of 3e-4.

We compare our proposed VQMIVC method with AutoVC [^Qian2019Autovc], AdaIN-VC [^Chou2019One-Shot] and VQVC+ [^Wu2020Vqvc+], which are among the state-of-the-art one-shot VC methods. 

<a id="mi-est">MI among content, speaker and pitch representations, where MI is estimated on all testing speakers for 10 rounds, and mean $\pm$ standard-variance are reported.</a>

<a id="con-to-spk">CER/WER for generated speech using speaker representations from Same and Mixed utterances by varying ${\lambda}_{MI}$.</a>

### Experimental results and analysis

#### Speech representation disentanglement performance

In the VC loss ([vc-loss](#vc-loss)), ${\lambda}_{MI}$ determines the capacity of MI to enable SRD, we first vary ${\lambda}_{MI}$ to evaluate disentanglement degrees between different speech representations extracted from all testing utterances  by computing vCLUB as shown in Table [mi-est](#mi-est).

We can see that when ${\lambda}_{MI}$ increases, MI tends to decrease to reduce the correlation among different speech representations. 

To measure how much content information is entangled with speaker representation, we adopt two ways to generate the speech, i.e., (1) *Same*, i.e., the content, speaker and pitch representations of the same utterance are used to generate the speech; (2) *Mixed*, i.e., the content and pitch representations of one utterance and speaker representation of another utterance are used to generate the speech, both utterances belong to the same speaker.

Then an automatic speech recognition (ASR) system is used to obtain character/word error rate (CER/WER) of the generated speech.

The increased CER and WER from `*Same*' to `*Mixed*' are denoted as ${\triangle}_C$ and ${\triangle}_W$ respectively.

As the only difference in inputs for speech generation is that of speaker representation, we can conclude that larger values of ${\triangle}_C$ and ${\triangle}_W$ reflect that more content information is leaked to speaker representation.

All testing speakers are used for speech generation, and the publicly released Jasper-based ASR system [^Li2019Jasper] is used.

The results are shown in Table [con-to-spk](#con-to-spk), we can see that when MI is not used (${\lambda}_{MI}$=0), the generated speech is severely contaminated by the undesired content information that resides in the speaker representations as indicated by the largest values of ${\triangle}_C$ and ${\triangle}_W$.

However, when MI is used (${\lambda}_{MI}${\textgreater}0), significant reductions of ${\triangle}_C$ and ${\triangle}_W$ can be obtained.

As ${\lambda}_{MI}$ increases, both ${\triangle}_C$ and ${\triangle}_W$ decrease, showing that higher ${\lambda}_{MI}$ can, to a larger degree, alleviate leakage of content information into the speaker representation.

In addition, we design two speaker classifiers, taking $\hat{**Z**}$ and **s** as inputs respectively; and one predictor, taking in $\hat{**Z**}$ to infer **p**.

The classifiers and predictor are all 4-layer fully-connected network with 256-dim hidden size.

Higher speaker classification accuracy denotes more speaker information in $\hat{**Z**}$ or **s**, while higher prediction loss (mean square error) for **p** denotes less pitch information in $\hat{**Z**}$.

The results are shown in Table [acc-loss](#acc-loss).

We can observe that $\hat{**Z**}$ contains less speaker and pitch information when ${\lambda}_{MI}$ increases to achieve lower accuracy and higher pitch loss.

Speaker classification accuracy on **s** is high for all ${\lambda}_{MI}$, while the accuracy decreases when ${\lambda}_{MI}$ increases, showing that **s** contains abundant speaker information, but higher ${\lambda}_{MI}$ tends to make **s** lose speaker information.

To ensure proper disentanglement, we set ${\lambda}_{MI}$ to 1e-2 for the following experiments.  

<a id="acc-loss">Speaker classification accuracy on $\hat{**Z**}$ (content) and **s** (speaker), and prediction loss for **p** (pitch) inferred by $\hat{**Z**}$.</a>

<a id="asr-pcc">ASR and $*F*_0$-PCC results for one-shot VC.</a>

#### Content preservation and \texorpdfstring{$\textit{F

_0$}{Lg} variation consistency}

To evaluate whether the converted voice maintains linguistic content and intonation variations of the source voice, we test the CER/WER of the converted speech and calculate the Pearson correlation coefficient (PCC) [^Benesty2009Pearson] between $*F*_0$ of source and converted voice.

PCC ranges from -1 to 1 and can be effectively used to measure the correlation between two variables, where a higher $*F*_0$-PCC denotes that the converted voice has higher $*F*_0$ variation consistency with the source voice. 10 testing speakers are randomly selected as the source speakers, and the remaining 10 testing speakers are treated as target speakers, which leads to 100 conversion pairs where all source utterances are used for conversion.

The results for different methods are shown in Table [asr-pcc](#asr-pcc), where the results for source speech are also reported as the performance upper bound.

It can be seen that VQMIVC achieves the lowest CER and WER among all methods, which shows the robustness of the proposed VQMIVC method to preserve the source linguistic content.

Besides, we observe that ASR performance drops significantly without using MI (w/o MI), as the converted voice is contaminated by undesired content information entangled with speaker representations.

In addition, by providing source pitch representations, we can explicitly and effectively control intonation variations of the converted voice to achieve high $*F*_0$ variation consistency, as indicated by largest $*F*_0$-PCC of 0.781 obtained by the proposed methods.

![](MOS.png)

<a id="MOS">Comparison results of MOS with 95\% confidence intervals for speech naturalness and speaker similarity.</a>

\vspace{-0.5em}

#### Speech naturalness and speaker similarity

Subjective tests are conducted by 15 subjects to evaluate the speech naturalness and speaker similarity, in terms of 5-point mean opinion score (MOS), i.e, 1-bad, 2-poor, 3-fair, 4-good, 5-excellent.

We randomly select two source speakers and two target speakers from the testing speakers, each source or target set contains one male and one female speaker, which results in 4 conversion pairs, where 18 converted utterances from each pair are evaluated by each subject.

The scores are averaged across all pairs and reported in Figure [MOS](#MOS).

Source (Oracle) and Target (Oracle) denote that the speech is synthesized with ground-truth mel-spectrograms of source and target utterances by Parallel WaveGAN respectively.

We observe that the proposed method, denoted as `w/o MI', outperforms AutoVC and VQVC+, but is inferior to AdaIN-VC.

Pronunciation errors are frequently detected in the converted voice by `w/o MI' via our official listening tests, which can be reflected by the high CER/WER of `w/o MI' in Table [asr-pcc](#asr-pcc).

These issues can be greatly alleviated by the proposed MI minimization, where improved speech naturalness and speaker similarity are achieved.

This indicates that MI minimization facilitates proper SRD to derive accurate content representation and effective speaker representation, which can be used to generate the natural speech with high voice similarity to the target speaker.

## 5·Conclusions

We propose a novel approach by combining VQCPC and MI for unsupervised SRD-based one-shot VC.

To achieve proper disentanglement of content, speaker and pitch representations, VC model is not only trained to minimize the reconstruction loss, but also VQCPC loss to explore local structures of speech for content, and MI loss to reduce the correlation between different speech representations.

Experiments verify the efficacy of proposed methods to mitigate information leakage issues by learning accurate content representation to preserve source linguistic content, speaker representation to capture desired speaker characteristics, and pitch representation to retain source intonation variations, which results in high-quality converted voice.
