# VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-Shot Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-Shot Voice Conversion."
- 作者:
  - 01 Disong Wang
  - 02 Liqun Deng
  - 03 Yu Ting Yeung
  - 04 Xiao Chen
  - 05 Xunying Liu
  - 06 Helen Meng
- 链接:
  - [ArXiv](https://arxiv.org/abs/2106.10132v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2106.10132v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2021.06.18_2106.10132v1_VQMIVC__Vector_Quantization_and_Mutual_Information-Based_Unsupervised_Speech_Representation_Disentanglement_for_One-Shot_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

One-shot voice conversion (VC), which performs conversion across arbitrary speakers with only a single target-speaker utterance for reference, can be effectively achieved by speech representation disentanglement.
Existing work generally ignores the correlation between different speech representations during training, which causes leakage of content information into the speaker representation and thus degrades VC performance.
To alleviate this issue, we employ vector quantization (VQ) for content encoding and introduce mutual information (MI) as the correlation metric during training, to achieve proper disentanglement of content, speaker and pitch representations, by reducing their inter-dependencies in an unsupervised manner.
Experimental results reflect the superiority of the proposed method in learning effective disentangled speech representations for retaining source linguistic content and intonation variations, while capturing target speaker characteristics.
In doing so, the proposed approach achieves higher speech naturalness and speaker similarity than current state-of-the-art one-shot VC systems.
Our code, pre-trained models and demo are available at \color{blue}\url{https://github.com/Wendison/VQMIVC}.

## 1·Introduction

Voice conversion (VC) is a technique used to modify para-linguistic factors of an utterance from a source speaker to sound like a target speaker.

Para-linguistic factors include speaker identity [^Mohammadi2017Overview], prosody [^Rentzos2003Transformation] and accent [^Oyamada2017Non-Native], etc.

In this paper, we focus on the conversion of speaker identity across arbitrary speakers under a one-shot scenario [^Liu2018Voice], [^Lu2019One-Shot], i.e., given only one target speaker's utterance for reference. 

Previous work that use methods based on speech representation disentanglement (SRD) [^Qian2019Autovc], [^Chou2019One-Shot], [^Wu2020Vqvc+] attempted to address one-shot VC by decomposing the speech into speaker and content representations, and then the speaker identity can be converted by changing the source speaker's representation to that of the target speaker.

However, it is difficult to measure the degree of SRD.

Besides, previous approaches generally do not impose correlation constraints between speaker and content representations during training, which results in leakage of content information into the speaker representation, leading to VC performance degradation.

To alleviate these issues, this paper proposes the vector quantization and mutual information-based VC (VQMIVC) approach, where mutual information (MI) measures the dependencies between different representations and can be effectively integrated into the training process to achieve SRD in an *unsupervised manner*.

Specifically, we first decompose an utterance into three factors: content, speaker and pitch, and then propose a VC system consisting of four components: (1) A content encoder using vector quantization with contrastive predictive coding (VQCPC) [^Niekerk2020Vector-Quantized], [^Baevski2019Vq-Wav2vec] to extract frame-level content representations from acoustic features; (2) A speaker encoder that takes in acoustic features to generate a single fixed-dimensional vector as the speaker representation; (3) A pitch extractor that is used to compute normalized fundamental frequency ($*F*_0$) at the utterance level as the pitch representation; and (4) A decoder that maps content, speaker and pitch representations to acoustic features.

During training, the VC system is optimized by minimizing VQCPC, reconstruction and MI losses.

VQCPC aims to explore local structures of speech, and MI reduces the inter-dependencies of different speech representations.

During inference, one-shot VC is achieved by only replacing the source speaker representation with the target speaker representation derived from a single target utterance.

The main contribution of this work lies in applying the combination of VQCPC and MI to achieve SRD, without any requirements of supervision information such as text transcriptions or speaker labels.

Extensive experiments have been conducted to thoroughly analyze the importance of MI, where information leakage issues can be significantly alleviated for enhanced SRD.

## 2·Related Work

VC performance is critically dependent on the availability of the target speaker’s voice data for training [^Toda2007Voice], [^Helander2010Voice], [^Erro2009Inca], [^Sun2016Phonetic], [^Hsu2017Voice], [^Kaneko2017Parallel-Data-Free], [^Kameoka2018Stargan-Vc].

Hence, the challenge of one-shot VC is in performing conversion across arbitrary speakers that may be unseen during training, and with only one single target-speaker utterance for reference.

Previous approaches for one-shot VC are based on SRD, which aims to separate speaker information from spoken content as far as possible.

Related work include: tunable information-constraining bottleneck [^Qian2019Autovc], [^Qian2020F0-Consistent], [^Qian2020Unsupervised], instance normalization techniques [^Chou2019One-Shot], [^Chen2021Again-Vc] and vector quantization (VQ) [^Wu2020Vqvc+], [^Chorowski2019Unsupervised].

We adopt VQCPC [^Niekerk2020Vector-Quantized], [^Baevski2019Vq-Wav2vec], which is an improved version of VQ, to extract accurate content representations.

Without explicit constraints between different speech representations, information leakage tends to occur, which degrades VC performance.

We draw inspirations from information theory [^Gierlichs2008Mutual] in using MI as a regularizer to constrain the dependency between variables.

As MI computation is challenging for variables with unknown distribution, various methods have been explored to estimate MI lower bound [^Nguyen2010Estimating], [^Gutmann2010Noise-Contrastive], [^Belghazi2018Mutual] in SRD-based speech tasks [^Ravanelli2019Learning], [^Kwon2020Intra-Class], [^Hu2020Unsupervised] .

To guarantee the reduction of MI values, we propose to use variational contrastive log-ratio upper bound (vCLUB) [^Cheng2020Club].

While a recent effort [^Yuan2021Improving] employs MI for VC by using speaker labels as the supervision for learning speaker representations, our proposed approach differs from [^Yuan2021Improving] in terms of the combination of VQCPC and MI for fully unsupervised training, and the incorporation of pitch representations to maintain source intonation variations.

![](architecture.png)

<a id="arc">Diagram of the proposed VQMIVC system.</a>

## 3·Proposed Approach

This section first describes the system architecture of the VQMIVC approach, then elaborates on the integration of MI minimization into the training process, and finally shows how one-shot VC is achieved. 

### Architecture of the VQMIVC system

As shown in Figure [arc](#arc), the proposed VQMIVC system includes four modules: content encoder, speaker encoder, pitch extractor and decoder.

The first three modules respectively extract content, speaker and pitch representations from the input voice; and the fourth module, the decoder, maps these representations back to acoustic features.

Assuming that there are *K* utterances, we use mel-spectrograms as acoustic features and randomly select *T* frames from each utterance for training.

The $*k*^{th}$ mel-spectrogram is denoted as $**X**_k=\{**x**_{k,1},**x**_{k,2},...,**x**_{k,T}\}$.

\vspace{0.1em}

**Content encoder** ${\theta}_c$: The content encoder strives to extract linguistic content information from $**X**_k$ by using VQCPC as shown in Figure [ce](#ce), which contains two networks *h*-net: $**X**_k$ $\rightarrow$ $**Z**_k$ and *g*-net: $\hat{**Z**}_k$ $\rightarrow$ $**R**_k$, and VQ operation *q*: $**Z**_k$ $\rightarrow$ $\hat{**Z**}_k$. *h*-net takes in $**X**_k$ to derive a sequence of dense features $**Z**_k=\{**z**_{k,1},**z**_{k,2},...,**z**_{k,T/2}\}$, where the length is reduced from *T* to *T/2*.

Then the quantizer *q* discretizes $**Z**_k$ with a trainable codebook *B* into $\hat{**Z**}_k$=\{$\hat{**z**}_{k,1}$,$\hat{**z**}_{k,2}$,...,$\hat{**z**}_{k,T/2}$\}, where $\hat{**z**}_{k,t}$ $\in *B*$ is the vector closest to $**z**_{k,t}$.

VQ imposes an information bottleneck to remove non-essential details in $**Z**_k$, making $\hat{**Z**}_k$ to be related with underlying linguistic information.

Then the content encoder ${\theta}_c$ is trained by minimizing the VQ loss [^Niekerk2020Vector-Quantized]:

\begin{footnotesize}

$$

{{L}_{VQ}}=\frac{2}{KT}\sum\limits_{k=1}^{K}{\sum\limits_{t=1}^{T/2} \left\| {{**z**}_{k,t}}-sg({{{\hat{**z**}}}_{k,t}}) \right\|_{2}^{2}} \label{vq-loss}

$$

\end{footnotesize}
where *sg*(·) denotes the stop-gradient operator.

To further encourage $\hat{**Z**}_k$ to capture local structures, contrastive predictive coding (CPC) is employed by adding RNN based *g*-net taking in $\hat{**Z**}_k$ to obtain aggregation $**R**_k=\{**r**_{k,1},**r**_{k,2},...,**r**_{k,T/2}\}$.

Given $**r**_{k,t}$, the model is trained to distinguish a positive sample $\hat{**z**}_{k,t+m}$ that is *m* steps in the future from negative samples drawn from the set ${\Omega}_{k,t,m}$ by minimizing the InfoNCE loss [^Oord2018Representation]:

\begin{footnotesize}

$$

{{L}_{CPC}}=-\frac{1}{KT'M}\!\sum\limits_{k=1}^{K}{\sum\limits_{t=1}^{T'}{\sum\limits_{m=1}^{M}{\!\log\! \left[\! \frac{\exp (\hat{**z**}_{k,t+m}^{T}{{\mathbf{W}}_{m}}{{**r**}_{k,t}})}{\sum\nolimits_{\tilde{**z**}\in {{\Omega }_{k,t,m}}}{\exp ({{{\tilde{**z**}}}^{T}}{{\mathbf{W}}_{m}}{{**r**}_{k,t}})}} \!\right]}}} \label{cpc-loss} 

$$

\end{footnotesize}
where ${T'}=T/2-M$, $**W**_m$ (*m*=1,2,...,*M*) is trainable projection matrix.

By predicting future samples with probabilistic contrastive loss ([cpc-loss](#cpc-loss)), local features (e.g., phonemes) spanning many time steps are encoded into $\hat{**Z**}_k$=*f*($**X**_k$;${\theta}_c$), which is the content representation used to accurately reconstruct the linguistic content.

During training, the negative samples set ${\Omega}_{k,t,m}$ is formed by randomly selecting samples from the current utterance.

![](content-encoder.png)

<a id="ce">Details of the VQCPC based content encoder.</a>

**Speaker encoder** ${\theta}_s$: The speaker encoder takes in $**X**_k$ to generate a vector $**s**_k$=*f*($**X**_k$;${\theta}_s$), which is used as the speaker representation. $**s**_k$ captures global speech characteristics to control the speaker identity of the generated speech. 

**Pitch extractor**: The pitch representation is expected to contain intonation variations but exclude content and speaker information, so we extract $*F*_0$ from the waveform and perform z-normalization for each utterance independently.

In our experiments, we adopt log-normalized $*F*_0$ (log-$*F*_0$) as $**p**_k$=($*p*_{k,1}$, $*p*_{k,2}$, ..., $*p*_{k,T}$), which is speaker-independent so that speaker encoder is forced to provide the speaker information, e.g., vocal ranges.

**Decoder** ${\theta}_d$: The decoder is used to map the content, speaker and pitch representations to mel-spectrograms.

Linear interpolation based upsampling ($\times$2) and repetition ($\times$*T*) are performed on $\hat{**Z**}_k$ and $**s**_k$ respectively to align with $**p**_k$ as inputs of decoder to generate mel-spectrograms $\hat{**X**}_k$=\{$\hat{**x**}_{k,1}$,$\hat{**x**}_{k,2}$,...,$\hat{**x**}_{k,T}$\}.

Decoder is jointly trained with content and speaker encoders by minimizing a reconstruction loss:

\begin{footnotesize}

$$

{{L}_{REC}}=\frac{1}{KT}\sum\limits_{k=1}^{K}{\sum\limits_{t=1}^{T}{\left[{{\left\| {{{\hat{**x**}}}_{t}}-{{**x**}_{t}} \right\|}_{1}}+{{\left\| {{{\hat{**x**}}}_{t}}-{{**x**}_{t}} \right\|}_{2}}\right]}} \label{rec-loss}
\vspace{-0.5em}

$$

\end{footnotesize}

\vspace{-1em}

### MI minimization integrated into VQMIVC training

Given the random variables **u** and **v**, the MI is Kullback-Leibler (KL) divergence between their joint and marginal distributions as $\mathbf{I}(\mathbf{u},\mathbf{v})={{D}_{KL}}(P(\mathbf{u},\mathbf{v});P(\mathbf{u})P(\mathbf{v}))$.

We adopt vCLUB [^Cheng2020Club] to compute the upper bound of MI as:

\begin{footnotesize}

$$

{**I**}\!\left( **u**,**v** \right)\!=\!{{E}_{P(**u**,**v**)}}\!\left[\! \log {{Q}_{{{\theta }_{**u**,**v**}}}}\!\left( **u**|**v** \right)\! \right]\!-\!{{E}_{P(**u**)}}{{E}_{P(**v**)}}\!\left[\! \log {{Q}_{{{\theta }_{**u**,**v**}}}}\!\left( **u**|**v** \right)\! \right] \label{club}

$$

\end{footnotesize}
where **u**,**v** $\in$ \{$\hat{**Z**}$, **s**, **p**\}, $\hat{**Z**}$, **s** and **p** are content, speaker and pitch representations respectively, ${{Q}_{{{\theta }_{**u**,**v**}}}}(**u**|**v**)$ is the variational approximation of ground-truth posterior of **u** given **v** and can be parameterized by a network ${{\theta }_{**u**,**v**}}$.

The unbiased estimation for vCLUB between different speech representations is given by:

\begin{footnotesize}

$$

\!{{\hat{**I**}}({\hat{**Z**},**s**})}\!=\!\frac{2}{{{K}^{2}}T}\sum\limits_{k=1}^{K}{\sum\limits_{l=1}^{K}{\sum\limits_{t=1}^{T/2}{\!\left[\! \log\!{{Q}_{{{\theta }_{\hat{**Z**},**s**}}}}\!\left( {{{\hat{**z**}}}_{k,t}}|{{**s**}_{k}}\! \right)\!-\!\log\! {{Q}_{{{\theta }_{\hat{**Z**},**s**}}}}\!\left( {{{\hat{**z**}}}_{l,t}}|{{**s**}_{k}}\! \right)\! \right]}}} \label{mi-1}

$$

\end{footnotesize}

\begin{footnotesize}

$$

\!{{\hat{**I**}}({{**p**},**s**})}\!=\!\frac{1}{{{K}^{2}}T}\sum\limits_{k=1}^{K}{\sum\limits_{l=1}^{K}{\sum\limits_{t=1}^{T}{\left[\! \log\!{{Q}_{{{\theta }_{{**p**},**s**}}}}\!\left( {{{{**p**}}}_{k,t}}|{{**s**}_{k}}\! \right)\!-\!\log\! {{Q}_{{{\theta }_{{**p**},**s**}}}}\!\left( {{{{**p**}}}_{l,t}}|{{**s**}_{k}}\! \right)\! \right]}}} \label{mi-2}

$$

\end{footnotesize}

\begin{footnotesize}

$$

\!\!\hat{**I**}(\!\hat{**Z**},\!**p**\!)\!=\!\frac{2}{{{K}^{2}}T}\!\sum\limits_{k=1}^{K}{\!\sum\limits_{l=1}^{K}{\!\sum\limits_{t=1}^{T/2}{\!\left[\! \log\! {{Q}_{{{\theta }_{\hat{**Z**},\!**p**}}}}\!\!\left( {{{\hat{**z**}}}_{k,t}}\!|{{{\hat{**p**}}}_{k,t}} \right)\!-\!\log\! {{Q}_{{{\theta }_{\hat{**Z**},\!**p**}}}}\!\!\left( {{{\hat{**z**}}}_{l,t}}\!|{{{\hat{**p**}}}_{k,t}} \!\right)\! \right]}}} \label{mi-3}

$$

\end{footnotesize}
where $\hat{**p**}_{k,t}=(**p**_{k,{2t-1}}+**p**_{k,{2t}})/2$.

With a good variational approximation, ([club](#club)) provides a reliable MI upper bound.

Therefore, we can decrease the correlation among different speech representations by minimizing ([mi-1](#mi-1))-([mi-3](#mi-3)), and the total MI loss is:
\vspace{-0.2em}

\begin{footnotesize}

$$

% {{L}_{MI}}=\hat{**I**}\left( \hat{**Z**},\mathbf{s} \right)\text{+}\hat{**I**}\left( \hat{**Z**},\mathbf{p} \right)\text{+}\hat{**I**}\left( \mathbf{p},\mathbf{s} \right) \label{mi-loss}
{{L}_{MI}}=\hat{**I**}(\hat{**Z**},\mathbf{s})\text{+}\hat{**I**}(\hat{**Z**},\mathbf{p})\text{+}\hat{**I**}(\mathbf{p},\mathbf{s}) \label{mi-loss}

$$

\end{footnotesize}
During training, variational approximation networks and VC network are optimized alternatively.

The variational approximation networks are trained to maximize the log-likelihood:
\vspace{-0.2em}

\begin{footnotesize}

$$

\begin{matrix}
{{L}_{**u**,**v**}}=\log {{Q}_{{{\theta }_{**u**,**v**}}}}\left( **u**|**v** \right), & \mathbf{u},\mathbf{v}\in \{\hat{**Z**},\mathbf{s},\mathbf{p}\}  \\
\end{matrix}
\label{ll-loss}

$$

\end{footnotesize}
while the VC network is trained to minimize VC loss:
\vspace{-0.1em}

\begin{footnotesize}

$$

{{L}_{VC}}={{L}_{VQ}}+{{L}_{CPC}}+{{L}_{REC}}+{\lambda}_{MI}{{L}_{MI}}
\label{vc-loss}

$$

\end{footnotesize}
where ${\lambda}_{MI}$ is a constant weight to control how MI loss enhances the disentanglement.

The final training process is summarized in Algorithm 1.

We note that no text transcriptions or speaker labels are used during training, so the proposed approach achieves disentanglement in a fully unsupervised way.

<a id="alo">Pseudocode for the proposed VQMIVC training</a>

### One-shot VC

During conversion, the content and pitch representations are first extracted from source speaker's utterance $**X**_*src*$ as $\hat{**Z**}_{src}=*f*({**X**_*src*};{{\theta }_{c}})$ and $**p**_{src}$ respectively, the speaker representation is extracted from only one target speaker's utterance $**X**_*tgt*$ as $**s**_{tgt}=*f*({**X**_{tgt}};{{\theta }_{s}})$, then the decoder generates the converted mel-spectrograms as *f*$(\hat{**Z**}_{src},**s**_{tgt},**p**_{src};{\theta}_d)$.

## 4·Experiments

### Experimental setup

All experiments are conducted on the VCTK corpus [^Veaux2016Superseded-CSTR] with 110 English speakers, which are randomly split into 90 and 20 speakers as training and testing sets respectively.

The testing speakers are treated as unseen speakers that are used to perform one-shot VC.

For acoustic features extraction, all audio recordings are downsampled to 16kHz, 80-dim mel-spectrograms and $*F*_0$ are both calculated with 25ms Hanning window, 10ms frame shift and 400-point fast Fourier transform.

The proposed VC network consists of the content encoder, speaker encoder and decoder.

The content encoder contains a *h*-net, a quantizer *q* and a *g*-net.

The *h*-net is composed of a convolutional layer with stride of 2, four blocks with layer normalization, 512-dim linear layer and ReLU activation function for each block.

The quantizer contains a codebook with 512 64-dim learnable vectors.

The *g*-net is a 256-dim uni-directional RNN layer.

For CPC, the future prediction step *M* is 6 and the number of negative samples $|{\Omega}_{k,t,m}|$ is 10.

The speaker encoder follows [^Chou2019One-Shot], which contains 8 ConvBank layers to encode the long-term information, 12 convolutional layers with 1 average-pooling layer, and 4 linear layers to derive the 256-dim speaker representation.

The decoder follows [^Qian2019Autovc] with a 1024-dim LSTM layer, three convolutional layers, two 1024-dim LSTM layers and a 80-dim linear layer.

Besides, a 5-layer convolutional based Postnet is added to refine predicted mel-spectrograms, which are converted to waveform by Parallel WaveGAN vocoder [^Yamamoto2020Parallel] that is trained by VCTK corpus.

The variational approximation ${{Q}_{{{\theta }_{**u**,**v**}}}}(**u**|**v**)$ for all MI is parameterized in Gaussian distribution as ${{Q}_{{{\theta }_{**u**,**v**}}}}(**u**|**v**)=\mathcal{N} (**u**|\mu (**v**),diag({{\sigma }^{2}}(**v**)))$ with mean $\mu (**v**)$ and variance ${\sigma }^{2}(**v**)$ inferred by a two-way fully-connected network ${{\theta }_{**u**,**v**}}$ that is composed of four 256-dim hidden layers.

The VC network is trained using Adam optimizer [^Kingma2014Adam] with 15-epoch warmup increasing the learning rate from 1e-6 to 1e-3, which is halved every 100 epochs after 200 epochs until 500 epochs in total.

Batch size is 256 and 128 frames are randomly selected from each utterance for training per iteration.

Variational approximation networks are also trained with the Adam optimizer with a learning rate of 3e-4.

We compare our proposed VQMIVC method with AutoVC [^Qian2019Autovc], AdaIN-VC [^Chou2019One-Shot] and VQVC+ [^Wu2020Vqvc+], which are among the state-of-the-art one-shot VC methods. 

<a id="mi-est">MI among content, speaker and pitch representations, where MI is estimated on all testing speakers for 10 rounds, and mean $\pm$ standard-variance are reported.</a>

<a id="con-to-spk">CER/WER for generated speech using speaker representations from Same and Mixed utterances by varying ${\lambda}_{MI}$.</a>

### Experimental results and analysis

#### Speech representation disentanglement performance

In the VC loss ([vc-loss](#vc-loss)), ${\lambda}_{MI}$ determines the capacity of MI to enable SRD, we first vary ${\lambda}_{MI}$ to evaluate disentanglement degrees between different speech representations extracted from all testing utterances  by computing vCLUB as shown in Table [mi-est](#mi-est).

We can see that when ${\lambda}_{MI}$ increases, MI tends to decrease to reduce the correlation among different speech representations. 

To measure how much content information is entangled with speaker representation, we adopt two ways to generate the speech, i.e., (1) *Same*, i.e., the content, speaker and pitch representations of the same utterance are used to generate the speech; (2) *Mixed*, i.e., the content and pitch representations of one utterance and speaker representation of another utterance are used to generate the speech, both utterances belong to the same speaker.

Then an automatic speech recognition (ASR) system is used to obtain character/word error rate (CER/WER) of the generated speech.

The increased CER and WER from `*Same*' to `*Mixed*' are denoted as ${\triangle}_C$ and ${\triangle}_W$ respectively.

As the only difference in inputs for speech generation is that of speaker representation, we can conclude that larger values of ${\triangle}_C$ and ${\triangle}_W$ reflect that more content information is leaked to speaker representation.

All testing speakers are used for speech generation, and the publicly released Jasper-based ASR system [^Li2019Jasper] is used.

The results are shown in Table [con-to-spk](#con-to-spk), we can see that when MI is not used (${\lambda}_{MI}$=0), the generated speech is severely contaminated by the undesired content information that resides in the speaker representations as indicated by the largest values of ${\triangle}_C$ and ${\triangle}_W$.

However, when MI is used (${\lambda}_{MI}${\textgreater}0), significant reductions of ${\triangle}_C$ and ${\triangle}_W$ can be obtained.

As ${\lambda}_{MI}$ increases, both ${\triangle}_C$ and ${\triangle}_W$ decrease, showing that higher ${\lambda}_{MI}$ can, to a larger degree, alleviate leakage of content information into the speaker representation.

In addition, we design two speaker classifiers, taking $\hat{**Z**}$ and **s** as inputs respectively; and one predictor, taking in $\hat{**Z**}$ to infer **p**.

The classifiers and predictor are all 4-layer fully-connected network with 256-dim hidden size.

Higher speaker classification accuracy denotes more speaker information in $\hat{**Z**}$ or **s**, while higher prediction loss (mean square error) for **p** denotes less pitch information in $\hat{**Z**}$.

The results are shown in Table [acc-loss](#acc-loss).

We can observe that $\hat{**Z**}$ contains less speaker and pitch information when ${\lambda}_{MI}$ increases to achieve lower accuracy and higher pitch loss.

Speaker classification accuracy on **s** is high for all ${\lambda}_{MI}$, while the accuracy decreases when ${\lambda}_{MI}$ increases, showing that **s** contains abundant speaker information, but higher ${\lambda}_{MI}$ tends to make **s** lose speaker information.

To ensure proper disentanglement, we set ${\lambda}_{MI}$ to 1e-2 for the following experiments.  

<a id="acc-loss">Speaker classification accuracy on $\hat{**Z**}$ (content) and **s** (speaker), and prediction loss for **p** (pitch) inferred by $\hat{**Z**}$.</a>

<a id="asr-pcc">ASR and $*F*_0$-PCC results for one-shot VC.</a>

#### Content preservation and \texorpdfstring{$\textit{F

_0$}{Lg} variation consistency}

To evaluate whether the converted voice maintains linguistic content and intonation variations of the source voice, we test the CER/WER of the converted speech and calculate the Pearson correlation coefficient (PCC) [^Benesty2009Pearson] between $*F*_0$ of source and converted voice.

PCC ranges from -1 to 1 and can be effectively used to measure the correlation between two variables, where a higher $*F*_0$-PCC denotes that the converted voice has higher $*F*_0$ variation consistency with the source voice. 10 testing speakers are randomly selected as the source speakers, and the remaining 10 testing speakers are treated as target speakers, which leads to 100 conversion pairs where all source utterances are used for conversion.

The results for different methods are shown in Table [asr-pcc](#asr-pcc), where the results for source speech are also reported as the performance upper bound.

It can be seen that VQMIVC achieves the lowest CER and WER among all methods, which shows the robustness of the proposed VQMIVC method to preserve the source linguistic content.

Besides, we observe that ASR performance drops significantly without using MI (w/o MI), as the converted voice is contaminated by undesired content information entangled with speaker representations.

In addition, by providing source pitch representations, we can explicitly and effectively control intonation variations of the converted voice to achieve high $*F*_0$ variation consistency, as indicated by largest $*F*_0$-PCC of 0.781 obtained by the proposed methods.

![](MOS.png)

<a id="MOS">Comparison results of MOS with 95\% confidence intervals for speech naturalness and speaker similarity.</a>

\vspace{-0.5em}

#### Speech naturalness and speaker similarity

Subjective tests are conducted by 15 subjects to evaluate the speech naturalness and speaker similarity, in terms of 5-point mean opinion score (MOS), i.e, 1-bad, 2-poor, 3-fair, 4-good, 5-excellent.

We randomly select two source speakers and two target speakers from the testing speakers, each source or target set contains one male and one female speaker, which results in 4 conversion pairs, where 18 converted utterances from each pair are evaluated by each subject.

The scores are averaged across all pairs and reported in Figure [MOS](#MOS).

Source (Oracle) and Target (Oracle) denote that the speech is synthesized with ground-truth mel-spectrograms of source and target utterances by Parallel WaveGAN respectively.

We observe that the proposed method, denoted as `w/o MI', outperforms AutoVC and VQVC+, but is inferior to AdaIN-VC.

Pronunciation errors are frequently detected in the converted voice by `w/o MI' via our official listening tests, which can be reflected by the high CER/WER of `w/o MI' in Table [asr-pcc](#asr-pcc).

These issues can be greatly alleviated by the proposed MI minimization, where improved speech naturalness and speaker similarity are achieved.

This indicates that MI minimization facilitates proper SRD to derive accurate content representation and effective speaker representation, which can be used to generate the natural speech with high voice similarity to the target speaker.

## 5·Conclusions

We propose a novel approach by combining VQCPC and MI for unsupervised SRD-based one-shot VC.

To achieve proper disentanglement of content, speaker and pitch representations, VC model is not only trained to minimize the reconstruction loss, but also VQCPC loss to explore local structures of speech for content, and MI loss to reduce the correlation between different speech representations.

Experiments verify the efficacy of proposed methods to mitigate information leakage issues by learning accurate content representation to preserve source linguistic content, speaker representation to capture desired speaker characteristics, and pitch representation to retain source intonation variations, which results in high-quality converted voice.

## 6·Acknowledgements

This research is partially supported by a grant from the HKSARG Research Grants Council General Research Fund (Project Reference No. 14208718).

\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
\fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'.

Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{mohammadi2017overview}

S.~H.

Mohammadi and A.~Kain, "An overview of voice conversion systems,''
\emph{Speech Communication}, vol.~88, pp. 65--82, 2017.

\bibitem{rentzos2003transformation}

D.~Rentzos, S.~Vaseghi, E.~Turajlic, Q.~Yan, and C.-H.

Ho, "Transformation of
speaker characteristics for voice conversion,'' in \emph{2003 IEEE Workshop
on Automatic Speech Recognition and Understanding (IEEE Cat.

No.
03EX721)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2003, pp. 706--711.

\bibitem{oyamada2017non}

K.~Oyamada, H.~Kameoka, T.~Kaneko, H.~Ando, K.~Hiramatsu, and K.~Kashino,
"Non-native speech conversion with consistency-aware recursive network and
generative adversarial network,'' in \emph{2017 Asia-Pacific Signal and
Information Processing Association Annual Summit and Conference (APSIPA
ASC)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2017, pp. 182--188.

\bibitem{liu2018voice}

S.~Liu, J.~Zhong, L.~Sun, X.~Wu, X.~Liu, and H.~Meng, "Voice conversion across
arbitrary speakers based on a single target-speaker utterance.'' in
\emph{Interspeech}, 2018, pp. 496--500.

\bibitem{lu2019one}

H.~Lu, Z.~Wu, D.~Dai, R.~Li, S.~Kang, J.~Jia, and H.~Meng, "One-shot voice
conversion with global speaker embeddings.'' in \emph{Interspeech}, 2019, pp.
669--673.

\bibitem{qian2019autovc}

K.~Qian, Y.~Zhang, S.~Chang, X.~Yang, and M.~Hasegawa-Johnson, "Autovc:
Zero-shot voice style transfer with only autoencoder loss,'' in
\emph{International Conference on Machine Learning}.\hskip 1em plus 0.5em
minus 0.4em\relax PMLR, 2019, pp. 5210--5219.

\bibitem{chou2019one}

J.-c.

Chou and H.-Y.

Lee, "One-shot voice conversion by separating speaker and
content representations with instance normalization,'' \emph{Interspeech},
pp. 664--668, 2019.

\bibitem{wu2020vqvc+}

D.-Y.

Wu, Y.-H.

Chen, and H.-y.

Lee, "Vqvc+: One-shot voice conversion by
vector quantization and u-net architecture,'' \emph{Interspeech}, pp.
4691--4695, 2020.

\bibitem{van2020vector}

B.~van Niekerk, L.~Nortje, and H.~Kamper, "Vector-quantized neural networks
for acoustic unit discovery in the zerospeech 2020 challenge,''
\emph{Interspeech}, pp. 4836--4840, 2020.

\bibitem{baevski2019vq}

A.~Baevski, S.~Schneider, and M.~Auli, "vq-wav2vec: Self-supervised learning
of discrete speech representations,'' \emph{arXiv preprint arXiv:1910.05453},
2019.

\bibitem{toda2007voice}

T.~Toda, A.~W.

Black, and K.~Tokuda, "Voice conversion based on
maximum-likelihood estimation of spectral parameter trajectory,'' \emph{IEEE
Transactions on Audio, Speech, and Language Processing}, vol.~15, no.~8, pp.
2222--2235, 2007.

\bibitem{helander2010voice}

E.~Helander, T.~Virtanen, J.~Nurminen, and M.~Gabbouj, "Voice conversion using
partial least squares regression,'' \emph{IEEE Transactions on Audio, Speech,
and Language Processing}, vol.~18, no.~5, pp. 912--921, 2010.

\bibitem{erro2009inca}

D.~Erro, A.~Moreno, and A.~Bonafonte, "Inca algorithm for training voice
conversion systems from nonparallel corpora,'' \emph{IEEE Transactions on
Audio, Speech, and Language Processing}, vol.~18, no.~5, pp. 944--953, 2009.

\bibitem{sun2016phonetic}

L.~Sun, K.~Li, H.~Wang, S.~Kang, and H.~Meng, "Phonetic posteriorgrams for
many-to-one voice conversion without parallel data training,'' in \emph{2016
IEEE International Conference on Multimedia and Expo (ICME)}.\hskip 1em plus
0.5em minus 0.4em\relax IEEE, 2016, pp. 1--6.

\bibitem{hsu2017voice}

C.-C.

Hsu, H.-T.

Hwang, Y.-C.

Wu, Y.~Tsao, and H.-M.

Wang, "Voice conversion
from unaligned corpora using variational autoencoding wasserstein generative
adversarial networks,'' \emph{arXiv preprint arXiv:1704.00849}, 2017.

\bibitem{kaneko2017parallel}

T.~Kaneko and H.~Kameoka, "Parallel-data-free voice conversion using
cycle-consistent adversarial networks,'' \emph{arXiv preprint
arXiv:1711.11293}, 2017.

\bibitem{kameoka2018stargan}

H.~Kameoka, T.~Kaneko, K.~Tanaka, and N.~Hojo, "Stargan-vc: Non-parallel
many-to-many voice conversion using star generative adversarial networks,''
in \emph{2018 IEEE Spoken Language Technology Workshop (SLT)}.\hskip 1em plus
0.5em minus 0.4em\relax IEEE, 2018, pp. 266--273.

\bibitem{qian2020f0}

K.~Qian, Z.~Jin, M.~Hasegawa-Johnson, and G.~J.

Mysore, "F0-consistent
many-to-many non-parallel voice conversion via conditional autoencoder,'' in
\emph{ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
2020, pp. 6284--6288.

\bibitem{qian2020unsupervised}

K.~Qian, Y.~Zhang, S.~Chang, M.~Hasegawa-Johnson, and D.~Cox, "Unsupervised
speech decomposition via triple information bottleneck,'' in
\emph{International Conference on Machine Learning}.\hskip 1em plus 0.5em
minus 0.4em\relax PMLR, 2020, pp. 7836--7846.

\bibitem{chen2021again}

Y.-H.

Chen, D.-Y.

Wu, T.-H.

Wu, and H.-y.

Lee, "Again-vc: A one-shot voice
conversion using activation guidance and adaptive instance normalization,''
in \emph{ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
2021, pp. 5954--5958.

\bibitem{chorowski2019unsupervised}

J.~Chorowski, R.~J.

Weiss, S.~Bengio, and A.~van~den Oord, "Unsupervised
speech representation learning using wavenet autoencoders,'' \emph{IEEE/ACM
transactions on audio, speech, and language processing}, vol.~27, no.~12, pp.
2041--2053, 2019.

\bibitem{gierlichs2008mutual}

B.~Gierlichs, L.~Batina, P.~Tuyls, and B.~Preneel, "Mutual information
analysis,'' in \emph{International Workshop on Cryptographic Hardware and
Embedded Systems}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2008, pp.
426--442.

\bibitem{nguyen2010estimating}

X.~Nguyen, M.~J.

Wainwright, and M.~I.

Jordan, "Estimating divergence
functionals and the likelihood ratio by convex risk minimization,''
\emph{IEEE Transactions on Information Theory}, vol.~56, no.~11, pp.
5847--5861, 2010.

\bibitem{gutmann2010noise}

M.~Gutmann and A.~Hyv{\"a}rinen, "Noise-contrastive estimation: A new
estimation principle for unnormalized statistical models,'' in
\emph{Proceedings of the Thirteenth International Conference on Artificial
Intelligence and Statistics}, 2010, pp. 297--304.

\bibitem{belghazi2018mutual}

M.~I.

Belghazi, A.~Baratin, S.~Rajeshwar, S.~Ozair, Y.~Bengio, A.~Courville,
and D.~Hjelm, "Mutual information neural estimation,'' in
\emph{International Conference on Machine Learning}, 2018, pp. 531--540.

\bibitem{ravanelli2019learning}

M.~Ravanelli and Y.~Bengio, "Learning speaker representations with mutual
information,'' \emph{Interspeech}, pp. 1153--1157, 2019.

\bibitem{kwon2020intra}

Y.~Kwon, S.-W.

Chung, and H.-G.

Kang, "Intra-class variation reduction of
speaker representation in disentanglement framework,'' \emph{Interspeech},
pp. 3231--3235, 2020.

\bibitem{hu2020unsupervised}

T.-Y.

Hu, A.~Shrivastava, O.~Tuzel, and C.~Dhir, "Unsupervised style and
content separation by minimizing mutual information for speech synthesis,''
in \emph{ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
2020, pp. 3267--3271.

\bibitem{cheng2020club}

P.~Cheng, W.~Hao, S.~Dai, J.~Liu, Z.~Gan, and L.~Carin, "Club: A contrastive
log-ratio upper bound of mutual information,'' in \emph{International
Conference on Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR,
2020, pp. 1779--1788.

\bibitem{yuan2021improving}

S.~Yuan, P.~Cheng, R.~Zhang, W.~Hao, Z.~Gan, and L.~Carin, "Improving
zero-shot voice style transfer via disentangled representation learning,''
\emph{arXiv preprint arXiv:2103.09420}, 2021.

\bibitem{oord2018representation}

A.~v.~d.

Oord, Y.~Li, and O.~Vinyals, "Representation learning with
contrastive predictive coding,'' \emph{arXiv preprint arXiv:1807.03748},
2018.

\bibitem{veaux2016superseded}

C.~Veaux, J.~Yamagishi, K.~MacDonald \emph{et~al.}, "Superseded-cstr vctk
corpus: English multi-speaker corpus for cstr voice cloning toolkit,'' 2016.

\bibitem{yamamoto2020parallel}

R.~Yamamoto, E.~Song, and J.-M.

Kim, "Parallel wavegan: A fast waveform
generation model based on generative adversarial networks with
multi-resolution spectrogram,'' in \emph{ICASSP 2020-2020 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em
plus 0.5em minus 0.4em\relax IEEE, 2020, pp. 6199--6203.

\bibitem{kingma2014adam}

D.~P.

Kingma and J.~Ba, "Adam: A method for stochastic optimization,''
\emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{li2019jasper}

J.~Li, V.~Lavrukhin, B.~Ginsburg, R.~Leary, O.~Kuchaiev, J.~M.

Cohen,
H.~Nguyen, and R.~T.

Gadde, "Jasper: An end-to-end convolutional neural
acoustic model,'' \emph{Interspeech}, pp. 71--75, 2019.

\bibitem{benesty2009pearson}

J.~Benesty, J.~Chen, Y.~Huang, and I.~Cohen, "Pearson correlation
coefficient,'' in \emph{Noise reduction in speech processing}.\hskip 1em plus
0.5em minus 0.4em\relax Springer, 2009, pp. 1--4.

\end{thebibliography}

\end{document}

## References

[^Mohammadi2017Overview]: An Overview of Voice Conversion Systems. Speech Communication 2017.
[^Rentzos2003Transformation]: Transformation of Speaker Characteristics for Voice Conversion. 2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No. 03EX721) 2003.
[^Oyamada2017Non-Native]: Non-Native Speech Conversion With Consistency-Aware Recursive Network and Generative Adversarial Network. 2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC) 2017.
[^Liu2018Voice]: Voice Conversion Across Arbitrary Speakers Based on a Single Target-Speaker Utterance.. Interspeech 2018.
[^Lu2019One-Shot]: One-Shot Voice Conversion With Global Speaker Embeddings.. Interspeech 2019.
[^Qian2019Autovc]: Autovc: Zero-Shot Voice Style Transfer With Only Autoencoder Loss. International Conference on Machine Learning 2019.
[^Chou2019One-Shot]: One-Shot Voice Conversion by Separating Speaker and Content Representations With Instance Normalization. Interspeech 2019.
[^Wu2020Vqvc+]: VQVC+: One-Shot Voice Conversion by Vector Quantization and U-Net Architecture. Interspeech 2020.
[^Niekerk2020Vector-Quantized]: Vector-Quantized Neural Networks for Acoustic Unit Discovery in the ZeroSpeech 2020 Challenge. Interspeech 2020.
[^Baevski2019Vq-Wav2vec]: Vq-Wav2vec: Self-Supervised Learning of Discrete Speech Representations. arXiv:1910.05453.
[^Toda2007Voice]: Voice Conversion Based on Maximum-Likelihood Estimation of Spectral Parameter Trajectory. IEEE Transactions on Audio, Speech, and Language Processing 2007.
[^Helander2010Voice]: Voice Conversion Using Partial Least Squares Regression. IEEE Transactions on Audio, Speech, and Language Processing 2010.
[^Erro2009Inca]: INCA Algorithm for Training Voice Conversion Systems From Nonparallel Corpora. IEEE Transactions on Audio, Speech, and Language Processing 2009.
[^Sun2016Phonetic]: Phonetic Posteriorgrams for Many-to-One Voice Conversion Without Parallel Data Training. 2016 IEEE International Conference on Multimedia and Expo (ICME) 2016.
[^Hsu2017Voice]: Voice Conversion From Unaligned Corpora Using Variational Autoencoding Wasserstein Generative Adversarial Networks. arXiv:1704.00849.
[^Kaneko2017Parallel-Data-Free]: Parallel-Data-Free Voice Conversion Using Cycle-Consistent Adversarial Networks. arXiv:1711.11293.
[^Kameoka2018Stargan-Vc]: Stargan-Vc: Non-Parallel Many-to-Many Voice Conversion Using Star Generative Adversarial Networks. 2018 IEEE Spoken Language Technology Workshop (SLT) 2018.
[^Qian2020F0-Consistent]: F0-Consistent Many-to-Many Non-Parallel Voice Conversion via Conditional Autoencoder. ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2020.
[^Qian2020Unsupervised]: Unsupervised Speech Decomposition via Triple Information Bottleneck. International Conference on Machine Learning 2020.
[^Chen2021Again-Vc]: Again-Vc: A One-Shot Voice Conversion Using Activation Guidance and Adaptive Instance Normalization. ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2021.
[^Chorowski2019Unsupervised]: Unsupervised Speech Representation Learning Using Wavenet Autoencoders. IEEE/ACM Transactions on Audio, Speech, and Language Processing 2019.
[^Gierlichs2008Mutual]: Mutual Information Analysis. International Workshop on Cryptographic Hardware and Embedded Systems 2008.
[^Nguyen2010Estimating]: Estimating Divergence Functionals and the Likelihood Ratio by Convex Risk Minimization. IEEE Transactions on Information Theory 2010.
[^Gutmann2010Noise-Contrastive]: Noise-Contrastive Estimation: A New Estimation Principle for Unnormalized Statistical Models. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics 2010.
[^Belghazi2018Mutual]: Mutual Information Neural Estimation. International Conference on Machine Learning 2018.
[^Ravanelli2019Learning]: Learning Speaker Representations With Mutual Information. Interspeech 2019.
[^Kwon2020Intra-Class]: Intra-Class Variation Reduction of Speaker Representation in Disentanglement Framework. Interspeech 2020.
[^Hu2020Unsupervised]: Unsupervised Style and Content Separation by Minimizing Mutual Information for Speech Synthesis. ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2020.
[^Cheng2020Club]: Club: A Contrastive Log-Ratio Upper Bound of Mutual Information. International Conference on Machine Learning 2020.
[^Yuan2021Improving]: Improving Zero-Shot Voice Style Transfer via Disentangled Representation Learning. arXiv:2103.09420.
[^Oord2018Representation]: Representation Learning With Contrastive Predictive Coding. arXiv:1807.03748.
[^Veaux2016Superseded-CSTR]: Superseded-CSTR VCTK Corpus: English Multi-Speaker Corpus for CSTR Voice Cloning Toolkit. 
[^Yamamoto2020Parallel]: Parallel WaveGAN: A Fast Waveform Generation Model Based on Generative Adversarial Networks With Multi-Resolution Spectrogram. ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2020.
[^Kingma2014Adam]: Adam: A Method for Stochastic Optimization. arXiv:1412.6980.
[^Li2019Jasper]: Jasper: An End-to-End Convolutional Neural Acoustic Model. Interspeech 2019.
[^Benesty2009Pearson]: Pearson Correlation Coefficient. Noise Reduction in Speech Processing 2009.