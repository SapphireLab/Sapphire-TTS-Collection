# VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-Shot Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-Shot Voice Conversion."
- 作者:
  - 01 Disong Wang
  - 02 Liqun Deng
  - 03 Yu Ting Yeung
  - 04 Xiao Chen
  - 05 Xunying Liu
  - 06 Helen Meng
- 链接:
  - [ArXiv](https://arxiv.org/abs/2106.10132v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2106.10132v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2021.06.18_2106.10132v1_VQMIVC__Vector_Quantization_and_Mutual_Information-Based_Unsupervised_Speech_Representation_Disentanglement_for_One-Shot_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

One-shot voice conversion (VC), which performs conversion across arbitrary speakers with only a single target-speaker utterance for reference, can be effectively achieved by speech representation disentanglement.
Existing work generally ignores the correlation between different speech representations during training, which causes leakage of content information into the speaker representation and thus degrades VC performance.
To alleviate this issue, we employ vector quantization (VQ) for content encoding and introduce mutual information (MI) as the correlation metric during training, to achieve proper disentanglement of content, speaker and pitch representations, by reducing their inter-dependencies in an unsupervised manner.
Experimental results reflect the superiority of the proposed method in learning effective disentangled speech representations for retaining source linguistic content and intonation variations, while capturing target speaker characteristics.
In doing so, the proposed approach achieves higher speech naturalness and speaker similarity than current state-of-the-art one-shot VC systems.
Our code, pre-trained models and demo are available at \color{blue}\url{https://github.com/Wendison/VQMIVC}.

## 1·Introduction

Voice conversion (VC) is a technique used to modify para-linguistic factors of an utterance from a source speaker to sound like a target speaker.

Para-linguistic factors include speaker identity [^Mohammadi2017Overview], prosody [^Rentzos2003Transformation] and accent [^Oyamada2017Non-Native], etc.

In this paper, we focus on the conversion of speaker identity across arbitrary speakers under a one-shot scenario [^Liu2018Voice], [^Lu2019One-Shot], i.e., given only one target speaker's utterance for reference. 

Previous work that use methods based on speech representation disentanglement (SRD) [^Qian2019Autovc], [^Chou2019One-Shot], [^Wu2020Vqvc+] attempted to address one-shot VC by decomposing the speech into speaker and content representations, and then the speaker identity can be converted by changing the source speaker's representation to that of the target speaker.

However, it is difficult to measure the degree of SRD.

Besides, previous approaches generally do not impose correlation constraints between speaker and content representations during training, which results in leakage of content information into the speaker representation, leading to VC performance degradation.

To alleviate these issues, this paper proposes the vector quantization and mutual information-based VC (VQMIVC) approach, where mutual information (MI) measures the dependencies between different representations and can be effectively integrated into the training process to achieve SRD in an *unsupervised manner*.

Specifically, we first decompose an utterance into three factors: content, speaker and pitch, and then propose a VC system consisting of four components: (1) A content encoder using vector quantization with contrastive predictive coding (VQCPC) [^Niekerk2020Vector-Quantized], [^Baevski2019Vq-Wav2vec] to extract frame-level content representations from acoustic features; (2) A speaker encoder that takes in acoustic features to generate a single fixed-dimensional vector as the speaker representation; (3) A pitch extractor that is used to compute normalized fundamental frequency ($*F*_0$) at the utterance level as the pitch representation; and (4) A decoder that maps content, speaker and pitch representations to acoustic features.

During training, the VC system is optimized by minimizing VQCPC, reconstruction and MI losses.

VQCPC aims to explore local structures of speech, and MI reduces the inter-dependencies of different speech representations.

During inference, one-shot VC is achieved by only replacing the source speaker representation with the target speaker representation derived from a single target utterance.

The main contribution of this work lies in applying the combination of VQCPC and MI to achieve SRD, without any requirements of supervision information such as text transcriptions or speaker labels.

Extensive experiments have been conducted to thoroughly analyze the importance of MI, where information leakage issues can be significantly alleviated for enhanced SRD.
