Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme
Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, Mikhail Kudinov, Jiansheng Wei
Voice conversion is a common speech synthesis task which can be solved in different ways depending on a particular real-world scenario. The most challenging one often referred to as one-shot many-to-many voice conversion consists in copying the target voice from only one reference utterance in the most general case when both source and target speakers do not belong to the training dataset. We present a scalable high-quality solution based on diffusion probabilistic modeling and demonstrate its superior quality compared to state-of-the-art one-shot voice conversion approaches. Moreover, focusing on real-time applications, we investigate general principles which can make diffusion models faster while keeping synthesis quality at a high level. As a result, we develop a novel Stochastic Differential Equations solver suitable for various diffusion model types and generative tasks as shown through empirical studies and justify it by theoretical analysis.
Subjects:	Sound (cs.SD); Machine Learning (cs.LG); Machine Learning (stat.ML)
Cite as:	arXiv:2109.13821 [cs.SD]
 	(or arXiv:2109.13821v2 [cs.SD] for this version)

https://doi.org/10.48550/arXiv.2109.13821
Focus to learn more
Submission history
From: Mikhail Kudinov [view email]
[v1] Tue, 28 Sep 2021 15:48:22 UTC (1,853 KB)
[v2] Thu, 4 Aug 2022 10:25:40 UTC (1,854 KB)# Diffusion-Based Voice Conversion With Fast Maximum Likelihood Sampling Scheme

<details>
<summary>基本信息</summary>

- 标题: "Diffusion-Based Voice Conversion With Fast Maximum Likelihood Sampling Scheme."
- 作者:
  - 01 Vadim Popov
  - 02 Ivan Vovk
  - 03 Vladimir Gogoryan
  - 04 Tasnima Sadekova
  - 05 Mikhail Kudinov
  - 06 Jiansheng Wei
- 链接:
  - [ArXiv](https://arxiv.org/abs/2109.13821v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2109.13821v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2021.09.28_2109.13821v1_Diffusion-Based_Voice_Conversion_With_Fast_Maximum_Likelihood_Sampling_Scheme.pdf)
  - [ArXiv:2109.13821v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2021.09.28_2109.13821v2_Diffusion-Based_Voice_Conversion_With_Fast_Maximum_Likelihood_Sampling_Scheme.pdf)
  - [Publication] #TODO

</details>

## Abstract

Voice conversion is a common speech synthesis task which can be solved in different ways depending on a particular real-world scenario.
The most challenging one often referred to as one-shot many-to-many voice conversion consists in copying target voice from only one reference utterance in the most general case when both source and target speakers do not belong to the training dataset.
We present a scalable high-quality solution based on diffusion probabilistic modeling and demonstrate its superior quality compared to state-of-the-art one-shot voice conversion approaches.
Moreover, focusing on real-time applications, we investigate general principles which can make diffusion models faster while keeping synthesis quality at a high level.
As a result, we develop a novel Stochastic Differential Equations solver suitable for various diffusion model types and generative tasks as shown through empirical studies and justify it by theoretical analysis.
The code is publicly available at \url{https://github.com/huawei-noah/Speech-Backbones/tree/main/DiffVC}.

## 1·Introduction

\label{sec:intro}

Voice conversion (VC) is the task of copying the target speaker's voice while preserving the linguistic content of the utterance pronounced by the source speaker.

Practical VC applications often require a model which is able to operate in one-shot mode (i.e. when only one reference utterance is provided to copy the target speaker's voice) for any source and target speakers.

Such models are usually referred to as one-shot many-to-many models (or sometimes zero-shot many-to-many models, or just any-to-any VC models).

It is challenging to build such a model since it should be able to adapt to a new unseen voice having only one spoken utterance pronounced with it, so it was not until recently that successful one-shot VC solutions started to appear.

Conventional one-shot VC models are designed as autoencoders whose latent space ideally contains only the linguistic content of the encoded utterance while target voice identity information (usually taking shape of speaker embedding) is fed to the decoder as conditioning.

Whereas in the pioneering AutoVC model [^Qian2019AutoVC] only speaker embedding from the pre-trained speaker verification network was used as conditioning, several other models improved on AutoVC enriching conditioning with phonetic features such as pitch and loudness [^Qian2020F0-Consistent], [^Nercessian2020Improved], or training voice conversion and speaker embedding networks jointly [^Chou2019One-Shot].

Also, several papers [^Lin2021FragmentVC], [^Ishihara2020Attention-Based], [^Liu2021Any-to-Many] made use of attention mechanism to better fuse specific features of the reference utterance into the source utterance thus improving the decoder performance.

Apart from providing the decoder with sufficiently rich information, one of the main problems autoencoder VC models face is to disentangle source speaker identity from speech content in the encoder.

Some models [^Qian2019AutoVC], [^Qian2020F0-Consistent], [^Nercessian2020Improved] solve this problem by introducing an information bottleneck.

Among other popular solutions of the disentanglement problem one can mention applying vector quantization technique to the content information [^Wu2020Vqvc+], [^Wang2021Vqmivc], utilizing features of Variational AutoEncoders [^Luong2021Many-to-Many], [^Saito2018Non-Parallel], [^Chou2019One-Shot], introducing instance normalization layers [^Chou2019One-Shot], [^Chen2021Again-Vc], and using Phonetic Posteriorgrams (PPGs) [^Nercessian2020Improved], [^Liu2021Any-to-Many].

The model we propose in this paper solves the disentanglement problem by employing the encoder predicting "average voice'': it is trained to transform mel features corresponding to each phoneme into mel features corresponding to this phoneme averaged across a large multi-speaker dataset.

As for decoder, in our VC model, it is designed as a part of a Diffusion Probabilistic Model (DPM) since this class of generative models has shown very good results in speech-related tasks like raw waveform generation [^Chen2021WaveGrad], [^Kong2021DiffWave] and mel feature generation [^Popov2021Grad-TTS], [^Jeong2021Diff-TTS].

However, this decoder choice poses a problem of slow inference because DPM forward pass scheme is iterative and to obtain high-quality results it is typically necessary to run it for hundreds of iterations [^Ho2020Denoising], [^Nichol2021Improved].

Addressing this issue, we develop a novel inference scheme that significantly reduces the number of iterations sufficient to produce samples of decent quality and does not require model re-training.

Although several attempts have been recently made to reduce the number of DPM inference steps [^Song2021Denoising], [^San-Roman2021Noise], [^Watson2021Learning], [^Kong2021On], [^Chen2021WaveGrad], most of them apply to some particular types of DPMs.

In contrast, our approach generalizes to all popular kinds of DPMs and has a strong connection with likelihood maximization.

This paper has the following structure: in Section [sec:vc_model](#sec:vc_model) we present a one-shot many-to-many VC model and describe DPM it relies on; Section [sec:ml_inference](#sec:ml_inference) introduces a novel DPM sampling scheme and establishes its connection with likelihood maximization; the experiments regarding voice conversion task as well as those demonstrating the benefits of the proposed sampling scheme are described in Section [sec:exp](#sec:exp); we conclude in Section [sec:outro](#sec:outro).

## 2·Voice Conversion Diffusion Model

\label{sec:vc_model}

As with many other VC models, the one we propose belongs to the family of autoencoders.

In fact, any conditional DPM with data-dependent prior (i.e. terminal distribution of forward diffusion) can be seen as such: forward diffusion gradually adding Gaussian noise to data can be regarded as encoder while reverse diffusion trying to remove this noise acts as a decoder.

DPMs are trained to minimize the distance (expressed in different terms for different model types) between the trajectories of forward and reverse diffusion processes thus, speaking from the perspective of autoencoders, minimizing reconstruction error.

Data-dependent priors have been proposed by [^Popov2021Grad-TTS] and [^Lee2021PriorGrad], and we follow the former paper due to the flexibility of the continuous DPM framework used there.

Our approach is summarized in Figure~[pic:main](#pic:main).

![](images/scheme.png)

<a id="pic:main">VC model training and inference. $Y$ stands for the training mel-spectrogram at training and the target mel-spectrogram at inference.

Speaker conditioning in the decoder is enabled by the speaker conditioning network $g_{t}(Y)$ where $Y=\{Y_{t}\}_{t\in[0, 1]}$ is the whole forward diffusion trajectory starting at $Y_{0}$.

Dotted arrows denote operations performed only at training.</a>

### Encoder

\label{subsec:encoder}

We choose average phoneme-level mel features as speaker-independent speech representation.

To train the encoder to convert input mel-spectrograms into those of "average voice'', we take three steps: (i) first, we apply Montreal Forced Aligner [^McAuliffe2017Montreal] to a large-scale multi-speaker LibriTTS dataset [^Zen2019LibriTTS] to align speech frames with phonemes;
(ii) next, we obtain average mel features for each particular phoneme by aggregating its mel features across the whole LibriTTS dataset;
(iii) the encoder is then trained to minimize mean square error between output mel-spectrograms and ground truth "average voice'' mel-spectrograms (i.e. input mel-spectrograms where each phoneme mel feature is replaced with the average one calculated on the previous step).

The encoder has exactly the same Transformer-based architecture used in Grad-TTS [^Popov2021Grad-TTS] except that its inputs are mel features rather than character or phoneme embeddings.

Note that unlike Grad-TTS the encoder is trained separately from the decoder described in the next section.

### Decoder

\label{subsec:decoder}

Whereas the encoder parameterizes the terminal distribution of the forward diffusion (i.e. the prior), the reverse diffusion is parameterized with the decoder.

Following [^Song2021Score-Based] we use It\^o calculus and define diffusions in terms of stochastic processes rather than discrete-time Markov chains.

The general DPM framework we utilize consists of forward and reverse diffusions given by the following Stochastic Differential Equations (SDEs):

$$

\label{eq:vc_fwd_sde}
dX_{t}=\frac{1}{2}\beta_{t}(\bar{X} - X_{t})dt + \sqrt{\beta_{t}}d\overrightarrow{W_{t}}\ ,

$$

$$

\label{eq:vc_rev_sde}
d\hat{X}_{t}=\left(\frac{1}{2}(\bar{X} - \hat{X}_{t}) - s_{\theta}(\hat{X}_{t}, \bar{X}, t)\right)\beta_{t}dt + \sqrt{\beta_{t}}d\overleftarrow{W_{t}}\ ,

$$

where $t\in [0,1]$, $\overrightarrow{W}$ and $\overleftarrow{W}$ are two independent Wiener processes in $\mathbb{R}^{n}$, $\beta_{t}$ is non-negative function referred to as *noise schedule*, $s_{\theta}$ is the score function with parameters $\theta$ and $\bar{X}$ is $n$-dimensional vector.

It can be shown [^Popov2021Grad-TTS] that the forward SDE ([eq:vc_fwd_sde](#eq:vc_fwd_sde)) allows for explicit solution:

$$

\label{eq:vc_solution}
\Law(X_{t}|X_{0}) = \mathcal{N}\left(e^{-\frac{1}{2}\int_{0}^{t}{\beta_{s}ds}}X_{0} + \left(1-e^{-\frac{1}{2}\int_{0}^{t}{\beta_{s}ds}}\right)\bar{X}, \left(1 - e^{-\int_{0}^{t}{\beta_{s}ds}}\right)\ide\right),

$$

where $\ide$ is $n\times n$ identity matrix.

Thus, if noise follows linear schedule $\beta_{t}=\beta_{0} + t(\beta_{1}-\beta_{0})$ for $\beta_{0}$ and $\beta_{1}$ such that $e^{-\int_{0}^{1}{\beta_{s}ds}}$ is close to zero, then $\Law{(X_{1})}$ is close to $\mathcal{N}(\bar{X},\ide)$ which is the prior in this DPM.

The reverse diffusion ([eq:vc_rev_sde](#eq:vc_rev_sde)) is trained by minimizing weighted $L_{2}$ loss:

$$

\label{eq:vc_training}
\theta^{*}=\argmin_{\theta}{\mathcal{L}(\theta)}=\argmin_{\theta}{\int_{0}^{1}{\lambda_{t}\mathbb{E}_{X_{0}, X_{t}}\Vert s_{\theta}(X_{t}, \bar{X}, t) - \nabla{\log{p_{t|0}(X_{t}|X_{0})}}\Vert_{2}^{2}}}dt,

$$

where $p_{t|0}(X_{t}|X_{0})$ is the probability density function (pdf) of the conditional distribution ([eq:vc_solution](#eq:vc_solution)) and $\lambda_{t}=1 - e^{-\int_{0}^{t}{\beta_{s}ds}}$.

The distribution ([eq:vc_solution](#eq:vc_solution)) is Gaussian, so we have

$$

\label{eq:vc_logp}
\nabla\log{p_{t|0}(X_{t}|X_{0})} = -\frac{X_{t}-X_{0}e^{-\frac{1}{2}\int_{0}^{t}{\beta_{s}ds}}-\bar{X}(1-e^{-\frac{1}{2}\int_{0}^{t}{\beta_{s}ds}})}{1 - e^{-\int_{0}^{t}{\beta_{s}ds}}}.

$$

At training, time variable $t$ is sampled uniformly from $[0,1]$, noisy samples $X_{t}$ are generated according to the formula ([eq:vc_solution](#eq:vc_solution)) and the formula ([eq:vc_logp](#eq:vc_logp)) is used to calculate loss function $\mathcal{L}$ on these samples.

Note that $X_{t}$ can be sampled without the necessity to calculate intermediate values $\{X_{s}\}_{0<s<t}$ which makes optimization task ([eq:vc_training](#eq:vc_training)) time and memory efficient.

A well-trained reverse diffusion ([eq:vc_rev_sde](#eq:vc_rev_sde)) has trajectories that are close to those of the forward diffusion ([eq:vc_fwd_sde](#eq:vc_fwd_sde)), so generating data with this DPM can be performed by sampling $\hat{X}_{1}$ from the prior $\mathcal{N}(\bar{X}, \ide)$ and solving SDE ([eq:vc_rev_sde](#eq:vc_rev_sde)) backwards in time.

The described above DPM was introduced by [^Popov2021Grad-TTS] for text-to-speech task and we adapt it for our purposes.

We put $\bar{X}=\varphi(X_{0})$ where $\varphi$ is the encoder, i.e. $\bar{X}$ is the "average voice'' mel-spectrogram which we want to transform into that of the target voice.

We condition the decoder $s_{\theta}=s_{\theta}(\hat{X}_{t},\bar{X},g_{t}(Y),t)$ on some trainable function $g_{t}(Y)$ to provide it with information about the target speaker ($Y$ stands for forward trajectories of the target mel-spectrogram at inference and the ones of the training mel-spectrogram at training).

This function is a neural network trained jointly with the decoder.

We experimented with three input types for this network:

-  *d-only* -- the input is the speaker embedding extracted from the target mel-spectrogram $Y_{0}$ with the pre-trained speaker verification network employed in [^Jia2018Transfer];

-  *wodyn* -- in addition, the noisy target mel-spectrogram $Y_{t}$ is used as input;

-  *whole* -- in addition, the whole dynamics of the target mel-spectrogram under forward diffusion $\{Y_{s}|s=0.5/15,1.5/15,..,14.5/15\}$ is used as input.

The decoder architecture is based on U-Net [^Ronneberger2015U-Net] and is the same as in Grad-TTS but with four times more channels to better capture the whole range of human voices.

The speaker conditioning network $g_{t}(Y)$ is composed of $2$D convolutions and MLPs and described in detail in Appendix [app:h](#app:h).

Its output is $128$-dimensional vector which is broadcast-concatenated to the concatenation of $\hat{X}_{t}$ and $\bar{X}$ as additional $128$ channels.

### Related VC models

\label{subsec:related}

To the best of our knowledge, there exist two diffusion-based voice conversion models: VoiceGrad [^Kameoka2020VoiceGrad] and DiffSVC [^Liu2021DiffSVC].

The one we propose differs from them in several important aspects.

First, neither of the mentioned papers considers a one-shot many-to-many voice conversion scenario.

Next, these models take no less than $100$ reverse diffusion steps at inference while we pay special attention to reducing the number of iterations (see Section [sec:ml_inference](#sec:ml_inference)) achieving good quality with as few as $6$ iterations.

Furthermore, VoiceGrad performs voice conversion by running Langevin dynamics starting from the source mel-spectrogram, thus implicitly assuming that forward diffusion trajectories starting from the mel-spectrogram we want to synthesize are likely to pass through the neighborhood of the source mel-spectrogram on their way to Gaussian noise.

Such an assumption allowing to have only one network instead of encoder-decoder architecture is too strong and hardly holds for real voices.

Finally, DiffSVC performs singing voice conversion and relies on PPGs as speaker-independent speech representation.

## 3·Maximum Likelihood SDE Solver

\label{sec:ml_inference}

In this section, we develop a fixed-step first-order reverse SDE solver that maximizes the log-likelihood of sample paths of the forward diffusion.

This solver differs from general-purpose Euler-Maruyama SDE solver [^Kloeden1992Numerical] by infinitesimally small values which can however become significant when we sample from diffusion model using a few iterations.

Consider the following forward and reverse SDEs defined in Euclidean space $\mathbb{R}^{n}$ for $t\in [0,1]$:

$$

\label{eq:fwd_rev_sde}
dX_{t}=-\frac{1}{2}\beta_{t}X_{t}dt + \sqrt{\beta_{t}}d\overrightarrow{W_{t}} \ \ (F), \ \ \ d\hat{X}_{t}=\left(-\frac{1}{2}\beta_{t}\hat{X}_{t} - \beta_{t}s_{\theta}(\hat{X}_t, t)\right)dt + \sqrt{\beta_{t}}d\overleftarrow{W_{t}} \ \ (R),

$$

where $\overrightarrow{W}$ is a forward Wiener process (i.e. its forward increments $\overrightarrow{W_{t}} - \overrightarrow{W_{s}}$ are independent of $\overrightarrow{W_{s}}$ for $t > s$) and $\overleftarrow{W}$ is a backward Wiener process (i.e. backward increments $\overleftarrow{W_{s}} - \overleftarrow{W_{t}}$ are independent of $\overleftarrow{W_{t}}$ for $s < t$).

Following [^Song2021Score-Based] we will call DPM ([eq:fwd_rev_sde](#eq:fwd_rev_sde)) Variance Preserving (VP).

For simplicity we will derive maximum likelihood solver for this particular type of diffusion models.

The equation ([eq:vc_fwd_sde](#eq:vc_fwd_sde)) underlying VC diffusion model described in Section [sec:vc_model](#sec:vc_model) can be transformed into the equation ([eq:fwd_rev_sde](#eq:fwd_rev_sde)-F) by a constant shift and we will call such diffusion models Mean Reverting Variance Preserving (MR-VP).

VP model analysis carried out in this section can be easily extended (see Appendices [app:mr_vp](#app:mr_vp), [app:sub_vp](#app:sub_vp) and [app:ve](#app:ve)) to MR-VP model as well as to other common diffusion model types such as sub-VP and VE described by [^Song2021Score-Based].

The forward SDE ([eq:fwd_rev_sde](#eq:fwd_rev_sde)-F) allows for explicit solution:

$$

\label{eq:xt_distribution}
\Law(X_{t} | X_{s}) = \mathcal{N}(\gamma_{s,t}X_{s}, (1-\gamma_{s,t}^2)\ide), \ \ \ \ \gamma_{s,t} = \exp{\left(-\frac{1}{2}\int_{s}^{t}{\beta_{u}du}\right)},

$$

for all $0\leq s < t \leq 1$.

This formula is derived by means of It\^o calculus in Appendix [app:sde_solution](#app:sde_solution).

The reverse SDE ([eq:fwd_rev_sde](#eq:fwd_rev_sde)-R) parameterized with a neural network $s_{\theta}$ is trained to approximate gradient of the log-density of noisy data $X_{t}$:

$$

\label{eq:objective}
\theta^{*}=\argmin_{\theta}{\int_{0}^{1}{\lambda_{t}\mathbb{E}_{X_{t}}\Vert s_{\theta}(X_{t}, t) - \nabla{\log{p_{t}(X_{t})}}\Vert_{2}^{2}}}dt,

$$

where the expectation is taken with respect to noisy data distribution $\Law(X_{t})$ with pdf $p_{t}(\cdot)$ and $\lambda_{t}$ is some positive weighting function.

Note that certain Lipschitz constraints should be satisfied by coefficients of SDEs ([eq:fwd_rev_sde](#eq:fwd_rev_sde)) to guarantee existence of strong solutions [^Liptser1978Statistics], and throughout this section we assume these conditions are satisfied as well as those from [^Anderson1982Reverse-Time] which guarantee that paths $\hat{X}$ generated by the reverse SDE ([eq:fwd_rev_sde](#eq:fwd_rev_sde)-R) for the optimal $\theta^{*}$ equal forward SDE ([eq:fwd_rev_sde](#eq:fwd_rev_sde)-F) paths $X$ in distribution.

The generative procedure of a VP DPM consists in solving the reverse SDE ([eq:fwd_rev_sde](#eq:fwd_rev_sde)-R) backwards in time starting from $\hat{X}_{1}\sim \mathcal{N}(0,\ide)$.

Common Euler-Maruyama solver introduces discretization error [^Kloeden1992Numerical] which may harm sample quality when the number of iterations is small.

At the same time, it is possible to design unbiased [^Henry-Labordère2017Unbiased] or even exact [^Beskos2005Exact] numerical solvers for some particular SDE types.

The Theorem~[th:main](#th:main) shows that in the case of diffusion models we can make use of the forward diffusion ([eq:fwd_rev_sde](#eq:fwd_rev_sde)-F) and propose a reverse SDE solver which is better than the general-purpose Euler-Maruyama one in terms of likelihood.

The solver proposed in the Theorem [th:main](#th:main) is expressed in terms of the values defined as follows:

$$

\label{eq:notation_gamma}
\mu_{s,t}=\gamma_{s,t}\frac{1-\gamma_{0,s}^{2}}{1-\gamma_{0,t}^{2}}, \ \ \ \  \nu_{s,t}=\gamma_{0,s}\frac{1-\gamma_{s,t}^{2}}{1-\gamma_{0,t}^{2}}, \ \ \ \
\sigma_{s,t}^{2}=\frac{(1-\gamma_{0,s}^{2})(1-\gamma_{s,t}^{2})}{1-\gamma_{0,t}^{2}},

$$

$$

\begin{split}
\label{eq:notation_correction}
\kappa_{t,h}^{*} =& \frac{\nu_{t-h,t}(1-\gamma_{0,t}^{2})}{\gamma_{0,t}\beta_{t}h} - 1,  \ \ \ \
\omega_{t,h}^{*}=\frac{\mu_{t-h,t}-1}{\beta_{t}h} + \frac{1+\kappa_{t,h}^{*}}{1-\gamma_{0,t}^{2}} -\frac{1}{2},
\\& (\sigma^{*}_{t,h})^{2} = \sigma^{2}_{t-h,t} + \frac{1}{n}\nu_{t-h,t}^{2}\mathbb{E}_{X_{t}}\left[\Tr{\left(\Var{(X_{0}|X_{t})}\right)}\right],
\end{split}

$$

where $n$ is data dimensionality, $\Var{(X_{0}|X_{t})}$ is the covariance matrix of the conditional data distribution $\Law(X_{0}|X_{t})$ (so, $\Tr(\Var{(X_{0}|X_{t})})$ is the overall variance across all $n$ dimensions) and the expectation $\mathbb{E}_{X_{t}}[\cdot]$ is taken with respect to the unconditional noisy data distribution $\Law(X_{t})$.

\begin{theorem}
\label{th:main}
Consider a DPM characterized by SDEs ([eq:fwd_rev_sde](#eq:fwd_rev_sde)) with reverse diffusion trained till optimality.

Let $N\in \mathbb{N}$ be any natural number and $h=1/N$.

Consider the following class of fixed step size $h$ reverse SDE solvers parameterized with triplets of real numbers \{$(\hat{\kappa}_{t,h}, \hat{\omega}_{t,h}, \hat{\sigma}_{t,h})|t=h,2h,..,1$\}:

$$

\label{eq:sde_solver}
\hat{X}_{t-h} = \hat{X}_{t} + \beta_{t}h\left(\left(\frac{1}{2}+\hat{\omega}_{t,h}\right)\hat{X}_{t} + (1+\hat{\kappa}_{t,h})s_{\theta^{*}}(\hat{X}_{t},t)\right) + \hat{\sigma}_{t,h}\xi_{t},

$$

where $\theta^{*}$ is given by ([eq:objective](#eq:objective)), $t=1,1-h,..,h$ and $\xi_{t}$ are i.i.d. samples from $\mathcal{N}(0,\ide)$.

Then:

\begin{enumerate}

- [(i)] Log-likelihood of sample paths $X=\{X_{kh}\}_{k=0}^{N}$ under generative model $\hat{X}$ is maximized for $\hat{\kappa}_{t,h}=\kappa_{t,h}^{*}$, $\hat{\omega}_{t,h}=\omega_{t,h}^{*}$ and $\hat{\sigma}_{t,h}=\sigma^{*}_{t,h}$.

- [(ii)] Assume that the SDE solver ([eq:sde_solver](#eq:sde_solver)) starts from random variable $\hat{X}_{1}\sim \Law{(X_{1})}$.

If $X_{0}$ is a constant or a Gaussian random variable with diagonal isotropic covariance matrix (i.e. $\delta^{2}\ide$ for $\delta>0$), then generative model $\hat{X}$ is exact for $\hat{\kappa}_{t,h}=\kappa_{t,h}^{*}$, $\hat{\omega}_{t,h}=\omega_{t,h}^{*}$ and $\hat{\sigma}_{t,h}=\sigma^{*}_{t,h}$.
\end{enumerate}

\end{theorem}

The Theorem [th:main](#th:main) provides an improved DPM sampling scheme which comes at no additional computational cost compared to standard methods (except for data-dependent term in $\sigma^{*}$ as discussed in Section [subsec:sampling](#subsec:sampling)) and requires neither model re-training nor extensive search on noise schedule space.

The proof of this theorem is given in Appendix [app:th_proof](#app:th_proof).

Note that it establishes optimality of the reverse SDE solver ([eq:sde_solver](#eq:sde_solver)) with the parameters ($[eq:notation_correction](#eq:notation_correction)$) in terms of likelihood of *discrete* paths $X=\{X_{kh}\}_{k=0}^{N}$ while the optimality of *continuous* model ([eq:fwd_rev_sde](#eq:fwd_rev_sde)-R) on *continuous* paths $\{X_{t}\}_{t\in[0,1]}$ is guaranteed for a model with parameters $\theta=\theta^{*}$ as shown in [^Song2021Score-Based].

The class of reverse SDE solvers considered in the Theorem [th:main](#th:main) is rather broad: it is the class of all fixed-step solvers whose increments at time $t$ are linear combination of $\hat{X}_{t}$, $s_{\theta}(\hat{X}_{t},t)$ and Gaussian noise with zero mean and diagonal isotropic covariance matrix.

As a particular case it includes Euler-Maruyama solver ($\hat{\kappa}_{t,h}\equiv 0$, $\hat{\omega}_{t,h}\equiv 0$, $\hat{\sigma}_{t,h}\equiv\sqrt{\beta_{t}h}$) and for fixed $t$ and $h\to 0$ we have $\kappa^{*}_{t,h}=\bar{o}(1)$, $\omega^{*}_{t,h}=\bar{o}(1)$ and $\sigma^{*}_{t,h}=\sqrt{\beta_{t}h}(1 + \bar{o}(1))$ (the proof is given in Appendix [app:asymptotics](#app:asymptotics)), so the optimal SDE solver significantly differs from general-purpose Euler-Maruyama solver only when $N$ is rather small or $t$ has the same order as $h$, i.e. on the final steps of DPM inference.

Appendix [app:toy](#app:toy) contains toy examples demonstrating the difference of the proposed optimal SDE solver and Euler-Maruyama one depending on step size.

The result *(ii)* from the Theorem [th:main](#th:main) strengthens the result *(i)* for some particular data distributions, but it may seem useless since in practice data distribution is far from being constant or Gaussian.

However, in case of generation with strong conditioning (e.g. mel-spectrogram inversion) the assumptions on the data distribution may become viable: in the limiting case when our model is conditioned on $c=\psi(X_{0})$ for an injective function $\psi$, random variable $X_{0}|c$ becomes a constant $\psi^{-1}(c)$.

## 4·Experiments

\label{sec:exp}

We trained two groups of models: *Diff-VCTK* models on VCTK [^Yamagishi2019Cstr] dataset containing $109$ speakers ($9$ speakers were held out for testing purposes) and *Diff-LibriTTS* models on LibriTTS [^Zen2019LibriTTS] containing approximately $1100$ speakers ($10$ speakers were held out).

For every model both encoder and decoder were trained on the same dataset.

Training hyperparameters, implementation and data processing details can be found in Appendix [app:details](#app:details).

For mel-spectrogram inversion, we used the pre-trained universal HiFi-GAN vocoder [^Kong2020HiFi-Gan] operating at $22.05$kHz.

All subjective human evaluation was carried out on Amazon Mechanical Turk (AMT) with Master assessors to ensure the reliability of the obtained Mean Opinion Scores (MOS).

In all AMT tests we considered unseen-to-unseen conversion with $25$ unseen (for both *Diff-VCTK* and *Diff-LibriTTS*) speakers: $9$ VCTK speakers, $10$ LibriTTS speakers and $6$ internal speakers.

For VCTK source speakers we also ensured that source phrases were unseen during training.

We place other details of listening AMT tests in Appendix [app:subj](#app:subj).

A small subset of speech samples used in them is available at our demo page \url{https://diffvc-fast-ml-solver.github.io} which we encourage to visit.

<a id="tab:conditioning">Input types for speaker conditioning $g_{t}(Y)$ compared in terms of speaker similarity.</a>

As for sampling, we considered the following class of reverse SDE solvers:

$$

\label{eq:vc_solver}
\hat{X}_{t-h} = \hat{X}_{t} + \beta_{t}h\left(\left(\frac{1}{2}+\hat{\omega}_{t,h}\right)(\hat{X}_{t} - \bar{X}) + (1+\hat{\kappa}_{t,h})s_{\theta}(\hat{X}_{t},\bar{X},g_{t}(Y),t)\right) + \hat{\sigma}_{t,h}\xi_{t},

$$

where $t=1,1-h,..,h$ and $\xi_{t}$ are i.i.d. samples from $\mathcal{N}(0,\ide)$.

For $\hat{\kappa}_{t,h}=\kappa^{*}_{t,h}$, $\hat{\omega}_{t,h}=\omega^{*}_{t,h}$ and $\hat{\sigma}_{t,h}=\sigma^{*}_{t,h}$ (where $\kappa^{*}_{t,h}$, $\omega^{*}_{t,h}$ and $\sigma^{*}_{t,h}$ are given by ([eq:notation_correction](#eq:notation_correction))) it becomes maximum likelihood reverse SDE solver for MR-VP DPM ([eq:vc_fwd_sde](#eq:vc_fwd_sde)-[eq:vc_rev_sde](#eq:vc_rev_sde)) as shown in Appendix [app:mr_vp](#app:mr_vp).

In practice it is not trivial to estimate variance of the conditional distribution $\Law{(X_{0}|X_{t})}$, so we skipped this term in $\sigma^{*}_{t,h}$ assuming this variance to be rather small because of strong conditioning on $g_{t}(Y)$ and just used $\hat{\sigma}_{t,h}=\sigma_{t-h,t}$ calling this sampling method *ML-N* ($N=1/h$ is the number of SDE solver steps).

We also experimented with Euler-Maruyama solver *EM-N* (i.e. $\hat{\kappa}_{t,h}=0$, $ \hat{\omega}_{t,h}=0$, $\hat{\sigma}_{t,h}=\sqrt{\beta_{t}h}$) and "probability flow sampling'' from [^Song2021Score-Based] which we denote by *PF-N* ($\hat{\kappa}_{t,h}=-0.5$, $ \hat{\omega}_{t,h}=0$, $\hat{\sigma}_{t,h}=0$).

### Speaker conditioning analysis

\label{subsec:ablation}

For each dataset we trained three models -- one for each input type for the speaker conditioning network $g_{t}(Y)$ (see Section [subsec:decoder](#subsec:decoder)).

Although these input types had much influence neither on speaker similarity nor on speech naturalness, we did two experiments to choose the best models (one for each training dataset) in terms of speaker similarity for further comparison with baseline systems.

We compared voice conversion results (produced by *ML-30* sampling scheme) on $92$ source-target pairs.

AMT workers were asked which of three models (if any) sounded most similar to the target speaker and which of them (if any) sounded least similar.

For *Diff-VCTK* and *Diff-LibriTTS* models each conversion pair was evaluated $4$ and $5$ times respectively.

Table~[tab:conditioning](#tab:conditioning) demonstrates that for both Â *Diff-VCTK* and *Diff-LibriTTS* the best option is *wodyn*, i.e. to condition the decoder at time $t$ on the speaker embedding together with the noisy target mel-spectrogram $Y_{t}$.

Conditioning on $Y_{t}$ allows making use of diffusion-specific information of how the noisy target sounds whereas embedding from the pre-trained speaker verification network contains information only about the clean target.

Taking these results into consideration, we used *Diff-VCTK-wodyn* and *Diff-LibriTTS-wodyn* in the remaining experiments.

### Any-to-any voice conversion

\label{subsec:any2any}

<a id="tab:main_vctk">Subjective evaluation (MOS) of one-shot VC models trained on VCTK.

Ground truth recordings were evaluated only for VCTK speakers.</a>

<a id="tab:main_large">Subjective evaluation (MOS) of one-shot VC models trained on large-scale datasets.</a>

We chose four recently proposed VC models capable of one-shot many-to-many synthesis as the baselines:

-  *AGAIN-VC* [^Chen2021Again-Vc], an improved version of a conventional autoencoder AdaIN-VC solving the disentanglement problem by means of instance normalization;

-  *FragmentVC* [^Lin2021FragmentVC], an attention-based model relying on wav2vec 2.0 [^Baevski2020Wav2vec] to obtain speech content from the source utterance;

-  *VQMIVC* [^Wang2021Vqmivc], state-of-the-art approach among those employing vector quantization techniques;

-  *BNE-PPG-VC* [^Liu2021Any-to-Many], an improved variant of PPG-based VC models combining a bottleneck feature extractor obtained from a phoneme recognizer with a seq2seq-based synthesis module.

As shown in [^Kim2021Assem-Vc], PPG-based VC models provide high voice conversion quality competitive even with that of the state-of-the-art VC models taking text transcription corresponding to the source utterance as input.

Therefore, we can consider *BNE-PPG-VC* a state-of-the-art model in our setting.

Baseline voice conversion results were produced by the pre-trained VC models provided in official GitHub repositories.

Since only *BNE-PPG-VC* has the model pre-trained on a large-scale dataset (namely, LibriTTS + VCTK), we did two subjective human evaluation tests: the first one comparing *Diff-VCTK* with *AGAIN-VC*, *FragmentVC* and *VQMIVC* trained on VCTK and the second one comparing *Diff-LibriTTS* with *BNE-PPG-VC*.

The results of these tests are given in Tables [tab:main_vctk](#tab:main_vctk) and [tab:main_large](#tab:main_large) respectively.

Speech naturalness and speaker similarity were assessed separately.

AMT workers evaluated voice conversion quality on $350$ source-target pairs on $5$-point scale.

In the first test, each pair was assessed $6$ times on average both in speech naturalness and speaker similarity evaluation; as for the second one, each pair was assessed $8$ and $9$ times on average in speech naturalness and speaker similarity evaluation correspondingly.

No less than $41$ unique assessors took part in each test.

Table~[tab:main_vctk](#tab:main_vctk) demonstrates that our model performs significantly better than the baselines both in terms of naturalness and speaker similarity even when $6$ reverse diffusion iterations are used.

Despite working almost equally well on VCTK speakers, the best baseline *VQMIVC* shows poor performance on other speakers perhaps because of not being able to generalize to different domains with lower recording quality.

Although *Diff-VCTK* performance also degrades on non-VCTK speakers, it achieves good speaker similarity of MOS $3.6$ on VCTK ones when *ML-30* sampling scheme is used and only slightly worse MOS $3.5$ when $5$x less iterations are used at inference.

<a id="tab:sampling">Reverse SDE solvers compared in terms of FID. $N$ is the number of SDE solver steps.</a>

Table~[tab:main_large](#tab:main_large) contains human evaluation results of *Diff-LibriTTS* for four sampling schemes: *ML-30* with $30$ reverse SDE solver steps and *ML-6*, *EM-6* and *PF-6* with $6$ steps of reverse diffusion.

The three schemes taking $6$ steps achieved real-time factor (RTF) around $0.1$ on GPU (i.e. inference was $10$ times faster than real time) while the one taking $30$ steps had RTF around $0.5$.

The proposed model *Diff-LibriTTS-ML-30* and the baseline *BNE-PPG-VC* show the same performance on the VCTK test set in terms of speech naturalness the latter being slightly better in terms of speaker similarity which can perhaps be explained by the fact that *BNE-PPG-VC* was trained on the union of VCTK and LibriTTS whereas our model was trained only on LibriTTS.

As for the whole test set containing unseen LibriTTS and internal speakers also, *Diff-LibriTTS-ML-30* outperforms *BNE-PPG-VC* model achieving MOS $4.0$ and $3.4$ in terms of speech naturalness and speaker similarity respectively.

Due to employing PPG extractor trained on a large-scale ASR dataset LibriSpeech [^Panayotov2015Librispeech], *BNE-PPG-VC* has fewer mispronunciation issues than our model but synthesized speech suffers from more sonic artifacts.

This observation makes us believe that incorporating PPG features in the proposed diffusion VC framework is a promising direction for future research.

Table~[tab:main_large](#tab:main_large) also demonstrates the benefits of the proposed maximum likelihood sampling scheme over other sampling methods for a small number of inference steps: only *ML-N* scheme allows us to use as few as $N=6$ iterations with acceptable quality degradation of MOS $0.2$ and $0.1$ in terms of naturalness and speaker similarity respectively while two other competing methods lead to much more significant quality degradation.

### Maximum likelihood sampling

\label{subsec:sampling}

![](images/em.png)

<a id="pic:cifar">CIFAR-$10$ images randomly sampled from VP DPM by running $10$ reverse diffusion steps with the following schemes (from left to right): "euler-maruyama'', "probability flow'', "maximum likelihood ($\tau=0.5$)'', "maximum likelihood ($\tau=1.0$)''.</a>

To show that the maximum likelihood sampling scheme proposed in Section [sec:ml_inference](#sec:ml_inference) generalizes to different tasks and DPM types, we took the models trained by [^Song2021Score-Based] on CIFAR-$10$ image generation task and compared our method with other sampling schemes described in that paper in terms of Fr\'{e}chet Inception Distance (FID).

The main difficulty in applying maximum likelihood SDE solver is estimating data-dependent term $\mathbb{E}[\Tr{(\Var{(X_{0}|X_{t})})}]$ in $\sigma^{*}_{t,h}$.

Although in the current experiments we just set this term to zero, we can think of two possible ways to estimate it: (i) approximate $\Var{(X_{0}|X_{t})}$ with $\Var{(\hat{X}_{0}|\hat{X}_{t}=X_{t})}$: sample noisy data $X_{t}$, solve reverse SDE with sufficiently small step size starting from terminal condition $\hat{X}_{t}=X_{t}$ several times, and calculate sample variance of the resulting solutions at initial points $\hat{X}_{0}$; (ii) use the formula ([eq:gaussian_revert](#eq:gaussian_revert)) from Appendix [app:th_proof](#app:th_proof) to calculate $\Var{(X_{0}|X_{t})}$ assuming that $X_{0}$ is distributed normally with mean and variance equal to sample mean and sample variance computed on the training dataset.

Experimenting with these techniques and exploring new ones seems to be an interesting future research direction.

Another important practical consideration is that the proposed scheme is proven to be optimal only for score matching networks trained till optimality.

Therefore, in the experiments whose results are reported in Table~[tab:sampling](#tab:sampling) we apply maximum likelihood sampling scheme only when $t\leq\tau$ while using standard Euler-Maruyama solver for $t>\tau$ for some hyperparameter $\tau\in [0,1]$.

Such a modification relies on the assumption that score matching network is closer to being optimal for smaller noise.

Table [tab:sampling](#tab:sampling) shows that despite likelihood and FID are two metrics that do not perfectly correlate [^Song2021Maximum], in most cases our maximum likelihood SDE solver performs best in terms of FID.

Also, it is worth mentioning that although $\tau=1$ is always rather a good choice, tuning this hyperparameter can lead to even better performance.

One can find randomly chosen generated images for various sampling methods in Figure~[pic:cifar](#pic:cifar).

## 5·Conclusion

\label{sec:outro}

In this paper, the novel one-shot many-to-many voice conversion model has been presented.

Its encoder design and powerful diffusion-based decoder made it possible to achieve good results both in terms of speaker similarity and speech naturalness even on out-of-domain unseen speakers.

Subjective human evaluation verified that the proposed model delivers scalable VC solution with competitive performance.

Furthermore, aiming at fast synthesis, we have developed and theoretically justified the novel sampling scheme.

The main idea behind it is to modify the general-purpose Euler-Maruyama SDE solver so as to maximize the likelihood of discrete sample paths of the forward diffusion.

Due to the proposed sampling scheme, our VC model is capable of high-quality voice conversion with as few as $6$ reverse diffusion steps.

Moreover, experiments on the image generation task show that all known diffusion model types can benefit from the proposed SDE solver.

\bibliography{diffusion_paper}
\bibliographystyle{iclr2022}

\newpage

\appendix

## 6·Forward VP SDE Solution

\label{app:sde_solution}

Since function $\gamma_{0,t}^{-1}X_{t}$ is linear in $X_{t}$, taking its differential does not require second order derivative term in It\^o's formula:

$$

\begin{split}
d(\gamma_{0,t}^{-1}X_{t})&=d\left(e^{\frac{1}{2}\int_{0}^{t}{\beta_{u}du}}X_{t}\right)\\&=e^{\frac{1}{2}\int_{0}^{t}{\beta_{u}du}}\cdot \frac{1}{2}\beta_{t}X_{t}dt + e^{\frac{1}{2}\int_{0}^{t}{\beta_{u}du}}\cdot \left(-\frac{1}{2}\beta_{t}X_{t}dt + \sqrt{\beta_{t}}d\overrightarrow{W_{t}}\right) \\&= \sqrt{\beta_{t}}e^{\frac{1}{2}\int_{0}^{t}{\beta_{u}du}}d\overrightarrow{W_{t}}.
\end{split}

$$

Integrating this expression from $s$ to $t$ results in an It\^o's integral:

$$

e^{\frac{1}{2}\int_{0}^{t}{\beta_{u}du}}X_{t} - e^{\frac{1}{2}\int_{0}^{s}{\beta_{u}du}}X_{s} = \int_{s}^{t}{\sqrt{\beta_{\tau}}e^{\frac{1}{2}\int_{0}^{\tau}{\beta_{u}du}}d\overrightarrow{W_{\tau}}},

$$

or

$$

X_{t} = e^{-\frac{1}{2}\int_{s}^{t}{\beta_{u}du}}X_{s} + \int_{s}^{t}{\sqrt{\beta_{\tau}}e^{-\frac{1}{2}\int_{\tau}^{t}{\beta_{u}du}}d\overrightarrow{W_{\tau}}}.

$$

The integrand on the right-hand side is deterministic and belongs to $L_{2}[0,1]$ (for practical noise schedule choices), so its It\^o's integral is a normal random variable, a martingale (meaning it has zero mean) and satisfies It\^o's isometry which allows to calculate its variance:

$$

\Var(X_{t}|X_{s}) = \int_{s}^{t}{\beta_{\tau}}e^{-\int_{\tau}^{t}{\beta_{u}du}}\ide d\tau = \left(1 - e^{-\int_{s}^{t}{\beta_{u}du}}\right)\ide .

$$

Thus

$$

\Law(X_{t}|X_{s}) = \mathcal{N}\left(e^{-\frac{1}{2}\int_{s}^{t}{\beta_{u}du}}X_{s}, \left(1 - e^{-\int_{s}^{t}{\beta_{u}du}}\right)\ide\right) = \mathcal{N}(\gamma_{s,t}X_{s}, (1-\gamma_{s,t}^{2})\ide)

$$

## 7·The Optimal Coefficients Asymptotics

\label{app:asymptotics}

First derive asymptotics for $\gamma$:

$$

\gamma_{t-h, t} = e^{-\frac{1}{2}\int_{t-h}^{t}{\beta_{u}du}} = 1 - \frac{1}{2}\beta_{t}h + \bar{o}(h),

$$

$$

\gamma_{0, t-h}^{2}=e^{-\int_{0}^{t-h}{\beta_{u}du}}=e^{-\int_{0}^{t}{\beta_{u}du}}e^{\int_{t-h}^{t}{\beta_{u}du}}=\gamma_{0,t}^{2}(1 + \beta_{t}h) + \bar{o}(h),

$$

$$

\gamma_{0, t-h}=e^{-\frac{1}{2}\int_{0}^{t-h}{\beta_{u}du}}=e^{-\frac{1}{2}\int_{0}^{t}{\beta_{u}du}}e^{\frac{1}{2}\int_{t-h}^{t}{\beta_{u}du}}=\gamma_{0,t}(1 + \frac{1}{2}\beta_{t}h) + \bar{o}(h),

$$

$$

\gamma_{t-h, t}^{2} = e^{-\int_{t-h}^{t}{\beta_{u}du}} = 1 - \beta_{t}h + \bar{o}(h).

$$

Then find asymptotics for $\mu$, $\nu$ and $\sigma^{2}$:

$$

\mu_{t-h,t}=\left(1 - \frac{1}{2}\beta_{t}h + \bar{o}(h)\right)\frac{1-\gamma_{0,t}^{2}-\gamma_{0,t}^{2}\beta_{t}h + \bar{o}(h)}{1-\gamma_{0,t}^{2}} = 1 - \frac{1}{2}\beta_{t}h-\frac{\gamma_{0,t}^{2}}{1-\gamma_{0,t}^{2}}\beta_{t}h + \bar{o}(h),

$$

$$

\nu_{t-h,t} = (\gamma_{0,t}(1 + \frac{1}{2}\beta_{t}h) + \bar{o}(h))\frac{\beta_{t}h + \bar{o}(h)}{1 - \gamma_{0,t}^{2}} = \frac{\gamma_{0,t}}{1 - \gamma_{0,t}^{2}}\beta_{t}h + \bar{o}(h),

$$

$$

\sigma^{2}_{t-h,t} = \frac{1}{1 - \gamma_{0,t}^{2}}(\beta_{t}h + \bar{o}(h))(1 - \gamma_{0,t}^{2}(1 + \beta_{t}h) + \bar{o}(h)) = \beta_{t}h + \bar{o}(h).

$$

Finally we get asymptotics for $\kappa^{*}$, $\omega^{*}$ and $\sigma^{*}$:

$$

\begin{split}
\kappa_{t,h}^{*}&=\frac{\nu_{t-h,t}(1 - \gamma_{0,t}^{2})}{\gamma_{0,t}\beta_{t}h} - 1 = \frac{\gamma_{0,t-h}(1 - \gamma_{t-h,t}^{2})}{\gamma_{0,t}\beta_{t}h} - 1 \\& =\frac{(\beta_{t}h + \bar{o}(h))((1 + \frac{1}{2}\beta_{t}h)\gamma_{0,t} + \bar{o}(h))}{\gamma_{0,t}\beta_{t}h} - 1 = \bar{o}(1),
\end{split}

$$

$$

\begin{split}
&\beta_{t} h \omega_{t,h}^{*} = \mu_{t-h,t} - 1 +  \frac{\nu_{t-h,t}}{\gamma_{0,t}} - \frac{1}{2}\beta_{t}h = 1 - \frac{1}{2}\beta_{t}h - \frac{\gamma_{0,t}^{2}}{1 - \gamma_{0,t}^{2}}\beta_{t}h - 1 - \frac{1}{2}\beta_{t}h + \frac{1}{\gamma_{0,t}}\times \\& \times\left(\frac{\gamma_{0,t}}{1 - \gamma_{0,t}^{2}}\beta_{t}h + \bar{o}(h)\right) + \bar{o}(h) = \beta_{t}h\left(-1 - \frac{\gamma_{0,t}^{2}}{1 - \gamma_{0,t}^{2}} + \frac{1}{1 - \gamma_{0,t}^{2}}\right) + \bar{o}(h) = \bar{o}(h),
\end{split}

$$

$$

\begin{split}
&(\sigma^{*}_{t,h})^{2} = \sigma^{2}_{t-h,t} + \nu_{t-h,t}^{2}\mathbb{E}_{X_{t}}\left[\Tr \left(\Var{(X_{0}|X_{t})}\right)\right]/n = \beta_{t}h + \bar{o}(h) \\
&+ \frac{\gamma_{0,t}^{2}}{(1 - \gamma_{0,t}^{2})^{2}}\beta_{t}^{2}h^{2}\mathbb{E}_{X_{t}}\left[\Tr \left(\Var{(X_{0}|X_{t})}\right)\right]/n = \beta_{t}h(1 + \bar{o}(1)).
\end{split}

$$

## 8·Proof of the Theorem

\label{app:th_proof}

The key fact necessary to prove the Theorem [th:main](#th:main) is established in the following

\begin{lemma}
\label{lm:net_optimal}
Let $p_{0|t}(\cdot|x)$ be pdf of conditional distribution $\Law{(X_{0}|X_{t}=x)}$.

Then for any $t\in [0,1]$ and $x\in \mathbb{R}^{n}$

$$

\label{eq:net_optimal}
s_{\theta^{*}}(x,t) = -\frac{1}{1-\gamma_{0,t}^{2}}\left(x - \gamma_{0,t}\mathbb{E}_{p_{0|t}(\cdot|x)}X_{0}\right).

$$

\end{lemma}

\begin{proof}[Proof of the Lemma [lm:net_optimal](#lm:net_optimal)]
As mentioned in [^Song2021Score-Based], an expression alternative to ([eq:objective](#eq:objective)) can be derived for $\theta^{*}$ under mild assumptions on the data density [^Hyv{{\"a}}rinen2005Estimation], [^Vincent2011A]:

$$

\theta^{*}=\argmin_{\theta}{\int_{0}^{1}{\lambda_{t}\mathbb{E}_{X_{0}\sim p_{0}(\cdot)}\mathbb{E}_{X_{t}\sim p_{t|0}(\cdot|X_{0})}\Vert s_{\theta}(X_{t}, t) - \nabla{\log{p_{t|0}(X_{t}|X_{0})}}\Vert_{2}^{2}}}dt,

$$

where $\Law{(X_{0})}$ is data distribution with pdf $p_{0}(\cdot)$ and $\Law{(X_{t}|X_{0}=x_{0})}$ has pdf $p_{t|0}(\cdot | x_{0})$.

By Bayes formula we can rewrite this in terms of pdfs $p_{t}(\cdot)$ and $p_{0|t}(\cdot | x_{t})$ of distributions $\Law{(X_{t})}$ and $\Law{(X_{0}|X_{t}=x_{t})}$ correspondingly:

$$

\theta^{*}=\argmin_{\theta}{\int_{0}^{1}{\lambda_{t}\mathbb{E}_{X_{t}\sim p_{t}(\cdot)}\mathbb{E}_{X_{0}\sim p_{0|t}(\cdot|X_{t})}\Vert s_{\theta}(X_{t}, t) - \nabla{\log{p_{t|0}(X_{t}|X_{0})}}\Vert_{2}^{2}}}dt.

$$

For any $n$-dimensional random variable $\xi$ with finite second moment and deterministic vector $a$ we have

$$

\begin{split}
\mathbb{E}\Vert\xi - a\Vert_{2}^{2} &= \mathbb{E}\Vert\xi - \mathbb{E}\xi + \mathbb{E}\xi - a\Vert_{2}^{2} = \mathbb{E}\Vert\xi - \mathbb{E}\xi\Vert_{2}^{2} + 2\langle\mathbb{E}[\xi - \mathbb{E}\xi],\mathbb{E}\xi - a\rangle \\&+ \mathbb{E}\Vert\mathbb{E}\xi - a\Vert_{2}^{2} = \mathbb{E}\Vert\xi - \mathbb{E}\xi\Vert_{2}^{2} + \Vert\mathbb{E}\xi - a\Vert_{2}^{2}.
\end{split}

$$

In our case $\xi=\nabla{\log{p_{t|0}(X_{t}|X_{0})}}$ and $a=s_{\theta}(X_{t}, t)$, so $\mathbb{E}\Vert\xi - \mathbb{E}\xi\Vert_{2}^{2}$ is independent of $\theta$.

Thus

$$

\theta^{*}=\argmin_{\theta}{\int_{0}^{1}{\lambda_{t}\mathbb{E}_{X_{t}\sim p_{t}(\cdot)}\Vert s_{\theta}(X_{t}, t) - \mathbb{E}_{X_{0}\sim p_{0|t}(\cdot|X_{t})} \left[\nabla{\log{p_{t|0}(X_{t}|X_{0})}}\right]\Vert_{2}^{2}}}dt.

$$

Therefore, the optimal score estimation network $s_{\theta^{*}}$ can be expressed as

$$

s_{\theta^{*}}(x, t) = \mathbb{E}_{p_{0|t}(\cdot|x)} \left[\nabla{\log{p_{t|0}(x|X_{0})}}\right]

$$

for all $t\in [0,1]$ and $x\in \supp{\{p_{t}\}}=\mathbb{R}^{n}$.

As proven in Appendix [app:sde_solution](#app:sde_solution), $\Law{(X_{t}|X_{0})}$ is Gaussian with mean vector $\gamma_{0,t}X_{0}$ and covariance matrix $(1-\gamma_{0,t}^{2})\ide$, so finally we obtain

$$

s_{\theta^{*}}(x,t) =
\mathbb{E}_{p_{0|t}(\cdot|x)}\left[-\frac{1}{1-\gamma_{0,t}^{2}}\left(x - \gamma_{0,t}X_{0}\right)\right]= -\frac{1}{1-\gamma_{0,t}^{2}}\left(x - \gamma_{0,t}\mathbb{E}_{p_{0|t}(\cdot|x)}X_{0}\right).

$$

\end{proof}

Now let us prove the Theorem [th:main](#th:main).

\begin{proof}[Proof of the Theorem [th:main](#th:main)]
The sampling scheme ([eq:sde_solver](#eq:sde_solver)) consists in adding Gaussian noise to a linear combination of $\hat{X}_{t}$ and $s_{\theta^{*}}(\hat{X}_{t}, t)$.

Combining ([eq:sde_solver](#eq:sde_solver)) and the Lemma [lm:net_optimal](#lm:net_optimal) we get

$$

\begin{split}
&\hat{X}_{t-h} = \hat{\sigma}_{t,h}\xi_{t} + \hat{X}_{t} + \beta_{t}h\left(\left(\frac{1}{2}+\hat{\omega}_{t,h}\right)\hat{X}_{t} + (1+\hat{\kappa}_{t,h})s_{\theta^{*}}(\hat{X}_{t},t)\right) = \hat{\sigma}_{t,h}\xi_{t} \\ &+ \left(1 + \beta_{t}h\left(\frac{1}{2} + \hat{\omega}_{t,h}\right)\right)\hat{X}_{t} + \beta_{t}h(1+\hat{\kappa}_{t,h})\left(-\frac{1}{1-\gamma_{0,t}^{2}}\left(\hat{X}_{t} - \gamma_{0,t}\mathbb{E}_{p_{0|t}(\cdot|\hat{X}_{t})}X_{0}\right)\right) \\& =\hat{\sigma}_{t,h}\xi_{t} + \left(1 + \beta_{t}h\left(\frac{1}{2} + \hat{\omega}_{t,h}-\frac{1+\hat{\kappa}_{t,h}}{1-\gamma_{0,t}^{2}}\right)\right)\hat{X}_{t} + \frac{\gamma_{0,t}\beta_{t}h(1+\hat{\kappa}_{t,h})}{1-\gamma_{0,t}^{2}}\mathbb{E}_{p_{0|t}(\cdot|\hat{X}_{t})}X_{0},
\end{split}

$$

where $\xi_{t}$ are i.i.d. random variables from standard normal distribution $\mathcal{N}(0,\ide)$ for $t=1,1-h,..,h$.

Thus, the distribution $\hat{X}_{t-h}|\hat{X}_{t}$ is also Gaussian:

$$

\label{eq:law_net}
\Law{(\hat{X}_{t-h} | \hat{X}_{t})} = \mathcal{N}\left(\hat{\mu}_{t,h}(\hat{\kappa}_{t,h},\hat{\omega}_{t,h})\hat{X}_{t} + \hat{\nu}_{t,h}(\hat{\kappa}_{t,h})\mathbb{E}_{p_{0|t}(\cdot|\hat{X}_{t})}X_{0},\hat{\sigma}_{t,h}^{2}\ide\right),

$$

$$

\hat{\mu}_{t,h}(\hat{\kappa}_{t,h},\hat{\omega}_{t,h}) = 1 + \beta_{t}h\left(\frac{1}{2} + \hat{\omega}_{t,h}-\frac{1+\hat{\kappa}_{t,h}}{1-\gamma_{0,t}^{2}}\right),

$$

$$

\hat{\nu}_{t,h}(\hat{\kappa}_{t,h}) = \frac{\gamma_{0,t}\beta_{t}h(1+\hat{\kappa}_{t,h})}{1-\gamma_{0,t}^{2}},

$$

which leads to the following formula for the transition densities of the reverse diffusion:

$$

\label{eq:p_net}
\hat{p}_{t-h|t}(x_{t-h}|x_{t})=\frac{1}{\sqrt{2\pi}\hat{\sigma}_{t,h}^{n}}\exp\left(-\frac{\Vert x_{t-h} - \hat{\mu}_{t,h}x_{t} - \hat{\nu}_{t,h}\mathbb{E}_{p_{0|t}(\cdot|x_{t})}X_{0}\Vert_{2}^{2}}{2\hat{\sigma}^{2}_{t,h}}\right).

$$

Moreover, comparing $\hat{\mu}_{t,h}$ and $\hat{\nu}_{t,h}$ with $\mu_{t-h,t}$ and $\nu_{t-h,t}$ defined in ([eq:notation_gamma](#eq:notation_gamma)) we deduce that

$$

\hat{\nu}_{t,h} = \nu_{t-h,t} \Leftrightarrow   \frac{\gamma_{0,t}\beta_{t}h(1+\hat{\kappa}_{t,h})}{1-\gamma_{0,t}^{2}} = \nu_{t-h,t} \Leftrightarrow \hat{\kappa}_{t,h}=\kappa^{*}_{t,h}.

$$

If we also want $\hat{\mu}_{t,h}=\mu_{t-h,t}$ to be satisfied, then we should have

$$

1 + \beta_{t}h\left(\frac{1}{2} + \hat{\omega}_{t,h}-\frac{1+\kappa^{*}_{t,h}}{1-\gamma_{0,t}^{2}}\right) = \mu_{t-h,t} \Leftrightarrow \left(\frac{\mu_{t-h,t} - 1}{\beta_{t}h}-\omega^{*}_{t,h}+\hat{\omega}_{t,h}\right)\beta_{t}h + 1 = \mu_{t-h,t},

$$

i.e. $\hat{\nu}_{t,h}=\nu_{t-h,t}$ and $\hat{\mu}_{t,h}=\mu_{t-h,t}$ iff $\hat{\kappa}_{t,h}=\kappa^{*}_{t,h}$ and $\hat{\omega}_{t,h}=\omega^{*}_{t,h}$ for the parameters $\kappa^{*}_{t,h}$ and $\omega^{*}_{t,h}$ defined in ([eq:notation_correction](#eq:notation_correction)).

As for the corresponding densities of the forward process $X$, they are Gaussian when conditioned on the initial data point $X_{0}$:

$$

\label{eq:law_true}
\Law{(X_{t-h} | X_{t}, X_{0})} = \mathcal{N}(\mu_{t-h,t}X_{t} + \nu_{t-h,t}X_{0},\sigma_{t-h,t}^{2}\ide),

$$

where coefficients $\mu_{t-h,t}$, $\nu_{t-h,t}$ and $\sigma_{t-h,t}$ are defined in ([eq:notation_gamma](#eq:notation_gamma)).

This formula for $\Law{(X_{t-h} | X_{t}, X_{0})}$ follows from the general fact about Gaussian distributions appearing in many recent works on diffusion probabilistic modeling [^Kingma2021Variational]: if $Z_{t}|Z_{s}\sim \mathcal{N}(\alpha_{t|s}Z_{s}, \sigma_{t|s}^{2}\ide)$ and $Z_{t}|Z_{0}\sim \mathcal{N}(\alpha_{t|0}Z_{0}, \sigma_{t|0}^{2}\ide)$ for $0 < s < t$, then

$$

\label{eq:gauss}
\Law{(Z_{s}|Z_{t}, Z_{0})} = \mathcal{N}\left(\frac{\sigma_{s|0}^{2}}{\sigma_{t|0}^{2}}\alpha_{t|s}Z_{t} + \frac{\sigma_{t|s}^{2}}{\sigma_{t|0}^{2}}\alpha_{s|0}Z_{0}, \frac{\sigma_{s|0}^{2}\sigma_{t|s}^{2}}{\sigma_{t|0}^{2}}\ide\right).

$$

This fact is a result of applying Bayes formula to normal distributions.

In our case $\alpha_{t|s}=\gamma_{s,t}$ and $\sigma_{t|s}^{2} = 1 - \gamma_{s,t}^{2}$.

To get an expression for the densities $p_{t-h|t}(x_{t-h}|x_{t})$ similar to ([eq:p_net](#eq:p_net)), we need to integrate out the dependency on data $X_{0}$ from Gaussian distribution $\Law{(X_{t-h}|X_{t}, X_{0})}$:

$$

\label{eq:p_exc}

\begin{split}
p_{t-h|t}(x_{t-h}|x_{t}) = & \int{p_{t-h,0|t}(x_{t-h},x_{0}|x_{t})dx_{0}}=\int{p_{t-h|t,0}(x_{t-h}|x_{t},x_{0})p_{0|t}(x_{0}|x_{t})dx_{0}} \\&= \mathbb{E}_{X_{0}\sim p_{0|t}(\cdot|x_{t})}[p_{t-h|t,0}(x_{t-h}|x_{t},X_{0})],
\end{split}

$$

which implies the following formula:

$$

\label{eq:p_true}
p_{t-h|t}(x_{t-h}|x_{t})=\frac{1}{\sqrt{2\pi}\sigma_{t-h,t}^{n}}\mathbb{E}_{p_{0|t}(\cdot|x_{t})}\left[\exp\left(-\frac{\Vert x_{t-h} - \mu_{t-h,t}x_{t} - \nu_{t-h,t}X_{0}\Vert_{2}^{2}}{2\sigma^{2}_{t-h,t}}\right)\right].

$$

Note that in contrast with the transition densities ([eq:p_net](#eq:p_net)) of the reverse process $\hat{X}$, the corresponding densities ([eq:p_true](#eq:p_true)) of the forward process $X$ are not normal in general.

Our goal is to find parameters $\hat{\kappa}$, $\hat{\omega}$ and $\hat{\sigma}$ that maximize log-likelihood of sample paths $X$ under probability measure with transition densities $\hat{p}$.

Put $t_{k}=kh$ for $k=0,1,..,N$ and write down this log-likelihood:

$$

\begin{split}
&\int{p(x_{1},x_{1-h},..,x_{0})\left(\sum_{k=0}^{N-1}{\log{\hat{p}_{t_{k}|t_{k+1}}}(x_{t_{k}}|x_{t_{k+1}})} + \log{\hat{p}_{1}(x_{1})}\right)dx_{1}dx_{1-h}..dx_{0}} \\
&=\sum_{k=0}^{N-1}\int{p(x_{t_{k}},x_{t_{k+1}})\log{\hat{p}_{t_{k}|t_{k+1}}(x_{t_{k}}|x_{t_{k+1}})}dx_{t_{k+1}}dx_{t_{k}}} + \int{p(x_{1})\log{\hat{p}_{1}}(x_{1})dx_{1}}.
\end{split}

$$

The last term does not depend on $\hat{\kappa}$, $\hat{\omega}$ and $\hat{\sigma}$, so we can ignore it.

Let $R_{k}$ be the $k$-th term in the sum above.

Since we are free to have different coefficients $\hat{\kappa}_{t,h}$, $\hat{\omega}_{t,h}$ and $\hat{\sigma}_{t,h}$ for different steps, we can maximize each $R_{k}$ separately.

Terms $R_{k}$ can be expressed as

$$

\begin{split}
R_{k} &= \int{p(x_{t_{k}},x_{t_{k+1}})\log{\hat{p}_{t_{k}|t_{k+1}}(x_{t_{k}}|x_{t_{k+1}})}dx_{t_{k+1}}dx_{t_{k}}} \\
&=\int{p(x_{t_{k+1}})p_{t_{k}|t_{k+1}}(x_{t_{k}}|x_{t_{k+1}})\log{\hat{p}_{t_{k}|t_{k+1}}(x_{t_{k}}|x_{t_{k+1}})}dx_{t_{k+1}}dx_{t_{k}}} \\
&=\mathbb{E}_{X_{t_{k+1}}}\left[\int{p_{t_{k}|t_{k+1}}(x_{t_{k}}|X_{t_{k+1}})\log{\hat{p}_{t_{k}|t_{k+1}}(x_{t_{k}}|X_{t_{k+1}})}dx_{t_{k}}}\right].
\end{split}

$$

From now on we will skip subscripts of $\mu$, $\nu$, $\sigma$, $\hat{\mu}$, $\hat{\nu}$, $\hat{\sigma}$, $\hat{\kappa}$, $\hat{\omega}$, $\kappa^{*}$ and $\omega^{*}$ for brevity.

Denote

$$

\label{eq:q}
Q(x_{t_{k}},X_{t_{k+1}},X_{0})=\frac{1}{\sqrt{2\pi}\sigma^{n}}\exp\left(-\frac{\Vert x_{t_{k}} - \mu X_{t_{k+1}} - \nu X_{0}\Vert_{2}^{2}}{2\sigma^{2}}\right)\log{\hat{p}_{t_{k}|t_{k+1}}(x_{t_{k}}|X_{t_{k+1}})}.

$$

Using the formula ([eq:p_exc](#eq:p_exc)) for the densities of $X$ together with the explicit expression for the Gaussian density $p_{t_{k}|t_{k+1},0}(x_{t_{k}}|X_{t_{k+1}},X_{0})$ and applying Fubini's theorem to change the order of integration, we rewrite $R_{k}$ as

$$

\begin{split}
R_{k}&=\mathbb{E}_{X_{t_{k+1}}}\left[\int{p_{t_{k}|t_{k+1}}(x_{t_{k}}|X_{t_{k+1}})}\log{\hat{p}_{t_{k}|t_{k+1}}(x_{t_{k}}|X_{t_{k+1}})}dx_{t_{k}}\right]\\
&=\mathbb{E}_{X_{t_{k+1}}}\left[\int{\mathbb{E}_{X_{0}\sim p_{0|t_{k+1}}(\cdot|X_{t_{k+1}})}\left[p_{t_{k}|t_{k+1},0}(x_{t_{k}}|X_{t_{k+1}},X_{0})\log{\hat{p}_{t_{k}|t_{k+1}}(x_{t_{k}}|X_{t_{k+1}})}\right]dx_{t_{k}}}\right] \\
&=\mathbb{E}_{X_{t_{k+1}}}\left[\int{\mathbb{E}_{X_{0}\sim p_{0|t_{k+1}}(\cdot|X_{t_{k+1}})}[Q(x_{t_{k}}, X_{t_{k+1}}, X_{0})]dx_{t_{k}}}\right]\\
&=\mathbb{E}_{X_{t_{k+1}}}\mathbb{E}_{X_{0}\sim p_{0|t_{k+1}}(\cdot|X_{t_{k+1}})}\left[\int{Q(x_{t_{k}}, X_{t_{k+1}}, X_{0})dx_{t_{k}}}\right].
\end{split}

$$

The formula ([eq:q](#eq:q)) implies that the integral of $Q(x_{t_{k}}, X_{t_{k+1}},X_{0})$ with respect to $x_{t_{k}}$ can be seen as expectation of $\log{\hat{p}_{t_{k}|t_{k+1}}(\xi|X_{t_{k+1}})}$ with respect to normal random variable $\xi$ with mean $\mu X_{t_{k+1}} + \nu X_{0}$ and covariance matrix $\sigma^{2}\ide$.

Plugging in the expression ([eq:p_net](#eq:p_net)) into ([eq:q](#eq:q)), we can calculate this integral:

$$

\begin{split}
&\mathbb{E}_{\xi}\left[-\log{\sqrt{2\pi}} -n\log{\hat{\sigma}} - \frac{\Vert\xi - \hat{\mu}X_{t_{k+1}}-\hat{\nu}\mathbb{E}_{X_{0}'\sim p_{0|t_{k+1}}(\cdot|X_{t_{k+1}})}X_{0}'\Vert_{2}^{2}}{2\hat{\sigma}^{2}}\right] \\
&=-\log{\sqrt{2\pi}} - n\log{\hat{\sigma}} - \frac{\mathbb{E}_{\xi}\Vert \xi - \hat{\mu}X_{t_{k+1}}-\hat{\nu}\mathbb{E}_{p_{0|t_{k+1}}(\cdot|X_{t_{k+1}})}X_{0}'\Vert_{2}^{2}}{2\hat{\sigma}^{2}}.
\end{split}

$$

Thus, terms $R_{k}$ we want to maximize equal

$$

R_{k} = -\log{\sqrt{2\pi}} - n\log{\hat{\sigma}} - \mathbb{E}_{X_{t_{k+1}}}\mathbb{E}_{X_{0}\sim p_{0|t_{k+1}}(\cdot|X_{t_{k+1}})}\frac{\mathbb{E}_{\xi}\Vert \xi - \hat{\mu}X_{t_{k+1}}-\hat{\nu}\mathbb{E}_{p_{0|t_{k+1}}(\cdot|X_{t_{k+1}})}X_{0}'\Vert_{2}^{2}}{2\hat{\sigma}^{2}}

$$

Maximizing $R_{k}$ with respect to ($\hat{\kappa}, \hat{\omega}, \hat{\sigma}$) is equivalent to minimizing $\mathbb{E}_{X_{t_{k+1}}}S_{k}$ where $S_{k}$ is given by

$$

\label{eq:s_term}
S_{k} = n\log{\hat{\sigma}} + \frac{1}{2\hat{\sigma}^{2}}\mathbb{E}_{X_{0}\sim p_{0|t_{k+1}}(\cdot|X_{t_{k+1}})}\mathbb{E}_{\xi}\Vert \xi - \hat{\mu}X_{t_{k+1}}-\hat{\nu}\mathbb{E}_{p_{0|t_{k+1}}(\cdot|X_{t_{k+1}})}X_{0}'\Vert_{2}^{2},

$$

where the expectation with respect to $\xi\sim \mathcal{N}(\mu X_{t_{k+1}} + \nu X_{0}, \sigma^{2}\ide)$ can be calculated using the fact that for every vector $\hat{a}$ we can express $\mathbb{E}_{\xi}\Vert \xi - \hat{a} \Vert_{2}^{2}$ as

$$

\mathbb{E}\Vert\xi - \mathbb{E}\xi + \mathbb{E}\xi - \hat{a}\Vert_{2}^{2} = \mathbb{E}\Vert\xi - \mathbb{E}\xi \Vert_{2}^{2} + 2\langle\mathbb{E}[\xi - \mathbb{E}\xi], \mathbb{E}\xi - \hat{a}\rangle + \mathbb{E}\Vert \mathbb{E}\xi - \hat{a}\Vert_{2}^{2} = n\sigma^{2} + \Vert \mathbb{E}\xi - \hat{a}\Vert_{2}^{2}.

$$

So, the outer expectation with respect to $\Law(X_{0}|X_{t_{k+1}})$ in ([eq:s_term](#eq:s_term)) can be simplified:

$$

\begin{split}
&\mathbb{E}_{X_{0}\sim p_{0|t_{k+1}}(\cdot|X_{t_{k+1}})}\left[n\sigma^{2} + \Vert(\mu - \hat{\mu})X_{t_{k+1}} + \nu X_{0} - \hat{\nu}\mathbb{E}_{X_{0}'\sim p_{0|t_{k+1}}(\cdot|X_{t_{k+1}})}X_{0}'\Vert^{2}_{2}\right] \\
&=n\sigma^{2} + \mathbb{E}_{X_{0}}\Vert((\mu - \hat{\mu})X_{t_{k+1}} + \nu X_{0}-\hat{\nu}\mathbb{E}_{X_{0}'}X_{0}'\Vert_{2}^{2}=n\sigma^{2} + (\mu - \hat{\mu})^{2}\Vert X_{t_{k+1}}\Vert_{2}^{2} \\
&+2\langle(\mu - \hat{\mu})X_{t_{k+1}},(\nu - \hat{\nu})\mathbb{E}_{X_{0}}X_{0}\rangle + \mathbb{E}_{X_{0}}{\Vert\nu X_{0}} - \hat{\nu}\mathbb{E}_{X_{0}'}X_{0}'\Vert_{2}^{2} = (\mu - \hat{\mu})^{2}\Vert X_{t_{k+1}}\Vert_{2}^{2} \\
& + 2\langle(\mu - \hat{\mu})X_{t_{k+1}},(\nu - \hat{\nu})\mathbb{E}_{X_{0}}X_{0}\rangle + \nu^{2}\mathbb{E}_{X_{0}}\Vert X_{0}\Vert_{2}^{2} + \hat{\nu}^{2}\Vert\mathbb{E}_{X_{0}}X_{0}\Vert_{2}^{2} + n\sigma^{2} \\
& -2\nu\hat{\nu}\langle\mathbb{E}_{X_{0}}X_{0},\mathbb{E}_{X'_{0}}X'_{0}\rangle = \Vert(\mu - \hat{\mu})X_{t_{k+1}} + (\nu - \hat{\nu})\mathbb{E}_{X_{0}}X_{0})\Vert_{2}^{2} + \nu^{2}\mathbb{E}_{X_{0}}\Vert X_{0}\Vert_{2}^{2} \\
& -\nu^{2}\Vert\mathbb{E}_{X_{0}}X_{0}\Vert_{2}^{2} + n\sigma^{2},
\end{split}

$$

where all the expectations in the formula above are taken with respect to the conditional data distribution $\Law(X_{0}|X_{t_{k+1}})$.

So, the resulting expression for the terms $S_{k}$ whose expectation with respect to $\Law{(X_{t_{k+1}})}$ we want to minimize is

$$

\label{eq:neg-log-lok-term}

\begin{split}
S_{k} = n\log&{\hat{\sigma}} + \frac{1}{2\hat{\sigma}^{2}}\Bigl(n\sigma^{2}+\Vert(\mu - \hat{\mu})X_{t_{k+1}} + (\nu - \hat{\nu})\mathbb{E}{[X_{0}|X_{t_{k+1}}]}\Vert_{2}^{2} \\& + \nu^{2}\left(\mathbb{E}\left[\Vert X_{0}\Vert_{2}^{2}|X_{t_{k+1}}\right]-\Vert\mathbb{E}[X_{0}|X_{t_{k+1}}]\Vert_{2}^{2}\right)\Bigr).
\end{split}

$$

Now it is clear that $\kappa^{*}_{t_{k+1},h}$ and $\omega^{*}_{t_{k+1},h}$ are optimal because $\hat{\mu}_{t_{k+1},h}(\kappa^{*}_{t_{k+1},h}, \omega^{*}_{t_{k+1},h}) = \mu_{t_{k},t_{k+1}}$ and $\hat{\nu}_{t_{k+1},h}(\kappa^{*}_{t_{k+1},h}) = \nu_{t_{k},t_{k+1}}$.

For this choice of parameters we have

$$

\mathbb{E}_{X_{t_{k+1}}}S_{k} = n\log{\hat{\sigma}} + \frac{1}{2\hat{\sigma}^{2}}\left(n\sigma^{2}+\nu^{2}\mathbb{E}_{X_{t_{k+1}}}\left[\mathbb{E}\left[\Vert X_{0}\Vert_{2}^{2}|X_{t_{k+1}}\right]-\Vert\mathbb{E}[X_{0}|X_{t_{k+1}}]\Vert_{2}^{2}\right]\right).

$$

Note that $\mathbb{E}\left[\Vert X_{0}\Vert_{2}^{2}|X_{t_{k+1}}\right]-\Vert\mathbb{E}[X_{0}|X_{t_{k+1}}]\Vert_{2}^{2} = \Tr{(\Var{(X_{0}|X_{t_{k+1}})})}$ is the overall variance of $\Law{(X_{0}|X_{t_{k+1}})}$ along all $n$ dimensions.

Differentiating $\mathbb{E}_{X_{t_{k+1}}}{S_{k}}$ with respect to $\hat{\sigma}$ shows that the optimal $\sigma^{*}_{t_{k+1},h}$ should satisfy

$$

\frac{n}{\sigma^{*}_{t_{k+1},h}} - \frac{1}{(\sigma^{*}_{t_{k+1},h})^{3}}\left(n\sigma_{t_{k},t_{k+1}}^{2}+\nu_{t_{k},t_{k+1}}^{2}\mathbb{E}_{X_{t_{k+1}}}\left[\Tr{(\Var{(X_{0}|X_{t_{k+1}})})}\right]\right)=0,

$$

which is indeed satisfied by the parameters $\sigma^{*}_{t,h}$ defined in ([eq:notation_correction](#eq:notation_correction)).

Thus, the statement (i) is proven.

When it comes to proving that $\hat{X}$ is exact, we have to show that $\Law{(\hat{X}_{t_{k}})}=\Law{(X_{t_{k}})}$ for every $k=0,1,..,N$.

By the assumption that $\Law{(\hat{X}_{1})} = \Law{(X_{1})}$ it is sufficient to prove that $\hat{p}_{t_{k}|t_{k+1}}(x_{t_{k}}|x_{t_{k+1}}) \equiv p_{t_{k}|t_{k+1}}(x_{t_{k}}|x_{t_{k+1}})$ since the exactness will follow from this fact by mathematical induction.

If $X_{0}$ is a constant random variable, $\Law(X_{0}|X_{t})=\Law{(X_{0})}$ also corresponds to the same constant, so $\Var{(X_{0}|X_{t})}=0$ meaning that $\sigma^{*}_{t,h}=\sigma_{t-h,t}$, and the formulae ([eq:p_net](#eq:p_net)) and ([eq:p_true](#eq:p_true)) imply the desired result.

Let us now consider the second case when $X_{0}\sim \mathcal{N}(\bar{\mu},\delta^{2}\ide)$.

It is a matter of simple but lengthy computations to prove another property of Gaussian distributions similar to ([eq:gauss](#eq:gauss)): if $Z_{0}\sim \mathcal{N}(\bar{\mu},\delta^{2}\ide)$ and $Z_{t}|Z_{0}\sim \mathcal{N}(a_{t}Z_{0},b^{2}_{t}\ide)$, then $Z_{0}|Z_{t}\sim \mathcal{N}(\frac{b_{t}^{2}}{b^{2}_{t}+\delta^{2}a^{2}_{t}}\bar{\mu} + \frac{\delta^{2}a_{t}}{b^{2}_{t}+\delta^{2}a^{2}_{t}}Z_{t},\frac{\delta^{2}b^{2}_{t}}{b_{t}^{2}+\delta^{2}a^{2}_{t}}\ide)$ and $Z_{t}\sim \mathcal{N}(\bar{\mu}a_{t},{(b^{2}_{t}+\delta^{2}a^{2}_{t}})\ide)$.

In our case $a_{t}=\gamma_{0,t}$ and $b^{2}_{t}=1-\gamma_{0,t}^{2}$, therefore

$$

\label{eq:gaussian_revert}
\Law{(X_{0}|X_{t})}=\mathcal{N}\left(\frac{1-\gamma^{2}_{0,t}}{1-\gamma_{0,t}^{2}+\delta^{2}\gamma^{2}_{0,t}}\bar{\mu}+\frac{\delta^{2}\gamma_{0,t}}{1-\gamma_{0,t}^{2}+\delta^{2}\gamma^{2}_{0,t}}X_{t},\frac{\delta^{2}(1-\gamma_{0,t}^{2})}{1-\gamma_{0,t}^{2}+\delta^{2}\gamma^{2}_{0,t}}\ide\right).

$$

So, $\Var{(X_{0}|X_{t})}$ does not depend on $X_{t}$ and

$$

(\sigma^{*}_{t,h})^{2} = \sigma_{t-h,t}^{2} + \frac{\nu_{t-h,t}^{2}}{n}\mathbb{E}_{X_{t}}\left[\Tr{(\Var{(X_{0}|X_{t})})}\right] = \sigma_{t-h,t}^{2} + \nu_{t-h,t}^{2}\frac{\delta^{2}(1-\gamma_{0,t}^{2})}{1-\gamma_{0,t}^{2}+\delta^{2}\gamma^{2}_{0,t}}.

$$

Since $\Law{(X_{t}|X_{t-h})}$, $\Law{(X_{t-h})}$ and $\Law{(X_{t})}$ are Gaussian, Bayes formula implies that $\Law{(X_{t-h}|X_{t})}$ is Gaussian as well with the following mean and covariance matrix:

$$

\mathbb{E}[X_{t-h}|X_{t}] = \frac{\gamma_{0,t-h}(1-\gamma_{t-h,t}^{2})}{1-\gamma^{2}_{0,t}+\delta^{2}\gamma_{0,t}^{2}}\bar{\mu} + \frac{\gamma_{t-h,t}(1-\gamma^{2}_{0,t-h}+\delta^{2}\gamma_{0,t-h}^{2})}{1-\gamma^{2}_{0,t}+\delta^{2}\gamma_{0,t}^{2}}X_{t},

$$

$$

\Var{(X_{t-h}|X_{t})}=\frac{(1-\gamma_{t-h,t}^{2})(1-\gamma_{0,t-h}^{2}+\delta^{2}\gamma_{0,t-h}^{2})}{1-\gamma_{0,t}^{2}+\delta^{2}\gamma_{0,t}^{2}}\ide.

$$

The distribution $\Law{(\hat{X}_{t-h}|\hat{X}_{t})}$ is also Gaussian by the formula ([eq:p_net](#eq:p_net)), so to conclude the proof we just need to show that $\mathbb{E}[\hat{X}_{t-h}|\hat{X}_{t}=x]=\mathbb{E}[X_{t-h}|X_{t}=x]$ and $\Var{(\hat{X}_{t-h}|\hat{X}_{t}=x)}=\Var{(X_{t-h}|X_{t}=x)}$ for every $x\in \mathbb{R}^{n}$ for the optimal parameters ([eq:notation_correction](#eq:notation_correction)).

Recall that for $\kappa^{*}_{t,h}$ and $\omega^{*}_{t,h}$ we have $\hat{\mu}_{t,h}(\kappa^{*}_{t,h}, \omega^{*}_{t,h}) = \mu_{t-h,t}$ and $\hat{\nu}_{t,h}(\kappa^{*}_{t,h}) = \nu_{t-h,t}$.

Utilizing the formulae ([eq:notation_gamma](#eq:notation_gamma)), ([eq:p_net](#eq:p_net)), ([eq:gaussian_revert](#eq:gaussian_revert)) and the fact that $\gamma_{0,t-h}\cdot\gamma_{t-h,t}=\gamma_{0,t}$ (following from the definition of $\gamma$ in ([eq:xt_distribution](#eq:xt_distribution))) we conclude that

$$

\begin{split}
&\mathbb{E}[\hat{X}_{t-h}|\hat{X}_{t}=x]=\hat{\mu}_{t,h}(\kappa^{*}_{t,h}, \omega^{*}_{t,h})x + \hat{\nu}_{t,h}(\kappa^{*}_{t,h})\mathbb{E}_{p_{0|t}(\cdot|x)}X_{0} \\
& = \gamma_{t-h,t}\frac{1-\gamma_{0,t-h}^{2}}{1-\gamma_{0,t}^{2}}x + \gamma_{0,t-h}\frac{1-\gamma_{t-h,t}^{2}}{1-\gamma_{0,t}^{2}}\left[\frac{1-\gamma_{0,t}^{2}}{1-\gamma_{0,t}^{2}+\delta^{2}\gamma_{0,t}^{2}}\bar{\mu} + \frac{\delta^{2}\gamma_{0,t}}{1-\gamma_{0,t}^{2}+\delta^{2}\gamma_{0,t}^{2}}x\right] \\
&= \gamma_{0,t-h}\frac{1-\gamma_{t-h,t}^{2}}{1-\gamma_{0,t}^{2} + \delta^{2}\gamma_{0,t}^{2}}\bar{\mu} + \gamma_{t-h,t}\frac{(1-\gamma_{0,t-h}^{2})(1-\gamma_{0,t}^{2})+\delta^{2}\gamma_{0,t}^{2}(1-\gamma^{2}_{0,t-h})}{(1-\gamma_{0,t}^{2})(1-\gamma_{0,t}^{2}+\delta^{2}\gamma_{0,t}^{2})}x \\
&+\gamma_{t-h,t}\frac{\delta^{2}\gamma_{0,t-h}^{2}(1-\gamma^{2}_{t-h,t})}{(1-\gamma_{0,t}^{2})(1-\gamma_{0,t}^{2}+\delta^{2}\gamma_{0,t}^{2})}x = \gamma_{0,t-h}\frac{1-\gamma_{t-h,t}^{2}}{1-\gamma_{0,t}^{2} + \delta^{2}\gamma_{0,t}^{2}}\bar{\mu} \\
&+\gamma_{t-h,t}\frac{(1-\gamma_{0,t-h}^{2})(1-\gamma_{0,t}^{2})+\delta^{2}\gamma_{0,t-h}^{2}(1-\gamma_{0,t}^{2})}{(1-\gamma_{0,t}^{2})(1-\gamma_{0,t}^{2}+\delta^{2}\gamma_{0,t}^{2})}x = \gamma_{0,t-h}\frac{1-\gamma_{t-h,t}^{2}}{1-\gamma_{0,t}^{2} + \delta^{2}\gamma_{0,t}^{2}}\bar{\mu} \\
&+ \gamma_{t-h,t}\frac{1-\gamma_{0,t-h}^{2}+\delta^{2}\gamma_{0,t-h}^{2}}{1-\gamma_{0,t}^{2}+\delta^{2}\gamma_{0,t}^{2}}x = \mathbb{E}[X_{t-h}|X_{t}=x],
\end{split}

$$

$$

\begin{split}
&\Var{(\hat{X}_{t-h}|\hat{X}_{t}=x)} = (\sigma_{t,h}^{*})^{2}\ide = \left(\sigma_{t-h,t}^{2} + \nu_{t-h,t}^{2}\frac{\delta^{2}(1-\gamma_{0,t}^{2})}{1-\gamma_{0,t}^{2}+\delta^{2}\gamma^{2}_{0,t}}\right)\ide \\
&=\left(\frac{(1-\gamma_{0,t-h}^{2})(1-\gamma_{t-h,t}^{2})}{1-\gamma_{0,t}^{2}}+\gamma_{0,t-h}^{2}\frac{\delta^{2}(1-\gamma_{t-h,t}^{2})^{2}}{(1-\gamma_{0,t}^{2})(1-\gamma_{0,t}^{2}+\delta^{2}\gamma^{2}_{0,t})}\right)\ide \\
&=\frac{1-\gamma_{t-h,t}^{2}}{(1-\gamma_{0,t}^{2})(1-\gamma_{0,t}^{2}+\delta^{2}\gamma^{2}_{0,t})}\left((1-\gamma_{0,t-h}^{2})(1-\gamma_{0,t}^{2})+\delta^{2}\gamma_{0,t}^{2}(1-\gamma_{0,t-h}^{2})\right)\ide \\
& + \frac{1-\gamma_{t-h,t}^{2}}{(1-\gamma_{0,t}^{2})(1-\gamma_{0,t}^{2}+\delta^{2}\gamma^{2}_{0,t})}\left(\delta^{2}\gamma_{0,t-h}^{2}(1-\gamma_{t-h,t}^{2})\right)\ide \\
&=\frac{1-\gamma_{t-h,t}^{2}}{(1-\gamma_{0,t}^{2})(1-\gamma_{0,t}^{2}+\delta^{2}\gamma^{2}_{0,t})}\left((1-\gamma_{0,t-h}^{2})(1-\gamma_{0,t}^{2})+\delta^{2}\gamma_{0,t-h}^{2}(1-\gamma_{0,t}^{2})\right)\ide \\
&=\frac{(1-\gamma_{t-h,t}^{2})(1-\gamma_{0,t-h}^{2}+\delta^{2}\gamma_{0,t-h}^{2})}{1-\gamma_{0,t}^{2}+\delta^{2}\gamma_{0,t}^{2}}\ide = \Var{(X_{t-h}|X_{t}=x)}.
\end{split}

$$

\end{proof}

## 9·Reverse MR-VP SDE Solver

\label{app:mr_vp}

MR-VP DPM is characterized by the following forward and reverse diffusions:

$$

\label{eq:mrvp_fwd_sde}
dX_{t}=\frac{1}{2}\beta_{t}(\bar{X} - X_{t})dt + \sqrt{\beta_{t}}d\overrightarrow{W_{t}}\ ,

$$

$$

\label{eq:mrvp_rev_sde}
d\hat{X}_{t}=\left(\frac{1}{2}\beta_{t}(\bar{X} - \hat{X}_{t}) - \beta_{t}s_{\theta}(\hat{X}_{t}, \bar{X}, t)\right)dt + \sqrt{\beta_{t}}d\overleftarrow{W_{t}}.

$$

Using the same method as in Appendix [app:sde_solution](#app:sde_solution), we can show that for $s<t$

$$

\Law{(X_{t}|X_{s})}=\mathcal{N}(\gamma_{s,t}X_{s}+(1-\gamma_{s,t})\bar{X},(1-\gamma_{s,t}^{2})\ide), \ \ \ \ \gamma_{s,t}=e^{-\frac{1}{2}\int_{s}^{t}{\beta_{u}du}}.

$$

With the following notation:

$$

\mu_{s,t}=\gamma_{s,t}\frac{1-\gamma_{0,s}^{2}}{1-\gamma_{0,t}^{2}}, \ \ \ \  \nu_{s,t}=\gamma_{0,s}\frac{1-\gamma_{s,t}^{2}}{1-\gamma_{0,t}^{2}}, \ \ \ \
\sigma_{s,t}^{2}=\frac{(1-\gamma_{0,s}^{2})(1-\gamma_{s,t}^{2})}{1-\gamma_{0,t}^{2}}

$$

we can write down the parameters of Gaussian distribution $X_{s}|X_{t},X_{0}$:

$$

\mathbb{E}[X_{s}|X_{t},X_{0}]=\bar{X} + \mu_{s,t}(X_{t}-\bar{X}) + \nu_{s,t}(X_{0}-\bar{X}), \ \ \Var{(X_{s}|X_{t},X_{0})}=\sigma_{s,t}^{2}\ide.

$$

The Lemma [lm:net_optimal](#lm:net_optimal) for MR-VP DPMs takes the following shape:

$$

\begin{split}
s_{\theta^{*}}(x,\bar{X},t) &= -\frac{1}{1-\gamma_{0,t}^{2}}\left(x - (1-\gamma_{0,t})\bar{X} - \gamma_{0,t}\mathbb{E}_{p_{0|t}(\cdot|x)}X_{0}\right) \\&
=-\frac{1}{1-\gamma_{0,t}^{2}}\left((x - \bar{X}) - \gamma_{0,t}\left(\mathbb{E}_{p_{0|t}(\cdot|x)}X_{0} - \bar{X}\right)\right).
\end{split}

$$

The class of reverse SDE solvers we consider is

$$

\hat{X}_{t-h} = \hat{X}_{t} + \beta_{t}h\left(\left(\frac{1}{2}+\hat{\omega}_{t,h}\right)(\hat{X}_{t} - \bar{X}) + (1+\hat{\kappa}_{t,h})s_{\theta}(\hat{X}_{t},\bar{X},t)\right) + \hat{\sigma}_{t,h}\xi_{t},

$$

where $t=1,1-h,..,h$ and $\xi_{t}$ are i.i.d. samples from $\mathcal{N}(0,\ide)$.

Repeating the argument of the Theorem [th:main](#th:main) leads to the following optimal (in terms of likelihood of the forward diffusion sample paths) parameters:

$$

\begin{split}
\kappa_{t,h}^{*} =& \frac{\nu_{t-h,t}(1-\gamma_{0,t}^{2})}{\gamma_{0,t}\beta_{t}h} - 1,  \ \ \ \
\omega_{t,h}^{*}=\frac{\mu_{t-h,t}-1}{\beta_{t}h} + \frac{1+\kappa_{t,h}^{*}}{1-\gamma_{0,t}^{2}} -\frac{1}{2},
\\& (\sigma^{*}_{t,h})^{2} = \sigma^{2}_{t-h,t} + \frac{1}{n}\nu_{t-h,t}^{2}\mathbb{E}_{X_{t}}\left[\Tr{\left(\Var{(X_{0}|X_{t})}\right)}\right],
\end{split}

$$

which are actually the same as the optimal parameters ([eq:notation_correction](#eq:notation_correction)) for VP DPM.

It is of no surprise since MR-VP DPM and VP-DPM differ only by a constant shift.
# Diffusion-Based Voice Conversion With Fast Maximum Likelihood Sampling Scheme

<details>
<summary>基本信息</summary>

- 标题: "Diffusion-Based Voice Conversion With Fast Maximum Likelihood Sampling Scheme."
- 作者:
  - 01 Vadim Popov
  - 02 Ivan Vovk
  - 03 Vladimir Gogoryan
  - 04 Tasnima Sadekova
  - 05 Mikhail Kudinov
  - 06 Jiansheng Wei
- 链接:
  - [ArXiv](https://arxiv.org/abs/2109.13821v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2109.13821v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2021.09.28_2109.13821v1_Diffusion-Based_Voice_Conversion_With_Fast_Maximum_Likelihood_Sampling_Scheme.pdf)
  - [ArXiv:2109.13821v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2021.09.28_2109.13821v2_Diffusion-Based_Voice_Conversion_With_Fast_Maximum_Likelihood_Sampling_Scheme.pdf)
  - [Publication] #TODO

</details>

## Abstract

Voice conversion is a common speech synthesis task which can be solved in different ways depending on a particular real-world scenario.
The most challenging one often referred to as one-shot many-to-many voice conversion consists in copying target voice from only one reference utterance in the most general case when both source and target speakers do not belong to the training dataset.
We present a scalable high-quality solution based on diffusion probabilistic modeling and demonstrate its superior quality compared to state-of-the-art one-shot voice conversion approaches.
Moreover, focusing on real-time applications, we investigate general principles which can make diffusion models faster while keeping synthesis quality at a high level.
As a result, we develop a novel Stochastic Differential Equations solver suitable for various diffusion model types and generative tasks as shown through empirical studies and justify it by theoretical analysis.
The code is publicly available at \url{https://github.com/huawei-noah/Speech-Backbones/tree/main/DiffVC}.

## 1·Introduction

\label{sec:intro}

Voice conversion (VC) is the task of copying the target speaker's voice while preserving the linguistic content of the utterance pronounced by the source speaker.

Practical VC applications often require a model which is able to operate in one-shot mode (i.e. when only one reference utterance is provided to copy the target speaker's voice) for any source and target speakers.

Such models are usually referred to as one-shot many-to-many models (or sometimes zero-shot many-to-many models, or just any-to-any VC models).

It is challenging to build such a model since it should be able to adapt to a new unseen voice having only one spoken utterance pronounced with it, so it was not until recently that successful one-shot VC solutions started to appear.

Conventional one-shot VC models are designed as autoencoders whose latent space ideally contains only the linguistic content of the encoded utterance while target voice identity information (usually taking shape of speaker embedding) is fed to the decoder as conditioning.

Whereas in the pioneering AutoVC model [^Qian2019AutoVC] only speaker embedding from the pre-trained speaker verification network was used as conditioning, several other models improved on AutoVC enriching conditioning with phonetic features such as pitch and loudness [^Qian2020F0-Consistent], [^Nercessian2020Improved], or training voice conversion and speaker embedding networks jointly [^Chou2019One-Shot].

Also, several papers [^Lin2021FragmentVC], [^Ishihara2020Attention-Based], [^Liu2021Any-to-Many] made use of attention mechanism to better fuse specific features of the reference utterance into the source utterance thus improving the decoder performance.

Apart from providing the decoder with sufficiently rich information, one of the main problems autoencoder VC models face is to disentangle source speaker identity from speech content in the encoder.

Some models [^Qian2019AutoVC], [^Qian2020F0-Consistent], [^Nercessian2020Improved] solve this problem by introducing an information bottleneck.

Among other popular solutions of the disentanglement problem one can mention applying vector quantization technique to the content information [^Wu2020Vqvc+], [^Wang2021Vqmivc], utilizing features of Variational AutoEncoders [^Luong2021Many-to-Many], [^Saito2018Non-Parallel], [^Chou2019One-Shot], introducing instance normalization layers [^Chou2019One-Shot], [^Chen2021Again-Vc], and using Phonetic Posteriorgrams (PPGs) [^Nercessian2020Improved], [^Liu2021Any-to-Many].

The model we propose in this paper solves the disentanglement problem by employing the encoder predicting "average voice'': it is trained to transform mel features corresponding to each phoneme into mel features corresponding to this phoneme averaged across a large multi-speaker dataset.

As for decoder, in our VC model, it is designed as a part of a Diffusion Probabilistic Model (DPM) since this class of generative models has shown very good results in speech-related tasks like raw waveform generation [^Chen2021WaveGrad], [^Kong2021DiffWave] and mel feature generation [^Popov2021Grad-TTS], [^Jeong2021Diff-TTS].

However, this decoder choice poses a problem of slow inference because DPM forward pass scheme is iterative and to obtain high-quality results it is typically necessary to run it for hundreds of iterations [^Ho2020Denoising], [^Nichol2021Improved].

Addressing this issue, we develop a novel inference scheme that significantly reduces the number of iterations sufficient to produce samples of decent quality and does not require model re-training.

Although several attempts have been recently made to reduce the number of DPM inference steps [^Song2021Denoising], [^San-Roman2021Noise], [^Watson2021Learning], [^Kong2021On], [^Chen2021WaveGrad], most of them apply to some particular types of DPMs.

In contrast, our approach generalizes to all popular kinds of DPMs and has a strong connection with likelihood maximization.

This paper has the following structure: in Section [sec:vc_model](#sec:vc_model) we present a one-shot many-to-many VC model and describe DPM it relies on; Section [sec:ml_inference](#sec:ml_inference) introduces a novel DPM sampling scheme and establishes its connection with likelihood maximization; the experiments regarding voice conversion task as well as those demonstrating the benefits of the proposed sampling scheme are described in Section [sec:exp](#sec:exp); we conclude in Section [sec:outro](#sec:outro).

## 2·Voice Conversion Diffusion Model

\label{sec:vc_model}

As with many other VC models, the one we propose belongs to the family of autoencoders.

In fact, any conditional DPM with data-dependent prior (i.e. terminal distribution of forward diffusion) can be seen as such: forward diffusion gradually adding Gaussian noise to data can be regarded as encoder while reverse diffusion trying to remove this noise acts as a decoder.

DPMs are trained to minimize the distance (expressed in different terms for different model types) between the trajectories of forward and reverse diffusion processes thus, speaking from the perspective of autoencoders, minimizing reconstruction error.

Data-dependent priors have been proposed by [^Popov2021Grad-TTS] and [^Lee2021PriorGrad], and we follow the former paper due to the flexibility of the continuous DPM framework used there.

Our approach is summarized in Figure~[pic:main](#pic:main).

![](images/scheme.png)

<a id="pic:main">VC model training and inference. $Y$ stands for the training mel-spectrogram at training and the target mel-spectrogram at inference.

Speaker conditioning in the decoder is enabled by the speaker conditioning network $g_{t}(Y)$ where $Y=\{Y_{t}\}_{t\in[0, 1]}$ is the whole forward diffusion trajectory starting at $Y_{0}$.

Dotted arrows denote operations performed only at training.</a>

### Encoder

\label{subsec:encoder}

We choose average phoneme-level mel features as speaker-independent speech representation.

To train the encoder to convert input mel-spectrograms into those of "average voice'', we take three steps: (i) first, we apply Montreal Forced Aligner [^McAuliffe2017Montreal] to a large-scale multi-speaker LibriTTS dataset [^Zen2019LibriTTS] to align speech frames with phonemes;
(ii) next, we obtain average mel features for each particular phoneme by aggregating its mel features across the whole LibriTTS dataset;
(iii) the encoder is then trained to minimize mean square error between output mel-spectrograms and ground truth "average voice'' mel-spectrograms (i.e. input mel-spectrograms where each phoneme mel feature is replaced with the average one calculated on the previous step).

The encoder has exactly the same Transformer-based architecture used in Grad-TTS [^Popov2021Grad-TTS] except that its inputs are mel features rather than character or phoneme embeddings.

Note that unlike Grad-TTS the encoder is trained separately from the decoder described in the next section.

### Decoder

\label{subsec:decoder}

Whereas the encoder parameterizes the terminal distribution of the forward diffusion (i.e. the prior), the reverse diffusion is parameterized with the decoder.

Following [^Song2021Score-Based] we use It\^o calculus and define diffusions in terms of stochastic processes rather than discrete-time Markov chains.

The general DPM framework we utilize consists of forward and reverse diffusions given by the following Stochastic Differential Equations (SDEs):

$$

\label{eq:vc_fwd_sde}
dX_{t}=\frac{1}{2}\beta_{t}(\bar{X} - X_{t})dt + \sqrt{\beta_{t}}d\overrightarrow{W_{t}}\ ,

$$

$$

\label{eq:vc_rev_sde}
d\hat{X}_{t}=\left(\frac{1}{2}(\bar{X} - \hat{X}_{t}) - s_{\theta}(\hat{X}_{t}, \bar{X}, t)\right)\beta_{t}dt + \sqrt{\beta_{t}}d\overleftarrow{W_{t}}\ ,

$$

where $t\in [0,1]$, $\overrightarrow{W}$ and $\overleftarrow{W}$ are two independent Wiener processes in $\mathbb{R}^{n}$, $\beta_{t}$ is non-negative function referred to as *noise schedule*, $s_{\theta}$ is the score function with parameters $\theta$ and $\bar{X}$ is $n$-dimensional vector.

It can be shown [^Popov2021Grad-TTS] that the forward SDE ([eq:vc_fwd_sde](#eq:vc_fwd_sde)) allows for explicit solution:

$$

\label{eq:vc_solution}
\Law(X_{t}|X_{0}) = \mathcal{N}\left(e^{-\frac{1}{2}\int_{0}^{t}{\beta_{s}ds}}X_{0} + \left(1-e^{-\frac{1}{2}\int_{0}^{t}{\beta_{s}ds}}\right)\bar{X}, \left(1 - e^{-\int_{0}^{t}{\beta_{s}ds}}\right)\ide\right),

$$

where $\ide$ is $n\times n$ identity matrix.

Thus, if noise follows linear schedule $\beta_{t}=\beta_{0} + t(\beta_{1}-\beta_{0})$ for $\beta_{0}$ and $\beta_{1}$ such that $e^{-\int_{0}^{1}{\beta_{s}ds}}$ is close to zero, then $\Law{(X_{1})}$ is close to $\mathcal{N}(\bar{X},\ide)$ which is the prior in this DPM.

The reverse diffusion ([eq:vc_rev_sde](#eq:vc_rev_sde)) is trained by minimizing weighted $L_{2}$ loss:

$$

\label{eq:vc_training}
\theta^{*}=\argmin_{\theta}{\mathcal{L}(\theta)}=\argmin_{\theta}{\int_{0}^{1}{\lambda_{t}\mathbb{E}_{X_{0}, X_{t}}\Vert s_{\theta}(X_{t}, \bar{X}, t) - \nabla{\log{p_{t|0}(X_{t}|X_{0})}}\Vert_{2}^{2}}}dt,

$$

where $p_{t|0}(X_{t}|X_{0})$ is the probability density function (pdf) of the conditional distribution ([eq:vc_solution](#eq:vc_solution)) and $\lambda_{t}=1 - e^{-\int_{0}^{t}{\beta_{s}ds}}$.

The distribution ([eq:vc_solution](#eq:vc_solution)) is Gaussian, so we have

$$

\label{eq:vc_logp}
\nabla\log{p_{t|0}(X_{t}|X_{0})} = -\frac{X_{t}-X_{0}e^{-\frac{1}{2}\int_{0}^{t}{\beta_{s}ds}}-\bar{X}(1-e^{-\frac{1}{2}\int_{0}^{t}{\beta_{s}ds}})}{1 - e^{-\int_{0}^{t}{\beta_{s}ds}}}.

$$

At training, time variable $t$ is sampled uniformly from $[0,1]$, noisy samples $X_{t}$ are generated according to the formula ([eq:vc_solution](#eq:vc_solution)) and the formula ([eq:vc_logp](#eq:vc_logp)) is used to calculate loss function $\mathcal{L}$ on these samples.

Note that $X_{t}$ can be sampled without the necessity to calculate intermediate values $\{X_{s}\}_{0<s<t}$ which makes optimization task ([eq:vc_training](#eq:vc_training)) time and memory efficient.

A well-trained reverse diffusion ([eq:vc_rev_sde](#eq:vc_rev_sde)) has trajectories that are close to those of the forward diffusion ([eq:vc_fwd_sde](#eq:vc_fwd_sde)), so generating data with this DPM can be performed by sampling $\hat{X}_{1}$ from the prior $\mathcal{N}(\bar{X}, \ide)$ and solving SDE ([eq:vc_rev_sde](#eq:vc_rev_sde)) backwards in time.

The described above DPM was introduced by [^Popov2021Grad-TTS] for text-to-speech task and we adapt it for our purposes.

We put $\bar{X}=\varphi(X_{0})$ where $\varphi$ is the encoder, i.e. $\bar{X}$ is the "average voice'' mel-spectrogram which we want to transform into that of the target voice.

We condition the decoder $s_{\theta}=s_{\theta}(\hat{X}_{t},\bar{X},g_{t}(Y),t)$ on some trainable function $g_{t}(Y)$ to provide it with information about the target speaker ($Y$ stands for forward trajectories of the target mel-spectrogram at inference and the ones of the training mel-spectrogram at training).

This function is a neural network trained jointly with the decoder.

We experimented with three input types for this network:

-  *d-only* -- the input is the speaker embedding extracted from the target mel-spectrogram $Y_{0}$ with the pre-trained speaker verification network employed in [^Jia2018Transfer];

-  *wodyn* -- in addition, the noisy target mel-spectrogram $Y_{t}$ is used as input;

-  *whole* -- in addition, the whole dynamics of the target mel-spectrogram under forward diffusion $\{Y_{s}|s=0.5/15,1.5/15,..,14.5/15\}$ is used as input.

The decoder architecture is based on U-Net [^Ronneberger2015U-Net] and is the same as in Grad-TTS but with four times more channels to better capture the whole range of human voices.

The speaker conditioning network $g_{t}(Y)$ is composed of $2$D convolutions and MLPs and described in detail in Appendix [app:h](#app:h).

Its output is $128$-dimensional vector which is broadcast-concatenated to the concatenation of $\hat{X}_{t}$ and $\bar{X}$ as additional $128$ channels.

### Related VC models

\label{subsec:related}

To the best of our knowledge, there exist two diffusion-based voice conversion models: VoiceGrad [^Kameoka2020VoiceGrad] and DiffSVC [^Liu2021DiffSVC].

The one we propose differs from them in several important aspects.

First, neither of the mentioned papers considers a one-shot many-to-many voice conversion scenario.

Next, these models take no less than $100$ reverse diffusion steps at inference while we pay special attention to reducing the number of iterations (see Section [sec:ml_inference](#sec:ml_inference)) achieving good quality with as few as $6$ iterations.

Furthermore, VoiceGrad performs voice conversion by running Langevin dynamics starting from the source mel-spectrogram, thus implicitly assuming that forward diffusion trajectories starting from the mel-spectrogram we want to synthesize are likely to pass through the neighborhood of the source mel-spectrogram on their way to Gaussian noise.

Such an assumption allowing to have only one network instead of encoder-decoder architecture is too strong and hardly holds for real voices.

Finally, DiffSVC performs singing voice conversion and relies on PPGs as speaker-independent speech representation.

## 3·Maximum Likelihood SDE Solver

\label{sec:ml_inference}

In this section, we develop a fixed-step first-order reverse SDE solver that maximizes the log-likelihood of sample paths of the forward diffusion.

This solver differs from general-purpose Euler-Maruyama SDE solver [^Kloeden1992Numerical] by infinitesimally small values which can however become significant when we sample from diffusion model using a few iterations.

Consider the following forward and reverse SDEs defined in Euclidean space $\mathbb{R}^{n}$ for $t\in [0,1]$:

$$

\label{eq:fwd_rev_sde}
dX_{t}=-\frac{1}{2}\beta_{t}X_{t}dt + \sqrt{\beta_{t}}d\overrightarrow{W_{t}} \ \ (F), \ \ \ d\hat{X}_{t}=\left(-\frac{1}{2}\beta_{t}\hat{X}_{t} - \beta_{t}s_{\theta}(\hat{X}_t, t)\right)dt + \sqrt{\beta_{t}}d\overleftarrow{W_{t}} \ \ (R),

$$

where $\overrightarrow{W}$ is a forward Wiener process (i.e. its forward increments $\overrightarrow{W_{t}} - \overrightarrow{W_{s}}$ are independent of $\overrightarrow{W_{s}}$ for $t > s$) and $\overleftarrow{W}$ is a backward Wiener process (i.e. backward increments $\overleftarrow{W_{s}} - \overleftarrow{W_{t}}$ are independent of $\overleftarrow{W_{t}}$ for $s < t$).

Following [^Song2021Score-Based] we will call DPM ([eq:fwd_rev_sde](#eq:fwd_rev_sde)) Variance Preserving (VP).

For simplicity we will derive maximum likelihood solver for this particular type of diffusion models.

The equation ([eq:vc_fwd_sde](#eq:vc_fwd_sde)) underlying VC diffusion model described in Section [sec:vc_model](#sec:vc_model) can be transformed into the equation ([eq:fwd_rev_sde](#eq:fwd_rev_sde)-F) by a constant shift and we will call such diffusion models Mean Reverting Variance Preserving (MR-VP).

VP model analysis carried out in this section can be easily extended (see Appendices [app:mr_vp](#app:mr_vp), [app:sub_vp](#app:sub_vp) and [app:ve](#app:ve)) to MR-VP model as well as to other common diffusion model types such as sub-VP and VE described by [^Song2021Score-Based].

The forward SDE ([eq:fwd_rev_sde](#eq:fwd_rev_sde)-F) allows for explicit solution:

$$

\label{eq:xt_distribution}
\Law(X_{t} | X_{s}) = \mathcal{N}(\gamma_{s,t}X_{s}, (1-\gamma_{s,t}^2)\ide), \ \ \ \ \gamma_{s,t} = \exp{\left(-\frac{1}{2}\int_{s}^{t}{\beta_{u}du}\right)},

$$

for all $0\leq s < t \leq 1$.

This formula is derived by means of It\^o calculus in Appendix [app:sde_solution](#app:sde_solution).

The reverse SDE ([eq:fwd_rev_sde](#eq:fwd_rev_sde)-R) parameterized with a neural network $s_{\theta}$ is trained to approximate gradient of the log-density of noisy data $X_{t}$:

$$

\label{eq:objective}
\theta^{*}=\argmin_{\theta}{\int_{0}^{1}{\lambda_{t}\mathbb{E}_{X_{t}}\Vert s_{\theta}(X_{t}, t) - \nabla{\log{p_{t}(X_{t})}}\Vert_{2}^{2}}}dt,

$$

where the expectation is taken with respect to noisy data distribution $\Law(X_{t})$ with pdf $p_{t}(\cdot)$ and $\lambda_{t}$ is some positive weighting function.

Note that certain Lipschitz constraints should be satisfied by coefficients of SDEs ([eq:fwd_rev_sde](#eq:fwd_rev_sde)) to guarantee existence of strong solutions [^Liptser1978Statistics], and throughout this section we assume these conditions are satisfied as well as those from [^Anderson1982Reverse-Time] which guarantee that paths $\hat{X}$ generated by the reverse SDE ([eq:fwd_rev_sde](#eq:fwd_rev_sde)-R) for the optimal $\theta^{*}$ equal forward SDE ([eq:fwd_rev_sde](#eq:fwd_rev_sde)-F) paths $X$ in distribution.

The generative procedure of a VP DPM consists in solving the reverse SDE ([eq:fwd_rev_sde](#eq:fwd_rev_sde)-R) backwards in time starting from $\hat{X}_{1}\sim \mathcal{N}(0,\ide)$.

Common Euler-Maruyama solver introduces discretization error [^Kloeden1992Numerical] which may harm sample quality when the number of iterations is small.

At the same time, it is possible to design unbiased [^Henry-Labordère2017Unbiased] or even exact [^Beskos2005Exact] numerical solvers for some particular SDE types.

The Theorem~[th:main](#th:main) shows that in the case of diffusion models we can make use of the forward diffusion ([eq:fwd_rev_sde](#eq:fwd_rev_sde)-F) and propose a reverse SDE solver which is better than the general-purpose Euler-Maruyama one in terms of likelihood.

The solver proposed in the Theorem [th:main](#th:main) is expressed in terms of the values defined as follows:

$$

\label{eq:notation_gamma}
\mu_{s,t}=\gamma_{s,t}\frac{1-\gamma_{0,s}^{2}}{1-\gamma_{0,t}^{2}}, \ \ \ \  \nu_{s,t}=\gamma_{0,s}\frac{1-\gamma_{s,t}^{2}}{1-\gamma_{0,t}^{2}}, \ \ \ \
\sigma_{s,t}^{2}=\frac{(1-\gamma_{0,s}^{2})(1-\gamma_{s,t}^{2})}{1-\gamma_{0,t}^{2}},

$$

$$

\begin{split}
\label{eq:notation_correction}
\kappa_{t,h}^{*} =& \frac{\nu_{t-h,t}(1-\gamma_{0,t}^{2})}{\gamma_{0,t}\beta_{t}h} - 1,  \ \ \ \
\omega_{t,h}^{*}=\frac{\mu_{t-h,t}-1}{\beta_{t}h} + \frac{1+\kappa_{t,h}^{*}}{1-\gamma_{0,t}^{2}} -\frac{1}{2},
\\& (\sigma^{*}_{t,h})^{2} = \sigma^{2}_{t-h,t} + \frac{1}{n}\nu_{t-h,t}^{2}\mathbb{E}_{X_{t}}\left[\Tr{\left(\Var{(X_{0}|X_{t})}\right)}\right],
\end{split}

$$

where $n$ is data dimensionality, $\Var{(X_{0}|X_{t})}$ is the covariance matrix of the conditional data distribution $\Law(X_{0}|X_{t})$ (so, $\Tr(\Var{(X_{0}|X_{t})})$ is the overall variance across all $n$ dimensions) and the expectation $\mathbb{E}_{X_{t}}[\cdot]$ is taken with respect to the unconditional noisy data distribution $\Law(X_{t})$.

\begin{theorem}
\label{th:main}
Consider a DPM characterized by SDEs ([eq:fwd_rev_sde](#eq:fwd_rev_sde)) with reverse diffusion trained till optimality.

Let $N\in \mathbb{N}$ be any natural number and $h=1/N$.

Consider the following class of fixed step size $h$ reverse SDE solvers parameterized with triplets of real numbers \{$(\hat{\kappa}_{t,h}, \hat{\omega}_{t,h}, \hat{\sigma}_{t,h})|t=h,2h,..,1$\}:

$$

\label{eq:sde_solver}
\hat{X}_{t-h} = \hat{X}_{t} + \beta_{t}h\left(\left(\frac{1}{2}+\hat{\omega}_{t,h}\right)\hat{X}_{t} + (1+\hat{\kappa}_{t,h})s_{\theta^{*}}(\hat{X}_{t},t)\right) + \hat{\sigma}_{t,h}\xi_{t},

$$

where $\theta^{*}$ is given by ([eq:objective](#eq:objective)), $t=1,1-h,..,h$ and $\xi_{t}$ are i.i.d. samples from $\mathcal{N}(0,\ide)$.

Then:

\begin{enumerate}

- [(i)] Log-likelihood of sample paths $X=\{X_{kh}\}_{k=0}^{N}$ under generative model $\hat{X}$ is maximized for $\hat{\kappa}_{t,h}=\kappa_{t,h}^{*}$, $\hat{\omega}_{t,h}=\omega_{t,h}^{*}$ and $\hat{\sigma}_{t,h}=\sigma^{*}_{t,h}$.

- [(ii)] Assume that the SDE solver ([eq:sde_solver](#eq:sde_solver)) starts from random variable $\hat{X}_{1}\sim \Law{(X_{1})}$.

If $X_{0}$ is a constant or a Gaussian random variable with diagonal isotropic covariance matrix (i.e. $\delta^{2}\ide$ for $\delta>0$), then generative model $\hat{X}$ is exact for $\hat{\kappa}_{t,h}=\kappa_{t,h}^{*}$, $\hat{\omega}_{t,h}=\omega_{t,h}^{*}$ and $\hat{\sigma}_{t,h}=\sigma^{*}_{t,h}$.
\end{enumerate}

\end{theorem}

The Theorem [th:main](#th:main) provides an improved DPM sampling scheme which comes at no additional computational cost compared to standard methods (except for data-dependent term in $\sigma^{*}$ as discussed in Section [subsec:sampling](#subsec:sampling)) and requires neither model re-training nor extensive search on noise schedule space.

The proof of this theorem is given in Appendix [app:th_proof](#app:th_proof).

Note that it establishes optimality of the reverse SDE solver ([eq:sde_solver](#eq:sde_solver)) with the parameters ($[eq:notation_correction](#eq:notation_correction)$) in terms of likelihood of *discrete* paths $X=\{X_{kh}\}_{k=0}^{N}$ while the optimality of *continuous* model ([eq:fwd_rev_sde](#eq:fwd_rev_sde)-R) on *continuous* paths $\{X_{t}\}_{t\in[0,1]}$ is guaranteed for a model with parameters $\theta=\theta^{*}$ as shown in [^Song2021Score-Based].

The class of reverse SDE solvers considered in the Theorem [th:main](#th:main) is rather broad: it is the class of all fixed-step solvers whose increments at time $t$ are linear combination of $\hat{X}_{t}$, $s_{\theta}(\hat{X}_{t},t)$ and Gaussian noise with zero mean and diagonal isotropic covariance matrix.

As a particular case it includes Euler-Maruyama solver ($\hat{\kappa}_{t,h}\equiv 0$, $\hat{\omega}_{t,h}\equiv 0$, $\hat{\sigma}_{t,h}\equiv\sqrt{\beta_{t}h}$) and for fixed $t$ and $h\to 0$ we have $\kappa^{*}_{t,h}=\bar{o}(1)$, $\omega^{*}_{t,h}=\bar{o}(1)$ and $\sigma^{*}_{t,h}=\sqrt{\beta_{t}h}(1 + \bar{o}(1))$ (the proof is given in Appendix [app:asymptotics](#app:asymptotics)), so the optimal SDE solver significantly differs from general-purpose Euler-Maruyama solver only when $N$ is rather small or $t$ has the same order as $h$, i.e. on the final steps of DPM inference.

Appendix [app:toy](#app:toy) contains toy examples demonstrating the difference of the proposed optimal SDE solver and Euler-Maruyama one depending on step size.

The result *(ii)* from the Theorem [th:main](#th:main) strengthens the result *(i)* for some particular data distributions, but it may seem useless since in practice data distribution is far from being constant or Gaussian.

However, in case of generation with strong conditioning (e.g. mel-spectrogram inversion) the assumptions on the data distribution may become viable: in the limiting case when our model is conditioned on $c=\psi(X_{0})$ for an injective function $\psi$, random variable $X_{0}|c$ becomes a constant $\psi^{-1}(c)$.

## 4·Experiments

\label{sec:exp}

We trained two groups of models: *Diff-VCTK* models on VCTK [^Yamagishi2019Cstr] dataset containing $109$ speakers ($9$ speakers were held out for testing purposes) and *Diff-LibriTTS* models on LibriTTS [^Zen2019LibriTTS] containing approximately $1100$ speakers ($10$ speakers were held out).

For every model both encoder and decoder were trained on the same dataset.

Training hyperparameters, implementation and data processing details can be found in Appendix [app:details](#app:details).

For mel-spectrogram inversion, we used the pre-trained universal HiFi-GAN vocoder [^Kong2020HiFi-Gan] operating at $22.05$kHz.

All subjective human evaluation was carried out on Amazon Mechanical Turk (AMT) with Master assessors to ensure the reliability of the obtained Mean Opinion Scores (MOS).

In all AMT tests we considered unseen-to-unseen conversion with $25$ unseen (for both *Diff-VCTK* and *Diff-LibriTTS*) speakers: $9$ VCTK speakers, $10$ LibriTTS speakers and $6$ internal speakers.

For VCTK source speakers we also ensured that source phrases were unseen during training.

We place other details of listening AMT tests in Appendix [app:subj](#app:subj).

A small subset of speech samples used in them is available at our demo page \url{https://diffvc-fast-ml-solver.github.io} which we encourage to visit.

<a id="tab:conditioning">Input types for speaker conditioning $g_{t}(Y)$ compared in terms of speaker similarity.</a>

As for sampling, we considered the following class of reverse SDE solvers:

$$

\label{eq:vc_solver}
\hat{X}_{t-h} = \hat{X}_{t} + \beta_{t}h\left(\left(\frac{1}{2}+\hat{\omega}_{t,h}\right)(\hat{X}_{t} - \bar{X}) + (1+\hat{\kappa}_{t,h})s_{\theta}(\hat{X}_{t},\bar{X},g_{t}(Y),t)\right) + \hat{\sigma}_{t,h}\xi_{t},

$$

where $t=1,1-h,..,h$ and $\xi_{t}$ are i.i.d. samples from $\mathcal{N}(0,\ide)$.

For $\hat{\kappa}_{t,h}=\kappa^{*}_{t,h}$, $\hat{\omega}_{t,h}=\omega^{*}_{t,h}$ and $\hat{\sigma}_{t,h}=\sigma^{*}_{t,h}$ (where $\kappa^{*}_{t,h}$, $\omega^{*}_{t,h}$ and $\sigma^{*}_{t,h}$ are given by ([eq:notation_correction](#eq:notation_correction))) it becomes maximum likelihood reverse SDE solver for MR-VP DPM ([eq:vc_fwd_sde](#eq:vc_fwd_sde)-[eq:vc_rev_sde](#eq:vc_rev_sde)) as shown in Appendix [app:mr_vp](#app:mr_vp).

In practice it is not trivial to estimate variance of the conditional distribution $\Law{(X_{0}|X_{t})}$, so we skipped this term in $\sigma^{*}_{t,h}$ assuming this variance to be rather small because of strong conditioning on $g_{t}(Y)$ and just used $\hat{\sigma}_{t,h}=\sigma_{t-h,t}$ calling this sampling method *ML-N* ($N=1/h$ is the number of SDE solver steps).

We also experimented with Euler-Maruyama solver *EM-N* (i.e. $\hat{\kappa}_{t,h}=0$, $ \hat{\omega}_{t,h}=0$, $\hat{\sigma}_{t,h}=\sqrt{\beta_{t}h}$) and "probability flow sampling'' from [^Song2021Score-Based] which we denote by *PF-N* ($\hat{\kappa}_{t,h}=-0.5$, $ \hat{\omega}_{t,h}=0$, $\hat{\sigma}_{t,h}=0$).

### Speaker conditioning analysis

\label{subsec:ablation}

For each dataset we trained three models -- one for each input type for the speaker conditioning network $g_{t}(Y)$ (see Section [subsec:decoder](#subsec:decoder)).

Although these input types had much influence neither on speaker similarity nor on speech naturalness, we did two experiments to choose the best models (one for each training dataset) in terms of speaker similarity for further comparison with baseline systems.

We compared voice conversion results (produced by *ML-30* sampling scheme) on $92$ source-target pairs.

AMT workers were asked which of three models (if any) sounded most similar to the target speaker and which of them (if any) sounded least similar.

For *Diff-VCTK* and *Diff-LibriTTS* models each conversion pair was evaluated $4$ and $5$ times respectively.

Table~[tab:conditioning](#tab:conditioning) demonstrates that for both Â *Diff-VCTK* and *Diff-LibriTTS* the best option is *wodyn*, i.e. to condition the decoder at time $t$ on the speaker embedding together with the noisy target mel-spectrogram $Y_{t}$.

Conditioning on $Y_{t}$ allows making use of diffusion-specific information of how the noisy target sounds whereas embedding from the pre-trained speaker verification network contains information only about the clean target.

Taking these results into consideration, we used *Diff-VCTK-wodyn* and *Diff-LibriTTS-wodyn* in the remaining experiments.

### Any-to-any voice conversion

\label{subsec:any2any}

<a id="tab:main_vctk">Subjective evaluation (MOS) of one-shot VC models trained on VCTK.

Ground truth recordings were evaluated only for VCTK speakers.</a>

<a id="tab:main_large">Subjective evaluation (MOS) of one-shot VC models trained on large-scale datasets.</a>

We chose four recently proposed VC models capable of one-shot many-to-many synthesis as the baselines:

-  *AGAIN-VC* [^Chen2021Again-Vc], an improved version of a conventional autoencoder AdaIN-VC solving the disentanglement problem by means of instance normalization;

-  *FragmentVC* [^Lin2021FragmentVC], an attention-based model relying on wav2vec 2.0 [^Baevski2020Wav2vec] to obtain speech content from the source utterance;

-  *VQMIVC* [^Wang2021Vqmivc], state-of-the-art approach among those employing vector quantization techniques;

-  *BNE-PPG-VC* [^Liu2021Any-to-Many], an improved variant of PPG-based VC models combining a bottleneck feature extractor obtained from a phoneme recognizer with a seq2seq-based synthesis module.

As shown in [^Kim2021Assem-Vc], PPG-based VC models provide high voice conversion quality competitive even with that of the state-of-the-art VC models taking text transcription corresponding to the source utterance as input.

Therefore, we can consider *BNE-PPG-VC* a state-of-the-art model in our setting.

Baseline voice conversion results were produced by the pre-trained VC models provided in official GitHub repositories.

Since only *BNE-PPG-VC* has the model pre-trained on a large-scale dataset (namely, LibriTTS + VCTK), we did two subjective human evaluation tests: the first one comparing *Diff-VCTK* with *AGAIN-VC*, *FragmentVC* and *VQMIVC* trained on VCTK and the second one comparing *Diff-LibriTTS* with *BNE-PPG-VC*.

The results of these tests are given in Tables [tab:main_vctk](#tab:main_vctk) and [tab:main_large](#tab:main_large) respectively.

Speech naturalness and speaker similarity were assessed separately.

AMT workers evaluated voice conversion quality on $350$ source-target pairs on $5$-point scale.

In the first test, each pair was assessed $6$ times on average both in speech naturalness and speaker similarity evaluation; as for the second one, each pair was assessed $8$ and $9$ times on average in speech naturalness and speaker similarity evaluation correspondingly.

No less than $41$ unique assessors took part in each test.

Table~[tab:main_vctk](#tab:main_vctk) demonstrates that our model performs significantly better than the baselines both in terms of naturalness and speaker similarity even when $6$ reverse diffusion iterations are used.

Despite working almost equally well on VCTK speakers, the best baseline *VQMIVC* shows poor performance on other speakers perhaps because of not being able to generalize to different domains with lower recording quality.

Although *Diff-VCTK* performance also degrades on non-VCTK speakers, it achieves good speaker similarity of MOS $3.6$ on VCTK ones when *ML-30* sampling scheme is used and only slightly worse MOS $3.5$ when $5$x less iterations are used at inference.

<a id="tab:sampling">Reverse SDE solvers compared in terms of FID. $N$ is the number of SDE solver steps.</a>

Table~[tab:main_large](#tab:main_large) contains human evaluation results of *Diff-LibriTTS* for four sampling schemes: *ML-30* with $30$ reverse SDE solver steps and *ML-6*, *EM-6* and *PF-6* with $6$ steps of reverse diffusion.

The three schemes taking $6$ steps achieved real-time factor (RTF) around $0.1$ on GPU (i.e. inference was $10$ times faster than real time) while the one taking $30$ steps had RTF around $0.5$.

The proposed model *Diff-LibriTTS-ML-30* and the baseline *BNE-PPG-VC* show the same performance on the VCTK test set in terms of speech naturalness the latter being slightly better in terms of speaker similarity which can perhaps be explained by the fact that *BNE-PPG-VC* was trained on the union of VCTK and LibriTTS whereas our model was trained only on LibriTTS.

As for the whole test set containing unseen LibriTTS and internal speakers also, *Diff-LibriTTS-ML-30* outperforms *BNE-PPG-VC* model achieving MOS $4.0$ and $3.4$ in terms of speech naturalness and speaker similarity respectively.

Due to employing PPG extractor trained on a large-scale ASR dataset LibriSpeech [^Panayotov2015Librispeech], *BNE-PPG-VC* has fewer mispronunciation issues than our model but synthesized speech suffers from more sonic artifacts.

This observation makes us believe that incorporating PPG features in the proposed diffusion VC framework is a promising direction for future research.

Table~[tab:main_large](#tab:main_large) also demonstrates the benefits of the proposed maximum likelihood sampling scheme over other sampling methods for a small number of inference steps: only *ML-N* scheme allows us to use as few as $N=6$ iterations with acceptable quality degradation of MOS $0.2$ and $0.1$ in terms of naturalness and speaker similarity respectively while two other competing methods lead to much more significant quality degradation.

### Maximum likelihood sampling

\label{subsec:sampling}

![](images/em.png)

<a id="pic:cifar">CIFAR-$10$ images randomly sampled from VP DPM by running $10$ reverse diffusion steps with the following schemes (from left to right): "euler-maruyama'', "probability flow'', "maximum likelihood ($\tau=0.5$)'', "maximum likelihood ($\tau=1.0$)''.</a>

To show that the maximum likelihood sampling scheme proposed in Section [sec:ml_inference](#sec:ml_inference) generalizes to different tasks and DPM types, we took the models trained by [^Song2021Score-Based] on CIFAR-$10$ image generation task and compared our method with other sampling schemes described in that paper in terms of Fr\'{e}chet Inception Distance (FID).

The main difficulty in applying maximum likelihood SDE solver is estimating data-dependent term $\mathbb{E}[\Tr{(\Var{(X_{0}|X_{t})})}]$ in $\sigma^{*}_{t,h}$.

Although in the current experiments we just set this term to zero, we can think of two possible ways to estimate it: (i) approximate $\Var{(X_{0}|X_{t})}$ with $\Var{(\hat{X}_{0}|\hat{X}_{t}=X_{t})}$: sample noisy data $X_{t}$, solve reverse SDE with sufficiently small step size starting from terminal condition $\hat{X}_{t}=X_{t}$ several times, and calculate sample variance of the resulting solutions at initial points $\hat{X}_{0}$; (ii) use the formula ([eq:gaussian_revert](#eq:gaussian_revert)) from Appendix [app:th_proof](#app:th_proof) to calculate $\Var{(X_{0}|X_{t})}$ assuming that $X_{0}$ is distributed normally with mean and variance equal to sample mean and sample variance computed on the training dataset.

Experimenting with these techniques and exploring new ones seems to be an interesting future research direction.

Another important practical consideration is that the proposed scheme is proven to be optimal only for score matching networks trained till optimality.

Therefore, in the experiments whose results are reported in Table~[tab:sampling](#tab:sampling) we apply maximum likelihood sampling scheme only when $t\leq\tau$ while using standard Euler-Maruyama solver for $t>\tau$ for some hyperparameter $\tau\in [0,1]$.

Such a modification relies on the assumption that score matching network is closer to being optimal for smaller noise.

Table [tab:sampling](#tab:sampling) shows that despite likelihood and FID are two metrics that do not perfectly correlate [^Song2021Maximum], in most cases our maximum likelihood SDE solver performs best in terms of FID.

Also, it is worth mentioning that although $\tau=1$ is always rather a good choice, tuning this hyperparameter can lead to even better performance.

One can find randomly chosen generated images for various sampling methods in Figure~[pic:cifar](#pic:cifar).

## 5·Conclusion

\label{sec:outro}

In this paper, the novel one-shot many-to-many voice conversion model has been presented.

Its encoder design and powerful diffusion-based decoder made it possible to achieve good results both in terms of speaker similarity and speech naturalness even on out-of-domain unseen speakers.

Subjective human evaluation verified that the proposed model delivers scalable VC solution with competitive performance.

Furthermore, aiming at fast synthesis, we have developed and theoretically justified the novel sampling scheme.

The main idea behind it is to modify the general-purpose Euler-Maruyama SDE solver so as to maximize the likelihood of discrete sample paths of the forward diffusion.

Due to the proposed sampling scheme, our VC model is capable of high-quality voice conversion with as few as $6$ reverse diffusion steps.

Moreover, experiments on the image generation task show that all known diffusion model types can benefit from the proposed SDE solver.

\bibliography{diffusion_paper}
\bibliographystyle{iclr2022}

\newpage

\appendix

## 6·Forward VP SDE Solution

\label{app:sde_solution}

Since function $\gamma_{0,t}^{-1}X_{t}$ is linear in $X_{t}$, taking its differential does not require second order derivative term in It\^o's formula:

$$

\begin{split}
d(\gamma_{0,t}^{-1}X_{t})&=d\left(e^{\frac{1}{2}\int_{0}^{t}{\beta_{u}du}}X_{t}\right)\\&=e^{\frac{1}{2}\int_{0}^{t}{\beta_{u}du}}\cdot \frac{1}{2}\beta_{t}X_{t}dt + e^{\frac{1}{2}\int_{0}^{t}{\beta_{u}du}}\cdot \left(-\frac{1}{2}\beta_{t}X_{t}dt + \sqrt{\beta_{t}}d\overrightarrow{W_{t}}\right) \\&= \sqrt{\beta_{t}}e^{\frac{1}{2}\int_{0}^{t}{\beta_{u}du}}d\overrightarrow{W_{t}}.
\end{split}

$$

Integrating this expression from $s$ to $t$ results in an It\^o's integral:

$$

e^{\frac{1}{2}\int_{0}^{t}{\beta_{u}du}}X_{t} - e^{\frac{1}{2}\int_{0}^{s}{\beta_{u}du}}X_{s} = \int_{s}^{t}{\sqrt{\beta_{\tau}}e^{\frac{1}{2}\int_{0}^{\tau}{\beta_{u}du}}d\overrightarrow{W_{\tau}}},

$$

or

$$

X_{t} = e^{-\frac{1}{2}\int_{s}^{t}{\beta_{u}du}}X_{s} + \int_{s}^{t}{\sqrt{\beta_{\tau}}e^{-\frac{1}{2}\int_{\tau}^{t}{\beta_{u}du}}d\overrightarrow{W_{\tau}}}.

$$

The integrand on the right-hand side is deterministic and belongs to $L_{2}[0,1]$ (for practical noise schedule choices), so its It\^o's integral is a normal random variable, a martingale (meaning it has zero mean) and satisfies It\^o's isometry which allows to calculate its variance:

$$

\Var(X_{t}|X_{s}) = \int_{s}^{t}{\beta_{\tau}}e^{-\int_{\tau}^{t}{\beta_{u}du}}\ide d\tau = \left(1 - e^{-\int_{s}^{t}{\beta_{u}du}}\right)\ide .

$$

Thus

$$

\Law(X_{t}|X_{s}) = \mathcal{N}\left(e^{-\frac{1}{2}\int_{s}^{t}{\beta_{u}du}}X_{s}, \left(1 - e^{-\int_{s}^{t}{\beta_{u}du}}\right)\ide\right) = \mathcal{N}(\gamma_{s,t}X_{s}, (1-\gamma_{s,t}^{2})\ide)

$$

## 7·The Optimal Coefficients Asymptotics

\label{app:asymptotics}

First derive asymptotics for $\gamma$:

$$

\gamma_{t-h, t} = e^{-\frac{1}{2}\int_{t-h}^{t}{\beta_{u}du}} = 1 - \frac{1}{2}\beta_{t}h + \bar{o}(h),

$$

$$

\gamma_{0, t-h}^{2}=e^{-\int_{0}^{t-h}{\beta_{u}du}}=e^{-\int_{0}^{t}{\beta_{u}du}}e^{\int_{t-h}^{t}{\beta_{u}du}}=\gamma_{0,t}^{2}(1 + \beta_{t}h) + \bar{o}(h),

$$

$$

\gamma_{0, t-h}=e^{-\frac{1}{2}\int_{0}^{t-h}{\beta_{u}du}}=e^{-\frac{1}{2}\int_{0}^{t}{\beta_{u}du}}e^{\frac{1}{2}\int_{t-h}^{t}{\beta_{u}du}}=\gamma_{0,t}(1 + \frac{1}{2}\beta_{t}h) + \bar{o}(h),

$$

$$

\gamma_{t-h, t}^{2} = e^{-\int_{t-h}^{t}{\beta_{u}du}} = 1 - \beta_{t}h + \bar{o}(h).

$$

Then find asymptotics for $\mu$, $\nu$ and $\sigma^{2}$:

$$

\mu_{t-h,t}=\left(1 - \frac{1}{2}\beta_{t}h + \bar{o}(h)\right)\frac{1-\gamma_{0,t}^{2}-\gamma_{0,t}^{2}\beta_{t}h + \bar{o}(h)}{1-\gamma_{0,t}^{2}} = 1 - \frac{1}{2}\beta_{t}h-\frac{\gamma_{0,t}^{2}}{1-\gamma_{0,t}^{2}}\beta_{t}h + \bar{o}(h),

$$

$$

\nu_{t-h,t} = (\gamma_{0,t}(1 + \frac{1}{2}\beta_{t}h) + \bar{o}(h))\frac{\beta_{t}h + \bar{o}(h)}{1 - \gamma_{0,t}^{2}} = \frac{\gamma_{0,t}}{1 - \gamma_{0,t}^{2}}\beta_{t}h + \bar{o}(h),

$$

$$

\sigma^{2}_{t-h,t} = \frac{1}{1 - \gamma_{0,t}^{2}}(\beta_{t}h + \bar{o}(h))(1 - \gamma_{0,t}^{2}(1 + \beta_{t}h) + \bar{o}(h)) = \beta_{t}h + \bar{o}(h).

$$

Finally we get asymptotics for $\kappa^{*}$, $\omega^{*}$ and $\sigma^{*}$:

$$

\begin{split}
\kappa_{t,h}^{*}&=\frac{\nu_{t-h,t}(1 - \gamma_{0,t}^{2})}{\gamma_{0,t}\beta_{t}h} - 1 = \frac{\gamma_{0,t-h}(1 - \gamma_{t-h,t}^{2})}{\gamma_{0,t}\beta_{t}h} - 1 \\& =\frac{(\beta_{t}h + \bar{o}(h))((1 + \frac{1}{2}\beta_{t}h)\gamma_{0,t} + \bar{o}(h))}{\gamma_{0,t}\beta_{t}h} - 1 = \bar{o}(1),
\end{split}

$$

$$

\begin{split}
&\beta_{t} h \omega_{t,h}^{*} = \mu_{t-h,t} - 1 +  \frac{\nu_{t-h,t}}{\gamma_{0,t}} - \frac{1}{2}\beta_{t}h = 1 - \frac{1}{2}\beta_{t}h - \frac{\gamma_{0,t}^{2}}{1 - \gamma_{0,t}^{2}}\beta_{t}h - 1 - \frac{1}{2}\beta_{t}h + \frac{1}{\gamma_{0,t}}\times \\& \times\left(\frac{\gamma_{0,t}}{1 - \gamma_{0,t}^{2}}\beta_{t}h + \bar{o}(h)\right) + \bar{o}(h) = \beta_{t}h\left(-1 - \frac{\gamma_{0,t}^{2}}{1 - \gamma_{0,t}^{2}} + \frac{1}{1 - \gamma_{0,t}^{2}}\right) + \bar{o}(h) = \bar{o}(h),
\end{split}

$$

$$

\begin{split}
&(\sigma^{*}_{t,h})^{2} = \sigma^{2}_{t-h,t} + \nu_{t-h,t}^{2}\mathbb{E}_{X_{t}}\left[\Tr \left(\Var{(X_{0}|X_{t})}\right)\right]/n = \beta_{t}h + \bar{o}(h) \\
&+ \frac{\gamma_{0,t}^{2}}{(1 - \gamma_{0,t}^{2})^{2}}\beta_{t}^{2}h^{2}\mathbb{E}_{X_{t}}\left[\Tr \left(\Var{(X_{0}|X_{t})}\right)\right]/n = \beta_{t}h(1 + \bar{o}(1)).
\end{split}

$$

## 8·Proof of the Theorem

\label{app:th_proof}

The key fact necessary to prove the Theorem [th:main](#th:main) is established in the following

\begin{lemma}
\label{lm:net_optimal}
Let $p_{0|t}(\cdot|x)$ be pdf of conditional distribution $\Law{(X_{0}|X_{t}=x)}$.

Then for any $t\in [0,1]$ and $x\in \mathbb{R}^{n}$

$$

\label{eq:net_optimal}
s_{\theta^{*}}(x,t) = -\frac{1}{1-\gamma_{0,t}^{2}}\left(x - \gamma_{0,t}\mathbb{E}_{p_{0|t}(\cdot|x)}X_{0}\right).

$$

\end{lemma}

\begin{proof}[Proof of the Lemma [lm:net_optimal](#lm:net_optimal)]
As mentioned in [^Song2021Score-Based], an expression alternative to ([eq:objective](#eq:objective)) can be derived for $\theta^{*}$ under mild assumptions on the data density [^Hyv{{\"a}}rinen2005Estimation], [^Vincent2011A]:

$$

\theta^{*}=\argmin_{\theta}{\int_{0}^{1}{\lambda_{t}\mathbb{E}_{X_{0}\sim p_{0}(\cdot)}\mathbb{E}_{X_{t}\sim p_{t|0}(\cdot|X_{0})}\Vert s_{\theta}(X_{t}, t) - \nabla{\log{p_{t|0}(X_{t}|X_{0})}}\Vert_{2}^{2}}}dt,

$$

where $\Law{(X_{0})}$ is data distribution with pdf $p_{0}(\cdot)$ and $\Law{(X_{t}|X_{0}=x_{0})}$ has pdf $p_{t|0}(\cdot | x_{0})$.

By Bayes formula we can rewrite this in terms of pdfs $p_{t}(\cdot)$ and $p_{0|t}(\cdot | x_{t})$ of distributions $\Law{(X_{t})}$ and $\Law{(X_{0}|X_{t}=x_{t})}$ correspondingly:

$$

\theta^{*}=\argmin_{\theta}{\int_{0}^{1}{\lambda_{t}\mathbb{E}_{X_{t}\sim p_{t}(\cdot)}\mathbb{E}_{X_{0}\sim p_{0|t}(\cdot|X_{t})}\Vert s_{\theta}(X_{t}, t) - \nabla{\log{p_{t|0}(X_{t}|X_{0})}}\Vert_{2}^{2}}}dt.

$$

For any $n$-dimensional random variable $\xi$ with finite second moment and deterministic vector $a$ we have

$$

\begin{split}
\mathbb{E}\Vert\xi - a\Vert_{2}^{2} &= \mathbb{E}\Vert\xi - \mathbb{E}\xi + \mathbb{E}\xi - a\Vert_{2}^{2} = \mathbb{E}\Vert\xi - \mathbb{E}\xi\Vert_{2}^{2} + 2\langle\mathbb{E}[\xi - \mathbb{E}\xi],\mathbb{E}\xi - a\rangle \\&+ \mathbb{E}\Vert\mathbb{E}\xi - a\Vert_{2}^{2} = \mathbb{E}\Vert\xi - \mathbb{E}\xi\Vert_{2}^{2} + \Vert\mathbb{E}\xi - a\Vert_{2}^{2}.
\end{split}

$$

In our case $\xi=\nabla{\log{p_{t|0}(X_{t}|X_{0})}}$ and $a=s_{\theta}(X_{t}, t)$, so $\mathbb{E}\Vert\xi - \mathbb{E}\xi\Vert_{2}^{2}$ is independent of $\theta$.

Thus

$$

\theta^{*}=\argmin_{\theta}{\int_{0}^{1}{\lambda_{t}\mathbb{E}_{X_{t}\sim p_{t}(\cdot)}\Vert s_{\theta}(X_{t}, t) - \mathbb{E}_{X_{0}\sim p_{0|t}(\cdot|X_{t})} \left[\nabla{\log{p_{t|0}(X_{t}|X_{0})}}\right]\Vert_{2}^{2}}}dt.

$$

Therefore, the optimal score estimation network $s_{\theta^{*}}$ can be expressed as

$$

s_{\theta^{*}}(x, t) = \mathbb{E}_{p_{0|t}(\cdot|x)} \left[\nabla{\log{p_{t|0}(x|X_{0})}}\right]

$$

for all $t\in [0,1]$ and $x\in \supp{\{p_{t}\}}=\mathbb{R}^{n}$.

As proven in Appendix [app:sde_solution](#app:sde_solution), $\Law{(X_{t}|X_{0})}$ is Gaussian with mean vector $\gamma_{0,t}X_{0}$ and covariance matrix $(1-\gamma_{0,t}^{2})\ide$, so finally we obtain

$$

s_{\theta^{*}}(x,t) =
\mathbb{E}_{p_{0|t}(\cdot|x)}\left[-\frac{1}{1-\gamma_{0,t}^{2}}\left(x - \gamma_{0,t}X_{0}\right)\right]= -\frac{1}{1-\gamma_{0,t}^{2}}\left(x - \gamma_{0,t}\mathbb{E}_{p_{0|t}(\cdot|x)}X_{0}\right).

$$

\end{proof}

Now let us prove the Theorem [th:main](#th:main).

\begin{proof}[Proof of the Theorem [th:main](#th:main)]
The sampling scheme ([eq:sde_solver](#eq:sde_solver)) consists in adding Gaussian noise to a linear combination of $\hat{X}_{t}$ and $s_{\theta^{*}}(\hat{X}_{t}, t)$.

Combining ([eq:sde_solver](#eq:sde_solver)) and the Lemma [lm:net_optimal](#lm:net_optimal) we get

$$

\begin{split}
&\hat{X}_{t-h} = \hat{\sigma}_{t,h}\xi_{t} + \hat{X}_{t} + \beta_{t}h\left(\left(\frac{1}{2}+\hat{\omega}_{t,h}\right)\hat{X}_{t} + (1+\hat{\kappa}_{t,h})s_{\theta^{*}}(\hat{X}_{t},t)\right) = \hat{\sigma}_{t,h}\xi_{t} \\ &+ \left(1 + \beta_{t}h\left(\frac{1}{2} + \hat{\omega}_{t,h}\right)\right)\hat{X}_{t} + \beta_{t}h(1+\hat{\kappa}_{t,h})\left(-\frac{1}{1-\gamma_{0,t}^{2}}\left(\hat{X}_{t} - \gamma_{0,t}\mathbb{E}_{p_{0|t}(\cdot|\hat{X}_{t})}X_{0}\right)\right) \\& =\hat{\sigma}_{t,h}\xi_{t} + \left(1 + \beta_{t}h\left(\frac{1}{2} + \hat{\omega}_{t,h}-\frac{1+\hat{\kappa}_{t,h}}{1-\gamma_{0,t}^{2}}\right)\right)\hat{X}_{t} + \frac{\gamma_{0,t}\beta_{t}h(1+\hat{\kappa}_{t,h})}{1-\gamma_{0,t}^{2}}\mathbb{E}_{p_{0|t}(\cdot|\hat{X}_{t})}X_{0},
\end{split}

$$

where $\xi_{t}$ are i.i.d. random variables from standard normal distribution $\mathcal{N}(0,\ide)$ for $t=1,1-h,..,h$.

Thus, the distribution $\hat{X}_{t-h}|\hat{X}_{t}$ is also Gaussian:

$$

\label{eq:law_net}
\Law{(\hat{X}_{t-h} | \hat{X}_{t})} = \mathcal{N}\left(\hat{\mu}_{t,h}(\hat{\kappa}_{t,h},\hat{\omega}_{t,h})\hat{X}_{t} + \hat{\nu}_{t,h}(\hat{\kappa}_{t,h})\mathbb{E}_{p_{0|t}(\cdot|\hat{X}_{t})}X_{0},\hat{\sigma}_{t,h}^{2}\ide\right),

$$

$$

\hat{\mu}_{t,h}(\hat{\kappa}_{t,h},\hat{\omega}_{t,h}) = 1 + \beta_{t}h\left(\frac{1}{2} + \hat{\omega}_{t,h}-\frac{1+\hat{\kappa}_{t,h}}{1-\gamma_{0,t}^{2}}\right),

$$

$$

\hat{\nu}_{t,h}(\hat{\kappa}_{t,h}) = \frac{\gamma_{0,t}\beta_{t}h(1+\hat{\kappa}_{t,h})}{1-\gamma_{0,t}^{2}},

$$

which leads to the following formula for the transition densities of the reverse diffusion:

$$

\label{eq:p_net}
\hat{p}_{t-h|t}(x_{t-h}|x_{t})=\frac{1}{\sqrt{2\pi}\hat{\sigma}_{t,h}^{n}}\exp\left(-\frac{\Vert x_{t-h} - \hat{\mu}_{t,h}x_{t} - \hat{\nu}_{t,h}\mathbb{E}_{p_{0|t}(\cdot|x_{t})}X_{0}\Vert_{2}^{2}}{2\hat{\sigma}^{2}_{t,h}}\right).

$$

Moreover, comparing $\hat{\mu}_{t,h}$ and $\hat{\nu}_{t,h}$ with $\mu_{t-h,t}$ and $\nu_{t-h,t}$ defined in ([eq:notation_gamma](#eq:notation_gamma)) we deduce that

$$

\hat{\nu}_{t,h} = \nu_{t-h,t} \Leftrightarrow   \frac{\gamma_{0,t}\beta_{t}h(1+\hat{\kappa}_{t,h})}{1-\gamma_{0,t}^{2}} = \nu_{t-h,t} \Leftrightarrow \hat{\kappa}_{t,h}=\kappa^{*}_{t,h}.

$$

If we also want $\hat{\mu}_{t,h}=\mu_{t-h,t}$ to be satisfied, then we should have

$$

1 + \beta_{t}h\left(\frac{1}{2} + \hat{\omega}_{t,h}-\frac{1+\kappa^{*}_{t,h}}{1-\gamma_{0,t}^{2}}\right) = \mu_{t-h,t} \Leftrightarrow \left(\frac{\mu_{t-h,t} - 1}{\beta_{t}h}-\omega^{*}_{t,h}+\hat{\omega}_{t,h}\right)\beta_{t}h + 1 = \mu_{t-h,t},

$$

i.e. $\hat{\nu}_{t,h}=\nu_{t-h,t}$ and $\hat{\mu}_{t,h}=\mu_{t-h,t}$ iff $\hat{\kappa}_{t,h}=\kappa^{*}_{t,h}$ and $\hat{\omega}_{t,h}=\omega^{*}_{t,h}$ for the parameters $\kappa^{*}_{t,h}$ and $\omega^{*}_{t,h}$ defined in ([eq:notation_correction](#eq:notation_correction)).

As for the corresponding densities of the forward process $X$, they are Gaussian when conditioned on the initial data point $X_{0}$:

$$

\label{eq:law_true}
\Law{(X_{t-h} | X_{t}, X_{0})} = \mathcal{N}(\mu_{t-h,t}X_{t} + \nu_{t-h,t}X_{0},\sigma_{t-h,t}^{2}\ide),

$$

where coefficients $\mu_{t-h,t}$, $\nu_{t-h,t}$ and $\sigma_{t-h,t}$ are defined in ([eq:notation_gamma](#eq:notation_gamma)).

This formula for $\Law{(X_{t-h} | X_{t}, X_{0})}$ follows from the general fact about Gaussian distributions appearing in many recent works on diffusion probabilistic modeling [^Kingma2021Variational]: if $Z_{t}|Z_{s}\sim \mathcal{N}(\alpha_{t|s}Z_{s}, \sigma_{t|s}^{2}\ide)$ and $Z_{t}|Z_{0}\sim \mathcal{N}(\alpha_{t|0}Z_{0}, \sigma_{t|0}^{2}\ide)$ for $0 < s < t$, then

$$

\label{eq:gauss}
\Law{(Z_{s}|Z_{t}, Z_{0})} = \mathcal{N}\left(\frac{\sigma_{s|0}^{2}}{\sigma_{t|0}^{2}}\alpha_{t|s}Z_{t} + \frac{\sigma_{t|s}^{2}}{\sigma_{t|0}^{2}}\alpha_{s|0}Z_{0}, \frac{\sigma_{s|0}^{2}\sigma_{t|s}^{2}}{\sigma_{t|0}^{2}}\ide\right).

$$

This fact is a result of applying Bayes formula to normal distributions.

In our case $\alpha_{t|s}=\gamma_{s,t}$ and $\sigma_{t|s}^{2} = 1 - \gamma_{s,t}^{2}$.

To get an expression for the densities $p_{t-h|t}(x_{t-h}|x_{t})$ similar to ([eq:p_net](#eq:p_net)), we need to integrate out the dependency on data $X_{0}$ from Gaussian distribution $\Law{(X_{t-h}|X_{t}, X_{0})}$:

$$

\label{eq:p_exc}

\begin{split}
p_{t-h|t}(x_{t-h}|x_{t}) = & \int{p_{t-h,0|t}(x_{t-h},x_{0}|x_{t})dx_{0}}=\int{p_{t-h|t,0}(x_{t-h}|x_{t},x_{0})p_{0|t}(x_{0}|x_{t})dx_{0}} \\&= \mathbb{E}_{X_{0}\sim p_{0|t}(\cdot|x_{t})}[p_{t-h|t,0}(x_{t-h}|x_{t},X_{0})],
\end{split}

$$

which implies the following formula:

$$

\label{eq:p_true}
p_{t-h|t}(x_{t-h}|x_{t})=\frac{1}{\sqrt{2\pi}\sigma_{t-h,t}^{n}}\mathbb{E}_{p_{0|t}(\cdot|x_{t})}\left[\exp\left(-\frac{\Vert x_{t-h} - \mu_{t-h,t}x_{t} - \nu_{t-h,t}X_{0}\Vert_{2}^{2}}{2\sigma^{2}_{t-h,t}}\right)\right].

$$

Note that in contrast with the transition densities ([eq:p_net](#eq:p_net)) of the reverse process $\hat{X}$, the corresponding densities ([eq:p_true](#eq:p_true)) of the forward process $X$ are not normal in general.

Our goal is to find parameters $\hat{\kappa}$, $\hat{\omega}$ and $\hat{\sigma}$ that maximize log-likelihood of sample paths $X$ under probability measure with transition densities $\hat{p}$.

Put $t_{k}=kh$ for $k=0,1,..,N$ and write down this log-likelihood:

$$

\begin{split}
&\int{p(x_{1},x_{1-h},..,x_{0})\left(\sum_{k=0}^{N-1}{\log{\hat{p}_{t_{k}|t_{k+1}}}(x_{t_{k}}|x_{t_{k+1}})} + \log{\hat{p}_{1}(x_{1})}\right)dx_{1}dx_{1-h}..dx_{0}} \\
&=\sum_{k=0}^{N-1}\int{p(x_{t_{k}},x_{t_{k+1}})\log{\hat{p}_{t_{k}|t_{k+1}}(x_{t_{k}}|x_{t_{k+1}})}dx_{t_{k+1}}dx_{t_{k}}} + \int{p(x_{1})\log{\hat{p}_{1}}(x_{1})dx_{1}}.
\end{split}

$$

The last term does not depend on $\hat{\kappa}$, $\hat{\omega}$ and $\hat{\sigma}$, so we can ignore it.

Let $R_{k}$ be the $k$-th term in the sum above.

Since we are free to have different coefficients $\hat{\kappa}_{t,h}$, $\hat{\omega}_{t,h}$ and $\hat{\sigma}_{t,h}$ for different steps, we can maximize each $R_{k}$ separately.

Terms $R_{k}$ can be expressed as

$$

\begin{split}
R_{k} &= \int{p(x_{t_{k}},x_{t_{k+1}})\log{\hat{p}_{t_{k}|t_{k+1}}(x_{t_{k}}|x_{t_{k+1}})}dx_{t_{k+1}}dx_{t_{k}}} \\
&=\int{p(x_{t_{k+1}})p_{t_{k}|t_{k+1}}(x_{t_{k}}|x_{t_{k+1}})\log{\hat{p}_{t_{k}|t_{k+1}}(x_{t_{k}}|x_{t_{k+1}})}dx_{t_{k+1}}dx_{t_{k}}} \\
&=\mathbb{E}_{X_{t_{k+1}}}\left[\int{p_{t_{k}|t_{k+1}}(x_{t_{k}}|X_{t_{k+1}})\log{\hat{p}_{t_{k}|t_{k+1}}(x_{t_{k}}|X_{t_{k+1}})}dx_{t_{k}}}\right].
\end{split}

$$

From now on we will skip subscripts of $\mu$, $\nu$, $\sigma$, $\hat{\mu}$, $\hat{\nu}$, $\hat{\sigma}$, $\hat{\kappa}$, $\hat{\omega}$, $\kappa^{*}$ and $\omega^{*}$ for brevity.

Denote

$$

\label{eq:q}
Q(x_{t_{k}},X_{t_{k+1}},X_{0})=\frac{1}{\sqrt{2\pi}\sigma^{n}}\exp\left(-\frac{\Vert x_{t_{k}} - \mu X_{t_{k+1}} - \nu X_{0}\Vert_{2}^{2}}{2\sigma^{2}}\right)\log{\hat{p}_{t_{k}|t_{k+1}}(x_{t_{k}}|X_{t_{k+1}})}.

$$

Using the formula ([eq:p_exc](#eq:p_exc)) for the densities of $X$ together with the explicit expression for the Gaussian density $p_{t_{k}|t_{k+1},0}(x_{t_{k}}|X_{t_{k+1}},X_{0})$ and applying Fubini's theorem to change the order of integration, we rewrite $R_{k}$ as

$$

\begin{split}
R_{k}&=\mathbb{E}_{X_{t_{k+1}}}\left[\int{p_{t_{k}|t_{k+1}}(x_{t_{k}}|X_{t_{k+1}})}\log{\hat{p}_{t_{k}|t_{k+1}}(x_{t_{k}}|X_{t_{k+1}})}dx_{t_{k}}\right]\\
&=\mathbb{E}_{X_{t_{k+1}}}\left[\int{\mathbb{E}_{X_{0}\sim p_{0|t_{k+1}}(\cdot|X_{t_{k+1}})}\left[p_{t_{k}|t_{k+1},0}(x_{t_{k}}|X_{t_{k+1}},X_{0})\log{\hat{p}_{t_{k}|t_{k+1}}(x_{t_{k}}|X_{t_{k+1}})}\right]dx_{t_{k}}}\right] \\
&=\mathbb{E}_{X_{t_{k+1}}}\left[\int{\mathbb{E}_{X_{0}\sim p_{0|t_{k+1}}(\cdot|X_{t_{k+1}})}[Q(x_{t_{k}}, X_{t_{k+1}}, X_{0})]dx_{t_{k}}}\right]\\
&=\mathbb{E}_{X_{t_{k+1}}}\mathbb{E}_{X_{0}\sim p_{0|t_{k+1}}(\cdot|X_{t_{k+1}})}\left[\int{Q(x_{t_{k}}, X_{t_{k+1}}, X_{0})dx_{t_{k}}}\right].
\end{split}

$$

The formula ([eq:q](#eq:q)) implies that the integral of $Q(x_{t_{k}}, X_{t_{k+1}},X_{0})$ with respect to $x_{t_{k}}$ can be seen as expectation of $\log{\hat{p}_{t_{k}|t_{k+1}}(\xi|X_{t_{k+1}})}$ with respect to normal random variable $\xi$ with mean $\mu X_{t_{k+1}} + \nu X_{0}$ and covariance matrix $\sigma^{2}\ide$.

Plugging in the expression ([eq:p_net](#eq:p_net)) into ([eq:q](#eq:q)), we can calculate this integral:

$$

\begin{split}
&\mathbb{E}_{\xi}\left[-\log{\sqrt{2\pi}} -n\log{\hat{\sigma}} - \frac{\Vert\xi - \hat{\mu}X_{t_{k+1}}-\hat{\nu}\mathbb{E}_{X_{0}'\sim p_{0|t_{k+1}}(\cdot|X_{t_{k+1}})}X_{0}'\Vert_{2}^{2}}{2\hat{\sigma}^{2}}\right] \\
&=-\log{\sqrt{2\pi}} - n\log{\hat{\sigma}} - \frac{\mathbb{E}_{\xi}\Vert \xi - \hat{\mu}X_{t_{k+1}}-\hat{\nu}\mathbb{E}_{p_{0|t_{k+1}}(\cdot|X_{t_{k+1}})}X_{0}'\Vert_{2}^{2}}{2\hat{\sigma}^{2}}.
\end{split}

$$

Thus, terms $R_{k}$ we want to maximize equal

$$

R_{k} = -\log{\sqrt{2\pi}} - n\log{\hat{\sigma}} - \mathbb{E}_{X_{t_{k+1}}}\mathbb{E}_{X_{0}\sim p_{0|t_{k+1}}(\cdot|X_{t_{k+1}})}\frac{\mathbb{E}_{\xi}\Vert \xi - \hat{\mu}X_{t_{k+1}}-\hat{\nu}\mathbb{E}_{p_{0|t_{k+1}}(\cdot|X_{t_{k+1}})}X_{0}'\Vert_{2}^{2}}{2\hat{\sigma}^{2}}

$$

Maximizing $R_{k}$ with respect to ($\hat{\kappa}, \hat{\omega}, \hat{\sigma}$) is equivalent to minimizing $\mathbb{E}_{X_{t_{k+1}}}S_{k}$ where $S_{k}$ is given by

$$

\label{eq:s_term}
S_{k} = n\log{\hat{\sigma}} + \frac{1}{2\hat{\sigma}^{2}}\mathbb{E}_{X_{0}\sim p_{0|t_{k+1}}(\cdot|X_{t_{k+1}})}\mathbb{E}_{\xi}\Vert \xi - \hat{\mu}X_{t_{k+1}}-\hat{\nu}\mathbb{E}_{p_{0|t_{k+1}}(\cdot|X_{t_{k+1}})}X_{0}'\Vert_{2}^{2},

$$

where the expectation with respect to $\xi\sim \mathcal{N}(\mu X_{t_{k+1}} + \nu X_{0}, \sigma^{2}\ide)$ can be calculated using the fact that for every vector $\hat{a}$ we can express $\mathbb{E}_{\xi}\Vert \xi - \hat{a} \Vert_{2}^{2}$ as

$$

\mathbb{E}\Vert\xi - \mathbb{E}\xi + \mathbb{E}\xi - \hat{a}\Vert_{2}^{2} = \mathbb{E}\Vert\xi - \mathbb{E}\xi \Vert_{2}^{2} + 2\langle\mathbb{E}[\xi - \mathbb{E}\xi], \mathbb{E}\xi - \hat{a}\rangle + \mathbb{E}\Vert \mathbb{E}\xi - \hat{a}\Vert_{2}^{2} = n\sigma^{2} + \Vert \mathbb{E}\xi - \hat{a}\Vert_{2}^{2}.

$$

So, the outer expectation with respect to $\Law(X_{0}|X_{t_{k+1}})$ in ([eq:s_term](#eq:s_term)) can be simplified:

$$

\begin{split}
&\mathbb{E}_{X_{0}\sim p_{0|t_{k+1}}(\cdot|X_{t_{k+1}})}\left[n\sigma^{2} + \Vert(\mu - \hat{\mu})X_{t_{k+1}} + \nu X_{0} - \hat{\nu}\mathbb{E}_{X_{0}'\sim p_{0|t_{k+1}}(\cdot|X_{t_{k+1}})}X_{0}'\Vert^{2}_{2}\right] \\
&=n\sigma^{2} + \mathbb{E}_{X_{0}}\Vert((\mu - \hat{\mu})X_{t_{k+1}} + \nu X_{0}-\hat{\nu}\mathbb{E}_{X_{0}'}X_{0}'\Vert_{2}^{2}=n\sigma^{2} + (\mu - \hat{\mu})^{2}\Vert X_{t_{k+1}}\Vert_{2}^{2} \\
&+2\langle(\mu - \hat{\mu})X_{t_{k+1}},(\nu - \hat{\nu})\mathbb{E}_{X_{0}}X_{0}\rangle + \mathbb{E}_{X_{0}}{\Vert\nu X_{0}} - \hat{\nu}\mathbb{E}_{X_{0}'}X_{0}'\Vert_{2}^{2} = (\mu - \hat{\mu})^{2}\Vert X_{t_{k+1}}\Vert_{2}^{2} \\
& + 2\langle(\mu - \hat{\mu})X_{t_{k+1}},(\nu - \hat{\nu})\mathbb{E}_{X_{0}}X_{0}\rangle + \nu^{2}\mathbb{E}_{X_{0}}\Vert X_{0}\Vert_{2}^{2} + \hat{\nu}^{2}\Vert\mathbb{E}_{X_{0}}X_{0}\Vert_{2}^{2} + n\sigma^{2} \\
& -2\nu\hat{\nu}\langle\mathbb{E}_{X_{0}}X_{0},\mathbb{E}_{X'_{0}}X'_{0}\rangle = \Vert(\mu - \hat{\mu})X_{t_{k+1}} + (\nu - \hat{\nu})\mathbb{E}_{X_{0}}X_{0})\Vert_{2}^{2} + \nu^{2}\mathbb{E}_{X_{0}}\Vert X_{0}\Vert_{2}^{2} \\
& -\nu^{2}\Vert\mathbb{E}_{X_{0}}X_{0}\Vert_{2}^{2} + n\sigma^{2},
\end{split}

$$

where all the expectations in the formula above are taken with respect to the conditional data distribution $\Law(X_{0}|X_{t_{k+1}})$.

So, the resulting expression for the terms $S_{k}$ whose expectation with respect to $\Law{(X_{t_{k+1}})}$ we want to minimize is

$$

\label{eq:neg-log-lok-term}

\begin{split}
S_{k} = n\log&{\hat{\sigma}} + \frac{1}{2\hat{\sigma}^{2}}\Bigl(n\sigma^{2}+\Vert(\mu - \hat{\mu})X_{t_{k+1}} + (\nu - \hat{\nu})\mathbb{E}{[X_{0}|X_{t_{k+1}}]}\Vert_{2}^{2} \\& + \nu^{2}\left(\mathbb{E}\left[\Vert X_{0}\Vert_{2}^{2}|X_{t_{k+1}}\right]-\Vert\mathbb{E}[X_{0}|X_{t_{k+1}}]\Vert_{2}^{2}\right)\Bigr).
\end{split}

$$

Now it is clear that $\kappa^{*}_{t_{k+1},h}$ and $\omega^{*}_{t_{k+1},h}$ are optimal because $\hat{\mu}_{t_{k+1},h}(\kappa^{*}_{t_{k+1},h}, \omega^{*}_{t_{k+1},h}) = \mu_{t_{k},t_{k+1}}$ and $\hat{\nu}_{t_{k+1},h}(\kappa^{*}_{t_{k+1},h}) = \nu_{t_{k},t_{k+1}}$.

For this choice of parameters we have

$$

\mathbb{E}_{X_{t_{k+1}}}S_{k} = n\log{\hat{\sigma}} + \frac{1}{2\hat{\sigma}^{2}}\left(n\sigma^{2}+\nu^{2}\mathbb{E}_{X_{t_{k+1}}}\left[\mathbb{E}\left[\Vert X_{0}\Vert_{2}^{2}|X_{t_{k+1}}\right]-\Vert\mathbb{E}[X_{0}|X_{t_{k+1}}]\Vert_{2}^{2}\right]\right).

$$

Note that $\mathbb{E}\left[\Vert X_{0}\Vert_{2}^{2}|X_{t_{k+1}}\right]-\Vert\mathbb{E}[X_{0}|X_{t_{k+1}}]\Vert_{2}^{2} = \Tr{(\Var{(X_{0}|X_{t_{k+1}})})}$ is the overall variance of $\Law{(X_{0}|X_{t_{k+1}})}$ along all $n$ dimensions.

Differentiating $\mathbb{E}_{X_{t_{k+1}}}{S_{k}}$ with respect to $\hat{\sigma}$ shows that the optimal $\sigma^{*}_{t_{k+1},h}$ should satisfy

$$

\frac{n}{\sigma^{*}_{t_{k+1},h}} - \frac{1}{(\sigma^{*}_{t_{k+1},h})^{3}}\left(n\sigma_{t_{k},t_{k+1}}^{2}+\nu_{t_{k},t_{k+1}}^{2}\mathbb{E}_{X_{t_{k+1}}}\left[\Tr{(\Var{(X_{0}|X_{t_{k+1}})})}\right]\right)=0,

$$

which is indeed satisfied by the parameters $\sigma^{*}_{t,h}$ defined in ([eq:notation_correction](#eq:notation_correction)).

Thus, the statement (i) is proven.

When it comes to proving that $\hat{X}$ is exact, we have to show that $\Law{(\hat{X}_{t_{k}})}=\Law{(X_{t_{k}})}$ for every $k=0,1,..,N$.

By the assumption that $\Law{(\hat{X}_{1})} = \Law{(X_{1})}$ it is sufficient to prove that $\hat{p}_{t_{k}|t_{k+1}}(x_{t_{k}}|x_{t_{k+1}}) \equiv p_{t_{k}|t_{k+1}}(x_{t_{k}}|x_{t_{k+1}})$ since the exactness will follow from this fact by mathematical induction.

If $X_{0}$ is a constant random variable, $\Law(X_{0}|X_{t})=\Law{(X_{0})}$ also corresponds to the same constant, so $\Var{(X_{0}|X_{t})}=0$ meaning that $\sigma^{*}_{t,h}=\sigma_{t-h,t}$, and the formulae ([eq:p_net](#eq:p_net)) and ([eq:p_true](#eq:p_true)) imply the desired result.

Let us now consider the second case when $X_{0}\sim \mathcal{N}(\bar{\mu},\delta^{2}\ide)$.

It is a matter of simple but lengthy computations to prove another property of Gaussian distributions similar to ([eq:gauss](#eq:gauss)): if $Z_{0}\sim \mathcal{N}(\bar{\mu},\delta^{2}\ide)$ and $Z_{t}|Z_{0}\sim \mathcal{N}(a_{t}Z_{0},b^{2}_{t}\ide)$, then $Z_{0}|Z_{t}\sim \mathcal{N}(\frac{b_{t}^{2}}{b^{2}_{t}+\delta^{2}a^{2}_{t}}\bar{\mu} + \frac{\delta^{2}a_{t}}{b^{2}_{t}+\delta^{2}a^{2}_{t}}Z_{t},\frac{\delta^{2}b^{2}_{t}}{b_{t}^{2}+\delta^{2}a^{2}_{t}}\ide)$ and $Z_{t}\sim \mathcal{N}(\bar{\mu}a_{t},{(b^{2}_{t}+\delta^{2}a^{2}_{t}})\ide)$.

In our case $a_{t}=\gamma_{0,t}$ and $b^{2}_{t}=1-\gamma_{0,t}^{2}$, therefore

$$

\label{eq:gaussian_revert}
\Law{(X_{0}|X_{t})}=\mathcal{N}\left(\frac{1-\gamma^{2}_{0,t}}{1-\gamma_{0,t}^{2}+\delta^{2}\gamma^{2}_{0,t}}\bar{\mu}+\frac{\delta^{2}\gamma_{0,t}}{1-\gamma_{0,t}^{2}+\delta^{2}\gamma^{2}_{0,t}}X_{t},\frac{\delta^{2}(1-\gamma_{0,t}^{2})}{1-\gamma_{0,t}^{2}+\delta^{2}\gamma^{2}_{0,t}}\ide\right).

$$

So, $\Var{(X_{0}|X_{t})}$ does not depend on $X_{t}$ and

$$

(\sigma^{*}_{t,h})^{2} = \sigma_{t-h,t}^{2} + \frac{\nu_{t-h,t}^{2}}{n}\mathbb{E}_{X_{t}}\left[\Tr{(\Var{(X_{0}|X_{t})})}\right] = \sigma_{t-h,t}^{2} + \nu_{t-h,t}^{2}\frac{\delta^{2}(1-\gamma_{0,t}^{2})}{1-\gamma_{0,t}^{2}+\delta^{2}\gamma^{2}_{0,t}}.

$$

Since $\Law{(X_{t}|X_{t-h})}$, $\Law{(X_{t-h})}$ and $\Law{(X_{t})}$ are Gaussian, Bayes formula implies that $\Law{(X_{t-h}|X_{t})}$ is Gaussian as well with the following mean and covariance matrix:

$$

\mathbb{E}[X_{t-h}|X_{t}] = \frac{\gamma_{0,t-h}(1-\gamma_{t-h,t}^{2})}{1-\gamma^{2}_{0,t}+\delta^{2}\gamma_{0,t}^{2}}\bar{\mu} + \frac{\gamma_{t-h,t}(1-\gamma^{2}_{0,t-h}+\delta^{2}\gamma_{0,t-h}^{2})}{1-\gamma^{2}_{0,t}+\delta^{2}\gamma_{0,t}^{2}}X_{t},

$$

$$

\Var{(X_{t-h}|X_{t})}=\frac{(1-\gamma_{t-h,t}^{2})(1-\gamma_{0,t-h}^{2}+\delta^{2}\gamma_{0,t-h}^{2})}{1-\gamma_{0,t}^{2}+\delta^{2}\gamma_{0,t}^{2}}\ide.

$$

The distribution $\Law{(\hat{X}_{t-h}|\hat{X}_{t})}$ is also Gaussian by the formula ([eq:p_net](#eq:p_net)), so to conclude the proof we just need to show that $\mathbb{E}[\hat{X}_{t-h}|\hat{X}_{t}=x]=\mathbb{E}[X_{t-h}|X_{t}=x]$ and $\Var{(\hat{X}_{t-h}|\hat{X}_{t}=x)}=\Var{(X_{t-h}|X_{t}=x)}$ for every $x\in \mathbb{R}^{n}$ for the optimal parameters ([eq:notation_correction](#eq:notation_correction)).

Recall that for $\kappa^{*}_{t,h}$ and $\omega^{*}_{t,h}$ we have $\hat{\mu}_{t,h}(\kappa^{*}_{t,h}, \omega^{*}_{t,h}) = \mu_{t-h,t}$ and $\hat{\nu}_{t,h}(\kappa^{*}_{t,h}) = \nu_{t-h,t}$.

Utilizing the formulae ([eq:notation_gamma](#eq:notation_gamma)), ([eq:p_net](#eq:p_net)), ([eq:gaussian_revert](#eq:gaussian_revert)) and the fact that $\gamma_{0,t-h}\cdot\gamma_{t-h,t}=\gamma_{0,t}$ (following from the definition of $\gamma$ in ([eq:xt_distribution](#eq:xt_distribution))) we conclude that

$$

\begin{split}
&\mathbb{E}[\hat{X}_{t-h}|\hat{X}_{t}=x]=\hat{\mu}_{t,h}(\kappa^{*}_{t,h}, \omega^{*}_{t,h})x + \hat{\nu}_{t,h}(\kappa^{*}_{t,h})\mathbb{E}_{p_{0|t}(\cdot|x)}X_{0} \\
& = \gamma_{t-h,t}\frac{1-\gamma_{0,t-h}^{2}}{1-\gamma_{0,t}^{2}}x + \gamma_{0,t-h}\frac{1-\gamma_{t-h,t}^{2}}{1-\gamma_{0,t}^{2}}\left[\frac{1-\gamma_{0,t}^{2}}{1-\gamma_{0,t}^{2}+\delta^{2}\gamma_{0,t}^{2}}\bar{\mu} + \frac{\delta^{2}\gamma_{0,t}}{1-\gamma_{0,t}^{2}+\delta^{2}\gamma_{0,t}^{2}}x\right] \\
&= \gamma_{0,t-h}\frac{1-\gamma_{t-h,t}^{2}}{1-\gamma_{0,t}^{2} + \delta^{2}\gamma_{0,t}^{2}}\bar{\mu} + \gamma_{t-h,t}\frac{(1-\gamma_{0,t-h}^{2})(1-\gamma_{0,t}^{2})+\delta^{2}\gamma_{0,t}^{2}(1-\gamma^{2}_{0,t-h})}{(1-\gamma_{0,t}^{2})(1-\gamma_{0,t}^{2}+\delta^{2}\gamma_{0,t}^{2})}x \\
&+\gamma_{t-h,t}\frac{\delta^{2}\gamma_{0,t-h}^{2}(1-\gamma^{2}_{t-h,t})}{(1-\gamma_{0,t}^{2})(1-\gamma_{0,t}^{2}+\delta^{2}\gamma_{0,t}^{2})}x = \gamma_{0,t-h}\frac{1-\gamma_{t-h,t}^{2}}{1-\gamma_{0,t}^{2} + \delta^{2}\gamma_{0,t}^{2}}\bar{\mu} \\
&+\gamma_{t-h,t}\frac{(1-\gamma_{0,t-h}^{2})(1-\gamma_{0,t}^{2})+\delta^{2}\gamma_{0,t-h}^{2}(1-\gamma_{0,t}^{2})}{(1-\gamma_{0,t}^{2})(1-\gamma_{0,t}^{2}+\delta^{2}\gamma_{0,t}^{2})}x = \gamma_{0,t-h}\frac{1-\gamma_{t-h,t}^{2}}{1-\gamma_{0,t}^{2} + \delta^{2}\gamma_{0,t}^{2}}\bar{\mu} \\
&+ \gamma_{t-h,t}\frac{1-\gamma_{0,t-h}^{2}+\delta^{2}\gamma_{0,t-h}^{2}}{1-\gamma_{0,t}^{2}+\delta^{2}\gamma_{0,t}^{2}}x = \mathbb{E}[X_{t-h}|X_{t}=x],
\end{split}

$$

$$

\begin{split}
&\Var{(\hat{X}_{t-h}|\hat{X}_{t}=x)} = (\sigma_{t,h}^{*})^{2}\ide = \left(\sigma_{t-h,t}^{2} + \nu_{t-h,t}^{2}\frac{\delta^{2}(1-\gamma_{0,t}^{2})}{1-\gamma_{0,t}^{2}+\delta^{2}\gamma^{2}_{0,t}}\right)\ide \\
&=\left(\frac{(1-\gamma_{0,t-h}^{2})(1-\gamma_{t-h,t}^{2})}{1-\gamma_{0,t}^{2}}+\gamma_{0,t-h}^{2}\frac{\delta^{2}(1-\gamma_{t-h,t}^{2})^{2}}{(1-\gamma_{0,t}^{2})(1-\gamma_{0,t}^{2}+\delta^{2}\gamma^{2}_{0,t})}\right)\ide \\
&=\frac{1-\gamma_{t-h,t}^{2}}{(1-\gamma_{0,t}^{2})(1-\gamma_{0,t}^{2}+\delta^{2}\gamma^{2}_{0,t})}\left((1-\gamma_{0,t-h}^{2})(1-\gamma_{0,t}^{2})+\delta^{2}\gamma_{0,t}^{2}(1-\gamma_{0,t-h}^{2})\right)\ide \\
& + \frac{1-\gamma_{t-h,t}^{2}}{(1-\gamma_{0,t}^{2})(1-\gamma_{0,t}^{2}+\delta^{2}\gamma^{2}_{0,t})}\left(\delta^{2}\gamma_{0,t-h}^{2}(1-\gamma_{t-h,t}^{2})\right)\ide \\
&=\frac{1-\gamma_{t-h,t}^{2}}{(1-\gamma_{0,t}^{2})(1-\gamma_{0,t}^{2}+\delta^{2}\gamma^{2}_{0,t})}\left((1-\gamma_{0,t-h}^{2})(1-\gamma_{0,t}^{2})+\delta^{2}\gamma_{0,t-h}^{2}(1-\gamma_{0,t}^{2})\right)\ide \\
&=\frac{(1-\gamma_{t-h,t}^{2})(1-\gamma_{0,t-h}^{2}+\delta^{2}\gamma_{0,t-h}^{2})}{1-\gamma_{0,t}^{2}+\delta^{2}\gamma_{0,t}^{2}}\ide = \Var{(X_{t-h}|X_{t}=x)}.
\end{split}

$$

\end{proof}
