# MaskVCT: Masked Voice Codec Transformer for Zero-Shot Voice Conversion With Increased Controllability via Multiple Guidances

<details>
<summary>基本信息</summary>

- 标题: "MaskVCT: Masked Voice Codec Transformer for Zero-Shot Voice Conversion With Increased Controllability via Multiple Guidances."
- 作者:
  - 01 Junhyeok Lee
  - 02 Helin Wang
  - 03 Yaohan Guan
  - 04 Thomas Thebaud
  - 05 Laureano Moro-Velazquez
  - 06 Jesús Villalba
  - 07 Najim Dehak
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.17143v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2509.17143v1](PDF/2025.09.21_2509.17143v1_MaskVCT__Masked_Voice_Codec_Transformer_for_Zero-Shot_Voice_Conversion_With_Increased_Controllability_via_Multiple_Guidances.pdf)
  - [Publication] #TODO

</details>

## Abstract

We introduce MaskVCT, a zero‑shot voice conversion (VC) model that offers multi‑factor controllability through multiple classifier‑free guidances (CFGs).
While previous VC models rely on a fixed conditioning scheme, MaskVCT integrates diverse conditions in a single model.
To further enhance robustness and control, the model can leverage continuous or quantized linguistic features to enhance intelligibility and speaker similarity, and can use or omit pitch contour to control prosody.
These choices allow users to seamlessly balance speaker identity, linguistic content, and prosodic factors in a zero‑shot VC setting.
Extensive experiments demonstrate that MaskVCT achieves the best target speaker and accent similarities while obtaining competitive word and character error rates compared to existing baselines.
Audio samples are available at \url{https://maskvct.github.io/}.

## 1·Introduction

\vspace{-4pt}

Voice conversion (VC) aims to transform a source utterance to match a targetâ€™s identity while preserving the source's linguistic content, but a key challenge is disentangling such content from pitch and timbre.

Recent VC models \cite{resynthesis,nansy,nansypp,freevc, diffhiervc,maskgct} used pre-trained self-supervised models' representations as linguistic features, however, these features contains rich information to reconstruct the mel-spectrogram itself while they mentioned "disentangled" \cite{nansy}.

Some models are conditioned on the target's pitch.

Those *pitch-conditioned* models \cite{nansy,nansypp,diffhiervc} have high controllability with frame-wise pitch, but a robust pitch predictor is needed.

Meanwhile, *pitch-unconditioned* models \cite{freevc, maskgct,genvc} generate speech without the need for an explicit pitch input.

One noticeable point is that pitch-unconditioned models can generate speech from tonal languages, such as Chinese \cite{maskgct}, due to the entangled pitch information in pre-trained linguistic features.

![](figure/overall_cols.pdf)

<a id="fig:overall">Overall system description of MaskVCT. 
%Trainable blocks are colored in yellow, and pre-trained modules are colored in white.
We perform column-wise addition of the embeddings and feed the result into MaskVCT.
We employ 9 codebooks for DAC, but display only 2 here for brevity.

All models operate at 50 Hz frame rate.</a>

Many models mainly focus on converting the vocal timbre \cite{nansypp,maskgct} of the source to that of the target.

Recently, *zero-shot* models have emerged, allowing conversion to previously unseen speaker voices without additional fine-tuning \cite{nansy,freevc,diffhiervc,maskgct}.

Another trend is leveraging codec-based language models (CodecLMs). 
Targeting discretized tokens from neural audio codecs \cite{dac,encodec,naturalspeech3}, CodecLMs cast generation as a Transformer-friendly classification task \cite{transformer} and reconstruct via autoregressive prediction \cite{valle} or masked modeling \cite{maskgct}, typically conditioned on a short 3-second speaker prompt.

In this work, we propose **MaskVCT**, a masked generative model for zero-shot voice conversion that combines temporal-quantized syllabic features with increased controllability. 
By using syllabic representations from SylBoost \cite{syllablelm}, MaskVCT gains flexibility in phoneme selection and accent variation within each segment. 
By leveraging multiple classifier-free guidances (CFGs) \cite{cfg}, we allow for a dynamic balance between conditions at inference time, effectively creating a highly controllable system.

Unlike prior VC systems bound to a single condition, our unified model supports different conditions and inference modes in a single model, with respect to:

[leftmargin=*]

-  Linguistics: discrete vs. continuous representations.

-  Pitch: source-conditioned vs. target-driven contour.

-  CFG weights: balancing content, speaker, and pitch.

From those potential configurations, we propose two modes: MaskVCT-All, which prioritizes pitch following and intelligibility, and MaskVCT-Spk, which focuses on speaker similarity.

Both modes deliver strong performance across all metrics, especially MaskVCT-Spk achieves the highest target speaker and accent similarity.

## 2·MaskVCT

### Linguistic Conditioning

Self-supervised speech representations provide time-aligned and fast-varying phonetic features \cite{ss_phonetic} so VC models built on them tend to reproduce source-aligned speech. 
Table [tab:results_main](#tab:results_main) shows that even pitch-unconditioned, linguistics-only-conditioned models \cite{freevc,maskgct} retain high pitch correlation with the source, indicating pitch leakage.

This motivates a coarser and pitch-stripped representation for source-pitch-agnostic conversion.

Recent works propose syllabic representations \cite{syllablelm,sylber} that discretize speech into slow-varying units by quantizing both within-vector and across adjacent frames.

Also, they relax strict source alignment to enable phoneme and accent variations.

We adopt these temporally coarse-grained tokens to mitigate non-linguistic attribute leakage.

We use the discrete tokens from the pre-trained syllabic model SylBoost and its quantizer \cite{syllablelm}, and train learnable embeddings for each token.

Continuous and discrete linguistic representations offer complementary strengths: continuous features improve intelligibility and phoneme alignment, whereas discrete syllable tokens better preserve target timbre/speaker similarity \cite{sylber} and mitigate source speaker traits leakage; conversely, continuous features can weaken speaker fidelity \cite{vevo}. 
To expose this trade-off, we implement dual conditioning paths: (i) discretized syllabic tokens and (ii) continuous features via a lightweight FFN–LayerNorm–FFN projection.

We train with balanced sampling as 50\% for each, allowing the model to run in either mode, so users can choose intelligibility-forward or target timbre-faithful at inference.

### Pitch Conditioning

We map continuous pitch to a vector using sinusoidal embedding \cite{transformer}, making the system agnostic to pitch-extractor resolution and requiring no fine-tuning when changing extractors. 
Unlike the standard interleaved layout for positional embedding, we concatenate sine and cosine terms to avoid confounding multi-head attention.

As the human perception of frequency is related to the log-scale, we convert pitch to the log-scale, adding an epsilon value of 1 Hz to the frequency, to prevent negative infinity with fundamental frequencies of 0 Hz. 
Therefore, for the frequency $f$, the $i$-th index of the pitch embedding $\mathbf{P}(f)_i$ is defined as follows:

$$
\mathbf{P}(f)_i =
\begin{cases}
\sin\left(\frac{\log(1+f)}{10000^{2\cdot i/d}}\right) & \text{if $i<d/2$} \\
\cos\left(\frac{\log(1+f)}{10000^{2\cdot (i-d/2)/d}}\right)& \text{if $i\geq d/2$}
\end{cases}

$$

where $i$ starts from 0 to $d-1$, and $d$ is the model's dimension. 
We extract all pitch contours with a rate of 50 Hz using Praat \cite{praat}.

### Speaker Conditioning

To perform zero-shot VC, we implemented a speaker prompt mechanism similar to Vall-E \cite{valle}. 
We process 3-second of the reference speech to extract the *prompt* conditions and then concatenate them to the beginning of the source conditions. 
By this, we can leverage in-context learning by showing how the given speaker reads the given conditions to increase the quality and condition satisfaction.

### Masked Codec Language Model

Inspired by masked language modeling \cite{bert},
MaskGIT \cite{maskgit} frames image synthesis as discrete token unmasking.

Extending this idea, similar to CodecLMs \cite{valle,musicgen}, non-autoregressive codec-based models adopt masked modeling to speech \cite{maskgct,soundstorm},
typically on residual vector quantization (RVQ) codecs \cite{dac,encodec}.

Recently, MaskGCT \cite{maskgct} achieved a state-of-the-art zero-shot text-to-speech with a masked model, and its S2A module can be utilized as a VC model. 

The masked model is trained by reconstructing the source's encoded acoustic tokens 
$\mathbf{A}_0 \in \{1,2,\dotsc,K\}^{T\times C}$ from a masked input $\mathbf{A}_{u,c} =\mathbf{A}_0 \odot \mathbf{M}_{u,c}$, where $K$ is the codebook vocabulary size, $T$ the number of frames, $C$ the number of the RVQ codebooks, $c\in\{0,1,\dotsc,C-1\}$ a target RVQ codebook layer, $u\in [0,1]$ a masking timestep, and $\mathbf{M}_{u,c}\in\{0,1\}^{T \times C}$ a binary mask. 
To be specific, for the mask $\mathbf{M}_{u,c}$, all layers below $c$ remain unmasked, all layers above $c$ are fully masked, and only layer $c$ is masked according to a masking schedule.

The reconstruction process can be framed as a classification task where the model $\theta$ learns the conditional probability $p_\theta(\mathbf{A}_{0}|\mathbf{A}_{u,c},\mathbf{C})$, where $\mathbf{C}$ are the conditioning features.

During training, we randomly sample a masking time step $u\sim\mathcal{U}[0,1]$, a target codebook layer $c\sim p(c) = 1 -\frac{2(c+1)}{C(C+1)}$ \cite{maskgct}, and cosine scheduled mask $\mathbf{M}_{u,c}\sim\mathrm{Bernoulli}\left(\cos\left(\pi u/2\right)\right)
$ \cite{maskgit}.

We train our model using a simple classification loss applied exclusively to the masked tokens, defined as follows:

$$
\mathcal{L}_\text{mask} = \displaystyle \mathop{\mathbb{E}}_{t | \mathbf{M}_{u,c}[t,c]=0} \left[-\log p_{\theta} \left(\mathbf{A}_{0}[t,c]|\mathbf{A}_{u,c},\mathbf{C} \right) \right].
$$

During inference, we start from all-masked sequence $\mathbf{A}_{1,0}=\mathbf{A}_N$, and iteratively unmask to $\mathbf{A}_0$ over $N$ iteration, predicting logits for masked positions of codebook $c$ as $z_\theta(\mathbf{A}_n \mid \mathbf{A}_{n+1}, \mathbf{C})$ and unmasking tokens at each step.

### Multiple Classifier-Free Guidances

CFG \cite{cfg} is a commonly used technique to improve conditional generation at inference time and is also used in CodecLMs that require fine time-aligned conditioning for accurate acoustic reconstruction \cite{valle,maskgct}.

There are several studies that applied dual CFG \cite{voiceldm,dualspeech},
notably, DualSpeech \cite{dualspeech} uses dual CFG to jointly steer intelligibility and speaker fidelity via phoneme and speaker-aware phoneme embeddings.

In VC, where outputs must simultaneously satisfy multiple conditioning factors, including speaker identity, linguistic content, and pitch contour, we extend CFG to triple CFGs.

Within masked generative modeling framework \cite{maskgct,maskgit,soundstorm}, we define speech synthesis as a conditional probability with three factors as speaker prompt $\mathbf{A}_p$, linguistic content $\mathbf{L}$, and pitch $\mathbf{P}$ as $\log p_{\theta}(\mathbf{A}_{n}|\mathbf{A}_{n+1},
\mathbf{C})$, where the conditioning variable $\mathbf{C}$ is set to either $(\mathbf{A}_{p}, \mathbf{L}, \mathbf{P})$ or $(\mathbf{A}_{p}, \mathbf{L}, \varnothing)$ depending on pitch condition.

Define the linguistic-only path as
$\log p_\theta(\mathbf{A}_n\mid \mathbf{A}_{n+1},\mathbf{L}) :=\log p_\theta(\mathbf{A}_n\mid \mathbf{A}_{n+1},\varnothing,\mathbf{L},\varnothing)$.

Following DualSpeech \cite{dualspeech}, the speaker condition couples the prompt with linguistic as
$\log p_\theta(\mathbf{A}_n\mid \mathbf{A}_{n+1},\mathbf{A}_p,\mathbf{L},\varnothing)$.

Since pitch alone is ill-posed, we use it jointly with speaker and text as
$\log p_\theta(\mathbf{A}_n\mid \mathbf{A}_{n+1},\mathbf{A}_p,\mathbf{L},\mathbf{P})$.

Unlike other generative modeling tasks, VC is a strictly conditional generative task that explicitly conditions on and preserves the source linguistic content. 
Therefore, we adopt a modified CFG formulation; instead of subtracting the unconditioned logit $\log p_\theta(\mathbf{A}_{n}|\mathbf{A}_{n+1}):=\log p_\theta(\mathbf{A}_{n}|\mathbf{A}_{n+1}, \varnothing,\varnothing,\varnothing)$ from the conditioned logit, we subtract the linguistic-only conditioned logit from the detailed conditioned logit as follows:

$$\begin{aligned}
\label{eq:cfg}
& \log \Tilde{p}_\theta(\mathbf{A}_{n}|\mathbf{A}_{n+1}, \mathbf{C})    \nonumber\\
&=\log p_\theta(\mathbf{A}_{n}|\mathbf{A}_{n+1},\mathbf{L}) \nonumber\\
&+\omega_{\text{all}}\left(\log p_\theta(\mathbf{A}_{n}|\mathbf{A}_{n+1}, \mathbf{A}_p,\mathbf{L},\mathbf{P})-\log p_\theta(\mathbf{A}_{n}|\mathbf{A}_{n+1},\mathbf{L}) \right) \nonumber \\
&+\omega_{\text{spk}}\left(\log p_\theta(\mathbf{A}_{n}|\mathbf{A}_{n+1}, \mathbf{A}_p,\mathbf{L},\varnothing)-\log p_\theta(\mathbf{A}_{n}|\mathbf{A}_{n+1},\mathbf{L}) \right) \nonumber \\
&+\omega_{\text{ling}}\left(\log p_\theta(\mathbf{A}_{n}|\mathbf{A}_{n+1}, \mathbf{L})-\log p_\theta(\mathbf{A}_{n}|\mathbf{A}_{n+1}) \right),
\end{aligned}$$

where $\omega_\text{all}$, $\omega_\text{spk}$, $\omega_\text{ling}$ are CFG coefficients of each conditioning, respectively.

We trained a model with four different conditions, as shown in Figure [fig:overall](#fig:overall), and all-conditioned, spk-conditioned, ling-conditioned, and null-conditioned are randomly sampled in a 6:2:2:1 ratio.

For each codebook layer of audio tokens, linguistic and pitch conditions have their own mask tokens, which replace the original token to mask it.

### Model Architecture Details

We utilize a simple Transformer encoder-based architecture \cite{transformer}, especially PreLN with rotary positional embedding.

For simplicity, FFNs use a ReLU activation and LayerNorm.

After the 16 Transformer encoder layers, each with 16 attention heads, model dimension of 1024, FFN dimension of 4096, we attach separate classification heads for each codebook index.
