# MaskVCT: Masked Voice Codec Transformer for Zero-Shot Voice Conversion With Increased Controllability via Multiple Guidances

<details>
<summary>基本信息</summary>

- 标题: "MaskVCT: Masked Voice Codec Transformer for Zero-Shot Voice Conversion With Increased Controllability via Multiple Guidances."
- 作者:
  - 01 Junhyeok Lee
  - 02 Helin Wang
  - 03 Yaohan Guan
  - 04 Thomas Thebaud
  - 05 Laureano Moro-Velazquez
  - 06 Jesús Villalba
  - 07 Najim Dehak
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.17143v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2509.17143v1](PDF/2025.09.21_2509.17143v1_MaskVCT__Masked_Voice_Codec_Transformer_for_Zero-Shot_Voice_Conversion_With_Increased_Controllability_via_Multiple_Guidances.pdf)
  - [Publication] #TODO

</details>

## Abstract

We introduce MaskVCT, a zero‑shot voice conversion (VC) model that offers multi‑factor controllability through multiple classifier‑free guidances (CFGs).
While previous VC models rely on a fixed conditioning scheme, MaskVCT integrates diverse conditions in a single model.
To further enhance robustness and control, the model can leverage continuous or quantized linguistic features to enhance intelligibility and speaker similarity, and can use or omit pitch contour to control prosody.
These choices allow users to seamlessly balance speaker identity, linguistic content, and prosodic factors in a zero‑shot VC setting.
Extensive experiments demonstrate that MaskVCT achieves the best target speaker and accent similarities while obtaining competitive word and character error rates compared to existing baselines.
Audio samples are available at \url{https://maskvct.github.io/}.

## 1·Introduction

\vspace{-4pt}

Voice conversion (VC) aims to transform a source utterance to match a targetâ€™s identity while preserving the source's linguistic content, but a key challenge is disentangling such content from pitch and timbre.

Recent VC models \cite{resynthesis,nansy,nansypp,freevc, diffhiervc,maskgct} used pre-trained self-supervised models' representations as linguistic features, however, these features contains rich information to reconstruct the mel-spectrogram itself while they mentioned "disentangled" \cite{nansy}.

Some models are conditioned on the target's pitch.

Those *pitch-conditioned* models \cite{nansy,nansypp,diffhiervc} have high controllability with frame-wise pitch, but a robust pitch predictor is needed.

Meanwhile, *pitch-unconditioned* models \cite{freevc, maskgct,genvc} generate speech without the need for an explicit pitch input.

One noticeable point is that pitch-unconditioned models can generate speech from tonal languages, such as Chinese \cite{maskgct}, due to the entangled pitch information in pre-trained linguistic features.

![](figure/overall_cols.pdf)

<a id="fig:overall">Overall system description of MaskVCT. 
%Trainable blocks are colored in yellow, and pre-trained modules are colored in white.
We perform column-wise addition of the embeddings and feed the result into MaskVCT.
We employ 9 codebooks for DAC, but display only 2 here for brevity.

All models operate at 50 Hz frame rate.</a>

Many models mainly focus on converting the vocal timbre \cite{nansypp,maskgct} of the source to that of the target.

Recently, *zero-shot* models have emerged, allowing conversion to previously unseen speaker voices without additional fine-tuning \cite{nansy,freevc,diffhiervc,maskgct}.

Another trend is leveraging codec-based language models (CodecLMs). 
Targeting discretized tokens from neural audio codecs \cite{dac,encodec,naturalspeech3}, CodecLMs cast generation as a Transformer-friendly classification task \cite{transformer} and reconstruct via autoregressive prediction \cite{valle} or masked modeling \cite{maskgct}, typically conditioned on a short 3-second speaker prompt.

In this work, we propose **MaskVCT**, a masked generative model for zero-shot voice conversion that combines temporal-quantized syllabic features with increased controllability. 
By using syllabic representations from SylBoost \cite{syllablelm}, MaskVCT gains flexibility in phoneme selection and accent variation within each segment. 
By leveraging multiple classifier-free guidances (CFGs) \cite{cfg}, we allow for a dynamic balance between conditions at inference time, effectively creating a highly controllable system.

Unlike prior VC systems bound to a single condition, our unified model supports different conditions and inference modes in a single model, with respect to:

[leftmargin=*]

-  Linguistics: discrete vs. continuous representations.

-  Pitch: source-conditioned vs. target-driven contour.

-  CFG weights: balancing content, speaker, and pitch.

From those potential configurations, we propose two modes: MaskVCT-All, which prioritizes pitch following and intelligibility, and MaskVCT-Spk, which focuses on speaker similarity.

Both modes deliver strong performance across all metrics, especially MaskVCT-Spk achieves the highest target speaker and accent similarity.
