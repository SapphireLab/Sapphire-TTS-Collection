# MaskVCT

<details>
<summary>基本信息</summary>

- 标题: "MaskVCT: Masked Voice Codec Transformer for Zero-Shot Voice Conversion With Increased Controllability via Multiple Guidances."
- 作者:
  - 01 Junhyeok Lee
  - 02 Helin Wang
  - 03 Yaohan Guan
  - 04 Thomas Thebaud
  - 05 Laureano Moro-Velazquez
  - 06 Jesús Villalba
  - 07 Najim Dehak
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.17143v1)
  - [Publication]()
  - [Github]()
  - [Demo](https://maskvct.github.io/)
- 文件:
  - [ArXiv:2509.17143v1](PDF/2025.09.21_2509.17143v1_MaskVCT__Masked_Voice_Codec_Transformer_for_Zero-Shot_Voice_Conversion_With_Increased_Controllability_via_Multiple_Guidances.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<!--
We introduce ***MaskVCT***, a zero‑shot voice conversion (VC) model that offers multi‑factor controllability through multiple classifier‑free guidances (CFGs).
While previous VC models rely on a fixed conditioning scheme, ***MaskVCT*** integrates diverse conditions in a single model.
To further enhance robustness and control, the model can leverage continuous or quantized linguistic features to enhance intelligibility and speaker similarity, and can use or omit pitch contour to control prosody.
These choices allow users to seamlessly balance speaker identity, linguistic content, and prosodic factors in a zero‑shot VC setting.
Extensive experiments demonstrate that ***MaskVCT*** achieves the best target speaker and accent similarities while obtaining competitive word and character error rates compared to existing baselines.
Audio samples are available at [this https URL](https://maskvct.github.io/).
-->
我们提出了 ***MaskVCT***, 一种零样本声音转换模型, 通过多个无分类器引导提供了多因素控制能力.
与之前依赖于固定条件化方案的声音转换模型不同, ***MaskVCT*** 在一个模型中集成了不同条件.
为了进一步增强稳健性和控制, 模型可以利用连续或量化的语言学特征来增强可理解性和说话人相似性, 并且可以使用或省略音高轮廓来控制韵律.
这些选择使得用户无缝地在零样本声音转换设置中平衡说话人身份, 语言内容和韵律因素.
经过广泛的实验, 我们证明了 ***MaskVCT*** 取得了最佳的目标说话人和口音相似度, 并取得了与现有基线相当的词和字符错误率.
音频示例可在[此处](https://maskvct.github.io/)获得.

## 1·Introduction: 引言

Voice conversion (VC) aims to transform a source utterance to match a targetâ€™s identity while preserving the source's linguistic content, but a key challenge is disentangling such content from pitch and timbre.
Recent VC models (**Polyak2021Speech**[^Polyak2021Speech], **NANSY**[^Choi2021NANSY], **NANSY++**[^Choi2022NANSY++], **FreeVC**[^Li2022FreeVC], **Diff-HierVC**[^Choi2023Diff-HierVC], **MaskGCT**[^Wang2024MaskGCT]) used pre-trained self-supervised models' representations as linguistic features, however, these features contains rich information to reconstruct the mel-spectrogram itself while they mentioned "disentangled" (**NANSY**[^Choi2021NANSY]).

Some models are conditioned on the target's pitch.
Those *pitch-conditioned* models (**NANSY**[^Choi2021NANSY], **NANSY++**[^Choi2022NANSY++], **Diff-HierVC**[^Choi2023Diff-HierVC]) have high controllability with frame-wise pitch, but a robust pitch predictor is needed.
Meanwhile, *pitch-unconditioned* models (**FreeVC**[^Li2022FreeVC], **MaskGCT**[^Wang2024MaskGCT], **GenVC**[^Cai2025GenVC]) generate speech without the need for an explicit pitch input.
One noticeable point is that pitch-unconditioned models can generate speech from tonal languages, such as Chinese (**MaskGCT**[^Wang2024MaskGCT]), due to the entangled pitch information in pre-trained linguistic features.

Many models mainly focus on converting the vocal timbre (**NANSY++**[^Choi2022NANSY++], **MaskGCT**[^Wang2024MaskGCT]) of the source to that of the target.

Recently, *zero-shot* models have emerged, allowing conversion to previously unseen speaker voices without additional fine-tuning (**NANSY**[^Choi2021NANSY], **FreeVC**[^Li2022FreeVC], **Diff-HierVC**[^Choi2023Diff-HierVC], **MaskGCT**[^Wang2024MaskGCT]).

Another trend is leveraging codec-based language models (CodecLMs).
Targeting discretized tokens from neural audio codecs (**DAC**[^Kumar2023DAC], **EnCodec**[^Defossez2022EnCodec], **FACodec**[^Ju2024NaturalSpeech3]), CodecLMs cast generation as a **Transformer**[^Vaswani2017Transformer]-friendly classification task and reconstruct via autoregressive prediction (**VALL-E**[^Wang2023VALL-E]) or masked modeling (**MaskGCT**[^Wang2024MaskGCT]), typically conditioned on a short 3-second speaker prompt.

In this work, we propose **MaskVCT**, a masked generative model for zero-shot voice conversion that combines temporal-quantized syllabic features with increased controllability.
By using syllabic representations from **SylBoost**[^Baade2024SyllableLM], ***MaskVCT*** gains flexibility in phoneme selection and accent variation within each segment.
By leveraging multiple **classifier-free guidances (CFGs)**[^Ho2022CFG], we allow for a dynamic balance between conditions at inference time, effectively creating a highly controllable system.

Unlike prior VC systems bound to a single condition, our unified model supports different conditions and inference modes in a single model, with respect to:

-  Linguistics: discrete vs. continuous representations.
-  Pitch: source-conditioned vs. target-driven contour.
-  CFG weights: balancing content, speaker, and pitch.

From those potential configurations, we propose two modes: ***MaskVCT-All***, which prioritizes pitch following and intelligibility, and ***MaskVCT-Spk***, which focuses on speaker similarity.
Both modes deliver strong performance across all metrics, especially ***MaskVCT-Spk*** achieves the highest target speaker and accent similarity.

## 2·Background: 背景

## 3·Methodology: 方法

### Linguistic Conditioning: 语言学条件化

Self-supervised speech representations provide time-aligned and fast-varying phonetic features [^Choi2024Self] so VC models built on them tend to reproduce source-aligned speech.
[Tab.02](#tab:results_main) shows that even pitch-unconditioned, linguistics-only-conditioned models (**FreeVC**[^Li2022FreeVC], **MaskGCT**[^Wang2024MaskGCT]) retain high pitch correlation with the source, indicating pitch leakage.
This motivates a coarser and pitch-stripped representation for source-pitch-agnostic conversion.

Recent works propose syllabic representations (**SyllabelLM**[^Baade2024SyllableLM], **Sylber**[^Cho2024Sylber]) that discretize speech into slow-varying units by quantizing both within-vector and across adjacent frames.
Also, they relax strict source alignment to enable phoneme and accent variations.
We adopt these temporally coarse-grained tokens to mitigate non-linguistic attribute leakage.
We use the discrete tokens from the pre-trained syllabic model **SylBoost**[^Baade2024SyllableLM] and its quantizer, and train learnable embeddings for each token.

Continuous and discrete linguistic representations offer complementary strengths: continuous features improve intelligibility and phoneme alignment, whereas discrete syllable tokens better preserve target timbre/speaker similarity (**Sylber**[^Cho2024Sylber]) and mitigate source speaker traits leakage; conversely, continuous features can weaken speaker fidelity (**Vevo**[^Zhang2025Vevo]).
To expose this trade-off, we implement dual conditioning paths: (i) discretized syllabic tokens and (ii) continuous features via a lightweight FFN–LayerNorm–FFN projection.
We train with balanced sampling as 50\% for each, allowing the model to run in either mode, so users can choose intelligibility-forward or target timbre-faithful at inference.

### Pitch Conditioning: 音高条件化

We map continuous pitch to a vector using sinusoidal embedding (**Transformer**[^Vaswani2017Transformer]), making the system agnostic to pitch-extractor resolution and requiring no fine-tuning when changing extractors.
Unlike the standard interleaved layout for positional embedding, we concatenate sine and cosine terms to avoid confounding multi-head attention.
As the human perception of frequency is related to the log-scale, we convert pitch to the log-scale, adding an epsilon value of 1 Hz to the frequency, to prevent negative infinity with fundamental frequencies of 0 Hz.
Therefore, for the frequency $f$, the $i$-th index of the pitch embedding $\mathbf{P}(f)_i$ is defined as follows:

$$
\mathbf{P}(f)_i =
\begin{cases}
\sin\left(\frac{\log(1+f)}{10000^{2\cdot i/d}}\right) & \text{if $i<d/2$} \\
\cos\left(\frac{\log(1+f)}{10000^{2\cdot (i-d/2)/d}}\right)& \text{if $i\geq d/2$}
\end{cases}
$$

where $i$ starts from 0 to $d-1$, and $d$ is the model's dimension.
We extract all pitch contours with a rate of 50 Hz using [Praat](http://www.praat.org/).

### Speaker Conditioning: 说话人条件化

To perform zero-shot VC, we implemented a speaker prompt mechanism similar to **VALL-E**[^Wang2023VALL-E].
We process 3-second of the reference speech to extract the *prompt* conditions and then concatenate them to the beginning of the source conditions.
By this, we can leverage in-context learning by showing how the given speaker reads the given conditions to increase the quality and condition satisfaction.

### Masked Codec Language Model: 掩膜编解码器模型

Inspired by masked language modeling (**BERT**[^Devlin2018BERT]), **MaskGIT**[^Chang2022MaskGIT] frames image synthesis as discrete token unmasking.
Extending this idea, similar to CodecLMs (**VALL-E**[^Wang2023VALL-E], **MusicGen**[^Copet2023MusicGen]), non-autoregressive codec-based models adopt masked modeling to speech (**MaskGCT**[^Wang2024MaskGCT], **SoundStorm**[^Borsos2023SoundStorm]),
typically on residual vector quantization (RVQ) codecs (**DAC**[^Kumar2023DAC], **EnCodec**[^Defossez2022EnCodec]).
Recently, **MaskGCT**[^Wang2024MaskGCT] achieved a state-of-the-art zero-shot text-to-speech with a masked model, and its S2A module can be utilized as a VC model.

The masked model is trained by reconstructing the source's encoded acoustic tokens $\mathbf{A}_0 \in \{1,2,\dotsc,K\}^{T\times C}$ from a masked input $\mathbf{A}_{u,c} =\mathbf{A}_0 \odot \mathbf{M}_{u,c}$, where $K$ is the codebook vocabulary size, $T$ the number of frames, $C$ the number of the RVQ codebooks, $c\in\{0,1,\dotsc,C-1\}$ a target RVQ codebook layer, $u\in [0,1]$ a masking timestep, and $\mathbf{M}_{u,c}\in\{0,1\}^{T \times C}$ a binary mask.
To be specific, for the mask $\mathbf{M}_{u,c}$, all layers below $c$ remain unmasked, all layers above $c$ are fully masked, and only layer $c$ is masked according to a masking schedule.
The reconstruction process can be framed as a classification task where the model $\theta$ learns the conditional probability $p_\theta(\mathbf{A}_{0}|\mathbf{A}_{u,c},\mathbf{C})$, where $\mathbf{C}$ are the conditioning features.
During training, we randomly sample a masking time step $u\sim\mathcal{U}[0,1]$, a target codebook layer $c\sim p(c) = 1 -\frac{2(c+1)}{C(C+1)}$ (**MaskGCT**[^Wang2024MaskGCT]), and cosine scheduled mask $\mathbf{M}_{u,c}\sim\mathrm{Bernoulli}\left(\cos\left(\pi u/2\right)\right)$ (**MaskGIT**[^Chang2022MaskGIT]).

We train our model using a simple classification loss applied exclusively to the masked tokens, defined as follows:

$$
\mathcal{L}_\text{mask} = \displaystyle \mathop{\mathbb{E}}_{t | \mathbf{M}_{u,c}[t,c]=0} \left[-\log p_{\theta} \left(\mathbf{A}_{0}[t,c]|\mathbf{A}_{u,c},\mathbf{C} \right) \right].
$$

During inference, we start from all-masked sequence $\mathbf{A}_{1,0}=\mathbf{A}_N$, and iteratively unmask to $\mathbf{A}_0$ over $N$ iteration, predicting logits for masked positions of codebook $c$ as $z_\theta(\mathbf{A}_n \mid \mathbf{A}_{n+1}, \mathbf{C})$ and unmasking tokens at each step.

### Multiple Classifier-Free Guidances: 多无分类器引导

**CFG**[^Ho2022CFG] is a commonly used technique to improve conditional generation at inference time and is also used in CodecLMs that require fine time-aligned conditioning for accurate acoustic reconstruction (**VALL-E**[^Wang2023VALL-E], **MaskGCT**[^Wang2024MaskGCT]).
There are several studies that applied dual CFG (**VoiceLDM**[^Lee2023VoiceLDM], **DuaSpeech**[^Yang2024DualSpeech]), notably, **DualSpeech**[^Yang2024DualSpeech] uses dual CFG to jointly steer intelligibility and speaker fidelity via phoneme and speaker-aware phoneme embeddings.

In VC, where outputs must simultaneously satisfy multiple conditioning factors, including speaker identity, linguistic content, and pitch contour, we extend CFG to triple CFGs.
Within masked generative modeling framework (**MaskGCT**[^Wang2024MaskGCT], **MaskGIT**[^Chang2022MaskGIT], **SoundStorm**[^Borsos2023SoundStorm]), we define speech synthesis as a conditional probability with three factors as speaker prompt $\mathbf{A}_p$, linguistic content $\mathbf{L}$, and pitch $\mathbf{P}$ as $\log p_{\theta}(\mathbf{A}_{n}|\mathbf{A}_{n+1},
\mathbf{C})$, where the conditioning variable $\mathbf{C}$ is set to either $(\mathbf{A}_{p}, \mathbf{L}, \mathbf{P})$ or $(\mathbf{A}_{p}, \mathbf{L}, \varnothing)$ depending on pitch condition.
Define the linguistic-only path as $\log p_\theta(\mathbf{A}_n\mid \mathbf{A}_{n+1},\mathbf{L}) :=\log p_\theta(\mathbf{A}_n\mid \mathbf{A}_{n+1},\varnothing,\mathbf{L},\varnothing)$.
Following **DualSpeech**[^Yang2024DualSpeech], the speaker condition couples the prompt with linguistic as $\log p_\theta(\mathbf{A}_n\mid \mathbf{A}_{n+1},\mathbf{A}_p,\mathbf{L},\varnothing)$.
Since pitch alone is ill-posed, we use it jointly with speaker and text as $\log p_\theta(\mathbf{A}_n\mid \mathbf{A}_{n+1},\mathbf{A}_p,\mathbf{L},\mathbf{P})$.

Unlike other generative modeling tasks, VC is a strictly conditional generative task that explicitly conditions on and preserves the source linguistic content.
Therefore, we adopt a modified CFG formulation; instead of subtracting the unconditioned logit $\log p_\theta(\mathbf{A}_{n}|\mathbf{A}_{n+1}):=\log p_\theta(\mathbf{A}_{n}|\mathbf{A}_{n+1}, \varnothing,\varnothing,\varnothing)$ from the conditioned logit, we subtract the linguistic-only conditioned logit from the detailed conditioned logit as follows:

$$
\begin{aligned}
& \log \tilde{p}_\theta(\mathbf{A}_{n}|\mathbf{A}_{n+1}, \mathbf{C})    \nonumber\\
&=\log p_\theta(\mathbf{A}_{n}|\mathbf{A}_{n+1},\mathbf{L}) \nonumber\\
&+\omega_{\text{all}}\left(\log p_\theta(\mathbf{A}_{n}|\mathbf{A}_{n+1}, \mathbf{A}_p,\mathbf{L},\mathbf{P})-\log p_\theta(\mathbf{A}_{n}|\mathbf{A}_{n+1},\mathbf{L}) \right) \nonumber \\
&+\omega_{\text{spk}}\left(\log p_\theta(\mathbf{A}_{n}|\mathbf{A}_{n+1}, \mathbf{A}_p,\mathbf{L},\varnothing)-\log p_\theta(\mathbf{A}_{n}|\mathbf{A}_{n+1},\mathbf{L}) \right) \nonumber \\
&+\omega_{\text{ling}}\left(\log p_\theta(\mathbf{A}_{n}|\mathbf{A}_{n+1}, \mathbf{L})-\log p_\theta(\mathbf{A}_{n}|\mathbf{A}_{n+1}) \right),
\end{aligned}
$$

where $\omega_\text{all}$, $\omega_\text{spk}$, $\omega_\text{ling}$ are CFG coefficients of each conditioning, respectively.

We trained a model with four different conditions, as shown in [Fig.01](#fig:overall), and all-conditioned, spk-conditioned, ling-conditioned, and null-conditioned are randomly sampled in a 6:2:2:1 ratio.
For each codebook layer of audio tokens, linguistic and pitch conditions have their own mask tokens, which replace the original token to mask it.

> ![](Images/2025.09.21_MaskVCT_Fig.01.png)
>
> <a id="fig:overall">
> Fig.01: Overall system description of MaskVCT.
> We perform column-wise addition of the embeddings and feed the result into MaskVCT.
> We employ 9 codebooks for DAC, but display only 2 here for brevity.
> All models operate at 50 Hz frame rate.
> </a>

### Model Architecture Details: 模型架构细节

We utilize a simple **Transformer**[^Vaswani2017Transformer] encoder-based architecture, especially PreLN with rotary positional embedding.
For simplicity, FFNs use a ReLU activation and LayerNorm.
After the 16 Transformer encoder layers, each with 16 attention heads, model dimension of 1024, FFN dimension of 4096, we attach separate classification heads for each codebook index.

## 4·Experiments: 实验

### Baselines: 基线

For our baseline, we utilized the official implementation and checkpoint of [**Diff-HierVC** [URL]](https://github.com/hayeong0/Diff-HierVC) [^Choi2023Diff-HierVC], [**FACodec** [URL]](https://hf.co/amphion/naturalspeech3_facodec) [^Ju2024NaturalSpeech3] from **Amphion**[^Zhang2024Amphion], [**MaskGCT** [URL]](https://hf.co/amphion/MaskGCT) [^Wang2024MaskGCT],  [**FreeVC** [URL]](https://github.com/OlaWod/FreeVC) [^Li2022FreeVC], [**GenVC** [URL]](https://github.com/caizexin/GenVC) [^Cai2025GenVC].
For accent conversion, we only tested the models that achieved the best objective metrics in the main evaluation.
[Tab.01](#tab:baselines) shows the number of parameters and training dataset hours for each models.

> ![](Images/2025.09.21_MaskVCT_Tab.01.png)
> <a id="tab:baselines">
> Tab.01: Comparison of baseline models and ours with number of parameters and training hours.
> The parentheses next to parameters indicate the number of parameters pre-trained models they utilized.
> The parentheses next to the hour are the total hours spent training the linguistic feature extractor.
> </a>

### Training: 训练

We employ the official checkpoint of the **DAC**[^Kumar2023DAC] 16 kHz as a codec, from which we extract 9 codebook indices to encode each audio frame.
For syllabic representations, we use **SylBoost**[^Baade2024SyllableLM] 8.33 Hz, which has a minimum of tokens.
To improve robustness to phase-related inconsistency [^Liu2024Analyzing], we apply **PhaseAug**[^Lee2023PhaseAug] to all inputs before DAC encoder.
Although SylBoost yields consistent features, self-reconstruction can leak information (**NANSY**[^Choi2021NANSY]); thus, following **NANSY**[^Choi2021NANSY], we pitch-shift clean speech to create perturbed variants and train with a 50\% property for each case.

We trained our model from scratch for 250k steps on 2 A100 GPUs using AdamW with a batch size of 168 and a learning rate of 0.0002.
Additionally, both the layer drop rate and the dropout rate were set to 5\%.
We apply **SpecAugment**[^Park2019SpecAugment] to combined embeddings by randomly masking 10\% of the channel dimension of the input vectors.
For all cases, we utilize 3-second speaker prompts and 10.24-second as the source.

### Dataset: 数据集

Our model was trained on diverse speech corpora to enhance its generalization across accents and speaking styles.
Specifically, we utilized the train-clean subsets of **LibriTTS‑R**[^Koizumi2023LibriTTS-R], the train subset of **MLS**‑en[^Pratap2020MLS], **VCTK**[^Yamagishi2019VCTK], **LibriHeavy-Large**[^Kahn2019Libri-Light], [^Kang2024Libriheavy], the clean subset of **Hi-Fi TTS**[^Bakhturina2021Hi-Fi-TTS], **LJSpeech**[^Ito2017LJSpeech], and the speech subset of **RAVDESS**[^Livingstone2018RAVDESS].

We utilized the test-clean subset of **LibriTTS-R**[^Koizumi2023LibriTTS-R] to test the general performance of VC models.
Since the models were not trained with the long length, we select the speeches that are shorter than 10 seconds and slice 3 seconds for the prompt speeches.
We test the models with 511 sample pairs.

To assess accent conversion performance, we utilize the non-native English speech corpus **L2-ARCTIC** [^Zhao2018L2-ARCTIC].
Two conversion directions are considered:
- Libri$\rightarrow$L2: converting from near-native LibriTTS-R speech to accented speech,
- L2$\rightarrow$L2: conversion between distinct accents.

Test utterance pairs were selected using the same criteria as in the main test.
Additionally, while the ground truth (GT) exhibits low intelligibility, we omit intelligibility metrics for this test.

> ![](Images/2025.09.21_MaskVCT_Tab.02.png)
>
> <a id="tab:results_main">
> Tab.02: Evaluation results for MaskVCT and the baseline methods on the LibriTTS-R test-clean dataset.
> </a>

> ![](Images/2025.09.21_MaskVCT_Tab.03.png)
>
> <a id="tab:accent">
> Tab.03: Accent Conversion Results, the audios are converted from **Libri**TTS-R or **L2**-ARCTIC to L2-ARCTIC.
> </a>

### Inference Setup: 推理设置

During every iteration, we select the top probability positions by **Gumbel-Softmax**[^Jang2017GumbelSoftmax] following the number of cosine mask schedules.
For each chosen position, we sample token indices to unmask using top-$k$ ($k=35$) followed by top-$p$ ($p=0.9$).
We mainly evaluate the total $N=64$ iterations with per-codebook steps [40,16, 2,1,1,1,1,1,1].

Since MaskVCT supports multiple modes, we evaluated a wide range of ablation studies for the large search space of CFG weights to identify the optimal CFG weights for each condition.
From these experiments, we propose two primary modes: **MaskVCT-All** and **MaskVCT-Spk**.
***MaskVCT-All*** is conditioned by continuous linguistic features with CFG weights $(\omega_\text{all}, \omega_\text{spk}, \omega_\text{ling})=(1.5,\,1.0,\,1.0)$ including pitch-conditioned weights, it achieves superior intelligibility and source pitch-following but sacrifices some speaker similarity.
***MaskVCT-Spk*** is conditioned by quantized linguistic features with
CFG weights $(\omega_\text{all}, \omega_\text{spk}, \omega_\text{ling})=(0,\,2.0,\,0.5)$ omitting pitch conditioning yields the highest speaker similarity, though intelligibility is reduced.
For the accent conversion, we found that pitch-conditioned generation was affected by the L2-ARCTIC dataset's ambient noise, so we only tested ***MaskVCT-Spk*** with $(\omega_\text{all}, \omega_\text{spk}, \omega_\text{ling})=(0,2.5,0.5)$.

### Metrics: 评价指标

We assess intelligibility with WER/CER from **Whisper Large-V3**[^Radford2022Whisper], speaker similarity (S-SIM) as the cosine between **WavLM**[^Chen2021WavLM] speaker embeddings of the converted utterance and the target prompt, pitch tracking via the F0 Pearson correlation (FPC), and automatic quality via **UTMOSv2** [^Baba2024UTMOSv2].
Note that FPC is condition-dependent: higher is better for pitch-conditioned models, whereas for pitch-unconditioned models, whose goal is generating pitch from a prompt, a lower FPC can be preferable.
For the accent conversion, we measure accent similarity (A-SIM) by **CommonAccent**[^Zuluaga2023CommonAccent].
For the subjective measures, we collect human-evaluated mean opinion scores (MOS) from 1 to 5 for quality (Q-MOS), speaker similarity (SS-MOS).
Especially for the accent conversion test, we also collect accent similarity (AS-MOS).
For all MOS, we also show 95\% confidence intervals (CI).

## 5·Results: 结果

[Tab.02](#tab:results_main) presents results on the LibriTTS-R test-clean subset.
While FACodec achieves the lowest WER and CER and the highest speaker similarity among baselines, it yields low quality metrics.
MaskGCT-S2A attains the highest UTMOS and Q-MOS but has higher WER and CER, and lower speaker similarity.
FreeVC offers the highest FPC among baselines for pitch tracking, but compromises on UTMOS and speaker similarity.
Our MaskVCT supports flexible trade-offs via CFG weight tuning.
Under the CFG-All setting, the model achieved the second-highest FPC in all cases, demonstrating precise source-pitch tracking and delivering the best intelligibility among masked models, albeit with some reduction in speaker similarity.
In contrast, the CFG-Spk configuration prioritizes speaker fidelity, attaining the top speaker similarity of 0.895 and the lowest FPC of 0.157, enabling generation that diverges from the source pitch.
In subjective MOS evaluations, MaskVCT variants rank second in overall quality but lead in SS-MOS with 3.69, surpassing the next-best, MaskGCT with 3.47.
These results underscore MaskVCT’s ability to balance intelligibility and pitch controllability (CFG-All) against speaker similarity (CFG-Spk).

In the Libri$\rightarrow$L2, MaskGCT-S2A achieves the highest quality in both the objective and subjective metrics.
However, ***MaskVCT-Spk*** leads in speaker similarity and accent similarity and also attains the top SS-MOS and a competitive second in AS-MOS.
Although MaskVCT’s UTMOS is slightly lower than the MaskGCT-S2A, it has overlapped in CI and remains substantially more natural than others.
All systems except MaskVCT tend to preserve the original LibriTTS's American or British accent when converting to accented speech.
In the L2$\rightarrow$L2, MaskGCT-S2A again tops UTMOS with ***MaskVCT-Spk*** close behind, and ***MaskVCT-Spk*** attains the highest SS-MOS and matches baselines on AS-MOS.
Overall, ***MaskVCT-Spk*** offers the strongest speaker and accent fidelity while maintaining competitive listening quality.

## 6·Conclusions: 结论

We present ***MaskVCT***, a zero-shot voice conversion system that delivers unprecedented multi‑factor controllability through multiple classifier‑free guidances.
Unlike conventional VC methods, which are limited to a single conditioning setup, such as continuous vs. quantized linguistic features or pitch‑conditioned vs. unconditioned, ***MaskVCT*** integrates diverse conditions in a single model.
This controllability and the state-of-the-art speaker similarity are enabled by the syllabic representation, which leaks less pitch information with the lowest FPC results, while other linguistic features exhibit greater leakage.
However, despite their advantages, both continuous and quantized syllabic representations can degrade intelligibility by causing misreadings, as they are not trained to reconstruct original speech.
Moreover, because the syllabic quantizer is determined by K-means clustering (**SyllableLM**[^Baade2024SyllableLM], **Sylber**[^Cho2024Sylber]), misreadings caused by incorrect syllable mappings cannot be easily corrected.
In future work, we plan to address this limitation by training our own quantized representation using vector quantization with masked model training, aiming to preserve the benefits of the syllable representations while reducing intelligibility errors.

## References: 参考文献

[^Polyak2021Speech]: [Speech Resynthesis from Discrete Disentangled Self-Supervised Representations](../Tokenizers/2021.04.01_Resynthesis.md). ArXiv:2104.00355/InterSpeech2021.
[^Choi2021NANSY]: [**NANSY**: Neural Analysis and Synthesis: Reconstructing Speech from Self-Supervised Representations](../_waitlist/2021.10.27_NANSY.md). ArXiv:2110.14513v2/NeurIPS2021.
[^Choi2022NANSY++]: [**NANSY++**: Unified Voice Synthesis with Neural Analysis and Synthesis](../_waitlist/2022.11.17_NANSY++.md). ArXiv:2211.09407v1/ICLR2023Poster.
[^Li2022FreeVC]: [**FreeVC**: Towards High-Quality Text-Free One-Shot Voice Conversion](2022.10.27_FreeVC.md). ArXiv:2210.15418v1.
[^Choi2023Diff-HierVC]: [**Diff-HierVC**: Diffusion-Based Hierarchical Voice Conversion with Robust Pitch Generation and Masked Prior for Zero-shot Speaker Adaptation](2023.11.08_Diff-HierVC.md). ArXiv:2311.04693v1/InterSpeech2023Oral.
[^Wang2024MaskGCT]: [**MaskGCT**: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer](../SpeechLM_TTS/2024.09.01_MaskGCT.md). ArXiv:2409.00750v3.
[^Cai2025GenVC]: [**GenVC**: Self-Supervised Zero-Shot Voice Conversion](2025.02.06_GenVC.md). ArXiv:2502.04519v2/ASRU2025.
[^Kumar2023DAC]: [**DAC**: High-Fidelity Audio Compression with Improved RVQGAN](../Tokenizers/2023.06.11_Descript-Audio-Codec.md). ArXiv:2306.06546/NeurIPS2023.
[^Defossez2022EnCodec]: [**EnCodec**: High Fidelity Neural Audio Compression](../Tokenizers/2022.10.24_EnCodec.md). ArXiv:2210.13438/TMLR2023.
[^Ju2024NaturalSpeech3]: [**NaturalSpeech3/FACodec**: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models](../Diffusion/2024.03.05_NaturalSpeech3.md). ArXiv:2403.03100v3/ICML2024.
[^Vaswani2017Transformer]: [**Transformer**: Attention Is All You Need](../_Basis/2017.06.12_Transformer.md). ArXiv:1706.03762/NeurIPS2017.
[^Wang2023VALL-E]: [**VALL-E**: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers](../SpeechLM_TTS/2023.01.05_VALL-E.md). ArXiv:2301.02111/TASLP2025.
[^Baade2024SyllableLM]: [**SyllableLM**: Learning Coarse Semantic Units for Speech Language Models](../SpeechLM/2024.10.05_SyllableLM.md). ArXiv:2410.04029.
[^Ho2022CFG]: [**CFG**: Classifier-Free Diffusion Guidance](../Diffusion/_2022.07.26_Classifier-Free_Guidance.md). ArXiv:2207.12598v1/NeurIPS2021Workshop.
[^Choi2024Self]: [Self-Supervised Speech Representations are More Phonetic than Semantic](../Tokenizers/_2024.06.12_Choi2024Self.md). ArXiv:2406.08619v1/InterSpeech2024.
[^Cho2024Sylber]: [**Sylber**: Syllabic Embedding Representation of Speech from Raw Audio](../Tokenizers/2024.10.09_Sylber.md). ArXiv:2410.07168v2/ICLR2025.
[^Zhang2025Vevo]: [**Vevo**: Controllable Zero-Shot Voice Imitation with Self-Supervised Disentanglement](../SpeechLM_TTS/2025.02.11_Vevo.md). ArXiv:2502.07243v1/ICLR2025.
[^Devlin2018BERT]: [**BERT**: Pre-training of Deep Bidirectional Transformers for Language Understanding](../TextLM/2018.10.11_BERT.md). ArXiv:1810.04805v2/NAACL2019.
[^Chang2022MaskGIT]: [**MaskGIT**: Masked Generative Image Transformer](../_Basis/2022.02.08_MaskGIT.md). ArXiv:2202.04200v1.
[^Copet2023MusicGen]: [**MusicGen**: Simple and Controllable Music Generation](../Music/2023.06.08_MusicGen.md). ArXiv:2306.05284/NeurIPS2023.
[^Borsos2023SoundStorm]: [**SoundStorm**: Efficient Parallel Audio Generation](../SpeechLM/ST2S/2023.05.16_SoundStorm.md). ArXiv:2305.09636.
[^Lee2023VoiceLDM]: [**VoiceLDM**: Text-to-Speech with Environmental Context](../Diffusion/2023.09.24_VoiceLDM.md). ArXiv:2309.13664v1.
[^Yang2024DualSpeech]: [**DualSpeech**: Enhancing Speaker-Fidelity and Text-Intelligibility through Dual Classifier-Free Guidance](../Diffusion/2024.08.26_DualSpeech.md). ArXiv:2408.14423v2/InterSpeech2024.
[^Zhang2024Amphion]: [**Amphion**: An Open-Source Audio, Music and Speech Generation Toolkit](../Toolkits/2023.12.15_Amphion.md). ArXiv:2312.09911v3/IEEE@SLT2024.
[^Liu2024Analyzing]: [Analyzing and Mitigating Inconsistency in Discrete Audio Tokens for Neural Codec Language Models](../Tokenizers/_2024.09.28_Liu2024Analyzing.md). ArXiv:2409.19283v2/ACL2025.
[^Lee2023PhaseAug]: [**PhaseAug**: A Differentiable Augmentation for Speech Synthesis to Simulate One-to-Many Mapping](../../Modules/DataAugmentation/2022.11.08_PhaseAug.md). ArXiv:2211.04610v2.
[^Park2019SpecAugment]: [**SpecAugment**: A Simple Data Augmentation Method for Automatic Speech Recognition](../../Modules/DataAugmentation/2019.04.18_SpecAugment.md). ArXiv:1904.08779v3/InterSpeech2019.
[^Koizumi2023LibriTTS-R]: [**LibriTTS-R**: A Restored Multi-Speaker Text-to-Speech Corpus](../../Datasets/2023.05.30_LibriTTS-R.md). ArXiv:2305.18802v1/InterSpeech2023.
[^Pratap2020MLS]: [**MLS**: A Large-Scale Multilingual Dataset for Speech Research](../../Datasets/2020.12.07_MLS.md). ArXiv:2012.03411v2/InterSpeech2020.
[^Yamagishi2019VCTK]: [**CSTR VCTK Corpus**: English multi-speaker corpus for CSTR voice cloning toolkit (version 0.92)](../../Datasets/2012.08.00_VCTK.md). .
[^Kahn2019Libri-Light]: [**Libri-Light**: A Benchmark for ASR with Limited or No Supervision](../../Datasets/2019.12.17_Libri-Light.md). ArXiv:1912.07875v1/ICASSP2020.
[^Kang2024Libriheavy]: [**Libriheavy**: A 50,000 Hours ASR Corpus with Punctuation Casing and Context](../../Datasets/2023.09.15_Libriheavy.md). ArXiv:2309.08105v2/ICASSP2024.
[^Bakhturina2021Hi-Fi-TTS]: [**Hi-Fi TTS**: Hi-Fi Multi-Speaker English TTS Dataset](../../Datasets/2021.04.03_Hi-Fi_TTS.md). ArXiv:2104.01497v3.
[^Ito2017LJSpeech]: [**LJSpeech**: The LJ Speech Dataset](../../Datasets/2017.07.05_LJSpeech.md). .
[^Livingstone2018RAVDESS]: [**RAVDESS**: The Ryerson Audio-Visual Database of Emotional Speech and Song: A Dynamic, Multimodal Set of Facial and Vocal Expressions in North American English](../../Datasets/2018.05.16_RAVDESS.md). PLoS ONE2018.
[^Zhao2018L2-ARCTIC]: [**L2-ARCTIC**: A Non-native English Speech Corpus](../../Datasets/2018.09.02_L2-ARCTIC.md). InterSpeech2018.
[^Jang2017GumbelSoftmax]: [**Gumbel Softmax**: Categorical Reparameterization with Gumbel-Softmax](../../Modules/Reparameterization/2016.11.03_GumbelSoftmax.md). ArXiv:1611.01144v5.
[^Radford2022Whisper]: [**Whisper**: Robust Speech Recognition via Large-Scale Weak Supervision](../-ASR/2022.12.06_Whisper.md). ArXiv:2212.04356/ICML2023.
[^Chen2021WavLM]: [**WavLM**: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](../Tokenizers/2021.10.26_WavLM.md). ArXiv:2110.13900v5/JSTSP2022.
[^Baba2024UTMOSv2]: [**UTMOSv2**: The T05 System for The VoiceMOS Challenge 2024: Transfer Learning from Deep Image Classifier to Naturalness MOS Prediction of High-Quality Synthetic Speech](../../Evaluations/2024.09.14_UTMOSv2.md). ArXiv:2409.09305v1/IEEE@SLT2024.
[^Zuluaga2023CommonAccent]: [**CommonAccent**: Exploring Large Acoustic Pretrained Models for Accent Classification Based on Common Voice](../../Evaluations/2023.05.29_CommonAccent.md). ArXiv:2305.18283v1/InterSpeech2023.