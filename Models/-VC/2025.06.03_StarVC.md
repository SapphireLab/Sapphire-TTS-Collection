# StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech Generation in Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech Generation in Voice Conversion."
- 作者:
  - 01 Fengjin Li
  - 02 Jie Wang
  - 03 Yadong Niu
  - 04 Yongqing Wang
  - 05 Meng Meng
  - 06 Jian Luan
  - 07 Zhiyong Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2506.02414v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2506.02414v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.06.03_2506.02414v1_StarVC__A_Unified_Auto-Regressive_Framework_for_Joint_Text_and_Speech_Generation_in_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

Voice Conversion (VC) modifies speech to match a target speaker while preserving linguistic content.
Traditional methods usually extract speaker information directly from speech while neglecting the explicit utilization of linguistic content.
Since VC fundamentally involves disentangling speaker identity from linguistic content, leveraging structured semantic features could enhance conversion performance.
However, previous attempts to incorporate semantic features into VC have shown limited effectiveness, motivating the integration of explicit text modeling.
We propose **StarVC**, a unified autoregressive VC framework that first predicts text tokens before synthesizing acoustic features.
The experiments demonstrate that StarVC outperforms conventional VC methods in preserving both linguistic content (i.e., WER and CER) and speaker characteristics (i.e., SECS and MOS).
Audio demo can be found at: \url{https://thuhcsi.github.io/StarVC/}.

## 1·Introduction

Speaker identity is one of the fundamental characteristics of human speech, influencing communication, personalization and speech perception.

Voice Conversion (VC) is a technique that modifies the speaker identity of an utterance while preserving its linguistic content.

Traditional VC methods [^Stylianou1998Continuous], [^Bargum2023Reimagining] often attempt to extract and manipulate speaker identity directly from speech signals, yet this approach is inherently challenging due to the strong coupling between speaker characteristics and linguistic content [^Sisman2020Overview].

Disentangling these factors effectively remains an open problem, as naive transformations often lead to timbre leakage, unnatural prosody or a loss of intelligibility.

Compared to speaker-related features, extracting semantic information from speech is a more mature and well-established field, with robust methodologies [^Chen2022Wavlm], [^Hsu2021Hubert], [^Schneider2019Wav2vec], [^Baevski2020Wav2vec] available.

This raises an intriguing question: if a VC system can effectively incorporate semantic representation into its framework, could it help to better disentangle speaker identity from linguistic content$?$ Rather than directly extracting and modifying speaker characteristics, leveraging structured semantic representations might enable more effective speaker adaptation while maintaining intelligibility and naturalness.

Some recent VC methods [^Baade2024Neural], [^Li2024Sef-Vc] have attempted to integrate semantic information into their frameworks, but their effectiveness remains limited.

Many of these approaches rely on latent text-based features rather than explicitly generating textual content, which may result in weaker constraints on linguistic consistency and speaker preservation.

Recent advancements in VC can be categorized into two main directions: diffusion-based methods and large language model (LLM)-based methods.

Diffusion-based VC models, such as Diff-VC [^Popov2021Diffusion-Based] and StableVC [^Yao2024StableVC], generate high-quality speech by directly modeling audio signals, which allows for high-fidelity conversion while benefiting from recent optimizations that enable faster inference.

These models now generate speech in a single step, significantly improving efficiency.

On the other hand, LLM-driven approaches, including OpenVoice V2 [^Qin2023OpenVoice], LM-VC [^Wang2023Lm-Vc], and DualVC3 [^Ning2024DualVC], operate on acoustic tokens rather than raw audio, which may lead to some information loss but provides advantages in handling long-form speech.

Despite these advancements, there remains a gap in integrating semantic information effectively into VC frameworks, highlighting the need for a more structured approach.

Furthermore, conversation-oriented models such as Moshi [^D{\'e}fossez2024Moshi], Mini-Omni [^Xie2024Mini-Omni], and Llama-Omni [^Fang2024Llama-Omni] have demonstrated the potential of integrating text generation with speech transformation.

These models highlight how explicitly modeling linguistic content alongside speech synthesis can improve coherence and intelligibility.

Inspired by this, we propose **StarVC** (**S**peech-**T**ext **A**uto-**R**egressive **V**oice **C**onversion), a novel VC framework that unifies speech conversion and text generation within one auto-regressive structure.

Unlike conventional approaches that either rely on ASR-based transcriptions or operate in a purely textless manner, StarVC introduces an intermediate step where text tokens are predicted before generating acoustic features.

By grounding speech synthesis in a structured semantic representation, this approach may provide stronger linguistic consistency while preserving the speaker’s unique characteristics.

StarVC is designed with a multi-stage training strategy.

Initially, the model undergoes ASR pretraining, which may refine its ability to extract and encode semantic information.

The VC training phase then optimizes the transformation of speaker identity while preserving intelligibility, leveraging both semantic and speaker representations.

Finally, a multi-task learning framework is introduced to jointly optimize ASR and VC objectives, which may encourage better disentanglement of speaker and linguistic content.

Additionally, data augmentation techniques, incorporating both real and synthesized utterances, may help improve generalization and robustness in diverse linguistic and speaker conditions.

![](pictures/model_structure_3.0.png)

<a id="fig:speech_production">Architecture of StarVC</a>

By bridging the gap between diffusion-based, LLM-based, and traditional VC methodologies, StarVC has the potential to enable high-quality voice conversion with synchronized text outputs.

If effective, this approach may enhance applications such as personalized speech synthesis, interactive voice assistants, and content creation, offering a more unified and efficient solution to the challenges of VC research. 
