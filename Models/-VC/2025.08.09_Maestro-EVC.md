# Maestro-Evc: Controllable Emotional Voice Conversion Guided by References and Explicit Prosody

<details>
<summary>基本信息</summary>

- 标题: "Maestro-Evc: Controllable Emotional Voice Conversion Guided by References and Explicit Prosody."
- 作者:
  - 01 Jinsung Yoon
  - 02 Wooyeol Jeong
  - 03 Jio Gim
  - 04 Young-Joo Suh
- 链接:
  - [ArXiv](https://arxiv.org/abs/2508.06890v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2508.06890v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.08.09_2508.06890v1_Maestro-Evc__Controllable_Emotional_Voice_Conversion_Guided_by_References_and_Explicit_Prosody.pdf)
  - [Publication] #TODO

</details>

## Abstract

Emotional voice conversion (EVC) aims to modify the emotional style of speech while preserving its linguistic content.
In practical EVC, controllability, the ability to independently control speaker identity and emotional style using distinct references, is crucial.
However, existing methods often struggle to fully disentangle these attributes and lack the ability to model fine-grained emotional expressions such as temporal dynamics.
We propose Maestro-EVC, a controllable EVC framework that enables independent control of content, speaker identity, and emotion by effectively disentangling each attribute from separate references.
We further introduce a temporal emotion representation and an explicit prosody modeling with prosody augmentation to robustly capture and transfer the temporal dynamics of the target emotion, even under prosody-mismatched conditions.
Experimental results confirm that Maestro-EVC achieves high-quality, controllable, and emotionally expressive speech synthesis.

## 1·Introduction

Emotional voice conversion (EVC) aims to transform a given utterance into a different emotional style while preserving the linguistic content[^Zhou2022Emotional].

EVC has gained prominence due to its high potential in various applications, such as digital avatars[^Hussain2022Training], virtual assistants[^Chatterjee2021Real-Time], and human-computer interaction[^Pittermann2010Handling], [^Erol2019Toward].

Practical EVC systems require two key capabilities.

The first is controllability, which refers to the ability to control content, speaker identity, and emotional style independently.

The second is the ability to convey fine-grained emotional expressions, including temporal dynamics.

In particular, scenarios such as emotional dubbing, where generating fine-grained emotional expressions in the target voice is required, demand both controllability and emotional expressiveness.

![](figure/fig1.pdf)

<a id="fig:example">An example of speech conversion using Maestro-EVC, harmoniously integrating content, speaker identity, emotion, and temporal dynamics.</a>

Several approaches have been proposed to independently convert both the emotional style and speaker identity.

Some of these methods rely on predefined emotion categories (e.g., “happy,” “sad”)[^Qi2024Pavits], [^Zhou2020Converting], [^Zhou2021Limited], instead of using utterance-level emotion embeddings extracted from an emotion reference[^Zhou2022Emotion], [^Zhou2021Seen], [^Zhu2023Emotional], [^Chen2022Speaker-Independent].

However, the use of emotion categories limits generalization to unseen emotion states and lacks the expressiveness for fine-grained emotion modeling.

Similarly, approaches that use predefined speaker IDs as input often struggle to generalize to unseen speakers.

To overcome these limitations, recent frameworks adopt fully reference-guided mechanisms that enable independent control of content, speaker, and emotion by directly conditioning on reference utterances[^Shah2023Nonparallel], [^Wang2025Enhancing], [^Dutta2024Zero].

![](figure/arc.pdf)

<a id="fig:example">Overview structure of the proposed {\sysname}. $\mathit{x_c}$, $\mathit{x_e}$, and $\mathit{x_s}$ denote the content, emotion, and speaker reference utterance, which are identical during training such that $\mathit{x}_c = \mathit{x}_e = \mathit{x}_s$, where the reference is a single utterance from the training dataset.

This condition is illustrated by the red dashed line.</a>

Among such approaches, most adopt a reconstruction-based framework[^Dutta2024Zero], [^Wang2025Enhancing] by disentangling content, speaker, and emotion representations from a single utterance and reconstructing speech from them.

Although such approaches often produce natural speech, they struggle to fully disentangle these attributes, limiting the model’s ability to control each factor independently.

Moreover, since these methods rely on utterance-level emotion representations, they fail to capture fine-grained temporal dynamics in the emotional expression.

To effectively transfer the fine-grained temporal dynamics of the emotion reference, it is essential to extract temporal emotion representations.

For this purpose, several EVC approaches have proposed modeling prosody, such as pitch (F0), energy, and rhythm, which serve as effective carriers of temporal emotional characteristics[^Qi2024Pavits], [^Dutta2024Zero], [^Lu2021Multi-Speaker].

Nevertheless, these studies model prosody implicitly, predicting prosodic patterns from latent representations rather than directly conditioning on prosody extracted from audio, which limits their ability to transfer fine-grained temporal dynamics.

Thus, an explicit prosody modeling strategy that conditions on actual prosody extracted from an emotion reference is required to more accurately transfer fine-grained temporal dynamics.

However, one key challenge in applying this strategy is the prosody mismatch between the emotion and content references, which arises from differences in both linguistic content and emotional expression.

Directly applying prosodic features from a mismatched reference without accounting for these discrepancies can lead to unnatural or distorted speech.

In this work, we propose Maestro-EVC, a novel controllable EVC framework that harmonizes various attributes of emotional speech, including content, speaker identity, emotion, and temporal dynamics.

We achieve controllability by effectively disentangling content, speaker, and emotion information from separate reference utterances, allowing each attribute to be independently controlled.

We also introduce a temporal emotion representation and explicitly model the prosody of the emotion reference even under prosody-mismatched conditions, thereby enabling the transfer of target temporal emotional dynamics.

Specifically, we first propose temporal content-aware emotion modeling ({\conemo}), which leverages a cross-attention mechanism[^Vaswani2017Attention] to generate linguistic structure-aware temporal emotion embeddings.

It allows the model to capture temporally fine-grained emotional dynamics from the emotion reference.

Second, we present explicit emotion prosody transfer ({\emopro}), incorporating a prosody augmentation strategy that simulates prosody-mismatched conditions during training, resulting in more robust prosody modeling.

Finally, we introduce the emotion-invariant speaker encoder ({\spkenc}), where emotional information in speaker embeddings is suppressed using a gradient reversal layer (GRL)[^Ganin2015Unsupervised], and speaker consistency is further reinforced via a triplet loss.

As a result, {\sysname} achieves high-quality emotional voice conversion that exhibits both controllability and accurate emotional expressiveness, guided by reference inputs.

Our contributions are summarized as follows:

-  We propose {\sysname}, a controllable EVC framework that independently controls linguistic content, speaker identity, and emotional style using three distinct references.

-  We introduce a temporal emotion representation and an explicit prosody modeling method to capture and transfer temporally fine-grained emotional styles, even under prosody-mismatched conditions.

-  Through objective and subjective evaluations, we demonstrate that our method generates high-quality speech with rich emotional expressiveness and accurate control over each target attribute.

Audio samples are available at \url{https://maestroevc.github.io/demo/}.

## 2·Methods

Fig. 1 illustrates the overall architecture of {\sysname}.

During inference, the model takes three reference utterances for content, emotion, and speaker identity, which are encoded into latent representations.

A cross-attention mechanism combines the content and temporal emotion style representations to produce a content-aware emotion embedding.

Target duration is predicted using this embedding and the duration representation from the emotion reference.

The FE (F0/Energy) predictor receives F0 and energy extracted from the emotion reference, along with content and temporal emotion representations.

The predicted content, emotion, prosody, and speaker representations are integrated and fed into a HiFi-GAN[^Kong2020Hifi-Gan] vocoder for waveform synthesis.

In the following subsections, we provide a detailed description of each component of {\sysname}.

### Content Encoder

\label{sec:content_encoder}

To extract a content representation that captures only linguistic information from input reference audio, we follow prior works[^Li2024Sef-Vc], [^Kreuk2021Textless] that utilize a pre-trained HuBERT model[^Hsu2021Hubert], which was trained with a masked prediction task on audio signals.

Given a content reference $x_c$, the HuBERT model encodes it into a sequence of frame-level continuous representations $z$.

To discretize $z$, we apply K-means clustering to obtain a sequence of discrete units $\hat{z}$, which are then mapped to learnable embeddings via an embedding table, resulting in the discrete content representation $c$.

### Temporal Content-aware Emotion Modeling (TCEM)

\label{sec:temporal_content_aware_emotion_modeling}

To achieve temporally fine-grained emotional style transfer, we extract emotion representations at the frame level and align them with the target content via cross-attention mechanism.

Assuming the resulting representations may contain unintended content information, we apply a gradient reversal layer (GRL) to the cross-attention output to suppress residual content cues.

#### Temporal Emotion Encoder

To extract fine-grained temporal emotion representation, we adopt the approach of Wang *et al.*[^Wang2023Speech], which formulates speech emotion diarization as a task of predicting both emotion labels and their frame-level boundaries.

We use pre-trained model which has proven effective in downstream tasks such as emotional speech synthesis.

#### Cross-attention Mechanism

We use a cross-attention mechanism to temporally align frame-level emotional cues with the separately encoded linguistic content.

Given the content and emotion references, $x_c$ and $x_e$, the content encoder produces a frame-level sequence $c$, while the temporal emotion encoder generates a sequence of emotion embeddings $e$.

To incorporate the emotional information into the linguistic content in a content-aware manner, we use $c$ as the query sequence $Q$, and $e$ as the key and value sequence $K$ and $V$, respectively, resulting in an aligned emotion representation $\hat{e}$.

#### Residual Content Disentanglement

Although the temporal emotion encoder is trained to extract frame-level emotion representations, its short-term acoustic inputs can inherently contain both emotional and phonetic information.

We hypothesize that this feature-level entanglement causes the resulting representation $\hat{e}$ to retain unintended linguistic cues.

This can yield prosodic artifacts, degrading both the naturalness and emotional expressiveness, especially when transferring emotion across mismatched linguistic content.

To mitigate this, we apply a projection block to the cross-attention output, followed by a GRL and content classifier during training.

The content classification loss $\mathcal{L}_{cont}^{GRL}$ is imposed adversarially through the GRL to suppress residual linguistic information in the emotion representation.

This yields a disentangled emotion representation $\hat{e}_d$ that effectively preserves fine-grained emotional style from $x_e$ while being temporally aligned with the target content.

Ablation results presented in Table~[table_1](#table_1) empirically support our hypothesis.

Removing the content GRL reduces both emotional expressiveness and content fidelity, indicating that residual linguistic cues in the emotion representation interfere with effective style transfer.

### Explicit Emotional Prosody Transfer (EEPT)

To explicitly transfer the F0 and energy of the target prosody, we apply smoothing and prosody augmentation to these features and use them as conditions for the FE predictor, which generates the predictions aligned with the target content.

#### Prosody Extractor

We first extract three prosodic features from the emotion reference: F0, energy, and duration denoted as $\nu$, $\eta$, and $d$, respectively.

To emphasize the overall contour of prosodic patterns while suppressing micro-level fluctuations, we apply a Savitzky-Golay filter[^Savitzky1964Smoothing] to smooth the extracted features.

This step ensures that prosody transfer relies on general prosodic trends rather than on content-specific perturbations.

The smoothed F0, energy and duration are denoted as $\nu_s$, $\eta_s$, and $d_s$.

#### Prosody Augmentation

\label{sec:prosody_augmentation}

Reconstruction-based framework constrains the content and emotion reference to have the same prosody.

Thus, it cannot directly learn from prosody-mismatched scenarios, often resulting in unnatural speech during inference.

To address this, we introduce a prosody augmentation strategy that enables indirect learning of prosody transfer under prosody-mismatched conditions.

During training, $\nu_s$ and $\eta_s$ are randomly augmented using either random shifting or piecewise time warping, each selected with equal probability.

Random shifting, which shifts the entire prosody sequence along the time axis by a random amount, simulates misalignment between content and prosody preserving the internal prosodic pattern.

Piecewise time warping segments the prosody sequence, randomly stretches or compresses each segment along the time axis, and then concatenates and rescales the result to the original length.

This simulates partial mismatches in speaking rate or rhythm.

Formally, the augmentation process is defined as:

$$

\nu_a, \eta_a = \mathrm{ProAug}\left(\nu_s, \eta_s\right),

$$

where $\nu_a$ and $\eta_a$ are the augmented F0 and energy, and $\mathrm{ProAug}(\cdot)$ denotes the prosody augmentation module.

These augmentations allow the model to explicitly transfer prosody from any prosody-mismatched reference pair, preserving naturalness and expressiveness of speech during inference.

#### FE Predictor

\label{prosody_predictor}

To enable explicit prosody transfer adapted to the target content, we leverage not only the augmented F0 and energy but also incorporate the content embedding and the voiced/unvoiced (VUV) mask of the content reference $x_c$.

The VUV information plays a crucial role in guiding the model toward the perceptually relevant regions for prosody transfer.

As F0 and energy have limited relevance in unvoiced segments, explicitly incorporating the VUV mask enables the model to assign the essential prosodic information from the emotion reference $x_e$ to the voiced regions of $x_c$.

The inputs to the FE predictor are formally defined as follows:

$$

\hat{\nu}, \hat{\eta} = \mathrm{FEPred}\left(\nu_e + \eta_e + c + v \right),

$$

where $\hat{\nu}$ and $\hat{\eta}$ denote the predicted F0 and energy, $\nu_e$ and $\eta_e$ are the F0 and energy projected into a shared embedding space, $c$ and $v$ denote the discrete content representation and VUV mask extracted from $x_c$, and $\mathrm{FEPred}(\cdot)$ denotes the FE predictor.

#### Duration Predictor

To incorporate the target duration patterns, we predict the unit durations of $x_c$ based on its unique unit sequence, the smoothed durations $d_s$ and disentangled emotion representation $\hat{e}_d$ derived from $x_e$.

We design the duration predictor to take these three inputs for estimating the duration of each unit.

We first extract sequences of discrete units, $\hat{z}_c$ and $\hat{z}_e$ from $x_c$ and $x_e$, respectively.

To obtain a distinct sequence of units and their corresponding repetition counts, we apply a deduplication operation:

$$

\hat{z}_{uniq}, n_{count} = \text{dedup}(\hat{z}),

$$

where $\hat{z}_{uniq}$ denotes the sequence of unique units, and $n_{count}$ indicates the number of consecutive occurrences for each unit, which serves as the duration $d$.

From $x_e$, we extract $n_{count}$, which is then smoothed using a Savitzky–Golay filter to obtain the $d_s$.

We also extract $\hat{e}_d$ from $x_e$.

These, together with the unique unit sequence from $x_c$, are fed into the duration predictor to estimate the predicted duration $\hat{d}$.

#### Prosody Loss

During training, the FE predictor and duration predictor are optimized to predict their respective ground-truth targets.

Specifically, the FE predictors are trained to estimate $\nu$ and $\eta$, while the duration predictor learns to predict $d$ extracted from $x_c$, as formulated below:

$$

\mathcal{L}_{prosody} = \mathcal{L}_{f0} + \mathcal{L}_{energy} + \mathcal{L}_{dur},

$$

where $\mathcal{L}_{f0}$ and $\mathcal{L}_{energy}$ are L2 losses for F0 and energy prediction, and $\mathcal{L}_{dur}$ is the L1 loss for duration prediction.

\setlength{\textfloatsep}{0pt}
\setlength{\intextsep}{0pt}
\setlength{\tabcolsep}{4pt}

<a id="table_1">Objective evaluation results for WER, CER, EECS, SCA, F0-PCC, and E-PCC.</a>

### Emotion-Invariant Speaker Encoder (EISE)

We derive embeddings from the speaker reference $x_s$ that are invariant to emotional attributes while preserving speaker identity.

We adopt a pre-trained ECAPA-TDNN[^Desplanques2020Ecapa-TDNN], widely used for robust speaker representations, though it may still encode emotional information.

In order to mitigate the entanglement between speaker identity and emotional information, we append trainable layers to the output of the frozen pre-trained ECAPA-TDNN, referred to as the speaker encoder.

A GRL and an emotion classifier are applied to the appended layers.

The speaker encoder is trained adversarially using the emotion classification loss $\mathcal{L}_{emo}^{GRL}$ reversed by the GRL to encourage the suppression of emotional cues in the appended layers.

Although the GRL discourages the encoder from retaining emotional information, it does not explicitly enforce consistency across embeddings of the same speaker under different emotional conditions.

To address this limitation, we incorporate a triplet loss based on cosine similarity, which encourages embeddings of the same speaker under different emotional states to be more similar than those of different speakers.

Each triplet consists of an anchor, a positive sample from the same speaker with different emotions, and a negative sample from a different speaker.

The loss is defined as:

$$
\begin{aligned}

\mathcal{L}_{trip} = \sum_{i=1}^{N} \big[
& \operatorname{sim}\left(E_s(x_i^a), E_s(x_i^n)\right) \nonumber \\
& - \operatorname{sim}\left(E_s(x_i^a), E_s(x_i^p)\right) + \alpha
\big]_+,

\end{aligned}
$$

where $\operatorname{sim}(\cdot,\cdot)$ denotes the cosine similarity, and $x_i^a, x_i^p, x_i^n$ represent the anchor, positive, and negative samples, respectively.

The margin $\alpha$, set to 0.3, defines the minimum desired separation between the positive and negative pairs.

The total speaker loss is defined as:

$$
\begin{aligned}

\mathcal{L}_{spk} = \mathcal{L}_{trip} + \mathcal{L}_{emo}^{GRL},

\end{aligned}
$$

By combining GRL and triplet loss, the speaker encoder is encouraged to suppress emotional information and to maintain speaker-consistent embeddings across emotions.

### Training strategy

\label{SCM}

The model is trained to reconstruct the input waveform.

A single input $x$ serves as $x_c$, $x_e$, and $x_s$ with HiFi-GAN[^Kong2020Hifi-Gan] as the vocoder.

The generator $G$ and discriminator $D$ are optimized with the following losses:

$$

\mathcal{L}_{G} = \mathcal{L}_{adv}(G; D) + \mathcal{L}_{fm} + \mathcal{L}_{recon}(G),

$$

$$

\mathcal{L}_{D} = \mathcal{L}_{adv}(D; G),

$$

where $\mathcal{L}_{adv}$, $\mathcal{L}_{fm}$, and $\mathcal{L}_{recon}(G)$ represent the adversarial, feature matching, and reconstruction losses, respectively.

The total loss for $G$ is given by:

$$

\mathcal{L}_{G}^{total} = \mathcal{L}_{G} + \mathcal{L}_{spk} + \mathcal{L}_{cont}^{GRL} + \mathcal{L}_{prosody},

$$

where $\mathcal{L}_{spk}$, $\mathcal{L}_{cont}^{GRL}$, $\mathcal{L}_{prosody}$ are auxiliary losses for speaker supervision, content disentanglement via GRL, and prosody modeling, each weighted by a tunable coefficient $\lambda$.

## 3·Experiments

### Experimental Setup

#### Dataset

We used the 12-layer base HuBERT model[^Hsu2021Hubert] pre-trained on 960 hours of the LibriSpeech dataset[^Panayotov2015Librispeech], and the ECAPA-TDNN pre-trained on the VoxCeleb dataset[^Nagrani2017Voxceleb].

For training and evaluation, we used the English partition of the Emotional Speech Dataset[^Zhou2022Emotional], which contains 350 parallel utterances at 16 kHz from 10 English speakers across five emotions: neutral, happy, angry, sad, and surprise.

#### Implementation details

In our implementation, the content encoder used a vocabulary size of 500, with each token embedded into a 256-dimensional vector.

Input audio was converted to an 80-bin Mel-spectrogram with a window size 1,024 and a hop size 256, which was used for both Mel-based reconstruction loss and frame-level energy extraction.

Additionally, F0 was extracted using the WORLD vocoder[^Morise2016World].

In prosody augmentation, shifting moves the sequence by a random value in [-15, 15] frames, and piecewise time warping randomly splits it into 2–5 segments, each scaled by a factor randomly sampled from [0.4, 1.6].

Both the FE and duration predictors consist of two stacked Transformer blocks with 1D convolution layers replacing the feed-forward network[^Vaswani2017Attention].

The weight $\lambda_{recon}$ for Mel-based reconstruction loss was set to 45, while all other loss weights were set to 1.

The AdamW optimizer was used, with a learning rate of $2 \times 10^{-4}$.

#### Baselines

We adopt StyleVC[^Du2021Disentanglement] and ZEST[^Dutta2024Zero] as our baseline models.

StyleVC is an any-to-any expressive voice conversion framework designed to disentangle linguistic content, speaker identity, pitch, and emotional style information, enabling simultaneous conversion of arbitrary speaker identity and emotional style.

ZEST is a zero-shot EVC framework that separates speaker and emotion representations and predicts F0 from the extracted content, speaker, and emotion features, allowing it to handle reference-guided conversion with prosody transfer.

To the best of the authors’ knowledge, there has been no prior EVC model that simultaneously considers both pitch and energy in prosody modeling.

Therefore, we selected these two models as baselines for their focus on pitch transfer.

### Evaluation Settings

Rather than restricting speaker reference to the neutral emotional state, this experiment employed emotionally expressive references to evaluate the conversion to the target emotion style, thus enabling a more comprehensive assessment.

#### Seen dataset evaluation

We randomly constructed 700 test input sets, each composed of a content, speaker, and emotion reference drawn from different speakers and containing distinct linguistic content.

This setting ensures diverse evaluation conditions.

#### Zero-shot evaluation

\label{unseen_scenario}

To assess generalization, we evaluated {\sysname} under two unseen scenarios: unseen speakers (US) using 18 speakers from the VCTK corpus[^Yamagishi2017Cstr], and unseen emotion states (UE) using held-out classes, fear and disgust from the CREMA-D[^Cao2014Crema-D] and frustration and excitement from IEMOCAP[^Busso2008Iemocap].

#### Evaluation metrics

We evaluated the converted speech using six objective metrics.

For intelligibility, we computed word error rate (WER) and character error rate (CER) using Whisper[^Radford2023Robust].

Emotion similarity was measured by emotion embedding cosine similarity (EECS) with the emotion2vec+ base9 model[^Ma2023Emotion2vec].

Speaker similarity was assessed via speaker classification accuracy (SCA) based on pre-trained classifier.

We evaluated prosody alignment via Pearson correlation coefficient (PCC)[^Cohen2009Pearson] between the F0 and energy trajectories of the synthesized speech and the reference, after aligning them using dynamic time warping[^M{\"u}ller2007Dynamic].

For subjective evaluation, we conducted a Mean Opinion Score (MOS) test with 25 human participants, using 30 randomly sampled test pairs.

Participants rated naturalness, emotional similarity, speaker similarity, and prosody similarity, by comparing each sample with the corresponding target reference.

For prosody, they assessed the similarity of temporal variations in pitch, intensity, and speaking rate.

## 4·Results

### Objective Evaluations

As shown in Table~[table_1](#table_1), under the seen scenario, {\sysname} outperforms both baselines across all objective metrics.

Higher PCCs for F0 and energy indicate superior prosody modeling and faithful transfer of emotion reference.

These results confirm that {\sysname} achieves controllability through effective disentanglement of each attribute, while accurately modeling the temporal dynamics of emotional expression.

Table~[table_2](#table_2) shows that, in zero-shot tests on unseen speakers and unseen emotions, {\sysname} consistently surpasses the baselines on all metrics.

These results confirm the strong generalization capability of {\sysname} to both unseen speakers and emotion states.

\setlength{\tabcolsep}{2pt}
\setlength{\textfloatsep}{0pt}
\setlength{\intextsep}{0pt}
\setlength{\floatsep}{0pt}

<a id="table_2">Objective evaluation results in unseen scenarios</a>

\setlength{\textfloatsep}{5pt}
\setlength{\intextsep}{0pt}

<a id="table_3">Subjective evaluation results in terms of MOS</a>

### Subjective Evaluations

The results of subjective evaluations are presented in Table~[table_3](#table_3).

Across all criteria, {\sysname} attains the highest scores, significantly surpassing both baselines.

In particular, {\sysname} shows a clear advantage in prosody similarity, indicating that explicit prosody modeling contributes significantly to the perception of expressiveness.

These results indicate that {\sysname} not only improves objective quality but also delivers superior perceptual performance in EVC.

### Ablation Study

To investigate the effect of our proposed methods in {\sysname}, we conduct an ablation study on three key components: (1) content classifier and GRL in the TCEM module, (2) temporal emotion representation, (3) prosody augmentation in the EEPT module, and (4) the speaker loss $\mathcal{L}_{spk}$ in EISE.

The results are summarized in Table~[table_1](#table_1).

First, we investigate the effect of content disentanglement in the TCEM module by removing the content classifier and GRL.

This ablation causes significant degradation across all metrics, especially in WER and CER, indicating that content leakage in the emotion representation impairs reconstruction under linguistic mismatch.

Adversarial training therefore improves emotional expressiveness and content preservation.

Second, we evaluate the effect of temporal emotion representation by replacing the temporal emotion encoder with pre-trained utterance-level emotion encoder.

The results show decreases across all metrics, with particularly large drops in SCA and E-PCC.

These findings indicate that the temporal emotion representation contributes to notable improvements in various aspects of performance.

Third, to assess the impact of prosody augmentation, we remove the augmentation step during training.

This yields notable drops in WER, CER, EECS, and SCA, indicating reduced robustness under prosody mismatches.

Without exposure to prosodic variation, the model becomes overly dependent on the reference and attempts to forcibly align mismatched prosodic patterns with the source content.

This often leads to unclear articulation and unnatural temporal dynamics, where the rhythm and emphasis fail to align with the linguistic structure.

Consequently, although F0-PCC and E-PCC slightly increase as the model rigidly follows the reference, overall naturalness and generalization decline.

These results validate prosody augmentation for robust, natural prosody transfer.

Lastly, we evaluate the speaker encoder by removing the speaker loss $\mathcal{L}_{spk}$.

This ablation lowers SCA and slightly reduces EECS, suggesting residual emotional information in the speaker embedding that hinders accurate target-emotion modeling.

It highlights the need to minimize emotional entanglement and maintain embedding consistency for reliable identity control.

![](figure/fig3_a.pdf)

<a id="fig:f0_comparison">Visualization of F0 contours. (a) shows F0 comparisons with baselines, while (b) shows the results of Maestro-EVC using different emotion references.

In all conversions, the content and emotion references differ in both emotion category and linguistic content.

The two curves in (b) correspond to conversions using different utterances from the "Surprise'' category as emotion references.</a>

### Explicit Prosody Transfer

Fig.~[fig:f0_comparison](#fig:f0_comparison) shows the F0 contours of the emotion reference and the converted speech.

As shown in (a), compared with the baseline models, {\sysname} more accurately follows the pitch contour of the emotion reference.

Furthermore, (b) shows that, even within the same emotion category, variations in prosodic expression across different references result in distinct outputs, indicating that our model effectively reflects fine-grained prosodic differences.

## 5·Conclusion

In this paper, we propose {\sysname}, a novel controllable EVC framework that harmonizes various attributes of emotional speech, including content, speaker identity, emotion, and temporal dynamics.

By disentangling content, speaker, and emotion representations, it enables independent control of each attribute using separate reference utterances, even with any reference combination.

To achieve rich expressiveness, we introduce a temporal emotion representation and explicit prosody transfer, enabling effective performance even in prosody-mismatched scenarios.

Experimental results demonstrate that {\sysname} outperforms existing baselines across all metrics in both seen and zero-shot scenarios, validating its controllability and expressiveness.

\input{Sections/6_Acknowledgment}

\clearpage

\bibliographystyle{IEEEtran}
\bibliography{bibliography}

\end{document}
