# Maestro-Evc: Controllable Emotional Voice Conversion Guided by References and Explicit Prosody

<details>
<summary>基本信息</summary>

- 标题: "Maestro-Evc: Controllable Emotional Voice Conversion Guided by References and Explicit Prosody."
- 作者:
  - 01 Jinsung Yoon
  - 02 Wooyeol Jeong
  - 03 Jio Gim
  - 04 Young-Joo Suh
- 链接:
  - [ArXiv](https://arxiv.org/abs/2508.06890v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2508.06890v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.08.09_2508.06890v1_Maestro-Evc__Controllable_Emotional_Voice_Conversion_Guided_by_References_and_Explicit_Prosody.pdf)
  - [Publication] #TODO

</details>

## Abstract

Emotional voice conversion (EVC) aims to modify the emotional style of speech while preserving its linguistic content.
In practical EVC, controllability, the ability to independently control speaker identity and emotional style using distinct references, is crucial.
However, existing methods often struggle to fully disentangle these attributes and lack the ability to model fine-grained emotional expressions such as temporal dynamics.
We propose Maestro-EVC, a controllable EVC framework that enables independent control of content, speaker identity, and emotion by effectively disentangling each attribute from separate references.
We further introduce a temporal emotion representation and an explicit prosody modeling with prosody augmentation to robustly capture and transfer the temporal dynamics of the target emotion, even under prosody-mismatched conditions.
Experimental results confirm that Maestro-EVC achieves high-quality, controllable, and emotionally expressive speech synthesis.

## 1·Introduction

Emotional voice conversion (EVC) aims to transform a given utterance into a different emotional style while preserving the linguistic content[^Zhou2022Emotional].

EVC has gained prominence due to its high potential in various applications, such as digital avatars[^Hussain2022Training], virtual assistants[^Chatterjee2021Real-Time], and human-computer interaction[^Pittermann2010Handling], [^Erol2019Toward].

Practical EVC systems require two key capabilities.

The first is controllability, which refers to the ability to control content, speaker identity, and emotional style independently.

The second is the ability to convey fine-grained emotional expressions, including temporal dynamics.

In particular, scenarios such as emotional dubbing, where generating fine-grained emotional expressions in the target voice is required, demand both controllability and emotional expressiveness.

![](figure/fig1.pdf)

<a id="fig:example">An example of speech conversion using Maestro-EVC, harmoniously integrating content, speaker identity, emotion, and temporal dynamics.</a>

Several approaches have been proposed to independently convert both the emotional style and speaker identity.

Some of these methods rely on predefined emotion categories (e.g., “happy,” “sad”)[^Qi2024Pavits], [^Zhou2020Converting], [^Zhou2021Limited], instead of using utterance-level emotion embeddings extracted from an emotion reference[^Zhou2022Emotion], [^Zhou2021Seen], [^Zhu2023Emotional], [^Chen2022Speaker-Independent].

However, the use of emotion categories limits generalization to unseen emotion states and lacks the expressiveness for fine-grained emotion modeling.

Similarly, approaches that use predefined speaker IDs as input often struggle to generalize to unseen speakers.

To overcome these limitations, recent frameworks adopt fully reference-guided mechanisms that enable independent control of content, speaker, and emotion by directly conditioning on reference utterances[^Shah2023Nonparallel], [^Wang2025Enhancing], [^Dutta2024Zero].

![](figure/arc.pdf)

<a id="fig:example">Overview structure of the proposed {\sysname}. $\mathit{x_c}$, $\mathit{x_e}$, and $\mathit{x_s}$ denote the content, emotion, and speaker reference utterance, which are identical during training such that $\mathit{x}_c = \mathit{x}_e = \mathit{x}_s$, where the reference is a single utterance from the training dataset.

This condition is illustrated by the red dashed line.</a>

Among such approaches, most adopt a reconstruction-based framework[^Dutta2024Zero], [^Wang2025Enhancing] by disentangling content, speaker, and emotion representations from a single utterance and reconstructing speech from them.

Although such approaches often produce natural speech, they struggle to fully disentangle these attributes, limiting the model’s ability to control each factor independently.

Moreover, since these methods rely on utterance-level emotion representations, they fail to capture fine-grained temporal dynamics in the emotional expression.

To effectively transfer the fine-grained temporal dynamics of the emotion reference, it is essential to extract temporal emotion representations.

For this purpose, several EVC approaches have proposed modeling prosody, such as pitch (F0), energy, and rhythm, which serve as effective carriers of temporal emotional characteristics[^Qi2024Pavits], [^Dutta2024Zero], [^Lu2021Multi-Speaker].

Nevertheless, these studies model prosody implicitly, predicting prosodic patterns from latent representations rather than directly conditioning on prosody extracted from audio, which limits their ability to transfer fine-grained temporal dynamics.

Thus, an explicit prosody modeling strategy that conditions on actual prosody extracted from an emotion reference is required to more accurately transfer fine-grained temporal dynamics.

However, one key challenge in applying this strategy is the prosody mismatch between the emotion and content references, which arises from differences in both linguistic content and emotional expression.

Directly applying prosodic features from a mismatched reference without accounting for these discrepancies can lead to unnatural or distorted speech.

In this work, we propose Maestro-EVC, a novel controllable EVC framework that harmonizes various attributes of emotional speech, including content, speaker identity, emotion, and temporal dynamics.

We achieve controllability by effectively disentangling content, speaker, and emotion information from separate reference utterances, allowing each attribute to be independently controlled.

We also introduce a temporal emotion representation and explicitly model the prosody of the emotion reference even under prosody-mismatched conditions, thereby enabling the transfer of target temporal emotional dynamics.

Specifically, we first propose temporal content-aware emotion modeling ({\conemo}), which leverages a cross-attention mechanism[^Vaswani2017Attention] to generate linguistic structure-aware temporal emotion embeddings.

It allows the model to capture temporally fine-grained emotional dynamics from the emotion reference.

Second, we present explicit emotion prosody transfer ({\emopro}), incorporating a prosody augmentation strategy that simulates prosody-mismatched conditions during training, resulting in more robust prosody modeling.

Finally, we introduce the emotion-invariant speaker encoder ({\spkenc}), where emotional information in speaker embeddings is suppressed using a gradient reversal layer (GRL)[^Ganin2015Unsupervised], and speaker consistency is further reinforced via a triplet loss.

As a result, {\sysname} achieves high-quality emotional voice conversion that exhibits both controllability and accurate emotional expressiveness, guided by reference inputs.

Our contributions are summarized as follows:

-  We propose {\sysname}, a controllable EVC framework that independently controls linguistic content, speaker identity, and emotional style using three distinct references.

-  We introduce a temporal emotion representation and an explicit prosody modeling method to capture and transfer temporally fine-grained emotional styles, even under prosody-mismatched conditions.

-  Through objective and subjective evaluations, we demonstrate that our method generates high-quality speech with rich emotional expressiveness and accurate control over each target attribute.

Audio samples are available at \url{https://maestroevc.github.io/demo/}.
