AUTOVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss
Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, Mark Hasegawa-Johnson
Non-parallel many-to-many voice conversion, as well as zero-shot voice conversion, remain under-explored areas. Deep style transfer algorithms, such as generative adversarial networks (GAN) and conditional variational autoencoder (CVAE), are being applied as new solutions in this field. However, GAN training is sophisticated and difficult, and there is no strong evidence that its generated speech is of good perceptual quality. On the other hand, CVAE training is simple but does not come with the distribution-matching property of a GAN. In this paper, we propose a new style transfer scheme that involves only an autoencoder with a carefully designed bottleneck. We formally show that this scheme can achieve distribution-matching style transfer by training only on a self-reconstruction loss. Based on this scheme, we proposed AUTOVC, which achieves state-of-the-art results in many-to-many voice conversion with non-parallel data, and which is the first to perform zero-shot voice conversion.
Comments:	To Appear in Thirty-sixth International Conference on Machine Learning (ICML 2019)
Subjects:	Audio and Speech Processing (eess.AS); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Sound (cs.SD); Machine Learning (stat.ML)
Cite as:	arXiv:1905.05879 [eess.AS]
 	(or arXiv:1905.05879v2 [eess.AS] for this version)

https://doi.org/10.48550/arXiv.1905.05879
Focus to learn more
Submission history
From: Xuesong Yang [view email]
[v1] Tue, 14 May 2019 23:19:04 UTC (527 KB)
[v2] Thu, 6 Jun 2019 05:44:48 UTC (352 KB)# AUTOVC: Zero-Shot Voice Style Transfer With Only Autoencoder Loss

<details>
<summary>基本信息</summary>

- 标题: "AUTOVC: Zero-Shot Voice Style Transfer With Only Autoencoder Loss."
- 作者:
  - 01 Kaizhi Qian
  - 02 Yang Zhang
  - 03 Shiyu Chang
  - 04 Xuesong Yang
  - 05 Mark Hasegawa-Johnson
- 链接:
  - [ArXiv](https://arxiv.org/abs/1905.05879v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:1905.05879v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2019.05.14_1905.05879v1_Zero-Shot_Voice_Style_Transfer_With_Only_Autoencoder_Loss.pdf)
  - [ArXiv:1905.05879v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2019.05.14_1905.05879v2_AUTOVC__Zero-Shot_Voice_Style_Transfer_With_Only_Autoencoder_Loss.pdf)
  - [Publication] #TODO

</details>

## Abstract

Non-parallel many-to-many voice conversion, as well as zero-shot voice conversion, remain under-explored areas.
Deep style transfer algorithms, such as generative adversarial networks (GAN) and conditional variational autoencoder (CVAE), are being applied as new solutions in this field.
However, GAN training is sophisticated and difficult, and there is no strong evidence that its generated speech is of good perceptual quality.
On the other hand, CVAE training is simple but does not come with the distribution-matching property of a GAN.
In this paper, we propose a new style transfer scheme that involves only an autoencoder with a carefully designed bottleneck.
We formally show that this scheme can achieve distribution-matching style transfer by training only on a self-reconstruction loss.
Based on this scheme, we proposed \algnamens, which achieves state-of-the-art results in many-to-many voice conversion with non-parallel data, and which is the first to perform zero-shot voice conversion.

## 1·Introduction

\label{sec:intro}

The idea of speaking in someone else's voice never fails to be a fascinating element in action and fiction movies, and it also finds its way to many practical applications, \emph{e.g.} privacy and identity protection, creative industry \emph{etc.}  In the speech research community, this task is referred to as the voice conversion problem, which involves modifying a given speech from a source speaker to match the vocal qualities of a target speaker.

Despite the continuing research efforts in voice conversion, three problems remain under-explored.

First, most voice conversion systems assume the availability of parallel training data, \emph{i.e.} speech pairs where the two speakers utter the same sentences.  Only a few can be trained on non-parallel data.

Second, among the few existing algorithms that work on non-parallel data, even fewer can work for many-to-many conversion, \emph{i.e.} converting from multiple source speakers to multiple target speakers.

Last but not least, no voice conversion systems are able to perform zero-shot conversion, \emph{i.e.} conversion to the voice of an unseen speaker by looking at only a few of his/her utterances.

With the recent advances in deep style transfer, the traditional voice conversion problem is being recast as a style transfer problem, where the vocal qualities can be regarded as styles, and speakers as domains.

There are many style transfer algorithms that do not require parallel data, and are applicable to multiple domains, so they are readily available as new solutions to voice conversion.

In particular, generative adversarial network (GAN) \cite{goodfellow2014generative} and conditional variational autoencoder (CVAE) \cite{kingma2013auto, kingma2014semi}, are gaining popularity in voice conversion.

However, neither GAN nor CVAE is perfect.

GAN comes with a nice theoretical justification that the generated data would match the distribution of the true data, and has achieved state-of-the-art results, particularly in computer vision.  However, it is widely acknowledged that GAN is very hard to train, and its convergence property is fragile.  Also, although there is an increasing number of works that introduce GAN to speech generation \cite{donahue2018adversarial} and speech domain transfer \cite{pascual2017segan, subakan2018generative, fan2018svsgan, hosseini2018multi}, there is no strong evidence that the generated speech ***sounds*** real.  Speech that is able to fool the discriminators has yet to fool human ears.  On the other hand, CVAE is easier to train.  All it needs to do is to perform self-reconstruction and maximize a variational lower bound of the output probability.

The intuition is to infer a hypothetical style-independent hidden variable, which is then combined with the new style information to generate the style-transferred output.

However, CVAE alone does not guarantee distribution matching, and often suffers from over-smoothing of the conversion output \cite{kameoka2018stargan}.

Due to the lack of a suitable style transfer algorithm, existing voice conversion systems have yet to produce satisfactory results, which naturally leads to the following formulation of the problem.

Is there a style transfer algorithm that can be proven to match the distribution as GAN does, that trains as easily as CVAE, and that works better for speech?

Motivated by this, in this paper, we propose a new style transfer scheme, which involves only a ***vanilla*** autoencoder with a carefully designed bottleneck.  Similar to CVAE, the proposed scheme only needs to be trained on the self-reconstruction loss, but it has a distribution matching property similar to GAN's.

This is because the correctly-designed bottleneck will learn to remove the style information from the source and get the style-independent code, which is the goal of CVAE, but which the training scheme of CVAE is unable to guarantee.

Based on this scheme, we propose \algnamens, a many-to-many voice style transfer algorithm without parallel data. \algname follows the autoencoder framework and is trained only on autoencoder loss, but it introduces carefully-tuned dimension reduction and temporal downsampling to constrain the information flow.

As we will show, this simple scheme leads to a significant performance gain. \algname achieves superior performance on a traditional many-to-many conversion task, where all the speakers are seen in the training set.

Also, equipped with a speaker embedding trained for speaker verification \cite{heigold2016end, wan2018generalized}, \algname is among the first to perform zero-shot voice conversion with decent performance.

Considering the quality of the results and the simplicity of its training scheme, \algname opens a new path towards a simpler and better voice conversion and general style transfer systems.

The implementation will become publicly available. %\shiyu{implementation will become available??}

## 2·Related Works

\label{sec:realted}

There are several works that perform non-parallel many-to-many voice conversion using VAE and its combination with adversarial training.

VAE-VC \cite{hsu2016voice} is a simple voice conversion system using VAE.

Afterward, much research focuses on removing the style information from the VAE code.

VAW-GAN \cite{hsu2017voice} introduces a GAN on the VAE output.

CDVAE-VC \cite{huang2018voice} introduces two VAEs on two spectral features and forces the latent codes of the two features to contain similar information.

ACVAE-VC \cite{kameoka2018acvae} introduces an auxiliary classifier on the output to encourage the conversion results to be correctly classified as the target speaker's utterances. \citet{chou2018multi} introduce a classifier on the code and a GAN on the output.

Similarly, StarGAN \cite{kaneko2017parallel} and CycleGAN \cite{zhu2017unpaired}, which consist of encoder-decoder architectures with GAN, are applied to voice conversion \cite{kameoka2018stargan, fang2018high}.

GAN alone is also applied to voice conversion \cite{gao2018voice}.

However, the conversion quality of these algorithms is still limited.

Text transcriptions are introduced to assist the learning of the latent code \cite{xie2016kl, saito2018non, biadsy2019parrotron}, but we will focus on voice conversion without text transcriptions, which is more flexible for low-resourced languages.

\citet{atalla2019look, chou2018multi, nachmani2019unsupervised} conduct research on style transfer using autoencoder, but none has unveiled its distribution-matching property by properly designing the bottleneck.
