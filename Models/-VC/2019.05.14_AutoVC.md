AUTOVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss
Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, Mark Hasegawa-Johnson
Non-parallel many-to-many voice conversion, as well as zero-shot voice conversion, remain under-explored areas. Deep style transfer algorithms, such as generative adversarial networks (GAN) and conditional variational autoencoder (CVAE), are being applied as new solutions in this field. However, GAN training is sophisticated and difficult, and there is no strong evidence that its generated speech is of good perceptual quality. On the other hand, CVAE training is simple but does not come with the distribution-matching property of a GAN. In this paper, we propose a new style transfer scheme that involves only an autoencoder with a carefully designed bottleneck. We formally show that this scheme can achieve distribution-matching style transfer by training only on a self-reconstruction loss. Based on this scheme, we proposed AUTOVC, which achieves state-of-the-art results in many-to-many voice conversion with non-parallel data, and which is the first to perform zero-shot voice conversion.
Comments:	To Appear in Thirty-sixth International Conference on Machine Learning (ICML 2019)
Subjects:	Audio and Speech Processing (eess.AS); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Sound (cs.SD); Machine Learning (stat.ML)
Cite as:	arXiv:1905.05879 [eess.AS]
 	(or arXiv:1905.05879v2 [eess.AS] for this version)

https://doi.org/10.48550/arXiv.1905.05879
Focus to learn more
Submission history
From: Xuesong Yang [view email]
[v1] Tue, 14 May 2019 23:19:04 UTC (527 KB)
[v2] Thu, 6 Jun 2019 05:44:48 UTC (352 KB)# AUTOVC: Zero-Shot Voice Style Transfer With Only Autoencoder Loss

<details>
<summary>基本信息</summary>

- 标题: "AUTOVC: Zero-Shot Voice Style Transfer With Only Autoencoder Loss."
- 作者:
  - 01 Kaizhi Qian
  - 02 Yang Zhang
  - 03 Shiyu Chang
  - 04 Xuesong Yang
  - 05 Mark Hasegawa-Johnson
- 链接:
  - [ArXiv](https://arxiv.org/abs/1905.05879v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:1905.05879v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2019.05.14_1905.05879v1_Zero-Shot_Voice_Style_Transfer_With_Only_Autoencoder_Loss.pdf)
  - [ArXiv:1905.05879v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2019.05.14_1905.05879v2_AUTOVC__Zero-Shot_Voice_Style_Transfer_With_Only_Autoencoder_Loss.pdf)
  - [Publication] #TODO

</details>

## Abstract

Non-parallel many-to-many voice conversion, as well as zero-shot voice conversion, remain under-explored areas.
Deep style transfer algorithms, such as generative adversarial networks (GAN) and conditional variational autoencoder (CVAE), are being applied as new solutions in this field.
However, GAN training is sophisticated and difficult, and there is no strong evidence that its generated speech is of good perceptual quality.
On the other hand, CVAE training is simple but does not come with the distribution-matching property of a GAN.
In this paper, we propose a new style transfer scheme that involves only an autoencoder with a carefully designed bottleneck.
We formally show that this scheme can achieve distribution-matching style transfer by training only on a self-reconstruction loss.
Based on this scheme, we proposed \algnamens, which achieves state-of-the-art results in many-to-many voice conversion with non-parallel data, and which is the first to perform zero-shot voice conversion.

## 1·Introduction

\label{sec:intro}

The idea of speaking in someone else's voice never fails to be a fascinating element in action and fiction movies, and it also finds its way to many practical applications, \emph{e.g.} privacy and identity protection, creative industry \emph{etc.}  In the speech research community, this task is referred to as the voice conversion problem, which involves modifying a given speech from a source speaker to match the vocal qualities of a target speaker.

Despite the continuing research efforts in voice conversion, three problems remain under-explored.

First, most voice conversion systems assume the availability of parallel training data, \emph{i.e.} speech pairs where the two speakers utter the same sentences.  Only a few can be trained on non-parallel data.

Second, among the few existing algorithms that work on non-parallel data, even fewer can work for many-to-many conversion, \emph{i.e.} converting from multiple source speakers to multiple target speakers.

Last but not least, no voice conversion systems are able to perform zero-shot conversion, \emph{i.e.} conversion to the voice of an unseen speaker by looking at only a few of his/her utterances.

With the recent advances in deep style transfer, the traditional voice conversion problem is being recast as a style transfer problem, where the vocal qualities can be regarded as styles, and speakers as domains.

There are many style transfer algorithms that do not require parallel data, and are applicable to multiple domains, so they are readily available as new solutions to voice conversion.

In particular, generative adversarial network (GAN) \cite{goodfellow2014generative} and conditional variational autoencoder (CVAE) \cite{kingma2013auto, kingma2014semi}, are gaining popularity in voice conversion.

However, neither GAN nor CVAE is perfect.

GAN comes with a nice theoretical justification that the generated data would match the distribution of the true data, and has achieved state-of-the-art results, particularly in computer vision.  However, it is widely acknowledged that GAN is very hard to train, and its convergence property is fragile.  Also, although there is an increasing number of works that introduce GAN to speech generation \cite{donahue2018adversarial} and speech domain transfer \cite{pascual2017segan, subakan2018generative, fan2018svsgan, hosseini2018multi}, there is no strong evidence that the generated speech ***sounds*** real.  Speech that is able to fool the discriminators has yet to fool human ears.  On the other hand, CVAE is easier to train.  All it needs to do is to perform self-reconstruction and maximize a variational lower bound of the output probability.

The intuition is to infer a hypothetical style-independent hidden variable, which is then combined with the new style information to generate the style-transferred output.

However, CVAE alone does not guarantee distribution matching, and often suffers from over-smoothing of the conversion output \cite{kameoka2018stargan}.

Due to the lack of a suitable style transfer algorithm, existing voice conversion systems have yet to produce satisfactory results, which naturally leads to the following formulation of the problem.

Is there a style transfer algorithm that can be proven to match the distribution as GAN does, that trains as easily as CVAE, and that works better for speech?

Motivated by this, in this paper, we propose a new style transfer scheme, which involves only a ***vanilla*** autoencoder with a carefully designed bottleneck.  Similar to CVAE, the proposed scheme only needs to be trained on the self-reconstruction loss, but it has a distribution matching property similar to GAN's.

This is because the correctly-designed bottleneck will learn to remove the style information from the source and get the style-independent code, which is the goal of CVAE, but which the training scheme of CVAE is unable to guarantee.

Based on this scheme, we propose \algnamens, a many-to-many voice style transfer algorithm without parallel data. \algname follows the autoencoder framework and is trained only on autoencoder loss, but it introduces carefully-tuned dimension reduction and temporal downsampling to constrain the information flow.

As we will show, this simple scheme leads to a significant performance gain. \algname achieves superior performance on a traditional many-to-many conversion task, where all the speakers are seen in the training set.

Also, equipped with a speaker embedding trained for speaker verification \cite{heigold2016end, wan2018generalized}, \algname is among the first to perform zero-shot voice conversion with decent performance.

Considering the quality of the results and the simplicity of its training scheme, \algname opens a new path towards a simpler and better voice conversion and general style transfer systems.

The implementation will become publicly available. %\shiyu{implementation will become available??}

## 2·Related Works

\label{sec:realted}

There are several works that perform non-parallel many-to-many voice conversion using VAE and its combination with adversarial training.

VAE-VC \cite{hsu2016voice} is a simple voice conversion system using VAE.

Afterward, much research focuses on removing the style information from the VAE code.

VAW-GAN \cite{hsu2017voice} introduces a GAN on the VAE output.

CDVAE-VC \cite{huang2018voice} introduces two VAEs on two spectral features and forces the latent codes of the two features to contain similar information.

ACVAE-VC \cite{kameoka2018acvae} introduces an auxiliary classifier on the output to encourage the conversion results to be correctly classified as the target speaker's utterances. \citet{chou2018multi} introduce a classifier on the code and a GAN on the output.

Similarly, StarGAN \cite{kaneko2017parallel} and CycleGAN \cite{zhu2017unpaired}, which consist of encoder-decoder architectures with GAN, are applied to voice conversion \cite{kameoka2018stargan, fang2018high}.

GAN alone is also applied to voice conversion \cite{gao2018voice}.

However, the conversion quality of these algorithms is still limited.

Text transcriptions are introduced to assist the learning of the latent code \cite{xie2016kl, saito2018non, biadsy2019parrotron}, but we will focus on voice conversion without text transcriptions, which is more flexible for low-resourced languages.

\citet{atalla2019look, chou2018multi, nachmani2019unsupervised} conduct research on style transfer using autoencoder, but none has unveiled its distribution-matching property by properly designing the bottleneck.

## 3·Style Transfer Autoencoder

\label{sec:framework}

In this section, we will discuss how and why an autoencoder can match the data distribution as GAN does.

Although our intended application is voice conversion, the discussion in this section is applicable to other style transfer applications as well.  As general mathematical notations, upper-case letters, \emph{e.g.} $X$, denote random variables/vectors; lower-case letters, \emph{e.g.} $x$, denote deterministic values or instances of random variables; $X(1:T)$ denotes a random process, with $(1:T)$ denoting a collection of time indices running from $1$ to $T$.

For notational ease, sometimes the time indices are omitted to represent the collection of the random process at all times. $p_X(\cdot | Y)$ denotes the probability mass function (PMF) or probability density function (PDF) of $X$ conditional on $Y$; $p_X(\cdot | Y=y)$, or sometimes $p_X(\cdot | y)$ without causing confusions, denotes the PMF/PDF of $X$ conditional on $Y$ taking a specific value $y$; similarly, $\mathbb{E}[X|Y]$, $\mathbb{E}[X|Y=y]$ and $\mathbb{E}[X|y]$ denote the corresponding conditional expectations.

It is worth mentioning that $\mathbb{E}[X|Y]$ is still a random, but $\mathbb{E}[X|Y=y]$ or $\mathbb{E}[X|y]$. $H(\cdot)$ denotes the entropy, and $H(\cdot | \cdot)$ denotes the conditional entropy. %It is worth mentioning that $\mathbb{E}[X|Y]$ is still a random variable because of the randomness in $Y$, but $\mathbb{E}[X|Y=y]$ and $\mathbb{E}[X|y]$ are no longer random.

### Problem Formulation

\label{subsec:formulation}

Assume that speech is generated by the following stochastic process.

First, a speaker identity $U$ is a random variable drawn from the speaker population $p_U(\cdot)$.

Then, a content vector $Z=Z(1:T)$ is a random process drawn from the joint content distribution $p_Z(\cdot)$.

Here content refers to the phonetic and prosodic information.

Finally, given the speaker identity and content, the speech segment $X=X(1:T)$ is a random process randomly sampled from the speech distribution, \emph{i.e.} $p_{X}(\cdot| U, Z)$, which characterizes the distribution of the speaker $U$'s speech uttering the content $Z$. $X(t)$ can represents a sample of a speech waveform, or a frame of a speech spectrogram.

In this paper, we will be working on speech spectrograms.

Here, we assume that each speaker produces the same amount of gross information, \emph{i.e.}

$$

\small
H(X | U = u) = h_\textrm{speech} = \textrm{constant},
\label{eq:constant_info}

$$

regardless of $u$.

Now, assume two sets of variables, $(U_1, Z_1, X_1)$ and $(U_2, Z_2, X_2)$, are independent and identically distributed (i.i.d.) random samples generated from this process. $(U_1, Z_1, X_1)$ belong to the \emph{source speaker} and $(U_2, Z_2, X_2)$ belong to the \emph{target speaker}.

Our goal is to design a speech converter that produces the conversion output, $\hat{X}_{1\rightarrow 2}$, which preserves the content in $X_1$, but matches the speaker characteritics of speaker $U_2$.

Formally, an ideal speech converter should have the following desirable property:

$$

\small

$$
\begin{aligned}

p_{\hat{X}_{1\rightarrow 2}}(\cdot | U_2 = u_2, Z_1 = z_1) = p_X(\cdot | U=u_2, Z=z_1).
\label{eq:ideal}

\end{aligned}
$$

$$

Eq.~\eqref{eq:ideal} means that given the target speaker's identity $U_2=u_2$ and the content in the source speech $Z_1 = z_1$, the converted speech should sound like $u_2$ uttering $z_1$.

When $U_1$ and $U_2$ are both seen in the training set, the problem is a standard multi-speaker conversion problem, which has been addressed by some existing works.

When $U_1$ or $U_2$ is not included in the training set, the problem becomes the more challenging zero-shot voice conversion problem, which is also a target task of the proposed \algnamens.  %This problem formulation can be extended to a general style transfer setting, where $U_1$ and $U_2$ can represent two domains and $X_1$ and $X_2$ can represent samples from their respective domains.

![](figures/framework2.pdf)

<a id="fig:framework">The style transfer autoencoder framework.

The ovals denote the probabilistic graphical model of the speech generation process.

The grey boxes denote pre-trained modules. (a) During conversion, the source speech is fed to the content encoder.

An utterance of the target speaker is fed to the speaker encoder.

The decoder produces the conversion results. (b) During training, the source speech is fed to the content encoder.

Another utterance of the same \emph{source} speaker is fed to the speaker encoder.

The content encoder and the decoder minimize the self-reconstruction error.</a>

### The Autoencoder Framework

\algname solves the voice conversion problem with a very simple autoencoder framework, as shown in Fig.~[fig:framework](#fig:framework).

The framework consists of three modules, a content encoder $E_c(\cdot)$ that produces a content embedding from speech, a speaker encoder $E_s(\cdot)$ that produces a speaker embedding from speech, and a decoder $D(\cdot, \cdot)$ that produce speech from content and speaker embeddings.

The inputs to these modules are different for conversion and training. %\shiyu{consider fig1 put training on the left and conversion on the right and talk about training first for the following text.}

\paragraph{Conversion:}

As shown in Fig.~[fig:framework](#fig:framework)(a), during the actual conversion, the source speech $X_1$ is fed into the content encoder to have content information extracted.

The target speech is fed into the speaker encoder to provide target speaker information.

The decoder produces the converted speech based on the content information in the source speech and the speaker information in the target speech.

$$

\small

$$
\begin{aligned}

C_1 = E_c(X_1),  \quad S_2 = E_s(X_2), \quad \hat{X}_{1\rightarrow2} = D(C_1, S_2).

\end{aligned}
$$

\label{eq:convert}

$$

Here $C_1$ and $\hat{X}_{1\rightarrow2}$ are both random processes. $S_2$ is simply a random vector.

\paragraph{Training:}

Throughout the paper, we will assume the speaker encoder is already pre-trained to extract some form of speaker dependent embedding, so by training we refer to the training of the content encoder and the decoder.

As shown in Fig.~[fig:framework](#fig:framework)(b), since we do not assume the availability of parallel data, only self-reconstruction is needed for training.

More specifically, the input to the content encoder is still $X_1$, but the input to the style encoder becomes an utterance from the same speaker $U_1$, denoted as $X'_1$.\footnote{$X_1'$ and $X_1$ can be the same or different.}

Then for each input speech $X_1$, \algname learns to reconstruct itself:

$$

\small

$$
\begin{aligned}

C_1 = E_c(X_1),  \quad S_1 = E_s(X'_1), \quad \hat{X}_{1\rightarrow1} = D(C_1, S_1).

\end{aligned}
$$

\label{eq:training}

$$

The loss function to minimize is simply the weighted combination of the self-reconstruction error and the content code reconstruction error, \emph{i.e.}

$$

\small
\min_{E_c(\cdot), D(\cdot, \cdot)}

L = L_\textrm{recon} + \lambda L_\textrm{content},
\label{eq:loss}

$$

where

$$

\small

$$
\begin{aligned}

&L_\textrm{recon} = \mathbb{E} [\lVert \hat{X}_{1\rightarrow1} - X_1 \rVert_2^2], \\
&L_\textrm{content} = \mathbb{E} [\lVert E_c(\hat{X}_{1\rightarrow1}) - C_1 \rVert_1].

\end{aligned}
$$

\label{eq:loss_detail}

$$

As it turns out, this simple training scheme is sufficient to produce the ideal distribution-matching voice conversion, as will be shown in the next section. %\shiyu{Consider to add "," and "." for all equations.}

### Why does it work?

![](figures/explain1.pdf)

<a id="fig:explain">An intuitive explanation of how \algname works.

The target speaker is the same as the source speaker during training ((a)-(c)), and different during the actual conversion ((d)).

Each speech segment contains two types of information: the speaker information (solid) and content information (striped). (a) When the bottleneck is too wide, the content embedding will contain some source speaker information. (b) When the bottleneck is too narrow, the content information is lost, which leads to imperfect reconstruction. (c) When the bottleneck is just right, perfect reconstruction is achievable, \emph{and} the content embedding contains no source speaker information. (d) During the actual conversion, the output should contain no information about the source speaker, so the conversion quality should be as high as if it were doing self-reconstruction.</a>

We will formally show this autoencoder-based training scheme is able to achieve ideal voice conversion (Eq.~\eqref{eq:ideal}).

The secret recipe is to have a proper information bottleneck.

We will first state the theoretical guarantee and then present an intuitive explanation.

The following theorem characterizes the theoretical guarantee of our proposed framework.

\begin{theorem}

Consider the autoencoder framework depicted in Eqs.~\eqref{eq:convert} and \eqref{eq:training}.

Given the following assumption:

1.

The speaker embedding of different utterances of the same speaker is the same.

Formally, if $U_1 = U_2$,  $E_s(X_1) = E_s(X_2)$.

2.

The speaker embedding of different speakers is different.

Formally, if $U_1 \neq U_2$, $E_s(X_1) \neq E_s(X_2)$.

3. $\{X_1(1:T)\}$ is an ergodic stationary order-$\tau$ Markov process with bounded second moment, \emph{i.e.}

$$

\small

$$
\begin{aligned}

% &p_{X_1(t)}( | X_1(1:t-1), U_1) \\
% = &p_{X_1(t)}( | X_1(t-\tau:t-1), U_1).
p_{X_1(t)}( \cdot | X_1(1:t-1), U_1) = p_{X_1(t)}(\cdot | X_1(t-\tau:t-1), U_1).

\end{aligned}
$$

$$

Further assume $X_1$ has finite cardinality.

4.

Denote $n$ as the dimension of $C_1$.

Then $n = \lfloor n^* + T^{2/3} \rfloor$,

where $n^*$ is the optimal coding length of $p_{X_1}(\cdot | U_1)$\footnote{From the assumpion in Eq.~\eqref{eq:constant_info}, $n^*$ is assumed to be a constant regardless of $U_1$}.

Then the following holds.

For each $T$, there exists a content encoder $E^*_c(\cdot; T)$ and a decoder $D^*(\cdot, \cdot; T)$, \emph{s.t.} $\lim_{T \rightarrow \infty}

L = 0$,

and

$$

\small

$$
\begin{aligned}

\lim_{T \rightarrow \infty} \frac{1}{T}

KL~(p_{\hat{X}_{1\rightarrow 2}}(\cdot | u_2, z_1) || p_X(\cdot | U=u_2, Z=z_1))
= 0,

\end{aligned}
$$

$$

where $KL(\cdot || \cdot)$ denotes the KL-divergence.

\label{thm:guarantee}
\end{theorem}

The conclusion of Thm.~[thm:guarantee](#thm:guarantee) can be interpreted as follows.

If the number of frames $T$ is large enough, and if the bottleneck dimension $n$ is properly set, then the global optimizer of the loss function in Eq.~\eqref{eq:loss} would approximately satisfy the ideal conversion property in Eq.~\eqref{eq:ideal}.

This conclusion is quite strong, because a major justification of applying GAN to style transfer, despite all its hassles, is that it can ideally match the distribution of the true samples from the target domain.

Now Thm.~[thm:guarantee](#thm:guarantee) conveys the following message: to achieve the desired distribution matching, an autoencoder is all you need.  The formal proof of Thm.~[thm:guarantee](#thm:guarantee) will be presented in the appendix.

Here, we will present an intuitive explanation, which is also the gist of our proof.

The basic idea is that the bottleneck dimension of the content encoder needs to be set such that it is just enough to code the speaker independent information.

As shown in Fig.~[fig:explain](#fig:explain), speech contains two types of information: the speaker information (shown as solid color) and the speaker-independent information (shown as striped), which we will refer to as the content information\footnote{The speaker-independent information includes but is not limited to the content information in $Z$, but for convenience, we will refer to the speaker-independent information as content information.}.

Suppose the bottleneck is very wide, as wide as the input speech $X_1$.

The most convenient way to do self-reconstruction is to copy $X_1$ as is to the content embedding $C_1$, and this will guarantee a perfect reconstruction.

However as the dimension of $C_1$ decreases, $C_1$ is forced to lose some information.

Since the autoencoder attempts to achieve perfect reconstruction, it will choose to lose speaker information because the speaker information is already supplied in $S_1$.

In this case, perfect reconstruction is still possible, but the $C_1$ may contain some speaker information, as shown in Fig.~[fig:explain](#fig:explain)(a).

On the other hand, if the bottleneck is very narrow, then the content encoder will be forced to lose so much information that not only the speaker information but also the content information is lost.

In this case, the perfect reconstruction is impossible, as shown in Fig.~[fig:explain](#fig:explain)(b).

Therefore, as shown in Fig.~[fig:explain](#fig:explain)(c), when the dimension of $C_1$ is chosen such that the dimension reduction is just enough to get rid of all the speaker information but no content information is harmed, we have reached our desirable condition, under which two important properties hold:

1.

Perfect reconstruction is achieved.

2.

The content embedding $C_1$ does not contain any information about the source speaker $U_1$, which we refer to as \emph{speaker disentanglement}.

We will now show by contradiction how these two properties imply an ideal conversion.

Suppose when \algname is performing an actual conversion (source and target speakers are different), the quality is low, or does not sound like the target speaker at all.

By property 1, we know that the reconstruction (source and target speakers are the same) quality is high.

However, according to Eq.~\eqref{eq:convert}, the output speech $\hat{X}_{1\rightarrow2}$ can only access $C_1$ and $S_2$, both of which do not contain any information of the source speaker $U_1$.

In other words, from the conversion output, one can never tell if it is produced by self-reconstruction or conversion, as shown in Fig.~[fig:explain](#fig:explain)(d).

If the conversion quality is low, but the reconstruction quality is high, one will be able to distinguish between conversion and reconstruction above chance, which leads to a contradiction.
