# GenVC: Self-Supervised Zero-Shot Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "GenVC: Self-Supervised Zero-Shot Voice Conversion."
- 作者:
  - 01 Zexin Cai
  - 02 Henry Li Xinyuan
  - 03 Ashi Garg
  - 04 Leibny Paola García-Perera
  - 05 Kevin Duh
  - 06 Sanjeev Khudanpur
  - 07 Matthew Wiesner
  - 08 Nicholas Andrews
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.04519v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2502.04519v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.02.06_2502.04519v1_GenVC__Self-Supervised_Zero-Shot_Voice_Conversion.pdf)
  - [ArXiv:2502.04519v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.02.06_2502.04519v2_GenVC__Self-Supervised_Zero-Shot_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

\begin{abstract}
Most current zero-shot voice conversion methods rely on externally supervised components, particularly speaker encoders, for training.
To explore alternatives that eliminate this dependency, this paper introduces GenVC, a novel framework that disentangles speaker identity and linguistic content from speech signals in a self-supervised manner.
GenVC leverages speech tokenizers and an autoregressive, Transformer-based language model as its backbone for speech generation.
This design supports large-scale training while enhancing both source speaker privacy protection and target speaker cloning fidelity.
Experimental results demonstrate that GenVC achieves notably higher speaker similarity, with naturalness on par with leading zero-shot approaches.
Moreover, due to its autoregressive formulation, GenVC introduces flexibility in temporal alignment, reducing the preservation of source prosody and speaker-specific traits, and making it highly effective for voice anonymization.\footnote{Audio samples, code, and model checkpoints are available at \\ \indent\url{https://caizexin.github.io/GenVC/index.html}}

## 1·Introduction

\label{intro}

Zero-shot Voice conversion (VC) seeks to transform a source voice to match an unseen target speaker, with minimal adaptation, while preserving the original linguistic content[^Qian2019A}uto{VC], [^Zhang2020Gazev], [^Zhang2022Sig-Vc].

Progress in this area has closely followed advancements in zero-shot text-to-speech (TTS) synthesis, with recent models achieving remarkable naturalness—generating speech that is perceptually indistinguishable from that of real human speakers[^Sisman2020An].

Despite these gains, significant challenges remain, particularly in cloning novel voices and adapting to diverse recording conditions.

These limitations arise primarily from the difficulty of training robust and scalable models capable of handling such variability.

Furthermore, leading VC approaches operate by converting linguistic features into acoustic representations in a parallel manner.

While these systems effectively modify acoustic traits such as timbre, the parallel conversion process often preserves the source speaker’s temporal and prosodic patterns[^Mary2006Prosodic], [^Li2023FreeVC], [^Li2024The], [^Cao2024NeuralVC].

As a result, the converted speech retains perceptual cues from the original speaker, which diminishes both the naturalness and the similarity of the intended voice transformation[^Cai2023Identifying], [^Cai2024Privacy].

Achieving high-quality zero-shot VC typically requires disentangling speaker identity and linguistic content through two dedicated modules: one that captures vocal characteristics and the other that extracts linguistic content.

To achieve this separation, many existing approaches leverage pre-trained models, such as automatic speech recognition (ASR), automatic speaker verification (ASV), or TTS, which rely on supervised training with labeled datasets[^Tan2021Zero-Shot], [^Casanova2022YourTTS].

Minimizing the level of supervision in training, however, enhances the scalability of VC systems and allows the exploitation of extensive unlabeled speech corpora[^Choi2021Neural], [^Choi2023Nansy++].

One promising strategy for improving zero-shot synthesis involves scaling data to include a broader range of voice types, thereby enhancing generalization to unseen speakers[^Betker2023Better].

Minimally supervised frameworks like NANCY[^Choi2021Neural] demonstrate the feasibility of this direction, but often rely on frame-aligned mapping, which can inadvertently preserve source utterance’s temporal and prosodic structure.

In contrast, autoregressive architectures provide a compelling alternative to model temporal dependencies more faithfully, and produce prosodic patterns that better reflect the target speaker’s style[^Wang2023Lm-Vc].

To address these challenges, we propose GenVC, a generative zero-shot VC system designed with three key objectives: (1) reducing dependency on external supervision through self-supervised learning for disentangling speaker and linguistic features, thereby enhancing scalability; (2) leveraging an autoregressive generation mechanism to better model target speaker style and improve voice similarity; and (3) maintaining controllability by encoding speaker characteristics into a compact high-dimensional space, enabling applications such as voice anonymization.

To achieve these goals, GenVC employs speech tokenization and is built upon a causal Transformer-based architecture.

Experimental results show that GenVC achieves significant improvements in speaker similarity for unseen VC tasks and enhanced privacy preservation in anonymization evaluations, while maintaining naturalness competitive with leading VC methods.

## 2·Related Work

\label{sec:related_works}

VC can be viewed as a subset of speech generation tasks.

The rise of self-supervised learning (SSL) models and language models (LMs), known for their strengths in contextual modeling and scalability in natural language processing, has prompted their adoption in sequential audio tasks.

Concurrently, neural audio tokenizers, also referred to as speech codecs, have enabled the transformation of continuous audio signals into discrete tokens while maintaining high-fidelity reconstruction.

These audio tokenizers are particularly well-suited for integration into LM-based architectures[^Kreuk2023AudioGen], [^Wu2024Towards], paving the way for a new wave of LM-driven speech generation models.

One leading example is AudioLM[^Borsos2023AudioLM], which leverages SoundStream[^Zeghidour2021SoundStream] for tokenization and employs a hierarchical arrangement of three LMs to predict semantic, coarse acoustic, and fine acoustic tokens in sequence.

This approach allows for the generation of natural and coherent audio continuations from short prompts.

These advancements in speech generation have significantly influenced the development of generative zero-shot TTS systems[^Kharitonov2023Speak,], [^Peng2024V}oice{C}raft], [^Du2024CosyVoice], [^Du2025CosyVoice], [^Anastassiou2024Seed-TTS].

Among the earliest examples is VALL-E[^Chen2025Neural], which employs Encodec[^D{\'e}fossez2023High] as the audio tokenizer and incorporates both auto-regressive and non-autoregressive LMs to predict audio tokens from input text or phonemes.

Later iterations of VALL-E extended its capabilities to tasks such as cross-lingual synthesis and more efficient token representation[^Zhang2023Speak], [^Chen2024Vall-E].

Beyond purely LM-based models, hybrid systems have also emerged.

For example, CosyVoice[^Du2024CosyVoice] combines LM-based text-to-token generation with conditional flow-matching models for token-to-speech synthesis.  Another model, MaskGCT[^Wang2025MaskGCT], extends the LM and tokenization paradigm by replacing the auto-regressive mechanism with non-autoregressive, masking-based generation strategies for efficient audio synthesis[^Wang2025MaskGCT].

A persistent challenge in LM-based speech generation lies in the nature of current speech codecs, which typically produce multiple tokens—often eight—per frame.

This parallel token prediction increases modeling complexity and computational cost.

Techniques like delayed token prediction[^Copet2023Simple], [^D{\'e}fossez2024Moshi] have been proposed to address this issue, yet the overhead remains significant.

In contrast, models such as Tortoise-TTS[^Betker2023Better] and XTTS[^Casanova2024Xtts] adopt a simpler approach, discretizing audio into a single token per frame.

This design choice not only streamlines the model architecture but also aligns more effectively with standard LM training paradigms.

Research on LM-based approaches for zero-shot VC remains limited.

Building on the AudioLM framework, Wang et al. proposed LM-VC[^Wang2023Lm-Vc], which incorporates three LMs to model semantic and acoustic features.

While the model demonstrates strong performance in preserving speaker identity and generating natural-sounding speech, its reliance on a multi-component architecture results in slow inference and makes it unsuitable for real-time applications.

To address this, StreamVoice[^Wang2024StreamVoice] introduces a simplified architecture using a single LM to enable streaming zero-shot VC.

However, it depends on a separately trained supervised ASR model to extract semantic content.

Furthermore, the output speech duration is fixed to match the source speech, preserving prosodic features but also leading to speaker information leakage from the source—an issue shared with many traditional VC systems.

A concurrent LM-based approach, Vevo[^Zhang2025Vevo], addresses similar limitations by introducing a controllable framework for timbre and style conversion.

The system comprises two stages: an autoregressive transformer followed by a flow-matching transformer.

Both stages are trained with self-supervised, in-context learning, making the framework scalable.

However, Vevo conditions speaker representation directly on the source spectrogram rather than using a high-dimensional embedding.

While this design is effective for voice imitation, it limits the model’s ability to generate pseudo voices, reducing its suitability for applications like speech anonymization[^Tomashenko2024The].

![](LLMVC_v6.pdf)

<a id="fig:genvc">System architecture and training scheme of GenVC:
Phase 1 involves the Discrete VAEs for audio tokenization.

Phase 2 has a causal Transformer-based language model alongside a Perceiver encoder.

Phase 3 includes a vocoder for waveform reconstruction.</a>
