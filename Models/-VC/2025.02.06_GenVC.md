# GenVC: Self-Supervised Zero-Shot Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "GenVC: Self-Supervised Zero-Shot Voice Conversion."
- 作者:
  - 01 Zexin Cai
  - 02 Henry Li Xinyuan
  - 03 Ashi Garg
  - 04 Leibny Paola García-Perera
  - 05 Kevin Duh
  - 06 Sanjeev Khudanpur
  - 07 Matthew Wiesner
  - 08 Nicholas Andrews
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.04519v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2502.04519v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.02.06_2502.04519v1_GenVC__Self-Supervised_Zero-Shot_Voice_Conversion.pdf)
  - [ArXiv:2502.04519v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.02.06_2502.04519v2_GenVC__Self-Supervised_Zero-Shot_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

\begin{abstract}
Most current zero-shot voice conversion methods rely on externally supervised components, particularly speaker encoders, for training.
To explore alternatives that eliminate this dependency, this paper introduces GenVC, a novel framework that disentangles speaker identity and linguistic content from speech signals in a self-supervised manner.
GenVC leverages speech tokenizers and an autoregressive, Transformer-based language model as its backbone for speech generation.
This design supports large-scale training while enhancing both source speaker privacy protection and target speaker cloning fidelity.
Experimental results demonstrate that GenVC achieves notably higher speaker similarity, with naturalness on par with leading zero-shot approaches.
Moreover, due to its autoregressive formulation, GenVC introduces flexibility in temporal alignment, reducing the preservation of source prosody and speaker-specific traits, and making it highly effective for voice anonymization.\footnote{Audio samples, code, and model checkpoints are available at \\ \indent\url{https://caizexin.github.io/GenVC/index.html}}

## 1·Introduction

\label{intro}

Zero-shot Voice conversion (VC) seeks to transform a source voice to match an unseen target speaker, with minimal adaptation, while preserving the original linguistic content[^Qian2019A}uto{VC], [^Zhang2020Gazev], [^Zhang2022Sig-Vc].

Progress in this area has closely followed advancements in zero-shot text-to-speech (TTS) synthesis, with recent models achieving remarkable naturalness—generating speech that is perceptually indistinguishable from that of real human speakers[^Sisman2020An].

Despite these gains, significant challenges remain, particularly in cloning novel voices and adapting to diverse recording conditions.

These limitations arise primarily from the difficulty of training robust and scalable models capable of handling such variability.

Furthermore, leading VC approaches operate by converting linguistic features into acoustic representations in a parallel manner.

While these systems effectively modify acoustic traits such as timbre, the parallel conversion process often preserves the source speaker’s temporal and prosodic patterns[^Mary2006Prosodic], [^Li2023FreeVC], [^Li2024The], [^Cao2024NeuralVC].

As a result, the converted speech retains perceptual cues from the original speaker, which diminishes both the naturalness and the similarity of the intended voice transformation[^Cai2023Identifying], [^Cai2024Privacy].

Achieving high-quality zero-shot VC typically requires disentangling speaker identity and linguistic content through two dedicated modules: one that captures vocal characteristics and the other that extracts linguistic content.

To achieve this separation, many existing approaches leverage pre-trained models, such as automatic speech recognition (ASR), automatic speaker verification (ASV), or TTS, which rely on supervised training with labeled datasets[^Tan2021Zero-Shot], [^Casanova2022YourTTS].

Minimizing the level of supervision in training, however, enhances the scalability of VC systems and allows the exploitation of extensive unlabeled speech corpora[^Choi2021Neural], [^Choi2023Nansy++].

One promising strategy for improving zero-shot synthesis involves scaling data to include a broader range of voice types, thereby enhancing generalization to unseen speakers[^Betker2023Better].

Minimally supervised frameworks like NANCY[^Choi2021Neural] demonstrate the feasibility of this direction, but often rely on frame-aligned mapping, which can inadvertently preserve source utterance’s temporal and prosodic structure.

In contrast, autoregressive architectures provide a compelling alternative to model temporal dependencies more faithfully, and produce prosodic patterns that better reflect the target speaker’s style[^Wang2023Lm-Vc].

To address these challenges, we propose GenVC, a generative zero-shot VC system designed with three key objectives: (1) reducing dependency on external supervision through self-supervised learning for disentangling speaker and linguistic features, thereby enhancing scalability; (2) leveraging an autoregressive generation mechanism to better model target speaker style and improve voice similarity; and (3) maintaining controllability by encoding speaker characteristics into a compact high-dimensional space, enabling applications such as voice anonymization.

To achieve these goals, GenVC employs speech tokenization and is built upon a causal Transformer-based architecture.

Experimental results show that GenVC achieves significant improvements in speaker similarity for unseen VC tasks and enhanced privacy preservation in anonymization evaluations, while maintaining naturalness competitive with leading VC methods.
