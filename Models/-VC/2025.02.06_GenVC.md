# GenVC: Self-Supervised Zero-Shot Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "GenVC: Self-Supervised Zero-Shot Voice Conversion."
- 作者:
  - 01 Zexin Cai
  - 02 Henry Li Xinyuan
  - 03 Ashi Garg
  - 04 Leibny Paola García-Perera
  - 05 Kevin Duh
  - 06 Sanjeev Khudanpur
  - 07 Matthew Wiesner
  - 08 Nicholas Andrews
- 链接:
  - [ArXiv](https://arxiv.org/abs/2502.04519v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2502.04519v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.02.06_2502.04519v1_GenVC__Self-Supervised_Zero-Shot_Voice_Conversion.pdf)
  - [ArXiv:2502.04519v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.02.06_2502.04519v2_GenVC__Self-Supervised_Zero-Shot_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

\begin{abstract}
Most current zero-shot voice conversion methods rely on externally supervised components, particularly speaker encoders, for training.
To explore alternatives that eliminate this dependency, this paper introduces GenVC, a novel framework that disentangles speaker identity and linguistic content from speech signals in a self-supervised manner.
GenVC leverages speech tokenizers and an autoregressive, Transformer-based language model as its backbone for speech generation.
This design supports large-scale training while enhancing both source speaker privacy protection and target speaker cloning fidelity.
Experimental results demonstrate that GenVC achieves notably higher speaker similarity, with naturalness on par with leading zero-shot approaches.
Moreover, due to its autoregressive formulation, GenVC introduces flexibility in temporal alignment, reducing the preservation of source prosody and speaker-specific traits, and making it highly effective for voice anonymization.\footnote{Audio samples, code, and model checkpoints are available at \\ \indent\url{https://caizexin.github.io/GenVC/index.html}}

## 1·Introduction

\label{intro}

Zero-shot Voice conversion (VC) seeks to transform a source voice to match an unseen target speaker, with minimal adaptation, while preserving the original linguistic content[^Qian2019A}uto{VC], [^Zhang2020Gazev], [^Zhang2022Sig-Vc].

Progress in this area has closely followed advancements in zero-shot text-to-speech (TTS) synthesis, with recent models achieving remarkable naturalness—generating speech that is perceptually indistinguishable from that of real human speakers[^Sisman2020An].

Despite these gains, significant challenges remain, particularly in cloning novel voices and adapting to diverse recording conditions.

These limitations arise primarily from the difficulty of training robust and scalable models capable of handling such variability.

Furthermore, leading VC approaches operate by converting linguistic features into acoustic representations in a parallel manner.

While these systems effectively modify acoustic traits such as timbre, the parallel conversion process often preserves the source speaker’s temporal and prosodic patterns[^Mary2006Prosodic], [^Li2023FreeVC], [^Li2024The], [^Cao2024NeuralVC].

As a result, the converted speech retains perceptual cues from the original speaker, which diminishes both the naturalness and the similarity of the intended voice transformation[^Cai2023Identifying], [^Cai2024Privacy].

Achieving high-quality zero-shot VC typically requires disentangling speaker identity and linguistic content through two dedicated modules: one that captures vocal characteristics and the other that extracts linguistic content.

To achieve this separation, many existing approaches leverage pre-trained models, such as automatic speech recognition (ASR), automatic speaker verification (ASV), or TTS, which rely on supervised training with labeled datasets[^Tan2021Zero-Shot], [^Casanova2022YourTTS].

Minimizing the level of supervision in training, however, enhances the scalability of VC systems and allows the exploitation of extensive unlabeled speech corpora[^Choi2021Neural], [^Choi2023Nansy++].

One promising strategy for improving zero-shot synthesis involves scaling data to include a broader range of voice types, thereby enhancing generalization to unseen speakers[^Betker2023Better].

Minimally supervised frameworks like NANCY[^Choi2021Neural] demonstrate the feasibility of this direction, but often rely on frame-aligned mapping, which can inadvertently preserve source utterance’s temporal and prosodic structure.

In contrast, autoregressive architectures provide a compelling alternative to model temporal dependencies more faithfully, and produce prosodic patterns that better reflect the target speaker’s style[^Wang2023Lm-Vc].

To address these challenges, we propose GenVC, a generative zero-shot VC system designed with three key objectives: (1) reducing dependency on external supervision through self-supervised learning for disentangling speaker and linguistic features, thereby enhancing scalability; (2) leveraging an autoregressive generation mechanism to better model target speaker style and improve voice similarity; and (3) maintaining controllability by encoding speaker characteristics into a compact high-dimensional space, enabling applications such as voice anonymization.

To achieve these goals, GenVC employs speech tokenization and is built upon a causal Transformer-based architecture.

Experimental results show that GenVC achieves significant improvements in speaker similarity for unseen VC tasks and enhanced privacy preservation in anonymization evaluations, while maintaining naturalness competitive with leading VC methods.

## 2·Related Work

\label{sec:related_works}

VC can be viewed as a subset of speech generation tasks.

The rise of self-supervised learning (SSL) models and language models (LMs), known for their strengths in contextual modeling and scalability in natural language processing, has prompted their adoption in sequential audio tasks.

Concurrently, neural audio tokenizers, also referred to as speech codecs, have enabled the transformation of continuous audio signals into discrete tokens while maintaining high-fidelity reconstruction.

These audio tokenizers are particularly well-suited for integration into LM-based architectures[^Kreuk2023AudioGen], [^Wu2024Towards], paving the way for a new wave of LM-driven speech generation models.

One leading example is AudioLM[^Borsos2023AudioLM], which leverages SoundStream[^Zeghidour2021SoundStream] for tokenization and employs a hierarchical arrangement of three LMs to predict semantic, coarse acoustic, and fine acoustic tokens in sequence.

This approach allows for the generation of natural and coherent audio continuations from short prompts.

These advancements in speech generation have significantly influenced the development of generative zero-shot TTS systems[^Kharitonov2023Speak,], [^Peng2024V}oice{C}raft], [^Du2024CosyVoice], [^Du2025CosyVoice], [^Anastassiou2024Seed-TTS].

Among the earliest examples is VALL-E[^Chen2025Neural], which employs Encodec[^D{\'e}fossez2023High] as the audio tokenizer and incorporates both auto-regressive and non-autoregressive LMs to predict audio tokens from input text or phonemes.

Later iterations of VALL-E extended its capabilities to tasks such as cross-lingual synthesis and more efficient token representation[^Zhang2023Speak], [^Chen2024Vall-E].

Beyond purely LM-based models, hybrid systems have also emerged.

For example, CosyVoice[^Du2024CosyVoice] combines LM-based text-to-token generation with conditional flow-matching models for token-to-speech synthesis.  Another model, MaskGCT[^Wang2025MaskGCT], extends the LM and tokenization paradigm by replacing the auto-regressive mechanism with non-autoregressive, masking-based generation strategies for efficient audio synthesis[^Wang2025MaskGCT].

A persistent challenge in LM-based speech generation lies in the nature of current speech codecs, which typically produce multiple tokens—often eight—per frame.

This parallel token prediction increases modeling complexity and computational cost.

Techniques like delayed token prediction[^Copet2023Simple], [^D{\'e}fossez2024Moshi] have been proposed to address this issue, yet the overhead remains significant.

In contrast, models such as Tortoise-TTS[^Betker2023Better] and XTTS[^Casanova2024Xtts] adopt a simpler approach, discretizing audio into a single token per frame.

This design choice not only streamlines the model architecture but also aligns more effectively with standard LM training paradigms.

Research on LM-based approaches for zero-shot VC remains limited.

Building on the AudioLM framework, Wang et al. proposed LM-VC[^Wang2023Lm-Vc], which incorporates three LMs to model semantic and acoustic features.

While the model demonstrates strong performance in preserving speaker identity and generating natural-sounding speech, its reliance on a multi-component architecture results in slow inference and makes it unsuitable for real-time applications.

To address this, StreamVoice[^Wang2024StreamVoice] introduces a simplified architecture using a single LM to enable streaming zero-shot VC.

However, it depends on a separately trained supervised ASR model to extract semantic content.

Furthermore, the output speech duration is fixed to match the source speech, preserving prosodic features but also leading to speaker information leakage from the source—an issue shared with many traditional VC systems.

A concurrent LM-based approach, Vevo[^Zhang2025Vevo], addresses similar limitations by introducing a controllable framework for timbre and style conversion.

The system comprises two stages: an autoregressive transformer followed by a flow-matching transformer.

Both stages are trained with self-supervised, in-context learning, making the framework scalable.

However, Vevo conditions speaker representation directly on the source spectrogram rather than using a high-dimensional embedding.

While this design is effective for voice imitation, it limits the model’s ability to generate pseudo voices, reducing its suitability for applications like speech anonymization[^Tomashenko2024The].

![](LLMVC_v6.pdf)

<a id="fig:genvc">System architecture and training scheme of GenVC:
Phase 1 involves the Discrete VAEs for audio tokenization.

Phase 2 has a causal Transformer-based language model alongside a Perceiver encoder.

Phase 3 includes a vocoder for waveform reconstruction.</a>

## 3·GenVC

Inspired by prior research in speech generation, we present GenVC, a zero-shot generative VC model.

Our approach integrates discrete audio tokenizers with a generative Transformer-based LM for VC, simplifying the complexity of earlier LM-based VC methods.

This design enables the model to be trained entirely in an self-supervised manner, supporting large-scale training.

Furthermore, the autoregressive nature of the model allows for the conversion of source utterances into the voice and style of a target speaker without preserving the prosodic structure of the source utterances.

Figure [fig:genvc](#fig:genvc) presents an overview of GenVC.

At its core, GenVC incorporates a causal Transformer-based LM designed to produce acoustic token sequences conditioned on a fixed-length voice style embedding and a sequence of linguistic tokens.

Two auxiliary components, the speech tokenizer and vocoder, handle the compression, discretization, and reconstruction of audio.

The voice conversion process unfolds in three sequential stages, each building on the last and trained via reconstruction losses, without requiring labeled data or external supervision.

\begin{enumerate}[label=\Roman{*}.]

-  **Tokenization**: Modules trained to convert audio signals into discrete tokens, ensuring compatibility with the LM architecture.

-  **Generative Modeling**: A decoder-only LM that autoregressively generates audio tokens conditioned on acoustic style representations and linguistic tokens.

-  **Vocoding**: A vocoder that reconstructs high-fidelity audio signals from the output of the LM.
\end{enumerate}

\subsection*{Phase 1: Tokenization}
\label{sec:tokenize}

Tokenization is a crucial step in LMs for breaking down inputs into smaller, manageable units.

In the context of VC, the objective is to transform the speaker’s timbre/style of a source utterance while preserving its original linguistic content.

To achieve this, two distinct types of tokens are used in GenVC: 1.

Phonetic Tokens: Capturing the linguistic content of the audio, representing “what” is being said. 2.

Acoustic Tokens: Encoding the acoustic properties of the audio, including timbre, prosody, environmental background, and other stylistic elements.

These tokens are derived from different audio representations.

Studies have shown that SSL speech models can consistently and significantly produce representations enriched with phonetic information[^Choi2024Self-Supervised].

In the context of VC, SSL models serve as ideal feature extractors for capturing the phonetic units of a given speech signal[^Lin2021S2vc], [^Huang2022S3prl-Vc].

For a given input utterance $x \in \mathbb{R}^T$, the phonetic embedding sequence extracted using a pre-trained SSL model is denoted as $\mathbf{O} \in \mathbb{R}^{T_o \times d_o}$, where $T_o$ represents the sequence length, and $d_o$ is the feature dimensionality.

For acoustic representations, spectrograms are commonly used to capture the complex properties of audio signals.

Such features can be extracted and represented as $\mathbf{A} \in \mathbb{R}^{T_a \times d_a}$, where $T_a$ denotes the sequence length, and $d_a$ specifies the dimensionality of the acoustic features.

As shown in Figure [fig:genvc](#fig:genvc), our approach utilizes discrete variational autoencoders (DVAEs)[^Van2017Neural] to compress $\mathbf{O}$ into phonetic tokens $[\mathbf{o}_1, \dots, \mathbf{o}_n], \mathbf{o}_i \in 1,2,\dots,K_p$ and $\mathbf{A}$ into acoustic tokens $[\mathbf{a}_1, \dots, \mathbf{a}_m], \mathbf{a}_i \in 1,2,\dots,K_a$.

Here, $n$ and $m$ denote the lengths of the tokenized sequences derived from the input feature sequences, while $K_o$ and $K_a$ represent the predefined number of discrete codes for the phonetic and acoustic tokenizers, respectively.

The DVAE architecture follows those used in Tortoise-TTS[^Betker2023Better] and XTTS[^Casanova2024Xtts].

\subsection*{Phase 2: Generative Modeling}

The backbone model in Phase 2 is an LM built on a decoder-only transformer architecture[^Vaswani2017Attention].

This phase also incorporates a Perceiver encoder to extract fixed-length style representations from the prompt utterance[^Alayrac2022Flamingo].

As depicted in Figure [fig:genvc](#fig:genvc), the causal LM predicts acoustic tokens by conditioning on both the style prompt derived from the Perceiver encoder and the phonetic tokens extracted from the source audio clip.

Note that the audio clip here refers to a segment of the source utterance.

The Perceiver encoder processes a variable-length sequence of acoustic features extracted from the audio prompt and generates a fixed-dimensional audio style representation[^Shen2024NaturalSpeech].

This is achieved by using a set of learned latent vectors as queries, while the keys and values are formed by concatenating these latent vectors with the acoustic features extracted from the prompt audio.

The Perceiver encoder architecture consists of cross-attention blocks.

The output of the Perceiver encoder $\mathbf{E}_{\text{style}} \in \mathbb{R}^{T_s \times d_\text{model}}$ follows the distribution $P(\mathbf{E}_{\text{style}} \mid \mathbf{A}_\text{prompt}, \mathbf{E}_{\text{latent}}; \theta_\text{Perceiver})$ where $\mathbf{E}_{\text{latent}}$ is the learnable latents that have the same shape as $\mathbf{E}_{\text{style}}$. $T_s$ denotes the fixed number of latent sequence, and $d_\text{model}$ is the dimensionality of the LM.

In this framework, the model learns the probability distribution $P(\mathbf{o}_{s:e}, \mathbf{a}_{s:e}, \mid \mathbf{E}_{\text{style}};\theta_\text{LM})$ by sequentially estimating the conditional distributions:

$$

\label{eq:ploss}
\prod_{t=1}^nP(\mathbf{o}_i \mid \mathbf{o}_{s:i-1},  \mathbf{E}_{\text{style}}; \theta_\text{LLM})

$$

and

$$

\label{eq:aloss}
\prod_{j=1}^mP(\mathbf{a}_j \mid \mathbf{a}_{s:j-1},  \mathbf{o}_{s:e}, \mathbf{E}_{\text{style}}; \theta_\text{LLM}).

$$

Here, the tokens $\mathbf{o}_s$ and $\mathbf{o}_e$ denote the start and end tokens of the phonetic token sequence, respectively, while $\mathbf{a}_s$ and $\mathbf{a}_e$ mark the start and end of the acoustic token sequence.

The training objective employs two linear prediction heads attached to the final hidden layer of the language model to predict phonetic and acoustic tokens.

The overall training loss is designed to maximize the log-likelihood of $P(\mathbf{o}_{s:e}, \mathbf{a}_{s:e}, \mid \mathbf{E}_{\text{style}};\theta_\text{LM})$, as defined in Equation [eq:genloss](#eq:genloss).

This loss combines the phonetic token classification loss and the acoustic token classification loss, weighted to reflect their respective contributions to the task:

$$

\label{eq:genloss}
L_{\text{gen}} = \alpha L_{\text{phonetic}} + \beta L_{\text{acoustic}}

$$

Here, $\alpha$ and $\beta$ denote the weights assigned to the phonetic and acoustic token prediction losses, respectively.

The phonetic token loss, $L_{\text{phonetic}}$ is calculated as the negative log-likelihood of the distribution defined in Expression [eq:ploss](#eq:ploss), while the acoustic token loss, $L_{\text{acoustic}}$ is the negative log-likelihood of the distribution defined in Expression [eq:aloss](#eq:aloss).

Since the primary task of the VC system aligns with that of TTS systems—predicting acoustic tokens—we follow the approach outlined in [^Casanova2024Xtts] and assign $\beta$ a significantly larger value. % than $\alpha$ ($\beta \gg \alpha$).

\subsection*{Phase 3: Vocoding}

Phase 3 focuses on reconstructing the audio waveform from the generative predictions.

Since Phase 2 outputs an acoustic token sequence where each token $\mathbf{a}_i$ is highly compressed due to vector quantization, directly reconstructing audio from these acoustic codes often introduces pronunciation issues and artifacts[^Casanova2024Xtts].

To mitigate these issues, we utilize a HiFiGAN[^Kong2020HiFi-Gan] vocoder conditioned on features $\mathbf{H} \in \mathbb{R}^{m \times d_\text{model}}$ derived from the final hidden layer of the LM.

Unlike systems such as XTTS, which rely on additional speaker embeddings obtained from a separate neural module, we found that conditioning the HiFiGAN vocoder solely on the LM’s final hidden features is sufficient for producing high-quality audio reconstructions.

### Training and Inference

During training, as shown in Figure [fig:genvc](#fig:genvc), both the audio prompt and the audio clip are extracted from the same source utterance using random start points and segment lengths.

Phonetic and acoustic tokens are derived from the audio clip.

Typically, the linguistic content of the audio prompt differs from that of the audio clip.

This setup allows the Perceiver encoder to capture acoustic attributes that are not represented by the linguistic tokens.

Additionally, it facilitates the disentanglement of linguistic content from speaker-specific information in the input audio clip.

As a result, the encoder effectively learns to extract both acoustic characteristics and speaker-specific representations from the audio prompt.

This approach eliminates the need for supervision from an external speaker embedding model, which is a common feature of traditional zero-shot VC methods.

During inference, auxiliary components can be omitted, such as the phonetic tokenizer decoder, the entire acoustic tokenizer, and the discriminator.

The inference process, given a source utterance $**Audio**_{\text{src}}$ and a target utterance $**Audio**_{\text{tgt}}$ specifying the target voice, proceeds as follows:

-  **Style Embedding Extraction**: Treat $**Audio**_{\text{tgt}}$ as the audio prompt.

Acoustic features are extracted from the target audio and processed using the Perceiver encoder to obtain the corresponding conditioned style embeddings.

-  **Phonetic Token Extraction**: Phonetic features are extracted from $**Audio**_{\text{src}}$ using the SSL model.

These features are then converted into phonetic tokens using the phonetic tokenizer.

-  **Acoustic Token Prediction**: The style embeddings and phonetic tokens are fed as input to the backbone LM, which autoregressively predicts the acoustic token sequence until the end token $\mathbf{a}_e$ is generated.

-  **Waveform Reconstruction**: The latent representations from the final hidden layer of the LM are passed to the HiFiGAN vocoder to reconstruct the converted audio waveform.
