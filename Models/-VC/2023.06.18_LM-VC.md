# LM-VC: Zero-Shot Voice Conversion via Speech Generation Based on Language Models

<details>
<summary>基本信息</summary>

- 标题: "LM-VC: Zero-Shot Voice Conversion via Speech Generation Based on Language Models."
- 作者:
  - 01 Zhichao Wang
  - 02 Yuanzhe Chen
  - 03 Lei Xie
  - 04 Qiao Tian
  - 05 Yuping Wang
- 链接:
  - [ArXiv](https://arxiv.org/abs/2306.10521v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2306.10521v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2023.06.18_2306.10521v1_LM-VC__Zero-Shot_Voice_Conversion_via_Speech_Generation_Based_on_Language_Models.pdf)
  - [ArXiv:2306.10521v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2023.06.18_2306.10521v2_LM-VC__Zero-Shot_Voice_Conversion_via_Speech_Generation_Based_on_Language_Models.pdf)
  - [Publication] #TODO

</details>

## Abstract

Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation.
In this paper, we explore the feasibility of LMs for *zero-shot voice conversion*.
An intuitive approach is to follow AudioLM -- Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker.
However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation.
To mitigate these problems, we propose *LM-VC*, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech.
Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling.
This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens.
Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling through shallow fusion.
Finally, a prefix LM reconstructs fine acoustic tokens from the coarse and results in the converted speech.
Experiments demonstrate that LM-VC outperforms competitive systems in speech naturalness and speaker similarity.

## 1·Introduction

\IEEEPARstart{V}{oice} conversion (VC) aims to convert speech from a source speaker to that of a target speaker without changing the linguistic content.

VC's main rationale is to decompose source speech into separated components, including speaker timbre, linguistic content, and speaking style.

Then the linguistic content and speaking style are combined with the target speaker's timbre to generate the converted speech.

Training a typical VC system desires at least a sizable amount of the target speaker's speech.

In contrast, *zero-shot*

VC or *any-to-any*

VC focuses on converting any source speech to that of any desired speaker with only one utterance available from the speaker, which is more practical for real-world applications.

But since only one target speaker utterance is available, decoupling speech components and meanwhile maintaining target speaker timbre becomes more challenging.

One intuitive approach is to leverage a speaker verification (SV) model to extract the speaker representation[^Qian2019AutoVC], [^Qian2020Unsupervised], [^Gu2021MediumVC], while automatic speech recognition (ASR)[^Sun2016Phonetic] or self-supervised learning (SSL) model[^Choi2021Neural], [^Lin2021FragmentVC], [^Yin2021Retriever], [^Maimon2022Speaking] are employed to extract the linguistic content.

Some studies[^Qian2020Unsupervised], [^Choi2021Neural] also use signal perturbation techniques to alter speech utterances to make it speaker irrelevant before content extraction.

Instead of attempting speech disentanglement prior to training the VC model, many studies rely on a specifically designed disentanglement approach to reduce the correlation among different speech components, including designs on complicated neural structures[^Chou2019One-Shot], [^Wang2023Multi-Level], loss functions[^Wang2021Vqmivc], [^Wang2021Adversarially], and training strategies[^Tang2022Avqvc], [^Ebbers2021Contrastive]. 

However, the current zero-shot VC approaches still generalize poorly to unseen speakers with low speaker similarity, mainly due to the inevitable trade-off during the speech disentanglement process and the model's limited capacity on leveraging large-scale speech data.

Recently, language models (LM)[^Borsos2022AudioLM], [^Agostinelli2023MusicLM], [^Wang2023Neural], [^Kharitonov2023Speak,] trained on large-scale datasets have achieved impressive performance in zero-shot audio generation.

A popular paradigm is first to tokenize audio into *semantic* and *acoustic* tokens respectively by a self-supervised learning (SSL) model and a neural codec, where the SSL model extracts the linguistic content from audio while the audio codec reconstructs high-quality audio at very low bitrate, and then the discrete tokens enable the audio generation task to benefit from the powerful large language models.

As a typical approach, AudioLM[^Borsos2022AudioLM] leverages 

semantic and acoustic tokens as audio representations and introduces a three-stage language modeling process for audio generation.

Specifically, using semantic tokens of a short utterance as a *prompt*, AudioLM generates the continuation of semantic tokens, which is then used as a conditioning signal for predicting coarse acoustic tokens and further restoring the fine acoustic details.

Variants of AudioLM have also shown remarkable performance for zero-shot music generation and text-to-speech (TTS)[^Agostinelli2023MusicLM], [^Wang2023Neural], [^Kharitonov2023Speak,].

In this letter, we explore the feasibility of language models in zero-shot VC. 
An intuitive way is to follow AudioLM -- applying coarse and fine acoustic modeling to form a variant.

However, such a straightforward LM approach encounters several issues in voice conversion:
1) the linguistic content contained in semantic tokens may get dispersed as the network deepens during multi-layer language modeling while the lengthy speech input makes contextual learning even harder; 2) the semantic tokens extracted by HuBERT[^Hsu2021HuBERT] still contain speaker-related information, which may be propagated to the converted speech and lead to low speaker similarity; 3) the inherent generation diversity in the sampling of LM inevitably leads to unnatural pronunciation and even speech quality degradation.

To address these issues, we propose a language model-based VC approach (LM-VC) -- a two-stage framework that first generates coarse acoustic tokens for recovering content and speaker timbre and then reconstructs the fine acoustic details as converted speech.

Specifically, to maintain linguistic content and facilitate better speech disentanglement, we use a masked prefixed language model (MPLM) with a mask prediction strategy for coarse acoustic modeling.

This model is encouraged to recover masked semantic tokens based on the context and predict target speech given the target speaker's utterance and the corrupted semantic tokens, thereby implicitly creating an information bottleneck on the source speech to reduce the source speaker information.

To further alleviate the sampling error in the generation process, we integrate an external language model (ELM) that employs window attention[^Liu2021Swin] to better capture the local context among acoustic tokens.

The ELM collaborates with the MPLM through *shallow fusion*[^Gulcehre2015On] to generate target speech.

Finally, the fine acoustic tokens are reconstructed from the coarse ones in a non-autoregressive manner using a prefix LM[^Wang2023Neural].

Experiments and ablations on large-scale speech data show that LM-VC is superior to YourTTS[^Casanova2022YourTTS] and an AudioLM[^Borsos2022AudioLM] baseline in both speaker similarity and speech naturalness.

![](figure/framework_b.jpg)

<a id="fig:lmvc">The architecture of LM-VC. (a) The LM-VC model. (b) The masked prefix language model. (c) The external language model.</a>

\vspace{-10pt}

## 2·Proposed Approach

\vspace{-3pt}

### Overview

\vspace{-5pt}

As shown in Fig.~[fig:lmvc](#fig:lmvc)(a), LM-VC incorporates three LMs: an MPLM, an ELM, and a PLM. 

Before language modeling, HuBERT[^Hsu2021HuBERT] and SoundStream[^Zeghidour2021SoundStream] are used to represent speech as semantic tokens $\mathbf{s} =\{s_1,s_2,...s_{T_s}\}$ and acoustic tokens $\mathbf{a} =\{a^1_1,a^2_1,...,a^L_1,a^1_2,...,a^L_{T_a}\}$, respectively.

Here, $T_s$ and $T_a$ denote the sequence length, and $L$ represents the number of quantizers in SoundStream. 

Similar to AudioLM[^Borsos2022AudioLM], LM-VC sequentially performs coarse and fine acoustic modeling. 

*Coarse acoustic modeling*: The MPLM adopts the semantic tokens $\{\mathbf{s},\mathbf{\tilde{s}}\}$ from the source and target speaker speech, as well as the first-layer acoustic tokens $\tilde{\mathbf{a}}^1$ from target speaker speech.

It autoregressively generates the acoustic tokens $\mathbf{a}^1$ of target speech, following the formulation $p(a^1_t|\tilde{\mathbf{s}},\mathbf{s},\tilde{\mathbf{a}}^1,\mathbf{a}^1_{1:t})$.

In this process, the ELM performs $p(a^1_t|\mathbf{a}^1_{t-w:t})$ with window length $w$ and collaborates with the MPLM to generate speech. 

*Fine acoustic modeling*: Taking the first-layer acoustic tokens as input, the PLM non-autoregressively generates fine acoustic tokens layer by layer.

The semantic and acoustic tokens from the source speech and target speaker are also regarded as the prompt of the PLM.

This process can be formulated as $p(\mathbf{a}^l|\tilde{\mathbf{s}},\mathbf{s},\tilde{\mathbf{a}},\mathbf{a}^{1:l-1},l)$ with $l \in [2, L]$.

Following the NAR model in VALL-E[^Wang2023Neural], the PLM is achieved by a multi-layer Transformer[^Vaswani2017Attention] with bidirectional attention, leading to fast and high-quality speech reconstruction.

Finally, SoundStream reconstructs waveform from the predicted acoustic tokens.

In the two-stage modeling, coarse acoustic modeling plays a crucial role in recovering linguistic content and speaker timbre, while fine acoustic modeling contributes to the acoustic fine details.

In LM-VC, we put more effort on coarse acoustic modeling as keeping the source content and the target speaker timbre is a challenging task in zero-shot VC.

The MPLM and ELM, designed for coarse acoustic modeling, are introduced in the following sections. 

\vspace{-13pt}

### Masked Prefix Language Model

\vspace{-3pt}

As just mentioned, obtaining high speaker similarity and preserving linguistic content are essential goals of the zero-shot VC.

However, accomplishing these goals in LM is challenging since the linguistic content may get lost as the network deepens during multi-layer modeling, and the lengthy speech input makes learning contextual information harder, which causes unnatural pronunciation.

Furthermore, the semantic tokens still contain speaker-related information.

And this inadequate decoupling causes the model to capture speaker timbre from both the target speaker speech and the source speech, thereby leading to low speaker similarity.

Inspired by the advances in language modeling[^Du2022Glm], [^Dong2019Unified], we introduce a masked prefix language model (MPLM) to address this issue.

As in Fig.~[fig:lmvc](#fig:lmvc) (b), MPLM is achieved by a multi-layer Transformer with two types of attention masks.

To enhance the model's ability to learn contextual information and maintain the source content throughout the multi-layer modeling, MPLM employs a mask prediction strategy to restore masked tokens based on the surrounding context.

Specifically, given a sequence of semantic tokens $\mathbf{s}=\{s_1,s_2,...s_{T_s}\}$, we randomly select several tokens as start indices at a ratio $r$, and spans of $l$ steps are masked by *[M]* token.

After masking, MPLM takes the corrupted semantic tokens $\mathbf{s}_{mask}$ as input and recovers the masked tokens.

The right part of Fig.~[fig:lmvc](#fig:lmvc) (b) illustrates the self-attention mask used in MPLM.

For the semantic tokens, a bidirectional attention mask allows them to attend to each other, enabling MPLM to capture contextual information from both directions.

The negative log-likelihood loss, computed over masked tokens, can be defined as:
\vspace{-5pt}

$$

\mathcal{L}_{mask} = -\log{\prod_{t\in M}p_{\mathrm{MPLM}}(s_t|\mathbf{s}_{mask},t)}.
\vspace{-5pt}

$$

For the acoustic generation, we employ the mask prediction strategy to make the model capture speaker timbre exclusively from the target speaker's speech, while extracting content from the corrupted semantic sequences.

This strategy encourages the model to learn better contextual information and implicitly creates an information bottleneck in the semantic tokens to facilitate disentanglement.

Moreover, during training, we do not explicitly use a speech clip as the acoustic prompt.

Instead, MPLM leverages the previous acoustic sequence $\mathbf{a}^1_{1:t-1}$ as acoustic prompts to capture fine-grained speaker information and autoregressively generate $a^1_{t}$.

In this process, we use unidirectional attention to achieve a left-to-right LM objective, where the acoustic token $a^1_t$ only attends to the previous sequence $\mathbf{a}^1_{1:t-1}$ and the semantic prefix $\mathbf{s}_{mask}$.

The loss is
\vspace{-7pt}

$$

\mathcal{L}_{ar} = -\log{\prod^{T_{a}-1}_{t = 0}p_{\mathrm{MPLM}}(a^1_t|\mathbf{a}^1_{1:t-1},\mathbf{s}_{mask},t)},
\vspace{-5pt}

$$

where $T_a$ represents the sequence length of acoustic tokens.

During training, the semantic recovery and acoustic generation are performed simultaneously as $\mathcal{L}_{mask}+\mathcal{L}_{ar}$.

\vspace{-10pt}

### External Language Model

In the generation process of MPLM, the generation diversity inherent in the sampling of the language model sometimes leads to unexpected results.

This issue can be further amplified by autoregressive propagation, resulting in unnatural pronunciation and even speech quality degradation. 

Lack of guidance in the generation process, MPLM is hard to prevent this issue.

Inspired by the phenomenon observed in contrastive predictive coding (CPC), previous studies[^Oord2018Representation], [^Baevski2020Wav2vec] have shown that adjacent speech frames within a speech segment of a specific length share the same local context, such as phoneme-related information.

Such characteristic allows speech frames to be predicted by frames from previous time steps.

As shown in Fig.[fig:lmvc](#fig:lmvc) (c), we introduce an external language model (ELM) to capture the local acoustic relations and provide contextual guidance during the generation process.

With a similar architecture to the MPLM, the ELM employs window attention[^Liu2021Swin] with shifted window to encode local contextual information and predict the distribution $p(a^1_t|\mathbf{a}^1_{t-w:t-1})$ with window length $w$.

The objective of ELM can be defined as:
\vspace{-5pt}

$$

\mathcal{L}_{war} = -\log{\prod^{T_{a}-1}_{t = 0}p_{\mathrm{ELM}}(a^1_t|\mathbf{a}^1_{t-w:t-1},t)}.
\vspace{-7pt}

$$

During training, we separately train the MPLM and ELM.

In inference, the ELM collaborates with the MPLM to generate acoustic tokens conditioned on the local context of the preceding acoustic tokens.

This collaboration is achieved through *shallow fusion*[^Gulcehre2015On] with fusion weight $\lambda$:
\vspace{-4pt}

$$

\begin{split}
a^1_t = argmax_{a^1_t}[\log{p_{\mathrm{MPLM}}(a^1_t|\mathbf{a}^1_{1:t-1},\mathbf{\tilde{a}^1},\mathbf{s},\mathbf{\tilde{s}},t)}\\+\lambda\log{p_{\mathrm{ELM}}(a^1_t|\mathbf{a}^1_{t-w:t-1},t)}].
\end{split}

\vspace{-13pt}

$$

Note that shallow fusion is wildly used in ASR[^Cabrera2021Language] to improve linguistic correctness during acoustic decoding.

\vspace{-10pt}

## 3·Experiments

\label{sec:exp}
\vspace{-5pt}

### Experimental Setup

#### Corpus

A mixed dataset comprising 1,400 hours of LibriTTS[^Zen2019LibriTTS] and an internal dataset are used to train LM-VC and the SoundSteam codec[^Zeghidour2021SoundStream].

To extract semantic tokens, we incorporate an open-source HuBERT\footnote{https://github.com/bshall/hubert}, which is trained on LibriSpeech[^Panayotov2015LibriSpeech].

For zero-shot testing, a set of 500 testing pairs is selected from VCTK[^Veaux2016Cstr], CMU Arctic[^Kominek2004Cmu], and EMIME[^Wester2010Emime], 
each with a source and target speaker utterance. 

% For zero-shot testing, we chose 500 pairs from VCTK, CMU Arctic, and EMIME, each with a source and target speaker utterance.

#### Implement details

The SoundStream codec has 6 quantizer layers with a 1024 codebook size, representing a 24KHz waveform in 12.5ms frame length.

The HuBERT compresses a 16KHz waveform into semantic tokens with 20ms frame length.

For LM-VC, we employ the same decoder-only Transformers for MPLM, ELM, and PLM, with 12 layers, 16 attention heads, embedding dimension of 1024, feed-forward layer dimension of 4096, and dropout of 0.1, as in AudioLM[^Borsos2022AudioLM]. 

During training, the training length is capped at 10s.

MPLM and PLM are trained using 8 A100 80G GPUs with a batch size of 12 per GPU for 600K steps, while ELM has a batch size of 20.

We use the AdamW optimizer with a learning rate of $5 \times 10^{-4}$ for MPLM and ELM and $1 \times 10^{-4}$ for PLM.

Exponential decay updates the learning rate after each epoch, using a decay ratio 0.986.

In MPLM, mask ratio $r$ ranges from $0.02$ to $0.04$, and span $l$ is set to 10.

Window length $w$ of the ELM is set to 20, whose temporal granularity is 250ms.

And the fusion weight $\lambda$ is set to 0.3.

#### Comparison systems

Two representative VC systems are compared.

We first implement a variant of AudioLM[^Borsos2022AudioLM] for VC (AuidoLM-VC), which uses semantic and first-layer acoustic tokens for coarse acoustic modeling.

For a fair comparison, AudioLM-VC and LM-VC use the same PLM for fine acoustic modeling and both are trained on the same dataset.

We also include a recent state-of-the-art VC system YourTTS[^Casanova2022YourTTS] with an open-source checkpoint as another comparison system.

#### Evaluation metrics

The mean opinion score (MOS) subjectively measures speech naturalness (NMOS) and speaker similarity (SMOS).

We randomly select 120 testing pairs for subjective evaluations, involving a group of 15 listeners.

For objective evaluations, a neural network-based system[^Shu2022Non-Intrusive] is used to measure speech quality (P-QMOS).

Word error rate (WER) measured by an ASR model\footnote{https://github.com/wenet-e2e/wenet/tree/main/examples/librispeech/s0} indicates the speech intelligibility.

Following previous work[^Lin2021FragmentVC], speaker accuracy (ACC) is calculated by an SV model[^Desplanques2020Ecapa-TDNN] to determine if the converted speech matches the target speaker.

Converted samples can be found in \href{https://kerwinchao.github.io/lmvc}{\url{https://kerwinchao.github.io/lmvc}}.

\vspace{-14pt}

### Experimental Results

\vspace{-2pt}

##