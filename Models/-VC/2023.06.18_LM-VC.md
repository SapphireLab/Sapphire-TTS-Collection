# LM-VC: Zero-Shot Voice Conversion via Speech Generation Based on Language Models

<details>
<summary>基本信息</summary>

- 标题: "LM-VC: Zero-Shot Voice Conversion via Speech Generation Based on Language Models."
- 作者:
  - 01 Zhichao Wang
  - 02 Yuanzhe Chen
  - 03 Lei Xie
  - 04 Qiao Tian
  - 05 Yuping Wang
- 链接:
  - [ArXiv](https://arxiv.org/abs/2306.10521v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2306.10521v1](_PDF/2023.06.18_2306.10521v1__LM-VC__Zero-Shot_Voice_Conversion_via_Speech_Generation_Based_on_Language_Models.pdf)
  - [ArXiv:2306.10521v2](_PDF/2023.06.18_2306.10521v2__LM-VC__Zero-Shot_Voice_Conversion_via_Speech_Generation_Based_on_Language_Models.pdf)
  - [Publication](_PDF/2023.06.17_2306.10521v0__LM-VC__Zero-Shot_Voice_Conversion_via_Speech_Generation_Based_on_Language_Models.pdf)

</details>

## Abstract

Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation.
In this paper, we explore the feasibility of LMs for *zero-shot voice conversion*.
An intuitive approach is to follow AudioLM -- Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker.
However, such an approach encounters several issues:
1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder;
2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity;
3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation.

To mitigate these problems, we propose *LM-VC*, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech.
Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling.
This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens.
Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling through shallow fusion.
Finally, a prefix LM reconstructs fine acoustic tokens from the coarse and results in the converted speech.
Experiments demonstrate that LM-VC outperforms competitive systems in speech naturalness and speaker similarity.

## 1·Introduction

Voice conversion (VC) aims to convert speech from a source speaker to that of a target speaker without changing the linguistic content.

VC's main rationale is to decompose source speech into separated components, including speaker timbre, linguistic content, and speaking style.

Then the linguistic content and speaking style are combined with the target speaker's timbre to generate the converted speech.

Training a typical VC system desires at least a sizable amount of the target speaker's speech.

In contrast, *zero-shot* VC or *any-to-any* VC focuses on converting any source speech to that of any desired speaker with only one utterance available from the speaker, which is more practical for real-world applications.

But since only one target speaker utterance is available, decoupling speech components and meanwhile maintaining target speaker timbre becomes more challenging.

One intuitive approach is to leverage a speaker verification (SV) model to extract the speaker representation[^Qian2019AutoVC], [^Qian2020Unsupervised], [^Gu2021MediumVC], while automatic speech recognition (ASR)[^Sun2016Phonetic] or self-supervised learning (SSL) model[^Choi2021Neural], [^Lin2021FragmentVC], [^Yin2021Retriever], [^Maimon2022Speaking] are employed to extract the linguistic content.

Some studies[^Qian2020Unsupervised], [^Choi2021Neural] also use signal perturbation techniques to alter speech utterances to make it speaker irrelevant before content extraction.

Instead of attempting speech disentanglement prior to training the VC model, many studies rely on a specifically designed disentanglement approach to reduce the correlation among different speech components, including designs on complicated neural structures[^Chou2019One-Shot], [^Wang2023Multi-Level], loss functions[^Wang2021Vqmivc], [^Wang2021Adversarially], and training strategies[^Tang2022Avqvc], [^Ebbers2021Contrastive].

However, the current zero-shot VC approaches still generalize poorly to unseen speakers with low speaker similarity, mainly due to the inevitable trade-off during the speech disentanglement process and the model's limited capacity on leveraging large-scale speech data.

Recently, language models (LM)[^Borsos2022AudioLM], [^Agostinelli2023MusicLM], [^Wang2023Neural], [^Kharitonov2023Speak,] trained on large-scale datasets have achieved impressive performance in zero-shot audio generation.

A popular paradigm is first to tokenize audio into *semantic* and *acoustic* tokens respectively by a self-supervised learning (SSL) model and a neural codec, where the SSL model extracts the linguistic content from audio while the audio codec reconstructs high-quality audio at very low bitrate, and then the discrete tokens enable the audio generation task to benefit from the powerful large language models.

As a typical approach, AudioLM[^Borsos2022AudioLM] leverages

semantic and acoustic tokens as audio representations and introduces a three-stage language modeling process for audio generation.

Specifically, using semantic tokens of a short utterance as a *prompt*, AudioLM generates the continuation of semantic tokens, which is then used as a conditioning signal for predicting coarse acoustic tokens and further restoring the fine acoustic details.

Variants of AudioLM have also shown remarkable performance for zero-shot music generation and text-to-speech (TTS)[^Agostinelli2023MusicLM], [^Wang2023Neural], [^Kharitonov2023Speak,].

In this letter, we explore the feasibility of language models in zero-shot VC.
An intuitive way is to follow AudioLM -- applying coarse and fine acoustic modeling to form a variant.

However, such a straightforward LM approach encounters several issues in voice conversion:
1) the linguistic content contained in semantic tokens may get dispersed as the network deepens during multi-layer language modeling while the lengthy speech input makes contextual learning even harder;
2) the semantic tokens extracted by HuBERT[^Hsu2021HuBERT] still contain speaker-related information, which may be propagated to the converted speech and lead to low speaker similarity;
3) the inherent generation diversity in the sampling of LM inevitably leads to unnatural pronunciation and even speech quality degradation.

To address these issues, we propose a language model-based VC approach (LM-VC) -- a two-stage framework that first generates coarse acoustic tokens for recovering content and speaker timbre and then reconstructs the fine acoustic details as converted speech.

Specifically, to maintain linguistic content and facilitate better speech disentanglement, we use a masked prefixed language model (MPLM) with a mask prediction strategy for coarse acoustic modeling.

This model is encouraged to recover masked semantic tokens based on the context and predict target speech given the target speaker's utterance and the corrupted semantic tokens, thereby implicitly creating an information bottleneck on the source speech to reduce the source speaker information.

To further alleviate the sampling error in the generation process, we integrate an external language model (ELM) that employs window attention[^Liu2021Swin] to better capture the local context among acoustic tokens.

The ELM collaborates with the MPLM through *shallow fusion*[^Gulcehre2015On] to generate target speech.

Finally, the fine acoustic tokens are reconstructed from the coarse ones in a non-autoregressive manner using a prefix LM[^Wang2023Neural].

Experiments and ablations on large-scale speech data show that LM-VC is superior to YourTTS[^Casanova2022YourTTS] and an AudioLM[^Borsos2022AudioLM] baseline in both speaker similarity and speech naturalness.

![](figure/framework_b.jpg)

<a id="fig:lmvc">The architecture of LM-VC. (a) The LM-VC model. (b) The masked prefix language model. (c) The external language model.</a>

## 2·Proposed Approach

### Overview

As shown in Fig.~[fig:lmvc](#fig:lmvc)(a), LM-VC incorporates three LMs: an MPLM, an ELM, and a PLM.

Before language modeling, HuBERT[^Hsu2021HuBERT] and SoundStream[^Zeghidour2021SoundStream] are used to represent speech as semantic tokens $\mathbf{s} =\{s_1,s_2,...s_{T_s}\}$ and acoustic tokens $\mathbf{a} =\{a^1_1,a^2_1,...,a^L_1,a^1_2,...,a^L_{T_a}\}$, respectively.

Here, $T_s$ and $T_a$ denote the sequence length, and $L$ represents the number of quantizers in SoundStream.

Similar to AudioLM[^Borsos2022AudioLM], LM-VC sequentially performs coarse and fine acoustic modeling.

*Coarse acoustic modeling*: The MPLM adopts the semantic tokens $\{\mathbf{s},\mathbf{\tilde{s}}\}$ from the source and target speaker speech, as well as the first-layer acoustic tokens $\tilde{\mathbf{a}}^1$ from target speaker speech.

It autoregressively generates the acoustic tokens $\mathbf{a}^1$ of target speech, following the formulation $p(a^1_t|\tilde{\mathbf{s}},\mathbf{s},\tilde{\mathbf{a}}^1,\mathbf{a}^1_{1:t})$.

In this process, the ELM performs $p(a^1_t|\mathbf{a}^1_{t-w:t})$ with window length $w$ and collaborates with the MPLM to generate speech.

*Fine acoustic modeling*: Taking the first-layer acoustic tokens as input, the PLM non-autoregressively generates fine acoustic tokens layer by layer.

The semantic and acoustic tokens from the source speech and target speaker are also regarded as the prompt of the PLM.

This process can be formulated as $p(\mathbf{a}^l|\tilde{\mathbf{s}},\mathbf{s},\tilde{\mathbf{a}},\mathbf{a}^{1:l-1},l)$ with $l \in [2, L]$.

Following the NAR model in VALL-E[^Wang2023Neural], the PLM is achieved by a multi-layer Transformer[^Vaswani2017Attention] with bidirectional attention, leading to fast and high-quality speech reconstruction.

Finally, SoundStream reconstructs waveform from the predicted acoustic tokens.

In the two-stage modeling, coarse acoustic modeling plays a crucial role in recovering linguistic content and speaker timbre, while fine acoustic modeling contributes to the acoustic fine details.

In LM-VC, we put more effort on coarse acoustic modeling as keeping the source content and the target speaker timbre is a challenging task in zero-shot VC.

The MPLM and ELM, designed for coarse acoustic modeling, are introduced in the following sections.

### Masked Prefix Language Model

As just mentioned, obtaining high speaker similarity and preserving linguistic content are essential goals of the zero-shot VC.

However, accomplishing these goals in LM is challenging since the linguistic content may get lost as the network deepens during multi-layer modeling, and the lengthy speech input makes learning contextual information harder, which causes unnatural pronunciation.

Furthermore, the semantic tokens still contain speaker-related information.

And this inadequate decoupling causes the model to capture speaker timbre from both the target speaker speech and the source speech, thereby leading to low speaker similarity.

Inspired by the advances in language modeling[^Du2022Glm], [^Dong2019Unified], we introduce a masked prefix language model (MPLM) to address this issue.

As in Fig.~[fig:lmvc](#fig:lmvc) (b), MPLM is achieved by a multi-layer Transformer with two types of attention masks.

To enhance the model's ability to learn contextual information and maintain the source content throughout the multi-layer modeling, MPLM employs a mask prediction strategy to restore masked tokens based on the surrounding context.

Specifically, given a sequence of semantic tokens $\mathbf{s}=\{s_1,s_2,...s_{T_s}\}$, we randomly select several tokens as start indices at a ratio $r$, and spans of $l$ steps are masked by *[M]* token.

After masking, MPLM takes the corrupted semantic tokens $\mathbf{s}_{mask}$ as input and recovers the masked tokens.

The right part of Fig.~[fig:lmvc](#fig:lmvc) (b) illustrates the self-attention mask used in MPLM.

For the semantic tokens, a bidirectional attention mask allows them to attend to each other, enabling MPLM to capture contextual information from both directions.

The negative log-likelihood loss, computed over masked tokens, can be defined as:

$$
\mathcal{L}_{mask} = -\log{\prod_{t\in M}p_{\mathrm{MPLM}}(s_t|\mathbf{s}_{mask},t)}.
\vspace{-5pt}
$$

For the acoustic generation, we employ the mask prediction strategy to make the model capture speaker timbre exclusively from the target speaker's speech, while extracting content from the corrupted semantic sequences.

This strategy encourages the model to learn better contextual information and implicitly creates an information bottleneck in the semantic tokens to facilitate disentanglement.

Moreover, during training, we do not explicitly use a speech clip as the acoustic prompt.

Instead, MPLM leverages the previous acoustic sequence $\mathbf{a}^1_{1:t-1}$ as acoustic prompts to capture fine-grained speaker information and autoregressively generate $a^1_{t}$.

In this process, we use unidirectional attention to achieve a left-to-right LM objective, where the acoustic token $a^1_t$ only attends to the previous sequence $\mathbf{a}^1_{1:t-1}$ and the semantic prefix $\mathbf{s}_{mask}$.

The loss is

$$
\mathcal{L}_{ar} = -\log{\prod^{T_{a}-1}_{t = 0}p_{\mathrm{MPLM}}(a^1_t|\mathbf{a}^1_{1:t-1},\mathbf{s}_{mask},t)},
\vspace{-5pt}
$$

where $T_a$ represents the sequence length of acoustic tokens.

During training, the semantic recovery and acoustic generation are performed simultaneously as $\mathcal{L}_{mask}+\mathcal{L}_{ar}$.

### External Language Model

In the generation process of MPLM, the generation diversity inherent in the sampling of the language model sometimes leads to unexpected results.

This issue can be further amplified by autoregressive propagation, resulting in unnatural pronunciation and even speech quality degradation.

Lack of guidance in the generation process, MPLM is hard to prevent this issue.

Inspired by the phenomenon observed in contrastive predictive coding (CPC), previous studies[^Oord2018Representation], [^Baevski2020Wav2vec] have shown that adjacent speech frames within a speech segment of a specific length share the same local context, such as phoneme-related information.

Such characteristic allows speech frames to be predicted by frames from previous time steps.

As shown in Fig.[fig:lmvc](#fig:lmvc) (c), we introduce an external language model (ELM) to capture the local acoustic relations and provide contextual guidance during the generation process.

With a similar architecture to the MPLM, the ELM employs window attention[^Liu2021Swin] with shifted window to encode local contextual information and predict the distribution $p(a^1_t|\mathbf{a}^1_{t-w:t-1})$ with window length $w$.

The objective of ELM can be defined as:

$$
\mathcal{L}_{war} = -\log{\prod^{T_{a}-1}_{t = 0}p_{\mathrm{ELM}}(a^1_t|\mathbf{a}^1_{t-w:t-1},t)}.
\vspace{-7pt}
$$

During training, we separately train the MPLM and ELM.

In inference, the ELM collaborates with the MPLM to generate acoustic tokens conditioned on the local context of the preceding acoustic tokens.

This collaboration is achieved through *shallow fusion*[^Gulcehre2015On] with fusion weight $\lambda$:
\vspace{-4pt}

$$
\begin{split}
a^1_t = argmax_{a^1_t}[\log{p_{\mathrm{MPLM}}(a^1_t|\mathbf{a}^1_{1:t-1},\mathbf{\tilde{a}^1},\mathbf{s},\mathbf{\tilde{s}},t)}\\+\lambda\log{p_{\mathrm{ELM}}(a^1_t|\mathbf{a}^1_{t-w:t-1},t)}].
\end{split}
$$

Note that shallow fusion is wildly used in ASR[^Cabrera2021Language] to improve linguistic correctness during acoustic decoding.

## 3·Experiments

### Experimental Setup

#### Corpus

A mixed dataset comprising 1,400 hours of LibriTTS[^Zen2019LibriTTS] and an internal dataset are used to train LM-VC and the SoundSteam codec[^Zeghidour2021SoundStream].

To extract semantic tokens, we incorporate an open-source HuBERT\footnote{https://github.com/bshall/hubert}, which is trained on LibriSpeech[^Panayotov2015LibriSpeech].

For zero-shot testing, a set of 500 testing pairs is selected from VCTK[^Veaux2016Cstr], CMU Arctic[^Kominek2004Cmu], and EMIME[^Wester2010Emime],
each with a source and target speaker utterance.

#### Implement details

The SoundStream codec has 6 quantizer layers with a 1024 codebook size, representing a 24KHz waveform in 12.5ms frame length.

The HuBERT compresses a 16KHz waveform into semantic tokens with 20ms frame length.

For LM-VC, we employ the same decoder-only Transformers for MPLM, ELM, and PLM, with 12 layers, 16 attention heads, embedding dimension of 1024, feed-forward layer dimension of 4096, and dropout of 0.1, as in AudioLM[^Borsos2022AudioLM].

During training, the training length is capped at 10s.

MPLM and PLM are trained using 8 A100 80G GPUs with a batch size of 12 per GPU for 600K steps, while ELM has a batch size of 20.

We use the AdamW optimizer with a learning rate of $5 \times 10^{-4}$ for MPLM and ELM and $1 \times 10^{-4}$ for PLM.

Exponential decay updates the learning rate after each epoch, using a decay ratio 0.986.

In MPLM, mask ratio $r$ ranges from $0.02$ to $0.04$, and span $l$ is set to 10.

Window length $w$ of the ELM is set to 20, whose temporal granularity is 250ms.

And the fusion weight $\lambda$ is set to 0.3.

#### Comparison systems

Two representative VC systems are compared.

We first implement a variant of AudioLM[^Borsos2022AudioLM] for VC (AuidoLM-VC), which uses semantic and first-layer acoustic tokens for coarse acoustic modeling.

For a fair comparison, AudioLM-VC and LM-VC use the same PLM for fine acoustic modeling and both are trained on the same dataset.

We also include a recent state-of-the-art VC system YourTTS[^Casanova2022YourTTS] with an open-source checkpoint as another comparison system.

#### Evaluation metrics

The mean opinion score (MOS) subjectively measures speech naturalness (NMOS) and speaker similarity (SMOS).

We randomly select 120 testing pairs for subjective evaluations, involving a group of 15 listeners.

For objective evaluations, a neural network-based system[^Shu2022Non-Intrusive] is used to measure speech quality (P-QMOS).

Word error rate (WER) measured by an ASR model\footnote{https://github.com/wenet-e2e/wenet/tree/main/examples/librispeech/s0} indicates the speech intelligibility.

Following previous work[^Lin2021FragmentVC], speaker accuracy (ACC) is calculated by an SV model[^Desplanques2020Ecapa-TDNN] to determine if the converted speech matches the target speaker.

Converted samples can be found in \href{https://kerwinchao.github.io/lmvc}{\url{https://kerwinchao.github.io/lmvc}}.

### Experimental Results

#### Subjective and objective results

As presented in Table~[exp:mos&obj](#exp:mos&obj), compared with AudioLM-VC, the proposed model LM-VC achieves superior results for speech naturalness while getting higher P-QMOS and better WER.

This indicates that the proposed model effectively preserves the linguistic content of source speech and produces natural speech.

From the SMOS results, it can be found that LM-VC is effective in capturing the target speaker's timbre in the zero-shot VC task.

Similar results are observed in terms of ACC.

Additionally, LM-VC makes an obvious improvement compared to YourTTS regarding speech naturalness and speaker similarity.

The P-QMOS and ACC also indicate the superiority of LM-VC.

Further assessment of LM-VC is conducted with ablations on the MPLM and ELM, as shown at the bottom of Table~[exp:mos&obj](#exp:mos&obj).

Specifically, we replace the MPLM with the autoregressive prefix LM, as in AudioLM[^Borsos2022AudioLM], forming the model *w/o MPLM*.

We observe a noticeable decrease in all evaluation metrics when the MPLM is discarded.

This indicates that the MPLM, trained with mask prediction behavior, effectively enhances the zero-shot performance in capturing target speaker timbre while maintaining the source linguistic content.

Furthermore, excluding the ELM from the inference process in the model *w/o ELM* leads to a performance decrease in both NMOS and SMOS, highlighting the beneficial role of ELM in zero-shot VC.

The objective metrics also report similar results.

<a id="exp:mos&obj">Results of subjective and objective evaluations.

NMOS and SMOS are calculated with 95$\%$ confidence intervals</a>

![](figure/1.pdf)

<a id="fig:windows">Validation of ELM under different window lengths and fusion weights</a>

![](figure/duration.pdf)

<a id="fig:duration">Zero-shot performance under the different duration of speaker prompt</a>

#### Validation of ELM

To examine the efficacy of ELM in LM-VC, we implement multiple ELM with varying window lengths ($w=10,20,30,40,50$) and fusion weights ($\lambda=0.1,0.3,0.5,0.8,1$).

The objective results in Fig.~[fig:windows](#fig:windows) show that LM-VC initially improves across all three aspects as the window length increases, but subsequently begins to decline.

When considering different fusion weights, we observe subtle variations in the trend.

Additionally, LM-VC achieves optimal performance under distinct fusion weights.

Notably, the setup of ($w=20, \lambda=0.3$), which covers
the common range of consonant-vowel syllables[^Steinschneider2013Representation], exhibits the best performance.

#### Varying duration

We further evaluate the zero-shot performance using different duration of target speaker utterances (1 $\sim$ 6s).

As depicted in Fig.~[fig:duration](#fig:duration), the results are generally affected by the duration of the speaker speech.

For the extreme cases, e.g., 1 $\sim$ 2s, the model exhibits poor performance in WER and ACC due to insufficient prompt information.

But from 3 to 4s, the objective scores noticeably improve in terms of speech intelligibility and speaker accuracy.

Beyond 5s, the P-QMOS and WER get worse.

This can be attributed to the long prompt (9 $\sim$ 12s) composed of the source and speaker speech.

During speech generation of LM-VC, the input length can reach 1170$\sim$1569 tokens, which is close to or exceeds the maximum training length (10s, 1300 tokens).

With bigger GPU memory or specific-designed structures[^Dao2023FlashAttention-2], [^Yu2023Megabyte], this problem may be alleviated with the increased training length.

Meanwhile, leveraging longer prompt more effectively[^Das2021Case-Based] is also a promising solution to break such length restriction.

## 4·Conclusions

In this letter, we propose an LM-based zero-shot VC.

Specifically, *LM-VC* adopts a two-stage framework.

For coarse acoustic modeling, an MPLM is adopted in a mask prediction manner to enhance context learning and facilitate better disentanglement.

Additionally, an ELM collaborates with MPLM to generate the target speech while ensuring speech generation stability.

Finally, a non-autoregressive PLM reconstructs the fine acoustic tokens from the coarse acoustic tokens.

Experiments demonstrate the superiority of LM-VC.

The proposed LM-VC can easily **clone** desired speaker timbre with only 3 seconds of speech, simplifying any-to-any VC for applications like voice assistants, dubbing, and other speech generation scenarios.

But it may cause potential risks in misuse, such as generating fake audio impersonating a specific speaker[^Yamagishi2021ASVspoof].

We have to point out that the current zero-shot VC approaches mainly consider the accurate delivery of the speaker timbre while lacking the modeling of other important speaker characteristics, such as accent and prosody, which are critical for identifying specific speakers.

Besides, the out-of-domain problem still exists.

Even trained with 60k hours of speech, LM-VC cannot ensure high speaker similarity for utterances with accents, strong emotions, or unseen recording environments, a similar limitation also pointed out in VALL-E[^Wang2023Neural].

## References

[^Qian2019AutoVC]: AutoVC: Zero-Shot Voice Style Transfer With Only Autoencoder Loss. International Conference on Machine Learning (ICML) 2019.
[^Qian2020Unsupervised]: Unsupervised Speech Decomposition via Triple Information Bottleneck. International Conference on Machine Learning (ICML) 2020.
[^Gu2021MediumVC]: MediumVC: Any-to-Any Voice Conversion Using Synthetic Specific-Speaker Speeches as Intermedium Features. Arxiv.
[^Sun2016Phonetic]: Phonetic Posteriorgrams for Many-to-One Voice Conversion Without Parallel Data Training. International Conference on Multimedia and Expo (ICME) 2016.
[^Choi2021Neural]: Neural Analysis and Synthesis: Reconstructing Speech From Self-Supervised Representations. Neural Information Processing Systems(NeurIPS) 2021.
[^Lin2021FragmentVC]: FragmentVC: Any-to-Any Voice Conversion by End-to-End Extracting and Fusing Fine-Grained Voice Fragments With Attention. International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2021.
[^Yin2021Retriever]: Retriever: Learning Content-Style Representation as a Token-Level Bipartite Graph. International Conference on Learning Representations (ICLR) 2021.
[^Maimon2022Speaking]: Speaking Style Conversion With Discrete Self-Supervised Units. Arxiv.
[^Chou2019One-Shot]: One-Shot Voice Conversion by Separating Speaker and Content Representations With Instance Normalization. International Speech Communication Association (Interspeech) 2019.
[^Wang2023Multi-Level]: Multi-Level Temporal-Channel Speaker Retrieval for Robust Zero-Shot Voice Conversion. Arxiv.
[^Wang2021Vqmivc]: VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-Shot Voice Conversion. International Speech Communication Association (Interspeech) 2021.
[^Wang2021Adversarially]: Adversarially Learning Disentangled Speech Representations for Robust Multi-Factor Voice Conversion. Arxiv.
[^Tang2022Avqvc]: AVQVC: One-Shot Voice Conversion by Vector Quantization With Applying Contrastive Learning. Conference on Acoustics, Speech and Signal Processing (ICASSP) 2022.
[^Ebbers2021Contrastive]: Contrastive Predictive Coding Supported Factorized Variational Autoencoder for Unsupervised Learning of Disentangled Speech Representations. International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2021.
[^Borsos2022AudioLM]: AudioLM: A Language Modeling Approach to Audio Generation. Arxiv.
[^Agostinelli2023MusicLM]: MusicLM: Generating Music From Text. Arxiv.
[^Wang2023Neural]: Neural Codec Language Models Are Zero-Shot Text to Speech Synthesizers. Arxiv.
[^Kharitonov2023Speak,]: Speak, Read and Prompt: High-Fidelity Text-to-Speech With Minimal Supervision. ArXiv.
[^Hsu2021HuBERT]: HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units. Transactions on Audio, Speech, and Language Processing 2021.
[^Liu2021Swin]: Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows. International Conference on Computer Vision (ICCV) 2021.
[^Gulcehre2015On]: On Using Monolingual Corpora in Neural Machine Translation. Arxiv.
[^Casanova2022YourTTS]: YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for Everyone. International Conference on Machine Learning (ICML) 2022.
[^Zeghidour2021SoundStream]: SoundStream: An End-to-End Neural Audio Codec. Transactions on Audio, Speech, and Language Processing 2021.
[^Vaswani2017Attention]: Attention Is All You Need. Neural Information Processing Systems (NerurIPS) 2017.
[^Du2022Glm]: GLM: General Language Model Pretraining With Autoregressive Blank Infilling. Association for Computational Linguistics (ACL) 2022.
[^Dong2019Unified]: Unified Language Model Pre-Training for Natural Language Understanding and Generation. Neural Information Processing Systems (NerurIPS) 2019.
[^Oord2018Representation]: Representation Learning With Contrastive Predictive Coding. Arxiv.
[^Baevski2020Wav2vec]: Wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations. Neural Information Processing Systems (NerurIPS) 2020.
[^Cabrera2021Language]: Language Model Fusion for Streaming End to End Speech Recognition. Arxiv.
[^Zen2019LibriTTS]: LibriTTS: A Corpus Derived From LibriSpeech for Text-to-Speech. International Speech Communication Association (Interspeech) 2019.
[^Panayotov2015LibriSpeech]: LibriSpeech: An ASR Corpus Based on Public Domain Audio Books. International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2015.
[^Veaux2016Cstr]: CSTR VCTK Corpus: English Multi-Speaker Corpus for CSTR Voice Cloning Toolkit.
[^Kominek2004Cmu]: The CMU Arctic Speech Databases. 5th ISCA Workshop on Speech Synthesis (SSW 5) 2004.
[^Wester2010Emime]: The EMIME Bilingual Database.
[^Shu2022Non-Intrusive]: Non-Intrusive Speech Quality Assessment With a Multi-Task Learning Based Subband Adaptive Attention Temporal Convolutional Neural Network. International Speech Communication Association (Interspeech) 2022.
[^Desplanques2020Ecapa-Tdnn]: ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification. International Speech Communication Association (Interspeech) 2020.
[^Steinschneider2013Representation]: Representation of Speech in Human Auditory Cortex: Is It Special?. Hearing Research 2013.
[^Dao2023FlashAttention-2]: FlashAttention-2: Faster Attention With Better Parallelism and Work Partitioning. Arxiv.
[^Yu2023Megabyte]: MEGABYTE: Predicting Million-Byte Sequences With Multiscale Transformers. Arxiv.
[^Das2021Case-Based]: Case-Based Reasoning for Natural Language Queries Over Knowledge Bases. Association for Computational Linguistics (ACL) 2021.
[^Yamagishi2021ASVspoof]: ASVspoof 2021: Accelerating Progress in Spoofed and Deepfake Speech Detection. Arxiv.