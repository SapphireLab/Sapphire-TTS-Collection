# REF-VC: Robust, Expressive and Fast Zero-Shot Voice Conversion With Diffusion Transformers

<details>
<summary>基本信息</summary>

- 标题: "REF-VC: Robust, Expressive and Fast Zero-Shot Voice Conversion With Diffusion Transformers."
- 作者:
  - 01 Yuepeng Jiang
  - 02 Ziqian Ning
  - 03 Shuai Wang
  - 04 Chengjia Wang
  - 05 Mengxiao Bi
  - 06 Pengcheng Zhu
  - 07 Zhonghua Fu
  - 08 Lei Xie
- 链接:
  - [ArXiv](https://arxiv.org/abs/2508.04996v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2508.04996v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.08.07_2508.04996v1_REF-VC__Robust,_Expressive_and_Fast_Zero-Shot_Voice_Conversion_With_Diffusion_Transformers.pdf)
  - [ArXiv:2508.04996v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.08.07_2508.04996v2_REF-VC__Robust,_Expressive_and_Fast_Zero-Shot_Voice_Conversion_With_Diffusion_Transformers.pdf)
  - [Publication] #TODO

</details>

## Abstract

In real-world voice conversion applications, environmental noise in source speech and user demands for expressive output pose critical challenges.
Traditional ASR-based methods ensure noise robustness but suppress prosody richness, while SSL-based models improve expressiveness but suffer from timbre leakage and noise sensitivity.
This paper proposes REF-VC, a noise-robust expressive voice conversion system.
Key innovations include: (1) A random erasing strategy to mitigate the information redundancy inherent in SSL features, enhancing noise robustness and expressiveness; (2) Implicit alignment inspired by E2TTS to suppress non-essential feature reconstruction; (3) Integration of Shortcut Models to accelerate flow matching inference, significantly reducing to 4 steps.
Experimental results demonstrate that REF-VC outperforms baselines such as Seed-VC in zero-shot scenarios on the noisy set, while also performing comparably to Seed-VC on the clean set.
In addition, REF-VC can be compatible with singing voice conversion within one model.
The samples can be found at: \href{https://rxy-j.github.io/asru2025/}{https://rxy-j.github.io/asru2025/}

## 1·Introduction

Voice conversion (VC) is a technique that transforms a speaker's voice into that of another speaker without altering the linguistic content.

VC has been widely used in various domains, including movie and game dubbing, voice chat, and other scenarios.

However, in real-world applications, noise is unavoidable in user recordings.

It's crucial to ignore noise in the source speech and generate clean, high-quality human voices as output.

Meanwhile, advancements in technology have led to increasing user expectations for VC applications.

Beyond preserving linguistic information, there is a growing demand to retain paralinguistic information, enabling more natural and spontaneous speech.

This requires VC systems to capture and reproduce aspects such as tone, emotion, and even non-verbal elements like laughter and crying, ensuring more realistic and expressive output.

To address the first issue, i.e., noise robustness, previous studies have explored the use of adversarial learning[^Chen2024Noise-Robust], [^Du2022Noise-Robust] or data augmentation[^He2024Noro], [^Huang2022Toward] to disentangle noise from the input speech. 
However, these approaches face significant limitations when dealing with unseen types of noise.

Another common solution leverages a well-trained automatic speech recognition (ASR) model, which inherently exhibits a certain degree of noise robustness due to its training objective[^Li2014Overview], [^Ahlawat2025Automatic].

From early phonetic posteriorgram (PPG)-based approaches[^Sun2016Phonetic], [^Liu2021Any-to-Many] to more recent bottleneck feature (BNF)-based methods[^Wang2021Accent], [^Ning2023DualVC], ASR-based content modeling has consistently demonstrated stable performance in VC tasks.

Nevertheless, the main drawback of these models is that the ASR training objective overly emphasizes linguistic content while heavily suppressing paralinguistic information.

Although this avoids source timbre leakage and provides strong noise robustness, it also eliminates prosodic information, leading to flatter rhythms and reduced naturalness in the converted speech.

To enhance the preservation of expressiveness, researchers have adopted self-supervised learning (SSL) models, such as Wav2Vec[^Baevski2020Wav2vec] and WavLM[^Chen2022WavLM], to replace automatic speech recognition (ASR) models in VC systems[^Lin2021S2vc:], [^Neekhara2024SelfVC], [^Li2023Freevc].

The features extracted from these models are compressed representations of audio that retain rich linguistic and paralinguistic information, thereby improving the naturalness and expressiveness of converted speech.

However, this approach introduces new challenges, such as source timbre leakage and reduced noise robustness.

To address these issues, some methods employ k-means clustering[^Ma2024Vec-Tok-Vc+], [^Sim2024Skqvc:] or vector quantization[^Yang2024Takin-Vc] to create information bottlenecks that filter out unwanted elements.

These approaches, however, are highly sensitive to parameter settings.

Improper configurations can easily result in instability in content representation or prosody.

Overall, the ASR-based model performs well in content modeling and exhibits excellent noise robustness, while SSL-based models are superior in capturing paralinguistic content.

To overcome the trade-off challenge between noise robustness and expressiveness preservation in existing voice conversion systems, we propose REF-VC-a **R**obust, **E**xpressive and **F**ast voice conversion system.

Our model adopts the diffusion transformers (DiT)[^Peebles2023Scalable] as its backbone and effectively integrates the complementary advantages of ASR and SSL models.

![](figures/interspeech2025-diffusionvc_overview_bold_new_color.pdf)

<a id="fig:system_overview">Architecture overview of REF-VC</a>

The contributions of this paper are summarized as follows,

-  We propose a VC system that integrates ASR and SSL features.

To address the instability issues caused by redundant information in SSL features, we introduce a simple yet effective random erasing strategy.

Unlike existing feature fusion approaches, our method requires neither adding perturbations to inputs nor employing information bottlenecks to resolve timbre leakage issues.

This approach avoids complex model tuning and potential information loss while enhancing system noise robustness and achieving expressive voice conversion.

-  Unlike conventional frame-to-frame conversion methods, this system employs an implicit alignment approach inspired by E2TTS[^Eskimez2024E2].

This alignment strategy serves to further minimize the model's reconstruction of unimportant information in the input features, thereby enhancing the quality of the conversion results.

-  To reduce the number of inference steps of flow matching[^Lipman2023Flow], Shortcut Models[^Frans2024One] is introduced.

It creates shortcuts by building self-consistency properties upon flow matching.

-  Experiments demonstrate the superiority of our proposed system.

Compared to baseline models such as Seed-VC[^Liu2024Zero-Shot], our method achieves higher speaker similarity and lower character error rate on both clean and noisy sets in zero-shot voice conversion.

The introduction of the Shortcut Models enables inference to be completed in just 4 steps.

Additionally, REF-VC is compatible with singing voice conversion\footnote{Samples can be found on demo page} within a single model.

-  The pretrained models trained on large-scale datasets, and the complete training recipe will be publicly available.

![](figures/interspeech2025-diffusionvc_fusion_bold.pdf)

<a id="fig:fusion">Detail of fusion module.</a>

## 2·Proposed Approach

### Overview

As illustrated in Figure~[fig:system_overview](#fig:system_overview), our proposed model comprises three core components: an input encoder, fusion module, and DiT-based estimator.

We employ pretrained Wenet[^Zhang2022WeNet] to extract bottleneck features $C_{BNF}$ and utilize WAVLM[^Chen2022WavLM] to extract self-supervised representations $C_{SSL}$.

The input encoder projects these $C_{BNF}$ and $C_{SSL}$ features into a low-dimensional latent space as content conditioning.

For fundamental frequency $C_{pitch}$ extracted from audio, we implement multi-scale pitch modeling through Parallel Biased Transposed Convolution (PBTC) modules[^Jayashankar2023Self-Supervised], [^Ning2023Vits-Based].

Drawing inspiration from E2TTS frameworks, we specifically design a VC-optimized fusion module to generate the estimator input $m_t$.

The estimator is trained based on flow matching.

Furthermore, we incorporate Shortcut Models to accelerate the sampling process while maintaining high-quality synthesis.

### Random Erasing Strategy

Constrained by ASR training objectives, BNF contains rich linguistic but lacks paralinguistic information.  As a compressed representation of audio, SSL features compensate for this shortcoming in BNF.

Incorporating SSL features alongside BNF as model inputs can effectively enhance the paralinguistic content performance (e.g., prosody) in converted speech.

However, the rich information within SSL features leads the model to overly rely on them for audio modeling, which is detrimental not only to timbre similarity but also to noise robustness.

Ideally, with the introduction of SSL features, the model should be able to rely primarily on BNF for audio modeling while only utilizing useful information from the SSL features.

We propose a simple yet effective random erasing strategy to regulate feature attention allocation.

During training, we randomly replace SSL features with noise through batch-wise erasure operations.

For each individual sample in a batch, the erasure probability ranges from 0 to 1, while maintaining an overall batch erasure probability of 0.5.

This mechanism effectively suppresses the model's reliance on SSL features, compelling it to primarily utilize BNF for audio reconstruction, thereby preserving robust noise-resistant capabilities.

For SSL features, the model focuses on the content in them that contributes to model convergence.

In our task, this refers to paralinguistic content.

Therefore, the random erasure strategy does not degrade the paralinguistic performance (e.g., prosody and emotional expression) of generated audio.

Meanwhile, with the support of the random erasure strategy, the timbre leakage issue induced by SSL features has also been significantly mitigated.

### Implicit Alignment for Voice Conversion

The frame-level input-output alignment characteristics in ASR-based or SSL-based voice conversion models merit particular attention.

Conventional approaches typically employ transposed convolutions or interpolation to reconcile the frame rate mismatch between input content features and training targets like mel-spectrograms.

This alignment strategy significantly reduces the model's difficulty in reconstructing speech-irrelevant content within current frames, thereby potentially compromising audio clarity and noise robustness.

In contrast, text-to-speech (TTS) systems conventionally address alignment challenges by mapping unaligned content to averaged representations (typically silence).

Implementing similar alignment mechanisms in voice conversion could mitigate noise robustness degradation caused by over-reconstruction.

Methods like StableVC[^Yao2024StableVC] introduce alignment through input feature quantization and repetitive token elimination, yet confront a critical trade-off between token repetition rate and codebook dimensions.

Oversized codebooks yield insufficient token repetition rates that nullify alignment effectiveness, while undersized codebooks achieve higher repetition rates at the expense of potential content information loss.

Furthermore, feature quantization inherently incurs unavoidable paralinguistic information degradation.

As shown in Figure~[fig:fusion](#fig:fusion), our framework introduces an implicit alignment mechanism for voice conversion via a feature fusion module inspired by E2TTS.

We employ blank frame padding to extend the length of the encoder outputs $E_{BNF}$ and $E_{SSL}$ h to match the length of $x_t$.

Since $E_{Pitch}$ has the same frame rate as $x_t$, it has the same length and does not need to be padded.

These processed features are concatenated along the channel dimension with $x_{cond}$ and $x_t$, forming the composite fusion feature $m_t$.

The estimator subsequently generates the target sequence $x_1$ conditioned on $m_t$, timestep $t$, and step size $d$ through iterative denoising.

### Shortcut Models

In practice, in addition to the performance of the model, the speed of inference is also a key concern.

Diffusion models often require dozens of sampling steps to achieve high quality. 
This greatly increases the inference complexity of the model. 
In this paper, we choose to use Shortcut Models to speed up our model.

![](figures/interspeech2025-diffusionvc_shortcutmodel_ht.pdf)

<a id="fig:shortcutmodel">Comparison of shortcut models and flow matching.</a>

Flow matching learns a path from noise to data based on ODE. 
This path is often curved. 
Sampling directly with fewer steps would lead to larger errors. 
As shown in Figure~[fig:shortcutmodel](#fig:shortcutmodel), shortcut models introduce step size $d$ to flow matching, which allows the model to adjust the direction of momentum according to $d$.

This allows the model to jump to the next point as much as possible instead of deviating from the path. 
Shortcut Models is equivalent to flow matching when $d \to 0$. 
For the Shortcut Models $s_{\theta}(x_t, t, d)$, the sampling process is

$$

x_{t+d} = x_t + s_{\theta}(x_t, t, d)d.

$$

This definition allows us to derive the inherent self-consistency of the Shortcut Models.

Once this property is derived, we can transition the model from multi-step sampling to fewer steps and then to one-step sampling.

$$

s(x_t, t, 2d) = s(x_t, t, d) / 2 + s(x_{t+d}, t + d, d) / 2.

$$

The complete loss of the Shortcut Models is as follows:

$$

\begin{gathered}
\mathcal{L} = E[
\underbrace{||s_{\theta}(x_t, t, 0)-(x_1-x_0)||^2}_{\text{Flow-Matching}}]
\\ +E[\underbrace{||s_{\theta}(x_t, t, 2 d)-s_{\text{target}}||^2}_{\text{Self-Consistency}}], \\
\text{where} \quad s_{\text{target}}=s_{\theta}(x_t, t, d) / 2+s_{\theta}(x_{t+d}^{\prime}, t, d) / 2 \quad \\ \text{and} \quad x_{t+d}^{\prime}=x_t+s_{\theta}(x_t, t, d)d.
\end{gathered}

$$

For our model $s_{\theta}(m_t, t, d)$, the sampling procedure needs to be changed to

$$

x_{t+d} = x_t + s_{\theta}(m_t, t, d)d, 

$$

where $m_t$ is as follows

$$

m_t = Concat(E_{BNF}, E_{SSL}, E_{Pitch}, x_{cond}, x_t).

$$

The loss of Shortcut Models consists of two parts: flow matching loss and self-consistency loss.

Flow matching loss determines the base path of the model, while self-consistency loss is responsible for building shortcuts. 
In our training, we split a batch to calculate the two parts of the loss. 
However, in the early stage of training, the path predicted by the model is not correct.

The assumption of the shortcut does not hold.

Therefore, we do not calculate the self-consistency loss in the early stage of training.

After the flow matching loss is reduced to a certain degree, we gradually increase the proportion of self-consistency loss in a batch until the proportion reaches 1/4.

<a id="tab:table1">Objective and subjective evaluation results of comparison and ablation systems for zero-shot voice conversion. **Bold** and \underline{Underline} values indicate the best and second best results.</a>
