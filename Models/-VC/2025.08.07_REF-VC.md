# REF-VC: Robust, Expressive and Fast Zero-Shot Voice Conversion With Diffusion Transformers

<details>
<summary>基本信息</summary>

- 标题: "REF-VC: Robust, Expressive and Fast Zero-Shot Voice Conversion With Diffusion Transformers."
- 作者:
  - 01 Yuepeng Jiang
  - 02 Ziqian Ning
  - 03 Shuai Wang
  - 04 Chengjia Wang
  - 05 Mengxiao Bi
  - 06 Pengcheng Zhu
  - 07 Zhonghua Fu
  - 08 Lei Xie
- 链接:
  - [ArXiv](https://arxiv.org/abs/2508.04996v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2508.04996v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.08.07_2508.04996v1_REF-VC__Robust,_Expressive_and_Fast_Zero-Shot_Voice_Conversion_With_Diffusion_Transformers.pdf)
  - [ArXiv:2508.04996v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.08.07_2508.04996v2_REF-VC__Robust,_Expressive_and_Fast_Zero-Shot_Voice_Conversion_With_Diffusion_Transformers.pdf)
  - [Publication] #TODO

</details>

## Abstract

In real-world voice conversion applications, environmental noise in source speech and user demands for expressive output pose critical challenges.
Traditional ASR-based methods ensure noise robustness but suppress prosody richness, while SSL-based models improve expressiveness but suffer from timbre leakage and noise sensitivity.
This paper proposes REF-VC, a noise-robust expressive voice conversion system.
Key innovations include: (1) A random erasing strategy to mitigate the information redundancy inherent in SSL features, enhancing noise robustness and expressiveness; (2) Implicit alignment inspired by E2TTS to suppress non-essential feature reconstruction; (3) Integration of Shortcut Models to accelerate flow matching inference, significantly reducing to 4 steps.
Experimental results demonstrate that REF-VC outperforms baselines such as Seed-VC in zero-shot scenarios on the noisy set, while also performing comparably to Seed-VC on the clean set.
In addition, REF-VC can be compatible with singing voice conversion within one model.
The samples can be found at: \href{https://rxy-j.github.io/asru2025/}{https://rxy-j.github.io/asru2025/}

## 1·Introduction

Voice conversion (VC) is a technique that transforms a speaker's voice into that of another speaker without altering the linguistic content.

VC has been widely used in various domains, including movie and game dubbing, voice chat, and other scenarios.

However, in real-world applications, noise is unavoidable in user recordings.

It's crucial to ignore noise in the source speech and generate clean, high-quality human voices as output.

Meanwhile, advancements in technology have led to increasing user expectations for VC applications.

Beyond preserving linguistic information, there is a growing demand to retain paralinguistic information, enabling more natural and spontaneous speech.

This requires VC systems to capture and reproduce aspects such as tone, emotion, and even non-verbal elements like laughter and crying, ensuring more realistic and expressive output.

To address the first issue, i.e., noise robustness, previous studies have explored the use of adversarial learning[^Chen2024Noise-Robust], [^Du2022Noise-Robust] or data augmentation[^He2024Noro], [^Huang2022Toward] to disentangle noise from the input speech. 
However, these approaches face significant limitations when dealing with unseen types of noise.

Another common solution leverages a well-trained automatic speech recognition (ASR) model, which inherently exhibits a certain degree of noise robustness due to its training objective[^Li2014Overview], [^Ahlawat2025Automatic].

From early phonetic posteriorgram (PPG)-based approaches[^Sun2016Phonetic], [^Liu2021Any-to-Many] to more recent bottleneck feature (BNF)-based methods[^Wang2021Accent], [^Ning2023DualVC], ASR-based content modeling has consistently demonstrated stable performance in VC tasks.

Nevertheless, the main drawback of these models is that the ASR training objective overly emphasizes linguistic content while heavily suppressing paralinguistic information.

Although this avoids source timbre leakage and provides strong noise robustness, it also eliminates prosodic information, leading to flatter rhythms and reduced naturalness in the converted speech.

To enhance the preservation of expressiveness, researchers have adopted self-supervised learning (SSL) models, such as Wav2Vec[^Baevski2020Wav2vec] and WavLM[^Chen2022WavLM], to replace automatic speech recognition (ASR) models in VC systems[^Lin2021S2vc:], [^Neekhara2024SelfVC], [^Li2023Freevc].

The features extracted from these models are compressed representations of audio that retain rich linguistic and paralinguistic information, thereby improving the naturalness and expressiveness of converted speech.

However, this approach introduces new challenges, such as source timbre leakage and reduced noise robustness.

To address these issues, some methods employ k-means clustering[^Ma2024Vec-Tok-Vc+], [^Sim2024Skqvc:] or vector quantization[^Yang2024Takin-Vc] to create information bottlenecks that filter out unwanted elements.

These approaches, however, are highly sensitive to parameter settings.

Improper configurations can easily result in instability in content representation or prosody.

Overall, the ASR-based model performs well in content modeling and exhibits excellent noise robustness, while SSL-based models are superior in capturing paralinguistic content.

To overcome the trade-off challenge between noise robustness and expressiveness preservation in existing voice conversion systems, we propose REF-VC-a **R**obust, **E**xpressive and **F**ast voice conversion system.

Our model adopts the diffusion transformers (DiT)[^Peebles2023Scalable] as its backbone and effectively integrates the complementary advantages of ASR and SSL models.

![](figures/interspeech2025-diffusionvc_overview_bold_new_color.pdf)

<a id="fig:system_overview">Architecture overview of REF-VC</a>

The contributions of this paper are summarized as follows,

-  We propose a VC system that integrates ASR and SSL features.

To address the instability issues caused by redundant information in SSL features, we introduce a simple yet effective random erasing strategy.

Unlike existing feature fusion approaches, our method requires neither adding perturbations to inputs nor employing information bottlenecks to resolve timbre leakage issues.

This approach avoids complex model tuning and potential information loss while enhancing system noise robustness and achieving expressive voice conversion.

-  Unlike conventional frame-to-frame conversion methods, this system employs an implicit alignment approach inspired by E2TTS[^Eskimez2024E2].

This alignment strategy serves to further minimize the model's reconstruction of unimportant information in the input features, thereby enhancing the quality of the conversion results.

-  To reduce the number of inference steps of flow matching[^Lipman2023Flow], Shortcut Models[^Frans2024One] is introduced.

It creates shortcuts by building self-consistency properties upon flow matching.

-  Experiments demonstrate the superiority of our proposed system.

Compared to baseline models such as Seed-VC[^Liu2024Zero-Shot], our method achieves higher speaker similarity and lower character error rate on both clean and noisy sets in zero-shot voice conversion.

The introduction of the Shortcut Models enables inference to be completed in just 4 steps.

Additionally, REF-VC is compatible with singing voice conversion\footnote{Samples can be found on demo page} within a single model.

-  The pretrained models trained on large-scale datasets, and the complete training recipe will be publicly available.

![](figures/interspeech2025-diffusionvc_fusion_bold.pdf)

<a id="fig:fusion">Detail of fusion module.</a>
