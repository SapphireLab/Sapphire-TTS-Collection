# REF-VC: Robust, Expressive and Fast Zero-Shot Voice Conversion With Diffusion Transformers

<details>
<summary>基本信息</summary>

- 标题: "REF-VC: Robust, Expressive and Fast Zero-Shot Voice Conversion With Diffusion Transformers."
- 作者:
  - 01 Yuepeng Jiang
  - 02 Ziqian Ning
  - 03 Shuai Wang
  - 04 Chengjia Wang
  - 05 Mengxiao Bi
  - 06 Pengcheng Zhu
  - 07 Zhonghua Fu
  - 08 Lei Xie
- 链接:
  - [ArXiv](https://arxiv.org/abs/2508.04996v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2508.04996v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.08.07_2508.04996v1_REF-VC__Robust,_Expressive_and_Fast_Zero-Shot_Voice_Conversion_With_Diffusion_Transformers.pdf)
  - [ArXiv:2508.04996v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.08.07_2508.04996v2_REF-VC__Robust,_Expressive_and_Fast_Zero-Shot_Voice_Conversion_With_Diffusion_Transformers.pdf)
  - [Publication] #TODO

</details>

## Abstract

In real-world voice conversion applications, environmental noise in source speech and user demands for expressive output pose critical challenges.
Traditional ASR-based methods ensure noise robustness but suppress prosody richness, while SSL-based models improve expressiveness but suffer from timbre leakage and noise sensitivity.
This paper proposes REF-VC, a noise-robust expressive voice conversion system.
Key innovations include: (1) A random erasing strategy to mitigate the information redundancy inherent in SSL features, enhancing noise robustness and expressiveness; (2) Implicit alignment inspired by E2TTS to suppress non-essential feature reconstruction; (3) Integration of Shortcut Models to accelerate flow matching inference, significantly reducing to 4 steps.
Experimental results demonstrate that REF-VC outperforms baselines such as Seed-VC in zero-shot scenarios on the noisy set, while also performing comparably to Seed-VC on the clean set.
In addition, REF-VC can be compatible with singing voice conversion within one model.
The samples can be found at: \href{https://rxy-j.github.io/asru2025/}{https://rxy-j.github.io/asru2025/}

## 1·Introduction

Voice conversion (VC) is a technique that transforms a speaker's voice into that of another speaker without altering the linguistic content.

VC has been widely used in various domains, including movie and game dubbing, voice chat, and other scenarios.

However, in real-world applications, noise is unavoidable in user recordings.

It's crucial to ignore noise in the source speech and generate clean, high-quality human voices as output.

Meanwhile, advancements in technology have led to increasing user expectations for VC applications.

Beyond preserving linguistic information, there is a growing demand to retain paralinguistic information, enabling more natural and spontaneous speech.

This requires VC systems to capture and reproduce aspects such as tone, emotion, and even non-verbal elements like laughter and crying, ensuring more realistic and expressive output.

To address the first issue, i.e., noise robustness, previous studies have explored the use of adversarial learning[^Chen2024Noise-Robust], [^Du2022Noise-Robust] or data augmentation[^He2024Noro], [^Huang2022Toward] to disentangle noise from the input speech. 
However, these approaches face significant limitations when dealing with unseen types of noise.

Another common solution leverages a well-trained automatic speech recognition (ASR) model, which inherently exhibits a certain degree of noise robustness due to its training objective[^Li2014Overview], [^Ahlawat2025Automatic].

From early phonetic posteriorgram (PPG)-based approaches[^Sun2016Phonetic], [^Liu2021Any-to-Many] to more recent bottleneck feature (BNF)-based methods[^Wang2021Accent], [^Ning2023DualVC], ASR-based content modeling has consistently demonstrated stable performance in VC tasks.

Nevertheless, the main drawback of these models is that the ASR training objective overly emphasizes linguistic content while heavily suppressing paralinguistic information.

Although this avoids source timbre leakage and provides strong noise robustness, it also eliminates prosodic information, leading to flatter rhythms and reduced naturalness in the converted speech.

To enhance the preservation of expressiveness, researchers have adopted self-supervised learning (SSL) models, such as Wav2Vec[^Baevski2020Wav2vec] and WavLM[^Chen2022WavLM], to replace automatic speech recognition (ASR) models in VC systems[^Lin2021S2vc:], [^Neekhara2024SelfVC], [^Li2023Freevc].

The features extracted from these models are compressed representations of audio that retain rich linguistic and paralinguistic information, thereby improving the naturalness and expressiveness of converted speech.

However, this approach introduces new challenges, such as source timbre leakage and reduced noise robustness.

To address these issues, some methods employ k-means clustering[^Ma2024Vec-Tok-Vc+], [^Sim2024Skqvc:] or vector quantization[^Yang2024Takin-Vc] to create information bottlenecks that filter out unwanted elements.

These approaches, however, are highly sensitive to parameter settings.

Improper configurations can easily result in instability in content representation or prosody.

Overall, the ASR-based model performs well in content modeling and exhibits excellent noise robustness, while SSL-based models are superior in capturing paralinguistic content.

To overcome the trade-off challenge between noise robustness and expressiveness preservation in existing voice conversion systems, we propose REF-VC-a **R**obust, **E**xpressive and **F**ast voice conversion system.

Our model adopts the diffusion transformers (DiT)[^Peebles2023Scalable] as its backbone and effectively integrates the complementary advantages of ASR and SSL models.

![](figures/interspeech2025-diffusionvc_overview_bold_new_color.pdf)

<a id="fig:system_overview">Architecture overview of REF-VC</a>

The contributions of this paper are summarized as follows,

-  We propose a VC system that integrates ASR and SSL features.

To address the instability issues caused by redundant information in SSL features, we introduce a simple yet effective random erasing strategy.

Unlike existing feature fusion approaches, our method requires neither adding perturbations to inputs nor employing information bottlenecks to resolve timbre leakage issues.

This approach avoids complex model tuning and potential information loss while enhancing system noise robustness and achieving expressive voice conversion.

-  Unlike conventional frame-to-frame conversion methods, this system employs an implicit alignment approach inspired by E2TTS[^Eskimez2024E2].

This alignment strategy serves to further minimize the model's reconstruction of unimportant information in the input features, thereby enhancing the quality of the conversion results.

-  To reduce the number of inference steps of flow matching[^Lipman2023Flow], Shortcut Models[^Frans2024One] is introduced.

It creates shortcuts by building self-consistency properties upon flow matching.

-  Experiments demonstrate the superiority of our proposed system.

Compared to baseline models such as Seed-VC[^Liu2024Zero-Shot], our method achieves higher speaker similarity and lower character error rate on both clean and noisy sets in zero-shot voice conversion.

The introduction of the Shortcut Models enables inference to be completed in just 4 steps.

Additionally, REF-VC is compatible with singing voice conversion\footnote{Samples can be found on demo page} within a single model.

-  The pretrained models trained on large-scale datasets, and the complete training recipe will be publicly available.

![](figures/interspeech2025-diffusionvc_fusion_bold.pdf)

<a id="fig:fusion">Detail of fusion module.</a>

## 2·Proposed Approach

### Overview

As illustrated in Figure~[fig:system_overview](#fig:system_overview), our proposed model comprises three core components: an input encoder, fusion module, and DiT-based estimator.

We employ pretrained Wenet[^Zhang2022WeNet] to extract bottleneck features $C_{BNF}$ and utilize WAVLM[^Chen2022WavLM] to extract self-supervised representations $C_{SSL}$.

The input encoder projects these $C_{BNF}$ and $C_{SSL}$ features into a low-dimensional latent space as content conditioning.

For fundamental frequency $C_{pitch}$ extracted from audio, we implement multi-scale pitch modeling through Parallel Biased Transposed Convolution (PBTC) modules[^Jayashankar2023Self-Supervised], [^Ning2023Vits-Based].

Drawing inspiration from E2TTS frameworks, we specifically design a VC-optimized fusion module to generate the estimator input $m_t$.

The estimator is trained based on flow matching.

Furthermore, we incorporate Shortcut Models to accelerate the sampling process while maintaining high-quality synthesis.

### Random Erasing Strategy

Constrained by ASR training objectives, BNF contains rich linguistic but lacks paralinguistic information.  As a compressed representation of audio, SSL features compensate for this shortcoming in BNF.

Incorporating SSL features alongside BNF as model inputs can effectively enhance the paralinguistic content performance (e.g., prosody) in converted speech.

However, the rich information within SSL features leads the model to overly rely on them for audio modeling, which is detrimental not only to timbre similarity but also to noise robustness.

Ideally, with the introduction of SSL features, the model should be able to rely primarily on BNF for audio modeling while only utilizing useful information from the SSL features.

We propose a simple yet effective random erasing strategy to regulate feature attention allocation.

During training, we randomly replace SSL features with noise through batch-wise erasure operations.

For each individual sample in a batch, the erasure probability ranges from 0 to 1, while maintaining an overall batch erasure probability of 0.5.

This mechanism effectively suppresses the model's reliance on SSL features, compelling it to primarily utilize BNF for audio reconstruction, thereby preserving robust noise-resistant capabilities.

For SSL features, the model focuses on the content in them that contributes to model convergence.

In our task, this refers to paralinguistic content.

Therefore, the random erasure strategy does not degrade the paralinguistic performance (e.g., prosody and emotional expression) of generated audio.

Meanwhile, with the support of the random erasure strategy, the timbre leakage issue induced by SSL features has also been significantly mitigated.

### Implicit Alignment for Voice Conversion

The frame-level input-output alignment characteristics in ASR-based or SSL-based voice conversion models merit particular attention.

Conventional approaches typically employ transposed convolutions or interpolation to reconcile the frame rate mismatch between input content features and training targets like mel-spectrograms.

This alignment strategy significantly reduces the model's difficulty in reconstructing speech-irrelevant content within current frames, thereby potentially compromising audio clarity and noise robustness.

In contrast, text-to-speech (TTS) systems conventionally address alignment challenges by mapping unaligned content to averaged representations (typically silence).

Implementing similar alignment mechanisms in voice conversion could mitigate noise robustness degradation caused by over-reconstruction.

Methods like StableVC[^Yao2024StableVC] introduce alignment through input feature quantization and repetitive token elimination, yet confront a critical trade-off between token repetition rate and codebook dimensions.

Oversized codebooks yield insufficient token repetition rates that nullify alignment effectiveness, while undersized codebooks achieve higher repetition rates at the expense of potential content information loss.

Furthermore, feature quantization inherently incurs unavoidable paralinguistic information degradation.

As shown in Figure~[fig:fusion](#fig:fusion), our framework introduces an implicit alignment mechanism for voice conversion via a feature fusion module inspired by E2TTS.

We employ blank frame padding to extend the length of the encoder outputs $E_{BNF}$ and $E_{SSL}$ h to match the length of $x_t$.

Since $E_{Pitch}$ has the same frame rate as $x_t$, it has the same length and does not need to be padded.

These processed features are concatenated along the channel dimension with $x_{cond}$ and $x_t$, forming the composite fusion feature $m_t$.

The estimator subsequently generates the target sequence $x_1$ conditioned on $m_t$, timestep $t$, and step size $d$ through iterative denoising.

### Shortcut Models

In practice, in addition to the performance of the model, the speed of inference is also a key concern.

Diffusion models often require dozens of sampling steps to achieve high quality. 
This greatly increases the inference complexity of the model. 
In this paper, we choose to use Shortcut Models to speed up our model.

![](figures/interspeech2025-diffusionvc_shortcutmodel_ht.pdf)

<a id="fig:shortcutmodel">Comparison of shortcut models and flow matching.</a>

Flow matching learns a path from noise to data based on ODE. 
This path is often curved. 
Sampling directly with fewer steps would lead to larger errors. 
As shown in Figure~[fig:shortcutmodel](#fig:shortcutmodel), shortcut models introduce step size $d$ to flow matching, which allows the model to adjust the direction of momentum according to $d$.

This allows the model to jump to the next point as much as possible instead of deviating from the path. 
Shortcut Models is equivalent to flow matching when $d \to 0$. 
For the Shortcut Models $s_{\theta}(x_t, t, d)$, the sampling process is

$$

x_{t+d} = x_t + s_{\theta}(x_t, t, d)d.

$$

This definition allows us to derive the inherent self-consistency of the Shortcut Models.

Once this property is derived, we can transition the model from multi-step sampling to fewer steps and then to one-step sampling.

$$

s(x_t, t, 2d) = s(x_t, t, d) / 2 + s(x_{t+d}, t + d, d) / 2.

$$

The complete loss of the Shortcut Models is as follows:

$$

\begin{gathered}
\mathcal{L} = E[
\underbrace{||s_{\theta}(x_t, t, 0)-(x_1-x_0)||^2}_{\text{Flow-Matching}}]
\\ +E[\underbrace{||s_{\theta}(x_t, t, 2 d)-s_{\text{target}}||^2}_{\text{Self-Consistency}}], \\
\text{where} \quad s_{\text{target}}=s_{\theta}(x_t, t, d) / 2+s_{\theta}(x_{t+d}^{\prime}, t, d) / 2 \quad \\ \text{and} \quad x_{t+d}^{\prime}=x_t+s_{\theta}(x_t, t, d)d.
\end{gathered}

$$

For our model $s_{\theta}(m_t, t, d)$, the sampling procedure needs to be changed to

$$

x_{t+d} = x_t + s_{\theta}(m_t, t, d)d, 

$$

where $m_t$ is as follows

$$

m_t = Concat(E_{BNF}, E_{SSL}, E_{Pitch}, x_{cond}, x_t).

$$

The loss of Shortcut Models consists of two parts: flow matching loss and self-consistency loss.

Flow matching loss determines the base path of the model, while self-consistency loss is responsible for building shortcuts. 
In our training, we split a batch to calculate the two parts of the loss. 
However, in the early stage of training, the path predicted by the model is not correct.

The assumption of the shortcut does not hold.

Therefore, we do not calculate the self-consistency loss in the early stage of training.

After the flow matching loss is reduced to a certain degree, we gradually increase the proportion of self-consistency loss in a batch until the proportion reaches 1/4.

<a id="tab:table1">Objective and subjective evaluation results of comparison and ablation systems for zero-shot voice conversion. **Bold** and \underline{Underline} values indicate the best and second best results.</a>

## 3·Experiments

### Dataset

We use Emilia[^He2024Emilia] as the training dataset, which contains about 100,000 hours of speech data covering a wide range of speaking styles and content.

This is crucial for training a robust zero-shot voice conversion model.

We set up two test sets: a clean set and a noisy set.

The clean set consists of 100 audio samples randomly selected from our internal test dataset, WenetSpeech[^Zhang2022Wenetspeech:] and Emilia.

The noisy test set consists of 50 pieces of audio recorded in real environments using everyday devices that contain environmental or background noise.

For the target speakers, we randomly select 10 speakers from seed-tts-eval dataset\footnote{\url{https://github.com/BytedanceSpeech/seed-tts-eval}}.

Note that all these test sets and the target speakers are unseen during training.

### Training

The DiT of our model consists of 12 layers, 8 attention heads, and a feedforward network dimension of 768, yielding a total of 100 million parameters.

We employ Wenet[^Zhang2022WeNet] for BNF extraction and Wavlm[^Chen2022WavLM] for SSL feature extraction.

All models are trained for 1 million steps on 8 NVIDIA A100 GPUs.

The total audio length per batch is 2560 seconds.

The optimizer is AdamW with an initial learning rate of 2e-4, and we use the cosine decay strategy to adjust the learning rate.

We use pretrained BigVGAN\footnote{\url{https://github.com/NVIDIA/BigVGAN}} model to transform the generated mel-spectrograms into audio waveforms.

### Baseline

We compare REF-VC with two other systems: Seed-VC\footnote{\url{https://github.com/Plachtaa/seed-vc}} and an internally developed VITS-based[^Kim2021Conditional] voice conversion model (VITS-VC).

Seed-VC is one of the state-of-the-art open-source voice conversion systems.

And it shares a similar architecture with our approach.

For a fair comparison, we utilize the official 100M-parameter checkpoint pre-trained on the Emilia, which ensures equivalent experimental conditions regarding model capacity and training data. 

### Evaluation Metrics

For the objective evaluation, we assess two aspects: speaker similarity and intelligibility, using speaker embedding cosine similarity (SECS) and character error rate (CER), respectively.

We use resemblyzer\footnote{\url{https://github.com/resemble-ai/Resemblyzer}} to evaluate SECS.

CER is evaluated using the toolkit provided by seed-tts-eval.

For the subjective evaluation, we use Mean Opinion Score (MOS) to assess two aspects: speaker similarity (SMOS) and speech naturalness (NMOS).

To evaluate model performance in paralinguistic reconstruction, we conduct ABX tests on three models using the clean set.

Listeners are tasked to select the sample closest to the source in prosody while retaining non-verbal elements like laughter and sighs.

## 4·Results

### Subjective Evaluation

In terms of speaker similarity, our model shows comparable performance to Seed-VC in zero-shot scenarios, while significantly outperforming VITS-VC.

Experimental results show a marginal performance gap between 4-step and 32-step sampling configurations.

It is noteworthy that the results on the noisy set show slightly lower similarity scores compared to the clean set, which is mainly due to the presence of samples with ambiguous pronunciation in the noisy set.

In terms of speech naturalness, our model achieves significant improvements over Seed-VC.

Taking advantage of the rich information encoded in the SSL features, the converted speech effectively mitigates the problems of robotic speech prosody and pitch distortion, thus achieving significantly improved naturalness performance.

As shown in Figure~[fig:abxtest](#fig:abxtest), ABX tests indicate that our model demonstrates significantly superior performance in prosody reconstruction compared to baseline models.

However, it should be noted that Seed-VC's conversion results exhibit high consistency with the prosody of the prompt audio from the target speaker, and its prosodic stability decreases when the prompt audio contains distinctive stylistic characteristics.

### Objective Evaluation

Our experimental results demonstrate significant performance differences across noise conditions.

While maintaining comparable speaker similarity to Seed-VC on clean test sets, our system achieves superior performance on the noisy set.

Both systems achieve an equivalent CER on the clean set; however, Seed-VC exhibits significantly degraded performance on the noisy set.

Notably, despite Seed-VC being an ASR-based voice conversion benchmark, these differences validate the enhanced effectiveness of our method in preserving speech clarity in real-world scenarios involving environmental noise.

Experimental results show 4-step sampling performs slightly inferior to 32-step due to minor audio quality degradation.

However, both configurations demonstrate comparable performance on subjective and objective metrics, indicating minimal speech content divergence.

![](figures/abxtest.pdf)

<a id="fig:abxtest">Result of ABX test.

For Seed-VC and REF-VC, we set the number of sampling steps to 32.</a>

### Ablation Study

We conduct ablation studies on two key designs: the random erasing strategy and implicit alignment.

For the random erasing strategy ablation, we set the random erasing ratio to 0 while maintaining implicit alignment.

In the implicit alignment ablation, we implement feature alignment through interpolation.

As demonstrated in Table~[tab:table1](#tab:table1), the removal of the random erasing strategy results in significant degradation of both audio quality and speaker similarity, confirming its dual functionality in not only reducing the model's attention to speech-irrelevant patterns in SSL features but also substantially mitigating voice timbre leakage induced by SSL features.

Furthermore, experimental results reveal that incorporating implicit alignment further enhances audio reconstruction quality.

The visual comparison in Figure~[fig:compare](#fig:compare) illustrates the quality improvement achieved by our proposed strategy.

The ground truth audio shown in Figure~[fig:compare](#fig:compare)~(a) contains noticeable background noise.

In the ablation study of the random erasing strategy (Figure~[fig:compare](#fig:compare)~(b)), the generated audio exhibit noticeable background noise.

The ablation study of implicit alignment (Figure~[fig:compare](#fig:compare)~(c)) still demonstrates residual background noise reconstruction.

In contrast, the audio reconstructed by our proposed model (Figure~[fig:compare](#fig:compare)~(d)) achieves  almost complete elimination of background noise.

![](figures/orig.jpeg)

<a id="fig:sub-a">Spectrogram visualization of ablation experiments.</a>

## 5·Conclusions

This paper proposes REF-VC, a noise-robust zero-shot voice conversion model that effectively combines BNF and SSL features via random erasing strategy to improve noise robustness while maintaining expressiveness.

An implicit alignment mechanism enhances audio fidelity in challenging environments.

Experiments show comparable performance to state-of-the-art open-source models on clean set and superior results on noisy set.

Notably, by introducing Shortcut Models, we reduce sampling steps from 32 to 4 with minimal quality loss.

## 6·Future Works

In our experiments, we observe that Seed-VC demonstrates capability in transferring target speaker styles, which contributes to improved speaker similarity.

In contrast, our proposed model prioritizes faithful preservation of source prosody.

Our design not only ensures good naturalness but also inherently supports singing voice conversion.

However, through practical investigations, we identify that users generally prefer models capable of simultaneously converting both timbre and style.

Future work will focus on investigating approaches to concurrently achieve prosody preservation and style transfer.

Moreover, unlike conventional VC systems, our model cannot generate arbitrarily long speech.

Our model performs similarly to TTS systems[^Eskimez2024E2], [^Chen2024F5-Tts:].

The introduction of implicit alignment prevents our model from synthesizing excessively long audio.

We will address this duration limitation in future research.

\bibliographystyle{IEEEtran}
\bibliography{mybib}

\end{document}
