# ReFlow-VC: Zero-Shot Voice Conversion Based on Rectified Flow and Speaker Feature Optimization

<details>
<summary>基本信息</summary>

- 标题: "ReFlow-VC: Zero-Shot Voice Conversion Based on Rectified Flow and Speaker Feature Optimization."
- 作者:
  - 01 Pengyu Ren
  - 02 Wenhao Guan
  - 03 Kaidi Wang
  - 04 Peijie Chen
  - 05 Qingyang Hong
  - 06 Lin Li
- 链接:
  - [ArXiv](https://arxiv.org/abs/2506.01032v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2506.01032v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.06.01_2506.01032v1_ReFlow-VC__Zero-Shot_Voice_Conversion_Based_on_Rectified_Flow_and_Speaker_Feature_Optimization.pdf)
  - [Publication] #TODO

</details>

## Abstract

% 1000 characters.
ASCII characters only.
No citations.
In recent years, diffusion-based generative models have demonstrated remarkable performance in speech conversion, including Denoising Diffusion Probabilistic Models (DDPM) and others.
However, the advantages of these models come at the cost of requiring a large number of sampling steps.
This limitation hinders their practical application in real-world scenarios.
In this paper, we introduce ReFlow-VC, a novel high-fidelity speech conversion method based on rectified flow.
Specifically, ReFlow-VC is an Ordinary Differential Equation (ODE) model that transforms a Gaussian distribution to the true Mel-spectrogram distribution along the most direct path.
Furthermore, we propose a modeling approach that optimizes speaker features by utilizing both content and pitch information, allowing speaker features to reflect the properties of the current speech more accurately.
Experimental results show that ReFlow-VC performs exceptionally well in small datasets and zero-shot scenarios.

## 1·Introduction

![](11111111111111.png)

<a id="fig:1">The diagram of the Rectified Flow model, with the meanings of the icons at the top.(a) Linear interpolation of data samples ($X_0$, $X_1$).(b) The rectified flow $Z_t$ induced by ($X_0$, $X_1$).(c) The linear interpolation of data samples ($Z_0$, $Z_1$) of rectified flow $Z_t$.(d) The rectified flow induced from ($Z_0$, $Z_1$), propagating along straight paths.</a>

Zero-shot voice conversion (VC) aims to convert speech from any source speaker to the speech of any target speaker without changing the linguistic content.

VC achieves this by decomposing the source speech into different components, including the speaker's timbre, linguistic content, and speaking style.

Its applications span across various practical fields such as speech anonymization and audiobook production [^Srivastava2022Privacy].

The core challenge of zero-shot VC lies in effectively modeling, disentangling, and utilizing various attributes of speech, including content and timbre.

Traditional zero-shot methods typically combine the linguistic content and speaking style of the source speaker with the timbre of the target speaker to generate the converted speech.

In the groundbreaking Auto-VC model [^Qian2019Autovc], speaker embeddings from a pre-trained speaker verification network are used as conditional inputs.

Other models have been improved upon this by enriching the conditional inputs with additional speech features, such as pitch and loudness, or by jointly training voice conversion and speaker embedding networks.

Additionally, several studies have utilized attention mechanisms to better integrate the features of reference speech into the source speech, thus enhancing the performance of the decoder [^Chen2021Again-Vc].

However, due to the complexity of speech signals [^Pan2024GEmo-Clap] and the limitations in modeling timbre and content [^Hussain2023Ace-Vc], these methods still leave significant room for performance improvement.

Most traditional voice conversion models are based on autoencoder architectures, such as Auto-VC, VAE-VC [^Tobing2019Non-Parallel], and CycleGAN-VC [^Kaneko2018Cyclegan-Vc], or generative adversarial networks (GANs) [^Goodfellow2020Generative], like CycleGAN-VC [^Kaneko2018Cyclegan-Vc], CycleGAN-VC2 [^Kaneko2019Cyclegan-Vc2], and StarGAN-VC [^Kameoka2018Stargan-Vc].

However, relatively few voice conversion models are based on flow models.

Notable voice conversion models, such as Free-VC [^Li2023Freevc], Auto-VC, and Diff-VC [^Popov2021Diffusion-Based], have achieved significant success.

However, there remains room for exploration in terms of model architecture and methodology.

Among them, diffusion models, including denoising diffusion probabilistic models (DDPM) and score-based generative models, have gained widespread attention due to their potential to generate high-quality samples.

A major drawback of diffusion models, however, is that generating satisfactory samples requires multiple iterations [^Guan2024Reflow-TTS].

To address this, several voice conversion methods have been proposed.

Diff-VC gradually converts noise into Mel spectrograms by constructing a stochastic differential equation (SDE) and solving the reverse SDE with a numerical ordinary differential equation (ODE) solver.

While it generates high-quality audio, the large number of iterations in the reverse process affects inference speed.

DDDM-VC addresses the issue of multiple iterations required for generating high-quality samples by introducing a decoupled denoising diffusion model and a prior mixing strategy [^Choi2024DDDM-Vc].

In this paper, we present ReFlow-VC, a voice conversion model based on a modified flow model.

Our proposed method achieves outstanding voice conversion results, and the contributions of this paper are as follows:

-  We propose ReFlow-VC, the first voice conversion acoustic model based on a Rectified Flow Model.

Specifically, the ReFlow-VC model is an ordinary differential equation (ODE) model that transforms a Gaussian distribution into the true mel-spectrogram distribution via a direct path as much as possible, and is trained using a simple unconstrained least squares optimization procedure [^Guan2024Reflow-TTS].

-  We propose a modeling approach that optimizes speaker features using both content and pitch.

Through cross-attention and gated fusion, it effectively integrates multiple input features (such as speaker characteristics, content information, pitch, etc.), thereby enhancing the model's expressive capability.

This enables fine-grained control over the target speaker’s attributes, making the voice conversion task more precise.

## 2·Rectified Flow Model

### Rectified Flow

The rectified flow model is an Ordinary Differential Equation (ODE) model designed to transform a distribution $\pi_0$ (standard Gaussian) to $\pi_1$ (the ground truth distribution) via straight-line paths.

Given samples $X_0 \sim \pi_0$ and $X_1 \sim \pi_1$, the rectified flow corresponds to an ODE defined as [^Liu2022Flow]:

$$

dZ_t = v(Z_t, t) \, dt,

$$

where $Z_0$ is from $\pi_0$, and the transformation follows the distribution $\pi_1$.

Here, $\upsilon$ represents the drift force of the ODE, designed to align the flow with the direction ($X_1$-$X_0$) between the two distributions.

The flow is learned by minimizing a least squares regression problem:

$$

\underset{\upsilon}{\min} \int_0^1 \lVert (X_1 - X_0) - v(X_t, t) \rVert^2 \, dt,

$$

where $X_t$ is the linear interpolation between $X_0$ and 
$X_1$, defined as:

$$

X_t = t X_1 + (1 - t) X_0.

$$

While the naive evolution of $X_t$ follows a non-causal path $dX_t = (X_1 - X_0) \, dt$, the rectified flow causalizes the path by adjusting $\upsilon$ based on $(X_1 - X_0)$, ensuring that the trajectories do not cross at any point, preserving the uniqueness of the solution.

The rectified flow thus avoids non-causal intersections, as shown in Figure [fig:1](#fig:1), ensuring a well-defined, non-crossing path.During training, the objective is to learn the drift force $\upsilon$ by minimizing:

$$

\hat{\theta} = \arg \min_{\theta} \mathbb{E}\left[\lVert (X_1 - X_0) - v(X_t, t) \rVert^2\right],

$$

where t$\sim$Uniform([0,1]).

After training, the learned model is used to transform $X_0$ to $X_1$ by solving the ODE $dZ_t=\hat{v}(Z_t, t) \, dt$.

This procedure can be recursively applied, forming a sequence of transformations $Z^{'} = \text{ReFlow}(Z_0, Z_1)$, leading to improved transport efficiency and more linear flow trajectories.

This recursive process helps reduce time-discretization errors and is computationally advantageous when simulating flows.

### Rectified Flow Model for VC

ReFlow-VC converts the noise distribution to a Mel spectrogram distribution conditioned on time t and the speaker condition features c after feature fusion.

We define $\pi_0$ as the standard Gaussian distribution and $\pi_1$ as the ground truth Mel-spectrogram data distribution, with $X_0 \sim \pi_0$ and $X_1 \sim \pi_1$.

The training objective of ReFlow-VC is as follows:

$$

L_\theta = \mathbb{E}\left[\lVert (X_1 - X_0) - v_\theta(X_t, t, c) \rVert^2\right],

$$

where $t \in \text{Uniform}([0, 1])$ and $X_t = t X_1 + (1 - t) X_0$.

ReFlow-VC does not require any auxiliary losses, except for the L2 loss function between the output of the model $v_\theta$ and $(X_1-X_0)$.

During inference, we directly solve the ODE starting from $Z_0 \sim \pi_0$ conditioned on the speaker feature c and based on the model $v_\theta$.

For high-fidelity generation, we can use the RK45 ODE solver.

For one-step generation, we can directly use the Euler ODE solver for competitive performance [^Guan2024Reflow-TTS].

Furthermore, the recursive rectified flow procedure can also be applied to VC, constructing a second ReFlow-VC, referred to as 2-ReFlow-VC.

The 2-ReFlow-VC is simply retraining the rectified flow model using the samples generated by ReFlow-VC.

## 3·ReFlow-VC Model Architecture

![](zuizhong222.pdf)

<a id="fig:fig1">An illustration of ReFlow-VC.</a>

### Encoder

The encoder consists of an average voice encoder, Hubert-Soft [^Van2022Comparison], a speaker encoder, VQ-VAE, and a feature fusion module.

We chose the average phoneme-level MEL features as the speaker-independent speech representation, similar to Diff-VC [^Popov2021Diffusion-Based].By using the average voice encoder, the source audio is transformed into the average speaker's mel, which is referred to as Average\_mel in Figure [fig:fig1](#fig:fig1).

Additionally, we used Hubert-Soft from Soft-VC to extract continuous content features.

By modeling uncertainty, Hubert-Soft captures more content information, which improves the clarity and naturalness of the converted speech.

Following the work of Polyak et al [^Polyak2021Speech], we used the YAPPT algorithm to extract pitch (F0) from the audio, encoding speaker-independent pitch information.

The F0 of each sample is normalized for each speaker to obtain speaker-independent pitch information, and the VQ-VAE is used to extract vector quantized pitch representations [^Polyak2021Speech].

For a fair comparison, during inference, we normalize the F0 for each sentence instead of for each speaker.

Essentially, zero-shot voice conversion is a challenging task, as it requires the model to generalize effectively to any unseen speaker without additional training or fine-tuning.

This places high demands on the model's ability to capture timbre [^Zhu2025Zsvc].

To enhance the overall timbre modeling capability of the proposed method, we introduce a feature fusion module.

Through multiple attention mechanisms, gated fusion, and iterative self-attention strategies, the speaker features can be precisely adjusted and optimized across various speech attributes [^Pan2024Ctefm-Vc].

Specifically, the model can dynamically adjust speaker features, flexibly modify them using content and pitch information, and enhance the expressiveness of speaker characteristics.

By leveraging cross-attention and gating mechanisms, it captures and generates personalized speaker traits with greater accuracy.

Additionally, the model improves the accuracy and naturalness of voice conversion, ensuring clear and natural speech across different speakers through the use of multiple attention and self-attention mechanisms.

These strategies significantly improve the expressiveness of the speaker features, resulting in more precise and natural voice conversion outcomes.

Specifically, the fusion encoder consists of multiple modules designed to enhance performance and expressive power.

The pitch conv projection layer transforms the input pitch features from a dimension of 1 to 256.

Then, through two layers of cross-attention, each layer receives an input of 256 dimensions and outputs 256 dimensions to facilitate mutual attention between different features.

Next, the gated fusion module processes the 256-dimensional input to enhance the information fusion effect.

The self-attention mechanism iteratively refines the 256-dimensional input's self-attention performance, gradually focusing on the key parts of the input.

Additionally, the model employs a multihead-attention mechanism with both input and output dimensions of 256 and 8 attention heads (with a dimension of 32 per head), further enhancing the model's ability to learn and focus on various features.

These modules work together to improve the model's ability to process speech features and speaker conditioning information.

### Decoder

The decoder architecture is based on U-Net [^Ronneberger2015U-Net] and is the same as in GradTTS [^Popov2021Grad-TTS] but with four times more channels to better capture the full range of human voices.

The speaker conditioning network $g_t(Y)$ consists of 2D convolutions and MLPs(multilayer perceptron).

Its output is a 128-dimensional vector, which is broadcast-concatenated with the concatenation of ${\hat{X}}_t$  and $\bar{X}$ as additional 128 channels.
