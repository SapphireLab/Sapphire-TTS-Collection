# CoDiff-VC: A Codec-Assisted Diffusion Model for Zero-Shot Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "CoDiff-VC: A Codec-Assisted Diffusion Model for Zero-Shot Voice Conversion."
- 作者:
  - 01 Yuke Li
  - 02 Xinfa Zhu
  - 03 Hanzhao Li
  - 04 JiXun Yao
  - 05 WenJie Tian
  - 06 XiPeng Yang
  - 07 YunLin Chen
  - 08 Zhifei Li
  - 09 Lei Xie
- 链接:
  - [ArXiv](https://arxiv.org/abs/2411.18918v3)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2411.18918v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2024.11.28_2411.18918v1_CoDiff-VC__A_Codec-Assisted_Diffusion_Model_for_Zero-Shot_Voice_Conversion.pdf)
  - [ArXiv:2411.18918v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2024.11.28_2411.18918v2_CoDiff-VC__A_Codec-Assisted_Diffusion_Model_for_Zero-Shot_Voice_Conversion.pdf)
  - [ArXiv:2411.18918v3](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2024.11.28_2411.18918v3_CoDiff-VC__A_Codec-Assisted_Diffusion_Model_for_Zero-Shot_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

Zero-shot voice conversion (VC) aims to convert the original speaker's timbre to any target speaker while keeping the linguistic content.
Current mainstream zero-shot voice conversion approaches depend on pre-trained recognition models to disentangle linguistic content and speaker representation.
This results in a timbre residue within the decoupled linguistic content and inadequacies in speaker representation modeling. 
In this study, we propose CoDiff-VC, an end-to-end framework for zero-shot voice conversion that integrates a speech codec and a diffusion model to produce high-fidelity waveforms.
Our approach involves employing a single-codebook codec to separate linguistic content from the source speech.
To enhance content disentanglement, we introduce Mix-Style layer normalization (MSLN) to perturb the original timbre.
Additionally, we incorporate a multi-scale speaker timbre modeling approach to ensure timbre consistency and improve voice detail similarity.
To improve speech quality and speaker similarity, we introduce dual classifier-free guidance, providing both content and timbre guidance during the generation process.
Objective and subjective experiments affirm that CoDiff-VC significantly improves speaker similarity, generating natural and higher-quality speech. 
Audio samples are available on the demo page\footnote{\url{https://aries457.github.io/CoDiff-VC/}}.

## 1·Introduction

![](11.drawio.pdf)

<a id="">Overall architecture of the CoDiff-VC.

The content module within the green dashed line is used to extract linguistic content from the source speech, while we employ a multi-scale timbre modeling module to capture the details of speaker timbre.

The diffusion module within the purple dashed line reconstructs the speech waveform conditioned on the linguistic content and speaker timbre.</a>

Zero-shot voice conversion (VC) aims to transfer the timbre of a source speaker to the timbre of an unseen target speaker while maintaining the original linguistic content[^Sisman2020Overview].

This approach requires only one utterance from the target speaker, making it applicable in various scenarios like movie dubbing[^Yao2023Preserving], speech translation[^S{\"u}ndermann2003VTLN-Based], and speech anonymization[^Yoo2020Speaker].

In real-world applications, zero-shot voice conversion can enhance personalized voice interactions[^Huang2013Personalized], [^Sisman2017Transformation] and improve entertainment experiences by converting speaker timbre characteristics.

However, since only one target speaker utterance is available, disentangling speech components and simultaneously converting to the target speaker's timbre becomes more challenging. 

In the zero-shot VC task, there are two primary challenges to address: firstly, how to disentangle the linguistic content and speaker timbre from the source speech; secondly, how to model the speaker representation precisely.

To solve the first challenge, many previous approaches[^Qian2019AutoVC], [^Wang2023Lm-Vc], [^Zhao2018Accent], [^Liu2021Diffsvc] utilize pre-trained automatic speech recognition (ASR) or self-supervised learning (SSL) models[^Baevski2020Wav2vec], [^Hsu2021HuBERT] to extract bottleneck features as linguistic content decoupled from the source speech.

Simultaneously, a speaker verification model is employed to extract the speaker representation.

The VC model then combines the original linguistic content with the target speaker representation to reconstruct the converted speech.

Despite the previous approach achieving some success in zero-shot VC[^Xiao2021Ca-Vc], [^Zhang2021Sig-Vc], the extracted bottleneck features still contain speaker-related information, leading to poor speaker similarity in the converted speech.

Meanwhile, most of the mentioned approaches depend on an acoustic model for predicting the mel-spectrogram-like latent representations and employ a vocoder to reconstruct the representations into speech waveform[^Dang2022Training], [^Liu2021Diffsvc], [^Shen2024NaturalSpeech].

This two-stage framework introduces cascading errors, thereby degrading the quality of the converted speech.

For the second challenge, the previous studies on speaker representation modeling can be broadly divided into two categories: coarse-grained modeling approach[^Doddipatla2017Speaker], [^Casanova2021YourTTS], [^Kinnunen2017Non-Parallel] and fine-grained modeling approach[^Zhou2022Content-Dependent], [^Jiang2023Mega-TTS], [^Lee2022Pvae-TTS].

In coarse-grained modeling, a pre-trained speaker verification model is utilized to extract a global speaker embedding as the coarse-grained speaker representation.

While effective in controlling the overall timbre characteristics of the converted speech, these approaches fall short in capturing detailed speaker timbre information and semantic-related timbre changes, leading to limited speaker similarity between the converted speech and target speaker speech.

Conversely, other studies employ an attention mechanism to capture fine-grained speaker representation from multiple reference utterances, with Mega-TTS2[^Jiang2023Mega-TTS] being the most prominent example, which allows for the generation of more natural speech for the target speaker.

However, the speaker similarity in converted speech using these approaches relies on the duration of the reference, resulting in a notable degradation in similarity performance when the reference speech duration is excessively short.

In this study, with particular consideration of the above two challenges, we propose CoDiff-VC, a codec-assisted end-to-end framework for zero-shot voice conversion, which can generate high-fidelity waveforms without any auxiliary losses and avoid cascading error issues. 
We employ a pre-trained codec model, featuring only a single codebook, to extract discrete tokens from the source speech as the linguistic content.

The single codebook architecture can partially disentangle the speaker's timbre by introducing a speaker reference encoder while retaining accurate linguistic content information.

Meanwhile,  we incorporate  Mix-Style layer normalization (MSLN)[^Huang2022Generspeech] to perturb the timbre information within the discrete tokens, facilitating a more thorough disentanglement of timbre characteristics from the tokens.

To improve timbre similarity and consistency, we introduce a multi-scale speaker timbre modeling approach to recover voice details when reconstructing the waveform.

Finally, we propose a dual classifier-free guidance strategy to train unconditional models of content and timbre for guiding the reverse process to generate high-quality waveforms.

Objective and subjective experiments demonstrate that CoDiff-VC outperforms the baseline systems in both speech quality and speaker similarity.

Ablation studies further demonstrate the effectiveness of each component in our proposed approach.

## 2·Proposed Approach

The overall architecture of our proposed system is shown in Figure 1, which consists of three parts: content module, multi-scale timbre modeling, and diffusion module.

For the content module, we utilize a pre-trained codec model with a reference encoder to encode audio into discrete tokens and employ mix-style layer normalization further to reduce the residual timbre in the discrete tokens.

To achieve more comprehensive timbre modeling, we introduce multi-scale speaker timbre modeling, encompassing modeling of both coarse-grained and fine-grained timbre representation.

Following this, a denoising diffusion probabilistic model serves as the backbone of the conversion model for an end-to-end reconstruction process.

To enhance the quality of the reconstructed speech, we propose a dual classifier-free guidance strategy to effectively guide content and timbre modeling, thereby improving speech quality and speaker similarity.

### Linguistic Content Extraction

To disentangle the linguistic content from the source speech, we utilize a pre-trained codec to extract discrete tokens as linguistic content.

The architecture closely resembles the codec in [^Betker2023Better].

Since the majority of information is preserved in the first codebook and lower bit rates make it easier to decouple timbres[^Ren2023Fewer-Token], we employ a single-codebook speech codec, comprising only 4096 tokens, to reduce bit rates.

And, to disentangle timbre characteristics within the tokens in the codec, we incorporate a reference encoder into the codec.

The output of the reference encoder, representing global representation related to acoustic information, is subtracted from the hidden representation of the encoder.

This subtraction process imposes implicit constraints on the tokens, leading them to spontaneously disentangle global representations, such as timbre and acoustic features, at lower bit rates.

We use an embedding layer to transform the discrete token into a high-level representation.

The token embedding is upsampled to match the target audio's shape, serving as the content condition for the diffusion process.

To eliminate the speaker timbre, we incorporate MSLN into the pre-encoder to perturb the original timbre information within the tokens by introducing mismatched timbre information, preventing the generation of timbre-consistent representations.

### Multi-scale Speaker Modeling

Many previous studies[^Kang2023Grad-StyleSpeech], [^Min2021Meta-Stylespeech] assume that speaker timbre is a time-invariant global representation, neglecting the potential timbre variations due to intense emotions or specific semantics.

In scenarios of zero-shot voice conversion, relying solely on global representation to model speaker timbre may result in a degradation of timbre similarity in the converted speech.

To address this limitation and leverage the powerful reconstruction capability of diffusion, we introduce multi-scale speaker timbre modeling within the diffusion model.

This approach enables the simultaneous capture of coarse-grained and fine-grained timbre information from reference speech, thereby enhancing the speaker similarity in the converted speech sample.

As shown in Figure 1, the speaker timbre modeling module comprises a speaker encoder and a transformer module.

Initially, we randomly select an utterance and clip it to a fixed-length segment as the reference speech.

The speaker encoder processes this reference to produce frame-level speaker representations, which serve as fine-grained timbre representations.

The transformer module then takes these frame-level representations and generates coarse-grained timbre representations through global pooling.

![](22.drawio.pdf)

<a id="fig:speech_production">The structure of U-net block.</a>

To integrate timbre characteristics at different levels, we use distinct approaches within the diffusion model.

As depicted in Figure 2, the coarse-grained timbre information is concatenated with the timestep embedding and then fed into the residual block.

This setup enables the supervision of diffusion by timbre at each timestep when predicting noise.

For fine-grained timbre modeling, we introduce a cross-attention convolution module every 4 U-net blocks, associating linguistic content information with timbre.

Specifically, the latent representation undergoes 256-time downsampling to match the same dimension as the mel spectrogram to be employed as the query in the cross-attention module, and the fine-grained timbre serves as the keys and values.

Ultimately, the upsampling module restores it to the same shape as the audio.

### Dual Classifier-free Guidance

To guide the reverse diffusion process, we propose a novel dual classifier-free guidance strategy.

We employ classifier-free guidance[^Ho2022Classifier-Free] to content representation $R_{c}$ and speaker timbre $R_{s}$, respectively.

By introducing two distinctive implicit classifiers, both content information and timbre information can guide noise generation in un-condition scenarios.

During training, we discard the content condition and timbre condition at a rate of 15\%.

The inference process is shown in Eq.(1), where the ${\hat{\epsilon}}_{\theta}$ denotes the denoising network which reverses original sample $x_0$ from Gaussian noise $x_t$.

Meanwhile, $w_{c}$ and $w_{s}$ represent the guidance scales of text conditions and timbre conditions for noise prediction, respectively.

$$
\begin{aligned}

\hat{\epsilon}_{\theta}\left(x_{t}, t, R_{c},R_{s}\right)

$$
\begin{aligned}
[t]
&=(1+w_c+w_s) \cdot \epsilon\left(x_{t}, t, R_{c},R_{s}\right) \\
&-w_{c} \cdot \epsilon\left(x_{t}, t, R_{s}\right)-w_{s}\cdot \epsilon\left(x_{t}, t,R_{c}\right)

\end{aligned}
$$

\end{aligned}
$$

During inference, we need to assign a guidance scale to unconditional estimation, which can determine the impact of real conditional estimation and unconditional estimation on the synthesis process.

With each timestep of inference, the coarse-grained information is first modeled before modeling fine-grained information.

Therefore, we use an annealing strategy for content representation, where the scale $w_{c}$ gradually decreases over timesteps.

Meanwhile, we set the guidance scale $w_{s}$ following a hardening strategy, which gradually increases over timesteps, indicating that fine-grained speaker information is gradually restored to improve speaker similarity.

### Training and Inference

We first train the speech codec and then extract quantized discrete tokens for conversion model training.

We select audio clips of the same speaker as the reference speech to get a multi-scale timbre representation.

Then, with the content representation and timbre representation as condition information, the proposed CoDiff-VC is optimized by minimizing the following unweighted variant of the ELBO[^Ho2020Denoising] as shown in Eq.(3), which has proved to be effective for high-quality generation[^Betker2023Better].

$$
\begin{aligned}

\bar\alpha _t = \prod_{t=1}^{T}(1-\beta_{t}) 

\end{aligned}
$$

$$
\begin{aligned}

\min _{\theta}

L(\theta)=\mathbb{E}_{x_{0}, \epsilon, t}||\epsilon-\epsilon_{\theta}\left(\sqrt{\bar{\alpha}_{t}} x_{0}+\sqrt{1-\bar{\alpha}_{t}} \epsilon, t, \text{cond}\right)||_{2}^{2}

\end{aligned}
$$

where the $\epsilon \sim N(0,1)$, $\beta_{t}$ is linear noise schedule and $cond$ means condition information. 
For the reverse process, it can also be defined as a Markov chain from the noisy data $x_t$ to the original data $x_0$ as shown in Eq.(4),

where $\mu_{\theta}$ and $\Sigma_{\theta}$ are the mean and variance terms respectively.

Finally, in order to ensure the speed and quality of inference, we adopt fast sampling[^Chen2021WaveGrad] and $T_{\text{infer}}=10$.

$$
\begin{aligned}

p_{\theta}\left(x_{t-1} \mid x_{t}\right)=N\left(x_{t-1} ; \mu_{\theta}\left(x_{t},t,\text{cond}\right), \Sigma_{\theta}\left(x_{t}, t\right)\right)

\end{aligned}
$$

## 3·Experiments

<a id="tab:mos">Results of CoDiff-VC compared with baseline systems and Ablation models regarding speaker similarity MOS, naturalness MOS and speech quality 
MOS with confidence intervals of 95\%.

SCOS and WER mean speaker cosine similarity and word error rate.</a>

### Datesets and Model Setup

We conduct our experiments using the LibriTTS dataset[^Zen2019LibriTTS], which comprises 580 hours of speech from 2400 speakers.

To standardize the data, we crop the audio samples to the same length and add silence at the end of shorter samples.

The dataset is divided into a training set with speech data from 2390 speakers and a test set with data from the remaining 10 speakers.

All audio samples are resampled to 24000 Hz.

In our implementation, we employ a codec based on the Tortoise approach[^Betker2023Better].

We trained the codec model on 4 NVIDIA V100 GPUs with a batch size of 1024 for 300k training steps.

For the conversion model, CoDiff-VC adopts a U-net structure with 30 blocks, each having a dimension of 128.

It utilizes a linear noise schedule for $\beta_{t} \in\left[1 \times 10^{-4}, 0.02\right]$ with T = 200 in the diffusion model, following the approach in DiffWave[^Kong2020DiffWave].

CoDiff-VC is trained on 4 NVIDIA V100 GPUs in the training setup with a batch size of 16 for 1M steps, using the Adam optimizer with an initial learning rate of 1e-4. %Random short audio clips of 10240 samples from each utterance are used during training.

### Baseline Systems

We adopt YourTTS[^Casanova2021YourTTS], SEF-VC[^Li2023Sef-Vc], LM-VC[^Wang2023Lm-Vc] and Diff-VC [^Popov2022Diffusion-Based] as the baseline systems for comparison with our proposed CoDiff-VC in the zero-shot voice conversion scenario:

-  **YourTTS**: A TTS model with a speaker voiceprint model to extract speaker embedding and employ HiFi-GAN vocoder to reconstruct the waveform.

The speaker embedding served as a condition of the flow-based decoder and posterior encoder for enhancing the zero-shot multi-speaker generation capability.

-  **SEF-VC**: A VC framework with a semantic encoder and a mel encoder.

The semantic encoder reconstructs the discrete representations extracted by Hubert into mel spectrograms as the backbone model.

The mel encoder processes the mel spectrogram to obtain speaker timbre representation via a powerful position-agnostic cross-attention mechanism.

-  **LM-VC**: A two-stage language modeling approach that generates acoustic tokens to recover linguistic content and timbre and reconstructs the fine for acoustic details as converted speech.

-  **Diff-VC**: A diffusion-based VC model that employs the average voice encoder to transform mel features into a speaker-average representation in the dataset for the disentanglement problem.

\vspace{-5pt}

### Subjective Evaluation

We use Mean Opinion Score (MOS) as the subjective metric to evaluate performance between the baseline and proposed systems regarding speaker similarity, naturalness, and speech quality.

The test set comprises 20 utterances involving 10 unseen speakers (5 females and 5 males).

Using 10 speakers as references, each of the 20 sentences serves as input, generating 200 audio samples.

Ten listeners are invited to participate in the subjective evaluation, scoring the similarity between the converted and reference speech.

Additionally, Naturalness MOS and Speech Quality MOS assessments are conducted to measure the intelligibility of the synthesized voice.

As shown in Table~[tab:mos](#tab:mos), the subjective evaluation results demonstrate the superior performance of our proposed approach in zero-shot voice conversion.

YourTTS solely relies on global timbre representation to model speaker timbre, while SEF-VC utilizes only local timbre representation and the self-supervised model extracting semantic information exhibits residual timbre information, leading to poor timbre similarity.

In addition, Diff-VC exhibits lower naturalness MOS than CoDiff-VC, suggesting that our content module extracts better content representations.

Notably, these three models are constrained by a two-stage method, first reconstructing the mel and then restoring it through the vocoder, resulting in degradation of generation quality. 

Limited to coarse acoustic tokens, LM-VC faces challenges in restoring unseen speaker timbres.

In contrast, CoDiff-VC modeling both coarse and fine-grained timbre, coupled with the application of dual CFG on the diffusion model, enhances the quality of converted speech while effectively capturing timbre information.

### Objective Evaluation

To evaluate speaker similarity, we calculate the cosine similarity between the converted speech and the reference speech using a pre-trained ECAPA-TDNN[^Desplanques2020Ecapa-TDNN].

For intelligibility evaluation, we calculate the Word Error Rate (WER) for the converted speech.

The WER assessment is computed using an open-source Wenet toolkit[^Yao2021WeNet] trained on the LibriSpeech dataset. 

As shown in Table~[tab:mos](#tab:mos), the objective evaluation results align with the subjective evaluation, showing that CoDiff-VC achieved the highest cosine similarity.

SEF-VC, LM-VC, and Diff-VC recorded higher WER scores than CoDiff-VC, suggesting that the discrete representation of content through timbre decoupling in Co-DiffVC facilitates more accurate modeling of content information.

Meanwhile, the lower WER results indicate that our proposed CoDiff-VC's converted speech outperforms all baseline intelligibility systems.

To demonstrate the effectiveness of timbre modeling, we visualize the coarse-grained timbre representations through t-SNE[^Maaten2008Visualizing].

We select 100 audio samples from five random speakers in the test set and extract coarse-grained timbre representations for clustering using the model’s coarse-grained timbre modeling structure.

The clustering results, shown in Figure~[fig:tsne](#fig:tsne), indicate that the timbre representations of each speaker are strongly related to the corresponding speakers.

\vspace{-5pt}

### Ablation Study

![](spkssss.pdf)

<a id="fig:tsne">T-SNE visualization of the coarse-grained timbre in multi-speakers settings.</a>

To evaluate the effectiveness of each component in CoDiff-VC, we conduct an ablation study to analyze the impact of three key components: MSLN, multi-scale timbre modeling, and dual classifier-free guidance.

We compare CoDiff-VC with three variants: first, without the MSLN module (-MSLN); second, using only coarse-grained speaker timbre instead of multi-scale ones (-fine-grained); and third, without dual classifier-free guidance (-dual cfg), utilizing only classifier-free guidance for content representation.

As illustrated in Table~[tab:mos](#tab:mos), the speaker similarity and speech naturalness are significantly degraded when removing the MSLN module.

These results suggest that the MSLN module is crucial for retaining content information while eliminating residual timbre information, playing an important role in disentangling linguistic content.

In terms of timbre modeling, relying solely on single-scale speaker representation results in a notable decline in speaker similarity, indicating the critical role of the multi-scale timbre modeling module in capturing the details of speaker timbre.

Additionally, naturalness experiences degradation with only coarse-grained timbre modeling, highlighting the importance of timbral details in synthesizing natural speech.

Furthermore, the model without dual classifier-free guidance exhibits lower speaker similarity and speech quality, emphasizing the contribution of additional regular timbre information guidance to improved timbre modeling and speech quality.

\vspace{-5pt}

## 4·Conclusions

In this paper, we propose CoDiff-VC, a codec-assisted end-to-end framework designed for zero-shot voice conversion.

We employ a pre-trained codec and a pre-encoder to decouple discrete content representations from the source speech.

Meanwhile, we introduce a multi-scale speaker timbre modeling module within the diffusion model for modeling fine-grained timbre details.

Our proposed CoDiff-VC excels in generating converted audio for unseen target speakers with more similar voice characteristics, higher naturalness, and improved speech quality.

Additionally, we present a dual classifier-free guidance method, creating implicit classifiers for content and speaker representations to enhance the inference results of the diffusion model.

However, it's worth noting that the inference speed of diffusion remains relatively slow.

We aim to address and improve the reverse process of diffusion in our future work.

\bibliographystyle{IEEEbib}

\bibliography{refs}

\end{document}

## References

[^Sisman2020Overview]: An Overview of Voice Conversion and Its Challenges: From Statistical Modeling to Deep Learning. IEEE/ACM Trans. Audio Speech Lang. Process. 2020.
[^Yao2023Preserving]: Preserving Background Sound in Noise-Robust Voice Conversion via Multi-Task Learning. Proc. ICASSP 2023.
[^S{\"u}ndermann2003VTLN-based]: VTLN-based Cross-Language Voice Conversion. Proc. ASRU 2003.
[^Yoo2020Speaker]: Speaker Anonymization for Personal Information Protection Using Voice Conversion Techniques. IEEE Access 2020.
[^Huang2013Personalized]: Personalized Spectral and Prosody Conversion Using Frame-Based Codeword Distribution and Adaptive CRF. IEEE/ACM Trans. Audio Speech Lang. Process. 2013.
[^Sisman2017Transformation]: Transformation of Prosody in Voice Conversion. Proc. {APSIPA ASC} 2017.
[^Qian2019AutoVC]: AutoVC: Zero-Shot Voice Style Transfer With Only Autoencoder Loss. Proc. ICML 2019.
[^Wang2023Lm-Vc]: {LM-VC}: Zero-Shot Voice Conversion via Speech Generation Based on Language Models. IEEE Sig. Proc. Lett. 2023.
[^Zhao2018Accent]: Accent Conversion Using Phonetic Posteriorgrams. Proc. {ICASSP} 2018.
[^Liu2021Diffsvc]: Diffsvc: A Diffusion Probabilistic Model for Singing Voice Conversion. Proc. {ASRU} 2021.
[^Baevski2020Wav2vec]: Wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations. Proc. {NIPS} 2020.
[^Hsu2021HuBERT]: {HuBERT}: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units. IEEE/ACM Trans. Audio Speech Lang. Process. 2021.
[^Xiao2021Ca-Vc]: {CA-VC}: A Novel Zero-Shot Voice Conversion Method With Channel Attention. Proc. {APSIPA ASC} 2021.
[^Zhang2021Sig-Vc]: {SIG-VC}: A Speaker Information Guided Zero-Shot Voice Conversion System for Both Human Beings and Machines. Proc. {ICASSP} 2021.
[^Dang2022Training]: Training Robust Zero-Shot Voice Conversion Models With Self-Supervised Features. Proc. {ICASSP} 2022.
[^Shen2024NaturalSpeech]: NaturalSpeech 2: Latent Diffusion Models Are Natural and Zero-Shot Speech and Singing Synthesizers. Proc. {ICLR} 2024.
[^Doddipatla2017Speaker]: Speaker Adaptation in DNN-Based Speech Synthesis Using D-Vectors. Proc. {INTERSPEECH} 2017.
[^Casanova2021YourTTS]: {YourTTS}: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for Everyone. Proc. ICML 2021.
[^Kinnunen2017Non-Parallel]: Non-Parallel Voice Conversion Using I-Vector PLDA: Towards Unifying Speaker Verification and Transformation. Proc. {ICASSP} 2017.
[^Zhou2022Content-Dependent]: Content-Dependent Fine-Grained Speaker Embedding for Zero-Shot Speaker Adaptation in Text-to-Speech Synthesis. Proc. {INTERSPEECH} 2022.
[^Jiang2023Mega-Tts]: {Mega-Tts} 2: Zero-Shot Text-to-Speech With Arbitrary Length Speech Prompts. ArXiv.
[^Lee2022Pvae-Tts]: {PVAE-TTS}: Adaptive Text-to-Speech via Progressive Style Adaptation. Proc. {ICASSP} 2022.
[^Huang2022Generspeech]: Generspeech: Towards Style Transfer for Generalizable Out-of-Domain Text-to-Speech. Proc. {NIPS} 2022.
[^Betker2023Better]: Better Speech Synthesis Through Scaling. ArXiv.
[^Ren2023Fewer-Token]: Fewer-Token Neural Speech Codec With Time-Invariant Codes. ArXiv.
[^Kang2023Grad-StyleSpeech]: Grad-StyleSpeech: Any-Speaker Adaptive Text-to-Speech Synthesis With Diffusion Models. Proc. {ICASSP} 2023.
[^Min2021Meta-Stylespeech]: Meta-Stylespeech: Multi-Speaker Adaptive Text-to-Speech Generation. Proc. ICML 2021.
[^Ho2022Classifier-Free]: Classifier-Free Diffusion Guidance. ArXiv.
[^Ho2020Denoising]: Denoising Diffusion Probabilistic Models. Proc. {NIPS} 2020.
[^Chen2021WaveGrad]: {WaveGrad}: Estimating Gradients for Waveform Generation. Proc. {ICLR} 2021.
[^Zen2019LibriTTS]: LibriTTS: A Corpus Derived From LibriSpeech for Text-to-Speech. Proc. {INTERSPEECH} 2019.
[^Kong2020DiffWave]: {DiffWave}: A Versatile Diffusion Model for Audio Synthesis. Proc. {ICLR} 2020.
[^Li2023Sef-Vc]: {SEF-VC}: Speaker Embedding Free Zero-Shot Voice Conversion With Cross Attention. ArXiv.
[^Popov2022Diffusion-Based]: Diffusion-Based Voice Conversion With Fast Maximum Likelihood Sampling Scheme. Proc, ICLR 2022.
[^Desplanques2020Ecapa-Tdnn]: ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification. Proc. {INTERSPEECH} 2020.
[^Yao2021WeNet]: {WeNet}: Production Oriented Streaming and Non-Streaming End-to-End Speech Recognition Toolkit. Proc. {INTERSPEECH} 2021.
[^Maaten2008Visualizing]: Visualizing Data Using T-Sne. Journal of Machine Learning Research 2008.