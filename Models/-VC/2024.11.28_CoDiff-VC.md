# CoDiff-VC: A Codec-Assisted Diffusion Model for Zero-Shot Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "CoDiff-VC: A Codec-Assisted Diffusion Model for Zero-Shot Voice Conversion."
- 作者:
  - 01 Yuke Li
  - 02 Xinfa Zhu
  - 03 Hanzhao Li
  - 04 JiXun Yao
  - 05 WenJie Tian
  - 06 XiPeng Yang
  - 07 YunLin Chen
  - 08 Zhifei Li
  - 09 Lei Xie
- 链接:
  - [ArXiv](https://arxiv.org/abs/2411.18918v3)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2411.18918v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2024.11.28_2411.18918v1_CoDiff-VC__A_Codec-Assisted_Diffusion_Model_for_Zero-Shot_Voice_Conversion.pdf)
  - [ArXiv:2411.18918v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2024.11.28_2411.18918v2_CoDiff-VC__A_Codec-Assisted_Diffusion_Model_for_Zero-Shot_Voice_Conversion.pdf)
  - [ArXiv:2411.18918v3](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2024.11.28_2411.18918v3_CoDiff-VC__A_Codec-Assisted_Diffusion_Model_for_Zero-Shot_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

Zero-shot voice conversion (VC) aims to convert the original speaker's timbre to any target speaker while keeping the linguistic content.
Current mainstream zero-shot voice conversion approaches depend on pre-trained recognition models to disentangle linguistic content and speaker representation.
This results in a timbre residue within the decoupled linguistic content and inadequacies in speaker representation modeling. 
In this study, we propose CoDiff-VC, an end-to-end framework for zero-shot voice conversion that integrates a speech codec and a diffusion model to produce high-fidelity waveforms.
Our approach involves employing a single-codebook codec to separate linguistic content from the source speech.
To enhance content disentanglement, we introduce Mix-Style layer normalization (MSLN) to perturb the original timbre.
Additionally, we incorporate a multi-scale speaker timbre modeling approach to ensure timbre consistency and improve voice detail similarity.
To improve speech quality and speaker similarity, we introduce dual classifier-free guidance, providing both content and timbre guidance during the generation process.
Objective and subjective experiments affirm that CoDiff-VC significantly improves speaker similarity, generating natural and higher-quality speech. 
Audio samples are available on the demo page\footnote{\url{https://aries457.github.io/CoDiff-VC/}}.
