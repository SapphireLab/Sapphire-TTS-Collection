# CoDiff-VC: A Codec-Assisted Diffusion Model for Zero-Shot Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "CoDiff-VC: A Codec-Assisted Diffusion Model for Zero-Shot Voice Conversion."
- 作者:
  - 01 Yuke Li
  - 02 Xinfa Zhu
  - 03 Hanzhao Li
  - 04 JiXun Yao
  - 05 WenJie Tian
  - 06 XiPeng Yang
  - 07 YunLin Chen
  - 08 Zhifei Li
  - 09 Lei Xie
- 链接:
  - [ArXiv](https://arxiv.org/abs/2411.18918v3)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2411.18918v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2024.11.28_2411.18918v1_CoDiff-VC__A_Codec-Assisted_Diffusion_Model_for_Zero-Shot_Voice_Conversion.pdf)
  - [ArXiv:2411.18918v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2024.11.28_2411.18918v2_CoDiff-VC__A_Codec-Assisted_Diffusion_Model_for_Zero-Shot_Voice_Conversion.pdf)
  - [ArXiv:2411.18918v3](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2024.11.28_2411.18918v3_CoDiff-VC__A_Codec-Assisted_Diffusion_Model_for_Zero-Shot_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

Zero-shot voice conversion (VC) aims to convert the original speaker's timbre to any target speaker while keeping the linguistic content.
Current mainstream zero-shot voice conversion approaches depend on pre-trained recognition models to disentangle linguistic content and speaker representation.
This results in a timbre residue within the decoupled linguistic content and inadequacies in speaker representation modeling. 
In this study, we propose CoDiff-VC, an end-to-end framework for zero-shot voice conversion that integrates a speech codec and a diffusion model to produce high-fidelity waveforms.
Our approach involves employing a single-codebook codec to separate linguistic content from the source speech.
To enhance content disentanglement, we introduce Mix-Style layer normalization (MSLN) to perturb the original timbre.
Additionally, we incorporate a multi-scale speaker timbre modeling approach to ensure timbre consistency and improve voice detail similarity.
To improve speech quality and speaker similarity, we introduce dual classifier-free guidance, providing both content and timbre guidance during the generation process.
Objective and subjective experiments affirm that CoDiff-VC significantly improves speaker similarity, generating natural and higher-quality speech. 
Audio samples are available on the demo page\footnote{\url{https://aries457.github.io/CoDiff-VC/}}.

## 1·Introduction

![](11.drawio.pdf)

<a id="">Overall architecture of the CoDiff-VC.

The content module within the green dashed line is used to extract linguistic content from the source speech, while we employ a multi-scale timbre modeling module to capture the details of speaker timbre.

The diffusion module within the purple dashed line reconstructs the speech waveform conditioned on the linguistic content and speaker timbre.</a>

Zero-shot voice conversion (VC) aims to transfer the timbre of a source speaker to the timbre of an unseen target speaker while maintaining the original linguistic content[^Sisman2020Overview].

This approach requires only one utterance from the target speaker, making it applicable in various scenarios like movie dubbing[^Yao2023Preserving], speech translation[^S{\"u}ndermann2003VTLN-Based], and speech anonymization[^Yoo2020Speaker].

In real-world applications, zero-shot voice conversion can enhance personalized voice interactions[^Huang2013Personalized], [^Sisman2017Transformation] and improve entertainment experiences by converting speaker timbre characteristics.

However, since only one target speaker utterance is available, disentangling speech components and simultaneously converting to the target speaker's timbre becomes more challenging. 

In the zero-shot VC task, there are two primary challenges to address: firstly, how to disentangle the linguistic content and speaker timbre from the source speech; secondly, how to model the speaker representation precisely.

To solve the first challenge, many previous approaches[^Qian2019AutoVC], [^Wang2023Lm-Vc], [^Zhao2018Accent], [^Liu2021Diffsvc] utilize pre-trained automatic speech recognition (ASR) or self-supervised learning (SSL) models[^Baevski2020Wav2vec], [^Hsu2021HuBERT] to extract bottleneck features as linguistic content decoupled from the source speech.

Simultaneously, a speaker verification model is employed to extract the speaker representation.

The VC model then combines the original linguistic content with the target speaker representation to reconstruct the converted speech.

Despite the previous approach achieving some success in zero-shot VC[^Xiao2021Ca-Vc], [^Zhang2021Sig-Vc], the extracted bottleneck features still contain speaker-related information, leading to poor speaker similarity in the converted speech.

Meanwhile, most of the mentioned approaches depend on an acoustic model for predicting the mel-spectrogram-like latent representations and employ a vocoder to reconstruct the representations into speech waveform[^Dang2022Training], [^Liu2021Diffsvc], [^Shen2024NaturalSpeech].

This two-stage framework introduces cascading errors, thereby degrading the quality of the converted speech.

For the second challenge, the previous studies on speaker representation modeling can be broadly divided into two categories: coarse-grained modeling approach[^Doddipatla2017Speaker], [^Casanova2021YourTTS], [^Kinnunen2017Non-Parallel] and fine-grained modeling approach[^Zhou2022Content-Dependent], [^Jiang2023Mega-TTS], [^Lee2022Pvae-TTS].

In coarse-grained modeling, a pre-trained speaker verification model is utilized to extract a global speaker embedding as the coarse-grained speaker representation.

While effective in controlling the overall timbre characteristics of the converted speech, these approaches fall short in capturing detailed speaker timbre information and semantic-related timbre changes, leading to limited speaker similarity between the converted speech and target speaker speech.

Conversely, other studies employ an attention mechanism to capture fine-grained speaker representation from multiple reference utterances, with Mega-TTS2[^Jiang2023Mega-TTS] being the most prominent example, which allows for the generation of more natural speech for the target speaker.

However, the speaker similarity in converted speech using these approaches relies on the duration of the reference, resulting in a notable degradation in similarity performance when the reference speech duration is excessively short.

In this study, with particular consideration of the above two challenges, we propose CoDiff-VC, a codec-assisted end-to-end framework for zero-shot voice conversion, which can generate high-fidelity waveforms without any auxiliary losses and avoid cascading error issues. 
We employ a pre-trained codec model, featuring only a single codebook, to extract discrete tokens from the source speech as the linguistic content.

The single codebook architecture can partially disentangle the speaker's timbre by introducing a speaker reference encoder while retaining accurate linguistic content information.

Meanwhile,  we incorporate  Mix-Style layer normalization (MSLN)[^Huang2022Generspeech] to perturb the timbre information within the discrete tokens, facilitating a more thorough disentanglement of timbre characteristics from the tokens.

To improve timbre similarity and consistency, we introduce a multi-scale speaker timbre modeling approach to recover voice details when reconstructing the waveform.

Finally, we propose a dual classifier-free guidance strategy to train unconditional models of content and timbre for guiding the reverse process to generate high-quality waveforms.

Objective and subjective experiments demonstrate that CoDiff-VC outperforms the baseline systems in both speech quality and speaker similarity.

Ablation studies further demonstrate the effectiveness of each component in our proposed approach.
