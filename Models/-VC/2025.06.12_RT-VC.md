# RT-VC: Real-Time Zero-Shot Voice Conversion With Speech Articulatory Coding

<details>
<summary>基本信息</summary>

- 标题: "RT-VC: Real-Time Zero-Shot Voice Conversion With Speech Articulatory Coding."
- 作者:
  - 01 Yisi Liu
  - 02 Chenyang Wang
  - 03 Hanjo Kim
  - 04 Raniya Khan
  - 05 Gopala Anumanchipalli
- 链接:
  - [ArXiv](https://arxiv.org/abs/2506.10289v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2506.10289v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.06.12_2506.10289v1_RT-VC__Real-Time_Zero-Shot_Voice_Conversion_With_Speech_Articulatory_Coding.pdf)
  - [Publication] #TODO

</details>

## Abstract

Voice conversion has emerged as a pivotal technology in numerous applications ranging from assistive communication to entertainment.
In this paper, we present RT-VC, a zero-shot real-time voice conversion system that delivers ultra-low latency and high-quality performance.
Our approach leverages an articulatory feature space to naturally disentangle content and speaker characteristics, facilitating more robust and interpretable voice transformations.
Additionally, the integration of differentiable digital signal processing (DDSP) enables efficient vocoding directly from articulatory features, significantly reducing conversion latency.
Experimental evaluations demonstrate that, while maintaining synthesis quality comparable to the current state-of-the-art (SOTA) method, RT-VC achieves a CPU latency of 61.4 ms, representing a 13.3\% reduction in latency. 

## 1·Introduction

Voice conversion (VC) modifies speech to match the timbre of a target speaker while preserving content information.

A central challenge in VC is the effective disentanglement of speaker identity from the underlying content.

This separation is critical to enable the transformation of voice characteristics while maintaining the linguistic and paralinguistic information, including emotion and accent.

There are three principal strategies to achieve disentanglement between speaker and content representations in voice conversion.

First, autoencoder‐based approaches employ encoder–decoder architectures (often variational) and incorporate carefully designed bottlenecks or specialized modules to isolate speaker identity from linguistic content [^Qian2019Autovc], [^Qian2020Unsupervised], [^Ju2024Naturalspeech], [^Lian2022Robust], [^Chou2019One-Shot].

Second, GAN‐based methods leverage generative adversarial networks and domain-mapping losses (e.g., cycle-consistency) to ensure that the converted speech retains the source content while convincingly mimicking the target speaker’s characteristics [^Kaneko2018Cyclegan-Vc], [^Kaneko2019Cyclegan-Vc2], [^Kameoka2018Stargan-Vc], [^Kaneko2019Stargan-Vc2], [^Wu2021Understanding].

Third, methods leveraging pretrained models for representation learning extract speaker-independent content representations from external systems, such as automatic speech recognition (ASR) [^Sun2016Phonetic], [^Kashkin2022Hifi-Vc], [^Du2024Cosyvoice], [^Du2024Cosyvoice], text-to-speech (TTS) [^Park2020Cotatron], or self-supervised learning frameworks [^Van2022Comparison], [^Yang2024StreamVC], [^Choi2021Neural], [^Qian2022Contentvec], [^Li2023Freevc].

While these methods achieve impressive performance, they often require meticulous architectural design and careful tuning of loss functions.

Moreover, they typically operate as black-box models, relying on abstract latent spaces that lack interpretability and universality.

To address these limitations and achieve a more natural, straightforward, and grounded disentanglement between speaker and content representations, we adopt the Speech Articulatory Coding (SPARC) framework [^Cho2024Coding].

In SPARC, content information is represented as vocal tract kinematics within a normalized, speaker-agnostic space, while speaker-specific characteristics are captured separately via a dedicated speaker encoder.

This approach yields a naturally disentangled and interpretable representation that supports accent-preserving, zero-shot voice conversion.

However, the transformation between speech and the articulatory feature space is computationally intensive, making SPARC less suitable for real-time applications.

In this paper, we present RT-VC, a zero-shot real-time voice conversion system that combines SPARC with efficient streaming architecture.

In order to accelerate the SPARC encoding process (speech to articulatory features), we train a causal source extractor and a causal acoustic-to-articulatory inversion model using the labels from SPARC encoding.

For SPARC decoding (articulatory features to speech), we utilize the differentiable digital signal processing (DDSP) vocoder from [^Liu2024Fast,], which is known for fast inference and high quality.

Our experimental results show that RT-VC achieves intelligibility and speaker similarity comparable to the current SOTA real-time zero-shot voice conversion system, StreamVC [^Yang2024StreamVC].

In addition, RT-VC achieves an end-to-end CPU latency of 61.4ms, which is 13.3\% faster than StreamVC.

