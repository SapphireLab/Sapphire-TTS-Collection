# RT-VC: Real-Time Zero-Shot Voice Conversion With Speech Articulatory Coding

<details>
<summary>基本信息</summary>

- 标题: "RT-VC: Real-Time Zero-Shot Voice Conversion With Speech Articulatory Coding."
- 作者:
  - 01 Yisi Liu
  - 02 Chenyang Wang
  - 03 Hanjo Kim
  - 04 Raniya Khan
  - 05 Gopala Anumanchipalli
- 链接:
  - [ArXiv](https://arxiv.org/abs/2506.10289v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2506.10289v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.06.12_2506.10289v1_RT-VC__Real-Time_Zero-Shot_Voice_Conversion_With_Speech_Articulatory_Coding.pdf)
  - [Publication] #TODO

</details>

## Abstract

Voice conversion has emerged as a pivotal technology in numerous applications ranging from assistive communication to entertainment.
In this paper, we present RT-VC, a zero-shot real-time voice conversion system that delivers ultra-low latency and high-quality performance.
Our approach leverages an articulatory feature space to naturally disentangle content and speaker characteristics, facilitating more robust and interpretable voice transformations.
Additionally, the integration of differentiable digital signal processing (DDSP) enables efficient vocoding directly from articulatory features, significantly reducing conversion latency.
Experimental evaluations demonstrate that, while maintaining synthesis quality comparable to the current state-of-the-art (SOTA) method, RT-VC achieves a CPU latency of 61.4 ms, representing a 13.3\% reduction in latency. 

## 1·Introduction

Voice conversion (VC) modifies speech to match the timbre of a target speaker while preserving content information.

A central challenge in VC is the effective disentanglement of speaker identity from the underlying content.

This separation is critical to enable the transformation of voice characteristics while maintaining the linguistic and paralinguistic information, including emotion and accent.

There are three principal strategies to achieve disentanglement between speaker and content representations in voice conversion.

First, autoencoder‐based approaches employ encoder–decoder architectures (often variational) and incorporate carefully designed bottlenecks or specialized modules to isolate speaker identity from linguistic content [^Qian2019Autovc], [^Qian2020Unsupervised], [^Ju2024Naturalspeech], [^Lian2022Robust], [^Chou2019One-Shot].

Second, GAN‐based methods leverage generative adversarial networks and domain-mapping losses (e.g., cycle-consistency) to ensure that the converted speech retains the source content while convincingly mimicking the target speaker’s characteristics [^Kaneko2018Cyclegan-Vc], [^Kaneko2019Cyclegan-Vc2], [^Kameoka2018Stargan-Vc], [^Kaneko2019Stargan-Vc2], [^Wu2021Understanding].

Third, methods leveraging pretrained models for representation learning extract speaker-independent content representations from external systems, such as automatic speech recognition (ASR) [^Sun2016Phonetic], [^Kashkin2022Hifi-Vc], [^Du2024Cosyvoice], [^Du2024Cosyvoice], text-to-speech (TTS) [^Park2020Cotatron], or self-supervised learning frameworks [^Van2022Comparison], [^Yang2024StreamVC], [^Choi2021Neural], [^Qian2022Contentvec], [^Li2023Freevc].

While these methods achieve impressive performance, they often require meticulous architectural design and careful tuning of loss functions.

Moreover, they typically operate as black-box models, relying on abstract latent spaces that lack interpretability and universality.

To address these limitations and achieve a more natural, straightforward, and grounded disentanglement between speaker and content representations, we adopt the Speech Articulatory Coding (SPARC) framework [^Cho2024Coding].

In SPARC, content information is represented as vocal tract kinematics within a normalized, speaker-agnostic space, while speaker-specific characteristics are captured separately via a dedicated speaker encoder.

This approach yields a naturally disentangled and interpretable representation that supports accent-preserving, zero-shot voice conversion.

However, the transformation between speech and the articulatory feature space is computationally intensive, making SPARC less suitable for real-time applications.

In this paper, we present RT-VC, a zero-shot real-time voice conversion system that combines SPARC with efficient streaming architecture.

In order to accelerate the SPARC encoding process (speech to articulatory features), we train a causal source extractor and a causal acoustic-to-articulatory inversion model using the labels from SPARC encoding.

For SPARC decoding (articulatory features to speech), we utilize the differentiable digital signal processing (DDSP) vocoder from [^Liu2024Fast,], which is known for fast inference and high quality.

Our experimental results show that RT-VC achieves intelligibility and speaker similarity comparable to the current SOTA real-time zero-shot voice conversion system, StreamVC [^Yang2024StreamVC].

In addition, RT-VC achieves an end-to-end CPU latency of 61.4ms, which is 13.3\% faster than StreamVC.

## 2·Related Work

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{overview_cropped.pdf}
\caption{Training and conversion pipeline of RT-VC. $s$ denotes input speech, $\hat{s}$ denotes reconstructed speech, $r(\cdot)$ denotes the pitch rescaling operation in ([rescaling](#rescaling)).}
\label{system}
\end{figure*}

### Zero-Shot Voice Conversion

Zero-shot voice conversion refers to converting speech from a source speaker to the voice of a new, previously unseen target speaker without requiring any parallel or fine-tuning data for that speaker during training.

Achieving this requires a precise disentanglement of speaker characteristics from the linguistic content.

One of the earliest approaches in this domain is AUTOVC [^Qian2019Autovc], which employs an autoencoder architecture with a carefully designed bottleneck to preserve content information while stripping away speaker-specific features.

This bottleneck concept is also demonstrated in NaturalSpeech 3 [^Ju2024Naturalspeech], where separate bottlenecks for prosody, content, and acoustic details are constructed to remove unnecessary information and facilitate disentanglement.

In contrast, the StarGAN-VC family [^Kameoka2018Stargan-Vc], [^Kaneko2019Stargan-Vc2] formulates voice conversion as a domain translation problem between speaker domains.

These methods utilize a combination of GAN loss and content preservation loss to guide the model to modify only speaker-related features.

Recent approaches utilize pretrained models for obtaining content representations.

For instance, HiFi-VC [^Kashkin2022Hifi-Vc] uses bottleneck features from a pretrained ASR system as the content representation, while the CosyVoice family [^Du2024Cosyvoice], [^Du2024Cosyvoice] further quantizes the ASR bottleneck features to enhance disentanglement.

Cotatron [^Park2020Cotatron] utilizes a pretrained autoregressive TTS model to provide text-speech alignment and employs the aligned phoneme features as content representations.

Additionally, SoftVC [^Van2022Comparison] and StreamVC [^Yang2024StreamVC] leverage the self-supervised learning model HuBERT [^Hsu2021Hubert] to derive discrete labels via k-means clustering; a content encoder is then trained to predict these labels, with the resulting continuous features serving as the content representation.

NANSY [^Choi2021Neural] employs information perturbation techniques to isolate linguistic information from wav2vec 2.0 [^Baevski2020Wav2vec], and ContentVec [^Qian2022Contentvec] applies the same techniques to HuBERT.

### Acoustic-to-Articulatory Inversion

Acoustic-to-articulatory inversion (AAI) aims to predict vocal tract kinematics from raw speech, with these kinematics typically measured via electromagnetic articulography (EMA).

EMA captures distinct patterns of articulator movements that naturally encode linguistic content [^Sun2016Phonetic], [^Cho2024Coding].

However, the scalability of EMA is limited by the high costs of data collection and its inherent entanglement with speaker-specific anatomical features.

Recent AAI models [^Wu2023Speaker-Independent], [^Gao2024Articulatory], [^Attia2024Improving], [^Siriwardena2023Secret] have been proposed to alleviate the collection burden, but they do not fully resolve the issue of speaker entanglement.

To address this, [^Cho2024Self-Supervised], [^Cho2024Coding] argue that the differences between individual speakers’ articulatory systems can be approximated by a single linear affine transformation, and propose the use of a universal articulatory space derived from a single speaker as a common template for all speakers.

These insights provide the foundation for developing voice conversion systems that leverage articulatory features to disentangle linguistic content from speaker characteristics.

### Articulatory Synthesis

Articulatory synthesis, the inverse task of acoustic-to-articulatory inversion (AAI), involves generating speech from articulatory features like EMA.

Recent deep learning approaches in this domain have predominantly employed GAN-based vocoders like HiFi-GAN [^Kong2020Hifi-Gan] to synthesize speech either from intermediate spectrograms [^Chen2021Ema2s], [^Kim2023Style] or directly from articulatory inputs [^Wu2022Deep], [^Cho2024Coding].

A recent study [^Liu2024Fast,] utilizes differentiable digital signal processing (DDSP) to achieve fast inference, high quality and improved parameter efficiency.

In our work, we adopt the DDSP vocoder from [^Liu2024Fast,] to enable real-time voice conversion.

## 3·Method

In this section, we first present an overview of the complete system during both training and inference (Section [overview](#overview)).

Next, we describe the architecture and training strategies for each module of the system (Sections [source_extractor](#source_extractor) through [vocoder](#vocoder)).

Finally, we outline the streaming strategy for real-time voice conversion (Section [streaming](#streaming)).

### System Overview

\label{overview}

Building on the framework presented in [^Cho2024Coding], our proposed system comprises four primary components: a source extractor, an EMA inverter, a speaker encoder, and a DDSP vocoder.

With the exception of the offline speaker encoder, all components are designed to be streamable. 

An overview of the complete system architecture is provided in Figure [system](#system).

During training, the input speech signal is decomposed into an articulatory feature space comprising pitch, periodicity, loudness, EMA, and speaker embedding.

The DDSP vocoder then reconstructs the speech signal from these features.

Notably, the source extractor and EMA inverter are initially trained independently of the whole system.

Subsequently, the speaker encoder and DDSP vocoder are jointly optimized using the outputs of the two pretrained modules.

During conversion, the speaker embedding is extracted from the target speaker's utterance, and the source pitch is adjusted to match the target speaker's range by scaling it with the ratio of the target speaker's median pitch ($m_{tgt}$) to the source speaker's median pitch ($m_{src}$):

$$

\tilde{f_0} = r(f_0) = f_0\cdot\frac{m_{tgt}}{m_{src}}
\label{rescaling}

$$

### Source Extractor

\label{source_extractor}

The source extractor is designed to isolate laryngeal source information from the input speech.

Specifically, it extracts source features including pitch (indicative of the vocal fold vibration frequency), periodicity (reflecting the presence or absence of vocal fold oscillation), and loudness (representing the energy of the airflow through the larynx).

We reformulate the pitch tracking problem as a frequency bin classification task, following the approach outlined in [^Kim2018Crepe], [^Wei2023Rmvpe].

In our method, the source extractor accepts a mel spectrogram as input and generates an encoding using a series of causal convolution blocks following the SoundStream encoder architecture [^Zeghidour2021Soundstream].

This encoding is then processed by three distinct linear output layers: a pitch head that transforms the encoding into a probability distribution over all potential frequency bins for each time frame, a periodicity head that determines whether each input frame is voiced or unvoiced, and a loudness head that predicts the frame-level energy.

To get the final pitch prediction, we use the local weighted average of frequencies closest to the frequency bin with the highest probability, as described in [^Wei2023Rmvpe].

Although a simple digital signal processing method such as a moving average could be used to estimate loudness, we have found that such an approach is highly sensitive to noise.

Therefore, we utilize a dedicated loudness head to produce a clean loudness estimate even under noisy conditions, thereby enhancing the overall noise robustness of the system.

To obtain ground truth labels for pitch and periodicity, we employ CREPE [^Kim2018Crepe] to generate the pitch values and RMVPE [^Wei2023Rmvpe] to derive binary voiced flags.

Loudness labels are computed by averaging the clean input spectrogram along the frequency axis.

Pitch, voiced flags and loudness are all sampled at 200Hz.

We follow the cross entropy loss introduced in [^Wei2023Rmvpe] to train our pitch and periodicity heads.

For loudness head, a simple L1 loss between the prediction and the ground truth is applied.

To enhance the noise robustness of our source extractor, we add noise augmentation using the \texttt{audiomentation} package\footnote{\href{https://github.com/iver56/audiomentations}{https://github.com/iver56/audiomentations}}.

Specifically, we utilize the \texttt{AddColorNoise} module to introduce noise with varied spectral characteristics and the \texttt{RoomSimulator} module to apply different room impulse responses.

### EMA Inverter

We train a real-time EMA inverter based on the SoundStream encoder architecture [^Zeghidour2021Soundstream].

It takes MFCC as input, and processes the input features through 11 dilated causal convolution layers followed by an MLP to get the predicted EMA output. 

We also add augmentation during EMA inverter training.

Prior to applying noise augmentation, we adopt the information perturbation technique proposed in [^Choi2021Neural], which sequentially applies a random parametric equalizer, pitch randomization, and formant shifting.

Since these operations preserve content-level information, they encourage the EMA inverter to focus primarily on content features, thereby promoting improved disentanglement from speaker-specific characteristics. 

To get the EMA ground truth, we generate pseudo EMA labels using the acoustic-to-articulatory inversion model from [^Cho2024Coding], [^Cho2024Self-Supervised], [^Cho2023Evidence].

We linearly interpolate these pseudo EMA from 50Hz to 200Hz.

The EMA inverter is trained to minimize the L1 loss between the predicted EMA and the pseudo EMA labels.

### Speaker Encoder

Similar to [^Cho2024Coding], our speaker encoder contains a frozen CNN feature extractor of WavLM [^Chen2022Wavlm] and a trainable dilated convolution network.

The output encoding will be aggregated into a 128-dimensional speaker embedding using the periodicity output from the pretrained source extractor as the weight.

The speaker encoder is trained together with the vocoder. 

### DDSP Vocoder

\label{vocoder}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{DDSP_Vocoder_cropped.pdf}
\caption{DDSP vocoder architecture. }
\label{vocoder_fig}
\end{figure}

We adopt the DDSP harmonic-plus-noise vocoder from [^Liu2024Fast,] to enable fast inference.

The model architecture is shown in Figure [vocoder_fig](#vocoder_fig).

The encoder accepts the previously described articulatory features as input and separately predicts control signals for harmonic generator and filtered noise generator to generate periodic (harmonic) and aperiodic (noise) components.

These components are summed and then filtered through a post convolution layer to produce the final speech output.

To condition the vocoder on speaker-specific characteristics, we integrate a FiLM layer [^Perez2018Film] that processes the speaker embedding and produces scaling and shifting parameters to modulate the intermediate encoding.

To make the vocoder streamable, we use the SoundStream encoder architecture [^Zeghidour2021Soundstream] with 11 dilated causal convolution layers.

The post convolution layer is also made causal.

We train the model using the loss functions described in [^Liu2024Fast,], namely, the multi-scale spectral loss and the multi-resolution adversarial loss.

### Real-Time Inference

\label{streaming}

For real-time inference, the input spectral features (mel spectrogram and MFCC) are calculated on the fly.

The window size is chosen to be 1024 at 16kHz for all spectral features, with reflection padding to center each output frame.

This translates into a lookahead of half the window size, i.e. 32ms.

Since our system is causal, we only need to maintain a ring buffer to store the running past context for each module during streaming, where the length of the context is determined by the receptive field of the causal convolution network.

Additionally, to facilitate pitch rescaling to the target speaker's range, a running median of the source pitch is also maintained.

The end-to-end latency $L$ is calculated as:

$$

L = t_{lookahead}+t_{chunksize}+t_{processing}

$$

Here $t_{lookahead}$ = 32ms (half the window size), $t_{chunksize}$ = 15ms (the input chunk size), and $t_{processing}$ = 14.4ms is the average processing time for each chunk on an Apple M3 CPU.

Therefore, the end-to-end latency is 61.4ms, which is faster than the current SOTA (StreamVC, 70.8ms) by 13.3\%.

## 4·System Design

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{web_demo.png}
\caption{Screenshot of the RT-VC web demo interface.}
\label{web_demo}
\end{figure}

A screen shot of the RT-VC web demo is shown in Figure [web_demo](#web_demo).

This demo enables real-time voice conversion directly through the web interface, eliminating the need for any downloads.

During conversion, the user speaks into the frontend, where the incoming audio is sampled at 16 kHz and segmented into 15ms chunks.

These chunks are then transmitted to the backend for real-time inference (see Section [streaming](#streaming)), and the converted audio is returned to the frontend for playback through the designated output device.

For audio input and output, they are configured to use the system's default devices.

We recommend using a high-quality microphone with echo cancellation to minimize input noise and reduce speaker feedback.

If necessary, users may modify their audio device settings via the system configuration and refresh the webpage to apply the changes. 

For target speaker selection, users may choose from 10 pre-enrolled target speakers drawn from the VCTK dataset [^Yamagishi2019CSTR], with all target speakers being unseen during training.

Moreover, the system allows users to dynamically switch the target speaker while speaking, with the generated voice updating instantly.

The web demo is deployed on an AWS CPU server (C7i instance type) equipped with an Intel Xeon Scalable processor.

Due to CPU resource constraints, only one user can access the web demo at a time for at most 5 minutes.

Additional users are queued and notified when their session begins.

\begin{table*}[t] 
\centering
\label{tab:metrics}
\renewcommand{\arraystretch}{1.3} 
\resizebox{\textwidth}{!}{ 

\begin{tabular}{l cc cc cc c c}
\toprule
\multirow{2}{*}{\raggedright**Model Name**} 
& \multicolumn{2}{c}{**Naturalness**} 
& \multicolumn{2}{c}{**Intelligibility**} 
& \multicolumn{2}{c}{**Speaker Similarity**} 
& **$f_0$ Consistency**
& **CPU** \\
\cmidrule(lr){2-3} 
\cmidrule(lr){4-5} 
\cmidrule(lr){6-7} 
\cmidrule(lr){8-8}

& **{UTMOS** $\uparrow$}& **{MOS** $\uparrow$}
& **{WER** $\downarrow$} & **{CER** $\downarrow$}
& **{Resemblyzer Score** $\uparrow$} & **{SMOS** $\uparrow$}
& **{$f_0$ PCC** $\uparrow$} 
& **{Latency** $\downarrow$}\\

\midrule
**Source (LibriTTS)** & 4.03 $\pm$ 0.04 & 4.13 $\pm$ 0.16 & 5.06\% & 1.36\% & - & - & - & - \\
\midrule
**StreamVC**  & - & - & **6.22\%** & 2.17\% & **77.81\%** & - & 0.842 & 70.8ms \\
**RT-VC** & 3.81 $\pm$ 0.02 & 3.87 $\pm$ 0.17 & 6.69\% & **2.12\%** & 76.65\% & 3.59 $\pm$ 0.19 & **0.865** & **61.4ms** \\
\bottomrule
\end{tabular}

}
\caption{Performance comparison of StreamVC and RT-VC.

StreamVC values are taken directly from its publication.

Values are presented with their corresponding 95\% confidence intervals where applicable.}
\end{table*}

## 5·Results

#