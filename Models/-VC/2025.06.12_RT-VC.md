# RT-VC: Real-Time Zero-Shot Voice Conversion With Speech Articulatory Coding

<details>
<summary>基本信息</summary>

- 标题: "RT-VC: Real-Time Zero-Shot Voice Conversion With Speech Articulatory Coding."
- 作者:
  - 01 Yisi Liu
  - 02 Chenyang Wang
  - 03 Hanjo Kim
  - 04 Raniya Khan
  - 05 Gopala Anumanchipalli
- 链接:
  - [ArXiv](https://arxiv.org/abs/2506.10289v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2506.10289v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2025.06.12_2506.10289v1_RT-VC__Real-Time_Zero-Shot_Voice_Conversion_With_Speech_Articulatory_Coding.pdf)
  - [Publication] #TODO

</details>

## Abstract

Voice conversion has emerged as a pivotal technology in numerous applications ranging from assistive communication to entertainment.
In this paper, we present RT-VC, a zero-shot real-time voice conversion system that delivers ultra-low latency and high-quality performance.
Our approach leverages an articulatory feature space to naturally disentangle content and speaker characteristics, facilitating more robust and interpretable voice transformations.
Additionally, the integration of differentiable digital signal processing (DDSP) enables efficient vocoding directly from articulatory features, significantly reducing conversion latency.
Experimental evaluations demonstrate that, while maintaining synthesis quality comparable to the current state-of-the-art (SOTA) method, RT-VC achieves a CPU latency of 61.4 ms, representing a 13.3\% reduction in latency. 

## 1·Introduction

Voice conversion (VC) modifies speech to match the timbre of a target speaker while preserving content information.

A central challenge in VC is the effective disentanglement of speaker identity from the underlying content.

This separation is critical to enable the transformation of voice characteristics while maintaining the linguistic and paralinguistic information, including emotion and accent.

There are three principal strategies to achieve disentanglement between speaker and content representations in voice conversion.

First, autoencoder‐based approaches employ encoder–decoder architectures (often variational) and incorporate carefully designed bottlenecks or specialized modules to isolate speaker identity from linguistic content [^Qian2019Autovc], [^Qian2020Unsupervised], [^Ju2024Naturalspeech], [^Lian2022Robust], [^Chou2019One-Shot].

Second, GAN‐based methods leverage generative adversarial networks and domain-mapping losses (e.g., cycle-consistency) to ensure that the converted speech retains the source content while convincingly mimicking the target speaker’s characteristics [^Kaneko2018Cyclegan-Vc], [^Kaneko2019Cyclegan-Vc2], [^Kameoka2018Stargan-Vc], [^Kaneko2019Stargan-Vc2], [^Wu2021Understanding].

Third, methods leveraging pretrained models for representation learning extract speaker-independent content representations from external systems, such as automatic speech recognition (ASR) [^Sun2016Phonetic], [^Kashkin2022Hifi-Vc], [^Du2024Cosyvoice], [^Du2024Cosyvoice], text-to-speech (TTS) [^Park2020Cotatron], or self-supervised learning frameworks [^Van2022Comparison], [^Yang2024StreamVC], [^Choi2021Neural], [^Qian2022Contentvec], [^Li2023Freevc].

While these methods achieve impressive performance, they often require meticulous architectural design and careful tuning of loss functions.

Moreover, they typically operate as black-box models, relying on abstract latent spaces that lack interpretability and universality.

To address these limitations and achieve a more natural, straightforward, and grounded disentanglement between speaker and content representations, we adopt the Speech Articulatory Coding (SPARC) framework [^Cho2024Coding].

In SPARC, content information is represented as vocal tract kinematics within a normalized, speaker-agnostic space, while speaker-specific characteristics are captured separately via a dedicated speaker encoder.

This approach yields a naturally disentangled and interpretable representation that supports accent-preserving, zero-shot voice conversion.

However, the transformation between speech and the articulatory feature space is computationally intensive, making SPARC less suitable for real-time applications.

In this paper, we present RT-VC, a zero-shot real-time voice conversion system that combines SPARC with efficient streaming architecture.

In order to accelerate the SPARC encoding process (speech to articulatory features), we train a causal source extractor and a causal acoustic-to-articulatory inversion model using the labels from SPARC encoding.

For SPARC decoding (articulatory features to speech), we utilize the differentiable digital signal processing (DDSP) vocoder from [^Liu2024Fast,], which is known for fast inference and high quality.

Our experimental results show that RT-VC achieves intelligibility and speaker similarity comparable to the current SOTA real-time zero-shot voice conversion system, StreamVC [^Yang2024StreamVC].

In addition, RT-VC achieves an end-to-end CPU latency of 61.4ms, which is 13.3\% faster than StreamVC.

## 2·Related Work

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{overview_cropped.pdf}
\caption{Training and conversion pipeline of RT-VC. $s$ denotes input speech, $\hat{s}$ denotes reconstructed speech, $r(\cdot)$ denotes the pitch rescaling operation in ([rescaling](#rescaling)).}
\label{system}
\end{figure*}

### Zero-Shot Voice Conversion

Zero-shot voice conversion refers to converting speech from a source speaker to the voice of a new, previously unseen target speaker without requiring any parallel or fine-tuning data for that speaker during training.

Achieving this requires a precise disentanglement of speaker characteristics from the linguistic content.

One of the earliest approaches in this domain is AUTOVC [^Qian2019Autovc], which employs an autoencoder architecture with a carefully designed bottleneck to preserve content information while stripping away speaker-specific features.

This bottleneck concept is also demonstrated in NaturalSpeech 3 [^Ju2024Naturalspeech], where separate bottlenecks for prosody, content, and acoustic details are constructed to remove unnecessary information and facilitate disentanglement.

In contrast, the StarGAN-VC family [^Kameoka2018Stargan-Vc], [^Kaneko2019Stargan-Vc2] formulates voice conversion as a domain translation problem between speaker domains.

These methods utilize a combination of GAN loss and content preservation loss to guide the model to modify only speaker-related features.

Recent approaches utilize pretrained models for obtaining content representations.

For instance, HiFi-VC [^Kashkin2022Hifi-Vc] uses bottleneck features from a pretrained ASR system as the content representation, while the CosyVoice family [^Du2024Cosyvoice], [^Du2024Cosyvoice] further quantizes the ASR bottleneck features to enhance disentanglement.

Cotatron [^Park2020Cotatron] utilizes a pretrained autoregressive TTS model to provide text-speech alignment and employs the aligned phoneme features as content representations.

Additionally, SoftVC [^Van2022Comparison] and StreamVC [^Yang2024StreamVC] leverage the self-supervised learning model HuBERT [^Hsu2021Hubert] to derive discrete labels via k-means clustering; a content encoder is then trained to predict these labels, with the resulting continuous features serving as the content representation.

NANSY [^Choi2021Neural] employs information perturbation techniques to isolate linguistic information from wav2vec 2.0 [^Baevski2020Wav2vec], and ContentVec [^Qian2022Contentvec] applies the same techniques to HuBERT.

### Acoustic-to-Articulatory Inversion

Acoustic-to-articulatory inversion (AAI) aims to predict vocal tract kinematics from raw speech, with these kinematics typically measured via electromagnetic articulography (EMA).

EMA captures distinct patterns of articulator movements that naturally encode linguistic content [^Sun2016Phonetic], [^Cho2024Coding].

However, the scalability of EMA is limited by the high costs of data collection and its inherent entanglement with speaker-specific anatomical features.

Recent AAI models [^Wu2023Speaker-Independent], [^Gao2024Articulatory], [^Attia2024Improving], [^Siriwardena2023Secret] have been proposed to alleviate the collection burden, but they do not fully resolve the issue of speaker entanglement.

To address this, [^Cho2024Self-Supervised], [^Cho2024Coding] argue that the differences between individual speakers’ articulatory systems can be approximated by a single linear affine transformation, and propose the use of a universal articulatory space derived from a single speaker as a common template for all speakers.

These insights provide the foundation for developing voice conversion systems that leverage articulatory features to disentangle linguistic content from speaker characteristics.

#