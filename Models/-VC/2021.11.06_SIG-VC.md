SIG-VC: A Speaker Information Guided Zero-shot Voice Conversion System for Both Human Beings and Machines
Haozhe Zhang, Zexin Cai, Xiaoyi Qin, Ming Li
Nowadays, as more and more systems achieve good performance in traditional voice conversion (VC) tasks, people's attention gradually turns to VC tasks under extreme conditions. In this paper, we propose a novel method for zero-shot voice conversion. We aim to obtain intermediate representations for speaker-content disentanglement of speech to better remove speaker information and get pure content information. Accordingly, our proposed framework contains a module that removes the speaker information from the acoustic feature of the source speaker. Moreover, speaker information control is added to our system to maintain the voice cloning performance. The proposed system is evaluated by subjective and objective metrics. Results show that our proposed system significantly reduces the trade-off problem in zero-shot voice conversion, while it also manages to have high spoofing power to the speaker verification system.
Subjects:	Sound (cs.SD); Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)
Cite as:	arXiv:2111.03811 [cs.SD]
 	(or arXiv:2111.03811v3 [cs.SD] for this version)

https://doi.org/10.48550/arXiv.2111.03811
Focus to learn more
Submission history
From: Haozhe Zhang [view email]
[v1] Sat, 6 Nov 2021 06:22:45 UTC (1,352 KB)
[v2] Fri, 4 Mar 2022 12:30:53 UTC (1,352 KB)
[v3] Sun, 2 Apr 2023 04:21:40 UTC (1,353 KB)# SIG-VC: A Speaker Information Guided Zero-Shot Voice Conversion System for Both Human Beings and Machines

<details>
<summary>基本信息</summary>

- 标题: "SIG-VC: A Speaker Information Guided Zero-Shot Voice Conversion System for Both Human Beings and Machines."
- 作者:
  - 01 Haozhe Zhang
  - 02 Zexin Cai
  - 03 Xiaoyi Qin
  - 04 Ming Li
- 链接:
  - [ArXiv](https://arxiv.org/abs/2111.03811v3)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2111.03811v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2021.11.06_2111.03811v1_SIG-VC__A_Speaker_Information_Guided_Zero-Shot_Voice_Conversion_System_for_Both_Human_Beings_and_Machines.pdf)
  - [ArXiv:2111.03811v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2021.11.06_2111.03811v2_SIG-VC__A_Speaker_Information_Guided_Zero-Shot_Voice_Conversion_System_for_Both_Human_Beings_and_Machines.pdf)
  - [ArXiv:2111.03811v3](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2021.11.06_2111.03811v3_SIG-VC__A_Speaker_Information_Guided_Zero-Shot_Voice_Conversion_System_for_Both_Human_Beings_and_Machines.pdf)
  - [Publication] #TODO

</details>

## Abstract

Nowadays, as more and more systems achieve good performance in traditional voice conversion (VC) tasks, people's attention gradually turns to VC tasks under extreme conditions.
In this paper,  we propose a novel method for zero-shot voice conversion.
We aim to obtain intermediate representations for speaker-content disentanglement of speech to better remove speaker information and get pure content information.
Accordingly, our proposed framework contains a module that removes the speaker information from the acoustic feature of the source speaker.
Moreover, speaker information control is added to our system to maintain the voice cloning performance.
The proposed system is evaluated by subjective and objective metrics.
Results show that our proposed system significantly reduces the trade-off problem in zero-shot voice conversion, while it also manages to have high spoofing power to the speaker verification system.

## 1·Introduction

\label{sec:intro}

Voice Conversion (VC) is a task that aims at converting the timbres from source speakers to target speakers while keeping the content of speech unchanged.

For traditional VC tasks, many systems manage to synthesize speech with good quality, in which the source and target speakers are seen in the training data.

On the other hand, it remains a challenge for voice conversion under the condition that the data of source or target speakers is scarce and unseen in the training stage.

Zero-shot voice conversion is one of the challenges, and most VC systems are not robust enough for it.

Previously, there are research works proposed for addressing the unseen voice conversion problem.

In the perspective of frameworks, generative adversarial networks (GAN) and auto-encoder are the most frequently used ones.

StarGAN is a GAN-based voice conversion framework proposed in [^Choi2018Stargan], then [^Wang2020One-Shot], [^Zhang2020Gazev] extended this framework to achieve one-shot VC. [^Rebryk2020ConVoice], [^Qian2019Autovc], [^Chou2019One-Shot], [^Chen2021Again-Vc], [^Yuan2021Improving], [^Mohammadi2019One-Shot], [^Lu2019One-Shot], [^Deng2020One-Shot], [^Wu2020Vqvc+] are all auto-encoder-based (AE-based) systems, with different variants.

In particular, [^Yuan2021Improving] proposes an AE-based system to extract the speaker embedding and content embedding using mutual information.

Alternatively, [^Wu2020Vqvc+] adopts the U-net architecture in AE-based VC, along with vector quantization [^Gray1984Vector] to disentangle the speaker information and the content information.

In addition, [^Chou2019One-Shot], [^Chen2021Again-Vc] disentangle the speaker and content information by instance normalization, which is also used in [^Wu2020Vqvc+], [^Zhang2020Gazev]. [^Ebbers2021Contrastive] try to separate speaker and content information using variational auto-encoder (VAE).

To better achieve the disentanglement capability, several works use pre-trained models as the speaker or content encoder.

For instance, with the support from the automatic speech recognition system, [^Mohammadi2019One-Shot], [^Lu2019One-Shot] use the phonetic posteriorgrams as the content representation in VC.

On the contrary, [^Qian2019Autovc], [^Deng2020One-Shot] uses speaker verification (SV) systems as their speaker encoders to obtain discriminative representations.

Many aforementioned systems disentangle speaker and content information by finding plausible latent representations.

Nevertheless, most of them apply the reconstruction loss to minimize the difference between the predicted acoustic feature and the target one during training without direct supervision on latent representations.

In this case, it is difficult for latent representations to be robust enough for disentangled content or speaker information.

We believe this might be the reason why the trade-off question mentioned in [^Chen2021Again-Vc] exists, which refers to the issue that the aforementioned systems are not very robust in synthesizing speech with both good similarity and naturalness under the zero-shot scenario. 

In this paper, we propose SIG-VC, a **S**peaker **I**nformation **G**uided **V**oice **C**onversion system.

In our proposed system, we introduce a novel approach to supervise the intermediate representation.

This supervision aims to remove the speaker information from the linguistic information.

Specifically, we employ a pre-trained speaker verification system for speaker representation extraction.

Regarding the linguistic content, a pre-trained acoustic model is applied to extract the linguistic feature.

Experiments in [^Yang2021Building] show that there is speaker information persisting in the linguistic information extracted by the acoustic recognition model.  We manage to get purified content information by eliminating the residual speaker information in the speech.

More detailedly, by sharing parameters for some modules in the system, we force the intermediate representation to be in the vector space of our selected acoustic feature.

Then we erase the speaker information from the intermediate representation with the supervision of the speaker encoder.

Finally, by conditioning on the speaker information of the target speaker, we manage to generate the target speech.

Our proposed system also applies the feedback loss control on the output, which is proposed by [^Cai2020From] and applied on VC in [^Du2021Optimizing].

By adding this loss, the generated results of our proposed system demonstrate high spoofing capability to speaker verification systems. 

To our knowledge, we are the first to propose an approach to supervise the intermediate representation in order to remove speaker information and get pure content representation.

By applying this intermediate representation supervision mechanism, our system outperforms the previous state-of-the-art system, AGAIN-VC [^Chen2021Again-Vc] in zero-shot voice conversion.

This paper is organized as follows: Section [sec:method](#sec:method) introduces our proposed framework for zero-shot voice conversion.

Section [implementation](#implementation) exhibits the implementation details of our proposed system.

The experimental details, along with the results, are presented in Section [sec:exp](#sec:exp).

Finally, we conclude our work in Section [sec:con](#sec:con).  
