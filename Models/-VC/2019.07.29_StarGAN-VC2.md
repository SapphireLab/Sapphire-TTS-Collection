# StarGAN-VC2: Rethinking Conditional Methods for StarGAN-Based Voice Conversion

<details>
<summary>基本信息</summary>

- 标题: "StarGAN-VC2: Rethinking Conditional Methods for StarGAN-Based Voice Conversion."
- 作者:
  - 01 Takuhiro Kaneko
  - 02 Hirokazu Kameoka
  - 03 Kou Tanaka
  - 04 Nobukatsu Hojo
- 链接:
  - [ArXiv](https://arxiv.org/abs/1907.12279v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:1907.12279v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2019.07.29_1907.12279v1_StarGAN-VC2__Rethinking_Conditional_Methods_for_StarGAN-Based_Voice_Conversion.pdf)
  - [ArXiv:1907.12279v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2019.07.29_1907.12279v2_StarGAN-VC2__Rethinking_Conditional_Methods_for_StarGAN-Based_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract

Non-parallel multi-domain voice conversion (VC) is a technique for learning mappings among multiple domains without relying on parallel data.
This is important but challenging owing to the requirement of learning multiple mappings and the non-availability of explicit supervision.
Recently, StarGAN-VC has garnered attention owing to its ability to solve this problem only using a single generator.
However, there is still a gap between real and converted speech.
To bridge this gap, we rethink conditional methods of StarGAN-VC, which are key components for achieving non-parallel multi-domain VC in a single model, and propose an improved variant called StarGAN-VC2.
Particularly, we rethink conditional methods in two aspects: training objectives and network architectures.
For the former, we propose a source-and-target conditional adversarial loss that allows all source domain data to be convertible to the target domain data.
For the latter, we introduce a modulation-based conditional method that can transform the modulation of the acoustic feature in a domain-specific manner.
We evaluated our methods on non-parallel multi-speaker VC.
An objective evaluation demonstrates that our proposed methods improve speech quality in terms of both global and local structure measures.
Furthermore, a subjective evaluation shows that StarGAN-VC2 outperforms StarGAN-VC in terms of naturalness and speaker similarity.\footnote{The converted speech samples are provided at \url{http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/stargan-vc2/index.html}.}

## 1·Introduction

\label{sec:introduction}

Voice conversion (VC) is a technique for converting the non/para-linguistic information between source and target speech while preserving the linguistic information.

VC has been studied intensively owing to its high potential for various applications, such as speaking aids~\cite{AKainSC2007,KNakamuraSC2012} and style~\cite{ZInanogluSC2009,TTodaTASLP2012} and pronunciation~\cite{TKanekoIS2017b} conversion.

One well-established approach to VC involves statistical methods based on Gaussian mixture models (GMMs)~\cite{YStylianouTASP1998,TTodaTASLP2007,EHelanderTASLP2010}, neural networks (NNs) (including restricted Boltzmann machines (RBMs)~\cite{LChenTASLP2014,TNakashikaIEICE2014}, feed forward NNs (FNNs)~\cite{SDesaiTASLP2010,SMohammadiSLT2014,OKeisukeAPSIPAASC2017}, recurrent NNs (RNNs)~\cite{TNakashikaIS2014,LSunICASSP2015}, convolutional NNs (CNNs)~\cite{TKanekoIS2017b}, attention networks~\cite{KTanakaICASSP2019,HKameokaArXiv2018b}, and generative adversarial networks (GANs)~\cite{TKanekoIS2017b}), and exemplar-based methods using non-negative matrix factorization (NMF)~\cite{RTakashimaIEICE2013,ZWuTASLP2014}.

Many VC methods (including the above-mentioned) are categorized as parallel VC, which learns a mapping using the training data of parallel utterance pairs.

However, obtaining such data is often time-consuming or impractical.

Moreover, even if such data are obtained, most VC methods rely on a time alignment procedure, which occasionally fails and requires other painstaking processes, i.e., careful pre-screening or manual correction.

As a solution, non-parallel VC has begun to be studied.

Non-parallel VC, which is comparable to parallel VC, is generally quite challenging to achieve  owing to its disadvantageous training conditions.

To mitigate this difficulty, several studies have used additional data (e.g., parallel utterance pairs among reference speakers~\cite{AMouchtarisTASLP2006,CHLeeIS2006,TTodaIS2006,DSaitoIS2011}) or extra modules (e.g., automatic speech recognition (ASR) modules~\cite{FLXieIS2016,YSaitoICASSP2018}).

These additional data and extra modules are useful for simplifying training but require other costs for preparation.

To avoid such additional costs, recent studies have introduced probabilistic deep generative models, such as an RBM~\cite{TNakashikaTASLP2016}, variational autoencoders (VAEs)~\cite{CHsuIS2017,HKameokaTASLP2019}), and GANs \cite{CHsuIS2017,TKanekoArXiv2017}.

Among them, CycleGAN-VC~\cite{TKanekoArXiv2017} (published \cite{TKanekoEUSIPCO2018} and further improved \cite{TKanekoICASSP2019}) shows promising results by configuring CycleGAN~\cite{JYZhuICCV2017,ZYiICCV2017,TKimICML2017} with a gated CNN~\cite{YDauphinICML2017} and identity-mapping loss~\cite{YTaigmanICLR2017}.

This makes it possible to learn a sequence-based mapping function without relying on parallel data.

With this improvement, CycleGAN-VC performs comparably to parallel VC~\cite{TTodaTASLP2007}.

Along with non-parallel VC, another practically important issue is non-parallel multi-domain VC, i.e., learning mappings among multiple domains (e.g., multiple speakers) without relying on parallel data.

This problem is challenging in terms of scalability because typical VC methods (including CycleGAN-VC) are designed to learn a one-to-one mapping; therefore, they require the learning of multiple generators to achieve multi-domain VC.

For this problem, StarGAN-VC~\cite{HKameokaSLT2018} provides a promising solution by extending CycleGAN-VC to a conditional setting and incorporating domain codes.

Through this extension, StarGAN-VC makes it possible to achieve non-parallel multi-domain VC by only using a single generator while maintaining the advantage of CycleGAN-VC.

The subjective evaluation~\cite{HKameokaSLT2018} demonstrates that StarGAN-VC outperforms another state-of-the-art method, i.e., VAE/GAN-VC~\cite{CHsuIS2017}.

![](figs/objective)

<a id="fig:objective">Comparison of conditional methods in training objectives. "$A$'' and "$B$'' denote the domain codes and "$A \rightarrow B$'' represents the data converted from "$A$'' to "$B$.'' Circle and triangle markers denote real and fake data, respectively. (a) In the classification loss, $G$ prefers to generate classifiable (i.e., far from the decision boundary) data. (b) In the target conditional adversarial loss, $D$ needs to simultaneously handle hard negative samples (e.g., conversion between the same speaker $A \rightarrow A$) and easy negative samples (e.g., conversion between completely different speakers $B \rightarrow A$). (c) The proposed source-and-target conditional adversarial loss can bring all the converted data close to the target data in both source-wise and target-wise manners.</a>

However, even using StarGAN-VC, there is still an insurmountable gap between real and converted speech.

To bridge this gap, we rethink conditional methods of StarGAN-VC, which are key components for solving non-parallel multi-domain VC using a single generator, and propose an improved variant called StarGAN-VC2.

In particular, we rethink conditional methods in two aspects: training objectives and network architectures.

For the former, we propose a source-and-target conditional adversarial loss, which encourages all source domain data to be converted into the target data.

For the latter, we introduce a modulation-based conditional method that can transform the modulation of acoustic features in a domain-dependent manner.

We examined the performance of the proposed methods on the multi-speaker VC task using the Voice Conversion Challenge 2018 (VCC 2018) dataset~\cite{VCC2018}.

An objective evaluation demonstrates that the proposed methods effectively bring the converted acoustic feature sequence close to the target one in terms of both global and local structure measures.

A subjective evaluation shows that StarGAN-VC2 outperforms StarGAN-VC in terms of both naturalness and speaker similarity.

In Section~[sec:stargan-vc](#sec:stargan-vc), we review the conventional StarGAN-VC.

In Section~[sec:stargan-vc2](#sec:stargan-vc2), we describe the proposed StarGAN-VC2.

In Section~[sec:experiments](#sec:experiments), we report the experimental results.

We conclude in Section~[sec:conclusions](#sec:conclusions) with a brief summary and mention of future work.
