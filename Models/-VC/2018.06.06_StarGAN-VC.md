# StarGAN-VC: Non-Parallel Many-to-Many Voice Conversion With Star Generative Adversarial Networks

<details>
<summary>基本信息</summary>

- 标题: "StarGAN-VC: Non-Parallel Many-to-Many Voice Conversion With Star Generative Adversarial Networks."
- 作者:
  - 01 Hirokazu Kameoka
  - 02 Takuhiro Kaneko
  - 03 Kou Tanaka
  - 04 Nobukatsu Hojo
- 链接:
  - [ArXiv](https://arxiv.org/abs/1806.02169v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:1806.02169v1](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2018.06.06_1806.02169v1_StarGAN-VC__Non-Parallel_Many-to-Many_Voice_Conversion_With_Star_Generative_Adversarial_Networks.pdf)
  - [ArXiv:1806.02169v2](D:\Speech\Sapphire-TTS-Collection\Models\-VC\_PDF\2018.06.06_1806.02169v2_StarGAN-VC__Non-Parallel_Many-to-Many_Voice_Conversion_With_Star_Generative_Adversarial_Networks.pdf)
  - [Publication] #TODO

</details>

## Abstract

This paper proposes a method that allows non-parallel many-to-many voice conversion (VC) by using a variant of a generative adversarial network (GAN) called StarGAN. 
Our method, which we call StarGAN-VC, is noteworthy in that it 
(1) requires no parallel utterances, transcriptions, or time alignment procedures for speech generator training, (2) simultaneously learns many-to-many mappings across different attribute domains using a single generator network, (3) is able to generate converted speech signals quickly enough to allow real-time implementations and (4) requires only several minutes of training examples to generate reasonably realistic-sounding speech.
Subjective evaluation experiments on a non-parallel many-to-many speaker identity conversion task
revealed that the proposed method obtained higher sound quality and speaker similarity than a state-of-the-art method based on variational autoencoding GANs.

## 1·Introduction

\label{sec:intro}

Voice conversion (VC) is a technique for converting 
para/non-linguistic information

contained in a given utterance while preserving linguistic information.

This technique can be applied to various
tasks such as speaker-identity modification for text-to-speech
(TTS) systems \cite{Kain1998}, speaking assistance \cite{Kain2007,Nakamura2012}, speech enhancement \cite{Inanoglu2009,Turk2010,Toda2012}, and pronunciation conversion \cite{Kaneko2017c}.

One successful VC framework involves statistical methods
based on Gaussian mixture models (GMMs) \cite{Stylianou1998,Toda2007,Helander2010}.  

Recently, a neural network (NN)-based framework based on feed-forward deep NNs \cite{Desai2010,Mohammadi2014,YSaito2017bshort}, recurrent NNs \cite{Sun2015}, and generative adversarial nets (GANs) \cite{Kaneko2017c}, and an exemplar-based framework based on non-negative matrix factorization (NMF) \cite{Takashima2013,Wu2014} have also proved successful. 
Many conventional VC methods including those mentioned above require accurately aligned parallel 
source and target speech data.

However, in many scenarios, 
it is not always possible to collect parallel utterances. 
Even if we could collect such data, we typically need to perform 
time alignment procedures, which becomes relatively
difficult when there is a large acoustic gap between
the source and target speech.

Since many frameworks are weak as regards the misalignment
found with parallel data, careful pre-screening and
manual correction may be required to make these frameworks work reliably.

To bypass these restrictions, this paper is concerned with developing a non-parallel  
VC method, which requires no parallel utterances, transcriptions, or time alignment procedures.

In general, the quality and conversion effect obtained with non-parallel methods
are usually limited compared with methods using parallel data
due to the disadvantage related to the training condition. 
Thus, developing non-parallel methods with as high an audio quality and conversion effect as parallel methods can be very challenging. 
Recently, some attempts have been made to develop non-parallel methods \cite{Chen2014,Nakashika2014a,Nakashika2014b,Nakashika2015short,Blaauw2016,Hsu2016,Hsu2017,Xie2016,Kinnunen2017,Kaneko2017d,vandenOord2017bshort,Hashimoto2017short,YSaito2018bshort}.

For example, a method using automatic speech recognition (ASR) was proposed in \cite{Xie2016}.

The idea is to convert input speech under the restriction that the posterior state probability of the acoustic model of an ASR system is preserved so that the transcription of the converted speech becomes consistent with that of the input speech. 
Since the performance of this method depends heavily on the quality of the acoustic model of ASR,
it can fail to work if ASR does not function reliably. 
A method using i-vectors \cite{Dehak2011}, known as a feature for speaker verification, was recently proposed in \cite{Kinnunen2017}.

Conceptually, the idea is to
shift the acoustic features of input speech towards target speech in the i-vector space so that

the converted speech is likely to be recognized as the target speaker by a speaker recognizer.

While this method is also free from parallel data, one limitation is that 
it is applicable only to speaker identity conversion tasks.

Recently, a framework based on conditional variational autoencoders (CVAEs) \cite{Kingma2014a,Kingma2014b} was proposed in \cite{Hsu2016,YSaito2018bshort}.

As the name implies, variational autoencoders (VAEs) are a probabilistic counterpart of autoencoders (AEs), consisting of encoder and decoder networks.

CVAEs \cite{Kingma2014b} are an extended version of VAEs where the encoder and decoder networks can take an auxiliary variable $c$ as an additional input.

By using acoustic features as the training examples and the associated attribute labels as $c$, 
the networks learn how to convert an attribute of source speech to a target attribute according to the attribute label fed into the decoder. 
This CVAE-based VC approach is notable in that it is completely free from parallel data and works even with unaligned corpora. 
However, one well-known problem as regards VAEs is that outputs from the decoder tend to be oversmoothed. 
For VC applications, this can be problematic since it usually results in poor quality buzzy-sounding speech. 

One powerful framework that can potentially overcome the weakness of VAEs involves GANs \cite{Goodfellow2014short}.

GANs offer a general framework for training a generator network in such a way that it can 

deceive a real/fake discriminator network. 
While they have been found to be effective for use with image generation, 
in recent years they 

have also been employed with notable success for various speech processing tasks \cite{Kaneko2017a,YSaito2018a,Pascual2017short,Kaneko2017b,Kaneko2017c,Oyamada2018}.

We previously reported a non-parallel VC method using a GAN variant called cycle-consistent GAN (CycleGAN) \cite{Kaneko2017d}, which
was originally proposed as a method for translating images using unpaired training examples \cite{Zhu2017,Kim2017,Yi2017}. 
This method, which we call CycleGAN-VC, is designed to
learn the mapping $G$ of acoustic features from one attribute $X$ to another attribute $Y$, 
its inverse mapping $F$, and 
a discriminator $D$, 
whose role is to distinguish the acoustic features of converted speech from those of real speech, 
through a training loss 
combining an adversarial loss and a cycle consistency loss.

\begin{comment}

The adversarial loss takes a large value when the discriminator $D$ correctly distinguishes 
the converted features obtained by $G$ and $F$ from real speech features. 
Since one of the goals of $G$ and $F$ is to generate realistic-sounding speech that are indistinguishable from real speech, 
$G$ and $F$ attempt to fool $D$ by minimizing this loss while $D$ is trained to maximize this loss not to be fooled by $G$ and $F$.

In theory, the output distributions of $G$ and $F$ trained in this way will match the empirical distributions of acoustic features in domain $X$ and $Y$ when $G$ and $F$ have enough capacity. 
It should be noted, however, that training $G$ and $F$ using only the adversarial loss does not guarantee that 
applying $G$ or $F$ will preserve the linguistic information of input speech 
since there are infinitely many mappings that will induce the same output distributions. 
To further regularize these mappings, the cycle consistency loss is introduced to encourage $F(G(x))\simeq x$ and $G(F(y))\simeq y$. 
This specific regularization ensures that both $G$ and $F$ become bijections and contributes to obtaining semantic-preserving conversion rules from unaligned training examples.
\end{comment}

Although this method was shown to work reasonably well, 
one major limitation is that it only learns one-to-one mappings. 
With a lot of VC application scenarios, it is desirable to obtain many-to-many mappings. 
One naive way of applying CycleGAN to many-to-many VC tasks would be to train different $G$ and $F$ pairs for all pairs of attribute domains. 
However, this may be ineffective since 
all attribute domains are common in the sense that they represent speech and so there must be common latent features that can be shared across different domains. 
In practice, the number of parameters will increase quadratically with the number of attribute domains, making parameter training challenging particularly when there are a limited number of training examples in each domain. 

A common limitation of CVAE-VC and CycleGAN-VC is that at test time the attribute of the input speech must be known.

As for CVAE-VC, the source attribute label $c$ must be fed into the encoder of the trained CVAE and with CycleGAN-VC, the source attribute domains at training and test times must be the same. 

To overcome the shortcomings and limitations of CVAE-VC \cite{Hsu2016} and CycleGAN-VC \cite{Kaneko2017d}, this paper 
proposes a non-parallel many-to-many VC method using
a recently proposed novel GAN variant called StarGAN \cite{Choi2017short}, which 
offers the advantages of CVAE-VC and CycleGAN-VC concurrently. 
Unlike CycleGAN-VC and as with CVAE-VC, our method, which we call StarGAN-VC, is 
capable of simultaneously learning many-to-many mappings using a single encoder-decoder type generator network $G$ where the attributes of the generator outputs are controlled by an auxiliary input $c$. 
Unlike CVAE-VC and as with CycleGAN-VC, StarGAN-VC uses an adversarial loss 

for generator training 
to encourage 
the generator outputs to become indistinguishable from real speech
and ensure that the mappings between each pair of attribute domains

will preserve linguistic information.

It is also noteworthy that unlike CVAE-VC and CycleGAN-VC, StarGAN-VC does not require any information about the attribute of 
the input speech at test time.

The VAE-GAN framework \cite{Larsen2015short} is perhaps another natural way of overcoming the weakness of VAEs.

A non-parallel VC method based on this framework has already been proposed in \cite{Hsu2017}.

With this approach, an adversarial loss derived using a GAN discriminator is incorporated into the training loss to encourage the decoder outputs of a CVAE to be indistinguishable from real speech features. 
Although the concept is similar to our StarGAN-VC approach, 
we will show in \refsec{experiments} that our approach outperforms this method 
in terms of both the audio quality and conversion effect.

Another related technique worth noting is 
the vector quantized VAE (VQ-VAE) approach \cite{vandenOord2017bshort}, 
which has performed impressively in non-parallel VC tasks. 
This approach 
is particularly notable in that it offers a novel way of overcoming the weakness of VAEs by using

the WaveNet model \cite{vandenOord2016short}, a sample-by-sample neural signal generator, to devise both the encoder and decoder of a discrete counterpart of CVAEs.

The original WaveNet model is a recursive model that makes it possible to predict the distribution of a sample conditioned on the samples the generator has produced. 
While a faster version \cite{vandenOord2017ashort} has recently been proposed,
it typically requires huge computational cost to generate a stream of samples, which can cause difficulties when implementing real-time systems.

The model is also known to require a huge number of training examples to be able to generate natural-sounding speech.

By contrast, our method is noteworthy in that it is able to generate signals quickly enough to allow real-time implementation and requires only several minutes of training examples to generate reasonably realistic-sounding speech.

The remainder of this paper is organized as follows.

We briefly review the formulation of CycleGAN-VC in \refsec{cyclegan-vc},
present the idea of StarGAN-VC in \refsec{stargan-vc} and 
show experimental results in \refsec{experiments}.

## 2·CycleGAN Voice Conversion

\label{sec:cyclegan-vc}

Since the present method is an extension of CycleGAN-VC, which we proposed previously \cite{Kaneko2017d},  
we start by briefly reviewing its formulation.

Let $\Vec{x}\in \mathbb{R}^{Q\times N}$ and $\Vec{y}\in\mathbb{R}^{Q\times M}$
be acoustic feature sequences of speech belonging to attribute domains $X$ and $Y$, respectively,
where $Q$ is the feature dimension and $N$ and $M$ are the lengths of the sequences.

The aim of CycleGAN-VC is to learn 
a mapping $G$ that converts the attribute of $\Vec{x}$ into $Y$
and a mapping $F$ that does the opposite.

Now, we introduce discriminators $D_X$ and $D_Y$,
whose roles are to predict whether or not their inputs are the acoustic features of real speech belonging to $X$ and $Y$,
and define 

$$
\begin{aligned}

\mathcal{L}_{\rm adv}^{D_Y}(D_Y) 
=&- 
\mathbb{E}_{\Vec{y}\sim p_Y(\Vec{y})}[\log D_Y(\Vec{y})]\nonumber\\
&-\mathbb{E}_{\Vec{x}\sim p_X(\Vec{x})}[\log (1- D_Y(G(\Vec{x})))],
\label{eq:cyclegan-advloss_dy}
\\
\mathcal{L}_{\rm adv}^{G}(G) 
=&\mathbb{E}_{\Vec{x}\sim p_X(\Vec{x})}[\log (1- D_Y(G(\Vec{x})))],
\label{eq:cyclegan-advloss_g}
\\
\mathcal{L}_{\rm adv}^{D_X}(D_X) 
=&-
\mathbb{E}_{\Vec{x}\sim p_X(\Vec{x})}[\log D_X(\Vec{x})]\nonumber\\
&-
\mathbb{E}_{\Vec{y}\sim p_Y(\Vec{y})}[\log (1- D_X(F(\Vec{y})))],
\label{eq:cyclegan-advloss_dx}
\\
\mathcal{L}_{\rm adv}^{F}(F) 
=&
\mathbb{E}_{\Vec{y}\sim p_Y(\Vec{y})}[\log (1- D_X(F(\Vec{y})))],
\label{eq:cyclegan-advloss_f}

\end{aligned}
$$

as the adversarial losses for $D_Y$, $G$, $D_X$ and $F$, 
respectively.
$\mathcal{L}_{\rm adv}^{D_Y}(D_Y)$ 
and 
$\mathcal{L}_{\rm adv}^{D_X}(D_X) $
measure how indistinguishable 
$G(\Vec{x})$ and $F(\Vec{y})$ are 
from acoustic features of real speech belonging to $Y$ and $X$. 

Since the goal of $D_X$ and $D_Y$ is to correctly distinguish 
the converted feature sequences obtained via $G$ and $F$
from real speech feature sequences, 
$D_X$ and $D_Y$ attempt to minimize these losses to avoid being fooled by $G$ and $F$.

Conversely, since one of the goals of $G$ and $F$ is to generate realistic-sounding speech that is indistinguishable from real speech, $G$ and $F$ attempt to maximize these losses or minimize 
$\mathcal{L}_{\rm adv}^{G}(G)$ and $\mathcal{L}_{\rm adv}^{F}(F)$ to fool $D_Y$ and $D_X$.

It can be shown that the output distributions of $G$ and $F$ trained in this way will match the empirical distributions 
$p_Y(\Vec{y})$ and $p_X(\Vec{x})$. 
Note that since 
$\mathcal{L}_{\rm adv}^{G}(G)$ and $\mathcal{L}_{\rm adv}^{F}(F)$ 
are minimized when $D_Y(G(\Vec{x}))\simeq 1$ and $D_X(F(\Vec{y}))\simeq 1$,
we can also use 
$- \mathbb{E}_{\Vec{x}\sim p_X(\Vec{x})}[\log D_Y(G(\Vec{x}))]$
and 
$- \mathbb{E}_{\Vec{x}\sim p_X(\Vec{x})}[\log D_Y(G(\Vec{x}))]$ 
as the adversarial losses for $G$ and $F$.

As mentioned in \refsec{intro}, training $G$ and $F$ using only the adversarial losses does not guarantee that 
$G$ or $F$ will preserve the linguistic information of the input speech since there are infinitely many mappings 
that will induce the same output distributions. 
To further regularize these mappings, we introduce a cycle consistency loss 

$$
\begin{aligned}

\mathcal{L}_{\rm cyc}(G,F) 
&= \mathbb{E}_{\Vec{x}\sim p_X(\Vec{x})}[\| F(G(\Vec{x})) - \Vec{x} \|_1]
\nonumber\\
&+\mathbb{E}_{\Vec{y}\sim p_Y(\Vec{y})}[\| G(F(\Vec{y})) - \Vec{y} \|_1],

\end{aligned}
$$

to encourage $F(G(\Vec{x}))\simeq \Vec{x}$ and $G(F(\Vec{y}))\simeq \Vec{y}$. 
With the same motivation, we also consider an identity mapping loss

$$
\begin{aligned}

\mathcal{L}_{\rm id}(G,F) 
&= \mathbb{E}_{\Vec{x}\sim p_X(\Vec{x})}[\| F(\Vec{x}) - \Vec{x} \|_1]
\nonumber\\
&+\mathbb{E}_{\Vec{y}\sim p_Y(\Vec{y})}[\| G(\Vec{y}) - \Vec{y} \|_1],

\end{aligned}
$$

to ensure that inputs to $G$ and $F$ 
are kept unchanged when the inputs already belong to $Y$ and $X$.

The full objectives of CycleGAN-VC to be minimized with respect to 
$G$, $F$, $D_X$ and $D_Y$ 
are thus given as 

$$
\begin{aligned}

\mathcal{I}_{G,F}(G,F) =&
\mathcal{L}_{\rm adv}^{G}(G) +
\mathcal{L}_{\rm adv}^{F}(F) 
\nonumber\\
&+ 
\lambda_{\rm cyc}
\mathcal{L}_{\rm cyc}(G,F) + 
\lambda_{\rm id}
\mathcal{L}_{\rm id}(G,F),\\
\mathcal{I}_{D}(D_X,D_Y) =&
\mathcal{L}_{\rm adv}^{D_X}(D_X)
+
\mathcal{L}_{\rm adv}^{D_Y}(D_Y),

\end{aligned}
$$

where $\lambda_{\rm cyc}\ge 0$ and $\lambda_{\rm id}\ge 0$ are 
regularization parameters, which weigh the importance of 
the cycle consistency loss and the identity mapping loss relative to the adversarial losses.

![](cyclegan-vc.eps)

<a id="fig:cyclegan-vc">Concept of CycleGAN training.</a>
