# ExHuBERT

<details>
<summary>基本信息</summary>

- 标题: "ExHuBERT: Enhancing HuBERT Through Block Extension and Fine-Tuning on 37 Emotion Datasets"
- 作者:
  - 01 Shahin Amiriparian,
  - 02 Filip Packan,
  - 03 Maurice Gerczuk,
  - 04 Bjorn W.Schuller
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.10275)
  - [Publication](https://doi.org/10.21437/Interspeech.2024-280)
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2406.10275v1__ExHuBERT__Enhancing_HuBERT_through_Block_Extension_and_Fine-Tuning_on_37_Emotion_Datasets.pdf)
  - [Publication](_PDF/2406.10275p0__ExHuBERT__InterSpeech2024.pdf)

</details>

## Abstract: 摘要

Foundation models have shown great promise in speech emotion recognition (SER) by leveraging their pre-trained representations to capture emotion patterns in speech signals.
To further enhance SER performance across various languages and domains, we propose a novel twofold approach.
First, we gather EmoSet++, a comprehensive multi-lingual, multi-cultural speech emotion corpus with 37 datasets, 150,907 samples, and a total duration of 119.5 hours.
Second, we introduce ExHuBERT, an enhanced version of HuBERT achieved by backbone extension and fine-tuning on EmoSet++.
We duplicate each encoder layer and its weights, then freeze the first duplicate, integrating an extra zero-initialized linear layer and skip connections to preserve functionality and ensure its adaptability for subsequent fine-tuning.
Our evaluation on unseen datasets shows the efficacy of ExHuBERT, setting a new benchmark for various SER tasks.
Model and details on EmoSet++: [this https URL](https://huggingface.co/amiriparian/ExHuBERT).

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论