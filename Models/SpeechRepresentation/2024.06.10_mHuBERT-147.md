# mHuBERT-147

<details>
<summary>基本信息</summary>

- 标题: "mHuBERT-147: A Compact Multilingual HuBERT Model"
- 作者:
  - 01 Marcely Zanon Boito,
  - 02 Vivek Iyer,
  - 03 Nikolaos Lagos,
  - 04 Laurent Besacier,
  - 05 Ioan Calapodescu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2406.06371)
  - [Publication](https://doi.org/10.21437/Interspeech.2024-938)
  - [Github](https://github.com/utter-project/fairseq)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2406.06371v4__mHuBERT-147__A_Compact_Multilingual_HuBERT_Model.pdf)
  - [Publication](_PDF/2406.06371p0__mHuBERT-147__InterSpeech2024.pdf)

</details>

## Abstract: 摘要

We present ***mHuBERT-147***, the first general-purpose massively multilingual HuBERT speech representation model trained on 90K hours of clean, open-license data.
To scale up the multi-iteration HuBERT approach, we use faiss-based clustering, achieving 5.2x faster label assignment than the original method.
We also apply a new multilingual batching up-sampling strategy, leveraging both language and dataset diversity.
After 3 training iterations, our compact 95M parameter ***mHuBERT-147*** outperforms larger models trained on substantially more data.
We rank second and first on the ML-SUPERB 10min and 1h leaderboards, with SOTA scores for 3 tasks.
Across ASR/LID tasks, our model consistently surpasses XLS-R (300M params; 436K hours) and demonstrates strong competitiveness against the much larger MMS (1B params; 491K hours).
Our findings indicate that ***mHuBERT-147*** is a promising model for multilingual speech tasks, offering an unprecedented balance between high performance and parameter efficiency.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论