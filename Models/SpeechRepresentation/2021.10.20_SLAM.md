# SLAM

<details>
<summary>基本信息</summary>

- 标题: "SLAM: A Unified Encoder for Speech and Language Modeling via Speech-Text Joint Pre-Training"
- 作者:
  - 01 Ankur Bapna,
  - 02 Yu-an Chung,
  - 03 Nan Wu,
  - 04 Anmol Gulati,
  - 05 Ye Jia,
  - 06 Jonathan H.Clark,
  - 07 Melvin Johnson,
  - 08 Jason Riesa,
  - 09 Alexis Conneau,
  - 10 Yu Zhang
- 链接:
  - [ArXiv](https://arxiv.org/abs/2110.10329)
  - [Publication]
  - [Github]
  - [Demo]
- 文件:
  - [ArXiv](_PDF/2110.10329v1__SLAM__A_Unified_Encoder_for_Speech_and_Language_Modeling_via_Speech-Text_Joint_Pre-Training.pdf)
  - [Publication]

</details>

## Abstract: 摘要

<table><tr><td width="50%">

Unsupervised pre-training is now the predominant approach for both text and speech understanding.
Self-attention models pre-trained on large amounts of unannotated data have been hugely successful when fine-tuned on downstream tasks from a variety of domains and languages.
This paper takes the universality of unsupervised language pre-training one step further, by unifying speech and text pre-training within a single model.
We build a single encoder with the **BERT** objective on unlabeled text together with the **w2v-BERT** objective on unlabeled speech.
To further align our model representations across modalities, we leverage alignment losses, specifically Translation Language Modeling (TLM) and Speech Text Matching (STM) that make use of supervised speech-text recognition data.
We demonstrate that incorporating both speech and text data during pre-training can significantly improve downstream quality on CoVoST 2 speech translation, by around 1 BLEU compared to single-modality pre-trained models, while retaining close to SoTA performance on **LibriSpeech** and **SpeechStew** ASR tasks.
On four GLUE tasks and text-normalization, we observe evidence of capacity limitations and interference between the two modalities, leading to degraded performance compared to an equivalent text-only model, while still being competitive with BERT.
Through extensive empirical analysis we also demonstrate the importance of the choice of objective function for speech pre-training, and the beneficial effect of adding additional supervised signals on the quality of the learned representations.

</td><td>

无监督预训练现在已经成为文本和语音理解的主要方法.
在大量未标注数据上预训练的自注意力模型, 在针对来自各个领域和语言的下游任务进行微调时, 取得了巨大的成功.
本文进一步推进了无监督语言预训练的普遍性, 通过在单一模型中统一语音和文本预训练.
我们构建了一个单一编码器, 结合了未标注文本的 **BERT** 目标和未标注语音的 **w2v-BERT** 目标.
为了进一步对齐模态之间的模型表示, 我们利用对齐损失, 特别是翻译语言建模 (Translation Language Modeling, TLM) 和语音文本匹配 (Speech Text Matching, STM), 利用监督的语音文本识别数据.

我们证明在预训练期间结合语音和文本数据可以显著提高 CoVoST 2 语音翻译的下游质量, 与单一模态预训练模型相比, 大约提高了 1 个 BLEU, 同时在 **LibriSpeech** 和 **SpeechStew** ASR 任务上保持接近最先进的表现.
在四个 GLUE 任务和文本规范化任务中, 我们观察到容量限制和两种模态之间的干扰, 导致性能比同等仅文本模型有所下降, 但仍然与 BERT 相当.
通过广泛的经验分析, 我们还展示了语音预训练的目标函数的选择的重要性, 以及添加额外监督信号对学习表示质量的有益影响.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td></tr></table>
