# W2V-BERT

<details>
<summary>基本信息</summary>

- 标题: "W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training"
- 作者:
  - 01 Yu-An Chung,
  - 02 Yu Zhang,
  - 03 Wei Han,
  - 04 Chung-Cheng Chiu,
  - 05 James Qin,
  - 06 Ruoming Pang,
  - 07 Yonghui Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2108.06209)
  - [Publication](https://doi.org/10.1109/ASRU51503.2021.9688253)
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2108.06209v2__W2V-BERT__Combining_Contrastive_Learning_and_Masked_Language_Modeling_for_Self-Supervised_Speech_Pre-Training.pdf)
  - [Publication](_PDF/2108.06209v2__W2V-BERT__ASRU2021.pdf)

</details>

## Abstract: 摘要

Motivated by the success of masked language modeling~(MLM) in pre-training natural language processing models, we propose ***W2V-BERT*** that explores MLM for self-supervised speech representation learning.
***W2V-BERT*** is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens.
In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, ***W2V-BERT*** can be optimized in an end-to-end fashion by solving the two self-supervised tasks~(the contrastive task and MLM) simultaneously.
Our experiments show that ***W2V-BERT*** achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light~60k corpus as the unsupervised data.
In particular, when compared to published models such as conformer-based wav2vec~2.0 and HuBERT, our model shows~5\% to~10\% relative WER reduction on the test-clean and test-other subsets.
When applied to the Google's Voice Search traffic dataset, ***W2V-BERT*** outperforms our internal conformer-based wav2vec~2.0 by more than~30\% relatively.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论