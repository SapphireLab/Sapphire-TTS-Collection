# HuBERT

<details>
<summary>基本信息</summary>

- 标题: "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units"
- 作者:
  - 01 Wei-Ning Hsu
  - 02 Benjamin Bolte
  - 03 Yao-Hung Hubert Tsai
  - 04 Kushal Lakhotia
  - 05 Ruslan Salakhutdinov
  - 06 Abdelrahman Mohamed
- 链接:
  - [ArXiv](https://arxiv.org/abs/2106.07447)
  - [Publication](https://doi.org/10.1109/TASLP.2021.3122291)
  - [Github](https://github.com/facebookresearch/fairseq/tree/main/examples/hubert)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2106.07447v1__HuBERT__Self-Supervised_Speech_Representation_Learning_by_Masked_Prediction_of_Hidden_Units.pdf)
  - [Publication](_PDF/2106.07447p0__HuBERT__TASLP2021.pdf)

</details>

## Abstract: 摘要

<details>
<summary>原文</summary>

Self-supervised approaches for speech representation learning are challenged by three unique problems:
(1) there are multiple sound units in each input utterance,
(2) there is no lexicon of input sound units during the pre-training phase,
(3) sound units have variable lengths with no explicit segmentation.

To deal with these three problems, we propose the ***Hidden-Unit BERT (HuBERT)*** approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss.
A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs.
***HuBERT*** relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels.
Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the ***HuBERT*** model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets.
Using a 1B parameter model, ***HuBERT*** shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.

OpenSource: https://github.com/pytorch/fairseq/tree/master/examples/hubert

</details>
<br>

用于语音表示学习的自监督方法面临三个独特的问题:
1. 每个输入发言中有多个声学单元;
2. 在预训练阶段没有输入声学单元的词典;
3. 声学单元具有可变长度, 没有明确的分割.

为了处理这三个问题, 本文提出了 ***Hidden-Unit BERT (HuBERT)*** 方法用于自监督语音表示学习, 它利用离线聚类步骤为 BERT 类预测损失提供对齐的目标标签.
这一方法的关键组件是只对掩膜区域应用预测损失, 使得模型能够在连续类型输入上学习一个声学和语言相结合的模型.
***HuBERT*** 主要依赖于无监督聚类步骤的一致性, 而不是分配的聚类标签的内在质量.
以 100 个聚类教师开始, 使用两次聚类, ***HuBERT*** 模型可以匹配或超过 wav2vec 2.0 在 LibriSpeech (960h) 和 Libri-light (60,000h) 基准上分别进行 10min, 1h, 10h, 100h, 和 960h 微调子集的性能.
使用 1B 参数模型, ***HuBERT*** 展示了 19% 和 13% 的相对 WER 减少在更具挑战性的 dev-other 和 test-other 评估子集上.

## 1·Introduction: 引言

The north star for many research programs has been learning speech and audio representations through listening and interaction, similar to how babies learn their first language.
High fidelity speech representation includes disentangled aspects of the spoken content along with non-lexical information of how it is delivered, e.g., speaker identity, emotion, hesitation, interruptions.
Furthermore, reaching a complete situational understanding requires modeling structured noise interleaving and overlapping with the speech signal, e.g., laughter, coughing, lip-smacking, background vehicle engine, birds chirping, or food sizzling sounds.

The need for such high-fidelity representations drove research in self-supervised learning for speech and audio where the targets driving the learning process of a designed pretext task are drawn from the input signal itself.
Examples of pretext tasks for self-supervised speech representation learning include distinguishing near-by features from temporally distant ones \cite{oord2018representation,schneider2019wav2vec,kharitonov2020data}, next-step prediction of audio features \cite{chung2019unsupervised}, masked prediction of audio features given unmasked context \cite{baevski2019vq,baevski2020wav2vec}.
Besides, self-supervised learning methods do not rely on any linguistic resources during training, allowing them to learn universal representations since labels, annotations, and text-only material ignores rich information in the input signal.

Learning speech representations without reliance on large volumes of labeled data is crucial for industrial applications and products with ever-increasing coverage of new languages and domains.
The time needed to collect large labeled datasets covering each of these scenarios is the real bottleneck in the current fast-moving AI industry, with time-to-market playing a critical role for product success.
Building more inclusive applications covering spoken-only dialects and languages is another significant benefit of reducing dependence on linguistic resources.
Given their non-standard orthographic rules, many of these languages and dialects have very little or no resources at all.

Pseudo-labeling (PL), also known as self-training and belongs to the family of semi-supervised learning techniques, has been the dominant approach for utilizing unlabeled speech and audio with successful applications dating back to the mid-1990s \cite{Zavaliagkos_98, ma_bbn_06, kahn2020self, hsu2020semi}.
PL starts with some supervised data to train a "teacher" model in one specific downstream task.
Pseudo-labels are then generated for the unlabeled data using the teacher model.
Next, a student model is trained using the combined supervised and teacher-labeled data either using the standard cross-entropy \cite{kahn2020self} loss or using a contrastive loss \cite{xiao2021contrastive} to account for noise in teacher-generated labels.
The pseudo-labeling process may be repeated multiple times to improve teacher label quality \cite{xu2020iterative} iteratively.

Without discounting the immense success of pseudo-labeling techniques, self-supervised representations offer two unique advantages: (1) Pseudo-label methods force student models to merely mimic a teacher model, which is limited by its supervised data size and the provided annotation quality.
On the other hand, self-supervised pretext tasks force the model to represent the entire input signal by compressing much more bits of information into the learned latent representation.
(2) In pseudo-labeling, the supervised data of the teacher model forces the whole learning to be geared towards a single downstream task.
On the contrary, self-supervised features show better generalization to a multitude of downstream applications.

There have been impressive successes for self-supervised learning in Computer Vision (CV) \cite{caron2020Swav, Chen2020SimSiam, grill2020byol} and Natural Language Processing (NLP) \cite{brown2020gpt3, liu2019roberta, lewis2019bart} applications.
Learning representations of discrete input sequences, such as in Natural Language Processing (NLP) applications, uses either masked prediction \cite{devlin2018bert, clark2020electra} or auto-regressive generation \cite{peters2018deep, lewis2019bart} of input sequences with partial obfuscation.
For continuous inputs, such as in Computer Vision (CV) applications, representations are often learned through instance classification, in which each image and its augmentations are treated as a single output class to be pulled together \cite{Chen2020SimSiam, grill2020byol} or contrasted against other negative samples \cite{he2020momentum}.

Speech signals differ from text and images in that they are \textit{continuous-valued} \textit{sequences}.
Self-supervised learning for the speech recognition domain faces unique challenges from those in CV and NLP.
Firstly, the presence of multiple sounds in each input utterance breaks the instance classification assumption used in many CV pre-training approaches.
Secondly, during pre-training, there is no prior lexicon of discrete sound units available, as in NLP applications in which words or word pieces are used, hindering the use of predictive losses.
Lastly, the boundaries between sound units are not known, which complicates masked prediction pre-training.

In this paper, we introduce \textbf{H}idden \textbf{u}nit \textbf{BERT} (HuBERT) that benefits from an offline clustering step to generate noisy labels for a BERT-like per-training.
Concretely, a BERT model consumes masked continuous speech features to predict pre-determined cluster assignments.
The predictive loss is only applied over the masked regions, forcing the model to learn good high-level representations of unmasked inputs to infer the targets of masked ones correctly.
Intuitively, the HuBERT model is forced to learn both acoustic and language models from continuous inputs.
First, the model needs to model unmasked inputs into meaningful continuous latent representations, which maps to the classical acoustic modeling problem.
Second, to reduce the prediction error, the model needs to capture the long-range temporal relations between learned representations.
One crucial insight motivating this work is the importance of consistency of the targets, not just their correctness, which enables the model to focus on modeling the sequential structure of input data.
Our approach draws inspiration from the DeepCluster method for self-supervised visual learning \cite{caron2018deep}; however, HuBERT benefits from the masked prediction loss over speech sequences to represent their sequential structure.

When the HuBERT model is pre-trained on either the standard Librispeech 960h \cite{panayotov2015librispeech} or the Libri-Light 60k hours \cite{kahn2020libri}, it either matches or improves upon the state-of-the-art wav2vec 2.0 \cite{baevski2020wav2vec} performance on all fine-tuning subsets of 10mins, 1h, 10h, 100h, and 960h.
We present systematic results on three model sizes pre-trained with HuBERT: \textsc{Base} (90M parameters), \textsc{Large} (300M), and \textsc{X-Large} (1B).
The \textsc{X-Large} model shows up to 19\% and 13\% relative WER improvement from \textsc{Large} models on dev-other and test-other evaluation subsets when pre-trained on the Libri-Light 60k hours.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

### Learning the Hidden Units for HuBERT

An acoustic model trained on text and speech pairs provides pseudo-phonetic labels for each frame via forced alignment in semi-supervised learning.
On the contrary, the self-supervised representation learning setup has access to speech-only data.
Nevertheless, simple discrete latent variable models such as k-means and Gaussian mixture models (GMMs) infer hidden units that exhibit non-trivial correlation with the underlying acoustic units~\cite{lee2012nonparametric} (see also Table~\ref{tab:loss}).
More advanced systems can achieve better acoustic unit discovery performance using better graphical models \cite{ondel2016variational, ebbers2017hidden} or parameterizes the distributions with more powerful neural network models~\cite{hsu2017learning, hsu2017unsupervised, chorowski2019unsupervised, khurana2019factorial, khurana2020convolutional}.

Inspired by this, we propose to use acoustic unit discovery models to provide frame-level targets.
Let $X$ denote a speech utterance $X = [x_1, \cdots, x_T]$ of $T$ frames.
Discovered hidden units are denoted with $h(X) = Z = [z_1, \cdots, z_T]$, where $z_t \in [C]$ is a $C$-class categorical variable and $h$ is a clustering model, e.g.
k-means.

### Representation Learning via Masked Prediction

Let $M \subset [T]$ denote the set of indices to be masked for a length-$T$ sequence $X$, and $\tilde{X} = r(X, M)$ denote a corrupted version of $X$ where $x_t$ is replaced with a mask embedding $\tilde{x}$ if $t \in M$.
A masked prediction model $f$ takes as input $\tilde{X}$ and predicts a distribution over the target indeces at each timestep $p_f(\cdot \mid \tilde{X}, t)$.
There are two decisions to be made for masked prediction: \textit{how to mask} and \textit{where to apply the prediction loss}.

Regarding the first decision, we adopt the same strategies used in SpanBERT~\cite{joshi2020spanbert} and wav2vec 2.0~\cite{baevski2020wav2vec} for mask generation, where $p$\% of the timesteps are randomly selected as start indices, and spans of $l$ steps are masked.
To address the second decision, we denote the cross-entropy loss computed over masked and unmasked timesteps as $L_m$ and $L_u$, respectively.
$L_m$ is defined as:

$$
L_m(f; X, M, Z) = \sum_{t \in M} \log p_f(z_t \mid \tilde{X}, t),
$$

and $L_u$ is of the same form except that it sums over $t \not\in M$.
The final loss is computed as a weighted sum of the two terms: $L = \alpha L_m + (1-\alpha)L_u$.
In the extreme case when $\alpha = 0$, the loss is computed over the unmasked timesteps, which is similar to acoustic modeling in hybrid speech recognition systems \cite{young1996large, abdel2012applying, povey2005discriminative, bourlard2012connectionist}.
In our setup, this limits the learning process to mimicking the clustering model.

In the other extreme with $\alpha=1$, the loss is only computed over the masked timesteps where the model has to predict the targets corresponding to the unseen frames from context, analogous to language modeling.
It forces the model to learn both the acoustic representation of unmasked segments and the long-range temporal structure of the speech data.
We hypothesize that the setup with $\alpha=1$ is more resilient to the quality of cluster targets, which is demonstrated in our experiments (see Table~\ref{tab:loss}).

### Learning with Cluster Ensembles

A simple idea to improve target quality is to utilize multiple clustering models.
While an individual clustering model may perform terribly, cluster ensembles can provide complementary information to facilitate representation learning.
For example, an ensemble of k-means models with different codebook sizes can create targets of different granularity, from manner classes (vowel/consonant) to sub-phone states (senones).
To extend the proposed framework, let $Z^{(k)}$ be the target sequences generated by the $k$-th clustering model.
We can now re-write $L_m$ as:

$$
L_m(f; X, \{ Z^{(k)} \}_k, M) =\sum_{t \in M} \sum_{k} \log p_f^{(k)}(z_t^{(k)} \mid \tilde{X}, t)
$$

and similarly for the unmasked loss $L_u$.
This is analogous to multi-task learning, but with tasks created by unsupervised clustering.

Additionally, ensembling is intriguing because it can be used alongside product quantization (PQ)~\cite{gray1998quantization}, where a feature space is partitioned into multiple subspaces, and each subspace is quantized separately.
PQ allows effective Euclidean distance-based quantization such as k-means for high-dimensional features and heterogeneous features whose scale differs significantly between subspaces.
In this case, the theoretical size of the target space is the product of all codebooks' sizes.

### Iterative Refinement of Cluster Assignments

In addition to using cluster ensembles, another direction for improved representation is \textit{refining} the cluster assignments throughout the learning process.
Since we expect a pre-trained model to provide better representations than the raw acoustic feature such as MFCCs, we can create a new generation of clusters by training a discrete latent model over the learned latent representations.
The learning process then proceeds with the newly discovered units.

### Implementation

Our pre-trained models follows the wav2vec 2.0 architecture~\cite{baevski2020wav2vec}, with a convolutional waveform encoder, a BERT encoder~\cite{devlin2018bert}, a projection layer and a code embedding layer.
We consider HuBERT in three different configurations: \textsc{Base}, \textsc{Large}, and \textsc{X-Large}.
The fisrt two follow the architectures of wav2vec 2.0 \textsc{Base} and \textsc{Large} closely.
The \textsc{X-Large} architecture expands the model size to about 1 billion parameters, similar to the size of the Conformer XXL model in ~\cite{zhang2020pushing}.
The waveform encoder is identical for all the three configurations, which is composed of seven 512-channel layers with strides `[5,2,2,2,2,2,2]` and kernel widths `[10,3,3,3,3,2,2]`.
The BERT encoder consists of many identical transformer blocks, whose parameters along with the parameter of the subsequent projection layer are specified in Table~\ref{tab:arch}.

The convolutional waveform encoder generates a feature sequence at a 20ms framerate for audio sampled at 16kHz (CNN encoder down-sampling factor is 320x).
The audio encoded features are then randomly masked as described in Section \ref{sec:maskpred}.
The BERT encoder takes as input the masked sequence and outputs a feature sequence $[o_1, \cdots, o_T]$.
The distribution over codewords is parameterized with

$$
    p_f^{(k)}(c \mid \tilde{X}, t) = \frac{\exp(\text{sim}(A^{(k)} o_t, e_c) / \tau)} {\sum_{c'=1}^C \exp(\text{sim}(A^{(k)} o_t, e_{c'}) / \tau)},
$$

where $A$ is the projection matrix, $e_c$ is the embedding for codeword $c$, $\text{sim}(\cdot, \cdot)$ computes the cosine similarity between two vectors, and $\tau$ scales the logit, which is set to 0.1.
When cluster ensembles are used, one projection matrix $A^{(k)}$ is applied for each clustering model $k$.

After HuBERT pre-training, We use the connectionist temporal classification (CTC)~\cite{graves2006connectionist} loss for ASR fine-tuning of the whole model weights except the convolutional audio encoder, which remains frozen.
The projection layer(s) is removed and replaced with a randomly initialized softmax layer.
The CTC target vocabulary includes 26 English characters, a space token, an apostrophe, and a special CTC blank symbol.

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论

<details>
<summary>原文</summary>

This paper presents ***HuBERT***, a speech representation learning approach that relies on predicting K-means cluster assignments of masked segments of continuous input.
On both the Librispeech 960 hours and the 60,000 hours Libri-light pre-training setups, ***HuBERT*** matches or outperforms the state-of-the-art systems over all fine-tuning subsets of 10mins, 1h, 10h, 100h, and 960h.
Furthermore, the learned representation quality improves dramatically with iteratively refining K-means cluster assignments using learned latent representations for a previous iteration.
Finally, ***HuBERT*** scales well to a 1B transformer model showing a relative reduction in WER of up to 13% on the test-other subset.
For future work, we plan to improve the ***HuBERT*** training procedure to consist of a single phase.
Furthermore, given the high quality of its representations, we will consider using ***HuBERT*** pre-trained representations for multiple downstream recognition and generation tasks beyond ASR.

</details>
<br>

本文展示了 ***HuBERT***, 一种依赖于预测掩膜片段的连续输入的 K-means 聚类分配的语音表示学习方法.
在 LibriSpeech 960 小时和 Libri-light 60,000 小时的预训练设置上, ***HuBERT*** 超过了所有微调子集的最新系统.
此外, 学习到的表示质量随着迭代式的修正 K-means 聚类分配而显著提高.
最后, ***HuBERT*** 适用于 1B 变压器模型, 显示了 WER 的相对减少在 test-other 子集上最多 13%.
为了未来的工作, 我们计划改进 ***HuBERT*** 训练过程, 使其包含单个阶段.
此外, 由于其高质量的表示, 我们将考虑使用 ***HuBERT*** 预训练表示来进行多任务的语音识别和生成.
