# HuBERT

<details>
<summary>基本信息</summary>

- 标题: "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units"
- 作者:
  - 01 Wei-Ning Hsu
  - 02 Benjamin Bolte
  - 03 Yao-Hung Hubert Tsai
  - 04 Kushal Lakhotia
  - 05 Ruslan Salakhutdinov
  - 06 Abdelrahman Mohamed
- 链接:
  - [ArXiv](https://arxiv.org/abs/2106.07447)
  - [Publication](https://doi.org/10.1109/TASLP.2021.3122291)
  - [Github](https://github.com/facebookresearch/fairseq/tree/main/examples/hubert)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2106.07447v1__HuBERT__Self-Supervised_Speech_Representation_Learning_by_Masked_Prediction_of_Hidden_Units.pdf)
  - [Publication](_PDF/2106.07447p0__HuBERT__TASLP2021.pdf)

</details>

## Abstract: 摘要

<details>
<summary>原文</summary>

Self-supervised approaches for speech representation learning are challenged by three unique problems:
(1) there are multiple sound units in each input utterance,
(2) there is no lexicon of input sound units during the pre-training phase,
(3) sound units have variable lengths with no explicit segmentation.

To deal with these three problems, we propose the ***Hidden-Unit BERT (HuBERT)*** approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss.
A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs.
***HuBERT*** relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels.
Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the ***HuBERT*** model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets.
Using a 1B parameter model, ***HuBERT*** shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.

OpenSource: https://github.com/pytorch/fairseq/tree/master/examples/hubert

</details>
<br>

用于语音表示学习的自监督方法面临三个独特的问题:
1. 每个输入发言中有多个声学单元;
2. 在预训练阶段没有输入声学单元的词典;
3. 声学单元具有可变长度, 没有明确的分割.

为了处理这三个问题, 本文提出了 ***Hidden-Unit BERT (HuBERT)*** 方法用于自监督语音表示学习, 它利用离线聚类步骤为 BERT 类预测损失提供对齐的目标标签.
这一方法的关键组件是只对掩膜区域应用预测损失, 使得模型能够在连续类型输入上学习一个声学和语言相结合的模型.
***HuBERT*** 主要依赖于无监督聚类步骤的一致性, 而不是分配的聚类标签的内在质量.
以 100 个聚类教师开始, 使用两次聚类, ***HuBERT*** 模型可以匹配或超过 wav2vec 2.0 在 LibriSpeech (960h) 和 Libri-light (60,000h) 基准上分别进行 10min, 1h, 10h, 100h, 和 960h 微调子集的性能.
使用 1B 参数模型, ***HuBERT*** 展示了 19% 和 13% 的相对 WER 减少在更具挑战性的 dev-other 和 test-other 评估子集上.

## 1.Introduction: 引言

## 2.Related Works: 相关工作

## 3.Methodology: 方法

### 3.1.Learning the Hidden Units for HuBERT

### 3.2.Representation Learning via Masked Prediction

### 3.3.Learning with Cluster Ensembles

### 3.4.Iterative Refinement of Cluster Assignments

### 3.5.Implementation

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

<details>
<summary>原文</summary>

This paper presents ***HuBERT***, a speech representation learning approach that relies on predicting K-means cluster assignments of masked segments of continuous input.
On both the Librispeech 960 hours and the 60,000 hours Libri-light pre-training setups, ***HuBERT*** matches or outperforms the state-of-the-art systems over all fine-tuning subsets of 10mins, 1h, 10h, 100h, and 960h.
Furthermore, the learned representation quality improves dramatically with iteratively refining K-means cluster assignments using learned latent representations for a previous iteration.
Finally, ***HuBERT*** scales well to a 1B transformer model showing a relative reduction in WER of up to 13% on the test-other subset.
For future work, we plan to improve the ***HuBERT*** training procedure to consist of a single phase.
Furthermore, given the high quality of its representations, we will consider using ***HuBERT*** pre-trained representations for multiple downstream recognition and generation tasks beyond ASR.

</details>
<br>

本文展示了 ***HuBERT***, 一种依赖于预测掩膜片段的连续输入的 K-means 聚类分配的语音表示学习方法.
在 LibriSpeech 960 小时和 Libri-light 60,000 小时的预训练设置上, ***HuBERT*** 超过了所有微调子集的最新系统.
此外, 学习到的表示质量随着迭代式的修正 K-means 聚类分配而显著提高.
最后, ***HuBERT*** 适用于 1B 变压器模型, 显示了 WER 的相对减少在 test-other 子集上最多 13%.
为了未来的工作, 我们计划改进 ***HuBERT*** 训练过程, 使其包含单个阶段.
此外, 由于其高质量的表示, 我们将考虑使用 ***HuBERT*** 预训练表示来进行多任务的语音识别和生成.
