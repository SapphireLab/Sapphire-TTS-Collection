# MuQ

<details>
<summary>基本信息</summary>

- 标题: "MuQ: Self-Supervised Music Representation Learning with Mel Residual Vector Quantization"
- 作者:
  - 01 Haina Zhu (SJTU@X-LANCE Lab)
  - 02 Yizhi Zhou (Nanjing University)
  - 03 Hangting Chen (Tencent AI Lab)
  - 04 Jianwei Yu (Tencent AI Lab)
  - 05 Ziyang Ma (SJTU@X-LANCE Lab)
  - 06 Rongzhi Gu (Tencent AI Lab)
  - 07 Yi Luo (Tencent AI Lab)
  - 08 Wei Tan (Tencent AI Lab)
  - 09 Xie Chen (SJTU@X-LANCE Lab)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.01108)
  - [Publication]()
  - [Github](https://github.com/tencent-ailab/MuQ)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2501.01108v2__MuQ__Self-Supervised_Music_Representation_Learning_with_Mel_Residual_Vector_Quantization.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Recent years have witnessed the success of foundation models pre-trained with self-supervised learning (SSL) in various music informatics understanding tasks, including music tagging, instrument classification, key detection, and more.
In this paper, we propose a self-supervised music representation learning model for music understanding.
Distinguished from previous studies adopting random projection or existing neural codec, the proposed model, named MuQ, is trained to predict tokens generated by Mel Residual Vector Quantization (Mel-RVQ).
Our Mel-RVQ utilizes residual linear projection structure for Mel spectrum quantization to enhance the stability and efficiency of target extraction and lead to better performance.
Experiments in a large variety of downstream tasks demonstrate that MuQ outperforms previous self-supervised music representation models with only 0.9K hours of open-source pre-training data.
Scaling up the data to over 160K hours and adopting iterative training consistently improve the model performance.
To further validate the strength of our model, we present MuQ-MuLan, a joint music-text embedding model based on contrastive learning, which achieves state-of-the-art performance in the zero-shot music tagging task on the MagnaTagATune dataset.
Code and checkpoints are open source in [this https URL](https://github.com/tencent-ailab/MuQ).

## 1·Introduction: 引言

Self-supervised learning (SSL) has been introduced into speech and audio signal processing as a technique for learning latent semantic relationships from unlabeled raw data.
Recently, several works \cite{li2023mert, won2023musicfm} apply SSL to music informatics understanding, and a number of pre-trained foundation models (PFMs) with generalized representation capabilities are built.
Learning from unsupervised music data, these foundation models can achieve stunning performance in music understanding tasks such as genre classification, emotion prediction, and key detection \cite{yuan2023marble}, and further provide semantic representations for more downstream tasks like music generation \cite{agostinelli2023musiclm} and music captioning \cite{deng2024musilingo}, as illustrated in Figure \ref{fig:task_illu}.

A key challenge in music understanding and representation is that music is an extremely specific modality.
Unlike speech or environmental sounds, music not only focuses on semantic information, but also emphasizes acoustic information, such as melody, chords, and tonality.
As a result, previous semantics-oriented SSL methods have struggled to perform well on music tasks\cite{spijkervet2021clmr, li2022mapmusic2vec, yuan2023marble}, as they fail to simultaneously capture both semantic and acoustic information.

Recently, several studies have sought to develop a universal music representation that integrates both semantic and acoustic aspects.
Among these efforts, two remarkable models are MERT \cite{li2023mert} and MusicFM \cite{won2023musicfm}.
MERT employs a BERT-style masked language modeling (MLM) proxy task to predict discrete tokens from the masked audio parts, with an Encodec \cite{defossez2022encodec} model as the tokenizer and uses an auxiliary Constant Q-Transform (CQT) target to enhance the modeling of acoustic information.
MusicFM, on the other hand, directly utilizes a random projection quantizer derived from BEST-RQ \cite{chiu2022bestrq}, and this tokenization approach provides a general target for learning music representation, without the need for additional CQT loss to capture acoustic modeling.

As discussed in \cite{chiu2022bestrq} and \cite{li2023mert}, the target extractor (i.e., tokenizer) plays an important role in SSL, as models are trained to predict the tokenized pseudo-labels.
BEST-RQ features a lightweight approach that allows for fast extraction of discrete targets.
However, its performance is highly dependent on the initialization of the random projection layer, often requiring multiple attempts or a specific random seed to achieve optimal results.
In contrast, the Encodec target \cite{defossez2022encodec} used in MERT produces a series of residual targets, with the multi-target strategy shown beneficial to musical SSL \cite{li2023mert}.
As a neural codec trained on audio data, Encodec produces more stable labels compared to its random counterparts.
However, using Encodec as tokenization is computational heavy and consumes a lot of GPU memory when applying online extraction, which can reduce the training efficiency.
Also, it needs to be coupled with additional CQT reconstruction loss to perform well in acoustic representation.

To address the initialization dependency of the random projection quantizer in BEST-RQ and the inefficiency stemming from the heavy computation cost of Encodec in MERT, we introduce a model called MuQ, which learns \textbf{Mu}sic representations from Mel \textbf{Q}uantization targets.
MuQ leverages a Mel-RVQ as the tokenizer to generate targets.
The proposed Mel-RVQ is pre-trained on music data and employs a linear RVQ to directly quantize audio Mel spectrograms.
Compared to the random-projection quantizer in BEST-RQ, the pre-trained Mel-RVQ produces more stable targets for SSL training and eliminates the model's dependence on initialization.
Additionally, compared to Encodec, the lightweight single-layer Mel-RVQ architecture offers greater extraction efficiency.

To further demonstrate the capabilities of the proposed MuQ model, we explore its application in another crucial area of music understanding: aligning music and text representations.
For example, MuLan \cite{huang2022mulan} employs contrastive learning to train both a music encoder and a text encoder, producing semantically consistent embeddings for both modalities and achieving coherent alignment between music and text.
Recognizing the role that self-supervised learning models can play in providing effective initialization for downstream tasks, we leverage our MuQ to construct a joint music-text embedding model, named MuQ-MuLan.

Our main contributions are listed as follows:
- We introduce a novel music SSL model MuQ, which demonstrates state-of-the-art performance across a wide range of downstream music understanding tasks over previous MERT and MusicFM models.
- We propose the Mel Residual Vector Quantization (Mel-RVQ), which directly quantizes the Mel spectrum using a single linear layer RVQ, improving both training stability and efficiency.
- We further develop the MuQ-MuLan model, trained with contrastive learning on MuQ.
MuQ-MuLan excels in aligning and jointly encoding music and text modalities, compared with the original MuLan model.

Our experiments demonstrate that the Mel-RVQ significantly enhances SSL performance across a variety of music downstream tasks.
Notably, MuQ outperforms previous state-of-the-art (SOTA) SSL models MERT and MusicFM, even when trained on just 0.9K hours of data, which is 100x less than what comparable models require \cite{li2023mert, won2023musicfm}.
Additionally, our results show that MuQ-MuLan achieves a ROC-AUC score of 79.3 on the MagnaTagATune zero-shot music tagging task, surpassing the previous SOTA result\cite{huang2022mulan}.

## 2·Related Works: 相关工作

### Self-Supervised Learning for Speech and Audio

Self-supervised learning (SSL) \cite{mohamed2022self} has been introduced to the field of speech and audio signal processing as a way to learn semantic representations from unlabeled audio data, and it is now widely used in tasks such as automatic speech recognition \cite{baevski2023d2v2} and audio event classification \cite{chen2024eat}.

SSL often relies on well-designed targets.
HuBERT \cite{hsu2021hubert}, for instance, uses K-means clustering labels to guide self-supervised learning in Masked Language Model (MLM) style, with these labels being extracted offline.
BEST-RQ \cite{chiu2022bestrq} learns directly from targets generated by a randomly initialized projection quantizer, which can be extracted online.
MT4SSL \cite{ma2022mt4ssl} integrates both online and offline objectives, applying the concept of multi-target self-supervised learning, where a distinct linear prediction head is used for each type of target.

Another important topic in SSL is iterative refinement training.
Most existing iterative training practices are based on clustering.
For example, HuBERT applies K-means clustering on the features of an existing model to train the next iteration of models.
Similarly, Seed-ASR \cite{bai2024seedasr} first trains an SSL model on BEST-RQ and then iteratively trains on its features using K-means clustering.

In this paper, we take an alternative approach by iteratively training directly through the quantizer itself, without the need to apply K-means clustering.

### Music Processing and Understanding

Music information retrieval (MIR) contains a large set of tasks to evaluate deep learning models' understanding of music from audio signals or symbolized notation.
For example, automatic music tagging task require models to do multi-label classification for tags like genre, instrument, and mood, given a music track \cite{won2020tagging}.
Other MIR tasks include key detection, pitch detection, emotion analysis, etc \cite{yuan2023marble}.

In recent years, SSL has been introduced into music understanding for building universal representational models capable of handling various MIR tasks.
Some of these efforts include CLMR \cite{spijkervet2021clmr} based on contrastive learning; MERT \cite{li2023mert} based on MLM proxy task; and more recently, MusicFM \cite{won2023musicfm}.
These SSL-based models yield impressive performance across multiple MIR tasks, showing a generalized understanding of music.

### Music-Text Joint Embedding Model

Bridging the gap between music and text modalities, music-text representation models learn joint embeddings from music-text pairwise data.
Most of these models use a two-tower structure, consisting of both a music encoder and a text encoder, and are trained by contrastive learning loss.

LAION-CLAP \cite{laionclap2023} is a powerful open-source music-text encoder, with various model structures and versions trained on extensive data.
MuLan \cite{huang2022mulan} is also an exceptional music-text embedding model, but it is not open source for either the model or the data.

The MuQ-MuLan model introduced in this paper follows the ideas of former work but replaces the music encoder with our proposed MuQ model.

## 3·Methodology: 方法

MuQ employs a self-supervised learning approach based on masked language modeling (MLM) and tokenized targets.
The overall framework of MuQ is presented in Figure \ref{fig:structure}.

### Self-Supervised Framework of MuQ

MuQ directly takes the Mel spectrum of the music audio signal as input.
 The Mel spectrum is partially masked as random noise with a masking probability $p$, and then fed into multiple layers of Conformer \cite{gulati2020conformer} for context learning.
The Conformer output is passed through linear layers, which serve as prediction heads.
Finally, we calculate the cross-entropy loss between the target and predicted labels as the optimization objective.

The target labels are tokens extracted from the Mel spectrum by the Mel-RVQ, a quantization tokenizer proposed in this paper.
Since Mel-RVQ produces $N$ tokens for each time step of the Mel spectrum, the MuQ model incorporates $N$ distinct linear layers to predict the different target tokens.
In Figure \ref{fig:structure}, these multiple linear layers are illustrated as stacked blocks following the Conformer.

### Mel Residual Vector Quantization (Mel-RVQ)

As shown in Figure \ref{fig:projection}, we depict the proposed Mel Residual Vector Quantization (Mel-RVQ) compared with the random-projection quantizer in BEST-RQ and the Encodec target used by MERT.

The proposed Mel-RVQ directly takes the Mel spectrum as input and then quantizes the Mel spectrum using residual vector quantization (RVQ).
The encoder of Mel-RVQ is designed as a simple single-layer linear projection, and the decoder is also a single linear layer.

Mel-RVQ needs to be trained on music data in advance before it can be used to generate quantization targets in SSL training.

#### Training of Mel-RVQ

During the training of Mel-RVQ, the final loss function can be decomposed into three terms, as shown in Figure \ref{fig:projection}(c):

- **Codebook loss** trains the RVQ component to improve the fit of the codebook embedding $Q_\tau$ to the dimensionality-reduced features $z$.
Formally,
$$
loss_{\mathrm{code}} = \sum_{ \substack{x \in B, \\ z = M_\mathrm{P} x} } { \| \mathrm{norm}(Q_\tau) - \mathrm{norm}(\mathrm{sg}(z)) \| ^2}
$$
where $B$ refers to a mini-batch of data, $x$ denotes Mel spectrum input and $\mathrm{sg}$ denotes stop-gradient.
$M_\mathrm{P}$ is the projection matrix and token $\tau$ is the closest to the projected vector $z$ in the $l_2$-normalized embedding space.

- **Commitment loss** trains the projection (i.e., encoder) for more optimal dimension reduction and fitting to embeddings.
Expressed as
$$
loss_{\mathrm{comm}} = \sum_{ \substack{x \in B, \\ z = M_\mathrm{P} x} }{ \| \mathrm{norm}(z) - \mathrm{norm}(\mathrm{sg}(Q_\tau)) \| ^2}
$$

- **Reconstruction loss** trains the linear decoder (denoted as $M_\mathrm{D}$) and the codebook to restore the original feature $x$.
The formula is
$$
loss_{\mathrm{recon}} = \sum_{x \in B} { \| M_\mathrm{D} Q_\tau - x \| ^2}
$$


The final loss used to train the Mel-RVQ is the weighted sum of the above three losses:

$$
loss = \alpha \cdot loss_{\mathrm{code}} + \beta \cdot loss_{\mathrm{comm}} + loss_{\mathrm{recon}}
$$

where $\alpha$ and $\beta$ are weighting factors, set as $\alpha=1, \beta=0.25$.

It is emphasized that both \(M_\mathrm{D}\) and \(M_\mathrm{P}\) used in Eq.(\ref{eq:final_loss}) are simple linear layers, which distinguishes them from those in neural codecs like Encodec \cite{defossez2022encodec}.

#### Residual modeling of Mel-RVQ

Applying the residual modeling method to Mel-RVQ means that it yields multiple tokens for each time step of the audio.
Assume there are $N$ codebooks in total, denoted as $\{Q_i^{(n)}\}_{i=1}^K$ for $n \in 1\ldots N$.
In this way, we have

$$
\begin{aligned}
z^{(n)} &= M_\mathrm{P}^{(n)} r^{(n-1)} \\
\tau^{(n)} &= {\arg\min_{i} \| \mathrm{norm}(z^{(n)}) - \mathrm{norm}(Q^{(n)}_i) \|} \\
r^{(n)} &= r^{(n-1)} - M_\mathrm{D}^{(n)} Q^{(n)}_{\tau^{(n)}} \\
\end{aligned}
$$

where $n \in 1\ldots N$; $r^{(n)}$ is the residual signal passed to next step in quantizer and $r^{(1)} = x$.
$M_\mathrm{P}^{(n)}$ and $M_\mathrm{D}^{(n)}$ denote the residual-projection matrices for the encoder and decoder, which contain multiple steps of projection.
The superscript $(n)$ denotes the components or features corresponding to the quantizer at step $n$.

#### Iterative refinement with RVQ

Iterative refinement is introduced in HuBERT \cite{hsu2021hubert} as a method to improve the performance of SSL models, where clustering techniques like K-means are employed to produce new labels for the next iteration of training.

We suggest that residual vector quantization (RVQ) itself can serve as an alternative to the clustering in iterative enhancement.
That is, we directly train a Mel-RVQ\textsubscript{iter} on the latent representations of an already trained MuQ model, to re-drive the training of MuQ at next iteration.

Specifically, the iterative training on MuQ has two stages:

- In the initial stage, a Mel-RVQ is trained on the Mel spectrum feature, and then the first version of MuQ is trained on the tokens produced by Mel-RVQ.
- In the iterative stage, the first version of MuQ is used to extract representation from the audio, and the $l$ th layer latent is used to train Mel-RVQ\textsubscript{iter}, which will be used in training the second version of MuQ, namely MuQ\textsubscript{iter}.

### Music-Text Contrastive Learning

To verify the effectiveness of MuQ in more sophisticated downstream tasks, we trained a music-text joint embedding model, MuQ-MuLan.
Similar to MuLan \cite{huang2022mulan}, MuQ-MuLan employs a two-tower multimodal architecture and is trained on a large amount of (music, text) paired data.

MuQ-MuLan consists of a music encoder and a text encoder.
During training, the music and the corresponding text are fed to the encoders separately, where the text is a description of the music track, as shown in Figure \ref{fig:muq_mulan}.

For music modality inputs, a MuQ model with pre-trained parameters is first used to encode the audio into a latent representation.
This representation is then average-pooled over the temporal dimension.
Finally, it passes through a linear projection layer to produce a music embedding $e_\mathrm{m}$ with dimension $d$.

For text modality inputs, they are encoded by a pre-trained RoBERTa \cite{liu2019roberta} model.
As described in a previous study \cite{vasilakis2024canlisten}, the text encoder is critical to the performance of the two-tower model, so we append a few additional layers of Transformer encoder after RoBERTa.
Likewise, the textual latent representations are average-pooled and passed through a linear projection layer to get text embedding $e_\mathrm{t}$ with the same dimension $d$.

We use decoupled contrastive learning (DCL) loss \cite{yeh2022decoupled}.
Formally, given the $i$-th (music, text) pair in a mini-batch, for the embeddings $(e_\mathrm{m}^{(i)}, e_\mathrm{t}^{(i)})$, we have

$$
L_{\mathrm{DCL}} = - \mathrm{log} \sum_{(i, j)}\frac{\mathrm{sim}(e_\mathrm{m}^{(i)}, e_\mathrm{t}^{(j)}) / \sigma}{ \sum_{i \neq j} \mathrm{sim}(e_\mathrm{m}^{(i)}, e_\mathrm{t}^{(j)}) / \sigma}
$$

where $\mathrm{sim}$ denotes dot-product similarity, and $\sigma$ is temperature coefficient.

## 4·Experiments: 实验

### Model Settings

#### MuQ

We stack 12 layers of Conformer in MuQ, with 310M parameters in total.
The number of codebooks for Mel-RVQ is set to $N=8$ and the codebook size is $K=1024$.
The input audio is fixed at 30 seconds with a sampling rate of 24K Hz.
Audio inputs are 128-dimensional Mel spectrum, which are pooled to 25Hz before feeding to Conformer.
The code is implemented with the Fairseq toolkit \cite{ott2019fairseq}.

#### MuQ-MuLan

We use the pre-trained MuQ as the initialization for the music encoder.
A pre-trained multilingual RoBERTa model \texttt{xlm-roberta-base}
% \footnote{\url{https://huggingface.co/FacebookAI/xlm-roberta-base}}
is used for the text encoder followed by 8 layers of Transformer, with the embedding dimension $d=512$.
MuQ-MuLan has 630M parameters in total, and all parameters stay unfrozen during the contrastive training.

### Pre-Training

#### Dataset

We initially conduct pre-training on a smaller dataset, Music4all \cite{santana2020music4all}, containing 0.9K hours of open access music data.
Subsequently, we scale up to a large-scale in-house dataset with 160K hours of music.

#### Training Details

Mel-RVQ is trained on the Music4all dataset using 1 GPU.
Since the Mel-RVQ is very small (less than 0.3M), the training of Mel-RVQ is extremely fast and it takes less than 1 hour to complete.

All pre-training experiments are running with 32 A100-40G GPUs.
Batch size is 192, and masking probability $p = 0.6$.
For the pre-training on the Music4all dataset, we train for only 75K steps.
For the experiments on the 160K hours of in-house data, we initially train MuQ for 200K steps.
Subsequently, the $l=10$th layer latent is used to train $\mathrm{Mel\text{-}RVQ_{iter}}$, followed by 150K steps of iterative training from scratch to obtain $\mathrm{MuQ_{iter}}$.
The entire initial \& iterative pre-training takes about 2 weeks.

### Contrastive Training

#### Dataset

The training data for contrast learning comes from 130K hours of music in-house, each with its corresponding text description.
The text descriptions are limited to non-personal information such as genre, mood, instrument, tempo, keywords of the music.

#### Training Details

MuQ-Mulan is trained on 32 V100-32G GPUs with a batch size of 768.
Following the practice of MuLan \cite{huang2022mulan}, we switch to taking 10 seconds as the length of the input audio.
This significantly reduces memory consumption and enlarges batch size, which is critical for contrastive learning.

### Downstream Tasks

We evaluate MuQ on the MARBLE benchmark \cite{yuan2023marble} and conduct experiments on a total of \NumOfTask downstream tasks.
Note that all datasets appearing in the downstream tasks are \textbf{not} included in the pre-training data of MuQ.

Due to page limitations, we only provide a brief overview of the downstream tasks here.
We encourage readers to refer to the Appendix \uppercase\expandafter{\romannumeral1} for details on these tasks.

We present here a detailed description of 9 downstream tasks involved in the evaluation, with their respective datasets and evaluation metrics.
Most of the downstream tasks are based on the awesome MARBLE-benchmark \cite{yuan2023marble} implementation.

In evaluating the pre-trained MuQ, we extract features from each layer and use a linear probe to perform downstream tasks.
For the MuQ-MuLan experiments, we use a zero-shot approach for music tagging.
This involves directly calculating the similarity between music and text embeddings to determine the prediction probability for each tag.

#### Genre Classification

**Genre classification** aims to determine the most suitable genre for a song.
We use the GTZAN dataset \cite{Tzanetakis_Cook_2002} and report the accuracy scores.

Music Genre classification is an important tasks of Music Information Retrieval (MIR).
It aims to determine the most suitable genre for each song in dataset.
We use the GTZAN dataset \cite{Tzanetakis_Cook_2002} for this task.

The GTZAN dataset has a collection of 10 genres with 100 audio files each, all having a length of 30 seconds.
We set windows of 10-seconds as inputs to get the features respectively, then combine the results to get the features used for the downstream task.
We utilize the standard "fail-filtered" split \cite{Kereliuk_Sturm_Larsen_2015} for training and evaluating, then we provide the accuracy scores on testing set to show the final result.

#### Key Detection

**Key detection** predicts the tonal scale and dominant pitch level.
We use the Giantsteps dataset \cite{Knees_2015_GS}, and a refined accuracy metric \cite{raffel2014mir_eval}.

Key detection estimates the tonal scale and dominant pitch level of each given song.
Specifically, the labels of the task contains major and minor scales for all pitch classes, that is, 24 classes in total.

We use the Giantsteps dataset \cite{Knees_2015_GS}, which has a collection of annotations for 604 2-min audio previews of songs, as the test set, and a commonly-used subset of the Giantsteps-MTG-keys dataset \cite{Korzeniowski_Widmer_2017}, for training and validation.
We used the split method that was described in \cite{Castellon_Donahue_Liang_2021}.
Finally, we used the refined accuracy with error tolerance score \cite{raffel2014mir_eval} following Table \ref{tab:r_acc} to evaluate the performance of models.

#### Emotion Analysis

**Emotional analysis** is taken on the Emomusic dataset \cite{Soleymani_Caro_Schmidt_Sha_Yang_2013}.
The metrics are determination coefficients for valence ($\mathrm{R2^V}$) and arousal ($\mathrm{R2^A}$) score.

The Music Emotion Recognition aims to get the Emotion score of songs.
We use the Emomusic dataset \cite{Soleymani_Caro_Schmidt_Sha_Yang_2013}, which contains music clips of 45 seconds.
The dataset has 1000 clips in total.
After removing the duplicates, the size of the dataset is reduced to 744 songs.
Each clip is labeled with two scores: valence score indicates positive and negative emotional responses, and arousal score indicates emotional intensity.
The dataset split is same as \cite{Castellon_Donahue_Liang_2021}.We divide the whole signal into 5s windows as the inputs of SSL models to get the features, then combine the features as the inputs of a simple regression models.

At testing stage, we can get multiple predictions.
Each prediction of a given song is a pair of scores of valence score and arousal score, which is same as the ground truth.
We calculate the determination coefficient (R2) between the model regression results and human annotations of arousal (R2A) and valence (R2V) \cite{Soleymani_Caro_Schmidt_Sha_Yang_2013} as metrics for the test results.

#### Singer Identification

**Singer identification** on the VocalSet \cite{Wilkins_2018_vocalset}
dataset.
The evaluation metric is accuracy.

Singer identification is to recognize the corresponding singer based on the given singing audio.

We use the Vocalset dataset \cite{Wilkins_2018_vocalset}, which contains 20 different singers (11 male and 9 female) who perform 17 different singing techniques in various contexts.
We randomly split the dataset into traning,validation and testing sets based on a ratio of 12:8:5, all of which contain the full 20 singers.
The evaluation metric is accuracy.

#### Vocal Technique Detection

**Vocal technique detection** on the VocalSet \cite{Wilkins_2018_vocalset}.
The evaluation metric is accuracy.

Vocal technique detection is to identify the techniques that the singer used in a given audio segment.
We use the VocalSet dataset for this task.
As the audio clips are divided into 3 seconds, the task only requires a judgement on the type of technique and not on the start and end of the technique.
We used the same 10 different singing techniques as in \cite{Yamamoto_Nam_Terasawa} as a subset and used the same 15 singers as the training and validation sets and 5 singers as the test set.
Due to the lack of a standardized division between training and validation sets, we opted for a selection of 9 singers as the training set and 6 singers as the validation set.
All segments originating from the same recording are assigned to the same portion of the split.
For evaluation purposes, we employ accuracy as the metric in this classification task.

#### Music Tagging

**Music tagging** involves multi-label classification of 50 tags on the MagnaTagATune \cite{law2009mtt} dataset.
The metrics are ROC-AUC and average precision (AP).

Music tagging assigns a set of fixed tags for a given song.
Tag contains genre, instrumentation, mood and so on.
We use the MagnaTagATune (MTT) \cite{Law_West_Mandel_Bay_Downie_2009} dataset.
MTT dataset contains 25,863 music clips.
Each clip is a 29-seconds-long excerpt belonging to one of the 5223 songs, 445 albums and 230 artists.
 The clips span a broad range of genres like Classical, Rock, Pop, Jazz, Blues and more.
Each audio clip is labeled by human with a vector of binary annotations of 188 tags.
The annotations include tags like "singer", "no singer", "violin", "drums", "classical", "jazz".
The top 50 most popular tags are typically used for evaluation to ensure that there is enough training data for each tag.

For both datasets, we limit the tag vocabulary according to official instructions.
We use all clips in MTT.
The metrics are the macro-average of ROC-AUCs, a multiclass AUC as the average of the area under multiple binary ROC curves, and the average precision (AP) / PR-AUC, the weighted mean of precisions at each threshold, among all top-50 tags.

#### Music Structure Analysis

**Music structure analysis** predicts the functional label for music segments on the Harmonix dataset \cite{nieto2019harmonix}.
The evaluation metric is accuracy.

Music structure analysis aims to split a music signal into non-overlapping sections and predict the functional label for each segment, including "intro',"verse',"chorus' and so on.
We use the Harmonix Set \cite{nieto2019harmonix} for the experiment.
This is a frame-wise classification task.
We first construct a probability curve based on the given data annotations, where each frame contains a boundary score and a function score.

To standardize the labels, we mapped the labels in the data following the setting in \cite{Wang_Hung_Smith_2022}.
Then, to ensure smooth training of the model, we create smoothed function and boundary activation curves.
We smooth the transitions of the function activation curves using a Hanning window: a 1-second ramp from 0 to 1 prior to the onset, and a 1-second ramp down after the offset, as in \cite{Wang_Smith_Chen_Song_Wang_2021}.
For boundary curves, we also set a duration of 0.6 seconds for each boundary using a Hanning window.

We use a simple LSTM model to predict frame-wise probabities of functional and boundary curves.
Frame-wise accuracy for functional labels is used as metric.

#### Instrument Classification

**Instrument classification** on the Nsynth dataset \cite{Engel_2017_NSynth}.
The evaluation metric is accuracy.

Instrument classification identify which instruments are used in a given music segment.
We use the NSynth dataset \cite{Engel_2017_NSynth} and MTG-instrument dataset.The NSynth Dataset is an audio dataset containing ~300k musical notes, each with a unique pitch, timbre, and envelope.
Each note is annotated with three additional pieces of information based on a combination of human evaluation and heuristic algorithms: Source, Family, and Qualities.
Each segment may contain multiple instruments.
We evaluate the model using ROC and AP scores.

#### Pitch Classification

**Pitch classification** of the 128-pitch on the Nsynth dataset \cite{Engel_2017_NSynth}.
The metric is accuracy.

Pitch classification aims to detect the certain pitch of a given short audio clip in 128 pitch categories.
We use the NSynth dataset \cite{Engel_2017_NSynth} for this task, then we use accuracy score to evaluate the performance.

## 5·Results: 结果

### Performance of Pre-trained MuQ

We compare MuQ with two popular models, MERT and MusicFM.
For MERT, we use \texttt{MERT-v1-330M} version \cite{li2023mert}, the most superior of its releases on HuggingFace. % \footnote{\url{https://huggingface.co/m-a-p/MERT-v1-330M}}
For MusicFM, we use the checkpoint pre-trained on the MSD dataset \cite{bertin2011msd}, which is publicly available by \cite{won2023musicfm}.
All models are of around 330M parameters.

Table \ref{tab:marble_res1} and Table \ref{tab:marble_res2} present the results for three versions of MuQ.
Considering the experiments are very extensive and time-consuming, we conduct 5 runs for MuQ models and report the mean scores and standard deviations for all tasks.

It is shown that for MuQ\textsubscript{m4a}, pre-trained on the 0.9K-hour Music4All dataset, already outperforms MERT and MusicFM.
Training MuQ on a larger dataset further amplifies this advantage.
Iterative training slightly improves the effectiveness of MuQ, resulting in MuQ\textsubscript{iter} with an average score of 77.0, which is our best-performing model.

MuQ outperforms previous models on many downstream tasks, especially in genre classification, singer identification, vocal technique detection, music structure analysis and instrument classification.
We visualize the result of MuQ in Figure \ref{fig:radar}.

### Ablation Study on Mel-RVQ

We carry out an ablation study for Mel-RVQ in Table \ref{tab:comp_abl} to demonstrate the effect of its key components.
In the ablation experiments, all models are pre-trained 75K steps on the Music4all with 310M parameters, and tested on 5 tasks.

The VQ type in Table \ref{tab:comp_abl} indicates whether the VQ is randomly initialized or trained on music data, and Residual indicates whether residual multiple codebooks are used.
As shown, it's crucial to train the VQ rather than leaving it random, as this boosts the performance of MusicFM and MuQ by 1.6 and 2.8, respectively.
Also, the residual structure of Mel-RVQ is beneficial and allows MuQ to gain a significant advantage over MusicFM.

### Ablation Study on Music SSL Target

We ablate the number of target tokens and tokenization SSL training, this involves some hyperparameters of Mel-RVQ, as well as a comparison with Encodec target.

As seen in Table \ref{tab:target_abl}, performance gradually improves as the number of codebooks increases.
We believe this is because more codebooks allow the model to capture fine-grained audio features more effectively.
 This also implies the advantage of multi-target learning for SSL pre-training.

We also explore the depth of the Mel-R\textbf{}VQ encoder and decoder by scaling up the linear projection to a 3-layer fully connected network, referred to as Mel-RVQ (deeper) in Table \ref{tab:target_abl}.
However, the results suggest that increasing the depth of the Mel-RVQ network actually degrades performance.
This indicates that a single layer of linear projection is sufficient for Mel-RVQ to achieve optimal performance.

Finally, we compare Mel-RVQ with the Encodec target.
 It can be seen in Table \ref{tab:target_abl} that Mel-RVQ, despite being a very lightweight structure, outperforms the Encodec tokenization target.
We attribute this to the fact that the token produced by Encodec is a high abstraction of the music audio, which leads to it being more difficult for the model to learn.

### Performance of MuQ-MuLan

In Table \ref{tab:mulan} , we compare MuQ-MuLan with LAION-CLAP \cite{laionclap2023}, Microsoft-CLAP 2023\cite{CLAP2023} and MuLan \cite{huang2022mulan}.
We use the open-source \texttt{larger\_clap\_music\_and\_speech} version for LAION-CLAP, which is the best-performing checkpoint among its releases in our experiments.
We also introduce two variants that employ MERT or MusicFM as the music encoder and refer to them as MERT-MuLan and MusicFM-MuLan.

Following previous study \cite{vasilakis2024canlisten}, we choose the proven superior MusCALL prompt: "A \textless label\textgreater \: track" for all models to test in our experiments.

MuQ-MuLan outperforms previous models in both ROC-AUC and PR-AUC and reaches the SOTA in the zero-shot setting.
Impressively, MuQ-MuLan still has a slight advantage over MuLan despite the gap in data amounts.

Although both use the same contrastive learning setup, MuQ-MuLan outperforms MERT-MuLan.
Furthermore, even though MuQ and MusicFM have identical model structures, MuQ-MuLan still performs better.
This suggests that MuQ provides a better initialization for contrastive training.

We visualize the performance of MuQ-MuLan in Figure \ref{fig:tagging}, and plot the comparision with LAION-CLAP \cite{laionclap2023}, Microsoft-CLAP 2023 \cite{CLAP2023} and MuLan \cite{huang2022mulan}.

### Layer-Wise Analysis

In Figure \ref{fig:task_layer}, we visualize the results of each layer's features in various downstream tasks during one run of the MuQ model.
The analysis can be summarized as follows:

- **Semantic tasks**, such as genre classification and structure analysis, generally achieve the best results at the higher layers.
They focus more on semantic information such as the direction of the music and the overall style.
- **Acoustic tasks**, such as singer identification, instrument classification, pitch classification, and key detection, perform best at the lower layers.
This implies that they are more closely associated with low-level characteristics such as the original audio features.
- **Comprehensive tasks**, such as music tagging, emotional analysis, and vocal tech classification, have a relatively even distribution across all layers.
This suggests that they rely on various aspects.

### Discussion: 讨论

#### Additional Explanation of Mel-RVQ

Although in this paper, we only discuss the case of Mel-RVQ with up to 8 codebooks, this does not mean that more codebooks are meaningless.
Exploring Mel-RVQ with more codebooks may help understand the reasons why Mel-RVQ works and the limits of this advantage.
The selection of layer 10 in iterative training is based on empirical intuition rather than rigorous experimental comparison.
Due to limited computational resources, we did not explore other layers.
We believe that iterating over other layers could potentially lead to better results, and this is a worthwhile topic of future research.

#### Potential Role of Virtual Classes in SSL

The concept of virtual classes, introduced in \cite{chen2018virtual}, suggests that adding dummy classes to classification tasks can improve performance.
We note that in fact the random projection quantizer in BEST-RQ, while having low codebook utilization, those codes that are never hit may in fact provide a role similar to that of virtual classes.
In addition, we also observe in our early experiments with Mel-RVQ that virtual classes have a potential effect on the SSL training.
Although our approach does not involve virtual classes, we believe their role in SSL merits further investigation.

#### Limitation and Future Work

Mel-RVQ is required to be pre-trained on music data before being used for SSL targeting.
While we emphasize that the training process for Mel-RVQ is very fast (less than 1 hour), this is still not a completely out-of-the-box approach like BEST-RQ.
In future work, we will also explore different model sizes (e.g., 90M $base$ size) for the requirements of different physical environments and computational conditions.

## 6·Conclusions: 结论

In this study, we introduce MuQ, a novel self-supervised learning model for music representation and understanding.
Building on this, we also present MuQ-MuLan, a joint music-text embedding model.
MuQ uses Mel-RVQ as a quantization target, applying a linear projection RVQ structure to the Mel spectrum, which provides a simple yet powerful SSL target.
Our model shows significant improvements over previous models like MERT and MusicFM, as demonstrated by its performance on the MARBLE benchmark.
Additionally, experiments on the zero-shot music tagging task reveal that MuQ-MuLan achieves state-of-the-art music-text modal alignment.
Our model code and checkpoints will be made open-source to support future research.
