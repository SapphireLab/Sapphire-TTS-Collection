# MuQ

<details>
<summary>基本信息</summary>

- 标题: "MuQ: Self-Supervised Music Representation Learning with Mel Residual Vector Quantization"
- 作者:
  - 01 Haina Zhu (SJTU@X-LANCE Lab)
  - 02 Yizhi Zhou (Nanjing University)
  - 03 Hangting Chen (Tencent AI Lab)
  - 04 Jianwei Yu (Tencent AI Lab)
  - 05 Ziyang Ma (SJTU@X-LANCE Lab)
  - 06 Rongzhi Gu (Tencent AI Lab)
  - 07 Yi Luo (Tencent AI Lab)
  - 08 Wei Tan (Tencent AI Lab)
  - 09 Xie Chen (SJTU@X-LANCE Lab)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.01108)
  - [Publication]()
  - [Github](https://github.com/tencent-ailab/MuQ)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2501.01108v2__MuQ__Self-Supervised_Music_Representation_Learning_with_Mel_Residual_Vector_Quantization.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Recent years have witnessed the success of foundation models pre-trained with self-supervised learning (SSL) in various music informatics understanding tasks, including music tagging, instrument classification, key detection, and more.
In this paper, we propose a self-supervised music representation learning model for music understanding.
Distinguished from previous studies adopting random projection or existing neural codec, the proposed model, named MuQ, is trained to predict tokens generated by Mel Residual Vector Quantization (Mel-RVQ).
Our Mel-RVQ utilizes residual linear projection structure for Mel spectrum quantization to enhance the stability and efficiency of target extraction and lead to better performance.
Experiments in a large variety of downstream tasks demonstrate that MuQ outperforms previous self-supervised music representation models with only 0.9K hours of open-source pre-training data.
Scaling up the data to over 160K hours and adopting iterative training consistently improve the model performance.
To further validate the strength of our model, we present MuQ-MuLan, a joint music-text embedding model based on contrastive learning, which achieves state-of-the-art performance in the zero-shot music tagging task on the MagnaTagATune dataset.
Code and checkpoints are open source in [this https URL](https://github.com/tencent-ailab/MuQ).

## 1·Introduction: 引言

Self-supervised learning (SSL) has been introduced into speech and audio signal processing as a technique for learning latent semantic relationships from unlabeled raw data.
Recently, several works \cite{li2023mert, won2023musicfm} apply SSL to music informatics understanding, and a number of pre-trained foundation models (PFMs) with generalized representation capabilities are built.
Learning from unsupervised music data, these foundation models can achieve stunning performance in music understanding tasks such as genre classification, emotion prediction, and key detection \cite{yuan2023marble}, and further provide semantic representations for more downstream tasks like music generation \cite{agostinelli2023musiclm} and music captioning \cite{deng2024musilingo}, as illustrated in Figure \ref{fig:task_illu}.

A key challenge in music understanding and representation is that music is an extremely specific modality.
Unlike speech or environmental sounds, music not only focuses on semantic information, but also emphasizes acoustic information, such as melody, chords, and tonality.
As a result, previous semantics-oriented SSL methods have struggled to perform well on music tasks\cite{spijkervet2021clmr, li2022mapmusic2vec, yuan2023marble}, as they fail to simultaneously capture both semantic and acoustic information.

Recently, several studies have sought to develop a universal music representation that integrates both semantic and acoustic aspects.
Among these efforts, two remarkable models are MERT \cite{li2023mert} and MusicFM \cite{won2023musicfm}.
MERT employs a BERT-style masked language modeling (MLM) proxy task to predict discrete tokens from the masked audio parts, with an Encodec \cite{defossez2022encodec} model as the tokenizer and uses an auxiliary Constant Q-Transform (CQT) target to enhance the modeling of acoustic information.
MusicFM, on the other hand, directly utilizes a random projection quantizer derived from BEST-RQ \cite{chiu2022bestrq}, and this tokenization approach provides a general target for learning music representation, without the need for additional CQT loss to capture acoustic modeling.

As discussed in \cite{chiu2022bestrq} and \cite{li2023mert}, the target extractor (i.e., tokenizer) plays an important role in SSL, as models are trained to predict the tokenized pseudo-labels.
BEST-RQ features a lightweight approach that allows for fast extraction of discrete targets.
However, its performance is highly dependent on the initialization of the random projection layer, often requiring multiple attempts or a specific random seed to achieve optimal results.
In contrast, the Encodec target \cite{defossez2022encodec} used in MERT produces a series of residual targets, with the multi-target strategy shown beneficial to musical SSL \cite{li2023mert}.
As a neural codec trained on audio data, Encodec produces more stable labels compared to its random counterparts.
However, using Encodec as tokenization is computational heavy and consumes a lot of GPU memory when applying online extraction, which can reduce the training efficiency.
Also, it needs to be coupled with additional CQT reconstruction loss to perform well in acoustic representation.

To address the initialization dependency of the random projection quantizer in BEST-RQ and the inefficiency stemming from the heavy computation cost of Encodec in MERT, we introduce a model called MuQ, which learns \textbf{Mu}sic representations from Mel \textbf{Q}uantization targets.
MuQ leverages a Mel-RVQ as the tokenizer to generate targets.
The proposed Mel-RVQ is pre-trained on music data and employs a linear RVQ to directly quantize audio Mel spectrograms.
Compared to the random-projection quantizer in BEST-RQ, the pre-trained Mel-RVQ produces more stable targets for SSL training and eliminates the model's dependence on initialization.
Additionally, compared to Encodec, the lightweight single-layer Mel-RVQ architecture offers greater extraction efficiency.

To further demonstrate the capabilities of the proposed MuQ model, we explore its application in another crucial area of music understanding: aligning music and text representations.
For example, MuLan \cite{huang2022mulan} employs contrastive learning to train both a music encoder and a text encoder, producing semantically consistent embeddings for both modalities and achieving coherent alignment between music and text.
Recognizing the role that self-supervised learning models can play in providing effective initialization for downstream tasks, we leverage our MuQ to construct a joint music-text embedding model, named MuQ-MuLan.

Our main contributions are listed as follows:
- We introduce a novel music SSL model MuQ, which demonstrates state-of-the-art performance across a wide range of downstream music understanding tasks over previous MERT and MusicFM models.
- We propose the Mel Residual Vector Quantization (Mel-RVQ), which directly quantizes the Mel spectrum using a single linear layer RVQ, improving both training stability and efficiency.
- We further develop the MuQ-MuLan model, trained with contrastive learning on MuQ.
MuQ-MuLan excels in aligning and jointly encoding music and text modalities, compared with the original MuLan model.

Our experiments demonstrate that the Mel-RVQ significantly enhances SSL performance across a variety of music downstream tasks.
Notably, MuQ outperforms previous state-of-the-art (SOTA) SSL models MERT and MusicFM, even when trained on just 0.9K hours of data, which is 100x less than what comparable models require \cite{li2023mert, won2023musicfm}.
Additionally, our results show that MuQ-MuLan achieves a ROC-AUC score of 79.3 on the MagnaTagATune zero-shot music tagging task, surpassing the previous SOTA result\cite{huang2022mulan}.

## 2·Related Works: 相关工作

### Self-Supervised Learning for Speech and Audio

Self-supervised learning (SSL)  \cite{mohamed2022self} has been introduced to the field of speech and audio signal processing as a way to learn semantic representations from unlabeled audio data, and it is now widely used in tasks such as automatic speech recognition \cite{baevski2023d2v2} and audio event classification \cite{chen2024eat}.

SSL often relies on well-designed targets.
HuBERT \cite{hsu2021hubert}, for instance, uses K-means clustering labels to guide self-supervised learning in Masked Language Model (MLM) style, with these labels being extracted offline.
BEST-RQ \cite{chiu2022bestrq} learns directly from targets generated by a randomly initialized projection quantizer, which can be extracted online.
MT4SSL \cite{ma2022mt4ssl} integrates both online and offline objectives, applying the concept of multi-target self-supervised learning, where a distinct linear prediction head is used for each type of target.

Another important topic in SSL is iterative refinement training.
Most existing iterative training practices are based on clustering.
For example, HuBERT applies K-means clustering on the features of an existing model to train the next iteration of models.
Similarly, Seed-ASR \cite{bai2024seedasr} first trains an SSL model on BEST-RQ and then iteratively trains on its features using K-means clustering.

In this paper, we take an alternative approach by iteratively training directly through the quantizer itself, without the need to apply K-means clustering.

### Music Processing and Understanding

Music information retrieval (MIR) contains a large set of tasks to evaluate deep learning models' understanding of music from audio signals or symbolized notation.
For example, automatic music tagging task require models to do multi-label classification for tags like genre, instrument, and mood, given a music track \cite{won2020tagging}.
Other MIR tasks include key detection,  pitch detection, emotion analysis, etc \cite{yuan2023marble}.

In recent years, SSL has been introduced into music understanding for building universal representational models capable of handling various MIR tasks.
Some of these efforts include CLMR \cite{spijkervet2021clmr} based on contrastive learning; MERT \cite{li2023mert} based on MLM proxy task; and more recently, MusicFM \cite{won2023musicfm}.
These SSL-based models yield impressive performance across multiple MIR tasks, showing a generalized understanding of music.

### Music-Text Joint Embedding Model

Bridging the gap between music and text modalities, music-text representation models learn joint embeddings from music-text pairwise data.
Most of these models use a two-tower structure, consisting of both a music encoder and a text encoder, and are trained by contrastive learning loss.

LAION-CLAP \cite{laionclap2023} is a powerful open-source music-text encoder, with various model structures and versions trained on extensive data.
MuLan \cite{huang2022mulan} is also an exceptional music-text embedding model, but it is not open source for either the model or the data.

The MuQ-MuLan model introduced in this paper follows the ideas of former work but replaces the music encoder with our proposed MuQ model.

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论

In this study, we introduce MuQ, a novel self-supervised learning model for music representation and understanding.
Building on this, we also present MuQ-MuLan, a joint music-text embedding model.
MuQ uses Mel-RVQ as a quantization target, applying a linear projection RVQ structure to the Mel spectrum, which provides a simple yet powerful SSL target.
Our model shows significant improvements over previous models like MERT and MusicFM, as demonstrated by its performance on the MARBLE benchmark.
Additionally, experiments on the zero-shot music tagging task reveal that MuQ-MuLan achieves state-of-the-art music-text modal alignment.
Our model code and checkpoints will be made open-source to support future research.
