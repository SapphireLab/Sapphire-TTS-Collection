# MuQ

<details>
<summary>基本信息</summary>

- 标题: "MuQ: Self-Supervised Music Representation Learning with Mel Residual Vector Quantization"
- 作者:
  - 01 Haina Zhu (SJTU@X-LANCE Lab)
  - 02 Yizhi Zhou (Nanjing University)
  - 03 Hangting Chen (Tencent AI Lab)
  - 04 Jianwei Yu (Tencent AI Lab)
  - 05 Ziyang Ma (SJTU@X-LANCE Lab)
  - 06 Rongzhi Gu (Tencent AI Lab)
  - 07 Yi Luo (Tencent AI Lab)
  - 08 Wei Tan (Tencent AI Lab)
  - 09 Xie Chen (SJTU@X-LANCE Lab)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.01108)
  - [Publication]()
  - [Github](https://github.com/tencent-ailab/MuQ)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2501.01108v2__MuQ__Self-Supervised_Music_Representation_Learning_with_Mel_Residual_Vector_Quantization.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Recent years have witnessed the success of foundation models pre-trained with self-supervised learning (SSL) in various music informatics understanding tasks, including music tagging, instrument classification, key detection, and more.
In this paper, we propose a self-supervised music representation learning model for music understanding.
Distinguished from previous studies adopting random projection or existing neural codec, the proposed model, named MuQ, is trained to predict tokens generated by Mel Residual Vector Quantization (Mel-RVQ).
Our Mel-RVQ utilizes residual linear projection structure for Mel spectrum quantization to enhance the stability and efficiency of target extraction and lead to better performance.
Experiments in a large variety of downstream tasks demonstrate that MuQ outperforms previous self-supervised music representation models with only 0.9K hours of open-source pre-training data.
Scaling up the data to over 160K hours and adopting iterative training consistently improve the model performance.
To further validate the strength of our model, we present MuQ-MuLan, a joint music-text embedding model based on contrastive learning, which achieves state-of-the-art performance in the zero-shot music tagging task on the MagnaTagATune dataset.
Code and checkpoints are open source in [this https URL](https://github.com/tencent-ailab/MuQ).

## 1·Introduction: 引言

Self-supervised learning (SSL) has been introduced into speech and audio signal processing as a technique for learning latent semantic relationships from unlabeled raw data.
Recently, several works \cite{li2023mert, won2023musicfm} apply SSL to music informatics understanding, and a number of pre-trained foundation models (PFMs) with generalized representation capabilities are built.
Learning from unsupervised music data, these foundation models can achieve stunning performance in music understanding tasks such as genre classification, emotion prediction, and key detection \cite{yuan2023marble}, and further provide semantic representations for more downstream tasks like music generation \cite{agostinelli2023musiclm} and music captioning \cite{deng2024musilingo}, as illustrated in Figure \ref{fig:task_illu}.

A key challenge in music understanding and representation is that music is an extremely specific modality.
Unlike speech or environmental sounds, music not only focuses on semantic information, but also emphasizes acoustic information, such as melody, chords, and tonality.
As a result, previous semantics-oriented SSL methods have struggled to perform well on music tasks\cite{spijkervet2021clmr, li2022mapmusic2vec, yuan2023marble}, as they fail to simultaneously capture both semantic and acoustic information.

Recently, several studies have sought to develop a universal music representation that integrates both semantic and acoustic aspects.
Among these efforts, two remarkable models are MERT \cite{li2023mert} and MusicFM \cite{won2023musicfm}.
MERT employs a BERT-style masked language modeling (MLM) proxy task to predict discrete tokens from the masked audio parts, with an Encodec \cite{defossez2022encodec} model as the tokenizer and uses an auxiliary Constant Q-Transform (CQT) target to enhance the modeling of acoustic information.
MusicFM, on the other hand, directly utilizes a random projection quantizer derived from BEST-RQ \cite{chiu2022bestrq}, and this tokenization approach provides a general target for learning music representation, without the need for additional CQT loss to capture acoustic modeling.

As discussed in \cite{chiu2022bestrq} and \cite{li2023mert}, the target extractor (i.e., tokenizer) plays an important role in SSL, as models are trained to predict the tokenized pseudo-labels.
BEST-RQ features a lightweight approach that allows for fast extraction of discrete targets.
However, its performance is highly dependent on the initialization of the random projection layer, often requiring multiple attempts or a specific random seed to achieve optimal results.
In contrast, the Encodec target \cite{defossez2022encodec} used in MERT produces a series of residual targets, with the multi-target strategy shown beneficial to musical SSL \cite{li2023mert}.
As a neural codec trained on audio data, Encodec produces more stable labels compared to its random counterparts.
However, using Encodec as tokenization is computational heavy and consumes a lot of GPU memory when applying online extraction, which can reduce the training efficiency.
Also, it needs to be coupled with additional CQT reconstruction loss to perform well in acoustic representation.

To address the initialization dependency of the random projection quantizer in BEST-RQ and the inefficiency stemming from the heavy computation cost of Encodec in MERT, we introduce a model called MuQ, which learns \textbf{Mu}sic representations from Mel \textbf{Q}uantization targets.
MuQ leverages a Mel-RVQ as the tokenizer to generate targets.
The proposed Mel-RVQ is pre-trained on music data and employs a linear RVQ to directly quantize audio Mel spectrograms.
Compared to the random-projection quantizer in BEST-RQ, the pre-trained Mel-RVQ produces more stable targets for SSL training and eliminates the model's dependence on initialization.
Additionally, compared to Encodec, the lightweight single-layer Mel-RVQ architecture offers greater extraction efficiency.

To further demonstrate the capabilities of the proposed MuQ model, we explore its application in another crucial area of music understanding: aligning music and text representations.
For example, MuLan \cite{huang2022mulan} employs contrastive learning to train both a music encoder and a text encoder, producing semantically consistent embeddings for both modalities and achieving coherent alignment between music and text.
Recognizing the role that self-supervised learning models can play in providing effective initialization for downstream tasks, we leverage our MuQ to construct a joint music-text embedding model, named MuQ-MuLan.

Our main contributions are listed as follows:
- We introduce a novel music SSL model MuQ, which demonstrates state-of-the-art performance across a wide range of downstream music understanding tasks over previous MERT and MusicFM models.
- We propose the Mel Residual Vector Quantization (Mel-RVQ), which directly quantizes the Mel spectrum using a single linear layer RVQ, improving both training stability and efficiency.
- We further develop the MuQ-MuLan model, trained with contrastive learning on MuQ.
MuQ-MuLan excels in aligning and jointly encoding music and text modalities, compared with the original MuLan model.

Our experiments demonstrate that the Mel-RVQ significantly enhances SSL performance across a variety of music downstream tasks.
Notably, MuQ outperforms previous state-of-the-art (SOTA) SSL models MERT and MusicFM, even when trained on just 0.9K hours of data, which is 100x less than what comparable models require \cite{li2023mert, won2023musicfm}.
Additionally, our results show that MuQ-MuLan achieves a ROC-AUC score of 79.3 on the MagnaTagATune zero-shot music tagging task, surpassing the previous SOTA result\cite{huang2022mulan}.


## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论