# MuQ

<details>
<summary>基本信息</summary>

- 标题: "MuQ: Self-Supervised Music Representation Learning with Mel Residual Vector Quantization"
- 作者:
  - 01 Haina Zhu (SJTU@X-LANCE Lab)
  - 02 Yizhi Zhou (Nanjing University)
  - 03 Hangting Chen (Tencent AI Lab)
  - 04 Jianwei Yu (Tencent AI Lab)
  - 05 Ziyang Ma (SJTU@X-LANCE Lab)
  - 06 Rongzhi Gu (Tencent AI Lab)
  - 07 Yi Luo (Tencent AI Lab)
  - 08 Wei Tan (Tencent AI Lab)
  - 09 Xie Chen (SJTU@X-LANCE Lab)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.01108)
  - [Publication]()
  - [Github](https://github.com/tencent-ailab/MuQ)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2501.01108v2__MuQ__Self-Supervised_Music_Representation_Learning_with_Mel_Residual_Vector_Quantization.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Recent years have witnessed the success of foundation models pre-trained with self-supervised learning (SSL) in various music informatics understanding tasks, including music tagging, instrument classification, key detection, and more.
In this paper, we propose a self-supervised music representation learning model for music understanding.
Distinguished from previous studies adopting random projection or existing neural codec, the proposed model, named MuQ, is trained to predict tokens generated by Mel Residual Vector Quantization (Mel-RVQ).
Our Mel-RVQ utilizes residual linear projection structure for Mel spectrum quantization to enhance the stability and efficiency of target extraction and lead to better performance.
Experiments in a large variety of downstream tasks demonstrate that MuQ outperforms previous self-supervised music representation models with only 0.9K hours of open-source pre-training data.
Scaling up the data to over 160K hours and adopting iterative training consistently improve the model performance.
To further validate the strength of our model, we present MuQ-MuLan, a joint music-text embedding model based on contrastive learning, which achieves state-of-the-art performance in the zero-shot music tagging task on the MagnaTagATune dataset.
Code and checkpoints are open source in [this https URL](https://github.com/tencent-ailab/MuQ).

## 1·Introduction: 引言

Self-supervised learning (SSL) has been introduced into speech and audio signal processing as a technique for learning latent semantic relationships from unlabeled raw data.
Recently, several works \cite{li2023mert, won2023musicfm} apply SSL to music informatics understanding, and a number of pre-trained foundation models (PFMs) with generalized representation capabilities are built.
Learning from unsupervised music data, these foundation models can achieve stunning performance in music understanding tasks such as genre classification, emotion prediction, and key detection \cite{yuan2023marble}, and further provide semantic representations for more downstream tasks like music generation \cite{agostinelli2023musiclm} and music captioning \cite{deng2024musilingo}, as illustrated in Figure \ref{fig:task_illu}.

A key challenge in music understanding and representation is that music is an extremely specific modality.
Unlike speech or environmental sounds, music not only focuses on semantic information, but also emphasizes acoustic information, such as melody, chords, and tonality.
As a result, previous semantics-oriented SSL methods have struggled to perform well on music tasks\cite{spijkervet2021clmr, li2022mapmusic2vec, yuan2023marble}, as they fail to simultaneously capture both semantic and acoustic information.

Recently, several studies have sought to develop a universal music representation that integrates both semantic and acoustic aspects.
Among these efforts, two remarkable models are MERT \cite{li2023mert} and MusicFM \cite{won2023musicfm}.
MERT employs a BERT-style masked language modeling (MLM) proxy task to predict discrete tokens from the masked audio parts, with an Encodec \cite{defossez2022encodec} model as the tokenizer and uses an auxiliary Constant Q-Transform (CQT) target to enhance the modeling of acoustic information.
MusicFM, on the other hand, directly utilizes a random projection quantizer derived from BEST-RQ \cite{chiu2022bestrq}, and this tokenization approach provides a general target for learning music representation, without the need for additional CQT loss to capture acoustic modeling.

As discussed in \cite{chiu2022bestrq} and \cite{li2023mert}, the target extractor (i.e., tokenizer) plays an important role in SSL, as models are trained to predict the tokenized pseudo-labels.
BEST-RQ features a lightweight approach that allows for fast extraction of discrete targets.
However, its performance is highly dependent on the initialization of the random projection layer, often requiring multiple attempts or a specific random seed to achieve optimal results.
In contrast, the Encodec target \cite{defossez2022encodec} used in MERT produces a series of residual targets, with the multi-target strategy shown beneficial to musical SSL \cite{li2023mert}.
As a neural codec trained on audio data, Encodec produces more stable labels compared to its random counterparts.
However, using Encodec as tokenization is computational heavy and consumes a lot of GPU memory when applying online extraction, which can reduce the training efficiency.
Also, it needs to be coupled with additional CQT reconstruction loss to perform well in acoustic representation.

To address the initialization dependency of the random projection quantizer in BEST-RQ and the inefficiency stemming from the heavy computation cost of Encodec in MERT, we introduce a model called MuQ, which learns \textbf{Mu}sic representations from Mel \textbf{Q}uantization targets.
MuQ leverages a Mel-RVQ as the tokenizer to generate targets.
The proposed Mel-RVQ is pre-trained on music data and employs a linear RVQ to directly quantize audio Mel spectrograms.
Compared to the random-projection quantizer in BEST-RQ, the pre-trained Mel-RVQ produces more stable targets for SSL training and eliminates the model's dependence on initialization.
Additionally, compared to Encodec, the lightweight single-layer Mel-RVQ architecture offers greater extraction efficiency.

To further demonstrate the capabilities of the proposed MuQ model, we explore its application in another crucial area of music understanding: aligning music and text representations.
For example, MuLan \cite{huang2022mulan} employs contrastive learning to train both a music encoder and a text encoder, producing semantically consistent embeddings for both modalities and achieving coherent alignment between music and text.
Recognizing the role that self-supervised learning models can play in providing effective initialization for downstream tasks, we leverage our MuQ to construct a joint music-text embedding model, named MuQ-MuLan.

Our main contributions are listed as follows:
- We introduce a novel music SSL model MuQ, which demonstrates state-of-the-art performance across a wide range of downstream music understanding tasks over previous MERT and MusicFM models.
- We propose the Mel Residual Vector Quantization (Mel-RVQ), which directly quantizes the Mel spectrum using a single linear layer RVQ, improving both training stability and efficiency.
- We further develop the MuQ-MuLan model, trained with contrastive learning on MuQ.
MuQ-MuLan excels in aligning and jointly encoding music and text modalities, compared with the original MuLan model.

Our experiments demonstrate that the Mel-RVQ significantly enhances SSL performance across a variety of music downstream tasks.
Notably, MuQ outperforms previous state-of-the-art (SOTA) SSL models MERT and MusicFM, even when trained on just 0.9K hours of data, which is 100x less than what comparable models require \cite{li2023mert, won2023musicfm}.
Additionally, our results show that MuQ-MuLan achieves a ROC-AUC score of 79.3 on the MagnaTagATune zero-shot music tagging task, surpassing the previous SOTA result\cite{huang2022mulan}.

## 2·Related Works: 相关工作

### Self-Supervised Learning for Speech and Audio

Self-supervised learning (SSL)  \cite{mohamed2022self} has been introduced to the field of speech and audio signal processing as a way to learn semantic representations from unlabeled audio data, and it is now widely used in tasks such as automatic speech recognition \cite{baevski2023d2v2} and audio event classification \cite{chen2024eat}.

SSL often relies on well-designed targets.
HuBERT \cite{hsu2021hubert}, for instance, uses K-means clustering labels to guide self-supervised learning in Masked Language Model (MLM) style, with these labels being extracted offline.
BEST-RQ \cite{chiu2022bestrq} learns directly from targets generated by a randomly initialized projection quantizer, which can be extracted online.
MT4SSL \cite{ma2022mt4ssl} integrates both online and offline objectives, applying the concept of multi-target self-supervised learning, where a distinct linear prediction head is used for each type of target.

Another important topic in SSL is iterative refinement training.
Most existing iterative training practices are based on clustering.
For example, HuBERT applies K-means clustering on the features of an existing model to train the next iteration of models.
Similarly, Seed-ASR \cite{bai2024seedasr} first trains an SSL model on BEST-RQ and then iteratively trains on its features using K-means clustering.

In this paper, we take an alternative approach by iteratively training directly through the quantizer itself, without the need to apply K-means clustering.

### Music Processing and Understanding

Music information retrieval (MIR) contains a large set of tasks to evaluate deep learning models' understanding of music from audio signals or symbolized notation.
For example, automatic music tagging task require models to do multi-label classification for tags like genre, instrument, and mood, given a music track \cite{won2020tagging}.
Other MIR tasks include key detection,  pitch detection, emotion analysis, etc \cite{yuan2023marble}.

In recent years, SSL has been introduced into music understanding for building universal representational models capable of handling various MIR tasks.
Some of these efforts include CLMR \cite{spijkervet2021clmr} based on contrastive learning; MERT \cite{li2023mert} based on MLM proxy task; and more recently, MusicFM \cite{won2023musicfm}.
These SSL-based models yield impressive performance across multiple MIR tasks, showing a generalized understanding of music.

### Music-Text Joint Embedding Model

Bridging the gap between music and text modalities, music-text representation models learn joint embeddings from music-text pairwise data.
Most of these models use a two-tower structure, consisting of both a music encoder and a text encoder, and are trained by contrastive learning loss.

LAION-CLAP \cite{laionclap2023} is a powerful open-source music-text encoder, with various model structures and versions trained on extensive data.
MuLan \cite{huang2022mulan} is also an exceptional music-text embedding model, but it is not open source for either the model or the data.

The MuQ-MuLan model introduced in this paper follows the ideas of former work but replaces the music encoder with our proposed MuQ model.

## 3·Methodology: 方法

MuQ employs a self-supervised learning approach based on masked language modeling (MLM) and tokenized targets.
The overall framework of MuQ is presented in Figure \ref{fig:structure}.

### Self-Supervised Framework of MuQ

MuQ directly takes the Mel spectrum of the music audio signal as input.
 The Mel spectrum is partially masked as random noise with a masking probability $p$, and then fed into multiple layers of Conformer \cite{gulati2020conformer} for context learning.
The Conformer output is passed through linear layers, which serve as prediction heads.
Finally, we calculate the cross-entropy loss between the target and predicted labels as the optimization objective.

The target labels are tokens extracted from the Mel spectrum by the Mel-RVQ, a quantization tokenizer proposed in this paper.
Since Mel-RVQ produces $N$ tokens for each time step of the Mel spectrum, the MuQ model incorporates $N$ distinct linear layers to predict the different target tokens.
In Figure \ref{fig:structure}, these multiple linear layers are illustrated as stacked blocks following the Conformer.

### Mel Residual Vector Quantization (Mel-RVQ)

As shown in Figure \ref{fig:projection}, we depict the proposed Mel Residual Vector Quantization (Mel-RVQ) compared with the random-projection quantizer in BEST-RQ and the Encodec target used by MERT.

The proposed Mel-RVQ directly takes the Mel spectrum as input and then quantizes the Mel spectrum using residual vector quantization (RVQ).
The encoder of Mel-RVQ is designed as a simple single-layer linear projection, and the decoder is also a single linear layer.

Mel-RVQ needs to be trained on music data in advance before it can be used to generate quantization targets in SSL training.

#### Training of Mel-RVQ

During the training of Mel-RVQ, the final loss function can be decomposed into three terms, as shown in Figure \ref{fig:projection}(c):

- **Codebook loss** trains the RVQ component to improve the fit of the codebook embedding $Q_\tau$ to the dimensionality-reduced features $z$.
Formally,
$$
    loss_{\mathrm{code}} = \sum_{ \substack{x \in B, \\ z = M_\mathrm{P} x} } { \| \mathrm{norm}(Q_\tau) - \mathrm{norm}(\mathrm{sg}(z)) \| ^2}
$$
where $B$ refers to a mini-batch of data, $x$ denotes Mel spectrum input and $\mathrm{sg}$ denotes stop-gradient.
$M_\mathrm{P}$ is the projection matrix and token $\tau$ is the closest to the projected vector $z$ in the $l_2$-normalized embedding space.

- **Commitment loss** trains the projection (i.e., encoder) for more optimal dimension reduction and fitting to embeddings.
Expressed as
$$
loss_{\mathrm{comm}} = \sum_{ \substack{x \in B, \\ z = M_\mathrm{P} x} }{ \| \mathrm{norm}(z) - \mathrm{norm}(\mathrm{sg}(Q_\tau)) \| ^2}
$$

- **Reconstruction loss** trains the linear decoder (denoted as $M_\mathrm{D}$) and the codebook to restore the original feature $x$.
The formula is
$$
loss_{\mathrm{recon}} = \sum_{x \in B} { \| M_\mathrm{D} Q_\tau - x \| ^2}
$$


The final loss used to train the Mel-RVQ is the weighted sum of the above three losses:

$$
loss = \alpha \cdot loss_{\mathrm{code}} + \beta \cdot loss_{\mathrm{comm}} + loss_{\mathrm{recon}}
$$

where $\alpha$ and $\beta$ are weighting factors, set as $\alpha=1, \beta=0.25$.

It is emphasized that both \(M_\mathrm{D}\) and \(M_\mathrm{P}\) used in Eq.(\ref{eq:final_loss}) are simple linear layers, which distinguishes them from those in neural codecs like Encodec \cite{defossez2022encodec}.

#### Residual modeling of Mel-RVQ

Applying the residual modeling method to Mel-RVQ means that it yields multiple tokens for each time step of the audio.
Assume there are $N$ codebooks in total, denoted as $\{Q_i^{(n)}\}_{i=1}^K$ for $n \in 1\ldots N$.
In this way, we have

$$
\begin{aligned}
   z^{(n)} &= M_\mathrm{P}^{(n)} r^{(n-1)} \\
   \tau^{(n)} &= {\arg\min_{i} \| \mathrm{norm}(z^{(n)}) - \mathrm{norm}(Q^{(n)}_i) \|} \\
   r^{(n)} &= r^{(n-1)} - M_\mathrm{D}^{(n)} Q^{(n)}_{\tau^{(n)}} \\
\end{aligned}
$$

where  $n \in 1\ldots N$; $r^{(n)}$ is the residual signal passed to next step in quantizer and $r^{(1)} = x$.
$M_\mathrm{P}^{(n)}$ and $M_\mathrm{D}^{(n)}$ denote the residual-projection matrices for the encoder and decoder, which contain multiple steps of projection.
The superscript $(n)$ denotes the components or features corresponding to the quantizer at step $n$.

#### Iterative refinement with RVQ

Iterative refinement is introduced in HuBERT \cite{hsu2021hubert} as a method to improve the performance of SSL models, where clustering techniques like K-means are employed to produce new labels for the next iteration of training.

We suggest that residual vector quantization (RVQ) itself can serve as an alternative to the clustering in iterative enhancement.
That is, we directly train a Mel-RVQ\textsubscript{iter} on the latent representations of an already trained MuQ model, to re-drive the training of MuQ at next iteration.

Specifically, the iterative training on MuQ has two stages:

- In the initial stage, a Mel-RVQ is trained on the Mel spectrum feature, and then the first version of MuQ is trained on the tokens produced by Mel-RVQ.
- In the iterative stage, the first version of MuQ is used to extract representation from the audio, and the $l$ th layer latent is used to train Mel-RVQ\textsubscript{iter}, which will be used in training the second version of MuQ, namely MuQ\textsubscript{iter}.

### Music-Text Contrastive Learning

To verify the effectiveness of MuQ in more sophisticated downstream tasks, we trained a music-text joint embedding model, MuQ-MuLan.
Similar to MuLan \cite{huang2022mulan}, MuQ-MuLan employs a two-tower multimodal architecture and is trained on a large amount of (music, text) paired data.

MuQ-MuLan consists of a music encoder and a text encoder.
During training, the music and the corresponding text are fed to the encoders separately, where the text is a description of the music track, as shown in Figure \ref{fig:muq_mulan}.

For music modality inputs, a MuQ model with pre-trained parameters is first used to encode the audio into a latent representation.
This representation is then average-pooled over the temporal dimension.
Finally, it passes through a linear projection layer to produce a music embedding $e_\mathrm{m}$ with dimension $d$.

For text modality inputs, they are encoded by a pre-trained RoBERTa \cite{liu2019roberta} model.
As described in a previous study \cite{vasilakis2024canlisten}, the text encoder is critical to the performance of the two-tower model, so we append a few additional layers of Transformer encoder after RoBERTa.
Likewise, the textual latent representations are average-pooled and passed through a linear projection layer to get text embedding $e_\mathrm{t}$ with the same dimension $d$.

We use decoupled contrastive learning (DCL) loss \cite{yeh2022decoupled}.
Formally, given the $i$-th (music, text) pair in a mini-batch, for the embeddings $(e_\mathrm{m}^{(i)}, e_\mathrm{t}^{(i)})$, we have

$$
   L_{\mathrm{DCL}} = - \mathrm{log} \sum_{(i, j)}\frac{\mathrm{sim}(e_\mathrm{m}^{(i)}, e_\mathrm{t}^{(j)}) / \sigma}{ \sum_{i \neq j} \mathrm{sim}(e_\mathrm{m}^{(i)}, e_\mathrm{t}^{(j)}) / \sigma}
$$

where $\mathrm{sim}$ denotes dot-product similarity, and $\sigma$ is temperature coefficient.

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论

In this study, we introduce MuQ, a novel self-supervised learning model for music representation and understanding.
Building on this, we also present MuQ-MuLan, a joint music-text embedding model.
MuQ uses Mel-RVQ as a quantization target, applying a linear projection RVQ structure to the Mel spectrum, which provides a simple yet powerful SSL target.
Our model shows significant improvements over previous models like MERT and MusicFM, as demonstrated by its performance on the MARBLE benchmark.
Additionally, experiments on the zero-shot music tagging task reveal that MuQ-MuLan achieves state-of-the-art music-text modal alignment.
Our model code and checkpoints will be made open-source to support future research.
