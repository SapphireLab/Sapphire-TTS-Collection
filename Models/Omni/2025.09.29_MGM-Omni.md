# MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech

<details>
<summary>基本信息</summary>

- 标题: "MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech."
- 作者:
  - 01 Chengyao Wang
  - 02 Zhisheng Zhong
  - 03 Bohao Peng
  - 04 Senqiao Yang
  - 05 Yuqi Liu
  - 06 Haokun Gui
  - 07 Bin Xia
  - 08 Jingyao Li
  - 09 Bei Yu
  - 10 Jiaya Jia
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.25131v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2509.25131v1](PDF/2025.09.29_2509.25131v1_MGM-Omni__Scaling_Omni_LLMs_to_Personalized_Long-Horizon_Speech.pdf)
  - [Publication] #TODO

</details>

## Abstract

We present MGM-Omni, a unified Omni LLM for omni-modal understanding and expressive, long-horizon speech generation.
Unlike cascaded pipelines that isolate speech synthesis, MGM-Omni adopts a "brain-mouth" design with a dual-track, token-based architecture that cleanly decouples multimodal reasoning from real-time speech generation.
This design enables efficient cross-modal interaction and low-latency, streaming speech generation.
For understanding, a unified training strategy coupled with a dual audio encoder design enables long-form audio perception across diverse acoustic conditions.
For generation, a chunk-based parallel decoding scheme narrows the text speech token-rate gap, accelerating inference and supporting streaming zero-shot voice cloning with stable timbre over extended durations.
Compared to concurrent work, MGM-Omni achieves these capabilities with markedly data-efficient training.
Extensive experiments demonstrate that MGM-Omni outperforms existing open source models in preserving timbre identity across extended sequences, producing natural and context-aware speech, and achieving superior long-form audio and omnimodal understanding.
MGM-Omni establishes an efficient, end-to-end paradigm for omnimodal understanding and controllable, personalised long-horizon speech generation.

## 1·Introduction

\label{intro}
\vspace{-2mm}

The evolution of large language models (LLMs) from purely text-based systems[^OpenAI2023ChatGPT], [^Touvron2023LLaMA] to multimodal frameworks has marked a significant paradigm shift in artificial intelligence.

Vision language models (VLMs) such as LLaVA, GPT-4V, and Gemini[^Liu2023Visual], [^OpenAI2023GPT-4], [^Team2023Gemini] have demonstrated remarkable capabilities in understanding and processing visual information, effectively bridging the gap between vision and language.

Audio serves as a bridge between humans and AI.

However, integration of audio, particularly understanding and generating long-form and expressive audio, remains a significant challenge in multimodal systems.

Most existing approaches are vision-centric, treating audio as a secondary input modality and relying on separate, cascaded text-to-speech (TTS) systems for generation[^Van2016Wavenet], [^Anastassiou2024Seed-TTS], [^Du2024Cosyvoice].

These methods exhibit critical shortcomings, including limited capability to process and understand extended audio sequences, high latency in audio synthesis, and degraded vocal timbre consistency over long durations.

\begin{wraptable}{r}{0.7\linewidth}
\vspace{-\baselineskip}
\centering
\resizebox{\linewidth}{!}{

\begin{tabular}{lcccccc}
\toprule
Model & VU & AU & LAU & SG & LSG & VC \\ \midrule
CosyVoice2[^Du2024Cosyvoice]      &    &    &     & \checkmark &     & \checkmark \\
Higgs-Audio-v2[^{Boson AI}2025Higgs] &    &    &     & \checkmark & \checkmark & \checkmark \\
Qwen2.5-VL[^Bai2025Qwen2.5-Vl]        & \checkmark &    &     &    &     &    \\
Qwen2.5-Omni[^Xu2025Qwen2.]    & \checkmark & \checkmark &     & \checkmark &     &    \\
Lyra[^Zhong2024Lyra]                  & \checkmark & \checkmark & \checkmark & \checkmark &     &    \\
\rowcolor{mygray}

MGM-Omni                           & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\ \bottomrule
\end{tabular}
%
}
\caption{\small **Function comparison.**

VU, AU, LAU, SG, LSG, and VC denote visual understanding, audio understanding, long audio understanding, speech generation, long speech generation, and zero-shot voice cloning.}
\end{wraptable}

The integration of audio in multimodal systems is hindered by the disparity between audio and text modalities.

Audio token sequences are significantly more extensive and operate at a finer temporal resolution compared to their corresponding text token sequences[^Van2016Wavenet], [^Shen2018Natural].

This disparity creates three challenges.

First, existing systems lack robust long-form audio understanding, struggling to maintain contextual coherence and semantic accuracy across extended audio inputs.

Second, in generation, a one-to-many alignment problem complicates mapping semantic words or units to long acoustic sequences, leading to misaligned prosody and unnatural pacing in long-form speech.

Third, the autoregressive generation process is prone to error accumulation, where minor inaccuracies cascade, degrading timbre consistency and audio quality.

Despite recent progress[^Huang2025Step-Audio], [^Xu2025Qwen2.], [^Team2025Text], [^{Boson AI}2025Higgs], these systems do not address the intertwined issues of long-form audio understanding, alignment, and generation.

To address these limitations, we introduce MGM-Omni, an Omni LLM that unifies vision, language, and audio in an end-to-end framework for seamless, low-latency multimodal understanding and generation.

MGM-Omni adopts a dual-track architecture, separating multimodal reasoning (MLLM, the brain) from speech synthesis (SpeechLM, the mouth), enabling efficient cross-modal processing and real-time audio generation.

For audio understanding, we employ a dual-encoder design that fuses acoustic and semantic features, with unified training enabling unified inference across short and long audio.

For speech generation, we introduce Chunk-Based Parallel Decoding, which mitigates the token-rate gap between text and speech by segmenting text and predicting multiple speech tokens in parallel.

This improves multimodal alignment, reduces long-sequence error accumulation and boosts inference speed by up to 3x.

Trained on approximately 400k hours of audio, MGM-Omni supports zero-shot voice cloning from any personalized reference voice.

Furthermore, we propose Long-TTS-Eval, a benchmark that systematically assesses long-form speech generation capability.

Consequently, MGM-Omni delivers zero-shot voice cloning and expressive, personalized long-horizon speech, maintaining timbre consistency and robust text-speech alignment across extended contexts. 
Our main contributions are threefold:

-  We propose MGM-Omni, an Omni LLM featuring a novel dual-track design that unifies omni-modal understanding and expressive speech generation, moving beyond cascaded systems.

-  We introduce a Chunk-Based Parallel Decoding mechanism that mitigates the token-rate mismatch between text and speech, enabling efficient, high-fidelity, and context-aware long-form audio synthesis with customized voice.

-  Through extensive experiments, we demonstrate that MGM-Omni significantly outperforms existing methods in long audio understanding, and achieves leading performance in zero-shot voice cloning and natural, context-aware long-form speech generation.

## 2·Related Work

\label{related}

\paragraph{Multi-modal Large Language Models.}

The advent of large language models (LLMs)[^OpenAI2023ChatGPT], [^Touvron2023LLaMA] has revolutionized natural language processing, paving the way for multimodal extensions that integrate diverse data modalities such as text, image, video and audio[^Bai2025Qwen2.5-Vl], [^Xu2025Qwen2.], [^Li2024Llama-Vid], [^Li2024Mini-Gemini], [^Liu2023Visual], [^Liu2025Seg-Zero].

Early multimodal models centered on vision-language alignment via contrastive learning.

CLIP[^Radford2021Learning] demonstrated the efficacy of zero-shot image classification through joint embedding spaces.

Building on this foundation, vision language models (VLMs) like Flamingo, LLaVA and MiniGPT-4[^Alayrac2022Flamingo], [^Liu2023Visual], [^Zhu2023Minigpt-4] adapted frozen visual encoders (e.g., CLIP-ViT) to instruction-tuned LLMs to enable general-purpose multimodal understanding.

Subsequent works such as Mini-Gemini[^Li2024Mini-Gemini], the LLaVA series[^Liu2023Improved], [^Li2024Llava-Onevision], and the Qwen-VL series[^Wang2024Qwen2-Vl], [^Bai2025Qwen2.5-Vl] further advance VLMs with high-resolution image comprehension, video understanding and visual grounding.

Despite this progress, most MLLMs remain vision-centric, with limited support for audio modalities.

Recent efforts[^Fu2024Vita], [^Zhong2024Lyra], [^Xu2025Qwen2.] start to incorporate audio into MLLMs, but still struggle with understanding and generation of long-form audio, and cannot control the timbre of generated speech.

MGM-Omni address these limitations with a dual-track, token-based architecture that natively fuses language and audio, enabling omni-modal understanding and expressive, controllable long-form audio generation.

\paragraph{Speech Generation.}

In recent years, driven by the emergence of large language models (LLMs) and large-scale speech-text pre-training, zero-shot text-to-speech generation (TTS) has advanced markedly[^Anastassiou2024Seed-TTS], [^Du2024Cosyvoice], [^{Boson AI}2025Higgs].

CosyVoice2[^Du2024Cosyvoice] builds a TTS system with chunk-aware flow matching and LLMs, enabling streaming multilingual speech synthesis with zero-shot voice cloning.

Qwen2.5-Omni[^Xu2025Qwen2.] incorporates this design with a thinker-talker pipeline for end-to-end perception and generation across text, images, audio, and video.

However, these systems still struggle with long-form speech generation.

More recent efforts such as MOSS-TTSD[^Team2025Text] and Higgs-Audio-v2[^{Boson AI}2025Higgs] support expressive bilingual dialogue generation with personalized voice, yet challenges remain in maintaining timbre consistency over long sequences, ensuring real-time cross-modal fidelity, and achieving low latency.

MGM-Omni addresses this issue via a chunk-based parallel decoding approach, enabling expressive long-form speech generation with consistent timbre and low latency.

## 3·MGM-Omni

\label{method}

MGM-Omni is capable of processing text, images, video and speech, and can generate both textual and spoken outputs.

To support high-quality, long-form speech synthesis without compromising the efficiency and effectiveness of omnimodal understanding and text generation, MGM-Omni decouples multimodal understanding and speech generation into two components: MLLM, serving as the "brain" for multimodal understanding and text generation, and SpeechLM, serving as the "mouth" for real-time speech generation.

For input in different modalities, we employ modality-specific encoders to extract features, which are subsequently passed to the MLLM.

The MLLM generates text tokens and passes them to SpeechLM, which produces speech tokens in real-time via a Chunk-Based Parallel Decoding strategy.

These speech tokens are further converted into Mel-spectrograms through a flow-matching model~\citep {flow-matching}, and the final audio is synthesized using a vocoder.

The overall framework is illustrated in Figure~[framework](#framework).

![](figures/framework.pdf)

<a id="framework">**The overview of MGM-Omni.**

MGM-Omni decouples omni-modal understanding and speech generation into an MLLM and a SpeechLM.

The MLLM processes text, images, video, and audio to produce text, while the SpeechLM generates speech from the MLLM's output in real time.</a>

### Omni Understanding

MGM-Omni is built upon Qwen2.5-VL[^Bai2025Qwen2.5-Vl], a state-of-the-art open-source Vision-Language Model (VLM) that supports image and video understanding with a native-resolution ViT[^Dehghani2023Patch].

Based on Qwen2.5-VL, MGM-Omni attempts to extend towards Omni-Understanding, especially by incorporating audio understanding capabilities.

\paragraph{Dual Audio Encoder.}

MGM-Omni adopts a dual audio encoder design to capture both acoustic and semantic audio features.

The primary encoder, Qwen2-Audio[^Chu2024Qwen2-Audio], is an audio encoder continually trained on Whisper-large-v3[^Radford2022Robust] for enhanced general sound perception. 
To strengthen semantic understanding, especially for Chinese speech, we incorporate Belle-Whisper-large-v3[^BELLEGroup2023Belle], another Whisper-based encoder specialized in Chinese speech recognition.

This dual encoder setup yields two complementary representations: the main audio feature $X_{\text{main}}$ and the auxiliary audio feature $X_{\text{aux}}$.

\paragraph{Information Mining.}

To effectively integrate these complementary features, we design an audio information mining approach inspired by Mini-Gemini[^Li2024Mini-Gemini].

Specifically, $X_{\text{main}}$ serves as the query $Q\in\mathbb{R}^{N\times C}$, while $X_{\text{aux}}$ provides the key-value pair: $K\in\mathbb{R}^{N\times C}$ and $V\in\mathbb{R}^{N\times C}$, allowing the model to retrieve semantically relevant cues from $X_{\text{aux}}$ under the guidance of $X_{\text{main}}$.

Formally, information mining can be defined as:

$$
T_A= {\mathrm {MLP}}(Q + {\mathrm {Softmax}}(\phi(Q) \times \phi(K)^\top) \times \phi(V)),
$$

where $\phi$ denotes a projection layer and MLP represents a multi-layer perceptron.

This approach enhances the audio representation by making it both acoustically and semantically aware, yielding enhanced audio tokens $T_A$ for subsequent LLM processing.

\paragraph{Training Strategy.}

Following Lyra[^Zhong2024Lyra], we build a two-stage training pipeline to integrate audio understanding capabilities.

In the first stage, we conduct audio-to-text pre-training to align the audio encoder to LLM.

In the second stage, we perform unified omni-modal training.

The first stage primarily uses audio transcription data, while the second stage comprises audio transcription, audio QA, audio-instruct VQA, and text instruction tuning data.

This training strategy enables omni-cognition and robust cross-modal reasoning.

\paragraph{Omni Length Understanding.}

MGM-Omni aims to support both long and short sequence input.

However, training with sequences of diverse lengths is inefficient: large batch sizes cause long sequence samples to run out of memory, while small sizes waste memory on short sequence samples.

To address this issue, we propose a unified training pipeline.

First, we group audio of similar lengths into the same batch.

Second, we dynamically adjust the batch size, smaller for long-context inputs and larger for short-context inputs.

This strategy significantly improves training efficiency.

![](figures/speechlm.pdf)

<a id="speechlm">**The overview of SpeechLM in MGM-Omni.**

Conditioned on MLLM-generated text and the reference audio clip, SpeechLM generate speech with Chunk-based Parallel Decoding.</a>

### Omni Generation

MGM-Omni can generate both long-form text and speech.

The textual output is autoregressively produced by the Omni-MLLM.

The generated text, together with the personalize reference audio, is subsequently served as the conditioning for SpeechLM to synthesize speech via a Chunk-based Parallel Decoding method.

The overall speech generation pipeline is depicted in Figure~[speechlm](#speechlm).

\paragraph{Speech Generation.} 
SpeechLM takes text tokens from Omni-MLLM as input and generates speech tokens in an autoregressive manner.

It is initialized from the Qwen3[^Yang2025Qwen3] language model, with an additional TTS-Adapter appended to its output.

TTS-Adapter consists of six randomly initialized Qwen3 blocks, designed to transform text representations into speech representations.

The speech tokens produced by SpeechLM are then converted into Mel-spectrograms through a Flow-Matching model, and finally synthesized into audio via HiFi-GAN[^Kong2020Hifi-Gan] vocoder.

We used the flow-matching model from CosyVoice2[^Du2024Cosyvoice], which supports chunk-aware streaming decoding.

\paragraph{Speech Tokenizer.}

We employ the CosyVoice2 finite scalar quantization~(FSQ) speech tokenizer to obtain discrete speech representations for speech generation.

The tokenizer operates at a rate of 25 Hz, meaning that 25 tokens represent one second of audio.

In comparison, humans typically express only two or three words per second.

This discrepancy highlights that for a given utterance, the number of speech tokens is substantially larger than the number of text tokens.

This leads to the following two issues:

-  As the length of the speech increases, the gap between text and speech tokens widens, weakening their correlation and degrading the quality of long-form generation.

-  The much higher number of speech tokens compared to text tokens slows inference and harms streaming efficiency.  

To address these two challenges, we propose a Chunk-Based Parallel Decoding for efficient long-form speech generation.

\paragraph{Chunk-based Decoding.}

To improve text-speech alignment in long-form speech generation, we introduce Chunk-based Decoding for speech token generation.

As shown in Figure~[chunking](#chunking), the input text is divided into smaller chunks that are sequentially processed by SpeechLM, with each chunk producing a corresponding speech segment. 
During decoding, we adopt a token delay strategy: speech token generation within a chunk is initiated only after the first four text tokens, which are replaced by padding tokens in the speech sequence.

This design ensures that every speech token is aligned with its corresponding text token while avoiding early mis-synchronization.

By reducing the alignment distance between modalities, Chunk-based Decoding enhances cross-modal correspondence and improves the robustness of long-form speech synthesis.

In contrast to naive segmentation methods, our approach preserves both the previously generated text and speech as context, thereby maintaining global fluency and coherence in the final output.

Notably, Chunk-based Decoding is highly compatible with our dual-track "brain-mouth" design, preserving omnimodal understanding and text generation speed while improving speech synthesis quality.

![](figures/chunk.pdf)

<a id="chunking">**Decoding comparison.**

Chunk-based decoding narrows the gap between text and corresponding speech, enabling long-form speech generation.</a>

\paragraph{Parallel Decoding.}

To improve efficiency, we introduce a parallel decoding strategy for efficient speech token generation.

Specifically, we extend the vocabulary so that SpeechLM can decode both modalities in a single step.

Let $V_{\text{text}}$ denote the text vocabulary, $V_{\text{speech}}$ denote the speech tokenizer vocabulary, and $k$ denote the parallel size.

The extended vocabulary size is thus defined as:

$$
|V| = |V_{\text{text}}| + k |V_{\text{speech}}|.
$$

For speech tokenization, the input for each decoding step $t$ consists of one text token $x_t$ and $k$ speech tokens $\{s_t^1, s_t^2, \dots, s_t^k\}$.

We use $f(\cdot)$ to denote the embedding function, and the hidden features $h_{t}^{\text{in}}$ for LLM input can be averaged as:

$$
h_{t}^{\text{in}} = \frac{1}{k+1} \left( f(x_t) + \sum_{i=1}^{k} f(s_t^i) \right).
$$

For speech detokenization, we employ a TTS-Adapter to project the LLM output hidden state $h_{t}^{\text{out}}$ into the speech representation space, after which the lm\_head predicts the next set of speech tokens:

$$
\{\hat{s}_{t+1}^1, \dots, \hat{s}_{t+1}^k\} = \text{lm\_head}\!\left(\text{TTS-Adapter}(h_{t}^{\text{out}})\right).
$$

While parallel decoding is commonly used with RVQ speech tokenizers[^Xie2024Mini-Omni], [^Team2025Text], it is rarely applied to FSQ speech tokenizers.

We found that using parallel decoding with FSQ speech tokenizers not only maintains speech synthesis performance but also significantly improves efficiency.

Additionally, it further shortens the distance between text and speech tokens, enhancing their correlation.

### Omni Voice

MGM-Omni is capable of generating long-form speech in any personalized voice.

To enable this capability, we carefully designed both the data pipeline and the training strategy.

\paragraph{Training Data.}

To enable zero-shot voice cloning, we collected a large-scale dataset, including around 300k hours of raw speech data and approximately 100k hours of TTS-synthesized speech in Chinese and English.

The raw speech portion of our corpus incorporates diverse open-source datasets, including Emilia Dataset[^He2024Emilia], Libri-heavy[^Kang2024Libriheavy], Common Voice[^Ardila2019Common], and Aishell series[^Bu2017Aishell-1], [^Du2018Aishell-2], [^Shi2020Aishell-3].

We constructed a dataset for TTS synthesis by sampling Chinese conversations from Belle-10M[^BELLEGroup2023Belle] and English conversations from Lamini-Instruct[^Wu2023LaMini-Lm].

We uniformly sampled 900k Chinese and 700k English conversations based on length.

As these datasets are somewhat outdated, we enhanced the text quality by regenerating all responses using Qwen2.5-72B[^Yang2025Qwen2.5].

Subsequently, we synthesized audio from these refined conversations using megatts3[^Jiang2025Sparse].

For each sample, we randomly select a reference voice from the provided set of pre-processed reference audio.

\paragraph{Pre-training.}

The SpeechLM consists of a pre-trained Qwen3[^Yang2025Qwen3]

LLM paired with a randomly initialized TTS-Adapter.

The model is trained to generate speech from given text and reference audio through a next speech token prediction objective.

The goal of the pre-training stage is to align the speech and text modalities.

At this stage, the parameters of the pre-trained Qwen3 LLM remain frozen, while only the TTS-Adapter is updated.

Both raw and synthesized speech data are leveraged in pre-training to ensure robustness across diverse speaker timbres.

\paragraph{Post-training.}

The post-training phase aims to enhance SpeechLM's capacity for fluent and accurate speech generation.

During this phase, the parameters of both the LLM and the TTS-Adapter are jointly optimized with different learning rates.

The TTS-Adapter is trained at a rate five times higher than that of the LLM.

The training corpus is primarily composed of high-fidelity TTS-synthesised speech, supplemented with a smaller portion of raw speech data.

## 4·Experiments

\label{experiments}

### Main Properties

\vspace{-2mm}

In this section, we present a comprehensive evaluation covering audio understanding, omni-modality understanding, and speech generation, to demonstrate the main properties of MGM-Omni, with particular emphasis on its capacity for long audio understanding and long audio generation and zero-shot voice cloning.

<a id="tab:speech_understanding">**Omni-comparison on ASR benchmarks.**

We use Common-Voice, LibriSpeech and AISHELL to evaluate the ASR capability on Chinese and English.</a>

<a id="tab:audio_qa">**Omni-comparison on Audio QA benchmarks.**

We use AIR-Bench for audio QA evaluation.

The scores are evaluated by gpt-4-0125-preview.</a>

#### Audio Understanding

\vspace{-1mm}

\paragraph{Short Audio Understanding.}

We compare the audio understanding ability (audio $\rightarrow$ text) of MGM-Omni against leading Audio and Omni LLMs on two primary tasks: automatic speech recognition (ASR) and general audio QA.

First, we evaluate the ASR ability on LibriSpeech[^Panayotov2015Librispeech], CommonVoice[^Ardila2019Common] and AISHELL[^Bu2017Aishell-1].

As shown in Table~[tab:speech_understanding](#tab:speech_understanding), MGM-Omni delivers competitive or superior performance for both English and Chinese ASR.

In particular, with dual audio encoder, MGM-Omni achieves 4.0 CER on CommonVoice (ZH) and 1.8 CER on AISHELL, surpassing leading audio and Omni LLMs.

For general audio understanding, we evaluate audio QA on AIR-Bench[^Yang2024Air-Bench], a comprehensive benchmark covering speech, sound, and music inputs.

As summarized in Table~[tab:audio_qa](#tab:audio_qa), MGM-Omni outperforms all open source Omni LLMs, including Qwen2.5-Omni[^Xu2025Qwen2.].

![](figures/long-audio.pdf)

<a id="long-audio">**Omni-comparison for Long-form Audio.**

We adopt a needle-in-the-haystack evaluation and report the average success rate across five materials.</a>

\vspace{-2mm}
\paragraph{Long Audio Understanding.}

Unlike many open-source Audio and Omni LLMs, MGM-Omni is capable of processing audio inputs exceeding one hour in length.

To evaluate its ability on long-form audio understanding, we conducted a needle-in-the-haystack test.

As illustrated in Figure~[long-audio](#long-audio), MGM-Omni successfully handles audio inputs of up to 4,500 seconds, significantly outperforming Qwen2.5-Omni[^Xu2025Qwen2.].

The success rate is averaged over five diverse long-form audio.

Moreover, we provide quantitative comparison in Figure~[fig:long-asr](#fig:long-asr) in the appendix.

<a id="tab:vision-speech">**Omni-comparison on vision-speech benchmarks.**

We convert the textual questions in multiple VQA benchmarks into synthesized speech to evaluate the multimodal understanding ability.</a>

#### Omni-Modality Understanding

\vspace{-2mm}

MGM-Omni processes text, image, video, and audio inputs.

Following Lyra[^Zhong2024Lyra], we further evaluate its omni-modal understanding (multimodality $\rightarrow$ text) by comparing MGM-Omni against other omni-modal LLMs on several speech-instructed VQA benchmarks.

As shown in Table~\ref {tab:vision-speech}, MGM-Omni shows a strong ability to follow speech instructions.

<a id="tab:tts">Zero-shot short TTS comparison of error rate and speaker similarity in Seed-TTS-Eval.

For Qwen2.5-Omni, size indicates the talker module size.</a>

#### Speech Generation

\vspace{-2mm}

MGM-Omni supports long-form synthesis (exceeding 10 minutes) with customizable voices.

Here, we assess the speech generation capabilities (text $\rightarrow$ speech) in both short- and long-form setting.

\vspace{-2mm}
\paragraph{Short Speech Generation.}

We evaluated MGM-Omni against state-of-the-art zero-shot TTS systems and Omni LLMs to assess the speech generation capabilities.

As shown in Table~[tab:tts](#tab:tts), MGM achieves lower error rates and higher speaker similarity than open-source TTS models and Omni LLMs on seed-tts-eval[^Anastassiou2024Seed-TTS], demonstrating strong text-to-speech performance and robust zero-shot voice cloning.

\vspace{-2mm}
\paragraph{Long Speech Generation.}

Unlike many open-source Omni LLMs and TTS systems, MGM-Omni can generate over 10 minutes of speech in any personalized voice.

Quantitative examples are shown in Figure~[fig:long-tts](#fig:long-tts) in the appendix.

For benchmark evaluation, most existing benchmarks only evaluate short clips, typically ranging from a few seconds to a few dozen seconds, leaving a gap in assessing long-form performance.

Moreover, existing TTS benchmarks focus on normal text generation and do not cover more complex text, such as formulas, URLs, or classical Chinese poetry.

To address this, we introduce **Long-TTS-Eval**, a benchmark specifically designed to evaluate long-form text-to-speech generation systematically.

We leave more detailed information about the benchmark to Section~[sec:long-tts-eval](#sec:long-tts-eval) in the appendix. 

We compare MGM-Omni against two categories of open-source TTS systems: (1) Native long TTS models, represented by MOSS-TTSD-v0.5[^Team2025Text] and Higgs-Audio-v2[^{Boson AI}2025Higgs]. (2) Non-native models that extend via chunking, represented by CosyVoice2[^Du2024Cosyvoice].

We report WER for English TTS, CER for Chinese TTS, and RTF for inference efficiency.

As shown in Table~[tab:long-tts](#tab:long-tts), MGM-Omni achieves lower error rates across most speech generation scenarios, along with the lowest RTF.

It is worth noting that, MGM-Omni's two-stage training relies on less than 400k hours of audio, substantially fewer than the 1M or even 10M hours used in concurrent works.

This result demonstrates the efficiency, effectiveness, robustness and data efficiency of our model.

<a id="tab:ablation_encoder">Audio Encoder</a>

### Ablation Study

\vspace{-2mm}

\paragraph{Audio Encoder.}

We ablate different audio encoder designs and evaluate on CommonVoice ASR[^Ardila2019Common].

As shown in Table~[tab:ablation_encoder](#tab:ablation_encoder), incorporating both the Qwen2-Audio encoder[^Chu2024Qwen2-Audio] and the Belle-Whisper-large-v3 encoder[^BELLEGroup2023Belle] with information mining yields the best performance in audio understanding.

Note that, compared with the final model, we do not use the long audio QA data here.

\vspace{-2mm}
\paragraph{Chunk-based Decoding.}

We evaluate long-form speech generation on our Long-TTS-Eval to assess the impact of chunk-based decoding.

As shown in Table~[tab:ablation_chunk](#tab:ablation_chunk), removing chunk-based decoding leads to a substantially higher error rate, exceeding that of concurrent works.

Given that concurrent methods typically use millions to tens of millions of hours of audio, we attribute MGM-Omni's data efficiency primarily to its use of chunk-based decoding.

\vspace{-2mm}
\paragraph{Parallel Decoding.}

We ablate the impact of parallel decoding by comparing both TTS performance and inference speed.

TTS performance is measured on Seed-TTS-Eval[^Anastassiou2024Seed-TTS], while inference speed is assessed using 16 Chinese and 16 English samples drawn from Long-TTS-Eval.

We report the real-time factor (RTF) on a single H800 GPU to compare the inference speed.

As shown in Table~[tab:ablation_parallel](#tab:ablation_parallel), increasing the parallel size slightly raises the audio error rate but substantially accelerates inference by 3x.

To balance quality and speed, we set the parallel size to 4.

We anticipate that incorporating more advanced Multi-Token Prediction (MTP) techniques[^Liu2024Deepseek-V3] will further improve audio quality at larger parallel sizes.

## 5·Conclusion

\label{conclusion}
\vspace{-2mm}

We present MGM-Omni, a unified Omni LLM that supports long-form omnimodal understanding and robust long-duration speech generation with personalized voices.

Its dual-track architecture separates multimodal reasoning (MLLM) from real-time speech synthesis (SpeechLM), enabling efficient cross-modal interaction within an end-to-end framework.

For understanding, it employs a dual audio encoder that fuses acoustic and semantic cues, yielding robust long-form audio perception.

For generation, we introduce Chunk-Based Parallel Decoding to bridge the token-rate gap between text and speech, enabling efficient, low-latency synthesis, while conditioning SpeechLM on reference audio to support zero-shot voice cloning with consistent timbre.

Experiments show that MGM-Omni surpasses leading open source Omni LLMs in timbre consistency, context-aware speech, long audio comprehension, and omni-modal reasoning. 

\bibliography{iclr2026_conference}
\bibliographystyle{iclr2026_conference}

\clearpage
\appendix

## 6·Appendix

### Data Format

The data format for MLLM and SpeechLM with the same instruction is illustrated in Figure~[data_format](#data_format).

SpeechLM use chunk-based decoding to generate long-form speech.

![](figures/data-format-mllm.pdf)

<a id="data_format">The data format of MLLM (top) and SpeechLM (button) in MGM-Omni.</a>

### Long-TTS-Eval Benchmark

\label{sec:long-tts-eval}

In this section, we provide a detailed introduction to the data composition and evaluation protocol of the Long-TTS-Eval benchmark we constructed.

#### Data Composition

Long-TTS-Eval focuses on assessing TTS systems' capabilities in long-form speech generation and complex case handling. 

For long TTS evaluation, we collected six types of text: literature, news, knowledge, speeches, reviews, and academic papers, comprising 341 Chinese samples and 353 English samples.

The data were sourced from news outlets, Wikipedia, YouTube video transcripts, and arXiv papers.

We use the Qwen3 tokenizer[^Yang2025Qwen3] to calculate the token length.

As illustrated in Tabel~[tab:long-tts-info](#tab:long-tts-info), the maximum length is 1899 tokens in Chinese and 3277 tokens in English, and the average length is 689.57 tokens in Chinese and 1019.0 tokens in English.

As a single-point timing estimate, 1899 Chinese tokens correspond to about 10 minutes of speech (assuming 200 characters per minute and 1 token per character), and 3277 English tokens correspond to about 12 minutes (assuming 215 words per minute and 1.3-1.5 tokens per word).

For complex case handling, we collected five types of text: web URLs, emails, math formulas, phone numbers, and large numbers, comprising 265 Chinese samples and 260 English samples.

The detailed information is illustrated in Tabel~[tab:hard-tts-info](#tab:hard-tts-info).

Mathematical formulas were sourced from the reasoning process and solution from S1 Long-CoT Instruct dataset[^Muennighoff2025S1], while the other categories were generated by Gemini 2.5 Pro[^Comanici2025Gemini].

#### Evaluation Pipeline

We follow Seed-TTS-Eval[^Anastassiou2024Seed-TTS] to build our evaluation pipeline.

We use Whisper-large-v3[^Radford2022Robust] and Paraformer-zh[^Gao2023Funasr] as the automatic speech recognition (ASR) engines for English and Chinese, respectively.

Since both models accept only short audio, we segment each generated waveform into 28-second chunks, transcribe each chunk independently, and then concatenate the transcripts to obtain the final transcription.

We then compute word error rate (WER) for English and character error rate (CER) for Chinese.

#### Evaluation with Normalized Text

Conventional TTS benchmarks often transcribe generated speech with an ASR model and then compare the transcript to the ground-truth text to calculate the error rate.

This approach has a key flaw: for expressions with multiple valid readings, ASR outputs can legitimately differ from the written form.

For example, "5\%" spoken by TTS may be transcribed as "five percent." It differs from the ground truth, but it is still correct.

To address this issue, for each sample with ground-turth $G$, we prompt GPT-5[^OpenAI2025GPT-5] to generate a normalized ground-turth $N$ that reflects a natural spoken version.

We then synthesize speech, obtain the ASR transcript $T$, and compute two word error rates, between $T$ and $G$, and between $T$ and $N$.

The final per-sample error is the smaller of the two:

$$
WER_{sample} = min(WER(T, G), WER(T, N))
$$

This method lowers the risk of falsely flagging correct TTS, thereby enhancing the reliability of the reported error rates.

<a id="tab:long-tts-info">The composition and average length of our Long-TTS-Eval benchmark.</a>

<a id="tab:hard-tts-info">The composition and average length of the hard set in our Long-TTS-Eval benchmark.</a>

### Quantitative Results

#### Long Audio Understanding

To verify MGM-Omni's effectiveness in long audio understanding, we conducted a more in-depth evaluation.

We illustrate the quantitative result in Figure~[fig:long-asr](#fig:long-asr).

For long audio summarization, MGM-Omni provides more complete and detailed responses compared with Qwen2.5-Omni[^Xu2025Qwen2.].

For fine-grained understanding, MGM-Omni accurately extracts information from long audio inputs, while Qwen2.5-Omni refuses to respond.

#### Long Speech Generation

We compare MGM-Omni with concurrent long TTS systems, MOSS-TTSD-v0.5[^Team2025Text] and Higgs-Audio-v2[^{Boson AI}2025Higgs] to evaluate the long-form speech generation capability.

Specifically, we evaluate two challenging pieces: the renowned Chinese long prose poem "Preface to the Pavilion of Prince Teng" (Tengwang Ge Xu) and Tagore's famous poem "Stray Birds" excerpt "Life is as ephemeral as summer flowers" featuring mixed Chinese-English code switching.

As depicted in Figure~[fig:long-tts](#fig:long-tts), MGM-Omni produces accurate speech with appropriate pausing, while competing methods exhibit pronounced errors in the latter portions of the audio, including audible noise.

### Use of LLMs

In this study, we utilize large language models (LLMs), specifically GPT-5[^OpenAI2025GPT-5], to enhance the quality of our writing by correcting grammatical errors, improving sentence structure, and refining overall clarity.

All ideas, methodologies, experimental designs, and results are entirely the original work of the authors, with LLMs serving solely as tools for language enhancement.

\clearpage

![](figures/understand0.pdf)

<a id="fig:long-asr">MGM-Omni is capable of understanding long-form audio.</a>

![](figures/generate0.pdf)

<a id="fig:long-tts">MGM-Omni is capable of correctly generating long-form speech.</a>

\end{document}
