# MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech

<details>
<summary>基本信息</summary>

- 标题: "MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech."
- 作者:
  - 01 Chengyao Wang
  - 02 Zhisheng Zhong
  - 03 Bohao Peng
  - 04 Senqiao Yang
  - 05 Yuqi Liu
  - 06 Haokun Gui
  - 07 Bin Xia
  - 08 Jingyao Li
  - 09 Bei Yu
  - 10 Jiaya Jia
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.25131v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2509.25131v1](PDF/2025.09.29_2509.25131v1_MGM-Omni__Scaling_Omni_LLMs_to_Personalized_Long-Horizon_Speech.pdf)
  - [Publication] #TODO

</details>

## Abstract

We present MGM-Omni, a unified Omni LLM for omni-modal understanding and expressive, long-horizon speech generation.
Unlike cascaded pipelines that isolate speech synthesis, MGM-Omni adopts a "brain-mouth" design with a dual-track, token-based architecture that cleanly decouples multimodal reasoning from real-time speech generation.
This design enables efficient cross-modal interaction and low-latency, streaming speech generation.
For understanding, a unified training strategy coupled with a dual audio encoder design enables long-form audio perception across diverse acoustic conditions.
For generation, a chunk-based parallel decoding scheme narrows the text speech token-rate gap, accelerating inference and supporting streaming zero-shot voice cloning with stable timbre over extended durations.
Compared to concurrent work, MGM-Omni achieves these capabilities with markedly data-efficient training.
Extensive experiments demonstrate that MGM-Omni outperforms existing open source models in preserving timbre identity across extended sequences, producing natural and context-aware speech, and achieving superior long-form audio and omnimodal understanding.
MGM-Omni establishes an efficient, end-to-end paradigm for omnimodal understanding and controllable, personalised long-horizon speech generation.

## 1·Introduction

\label{intro}
\vspace{-2mm}

The evolution of large language models (LLMs) from purely text-based systems[^OpenAI2023ChatGPT], [^Touvron2023LLaMA] to multimodal frameworks has marked a significant paradigm shift in artificial intelligence.

Vision language models (VLMs) such as LLaVA, GPT-4V, and Gemini[^Liu2023Visual], [^OpenAI2023GPT-4], [^Team2023Gemini] have demonstrated remarkable capabilities in understanding and processing visual information, effectively bridging the gap between vision and language.

Audio serves as a bridge between humans and AI.

However, integration of audio, particularly understanding and generating long-form and expressive audio, remains a significant challenge in multimodal systems.

Most existing approaches are vision-centric, treating audio as a secondary input modality and relying on separate, cascaded text-to-speech (TTS) systems for generation[^Van2016Wavenet], [^Anastassiou2024Seed-TTS], [^Du2024Cosyvoice].

These methods exhibit critical shortcomings, including limited capability to process and understand extended audio sequences, high latency in audio synthesis, and degraded vocal timbre consistency over long durations.

\begin{wraptable}{r}{0.7\linewidth}
\vspace{-\baselineskip}
\centering
\resizebox{\linewidth}{!}{

\begin{tabular}{lcccccc}
\toprule
Model & VU & AU & LAU & SG & LSG & VC \\ \midrule
CosyVoice2[^Du2024Cosyvoice]      &    &    &     & \checkmark &     & \checkmark \\
Higgs-Audio-v2[^{Boson AI}2025Higgs] &    &    &     & \checkmark & \checkmark & \checkmark \\
Qwen2.5-VL[^Bai2025Qwen2.5-Vl]        & \checkmark &    &     &    &     &    \\
Qwen2.5-Omni[^Xu2025Qwen2.]    & \checkmark & \checkmark &     & \checkmark &     &    \\
Lyra[^Zhong2024Lyra]                  & \checkmark & \checkmark & \checkmark & \checkmark &     &    \\
\rowcolor{mygray}

MGM-Omni                           & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\ \bottomrule
\end{tabular}
%
}
\caption{\small **Function comparison.**

VU, AU, LAU, SG, LSG, and VC denote visual understanding, audio understanding, long audio understanding, speech generation, long speech generation, and zero-shot voice cloning.}
\end{wraptable}

The integration of audio in multimodal systems is hindered by the disparity between audio and text modalities.

Audio token sequences are significantly more extensive and operate at a finer temporal resolution compared to their corresponding text token sequences[^Van2016Wavenet], [^Shen2018Natural].

This disparity creates three challenges.

First, existing systems lack robust long-form audio understanding, struggling to maintain contextual coherence and semantic accuracy across extended audio inputs.

Second, in generation, a one-to-many alignment problem complicates mapping semantic words or units to long acoustic sequences, leading to misaligned prosody and unnatural pacing in long-form speech.

Third, the autoregressive generation process is prone to error accumulation, where minor inaccuracies cascade, degrading timbre consistency and audio quality.

Despite recent progress[^Huang2025Step-Audio], [^Xu2025Qwen2.], [^Team2025Text], [^{Boson AI}2025Higgs], these systems do not address the intertwined issues of long-form audio understanding, alignment, and generation.

To address these limitations, we introduce MGM-Omni, an Omni LLM that unifies vision, language, and audio in an end-to-end framework for seamless, low-latency multimodal understanding and generation.

MGM-Omni adopts a dual-track architecture, separating multimodal reasoning (MLLM, the brain) from speech synthesis (SpeechLM, the mouth), enabling efficient cross-modal processing and real-time audio generation.

For audio understanding, we employ a dual-encoder design that fuses acoustic and semantic features, with unified training enabling unified inference across short and long audio.

For speech generation, we introduce Chunk-Based Parallel Decoding, which mitigates the token-rate gap between text and speech by segmenting text and predicting multiple speech tokens in parallel.

This improves multimodal alignment, reduces long-sequence error accumulation and boosts inference speed by up to 3x.

Trained on approximately 400k hours of audio, MGM-Omni supports zero-shot voice cloning from any personalized reference voice.

Furthermore, we propose Long-TTS-Eval, a benchmark that systematically assesses long-form speech generation capability.

Consequently, MGM-Omni delivers zero-shot voice cloning and expressive, personalized long-horizon speech, maintaining timbre consistency and robust text-speech alignment across extended contexts. 
Our main contributions are threefold:

-  We propose MGM-Omni, an Omni LLM featuring a novel dual-track design that unifies omni-modal understanding and expressive speech generation, moving beyond cascaded systems.

-  We introduce a Chunk-Based Parallel Decoding mechanism that mitigates the token-rate mismatch between text and speech, enabling efficient, high-fidelity, and context-aware long-form audio synthesis with customized voice.

-  Through extensive experiments, we demonstrate that MGM-Omni significantly outperforms existing methods in long audio understanding, and achieves leading performance in zero-shot voice cloning and natural, context-aware long-form speech generation.

## 2·Related Work

\label{related}

\paragraph{Multi-modal Large Language Models.}

The advent of large language models (LLMs)[^OpenAI2023ChatGPT], [^Touvron2023LLaMA] has revolutionized natural language processing, paving the way for multimodal extensions that integrate diverse data modalities such as text, image, video and audio[^Bai2025Qwen2.5-Vl], [^Xu2025Qwen2.], [^Li2024Llama-Vid], [^Li2024Mini-Gemini], [^Liu2023Visual], [^Liu2025Seg-Zero].

Early multimodal models centered on vision-language alignment via contrastive learning.

CLIP[^Radford2021Learning] demonstrated the efficacy of zero-shot image classification through joint embedding spaces.

Building on this foundation, vision language models (VLMs) like Flamingo, LLaVA and MiniGPT-4[^Alayrac2022Flamingo], [^Liu2023Visual], [^Zhu2023Minigpt-4] adapted frozen visual encoders (e.g., CLIP-ViT) to instruction-tuned LLMs to enable general-purpose multimodal understanding.

Subsequent works such as Mini-Gemini[^Li2024Mini-Gemini], the LLaVA series[^Liu2023Improved], [^Li2024Llava-Onevision], and the Qwen-VL series[^Wang2024Qwen2-Vl], [^Bai2025Qwen2.5-Vl] further advance VLMs with high-resolution image comprehension, video understanding and visual grounding.

Despite this progress, most MLLMs remain vision-centric, with limited support for audio modalities.

Recent efforts[^Fu2024Vita], [^Zhong2024Lyra], [^Xu2025Qwen2.] start to incorporate audio into MLLMs, but still struggle with understanding and generation of long-form audio, and cannot control the timbre of generated speech.

MGM-Omni address these limitations with a dual-track, token-based architecture that natively fuses language and audio, enabling omni-modal understanding and expressive, controllable long-form audio generation.

\paragraph{Speech Generation.}

In recent years, driven by the emergence of large language models (LLMs) and large-scale speech-text pre-training, zero-shot text-to-speech generation (TTS) has advanced markedly[^Anastassiou2024Seed-TTS], [^Du2024Cosyvoice], [^{Boson AI}2025Higgs].

CosyVoice2[^Du2024Cosyvoice] builds a TTS system with chunk-aware flow matching and LLMs, enabling streaming multilingual speech synthesis with zero-shot voice cloning.

Qwen2.5-Omni[^Xu2025Qwen2.] incorporates this design with a thinker-talker pipeline for end-to-end perception and generation across text, images, audio, and video.

However, these systems still struggle with long-form speech generation.

More recent efforts such as MOSS-TTSD[^Team2025Text] and Higgs-Audio-v2[^{Boson AI}2025Higgs] support expressive bilingual dialogue generation with personalized voice, yet challenges remain in maintaining timbre consistency over long sequences, ensuring real-time cross-modal fidelity, and achieving low latency.

MGM-Omni addresses this issue via a chunk-based parallel decoding approach, enabling expressive long-form speech generation with consistent timbre and low latency.
