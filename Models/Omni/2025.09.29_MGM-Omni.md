# MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech

<details>
<summary>基本信息</summary>

- 标题: "MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech."
- 作者:
  - 01 Chengyao Wang
  - 02 Zhisheng Zhong
  - 03 Bohao Peng
  - 04 Senqiao Yang
  - 05 Yuqi Liu
  - 06 Haokun Gui
  - 07 Bin Xia
  - 08 Jingyao Li
  - 09 Bei Yu
  - 10 Jiaya Jia
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.25131v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2509.25131v1](PDF/2025.09.29_2509.25131v1_MGM-Omni__Scaling_Omni_LLMs_to_Personalized_Long-Horizon_Speech.pdf)
  - [Publication] #TODO

</details>

## Abstract

We present MGM-Omni, a unified Omni LLM for omni-modal understanding and expressive, long-horizon speech generation.
Unlike cascaded pipelines that isolate speech synthesis, MGM-Omni adopts a "brain-mouth" design with a dual-track, token-based architecture that cleanly decouples multimodal reasoning from real-time speech generation.
This design enables efficient cross-modal interaction and low-latency, streaming speech generation.
For understanding, a unified training strategy coupled with a dual audio encoder design enables long-form audio perception across diverse acoustic conditions.
For generation, a chunk-based parallel decoding scheme narrows the text speech token-rate gap, accelerating inference and supporting streaming zero-shot voice cloning with stable timbre over extended durations.
Compared to concurrent work, MGM-Omni achieves these capabilities with markedly data-efficient training.
Extensive experiments demonstrate that MGM-Omni outperforms existing open source models in preserving timbre identity across extended sequences, producing natural and context-aware speech, and achieving superior long-form audio and omnimodal understanding.
MGM-Omni establishes an efficient, end-to-end paradigm for omnimodal understanding and controllable, personalised long-horizon speech generation.
