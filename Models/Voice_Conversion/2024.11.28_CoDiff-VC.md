# CoDiff-VC

<details>
<summary>基本信息</summary>

- 标题: "CoDiff-VC: A Codec-Assisted Diffusion Model for Zero-shot Voice Conversion"
- 作者:
  - 01 Yuke Li,
  - 02 Xinfa Zhu,
  - 03 Hanzhao Li,
  - 04 JiXun Yao,
  - 05 WenJie Tian,
  - 06 XiPeng Yang,
  - 07 YunLin Chen,
  - 08 Zhifei Li,
  - 09 Lei Xie
- 链接:
  - [ArXiv](https://arxiv.org/abs/2411.18918)
  - [Publication]
  - [Github]
  - [Demo](https://aries457.github.io/CoDiff-VC/)
- 文件:
  - [ArXiv](_PDF/2411.18918v3__CoDiff-VC__A_Codec-Assisted_Diffusion_Model_for_Zero-shot_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Zero-shot voice conversion (VC) aims to convert the original speaker's timbre to any target speaker while keeping the linguistic content.
Current mainstream zero-shot voice conversion approaches depend on pre-trained recognition models to disentangle linguistic content and speaker representation.
This results in a timbre residue within the decoupled linguistic content and inadequacies in speaker representation modeling.
In this study, we propose ***CoDiff-VC***, an end-to-end framework for zero-shot voice conversion that integrates a speech codec and a diffusion model to produce high-fidelity waveforms.
Our approach involves employing a single-codebook codec to separate linguistic content from the source speech.
To enhance content disentanglement, we introduce ***Mix-Style layer normalization (MSLN)*** to perturb the original timbre.
Additionally, we incorporate a multi-scale speaker timbre modeling approach to ensure timbre consistency and improve voice detail similarity.
To improve speech quality and speaker similarity, we introduce dual classifier-free guidance, providing both content and timbre guidance during the generation process.
Objective and subjective experiments affirm that ***CoDiff-VC*** significantly improves speaker similarity, generating natural and higher-quality speech.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论