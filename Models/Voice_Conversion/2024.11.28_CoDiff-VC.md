# CoDiff-VC

<details>
<summary>基本信息</summary>

- 标题: "CoDiff-VC: A Codec-Assisted Diffusion Model for Zero-shot Voice Conversion"
- 作者:
  - 01 Yuke Li,
  - 02 Xinfa Zhu,
  - 03 Hanzhao Li,
  - 04 JiXun Yao,
  - 05 WenJie Tian,
  - 06 XiPeng Yang,
  - 07 YunLin Chen,
  - 08 Zhifei Li,
  - 09 Lei Xie
- 链接:
  - [ArXiv](https://arxiv.org/abs/2411.18918)
  - [Publication]
  - [Github]
  - [Demo](https://aries457.github.io/CoDiff-VC/)
- 文件:
  - [ArXiv](_PDF/2411.18918v3__CoDiff-VC__A_Codec-Assisted_Diffusion_Model_for_Zero-shot_Voice_Conversion.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Zero-shot voice conversion (VC) aims to convert the original speaker's timbre to any target speaker while keeping the linguistic content.
Current mainstream zero-shot voice conversion approaches depend on pre-trained recognition models to disentangle linguistic content and speaker representation.
This results in a timbre residue within the decoupled linguistic content and inadequacies in speaker representation modeling.
In this study, we propose ***CoDiff-VC***, an end-to-end framework for zero-shot voice conversion that integrates a speech codec and a diffusion model to produce high-fidelity waveforms.
Our approach involves employing a single-codebook codec to separate linguistic content from the source speech.
To enhance content disentanglement, we introduce ***Mix-Style layer normalization (MSLN)*** to perturb the original timbre.
Additionally, we incorporate a multi-scale speaker timbre modeling approach to ensure timbre consistency and improve voice detail similarity.
To improve speech quality and speaker similarity, we introduce dual classifier-free guidance, providing both content and timbre guidance during the generation process.
Objective and subjective experiments affirm that ***CoDiff-VC*** significantly improves speaker similarity, generating natural and higher-quality speech.

## 1·Introduction: 引言

Zero-shot voice conversion (VC) aims to transfer the timbre of a source speaker to the timbre of an unseen target speaker while maintaining the original linguistic content~\cite{sisman2020overview}.
This approach requires only one utterance from the target speaker, making it applicable in various scenarios like movie dubbing~\cite{yao2023preserving}, speech translation~\cite{sundermann2003vtln}, and speech anonymization~\cite{yoo2020speaker}.
In real-world applications, zero-shot voice conversion can enhance personalized voice interactions~\cite{huang2012personalized,csicsman2017transformation} and improve entertainment experiences by converting speaker timbre characteristics.
However, since only one target speaker utterance is available, disentangling speech components and simultaneously converting to the target speaker's timbre becomes more challenging.

In the zero-shot VC task, there are two primary challenges to address: firstly, how to disentangle the linguistic content and speaker timbre from the source speech; secondly, how to model the speaker representation precisely.
To solve the first challenge, many previous approaches~\cite{qian2019autovc,wang2023lm,zhao2018accent,liu2021diffsvc} utilize pre-trained automatic speech recognition (ASR) or self-supervised learning (SSL) models~\cite{baevski2020wav2vec,hsu2021hubert} to extract bottleneck features as linguistic content decoupled from the source speech.
Simultaneously, a speaker verification model is employed to extract the speaker representation.
The VC model then combines the original linguistic content with the target speaker representation to reconstruct the converted speech.
Despite the previous approach achieving some success in zero-shot VC~\cite{DBLP:conf/apsipa/XiaoXYX21,DBLP:journals/corr/abs-2111-03811}, the extracted bottleneck features still contain speaker-related information, leading to poor speaker similarity in the converted speech.
Meanwhile, most of the mentioned approaches depend on an acoustic model for predicting the mel-spectrogram-like latent representations and employ a vocoder to reconstruct the representations into speech waveform~\cite{dang2022training,liu2021diffsvc,DBLP:conf/iclr/ShenJ0LL00Z024}.
This two-stage framework introduces cascading errors, thereby degrading the quality of the converted speech.

For the second challenge, the previous studies on speaker representation modeling can be broadly divided into two categories: coarse-grained modeling approach~\cite{doddipatla2017speaker,casanova2022yourtts,kinnunen2017non} and fine-grained modeling approach~\cite{zhou2022content,jiang2023mega,lee2022pvae}.
In coarse-grained modeling, a pre-trained speaker verification model is utilized to extract a global speaker embedding as the coarse-grained speaker representation.
While effective in controlling the overall timbre characteristics of the converted speech, these approaches fall short in capturing detailed speaker timbre information and semantic-related timbre changes, leading to limited speaker similarity between the converted speech and target speaker speech.
Conversely, other studies employ an attention mechanism to capture fine-grained speaker representation from multiple reference utterances, with Mega-TTS2~\cite{jiang2023mega} being the most prominent example, which allows for the generation of more natural speech for the target speaker.
However, the speaker similarity in converted speech using these approaches relies on the duration of the reference, resulting in a notable degradation in similarity performance when the reference speech duration is excessively short.

In this study, with particular consideration of the above two challenges, we propose CoDiff-VC, a codec-assisted end-to-end framework for zero-shot voice conversion, which can generate high-fidelity waveforms without any auxiliary losses and avoid cascading error issues.

We employ a pre-trained codec model, featuring only a single codebook, to extract discrete tokens from the source speech as the linguistic content.
The single codebook architecture can partially disentangle the speaker's timbre by introducing a speaker reference encoder while retaining accurate linguistic content information.
Meanwhile, we incorporate Mix-Style layer normalization (MSLN)~\cite{huang2022generspeech} to perturb the timbre information within the discrete tokens, facilitating a more thorough disentanglement of timbre characteristics from the tokens.
To improve timbre similarity and consistency, we introduce a multi-scale speaker timbre modeling approach to recover voice details when reconstructing the waveform.
Finally, we propose a dual classifier-free guidance strategy to train unconditional models of content and timbre for guiding the reverse process to generate high-quality waveforms.
Objective and subjective experiments demonstrate that CoDiff-VC outperforms the baseline systems in both speech quality and speaker similarity.
Ablation studies further demonstrate the effectiveness of each component in our proposed approach.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

The overall architecture of our proposed system is shown in Figure 1, which consists of three parts: content module, multi-scale timbre modeling, and diffusion module.
For the content module, we utilize a pre-trained codec model with a reference encoder to encode audio into discrete tokens and employ mix-style layer normalization further to reduce the residual timbre in the discrete tokens. To achieve more comprehensive timbre modeling, we introduce multi-scale speaker timbre modeling, encompassing modeling of both coarse-grained and fine-grained timbre representation. Following this, a denoising diffusion probabilistic model serves as the backbone of the conversion model for an end-to-end reconstruction process. To enhance the quality of the reconstructed speech, we propose a dual classifier-free guidance strategy to effectively guide content and timbre modeling, thereby improving speech quality and speaker similarity.

### Linguistic Content Extraction

To disentangle the linguistic content from the source speech, we utilize a pre-trained codec to extract discrete tokens as linguistic content. The architecture closely resembles the codec in ~\cite{tortoise}. Since the majority of information is preserved in the first codebook and lower bit rates make it easier to decouple timbres~\cite{ren2023fewer}, we employ a single-codebook speech codec, comprising only 4096 tokens, to reduce bit rates. And, to disentangle timbre characteristics within the tokens in the codec, we incorporate a reference encoder into the codec. The output of the reference encoder, representing global representation related to acoustic information, is subtracted from the hidden representation of the encoder. This subtraction process imposes implicit constraints on the tokens, leading them to spontaneously disentangle global representations, such as timbre and acoustic features, at lower bit rates.

### Multi-scale Speaker Modeling

Many previous studies~\cite{kang2023grad,min2021meta} assume that speaker timbre is a time-invariant global representation, neglecting the potential timbre variations due to intense emotions or specific semantics. In scenarios of zero-shot voice conversion, relying solely on global representation to model speaker timbre may result in a degradation of timbre similarity in the converted speech. To address this limitation and leverage the powerful reconstruction capability of diffusion, we introduce multi-scale speaker timbre modeling within the diffusion model. This approach enables the simultaneous capture of coarse-grained and fine-grained timbre information from reference speech, thereby enhancing the speaker similarity in the converted speech sample. As shown in Figure 1, the speaker timbre modeling module comprises a speaker encoder and a transformer module. Initially, we randomly select an utterance and clip it to a fixed-length segment as the reference speech. The speaker encoder processes this reference to produce frame-level speaker representations, which serve as fine-grained timbre representations. The transformer module then takes these frame-level representations and generates coarse-grained timbre representations through global pooling.
To integrate timbre characteristics at different levels, we use distinct approaches within the diffusion model. As depicted in Figure 2, the coarse-grained timbre information is concatenated with the timestep embedding and then fed into the residual block. This setup enables the supervision of diffusion by timbre at each timestep when predicting noise. For fine-grained timbre modeling, we introduce a cross-attention convolution module every 4 U-net blocks, associating linguistic content information with timbre. Specifically, the latent representation undergoes 256-time downsampling to match the same dimension as the mel spectrogram to be employed as the query in the cross-attention module, and the fine-grained timbre serves as the keys and values. Ultimately, the upsampling module restores it to the same shape as the audio.

### Dual Classifier-free Guidance

To guide the reverse diffusion process, we propose a novel dual classifier-free guidance strategy. We employ classifier-free guidance~\cite{ho2022classifier} to content representation $R_{c}$ and speaker timbre $R_{s}$, respectively. By introducing two distinctive implicit classifiers, both content information and timbre information can guide noise generation in un-condition scenarios. During training, we discard the content condition and timbre condition at a rate of 15\%. The inference process is shown in Eq.(1), where the ${\hat{\epsilon}}_{\theta}$ denotes the denoising network which reverses original sample $x_0$ from Gaussian noise $x_t$. Meanwhile, $w_{c}$ and $w_{s}$ represent the guidance scales of text conditions and timbre conditions for noise prediction, respectively.

$$
\begin{aligned}
    \hat{\epsilon}_{\theta}\left(x_{t}, t, R_{c},R_{s}\right)[t]
    &=(1+w_c+w_s) \cdot \epsilon\left(x_{t}, t, R_{c},R_{s}\right) \\
    &-w_{c} \cdot \epsilon\left(x_{t}, t, R_{s}\right)-w_{s}\cdot \epsilon\left(x_{t}, t,R_{c}\right)
\end{aligned}
$$

During inference, we need to assign a guidance scale to unconditional estimation, which can determine the impact of real conditional estimation and unconditional estimation on the synthesis process. With each timestep of inference, the coarse-grained information is first modeled before modeling fine-grained information. Therefore, we use an annealing strategy for content representation, where the scale $w_{c}$ gradually decreases over timesteps. Meanwhile, we set the guidance scale $w_{s}$ following a hardening strategy, which gradually increases over timesteps, indicating that fine-grained speaker information is gradually restored to improve speaker similarity.

### Training and Inference

We first train the speech codec and then extract quantized discrete tokens for conversion model training. We select audio clips of the same speaker as the reference speech to get a multi-scale timbre representation. Then, with the content representation and timbre representation as condition information, the proposed CoDiff-VC is optimized by minimizing the following unweighted variant of the ELBO~\cite{ho2020denoising} as shown in Eq.(3), which has proved to be effective for high-quality generation~\cite{tortoise}.

$$
    \bar\alpha _t = \prod_{t=1}^{T}(1-\beta_{t})
$$

$$
\min _{\theta} L(\theta)=\mathbb{E}_{x_{0}, \epsilon, t}||\epsilon-\epsilon_{\theta}\left(\sqrt{\bar{\alpha}_{t}} x_{0}+\sqrt{1-\bar{\alpha}_{t}} \epsilon, t, \text{cond}\right)||_{2}^{2}
$$

where the $\epsilon \sim N(0,1)$, $\beta_{t}$ is linear noise schedule and $cond$ means condition information.
For the reverse process, it can also be defined as a Markov chain from the noisy data $x_t$ to the original data $x_0$ as shown in Eq.(4), where $\mu_{\theta}$ and $\Sigma_{\theta}$ are the mean and variance terms respectively. Finally, in order to ensure the speed and quality of inference, we adopt fast sampling~\cite{chen2020wavegrad} and $T_{\text{infer}}=10$.

$$
p_{\theta}\left(x_{t-1} \mid x_{t}\right)=N\left(x_{t-1} ; \mu_{\theta}\left(x_{t},t,\text{cond}\right), \Sigma_{\theta}\left(x_{t}, t\right)\right)
$$

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论