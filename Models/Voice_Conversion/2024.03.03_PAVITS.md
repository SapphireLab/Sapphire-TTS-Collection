---
PDF: 
标题: PAVITS Exploring Prosody-Aware VITS for End-to-End Emotional Voice Conversion
作者:
  - Tianhua Qi
  - Wenming Zheng
  - Cheng Lu
  - Yuan Zong
  - Hailun Lian
机构:
  - 东南大学
代码: 
ArXiv: https://arxiv.org/abs/2403.01494
提出时间: 2024-03-03
出版社:
  - IEEE
发表期刊:
  - International Conference on Acoustics, Speech and Signal Processing (ICASSP)
发表时间: 2024-04-14
引文数量: 26
被引次数: 0
tags:
  - 端到端_End-to-End
  - 声音克隆_VC
  - 情感_Emotional
  - 声音转换_VoiceCoversion
DOI: https://doi.org/10.1109/ICASSP48485.2024.10446191
aliases:
  - PAVITS
  - PAVITS (2024)
ArXiv最新版本: "1"
ArXiv最新时间: 2024-03-03
PageNum: "5"
Demo: https://jeremychee4.github.io/pavits4EVC/
---
# PAVITS: Exploring Prosody-Aware VITS for End-to-End Emotional Voice Conversion

## Abstract

> In this paper, we propose **Prosody-Aware VITS (PAVITS)** for emotional voice conversion (EVC), aiming to achieve two major objectives of EVC: high content naturalness and high emotional naturalness, which are crucial for meeting the demands of human perception.
> To improve the content naturalness of converted audio, we have developed an end-to-end EVC architecture inspired by the high audio quality of VITS.
> By seamlessly integrating an acoustic converter and vocoder, we effectively address the common issue of mismatch between emotional prosody training and run-time conversion that is prevalent in existing EVC models.
> To further enhance the emotional naturalness, we introduce an emotion descriptor to model the subtle prosody variations of different speech emotions.
> Additionally, we propose a prosody predictor, which predicts prosody features from text based on the provided emotion label.
> Notably, we introduce a prosody alignment loss to establish a connection between latent prosody features from two distinct modalities, ensuring effective training.
> Experimental results show that the performance of **PAVITS** is superior to the state-of-the-art EVC methods.
> Speech Samples are available at https://jeremychee4.github.io/pavits4EVC/.

## 1.Introduction

> Emotional voice conversion (EVC) endeavors to transform the state of a spoken utterance from one emotion to another, while preserving the linguistic content and speaker identity [1].
> It brings the capability to facilitate emotional communication between individuals [2], enhancing the user experience in human-computer interaction [3], and even achieving a seamless integration of human presence within the virtual world [4].

> There are two distinct challenges in EVC: one is low content naturalness, and the other is that the converted audio lacks the richness of emotion compared to human voice [1].
> Previous studies were focused on frame-based solutions, such as CycleGAN [5] and StarGAN [6,7].
> However, due to the fixed-length nature and poor training stability, the naturalness of converted audio is quite low to apply in practice.
> To address this challenge, autoencoder-based [8,9] especially for sequence-to-sequence (seq2seq) [10,11] frameworks raise much interests for its variable-length speech generation.
> It achieves an acceptable naturalness through the joint training with Text-to-speech (TTS) [12], which is used to capture linguistic information and avoid mispronunciation as well as skipping-words.
> Since speech emotion is inherently supra-segmental [13], it is difficult to learn emotional representation from the spectrogram.
> To tackle this, various pretraining methods, such as leveraging speech emotion recognition (SER) model [14] and 2-stage training strategy [15], are introduced to extract emotional feature for EVC system.

> Despite these works have achieved great success in EVC, the converted audio still falls short in meeting human’s perceptual needs, which implies that these two challenges still remain to be effectively addressed.
> Remarkably, current EVC models generally operate in a cascade manner, i.e., the acoustic converter and the vocoder [1, 5, 7, 8], resulting in a mismatch between emotional prosody training and run-time conversion, ultimately leading to a degradation in audio quality, which is vital to evaluate content naturalness and impacts the perceptual experience of emotional utterance.
> However, there is no EVC model that attempt to bridge this gap, let alone models that aim to capture prosody variations at a finer granularity.
> To handle the similar issue, multiple solutions have been explored in TTS, including [FastSpeech2s (2020)](../TTS2_Acoustic/2020.06.08_FastSpeech2.md), EATS [17], [VITS (2021)](../E2E/2021.06.11_VITS.md) [19], etc., seeking to alleviate the mismatch between acoustic feature generation and waveform reconstruction by integrating these two stages together.

> In this paper, inspired by the high audio quality of [VITS (2021)](../E2E/2021.06.11_VITS.md), we propose **Prosody-Aware VITS (PAVITS)** for EVC, a novel end-to-end system with implicit prosody modeling to enhance content naturalness and emotional naturalness.
> To our best knowledge, **PAVITS** is the first EVC method in solving the mismatch between acoustic feature conversion and waveform reconstruction.
> Compared to [original VITS (2021)](../E2E/2021.06.11_VITS.md), our approach involves several key innovations.
> In order to improve content naturalness with speech quality, we build upon [VITS (2021)](../E2E/2021.06.11_VITS.md) to solve the two-stage mismatch in EVC, and apply multi-task learning since TTS can significantly reduce the mispronunciation.
> To enhance emotional naturalness, we introduce an emotion descriptor to capture prosody differences associated with different emotional states in speech.
> By utilizing Valence-Arousal-Dominance values as condition, emotional representation at utterance-level is learned.
> Latent code is further refined by a prosody integrator, which incorporates with speaker identity and linguistic content to model finer-grained prosody variations.
> Then frame-level prosody features are obtained from normalizing flow.
> We also introduce a prosody predictor that leverages emotion labels and phoneme-level text embedding to predict frame-level emotional prosody features.
> Finally, we devise a prosody alignment loss to connect two modalities, aligning prosody features obtained from audio and text, respectively.

## 2.Proposed Method

> As shown in Fig.01, inspired by [VITS (2021)](../E2E/2021.06.11_VITS.md), the proposed model is constructed based on conditional variational autoencoder (CVAE), consisting of four parts: a textual prosody prediction module, an acoustic prosody modeling module, an information alignment module, and an emotional speech synthesis module.

> The textual prosody prediction (TPP) module predicts the prior distribution $p(z_1|c_1)$ as:

$$
    z_1= TPP (c_1) \sim p (z_1| c_1)\tag{1} 
$$

> where $c_1$ including text $t$ and emotion label $e$.

> The acoustic prosody modeling (APM) module disentangles emotional features with intricate prosody variation, speaker identity, and linguistic content from the source audio given emotion label, forming the posterior distribution $q(z_2|c_2)$ as: 

$$
    z_2= APM (c_2) \sim q (z_2|c_2)\tag{2} 
$$

> where $c_2$ including audio $y$ and emotion label $e$.

> The information alignment module facilitates the alignment of text and speech, as well as the alignment of textual and acoustic prosody representations.
> In emotional speech synthesis (ESS) module, the decoder reconstructs waveform $\hat{y}$ according to latent representation $z$.

$$
    \hat{y} = Decoder (z) \sim p (y | z)\tag{3}
$$

> where $z$ comes from $z_1$ or $z_2$.

> While the proposed model can perform both EVC and emotional TTS after training, EVC will be the main focus of this paper.
> In the following, we will introduce the details of the four modules.

### 2.1.Textual Prosody Prediction Module

> Given condition $c_1$ including text $t$ and emotion label $e$, the textual prosody prediction module provides the prior distribution $p(z_1|c_1)$ of CVAE.
> The text encoder takes phonemes as input and extracts linguistic information $h_{text}$ at first.
> Considering the extensive prosody variation associated with each phoneme, we employ a prosody predictor to extend the representation to frame-level and predict the prosody variation (a fine-grained prior normal distribution with mean $\mu_{\theta}$ and variance $\sigma_{\theta}$ generated by a normalizing flow $f_{\theta}$) based on emotion label. 

$$
    p(z_1|c_1) = \mathcal{N}(f_{\theta}(z_1); \mu_{\theta}(c_1);\sigma_{\theta}(c_1))\left|\det\dfrac{\partial f_{\theta}(z_1)}{\partial z}\right|\tag{4}
$$

> Text Encoder: Since the training process is constrained by the volume of textual content within parallel datasets, we initially convert text or characters into a phoneme sequence as a preprocessing step to maximize the utility of the available data, resulting in improved compatibility with the acoustic prosody modeling module.
> Similar to [VITS (2021)](../E2E/2021.06.11_VITS.md), text encoder comprises multiple Feed-Forward Transformer (FFT) blocks with a linear projection layer for representing linguistic information.

> Prosody Predictor: Prosody predictor leverages phoneme-level linguistic information extracted by the text encoder to anticipate frame-level prosody variation given discrete emotion label.
> It has been observed that simply increasing the depth of stacked flow does not yield satisfactory emotional prosody variations, unlike the prosody predictor.
> Therefore, the inclusion of the prosody predictor guarantees a continuous enhancement in prosody modeling for both the TPP and APM modules.
> The prosody predictor comprises multiple one-dimensional convolution layers and a linear projection layer.
> Furthermore, we integrate predicted emotional prosody information with linguistic information as input for the duration predictor, which significantly benefits the modeling of emotional speech duration.

### 2.2.Acoustic Prosody Modeling Module

> The acoustic prosody modeling module provides emotional features with fine-grained prosody variation based on dimensional emotion representation, i.e., Valence-Arousal-Dominance values.Speaker identity and speech content information are also disentangled from the source audio and then complete feature fusion through the prosody integrator as the posterior distribution $q (z_2|c_2)$. 

$$
    q(z_2|c_2) = \mathcal{N}(f_{\theta}(z_2); \mu_{\theta}(c_2);\sigma_{\theta}(c_2))\tag{5}
$$

> Speaker encoder: Considering the APM module’s increased focus on understanding emotional prosody more thoroughly compared to previous models, it’s apparent that speaker characteristics could unintentionally be overlooked during conversion.Recognizing the critical role of fundamental frequency (F0) in speaker modeling [20], we augment the F0 predictor of [VISinger (2021)](../../Models/Singing_Voice/2021.10.17_VISinger.md) by adding multiple one-dimensional convolutional layers and a linear layer to construct the speaker encoder, which tackles the issue effectively.

> Emotion descriptor: To enhance **PAVITS**’s emotional naturalness, we employ a specific SER system rooted in Russell’s circumplex theory [22] to predict dimensional emotion representation, encompassing Valence-Arousal-Dominance values as a conditional input.
> This input guides the capture of nuanced prosody variations, which ensures that while satisfying human perception of emotions at utterance-level, natural prosody variations are retained from segment-level down to frame-level, preserving intricate details.
> It consists of a SER module [23] and a linear projection layer.

> Prosody Integrator: The prosody integrator incorporates a combination of speaker identity attributes, emotional prosody characteristics, and intrinsic content properties extracted from the linear spectrogram.
> It is constructed using multiple convolution layers, WaveNet residual blocks, and a linear projection layer.

### 2.3.Information Alignment Module

> In [VITS (2021)](../E2E/2021.06.11_VITS.md), the existing alignment mechanism, which is called Monotonic Alignment Search (MAS), solely relies on textual and acoustic features from parallel datasets.
> Thus, it is insufficient in capturing emotional prosody nuances, hindering effective linkage between the TPP and APM modules.
> To overcome this limitation, we propose an additional prosody alignment loss function based on Kullback-Leibler divergence, to facilitate joint training for frame-level prosody modeling across the TPP and APM modules, with the goal of enhancing prosody information integration and synchronization within our model.

$$
    L_{psd} = D_{KL}(q(z_2|c+2)\| p(z_1|c_1))\tag{6}
$$

### 2.4.Emotional Speech Synthesis Module

> In the emotional speech synthesis module, the decoder generates a waveform based on latent $z$, employing adversarial learning to continuously enhance naturalness in both content and emotion.
> To improve the naturalness of content, $L_{recon-cls}$ minimizes the $L_1$ distance between predicted and target spectrograms, $L_{recon-fm}$ minimizes the $L_1$ distance between feature maps extracted from intermediate layers in each discriminator, aimed at enhancing training stability.
> Since the former predominantly influences the early-to-mid stage, while the latter assumes a more prominent role in mid-to-late stage, we introduce two coefficients to balance their contributions as follows.

$$
    L_{recon}= \gamma L_{recon-cls}+ \beta L_{recon-fm}(G)\tag{7}
$$

> To enhance the perception of emotions, $L_{emo-cls}$ represents the loss function for emotional classification, while $L_{emo-fm}$ denotes the loss associated with feature mapping for emotion discrimination.

$$
    L_{emo}= L_{emo-cls}+ \beta L_{emo-fm}(G)\tag{8}
$$

### 2.5.Final Loss

> By combining CVAE with adversarial training, we formulate the overall loss function as follows:

$$
\begin{align}
    L &= L_{recon}+ L_{adv}(G) + L_{emo}+ L_{psd}+ L_{F0}+ L_{dur}\tag{9}\\
    L(D) &= L_{adv}(D)\tag{10}
\end{align}
$$

> where $L_{adv}(G)$ and $L_{adv}(D)$ represent the adversarial loss for the Generator and Discriminator respectively, $L_{F0}$ minimizes the $L_2$ distance between the predicted F0 and corresponding ground truth, $L_{dur}$ minimizes the $L_2$ distance between the predicted duration and ground truth which is obtained through estimated alignment.

### 2.6.Run-Time Conversion

> At runtime, there are two converting methods: a fixed-length approach (Audio-$z_2$-Audio, named **PAVITS-FL**) and a variable-length approach (Audio-Text-$z_1$-Audio, named **PAVITS-VL**).
> The former uses APM module for latent $z$ prediction from audio, ensuring robustness as it remains unaffected by text encoding, but is constrained by a fixed spectrum length due to Dynamic Time Warping (DTW) limitations.
> The latter employs TPP module to predict latent $z$ from corresponding text obtained through automatic speech recognition (ASR) technique, which is not bound by duration modeling and offers greater naturalness.
> Finally, the ESS module’s decoder takes latent $z$ (either $z_1$ or $z_2$) as input and synthesizes the converted waveform without a separate vocoder.

## 3.Experiments

### 3.1.Datasets

> We perform emotional conversion on a Mandarin corpus belonged to Emotional Speech Dataset (ESD) [24] from neutral to angry, happy, sad, and surprise, denoted as Neu-Ang, Neu-Hap, Neu-Sad, Neu-Sur respectively.
> For each emotion pair, we use 300 utterances for training, 30 utterances for evaluation, and 20 utterances for test.
> The total duration of training data is around 80 minutes (16 minutes per emotion category), which is absolutely small compared to others.

### 3.2.Experimental Setup

> We train the following models for comparison.
> - CycleGAN [25] (baseline): CycleGAN-based EVC model with WORLD vocoder.
> - StarGAN [26] (baseline): StarGAN-based EVC model with WORLD vocoder.
> - Seq2seq-WA2 [15] (baseline): Seq2seq-based EVC model employing 2-stage training strategy with WaveRNN vocoder.
> - [VITS (2021)](../E2E/2021.06.11_VITS.md) (baseline):EVC model constructed by original VITS, operating independently in both fixed-length and variable-length, take the average as the result.
> - **PAVITS-FL** (proposed): the proposed model based on VITS, incorporates all the contributions outlined in the paper, but operate within a fixed-length framework.
> - **PAVITS-VL** (proposed): the proposed model based on VITS, incorporates all the contributions outlined in the paper, but operate within a variable-length framework leveraging ASR to obtain text from source audio.

### 3.3.Results & Discussion

> Mel-cepstral distortion (MCD) was calculated for objective evaluation, as depicted in Tab.01.In terms of subjective evaluation, Mean Opinion Score (MOS) tests were conducted to appraise both the quality and naturalness of speech as shown in Tab.02.
> The naturalness score was derived by averaging the scores for content naturalness and emotional prosody naturalness, as rated by 24 participants, each of whom assessed a total of 148 utterances.
> We further report emotional similarity results between converted audio and human voice to gauge emotional naturalness as illustrated in Fig.02.

> Through the above-mentioned metrics, it is obvious that the proposed **PAVITS** achieves competitive performance on both objective and subjective evaluation.
> From the perspective of objective MCD and subjective MOS, both original VITS and our proposed **PAVITS** models always outperform other models with traditional vocoder or neural vocoder, which proves that the integration of neural acoustic converter and vocoder is suitable for EVC task to enhance speech quality and naturalness.
> It is worth noting that even in the case of the fixed-length **PAVITS-FL** model, there is a reduction of over 0.4 in MCD when compared to the variable-length seq2seq model and the original VITS model.
> Furthermore, there has been an enhancement of 0.6 and 0.2 in MOS, respectively.
> To some extent, it reflects how human tend to be influenced by audio quality when assessing model naturalness, especially when there are significant differences in quality being compared.

> As depicted in Fig.02, our proposed **PAVITS-VL** (variable-length) model aligns more closely with human perception in the converted audio, which attributed to the model’s capacity for fine-grained granularity in modeling speech emotion, incorporating implicit prosody cues.To further show the effectiveness of our method, we visualize the spectrogram of testing clips, as exemplified in Fig.03.
> It is readily apparent that the spectrogram converted by **PAVITS** exhibits finer details in prosody variations within the pertinent frequency bands, while simultaneously preserving descriptive information for other frequency bands.
> Consequently, the audio generated by **PAVITS** possesses a prosody naturalness and emotional accuracy that closely approximates the ground truth spectrogram.

### 3.4.Ablation Study

> We further conduct an ablation study to validate different contributions.We remove prosody predictor, prosody alignment, and prosody integrator in turn and let the subjects evaluate quality and naturalness of converted audio.
> From Tab.03, we can see that all scores are degraded with the removal of different components.
> When remove prosody predictor, the speech quality does not undergo significant changes, as the original VITS primarily relies on textual features as input.
> However, a significant decrease in naturalness is observed, attributed to the loss of explicit emotion label for TPP module as a conditioning factor.
> This highlights the importance of aligning with APM module on the basis of information asymmetry, which reflects the ingenious design of prosody modeling structure.
> Note that the performance of **PAVITS** is worse than VITS after deleting prosody alignment, it might be attributed the fact that latent prosody representations are not constrained during training, which damages the original MAS mechanism present in VITS.
> To further show the contribution from the prosody integrator, we replace it with a simple concatenation.
> Both speech quality and naturalness show a slight decrease, indicating that utilizing prosody integrator for information fusion is quite effective for APM module.

## 4.Conclusion

> In this paper, we propose **Prosody-Aware VITS (PAVITS)** for emotional voice conversion (EVC).
> By integrating acoustic prosody modeling (APM) module with textual prosody prediction (TPP) module through prosody alignment, the fine-grained emotional prosody features across various scales of emotional speech can be learned effectively.
> Experimental results on ESD corpus demonstrate the superiority of our proposed **PAVITS** for content naturalness and emotional naturalness, even when dealing with limited data scenarios.
> In the future, we will explore the controllable emotional prosody modeling to allow better interpretability of EVC.