# VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning

<details>
<summary>基本信息</summary>

- 标题: "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning."
- 作者:
  - 01 Yixuan Zhou
  - 02 Guoyang Zeng
  - 03 Xin Liu
  - 04 Xiang Li
  - 05 Renjie Yu
  - 06 Ziyang Wang
  - 07 Runchuan Ye
  - 08 Weiyue Sun
  - 09 Jiancheng Gui
  - 10 Kehan Li
  - 11 Zhiyong Wu
  - 12 Zhiyuan Liu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.24650v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2509.24650v1](PDF/2025.09.29_2509.24650v1_VoxCPM__Tokenizer-Free_TTS_for_Context-Aware_Speech_Generation_and_True-to-Life_Voice_Cloning.pdf)
  - [Publication] #TODO

</details>

## Abstract

Generative models for speech synthesis face a fundamental trade-off: discrete tokens ensure stability but sacrifice expressivity, while continuous signals retain acoustic richness but suffer from error accumulation due to task entanglement. 
This challenge has driven the field towards multi-stage pipelines that rely on pre-trained speech tokenizers, but these create a semantic-acoustic divide, limiting holistic and expressive speech generation.  
We resolve these dilemma through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel tokenizer-free TTS model--\modelname{}.
Our framework introduces a differentiable quantization bottleneck that induces natural specialization: 
a Text-Semantic Language Model (TSLM) generates semantic-prosodic plans, while a Residual Acoustic Model (RALM) recovers fine-grained acoustic details.
This hierarchical semantic-acoustic representation guides a local diffusion-based decoder to generate high-fidelity speech latents. 
Critically, the entire architecture is trained end-to-end under a simple diffusion objective, eliminating dependency on external speech tokenizers. 
Trained on a massive 1.8 million hours of bilingual corpus, our \modelname{}-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis. 
Besides, VoxCPM shows the capability to comprehend text to infer and generate appropriate prosody and style, delivering speech with context-aware expressiveness and natural flow. 
To facilitate community-driven research and development, \modelname{} is publicly accessible under Apache 2.0.

## 1·Introduction

The pursuit of modern text-to-speech (TTS) systems has evolved beyond intelligibility toward the synthesis of genuinely human-like audio, capable of conveying subtle emotions, speaker identity, and contextual nuances [^Shen2018Natural], [^Ping2017Deep], [^Ren2020FastSpeech], [^Li2019Neural]. 
This leap is critical for applications like empathetic virtual assistants and immersive digital avatars, and hinges on a core technical challenge: simultaneously capturing the fine-grained acoustic details that define vocal richness and the long-range semantic structures governing intelligibility and natural prosody.

Inspired by the success of large language models (LLMs), a dominant paradigm frames TTS as a sequence modeling task over discrete tokens from pre-trained neural audio codecs (e.g., EnCodec [^D{\'e}fossez2022High]).

Autoregressively or Non-autoregressively predicting these tokens from text or phonemes [^Borsos2023Audiolm], [^Kharitonov2023Speak,], [^Chen2025Neural], [^WangMaskGCT], [^Peng2024VoiceCraft] offers excellent scalability and in-context learning capabilities. 
However, this approach faces a fundamental "quantization ceiling", as the compression process irreversibly discards subtle acoustic details.

To mitigate this quality loss, state-of-the-art TTS systems [^Du2024Cosyvoice], [^Du2024Cosyvoice], [^Du2025Cosyvoice], [^Zhou2025IndexTTS2], [^Casanova2024Xtts] adopt multi-stage hybrid pipelines.

Here, an LLM generates discrete tokens which condition a separate diffusion-based decoder.

While improving fidelity, this solution creates a stark semantic-acoustic divide: the LLM operates in an abstract, discrete space unaware of acoustic reality, while the diffusion model performs local refinement without high-level context. 
This fragmentation prevents end-to-end optimization and limits holistic, expressive and context-aware speech synthesis.

Alternatively, other approaches directly model continuous speech representations to avoid quantization loss. 
Early systems like Tacotron 2 [^Shen2018Natural] and more recent models such as MELLE [^Meng2024Autoregressive] generate mel-spectrograms autoregressively. 
However, predicting continuous targets under standard regression losses often yields over-smoothed and low-diversity outputs. 
To address this, recent innovations have explored replacing the regression objective with a denoising process to model the distribution of the next continuous representations, spanning both non-autoregressive paradigms [^Shen2023NaturalSpeech], [^Le2023Voicebox], [^Chen2024F5-TTS] and autoregressive methods[^Li2024Autoregressive], [^Jia2025Ditar], [^Peng2025Vibevoice]. 
Among these, autoregressive approaches have often demonstrated superior performance in capturing natural prosody and expressive variation.

This innovation successfully enhances the detail and diversity of generated continuous representations. 
However, a more fundamental issue persists: in a fully continuous autoregressive model, the tasks of high-level semantic-prosodic planning and fine-grained acoustic rendering are conflated within a single learning objective. 
The model is forced to simultaneously solve two disparate tasks—requiring different inductive biases—in a continuous output space. 
This entanglement presents a significant challenge to the modeling capacity of a single LLM, as it must learn to be both a global planner and a local renderer without an inherent architectural bias to separate these functions.

We argue that this conflation is a root cause of instability. 
The model's focus is inevitably pulled towards fitting low-level acoustic textures, which compromises its ability to maintain high-level semantic coherence, leading to the well-known problem of error accumulation over long sequences [^Pasini2024Continuous].

In this work, we introduce a tokenizer-free, end-to-end framework that resolves this trade-off through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel TTS model--\modelname{}.

Our key insight is that holistic and expressive speech synthesis requires explicit architectural separation between semantic-prosodic planning and acoustic rendering, yet should remain within a cohesive, end-to-end trainable system.

The core innovation is a differentiable Finite Scalar Quantization (FSQ) [^MentzerFinite] bottleneck that induces natural specialization:
(1) a Text-Semantic Language Model (TSLM) generates semantic-prosodic plans stabilized through quantization, focusing on linguistically meaningful patterns;
and (2) a Residual Acoustic Language Model (RALM) recovers fine-grained details lost during quantization, specializing in acoustic refinement.

This hierarchical design enables each component to excel at its respective role while maintaining differentiability, and both of them will be used to guide a local diffusion decoder to generate high-fidelity speech latents.

Critically, the entire hierarchical model is trained end-to-end under a simple diffusion objective, seamlessly integrating planning and rendering without pre-trained tokenizers.

Trained on a massive 1.8 million hours of bilingual corpus, our \modelname{}-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis. 
Our main contributions are as follows:
\vspace{-2mm}

[itemsep=0.2em, leftmargin=2em]

-  We propose an end-to-end hierarchical architecture that introduces an internal semi-discrete bottleneck to resolve the expressivity-stability trade-off.

This mechanism implicitly addresses task entanglement in continuous models by inducing a beneficial separation between semantic-prosodic planning and fine-grained acoustic modeling within a single, unified framework.

-  We introduce a residual learning strategy that, in conjunction with the bottleneck, enables a holistic yet specialized modeling process.

Unlike fragmented multi-stage pipelines, our approach achieves functional separation without architectural fragmentation, simplifying the training pipeline and eliminating dependency on external speech tokenizers.

-  We demonstrate the efficacy of our approach through large-scale training on a massive 1.8 million hours of bilingual speech.

The resulting model, VoxCPM-0.5B, achieves state-of-the-art zero-shot TTS performance among open-source systems with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, validating its practical strength.

-  We provide extensive ablation studies that conclusively validate the semi-discrete residual representations as the crucial component for robust, expressive, and l context-aware synthesis.

Besides, we release the codes and models publicly to support community development and future research.

## 2·Related Work

### Discrete Token-Based TTS

The discrete token paradigm has emerged as a dominant approach in modern TTS, leveraging the success of large language models.

This method converts speech into discrete representations using neural audio codecs such as EnCodec [^D{\'e}fossez2022High] and DAC [^Kumar2023High-Fidelity] through residual vector quantization (RVQ).

AudioLM [^Borsos2023Audiolm] and VALL-E [^Chen2025Neural] pioneered this direction by framing audio generation and TTS as an autoregressive sequence prediction task over discrete acoustic tokens.

Subsequent developments include SoundStorm [^Borsos2023Soundstorm], which introduced non-autoregressive generation for improved efficiency, and Spear-TTS [^Kharitonov2023Speak,], which focused on multilingual capabilities with minimum supervision. 
Besides, VoiceCraft [^Peng2024VoiceCraft] and XTTS [^Casanova2024Xtts] further advanced zero-shot TTS with in-context learning.

Recent advancements have focused on enhancing the scalability, controllability and zero-shot adaptation. 
CosyVoice [^Du2024Cosyvoice] proposed supervised semantic tokens for improved zero-shot performance, while its successors, 

CosyVoice 2 and 3 [^Du2024Cosyvoice], [^Du2025Cosyvoice]
incorporated text-based LLM initialization, streaming synthesis, and large-scale training data for human-parity quality, low latency and in-the-wild scenarios.

IndexTTS [^Deng2025Indextts] and IndexTTS2 [^Zhou2025IndexTTS2] introduced precise duration and emotion control in autoregressive token generation, enabling applications with strict timing and expressivity requirements.

SparkTTS [^Wang2025Spark-TTS] utilized single-stream decoupled speech tokens for modeling efficiency, and FireRedTTS [^Guo2024Fireredtts] along with its update FireRedTTS-2 [^Xie2025FireRedTTS-2] established frameworks for industry-level generative speech, including long-form multi-speaker dialogue.   
Openaudio-s1 [^OpenAudio2024OpenAudio] used dual AR architecture and online Reinforcement Learning from Human Feedback (RLHF) to improve expressiveness and instruction-following capabilities.

Higgs Audio v2 [^BosonAI2025Higgs] proposed a unified audio tokenizer captures
both semantic and acoustic features, and  pretrained on over 10 million hours of audio data, providing a powerful foundation model. 
Despite these progresses, discrete approaches suffer from inherent quantization artifacts, limiting acoustic fidelity and prompting hybrid solutions.

\vspace{-0.2cm}

### Continuous Representation TTS

To circumvent quantization losses in discrete models, continuous representation approaches directly model speech features such as mel-spectrograms or audio latents.

Early systems like Tacotron 2 [^Shen2018Natural] established the encoder-decoder framework for text-to-mel mapping, while FastSpeech [^Ren2020FastSpeech] introduced explicit duration modeling for alignment stability.

Inspired from VALL-E,  MELLE [^Meng2024Autoregressive] autoregressively generated continuous mel-spectrogram frames directly from text condition, and incorporated variational inference to facilitate sampling mechanisms.

Recent developments have integrated diffusion processes to enhance detail and diversity.

Non-autoregressive models like NaturalSpeech 2 [^Shen2023NaturalSpeech] and VoiceBox [^Le2023Voicebox] apply diffusion directly on continuous representations.

F5-TTS [^Chen2024F5-TTS] advanced flow-matching for efficient synthesis. 
Autoregressive paradigms, often superior in prosody and variation, additionally possess the capability for streaming synthesis.

Innovations like ARDiT [^Li2024Autoregressive] use an autogressive diffusion transformer for TTS, unifying semantic coherence and acoustic naturalness via parameter sharing. 
DiTAR [^Jia2025Ditar] extended this with a patch-based design: a causal LM for inter-patch stability and a bidirectional local diffusion transformer for intra-patch refinement. 
VibeVoice [^Peng2025Vibevoice] employed next-token diffusion for long-form multi-speaker synthesis. 
Besides, recent models such as CLEAR [^Wu2025Clear] and FELLE [^Wang2025Felle] focus on latent autoregressive modeling with token-wise coarse-to-fine hierarchies, while MELA-TTS [^An2025Mela-TTS] and KALL-E [^Zhu2024Autoregressive] combine joint transformer-diffusion with next-distribution prediction for improved efficiency and quality.

Despite these advances, continuous models often entangle high-level semantic planning with low-level acoustic rendering, leading to instability in long sequences without explicit separation.

\vspace{-0.2cm}

### Hierarchical and Residual Modeling in TTS

Hierarchical and residual approaches decompose TTS into layered tasks to balance stability and expressivity.

HierSpeech++ [^Lee2025Hierspeech++] employed variational inference for semantic-acoustic mapping. 
HALL-E [^NishimuraHall-E] uses hierarchical neural codecs with LLMs for minute-long synthesis.

MARS6 [^Baas2025Mars6] builds robust encoder-decoder transformers with hierarchical tokens.

DiffStyleTTS [^Liu2024DiffStyleTTS] applies diffusion for hierarchical prosody modeling.

HAM-TTS [^Wang2024Ham-TTS] introduces hierarchical acoustic modeling with data augmentation for zero-shot TTS. 
QTTS [^Han2025Quantize] features hierarchical parallel architectures for residually quantized codes. 
In song generation, LeVo [^Lei2025LeVo] likewise introduced a hierarchical framework using two decoder-only transformers for layered modeling of mixed and separated part in a song, achieving enhanced generation quality.

These methods address flaws in prior paradigms: implicit designs lack regulated bottlenecks, tokenizer-dependent models suffer discrete losses, and fragmented stages hinder end-to-end optimization.

However, few fully integrate explicit residual designs with semi-discrete bottlenecks in a unified framework, as proposed in our work, to achieve implicit disentanglement without external dependencies.

## 3·Methodology

![](images/draft_1.png)

<a id="fig:architecture">Overall architecture of VoxCPM.

The model hierarchically generates speech by first processing audio latents through a LocEnc, then producing a semi-discrete speech skeleton with the TSLM and FSQ, refining acoustic details with the RALM, and finally generating high-fidelity latent output with the LocDiT.</a>

### Core Design Motivation

\label{sec:motivation}

Generative speech synthesis faces a fundamental tension between expressivity and stability.

Discrete tokenization methods (e.g., speech tokenizers with language models) ensure stable autoregressive generation but irreversibly discard fine-grained acoustic details through quantization. 
Continuous approaches preserve full fidelity but suffer from error accumulation in long sequences due to information entanglement, often leading to catastrophic failure in intelligibility.

Critically, we identify a key limitation in existing discrete tokenization approaches: methods that directly use FSQ or VQ to obtain discrete codebooks for language modeling face an inherent scalability challenge.

As the dimensionality increases to capture richer acoustic information, the codebook size grows exponentially, creating an unmanageably large and sparse vocabulary that language models struggle to predict accurately.

We hypothesize that an effective solution should **structurally separate** the modeling of stable semantic-prosodic content from fine-grained acoustic details while maintaining differentiability for end-to-end training. 
Our key insight is to introduce a **differentiable quantization bottleneck** that naturally induces this separation through scalar quantization, splitting information into a discrete-like skeleton for content stability and continuous residual components for detail expressivity.

Unlike multi-stage TTS systems composed of seperate LM and diffusion that treat quantization as a means to obtain discrete prediction targets, our approach uses quantization solely as a regularization mechanism to constrain the hidden state space. 
This distinction allows us to avoid the vocabulary explosion problem while still benefiting from the stabilizing effects of discrete representations.

### Model Overview

\label{sec:overview}

VoxCPM employs a hierarchical autoregressive architecture that generates sequences of continuous speech latents $\mathbf{Z} = \{\mathbf{z}_1, ..., \mathbf{z}_M\}$ conditioned on input text tokens $\mathbf{T} = \{t_1, ..., t_N\}$,
where each $\mathbf{z}_i \in \mathbb{R}^{P \times D}$ represents a patch of $P$ frames with $D$-dimensional VAE latent vectors. 
The generation process follows:

$$
p(\mathbf{Z}|\mathbf{T}) = \prod_{i=1}^M p(\mathbf{z}_i | \mathbf{T}, \mathbf{Z}_{<i})
$$

The core innovation lies in our hierarchical conditioning mechanism with residual representation learning.

It is made up of a local audio encoder (LocEnc), a text-semantic language model (TSLM), a residual acoustic language model (RALM) and a local diffusion transformer decoder (LocDiT).

A stop predictor is attached to the output of the TSLM to determine the endpoint of generation. 
As shown in Figure~[fig:architecture](#fig:architecture), each patch generation involves:

$$
\mathbf{z}_i \sim \text{LocDiT}(\mathbf{h}_i^{\text{final}}), \quad 
\mathbf{h}_i^{\text{final}} = \underbrace{\text{FSQ}(\text{TSLM}(\mathbf{T}, \mathbf{E}_{<i}))}_{\text{stable skeleton}} + \underbrace{\text{RALM}(\cdot)}_{\text{residual details}}
$$

where $\mathbf{E}_{<i} = \text{LocEnc}(\mathbf{Z}_{<i})$ represents historical audio context aggregated by a lightweight LocEnc that compresses VAE latent patches into compact acoustic embeddings.

The hierarchical backbone produces a conditioning signal $\mathbf{h}_i^{\text{final}}$ that encapsulates both semantic content from TSLM (with FSQ) and acoustic details from RALM. 
This signal guides the LocDiT to generate the current latent patch $z_i$ through a denoising diffusion process.

The entire model is trained end-to-end with gradients flowing through all components, including the FSQ bottleneck via straight-through estimation, ensuring coordinated optimization toward holistic speech synthesis.

### Hierarchical Semantic-Acoustic Modeling

\label{sec:hierarchical}

Our hierarchical modeling approach is designed to implicitly separate semantic-prosodic planning from fine-grained acoustic synthesis, addressing the fundamental stability-expressivity trade-off through structured representation learning.

#### Text-Semantic Language Model (TSLM)

\label{sec:tslm}

The Text-Semantic Language Model forms the main part of our hierarchical architecture, responsible for capturing high-level linguistic structure and generating contextually appropriate speech patterns. 
Unlike conventional TTS systems that typically operate on phoneme sequences, our approach leverages a pre-trained text language model (MiniCPM-4[^Team2025Minicpm4]) as its initial backbone, enabling richer contextual understanding and more natural prosody prediction directly from raw text.

Specifically, we employ character-level segmentation for Chinese BPE Tokenizer to mitigate the vocabulary sparsity issue in TTS tasks.

By processing both text tokens and historical audio context, the TSLM learns to generate semantic content and prosodic structure that evolve naturally throughout an utterance, reflecting the underlying linguistic meaning rather than simply mapping phonemes to acoustic features.

The TSLM produces continuous semantic-prosodic representations that encode both the content to be spoken and how it should be prosodically realized, serving as input to the subsequent quantization stage.

#### Semi-Discrete Representation Learning via FSQ

\label{sec:fsq}

At the core of our approach lies the Finite Scalar Quantization (FSQ) layer, which projects the continuous hidden states from the TSLM onto a structured lattice to create a semi-discrete representation. 
The FSQ operation transforms each dimension of the continuous vector through a deterministic scalar quantization:

$$
\mathbf{h}_{i,j}^{\text{FSQ}} = \Delta \cdot \text{clip}\left( \text{round}\left( \frac{\mathbf{h}_{i,j}^{\text{TSLM}}}{\Delta} \right), -L, L \right)
$$

where $\Delta$ is the quantization step size, $L$ is the clipping range, and $\text{round}$ maps values to discrete levels. 
This transformation creates a structured discrete representation while maintaining differentiability through the straight-through estimator during backward passes.

The FSQ layer acts as a bottleneck, analogous to the first layer of Residual Vector Quantization (RVQ), which captures a coarse semantic-prosodic skeleton (e.g., content, intonation patterns). 
We term this representation "semi-discrete" as it employs a significantly larger dimensionality than standard FSQ to ensure sufficient informational capacity.

Unlike RVQ, where the first layer is a prediction target and subsequent layers model finer details, our FSQ bottleneck serves as an intermediate, differentiable inductive bias  within the continuous data flow. 
It encourages the model to prioritize modeling stable, high-level components (the semantic-prosodic skeleton) by providing a clear learning signal for what information should be preserved through the bottleneck. 
This structured approach mitigates error accumulation by reducing the modeling burden on the TSLM, allowing it to focus on the major components of the speech.

#### Residual Acoustic Modeling

\label{sec:ralm}

To recover the fine-grained acoustic information attenuated by quantization, we introduce the Residual Acoustic Language Model (RALM). 
This module specializes in reconstructing those subtle vocal characteristics that conventional discrete methods sacrifice for stability. 
It processes the quantization residuals along with contextual information to recover speaker identity, spectral fine structure, and micro-prosodic variations:

$$
\mathbf{h}_i^{\text{residual}} = \text{RALM}( \mathbf{H_{\text{text}}^{\text{TSLM}}}, \mathbf{H}_{<i}^{\text{FSQ}} \oplus \mathbf{E}_{<i})
$$

Here, the RALM conditions its predictions on both the TSLM hidden states of the text part $\mathbf{H_{\text{text}}^{\text{TSLM}}}$, the semi-discrete representation of speech part $\mathbf{H}_{<i}^{\text{FSQ}}$, and the historical acoustic embeddings $\mathbf{E}_{<i}$.

This residual learning approach  creates a natural division of labor: the TSLM+FSQ pathway focuses on content stability and prosodic coherence, while the RALM pathway specializes in acoustic expressivity and speaker characteristics. 

The final combined representation $\mathbf{h}_i^{\text{final}} = \mathbf{h}_i^{\text{FSQ}} + \mathbf{h}_i^{\text{residual}}$ thus encapsulates both semantic stability and acoustic expressivity, creating a comprehensive signal that guides the subsequent local diffusion process.

#### Local Diffusion Transformer  Decoder

\label{sec:locdit}

The Local Diffusion Transformer (LocDiT) serves as our high-fidelity synthesis module, generating continuous latent patches conditioned on the hierarchical representation $\mathbf{h}_i^{\text{final}}$ produced by the preceding modules.

Following DiTAR [^Jia2025Ditar], we employ a bidirectional Transformer architecture that enables full receptive field modeling within each patch. 
To enhance generation consistency, we incorporate the previous patch $\mathbf{z}_{i-1}$ as additional conditioning context, which has been empirically validated to significantly improve output quality by framing the task as outpainting rather than independent patch generation.

Besides, we mask the LM guidance in LocDiT condition with a specific probability ratio, for enabling classifier-free guidance (CFG) during inference.

### Training Objective

The entire model is trained end-to-end using a flow-matching objective that directly optimizes the quality of the generated speech latents.

We adopt the conditional flow-matching formulation for its training stability and sampling efficiency:

$$
\mathcal{L}_{\text{FM}} = \mathbb{E}{t, \mathbf{z}_i^0, \boldsymbol{\epsilon}} \left[ | \mathbf{v}_{\theta}(\mathbf{z}_i^t, t, \mathbf{h}_i^{\text{final}}, \mathbf{z}_{i-1}) - \frac{d}{dt}(\alpha_t \mathbf{z}_i^0 + \sigma_t \boldsymbol{\epsilon}) |^2 \right]
$$

where $\mathbf{z}_i^t = \alpha_t \mathbf{z}_i^0 + \sigma_t \boldsymbol{\epsilon}$ is the noisy latent at time $t$, with $\boldsymbol{\epsilon} \sim \mathcal{N}(0, I)$, and $\mathbf{v}_{\theta}$ is the velocity field predicted by the LocDiT.

Simultaneously, a binary classification loss is applied to train the model to predict the end of a speech sequence:

$$
\mathcal{L}_{\text{Stop}} = \mathbb{E}_{i \sim \text{sequence}} \left[ \text{BCE}\left(s_{\theta}(\mathbf{h}_i^{\text{FSQ}}), \mathbb{1}[\text{token } i \text{ is the last}]\right) \right]
$$

where $s_{\theta}$ is a stop-logit projection layer, and BCE denotes the binary cross-entropy loss.

The gradients from this loss are backpropagated through the entire autoregressive hierarchy, including the FSQ layer (via straight-through estimation), the TSLM and the LocEnc.

This end-to-end optimization 
under the combined objective $\mathcal{L} = \mathcal{L}_{\text{FM}} + \lambda \mathcal{L}_{\text{Stop}}$ 
allows each component to learn its specialized role—semantic planning, stabilization, and acoustic refinement—in a coordinated manner, guided by the unified objective of accurately modeling the continuous speech latents.

### Causal Audio VAE

\label{sec:vae}

To enable efficient streaming synthesis, we employ a causal Variational Autoencoder that operates in a computationally efficient latent space. 
VAE is trained separately using a composite objective that combines reconstruction loss in the Mel-spectrogram domain, adversarial training with multi-period and multi-scale discriminators, and a minimal KL-divergence term to regularize the latent space.

The use of a latent space rather than raw audio waveforms significantly reduces computational requirements while preserving perceptual quality.

The causal nature of the VAE ensures that both encoding and decoding operations can be performed in a streaming fashion, making the entire system suitable for real-time applications where low latency is critical.

Specifically, the Audio VAE operates continuous speech tokens at a 25 Hz frame rate.

The VAE's architecture is similar to DAC[^Kumar2023High-Fidelity], with both its encoder and decoder implemented using stacked Causal Convolutional Networks (Causal CNNs). 
For 16 kHz single-channel audio, the encoder achieves a 640x downsampling factor through a series of strided convolutions with a stride sequence of [2, 5, 8, 8], compressing the audio into a 25 Hz latent representation. 
The decoder then reconstructs the original waveform by upsampling from this latent representation. 
The training objectives consist of an adversarial (GAN) loss, a Mel-spectrogram loss, and a KL divergence loss, with the latter's weight set to a very small value $5e-5$.
