# VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning

<details>
<summary>基本信息</summary>

- 标题: "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning."
- 作者:
  - 01 Yixuan Zhou
  - 02 Guoyang Zeng
  - 03 Xin Liu
  - 04 Xiang Li
  - 05 Renjie Yu
  - 06 Ziyang Wang
  - 07 Runchuan Ye
  - 08 Weiyue Sun
  - 09 Jiancheng Gui
  - 10 Kehan Li
  - 11 Zhiyong Wu
  - 12 Zhiyuan Liu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.24650v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2509.24650v1](PDF/2025.09.29_2509.24650v1_VoxCPM__Tokenizer-Free_TTS_for_Context-Aware_Speech_Generation_and_True-to-Life_Voice_Cloning.pdf)
  - [Publication] #TODO

</details>

## Abstract

Generative models for speech synthesis face a fundamental trade-off: discrete tokens ensure stability but sacrifice expressivity, while continuous signals retain acoustic richness but suffer from error accumulation due to task entanglement. 
This challenge has driven the field towards multi-stage pipelines that rely on pre-trained speech tokenizers, but these create a semantic-acoustic divide, limiting holistic and expressive speech generation.  
We resolve these dilemma through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel tokenizer-free TTS model--\modelname{}.
Our framework introduces a differentiable quantization bottleneck that induces natural specialization: 
a Text-Semantic Language Model (TSLM) generates semantic-prosodic plans, while a Residual Acoustic Model (RALM) recovers fine-grained acoustic details.
This hierarchical semantic-acoustic representation guides a local diffusion-based decoder to generate high-fidelity speech latents. 
Critically, the entire architecture is trained end-to-end under a simple diffusion objective, eliminating dependency on external speech tokenizers. 
Trained on a massive 1.8 million hours of bilingual corpus, our \modelname{}-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis. 
Besides, VoxCPM shows the capability to comprehend text to infer and generate appropriate prosody and style, delivering speech with context-aware expressiveness and natural flow. 
To facilitate community-driven research and development, \modelname{} is publicly accessible under Apache 2.0.

## 1·Introduction

The pursuit of modern text-to-speech (TTS) systems has evolved beyond intelligibility toward the synthesis of genuinely human-like audio, capable of conveying subtle emotions, speaker identity, and contextual nuances [^Shen2018Natural], [^Ping2017Deep], [^Ren2020FastSpeech], [^Li2019Neural]. 
This leap is critical for applications like empathetic virtual assistants and immersive digital avatars, and hinges on a core technical challenge: simultaneously capturing the fine-grained acoustic details that define vocal richness and the long-range semantic structures governing intelligibility and natural prosody.

Inspired by the success of large language models (LLMs), a dominant paradigm frames TTS as a sequence modeling task over discrete tokens from pre-trained neural audio codecs (e.g., EnCodec [^D{\'e}fossez2022High]).

Autoregressively or Non-autoregressively predicting these tokens from text or phonemes [^Borsos2023Audiolm], [^Kharitonov2023Speak,], [^Chen2025Neural], [^WangMaskGCT], [^Peng2024VoiceCraft] offers excellent scalability and in-context learning capabilities. 
However, this approach faces a fundamental "quantization ceiling", as the compression process irreversibly discards subtle acoustic details.

To mitigate this quality loss, state-of-the-art TTS systems [^Du2024Cosyvoice], [^Du2024Cosyvoice], [^Du2025Cosyvoice], [^Zhou2025IndexTTS2], [^Casanova2024Xtts] adopt multi-stage hybrid pipelines.

Here, an LLM generates discrete tokens which condition a separate diffusion-based decoder.

While improving fidelity, this solution creates a stark semantic-acoustic divide: the LLM operates in an abstract, discrete space unaware of acoustic reality, while the diffusion model performs local refinement without high-level context. 
This fragmentation prevents end-to-end optimization and limits holistic, expressive and context-aware speech synthesis.

Alternatively, other approaches directly model continuous speech representations to avoid quantization loss. 
Early systems like Tacotron 2 [^Shen2018Natural] and more recent models such as MELLE [^Meng2024Autoregressive] generate mel-spectrograms autoregressively. 
However, predicting continuous targets under standard regression losses often yields over-smoothed and low-diversity outputs. 
To address this, recent innovations have explored replacing the regression objective with a denoising process to model the distribution of the next continuous representations, spanning both non-autoregressive paradigms [^Shen2023NaturalSpeech], [^Le2023Voicebox], [^Chen2024F5-TTS] and autoregressive methods[^Li2024Autoregressive], [^Jia2025Ditar], [^Peng2025Vibevoice]. 
Among these, autoregressive approaches have often demonstrated superior performance in capturing natural prosody and expressive variation.

This innovation successfully enhances the detail and diversity of generated continuous representations. 
However, a more fundamental issue persists: in a fully continuous autoregressive model, the tasks of high-level semantic-prosodic planning and fine-grained acoustic rendering are conflated within a single learning objective. 
The model is forced to simultaneously solve two disparate tasks—requiring different inductive biases—in a continuous output space. 
This entanglement presents a significant challenge to the modeling capacity of a single LLM, as it must learn to be both a global planner and a local renderer without an inherent architectural bias to separate these functions.

We argue that this conflation is a root cause of instability. 
The model's focus is inevitably pulled towards fitting low-level acoustic textures, which compromises its ability to maintain high-level semantic coherence, leading to the well-known problem of error accumulation over long sequences [^Pasini2024Continuous].

In this work, we introduce a tokenizer-free, end-to-end framework that resolves this trade-off through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel TTS model--\modelname{}.

Our key insight is that holistic and expressive speech synthesis requires explicit architectural separation between semantic-prosodic planning and acoustic rendering, yet should remain within a cohesive, end-to-end trainable system.

The core innovation is a differentiable Finite Scalar Quantization (FSQ) [^MentzerFinite] bottleneck that induces natural specialization:
(1) a Text-Semantic Language Model (TSLM) generates semantic-prosodic plans stabilized through quantization, focusing on linguistically meaningful patterns;
and (2) a Residual Acoustic Language Model (RALM) recovers fine-grained details lost during quantization, specializing in acoustic refinement.

This hierarchical design enables each component to excel at its respective role while maintaining differentiability, and both of them will be used to guide a local diffusion decoder to generate high-fidelity speech latents.

Critically, the entire hierarchical model is trained end-to-end under a simple diffusion objective, seamlessly integrating planning and rendering without pre-trained tokenizers.

Trained on a massive 1.8 million hours of bilingual corpus, our \modelname{}-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis. 
Our main contributions are as follows:
\vspace{-2mm}

[itemsep=0.2em, leftmargin=2em]

-  We propose an end-to-end hierarchical architecture that introduces an internal semi-discrete bottleneck to resolve the expressivity-stability trade-off.

This mechanism implicitly addresses task entanglement in continuous models by inducing a beneficial separation between semantic-prosodic planning and fine-grained acoustic modeling within a single, unified framework.

-  We introduce a residual learning strategy that, in conjunction with the bottleneck, enables a holistic yet specialized modeling process.

Unlike fragmented multi-stage pipelines, our approach achieves functional separation without architectural fragmentation, simplifying the training pipeline and eliminating dependency on external speech tokenizers.

-  We demonstrate the efficacy of our approach through large-scale training on a massive 1.8 million hours of bilingual speech.

The resulting model, VoxCPM-0.5B, achieves state-of-the-art zero-shot TTS performance among open-source systems with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, validating its practical strength.

-  We provide extensive ablation studies that conclusively validate the semi-discrete residual representations as the crucial component for robust, expressive, and l context-aware synthesis.

Besides, we release the codes and models publicly to support community development and future research.

## 2·Related Work

### Discrete Token-Based TTS

The discrete token paradigm has emerged as a dominant approach in modern TTS, leveraging the success of large language models.

This method converts speech into discrete representations using neural audio codecs such as EnCodec [^D{\'e}fossez2022High] and DAC [^Kumar2023High-Fidelity] through residual vector quantization (RVQ).

AudioLM [^Borsos2023Audiolm] and VALL-E [^Chen2025Neural] pioneered this direction by framing audio generation and TTS as an autoregressive sequence prediction task over discrete acoustic tokens.

Subsequent developments include SoundStorm [^Borsos2023Soundstorm], which introduced non-autoregressive generation for improved efficiency, and Spear-TTS [^Kharitonov2023Speak,], which focused on multilingual capabilities with minimum supervision. 
Besides, VoiceCraft [^Peng2024VoiceCraft] and XTTS [^Casanova2024Xtts] further advanced zero-shot TTS with in-context learning.

Recent advancements have focused on enhancing the scalability, controllability and zero-shot adaptation. 
CosyVoice [^Du2024Cosyvoice] proposed supervised semantic tokens for improved zero-shot performance, while its successors, 

CosyVoice 2 and 3 [^Du2024Cosyvoice], [^Du2025Cosyvoice]
incorporated text-based LLM initialization, streaming synthesis, and large-scale training data for human-parity quality, low latency and in-the-wild scenarios.

IndexTTS [^Deng2025Indextts] and IndexTTS2 [^Zhou2025IndexTTS2] introduced precise duration and emotion control in autoregressive token generation, enabling applications with strict timing and expressivity requirements.

SparkTTS [^Wang2025Spark-TTS] utilized single-stream decoupled speech tokens for modeling efficiency, and FireRedTTS [^Guo2024Fireredtts] along with its update FireRedTTS-2 [^Xie2025FireRedTTS-2] established frameworks for industry-level generative speech, including long-form multi-speaker dialogue.   
Openaudio-s1 [^OpenAudio2024OpenAudio] used dual AR architecture and online Reinforcement Learning from Human Feedback (RLHF) to improve expressiveness and instruction-following capabilities.

Higgs Audio v2 [^BosonAI2025Higgs] proposed a unified audio tokenizer captures
both semantic and acoustic features, and  pretrained on over 10 million hours of audio data, providing a powerful foundation model. 
Despite these progresses, discrete approaches suffer from inherent quantization artifacts, limiting acoustic fidelity and prompting hybrid solutions.

\vspace{-0.2cm}

### Continuous Representation TTS

To circumvent quantization losses in discrete models, continuous representation approaches directly model speech features such as mel-spectrograms or audio latents.

Early systems like Tacotron 2 [^Shen2018Natural] established the encoder-decoder framework for text-to-mel mapping, while FastSpeech [^Ren2020FastSpeech] introduced explicit duration modeling for alignment stability.

Inspired from VALL-E,  MELLE [^Meng2024Autoregressive] autoregressively generated continuous mel-spectrogram frames directly from text condition, and incorporated variational inference to facilitate sampling mechanisms.

Recent developments have integrated diffusion processes to enhance detail and diversity.

Non-autoregressive models like NaturalSpeech 2 [^Shen2023NaturalSpeech] and VoiceBox [^Le2023Voicebox] apply diffusion directly on continuous representations.

F5-TTS [^Chen2024F5-TTS] advanced flow-matching for efficient synthesis. 
Autoregressive paradigms, often superior in prosody and variation, additionally possess the capability for streaming synthesis.

Innovations like ARDiT [^Li2024Autoregressive] use an autogressive diffusion transformer for TTS, unifying semantic coherence and acoustic naturalness via parameter sharing. 
DiTAR [^Jia2025Ditar] extended this with a patch-based design: a causal LM for inter-patch stability and a bidirectional local diffusion transformer for intra-patch refinement. 
VibeVoice [^Peng2025Vibevoice] employed next-token diffusion for long-form multi-speaker synthesis. 
Besides, recent models such as CLEAR [^Wu2025Clear] and FELLE [^Wang2025Felle] focus on latent autoregressive modeling with token-wise coarse-to-fine hierarchies, while MELA-TTS [^An2025Mela-TTS] and KALL-E [^Zhu2024Autoregressive] combine joint transformer-diffusion with next-distribution prediction for improved efficiency and quality.

Despite these advances, continuous models often entangle high-level semantic planning with low-level acoustic rendering, leading to instability in long sequences without explicit separation.

\vspace{-0.2cm}

### Hierarchical and Residual Modeling in TTS

Hierarchical and residual approaches decompose TTS into layered tasks to balance stability and expressivity.

HierSpeech++ [^Lee2025Hierspeech++] employed variational inference for semantic-acoustic mapping. 
HALL-E [^NishimuraHall-E] uses hierarchical neural codecs with LLMs for minute-long synthesis.

MARS6 [^Baas2025Mars6] builds robust encoder-decoder transformers with hierarchical tokens.

DiffStyleTTS [^Liu2024DiffStyleTTS] applies diffusion for hierarchical prosody modeling.

HAM-TTS [^Wang2024Ham-TTS] introduces hierarchical acoustic modeling with data augmentation for zero-shot TTS. 
QTTS [^Han2025Quantize] features hierarchical parallel architectures for residually quantized codes. 
In song generation, LeVo [^Lei2025LeVo] likewise introduced a hierarchical framework using two decoder-only transformers for layered modeling of mixed and separated part in a song, achieving enhanced generation quality.

These methods address flaws in prior paradigms: implicit designs lack regulated bottlenecks, tokenizer-dependent models suffer discrete losses, and fragmented stages hinder end-to-end optimization.

However, few fully integrate explicit residual designs with semi-discrete bottlenecks in a unified framework, as proposed in our work, to achieve implicit disentanglement without external dependencies.

## 3·Methodology

![](images/draft_1.png)

<a id="fig:architecture">Overall architecture of VoxCPM.

The model hierarchically generates speech by first processing audio latents through a LocEnc, then producing a semi-discrete speech skeleton with the TSLM and FSQ, refining acoustic details with the RALM, and finally generating high-fidelity latent output with the LocDiT.</a>

### Core Design Motivation

\label{sec:motivation}

Generative speech synthesis faces a fundamental tension between expressivity and stability.

Discrete tokenization methods (e.g., speech tokenizers with language models) ensure stable autoregressive generation but irreversibly discard fine-grained acoustic details through quantization. 
Continuous approaches preserve full fidelity but suffer from error accumulation in long sequences due to information entanglement, often leading to catastrophic failure in intelligibility.

Critically, we identify a key limitation in existing discrete tokenization approaches: methods that directly use FSQ or VQ to obtain discrete codebooks for language modeling face an inherent scalability challenge.

As the dimensionality increases to capture richer acoustic information, the codebook size grows exponentially, creating an unmanageably large and sparse vocabulary that language models struggle to predict accurately.

We hypothesize that an effective solution should **structurally separate** the modeling of stable semantic-prosodic content from fine-grained acoustic details while maintaining differentiability for end-to-end training. 
Our key insight is to introduce a **differentiable quantization bottleneck** that naturally induces this separation through scalar quantization, splitting information into a discrete-like skeleton for content stability and continuous residual components for detail expressivity.

Unlike multi-stage TTS systems composed of seperate LM and diffusion that treat quantization as a means to obtain discrete prediction targets, our approach uses quantization solely as a regularization mechanism to constrain the hidden state space. 
This distinction allows us to avoid the vocabulary explosion problem while still benefiting from the stabilizing effects of discrete representations.

### Model Overview

\label{sec:overview}

VoxCPM employs a hierarchical autoregressive architecture that generates sequences of continuous speech latents $\mathbf{Z} = \{\mathbf{z}_1, ..., \mathbf{z}_M\}$ conditioned on input text tokens $\mathbf{T} = \{t_1, ..., t_N\}$,
where each $\mathbf{z}_i \in \mathbb{R}^{P \times D}$ represents a patch of $P$ frames with $D$-dimensional VAE latent vectors. 
The generation process follows:

$$
p(\mathbf{Z}|\mathbf{T}) = \prod_{i=1}^M p(\mathbf{z}_i | \mathbf{T}, \mathbf{Z}_{<i})
$$

The core innovation lies in our hierarchical conditioning mechanism with residual representation learning.

It is made up of a local audio encoder (LocEnc), a text-semantic language model (TSLM), a residual acoustic language model (RALM) and a local diffusion transformer decoder (LocDiT).

A stop predictor is attached to the output of the TSLM to determine the endpoint of generation. 
As shown in Figure~[fig:architecture](#fig:architecture), each patch generation involves:

$$
\mathbf{z}_i \sim \text{LocDiT}(\mathbf{h}_i^{\text{final}}), \quad 
\mathbf{h}_i^{\text{final}} = \underbrace{\text{FSQ}(\text{TSLM}(\mathbf{T}, \mathbf{E}_{<i}))}_{\text{stable skeleton}} + \underbrace{\text{RALM}(\cdot)}_{\text{residual details}}
$$

where $\mathbf{E}_{<i} = \text{LocEnc}(\mathbf{Z}_{<i})$ represents historical audio context aggregated by a lightweight LocEnc that compresses VAE latent patches into compact acoustic embeddings.

The hierarchical backbone produces a conditioning signal $\mathbf{h}_i^{\text{final}}$ that encapsulates both semantic content from TSLM (with FSQ) and acoustic details from RALM. 
This signal guides the LocDiT to generate the current latent patch $z_i$ through a denoising diffusion process.

The entire model is trained end-to-end with gradients flowing through all components, including the FSQ bottleneck via straight-through estimation, ensuring coordinated optimization toward holistic speech synthesis.

### Hierarchical Semantic-Acoustic Modeling

\label{sec:hierarchical}

Our hierarchical modeling approach is designed to implicitly separate semantic-prosodic planning from fine-grained acoustic synthesis, addressing the fundamental stability-expressivity trade-off through structured representation learning.

#### Text-Semantic Language Model (TSLM)

\label{sec:tslm}

The Text-Semantic Language Model forms the main part of our hierarchical architecture, responsible for capturing high-level linguistic structure and generating contextually appropriate speech patterns. 
Unlike conventional TTS systems that typically operate on phoneme sequences, our approach leverages a pre-trained text language model (MiniCPM-4[^Team2025Minicpm4]) as its initial backbone, enabling richer contextual understanding and more natural prosody prediction directly from raw text.

Specifically, we employ character-level segmentation for Chinese BPE Tokenizer to mitigate the vocabulary sparsity issue in TTS tasks.

By processing both text tokens and historical audio context, the TSLM learns to generate semantic content and prosodic structure that evolve naturally throughout an utterance, reflecting the underlying linguistic meaning rather than simply mapping phonemes to acoustic features.

The TSLM produces continuous semantic-prosodic representations that encode both the content to be spoken and how it should be prosodically realized, serving as input to the subsequent quantization stage.

#### Semi-Discrete Representation Learning via FSQ

\label{sec:fsq}

At the core of our approach lies the Finite Scalar Quantization (FSQ) layer, which projects the continuous hidden states from the TSLM onto a structured lattice to create a semi-discrete representation. 
The FSQ operation transforms each dimension of the continuous vector through a deterministic scalar quantization:

$$
\mathbf{h}_{i,j}^{\text{FSQ}} = \Delta \cdot \text{clip}\left( \text{round}\left( \frac{\mathbf{h}_{i,j}^{\text{TSLM}}}{\Delta} \right), -L, L \right)
$$

where $\Delta$ is the quantization step size, $L$ is the clipping range, and $\text{round}$ maps values to discrete levels. 
This transformation creates a structured discrete representation while maintaining differentiability through the straight-through estimator during backward passes.

The FSQ layer acts as a bottleneck, analogous to the first layer of Residual Vector Quantization (RVQ), which captures a coarse semantic-prosodic skeleton (e.g., content, intonation patterns). 
We term this representation "semi-discrete" as it employs a significantly larger dimensionality than standard FSQ to ensure sufficient informational capacity.

Unlike RVQ, where the first layer is a prediction target and subsequent layers model finer details, our FSQ bottleneck serves as an intermediate, differentiable inductive bias  within the continuous data flow. 
It encourages the model to prioritize modeling stable, high-level components (the semantic-prosodic skeleton) by providing a clear learning signal for what information should be preserved through the bottleneck. 
This structured approach mitigates error accumulation by reducing the modeling burden on the TSLM, allowing it to focus on the major components of the speech.

#### Residual Acoustic Modeling

\label{sec:ralm}

To recover the fine-grained acoustic information attenuated by quantization, we introduce the Residual Acoustic Language Model (RALM). 
This module specializes in reconstructing those subtle vocal characteristics that conventional discrete methods sacrifice for stability. 
It processes the quantization residuals along with contextual information to recover speaker identity, spectral fine structure, and micro-prosodic variations:

$$
\mathbf{h}_i^{\text{residual}} = \text{RALM}( \mathbf{H_{\text{text}}^{\text{TSLM}}}, \mathbf{H}_{<i}^{\text{FSQ}} \oplus \mathbf{E}_{<i})
$$

Here, the RALM conditions its predictions on both the TSLM hidden states of the text part $\mathbf{H_{\text{text}}^{\text{TSLM}}}$, the semi-discrete representation of speech part $\mathbf{H}_{<i}^{\text{FSQ}}$, and the historical acoustic embeddings $\mathbf{E}_{<i}$.

This residual learning approach  creates a natural division of labor: the TSLM+FSQ pathway focuses on content stability and prosodic coherence, while the RALM pathway specializes in acoustic expressivity and speaker characteristics. 

The final combined representation $\mathbf{h}_i^{\text{final}} = \mathbf{h}_i^{\text{FSQ}} + \mathbf{h}_i^{\text{residual}}$ thus encapsulates both semantic stability and acoustic expressivity, creating a comprehensive signal that guides the subsequent local diffusion process.

#### Local Diffusion Transformer  Decoder

\label{sec:locdit}

The Local Diffusion Transformer (LocDiT) serves as our high-fidelity synthesis module, generating continuous latent patches conditioned on the hierarchical representation $\mathbf{h}_i^{\text{final}}$ produced by the preceding modules.

Following DiTAR [^Jia2025Ditar], we employ a bidirectional Transformer architecture that enables full receptive field modeling within each patch. 
To enhance generation consistency, we incorporate the previous patch $\mathbf{z}_{i-1}$ as additional conditioning context, which has been empirically validated to significantly improve output quality by framing the task as outpainting rather than independent patch generation.

Besides, we mask the LM guidance in LocDiT condition with a specific probability ratio, for enabling classifier-free guidance (CFG) during inference.

### Training Objective

The entire model is trained end-to-end using a flow-matching objective that directly optimizes the quality of the generated speech latents.

We adopt the conditional flow-matching formulation for its training stability and sampling efficiency:

$$
\mathcal{L}_{\text{FM}} = \mathbb{E}{t, \mathbf{z}_i^0, \boldsymbol{\epsilon}} \left[ | \mathbf{v}_{\theta}(\mathbf{z}_i^t, t, \mathbf{h}_i^{\text{final}}, \mathbf{z}_{i-1}) - \frac{d}{dt}(\alpha_t \mathbf{z}_i^0 + \sigma_t \boldsymbol{\epsilon}) |^2 \right]
$$

where $\mathbf{z}_i^t = \alpha_t \mathbf{z}_i^0 + \sigma_t \boldsymbol{\epsilon}$ is the noisy latent at time $t$, with $\boldsymbol{\epsilon} \sim \mathcal{N}(0, I)$, and $\mathbf{v}_{\theta}$ is the velocity field predicted by the LocDiT.

Simultaneously, a binary classification loss is applied to train the model to predict the end of a speech sequence:

$$
\mathcal{L}_{\text{Stop}} = \mathbb{E}_{i \sim \text{sequence}} \left[ \text{BCE}\left(s_{\theta}(\mathbf{h}_i^{\text{FSQ}}), \mathbb{1}[\text{token } i \text{ is the last}]\right) \right]
$$

where $s_{\theta}$ is a stop-logit projection layer, and BCE denotes the binary cross-entropy loss.

The gradients from this loss are backpropagated through the entire autoregressive hierarchy, including the FSQ layer (via straight-through estimation), the TSLM and the LocEnc.

This end-to-end optimization 
under the combined objective $\mathcal{L} = \mathcal{L}_{\text{FM}} + \lambda \mathcal{L}_{\text{Stop}}$ 
allows each component to learn its specialized role—semantic planning, stabilization, and acoustic refinement—in a coordinated manner, guided by the unified objective of accurately modeling the continuous speech latents.

### Causal Audio VAE

\label{sec:vae}

To enable efficient streaming synthesis, we employ a causal Variational Autoencoder that operates in a computationally efficient latent space. 
VAE is trained separately using a composite objective that combines reconstruction loss in the Mel-spectrogram domain, adversarial training with multi-period and multi-scale discriminators, and a minimal KL-divergence term to regularize the latent space.

The use of a latent space rather than raw audio waveforms significantly reduces computational requirements while preserving perceptual quality.

The causal nature of the VAE ensures that both encoding and decoding operations can be performed in a streaming fashion, making the entire system suitable for real-time applications where low latency is critical.

Specifically, the Audio VAE operates continuous speech tokens at a 25 Hz frame rate.

The VAE's architecture is similar to DAC[^Kumar2023High-Fidelity], with both its encoder and decoder implemented using stacked Causal Convolutional Networks (Causal CNNs). 
For 16 kHz single-channel audio, the encoder achieves a 640x downsampling factor through a series of strided convolutions with a stride sequence of [2, 5, 8, 8], compressing the audio into a 25 Hz latent representation. 
The decoder then reconstructs the original waveform by upsampling from this latent representation. 
The training objectives consist of an adversarial (GAN) loss, a Mel-spectrogram loss, and a KL divergence loss, with the latter's weight set to a very small value $5e-5$.

## 4·Experiments and Results

### Experimental Setup

**Datasets**

We conducted experiments on two primary datasets:
(1) **Large-scale Bilingual Corpus**: To explore the best performance, we collected an internal large-scale, bilingual dataset totaling on a massive 1.8 million hours, mainly comprising of Chinese and English speech. 
The raw audio was sourced from a diverse set of domains, including audiobooks, podcasts, interviews, and broadcast dramas. 
To enhance model robustness and enable advanced functionalities such as pronunciation correction, we further constructed some specialized training samples by applying data augmentation techniques, including random phoneme replacement on the transcriptions.

All audio was resampled to 16kHz mono, processed with source separation, voice activity detection (VAD), and automatic speech recognition (ASR) system to obtain text-audio alignment.
(2) **Emilia Dataset**: For comparisons and ablation studies, we used the publicly available Emilia dataset [^He2024Emilia] (95K hours) including Chinese and English utterances.

**Architecture Configurations**

We implemented VoxCPM using the Megatron framework, with a 0.5B-parameter configuration, comprising a 24-layer Text-Semantic Language Model (TSLM), initialized from the pre-trained MiniCPM-4-0.5B [^Team2025Minicpm4]\footnote{\url{https://huggingface.co/openbmb/MiniCPM4-0.5B}}, and a randomly initialized 6-layer Residual Acoustic Language Model (RALM). 
The FSQ layer uses 256 dimensions with 9 scalar levels. 
The LocEnc and the LocDiT has 4 Transformers layers, designed for high-efficacy latent extraction and generation.

Detail Configrations are shown in Table [tab: model_config](#tab: model_config).

<a id="tab:voxcpm_arch">The model architecture of VoxCPM-0.5B.</a>

**Training Details**

We trained two models for comparisons: 
1) **VoxCPM** was trained with internal large-scale bilingual corpus for 500K iterations using 40 NVIDIA H100 GPUs;
2) **VoxCPM-Emilia** was trained on the Emilia dataset for 200K iterations using 24 H100 GPUs.

Both VoxCPM and VoxCPM-Emilia used the AdamW optimizer with a peak learning rate of $1\times10^{-4}$ and a Warmup-Stable-Decay (WSD) schedule [^Hu2024Minicpm] which we found essential for optimal convergence.

Specifically, the decay phase with annealing to a very low learning rate (combined with batch size doubling) significantly enhances model performance, particularly for zero-shot speaker similarity, as demonstrated in Table [tab:phase_performance](#tab:phase_performance).

All ablation studies followed the same 200K-iteration training protocol on 8 H100 GPUs using the Emilia dataset, employing a fixed learning rate (i.e., without the WSD schedule) of $1\times10^{-4}$
to ensure a consistent comparison.

For LocDiT, we mask the LM condition guidance with a probability ratio of 0.1 for enabling CFG during inference.

<a id="tab:training_schedule">Training configurations for VoxCPM variants.</a>

**Evaluation Metrics and Benchmarks**

We employed comprehensive subjective and objective evaluations.

Objective metrics included Word / Character Error Rate (WER / CER) for intelligibility, speaker embedding cosine similarity (SIM) for voice cloning, and DNSMOS for overall quality. 
Subjective evaluation involved Mean Opinion Score (MOS) tests rated by 20 native speakers on naturalness (N-MOS) and speaker similarity (S-MOS) using 5-point scales.

Models were %rigorously 
assessed on two challenging benchmarks: 1) **SEED-TTS-EVAL**\footnote{\url{https://github.com/BytedanceSpeech/seed-tts-eval}}, focusing on general TTS intelligibility and similarity in English and Chinese, including a "Hard" set with complex sentences; 2) **CV3-EVAL**\footnote{\url{https://github.com/FunAudioLLM/CV3-Eval}}, derived from CosyVoice 3 competition, emphasizing  expressive and in-the-wild voice cloning.

**Baselines**

We compared VoxCPM against a wide range of state-of-the-art open-source TTS systems, including CosyVoice series [^Du2024Cosyvoice], [^Du2024Cosyvoice], MaskGCT [^WangMaskGCT], F5-TTS [^Chen2024F5-TTS], SparkTTS [^Wang2025Spark-TTS], FireRedTTS series [^Guo2024Fireredtts], [^Xie2025FireRedTTS-2], IndexTTS 2 [^Zhou2025IndexTTS2], HiggsAudio v2 [^BosonAI2025Higgs] and so on.

All baseline results were obtained using official implementations with default settings, or as reported in their original papers.

### Main Results: Comparison with State-of-the-Art TTS

As shown in Table [tab:tts_seed_benchmark](#tab:tts_seed_benchmark), VoxCPM achieves state-of-the-art performance among open-source models on the SEED-TTS-EVAL benchmark. 
It attains an English WER of 1.85\% and a Chinese CER of 0.93\%, surpassing strong competitors like IndexTTS2 and CosyVoice2. 
Concurrently, VoxCPM maintains high speaker similarity, with SIM scores of 72.9\% (EN) and 77.2\% (ZH). 
This demonstrates that the proposed semi-discrete bottleneck effectively balances intelligibility and expressivity by hierarchical semantic-acoustic modeling, mitigating the instability common in continuous models while preserving details often lost in discrete models.

The VoxCPM-Emilia variant, trained on a smaller public dataset, delivers competitive results (EN-WER: 2.34\%, ZH-CER: 1.11\%). 
This highlights the data efficiency and architectural robustness of our approach, as the FSQ bottleneck stabilizes the learning of semantic-acoustic representations even with less training data.

Notably, while DiTAR's phoneme-based approach shows slightly better stability, 
VoxCPM's use of BPE tokens with pre-trained LLM initialization provides superior text understanding capabilities and eliminates dependency on external phonemizers.

Besides, our hierarchical design with residual acoustic modeling reduces the fundamental limitation of direct continuous token modeling, as evidenced in ablation studies.

On the CV3-EVAL benchmark (Table [tab:tts_cv3_combined](#tab:tts_cv3_combined)), designed to evaluate expressive and in-the-wild performance, 
VoxCPM excels with a ZH-CER of 3.40\% and an EN-WER of 4.04\%. 
Its robustness is further confirmed on the challenging CV3 Hard-Test set, where it achieves an EN-WER of 7.89\%, outperforming even close-sourced CosyVoice 3. 
These results underscore the model's capability to handle complex, realistic inputs, a strength attributed to the RALM's role in recovering fine-grained acoustic details subsequent to the TSLM-FSQ-based semantic-prosodic modeling.

<a id="tab:tts_seed_benchmark">Performance on Seed-TTS-eval Benchmark</a>

<a id="tab:tts_cv3_combined">Performance on CV3-eval Benchmark. *denotes close-sourced systems.</a>

Subjective evaluations (Table [tab:tts_mos](#tab:tts_mos)) further validate the objective findings, with VoxCPM achieving competitive performance across both languages.

On English tests, VoxCPM obtains the highest scores in speaker similarity and good results in naturalness.

For Chinese, while VoxCPM trails IndexTTS 2 in naturalness, it achieves slightly superior speaker similarity.

This pattern suggests that VoxCPM excels at voice cloning consistency, while IndexTTS 2 may have advantages in prosodic naturalness for Chinese.

VoxCPM-Emilia shows competitive speaker similarity but relatively lower naturalness, highlighting the impact of training data scale. 

<a id="tab:tts_mos">Subjective Evaluations in terms of Naturalness and Speaker Similarity.</a>

### Ablation Study: Effect of the Semi-discrete Bottleneck

As shown in Table [tab:fsq_dimension](#tab:fsq_dimension), the ablation studies on the FSQ bottleneck dimensionality provide critical insights. 
The catastrophic performance degradation of the purely continuous model (w/o FSQ), especially on hard cases (ZH-CER: 24.92\%), validates our core hypothesis: entangling semantic planning and acoustic rendering in a continuous space leads to instability.

Without the inductive bias imposed by FSQ, the model struggles to separate these tasks even with a hierarchical design, resulting in error accumulation on complex utterances.

<a id="tab:fsq_dimension">FSQ dimension selection study on the Emilia dataset. *Note:*

The 256-dim was selected for the final VoxCPM configuration, with the understanding that larger training datasets needs more powerful modeling capabilities.</a>

The optimal performance observed at %intermediate 
FSQ levels (FSQ-d128/d256) reveals a key trade-off.

Lower dimensions (e.g., FSQ-d4) over-constrain the representation, limiting prosodic capacity.

Higher dimensions (e.g., FSQ-d1024) provide insufficient discretization strength, allowing task entanglement to persist.

The peak at FSQ-d256 indicates the bottleneck creates an effective "summary space": discrete enough to stabilize long-range semantic planning yet continuous enough to retain crucial prosodic and speaker information, thereby enforcing a beneficial division of labor within the model.

### Ablation Study: Effect of Residual Acoustic Modeling

As shown in Table [tab:ralm_ablation](#tab:ralm_ablation), the ablation studies about the residual language modeling validate our core architectural innovations.

Notably, the purely continuous variant (w/o RALM: TSLM $\rightarrow$ LocDiT) —analogous to DiTAR's approach—shows significantly degraded performance, particularly on challenging cases.

The performance gap persists across different TSLM configurations, confirming that the challenge is fundamental to the learning objective rather than parameter allocation.

This conclusively demonstrates the advantage of our explicit separation between semantic and acoustic modeling.

Secondly, the critical role of residual acoustic input is further evidenced by the substantial degradation when ablating original acoustic embeddings (w/o $E_{<i}$ in RALM), highlighting that the RALM requires fine-grained acoustic information to accurately recover acoustic details.

Finally, the best performance of the default setting demonstrates the effectiveness of the residual connection. 
By summing the TSLM and RALM hidden states, the model explicitly delegates semantic-prosodic planning to the TSLM and acoustic refinement to the RALM, achieving optimal integration.

<a id="tab:ralm_ablation">Ablation Studies about core architecture designs.</a>

### Effect of Training Phase on Performance

As mentioned in Table [tab:train_config](#tab:train_config), the two-phase Warmup-Stable-Decay (WSD) learning rate schedule is critical for achieving optimal model performance.

The initial Stable phase allows the model to converge reliably to a strong baseline.

The subsequent Decay phase is then essential for refining the model, particularly for improving its zero-shot voice similarity capabilities.

The performance gains from this two-phase strategy are substantiated in Table [tab:phase_performance](#tab:phase_performance). 
Compared to the Stable phase, the Decay phase achieves consistent improvements across all metrics: reducing word error rates, while simultaneously enhancing speaker similarity.

Most notably, the model demonstrates a remarkable leap in robustness on challenging cases, with the CER on ZH-Hard dropping from 13.22\% to 8.87\%, alongside a 4.4-point SIM improvement.

<a id="tab:phase_performance">Performance across training phases.</a>

### Effect of LM Guidance on LocDiT

To investigate the influence of Classifier-Free Guidance (CFG) and identify the optimal inference setting, we tested different CFG value, that is, the LM (the sum of TSLM-FSQ hidden and RALM hidden) guidance on LocDiT.

As detailed in Table [tab:cfg](#tab:cfg), the CFG scale exerts a profound and non-monotonic influence on the trade-off between speech intelligibility and speaker similarity. 
The absence of CFG (a scale of 1.0) results in poor performance, characterized by high error rates and low similarity scores, as the model lacks sufficient incentive to strongly condition on the linguistic input. 
Employing a moderate CFG value of 2.0 yields the optimal balance, effectively enhancing voice similarity without compromising intelligibility, while higher values ($\geq$3.0) degraded intelligibility significantly.

<a id="tab:cfg">Effect of LM guidance on LocDiT, tested with **VoxCPM**.</a>

### Analysis and Discussion

**Visual Analysis of Hierarchical Representations** 

![](images/tsne1.png)

<a id="fig:tsne1">The T-SNE visualization of latent space distributions in zero-shot voice cloning task.</a>

![](images/tsne2.png)

<a id="fig:tsne2">The T-SNE visualization of latent space distributions in text-to-speech task, without prompt speech.</a>

To validate our core hypothesis of learned implicit semantic-acoustic disentanglement, we conducted a t-SNE visualization of the internal representations in our hierarchical model.

The resulting distributions, shown in Figures [fig:tsne1](#fig:tsne1) and [fig:tsne2](#fig:tsne2), empirically confirm the specialized roles of the TSLM and the RALM.

Figure [fig:tsne1](#fig:tsne1) illustrates the model's behavior in a zero-shot voice cloning task, where each color corresponds to a distinct utterance from an unseen speaker.

The TSLM-FSQ outputs form semantic-prosodic structure closely tied to text content, while the RALM residuals  exhibit strong speaker-related variations for acoustic rendering, confirming their specialized roles in content planning and acoustic refinement.

Figure [fig:tsne2](#fig:tsne2) further demonstrates the VoxCPM's capability to infer appropriate prosody and style directly from text, when not using any speech prompt. 
When processing different text genres (news, poetry, conversation), TSLM-FSQ representations cluster by semantic category, showing that the pre-trained language model backbone effectively infers appropriate prosodic patterns directly from text content. 
For example, embeddings for "news" group together, separate from "story-telling" or "rap-lyrics."
The RALM outputs display greater within-category variation, indicating its role in adding fine-grained acoustic nuances to the semantic-prosodic plan.

**Expressive and Context-Aware Synthesis Capabilities**

Beyond quantitative metrics, VoxCPM shows
good expressive and context-aware synthesis capabilities directly from text benfiting from the architecture design and training data. 
The powerful pre-trained LM backbone provides inherent text understanding, enabling appropriate prosodic variations across different content types, as mentioned above. 
When not using prompt speech, the model tends to express suitable style from contextual cues, also shown in Figure [fig:tsne2](#fig:tsne2). 
We strongly recommend readers to listen our demo samples\footnote{\url{https://openbmb.github.io/VoxCPM-demopage/}}.

**Scalability and Efficiency** 
The performance improvement from VoxCPM-Emilia to VoxCPM highlights the architecture's scalability with increased data. 
The hierarchical design allows larger models to effectively utilize increased capacity for learning complex patterns. 
In terms of inference efficiency, VoxCPM-0.5B achieves a real-time factor (RTF) of 0.17 on a single NVIDIA RTX 4090, confirming practical deployment feasibility.

## 5·Conclusion

In this work, we present a novel tokenizer free TTS model VoxCPM to achieve context-aware speech generation and true-to-life voice cloning.

It resolves the fundamental trade-off between expressivity and stability in text-to-speech synthesis by introducing a unified, end-to-end framework based on hierarchical semantic-acoustic modeling with semi-discrete residual representations. 
Our approach leverages a differentiable quantization bottleneck to induce a natural separation of concerns: a text-semantic language model captures high-level semantic-prosodic structure, while a residual acoustic model recovers fine-grained details. 
This eliminates the dependency on external speech tokenizers and mitigates the error accumulation that plagues purely continuous autoregressive models. 
Extensive experiments demonstrate that our model achieves state-of-the-art zero-shot TTS performance among open-source systems, excelling in both intelligibility and speaker similarity.

The success of VoxCPM validates that learning structured, regularized latent spaces provides a principled foundation for expressive generative audio modeling.

**Limitations**

Despite these advancements, our work still has several limitations. 
First, the model's multilingual capability remains limited, as it is primarily optimized for Chinese and English, with uncertain generalization to other languages. 
Second, the controllability of speech attributes—such as fine-grained prosody and emotional expression—is still constrained, lacking both intuitive user guidance and precise adjustment mechanisms. 
Finally, the current AudioVAE only supports 16kHz audio generation, which restricts perceptual quality and falls short of high-fidelity application requirements that typically demand 24kHz or 44.1kHz sampling rates. 
These limitations point to meaningful directions for future research.

**Ethics statement**

Since our zero-shot TTS model achieves high-quality speech synthesis with the ability to closely mimic speaker characteristics, it carries potential risks of misuse.

These risks include, but are not limited to, spoofing voice authentication systems or impersonating a specific speaker without their consent.

Our experiments were conducted under the assumption that the use of any reference speaker's voice is authorized and intended for legitimate synthesis purposes.

To mitigate these risks, we strongly advocate for the development of robust synthesized speech detection algorithms.

Furthermore, we believe it is crucial to establish clear ethical guidelines and reporting mechanisms for the responsible deployment of such technology.

## 6·Contributors

\modelname{} is a collaborative release by the Tsinghua Shenzhen International Graduate School (SIGS) Human-Computer Speech Interaction Lab (THUHCSI), Natural Language Processing Lab at Tsinghua University (THUNLP) and ModelBest.

We would also like to thank the OpenBMB community for all their support.

**Core Contributors** \quad
Yixuan Zhou, Guoyang Zeng, Xin Liu, Xiang Li, Renjie Yu, Ziyang Wang, Runchuan Ye, Weiyue Sun, Jiancheng Gui, Kehan Li, Zhiyong Wu, Zhiyuan Liu

**Other Contributors (Alphabetical order)** \quad
Biyuan Lin, Chao Jia, Chenzhe Jing, Hongyu Liu, Jie Cai, Jie Zhou, Junshao Guo, Lei Chen, Rongting Tang, Rui Li, Ruiqi Shao, Qundong Shi, Shuo Wang, Siyuan Huang, Shun Lei, Wenxi Yang, Xiaoshuang Wang, Yihang He, Zichao Nie

\newpage

\bibliographystyle{citation}

\bibliography{citation}

\end{document}
