# VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning

<details>
<summary>基本信息</summary>

- 标题: "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning."
- 作者:
  - 01 Yixuan Zhou
  - 02 Guoyang Zeng
  - 03 Xin Liu
  - 04 Xiang Li
  - 05 Renjie Yu
  - 06 Ziyang Wang
  - 07 Runchuan Ye
  - 08 Weiyue Sun
  - 09 Jiancheng Gui
  - 10 Kehan Li
  - 11 Zhiyong Wu
  - 12 Zhiyuan Liu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.24650v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2509.24650v1](PDF/2025.09.29_2509.24650v1_VoxCPM__Tokenizer-Free_TTS_for_Context-Aware_Speech_Generation_and_True-to-Life_Voice_Cloning.pdf)
  - [Publication] #TODO

</details>

## Abstract

Generative models for speech synthesis face a fundamental trade-off: discrete tokens ensure stability but sacrifice expressivity, while continuous signals retain acoustic richness but suffer from error accumulation due to task entanglement. 
This challenge has driven the field towards multi-stage pipelines that rely on pre-trained speech tokenizers, but these create a semantic-acoustic divide, limiting holistic and expressive speech generation.  
We resolve these dilemma through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel tokenizer-free TTS model--\modelname{}.
Our framework introduces a differentiable quantization bottleneck that induces natural specialization: 
a Text-Semantic Language Model (TSLM) generates semantic-prosodic plans, while a Residual Acoustic Model (RALM) recovers fine-grained acoustic details.
This hierarchical semantic-acoustic representation guides a local diffusion-based decoder to generate high-fidelity speech latents. 
Critically, the entire architecture is trained end-to-end under a simple diffusion objective, eliminating dependency on external speech tokenizers. 
Trained on a massive 1.8 million hours of bilingual corpus, our \modelname{}-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis. 
Besides, VoxCPM shows the capability to comprehend text to infer and generate appropriate prosody and style, delivering speech with context-aware expressiveness and natural flow. 
To facilitate community-driven research and development, \modelname{} is publicly accessible under Apache 2.0.

## 1·Introduction

The pursuit of modern text-to-speech (TTS) systems has evolved beyond intelligibility toward the synthesis of genuinely human-like audio, capable of conveying subtle emotions, speaker identity, and contextual nuances [^Shen2018Natural], [^Ping2017Deep], [^Ren2020FastSpeech], [^Li2019Neural]. 
This leap is critical for applications like empathetic virtual assistants and immersive digital avatars, and hinges on a core technical challenge: simultaneously capturing the fine-grained acoustic details that define vocal richness and the long-range semantic structures governing intelligibility and natural prosody.

Inspired by the success of large language models (LLMs), a dominant paradigm frames TTS as a sequence modeling task over discrete tokens from pre-trained neural audio codecs (e.g., EnCodec [^D{\'e}fossez2022High]).

Autoregressively or Non-autoregressively predicting these tokens from text or phonemes [^Borsos2023Audiolm], [^Kharitonov2023Speak,], [^Chen2025Neural], [^WangMaskGCT], [^Peng2024VoiceCraft] offers excellent scalability and in-context learning capabilities. 
However, this approach faces a fundamental "quantization ceiling", as the compression process irreversibly discards subtle acoustic details.

To mitigate this quality loss, state-of-the-art TTS systems [^Du2024Cosyvoice], [^Du2024Cosyvoice], [^Du2025Cosyvoice], [^Zhou2025IndexTTS2], [^Casanova2024Xtts] adopt multi-stage hybrid pipelines.

Here, an LLM generates discrete tokens which condition a separate diffusion-based decoder.

While improving fidelity, this solution creates a stark semantic-acoustic divide: the LLM operates in an abstract, discrete space unaware of acoustic reality, while the diffusion model performs local refinement without high-level context. 
This fragmentation prevents end-to-end optimization and limits holistic, expressive and context-aware speech synthesis.

Alternatively, other approaches directly model continuous speech representations to avoid quantization loss. 
Early systems like Tacotron 2 [^Shen2018Natural] and more recent models such as MELLE [^Meng2024Autoregressive] generate mel-spectrograms autoregressively. 
However, predicting continuous targets under standard regression losses often yields over-smoothed and low-diversity outputs. 
To address this, recent innovations have explored replacing the regression objective with a denoising process to model the distribution of the next continuous representations, spanning both non-autoregressive paradigms [^Shen2023NaturalSpeech], [^Le2023Voicebox], [^Chen2024F5-TTS] and autoregressive methods[^Li2024Autoregressive], [^Jia2025Ditar], [^Peng2025Vibevoice]. 
Among these, autoregressive approaches have often demonstrated superior performance in capturing natural prosody and expressive variation.

This innovation successfully enhances the detail and diversity of generated continuous representations. 
However, a more fundamental issue persists: in a fully continuous autoregressive model, the tasks of high-level semantic-prosodic planning and fine-grained acoustic rendering are conflated within a single learning objective. 
The model is forced to simultaneously solve two disparate tasks—requiring different inductive biases—in a continuous output space. 
This entanglement presents a significant challenge to the modeling capacity of a single LLM, as it must learn to be both a global planner and a local renderer without an inherent architectural bias to separate these functions.

We argue that this conflation is a root cause of instability. 
The model's focus is inevitably pulled towards fitting low-level acoustic textures, which compromises its ability to maintain high-level semantic coherence, leading to the well-known problem of error accumulation over long sequences [^Pasini2024Continuous].

In this work, we introduce a tokenizer-free, end-to-end framework that resolves this trade-off through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel TTS model--\modelname{}.

Our key insight is that holistic and expressive speech synthesis requires explicit architectural separation between semantic-prosodic planning and acoustic rendering, yet should remain within a cohesive, end-to-end trainable system.

The core innovation is a differentiable Finite Scalar Quantization (FSQ) [^MentzerFinite] bottleneck that induces natural specialization:
(1) a Text-Semantic Language Model (TSLM) generates semantic-prosodic plans stabilized through quantization, focusing on linguistically meaningful patterns;
and (2) a Residual Acoustic Language Model (RALM) recovers fine-grained details lost during quantization, specializing in acoustic refinement.

This hierarchical design enables each component to excel at its respective role while maintaining differentiability, and both of them will be used to guide a local diffusion decoder to generate high-fidelity speech latents.

Critically, the entire hierarchical model is trained end-to-end under a simple diffusion objective, seamlessly integrating planning and rendering without pre-trained tokenizers.

Trained on a massive 1.8 million hours of bilingual corpus, our \modelname{}-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis. 
Our main contributions are as follows:
\vspace{-2mm}

[itemsep=0.2em, leftmargin=2em]

-  We propose an end-to-end hierarchical architecture that introduces an internal semi-discrete bottleneck to resolve the expressivity-stability trade-off.

This mechanism implicitly addresses task entanglement in continuous models by inducing a beneficial separation between semantic-prosodic planning and fine-grained acoustic modeling within a single, unified framework.

-  We introduce a residual learning strategy that, in conjunction with the bottleneck, enables a holistic yet specialized modeling process.

Unlike fragmented multi-stage pipelines, our approach achieves functional separation without architectural fragmentation, simplifying the training pipeline and eliminating dependency on external speech tokenizers.

-  We demonstrate the efficacy of our approach through large-scale training on a massive 1.8 million hours of bilingual speech.

The resulting model, VoxCPM-0.5B, achieves state-of-the-art zero-shot TTS performance among open-source systems with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, validating its practical strength.

-  We provide extensive ablation studies that conclusively validate the semi-discrete residual representations as the crucial component for robust, expressive, and l context-aware synthesis.

Besides, we release the codes and models publicly to support community development and future research.

## 2·Related Work

### Discrete Token-Based TTS

The discrete token paradigm has emerged as a dominant approach in modern TTS, leveraging the success of large language models.

This method converts speech into discrete representations using neural audio codecs such as EnCodec [^D{\'e}fossez2022High] and DAC [^Kumar2023High-Fidelity] through residual vector quantization (RVQ).

AudioLM [^Borsos2023Audiolm] and VALL-E [^Chen2025Neural] pioneered this direction by framing audio generation and TTS as an autoregressive sequence prediction task over discrete acoustic tokens.

Subsequent developments include SoundStorm [^Borsos2023Soundstorm], which introduced non-autoregressive generation for improved efficiency, and Spear-TTS [^Kharitonov2023Speak,], which focused on multilingual capabilities with minimum supervision. 
Besides, VoiceCraft [^Peng2024VoiceCraft] and XTTS [^Casanova2024Xtts] further advanced zero-shot TTS with in-context learning.

Recent advancements have focused on enhancing the scalability, controllability and zero-shot adaptation. 
CosyVoice [^Du2024Cosyvoice] proposed supervised semantic tokens for improved zero-shot performance, while its successors, 

CosyVoice 2 and 3 [^Du2024Cosyvoice], [^Du2025Cosyvoice]
incorporated text-based LLM initialization, streaming synthesis, and large-scale training data for human-parity quality, low latency and in-the-wild scenarios.

IndexTTS [^Deng2025Indextts] and IndexTTS2 [^Zhou2025IndexTTS2] introduced precise duration and emotion control in autoregressive token generation, enabling applications with strict timing and expressivity requirements.

SparkTTS [^Wang2025Spark-TTS] utilized single-stream decoupled speech tokens for modeling efficiency, and FireRedTTS [^Guo2024Fireredtts] along with its update FireRedTTS-2 [^Xie2025FireRedTTS-2] established frameworks for industry-level generative speech, including long-form multi-speaker dialogue.   
Openaudio-s1 [^OpenAudio2024OpenAudio] used dual AR architecture and online Reinforcement Learning from Human Feedback (RLHF) to improve expressiveness and instruction-following capabilities.

Higgs Audio v2 [^BosonAI2025Higgs] proposed a unified audio tokenizer captures
both semantic and acoustic features, and  pretrained on over 10 million hours of audio data, providing a powerful foundation model. 
Despite these progresses, discrete approaches suffer from inherent quantization artifacts, limiting acoustic fidelity and prompting hybrid solutions.

\vspace{-0.2cm}

### Continuous Representation TTS

To circumvent quantization losses in discrete models, continuous representation approaches directly model speech features such as mel-spectrograms or audio latents.

Early systems like Tacotron 2 [^Shen2018Natural] established the encoder-decoder framework for text-to-mel mapping, while FastSpeech [^Ren2020FastSpeech] introduced explicit duration modeling for alignment stability.

Inspired from VALL-E,  MELLE [^Meng2024Autoregressive] autoregressively generated continuous mel-spectrogram frames directly from text condition, and incorporated variational inference to facilitate sampling mechanisms.

Recent developments have integrated diffusion processes to enhance detail and diversity.

Non-autoregressive models like NaturalSpeech 2 [^Shen2023NaturalSpeech] and VoiceBox [^Le2023Voicebox] apply diffusion directly on continuous representations.

F5-TTS [^Chen2024F5-TTS] advanced flow-matching for efficient synthesis. 
Autoregressive paradigms, often superior in prosody and variation, additionally possess the capability for streaming synthesis.

Innovations like ARDiT [^Li2024Autoregressive] use an autogressive diffusion transformer for TTS, unifying semantic coherence and acoustic naturalness via parameter sharing. 
DiTAR [^Jia2025Ditar] extended this with a patch-based design: a causal LM for inter-patch stability and a bidirectional local diffusion transformer for intra-patch refinement. 
VibeVoice [^Peng2025Vibevoice] employed next-token diffusion for long-form multi-speaker synthesis. 
Besides, recent models such as CLEAR [^Wu2025Clear] and FELLE [^Wang2025Felle] focus on latent autoregressive modeling with token-wise coarse-to-fine hierarchies, while MELA-TTS [^An2025Mela-TTS] and KALL-E [^Zhu2024Autoregressive] combine joint transformer-diffusion with next-distribution prediction for improved efficiency and quality.

Despite these advances, continuous models often entangle high-level semantic planning with low-level acoustic rendering, leading to instability in long sequences without explicit separation.

\vspace{-0.2cm}

### Hierarchical and Residual Modeling in TTS

Hierarchical and residual approaches decompose TTS into layered tasks to balance stability and expressivity.

HierSpeech++ [^Lee2025Hierspeech++] employed variational inference for semantic-acoustic mapping. 
HALL-E [^NishimuraHall-E] uses hierarchical neural codecs with LLMs for minute-long synthesis.

MARS6 [^Baas2025Mars6] builds robust encoder-decoder transformers with hierarchical tokens.

DiffStyleTTS [^Liu2024DiffStyleTTS] applies diffusion for hierarchical prosody modeling.

HAM-TTS [^Wang2024Ham-TTS] introduces hierarchical acoustic modeling with data augmentation for zero-shot TTS. 
QTTS [^Han2025Quantize] features hierarchical parallel architectures for residually quantized codes. 
In song generation, LeVo [^Lei2025LeVo] likewise introduced a hierarchical framework using two decoder-only transformers for layered modeling of mixed and separated part in a song, achieving enhanced generation quality.

These methods address flaws in prior paradigms: implicit designs lack regulated bottlenecks, tokenizer-dependent models suffer discrete losses, and fragmented stages hinder end-to-end optimization.

However, few fully integrate explicit residual designs with semi-discrete bottlenecks in a unified framework, as proposed in our work, to achieve implicit disentanglement without external dependencies.
