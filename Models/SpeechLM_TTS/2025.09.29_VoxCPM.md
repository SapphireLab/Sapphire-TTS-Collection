# VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning

<details>
<summary>基本信息</summary>

- 标题: "VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning."
- 作者:
  - 01 Yixuan Zhou
  - 02 Guoyang Zeng
  - 03 Xin Liu
  - 04 Xiang Li
  - 05 Renjie Yu
  - 06 Ziyang Wang
  - 07 Runchuan Ye
  - 08 Weiyue Sun
  - 09 Jiancheng Gui
  - 10 Kehan Li
  - 11 Zhiyong Wu
  - 12 Zhiyuan Liu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.24650v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2509.24650v1](PDF/2025.09.29_2509.24650v1_VoxCPM__Tokenizer-Free_TTS_for_Context-Aware_Speech_Generation_and_True-to-Life_Voice_Cloning.pdf)
  - [Publication] #TODO

</details>

## Abstract

Generative models for speech synthesis face a fundamental trade-off: discrete tokens ensure stability but sacrifice expressivity, while continuous signals retain acoustic richness but suffer from error accumulation due to task entanglement. 
This challenge has driven the field towards multi-stage pipelines that rely on pre-trained speech tokenizers, but these create a semantic-acoustic divide, limiting holistic and expressive speech generation.  
We resolve these dilemma through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel tokenizer-free TTS model--\modelname{}.
Our framework introduces a differentiable quantization bottleneck that induces natural specialization: 
a Text-Semantic Language Model (TSLM) generates semantic-prosodic plans, while a Residual Acoustic Model (RALM) recovers fine-grained acoustic details.
This hierarchical semantic-acoustic representation guides a local diffusion-based decoder to generate high-fidelity speech latents. 
Critically, the entire architecture is trained end-to-end under a simple diffusion objective, eliminating dependency on external speech tokenizers. 
Trained on a massive 1.8 million hours of bilingual corpus, our \modelname{}-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis. 
Besides, VoxCPM shows the capability to comprehend text to infer and generate appropriate prosody and style, delivering speech with context-aware expressiveness and natural flow. 
To facilitate community-driven research and development, \modelname{} is publicly accessible under Apache 2.0.

## 1·Introduction

The pursuit of modern text-to-speech (TTS) systems has evolved beyond intelligibility toward the synthesis of genuinely human-like audio, capable of conveying subtle emotions, speaker identity, and contextual nuances [^Shen2018Natural], [^Ping2017Deep], [^Ren2020FastSpeech], [^Li2019Neural]. 
This leap is critical for applications like empathetic virtual assistants and immersive digital avatars, and hinges on a core technical challenge: simultaneously capturing the fine-grained acoustic details that define vocal richness and the long-range semantic structures governing intelligibility and natural prosody.

Inspired by the success of large language models (LLMs), a dominant paradigm frames TTS as a sequence modeling task over discrete tokens from pre-trained neural audio codecs (e.g., EnCodec [^D{\'e}fossez2022High]).

Autoregressively or Non-autoregressively predicting these tokens from text or phonemes [^Borsos2023Audiolm], [^Kharitonov2023Speak,], [^Chen2025Neural], [^WangMaskGCT], [^Peng2024VoiceCraft] offers excellent scalability and in-context learning capabilities. 
However, this approach faces a fundamental "quantization ceiling", as the compression process irreversibly discards subtle acoustic details.

To mitigate this quality loss, state-of-the-art TTS systems [^Du2024Cosyvoice], [^Du2024Cosyvoice], [^Du2025Cosyvoice], [^Zhou2025IndexTTS2], [^Casanova2024Xtts] adopt multi-stage hybrid pipelines.

Here, an LLM generates discrete tokens which condition a separate diffusion-based decoder.

While improving fidelity, this solution creates a stark semantic-acoustic divide: the LLM operates in an abstract, discrete space unaware of acoustic reality, while the diffusion model performs local refinement without high-level context. 
This fragmentation prevents end-to-end optimization and limits holistic, expressive and context-aware speech synthesis.

Alternatively, other approaches directly model continuous speech representations to avoid quantization loss. 
Early systems like Tacotron 2 [^Shen2018Natural] and more recent models such as MELLE [^Meng2024Autoregressive] generate mel-spectrograms autoregressively. 
However, predicting continuous targets under standard regression losses often yields over-smoothed and low-diversity outputs. 
To address this, recent innovations have explored replacing the regression objective with a denoising process to model the distribution of the next continuous representations, spanning both non-autoregressive paradigms [^Shen2023NaturalSpeech], [^Le2023Voicebox], [^Chen2024F5-TTS] and autoregressive methods[^Li2024Autoregressive], [^Jia2025Ditar], [^Peng2025Vibevoice]. 
Among these, autoregressive approaches have often demonstrated superior performance in capturing natural prosody and expressive variation.

This innovation successfully enhances the detail and diversity of generated continuous representations. 
However, a more fundamental issue persists: in a fully continuous autoregressive model, the tasks of high-level semantic-prosodic planning and fine-grained acoustic rendering are conflated within a single learning objective. 
The model is forced to simultaneously solve two disparate tasks—requiring different inductive biases—in a continuous output space. 
This entanglement presents a significant challenge to the modeling capacity of a single LLM, as it must learn to be both a global planner and a local renderer without an inherent architectural bias to separate these functions.

We argue that this conflation is a root cause of instability. 
The model's focus is inevitably pulled towards fitting low-level acoustic textures, which compromises its ability to maintain high-level semantic coherence, leading to the well-known problem of error accumulation over long sequences [^Pasini2024Continuous].

In this work, we introduce a tokenizer-free, end-to-end framework that resolves this trade-off through hierarchical semantic-acoustic modeling with semi-discrete residual representations and present a novel TTS model--\modelname{}.

Our key insight is that holistic and expressive speech synthesis requires explicit architectural separation between semantic-prosodic planning and acoustic rendering, yet should remain within a cohesive, end-to-end trainable system.

The core innovation is a differentiable Finite Scalar Quantization (FSQ) [^MentzerFinite] bottleneck that induces natural specialization:
(1) a Text-Semantic Language Model (TSLM) generates semantic-prosodic plans stabilized through quantization, focusing on linguistically meaningful patterns;
and (2) a Residual Acoustic Language Model (RALM) recovers fine-grained details lost during quantization, specializing in acoustic refinement.

This hierarchical design enables each component to excel at its respective role while maintaining differentiability, and both of them will be used to guide a local diffusion decoder to generate high-fidelity speech latents.

Critically, the entire hierarchical model is trained end-to-end under a simple diffusion objective, seamlessly integrating planning and rendering without pre-trained tokenizers.

Trained on a massive 1.8 million hours of bilingual corpus, our \modelname{}-0.5B model achieves state-of-the-art zero-shot TTS performance among open-source systems, demonstrating that our approach delivers expressive and stable synthesis. 
Our main contributions are as follows:
\vspace{-2mm}

[itemsep=0.2em, leftmargin=2em]

-  We propose an end-to-end hierarchical architecture that introduces an internal semi-discrete bottleneck to resolve the expressivity-stability trade-off.

This mechanism implicitly addresses task entanglement in continuous models by inducing a beneficial separation between semantic-prosodic planning and fine-grained acoustic modeling within a single, unified framework.

-  We introduce a residual learning strategy that, in conjunction with the bottleneck, enables a holistic yet specialized modeling process.

Unlike fragmented multi-stage pipelines, our approach achieves functional separation without architectural fragmentation, simplifying the training pipeline and eliminating dependency on external speech tokenizers.

-  We demonstrate the efficacy of our approach through large-scale training on a massive 1.8 million hours of bilingual speech.

The resulting model, VoxCPM-0.5B, achieves state-of-the-art zero-shot TTS performance among open-source systems with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, validating its practical strength.

-  We provide extensive ablation studies that conclusively validate the semi-discrete residual representations as the crucial component for robust, expressive, and l context-aware synthesis.

Besides, we release the codes and models publicly to support community development and future research.
