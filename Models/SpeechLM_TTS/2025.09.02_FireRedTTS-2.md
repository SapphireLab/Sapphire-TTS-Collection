# FireRedTTS-2

<details>
<summary>基本信息</summary>

- 标题: "FireRedTTS-2: Towards Long Conversational Speech Generation for Podcast and Chatbot."
- 作者:
  - 01 Kun Xie
  - 02 Feiyu Shen
  - 03 Junjie Li
  - 04 Fenglong Xie
  - 05 Xu Tang
  - 06 Yao Hu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.02020v2)
  - [Publication]()
  - [Github](https://github.com/FireRedTeam/FireRedTTS2) 开放预训练权重和推理代码
  - [Demo](https://fireredteam.github.io/demos/firered_tts_2/)
- 文件:
  - [ArXiv:2509.02020v1](PDF/2025.09.02_2509.02020v1__FireRedTTS-2__Towards_Long_Conversational_Speech_Generation_for_Podcast_and_Chatbot.pdf)
  - [ArXiv:2509.02020v2](PDF/2025.09.04_2509.02020v2__FireRedTTS-2__Towards_Long_Conversational_Speech_Generation_for_Podcast_and_Chatbot.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<!--
Current dialogue generation approaches typically require the complete dialogue text before synthesis and produce a single, inseparable speech containing all voices, making them unsuitable for interactive chat; moreover, they suffer from unstable synthesis, inaccurate speaker transitions, and incoherent prosody.
In this work, we present ***FireRedTTS-2***, a long-form streaming TTS system for multi-speaker dialogue generation, delivering stable, natural speech with reliable speaker switching and context-aware prosody.
A new 12.5Hz streaming speech tokenizer accelerates training and inference, extends maximum dialogue length, encodes richer semantics to stabilize text-to-token modeling and supports high-fidelity streaming generation for real-time applications.
We adopt a text–speech interleaved format, concatenating speaker-labeled text with aligned speech tokens in chronological order, and model it with a dual-transformer: a large decoder-only transformer predicts tokens at the first layer, and a smaller one completes subsequent layers.
Experimental results show that ***FireRedTTS-2*** integrates seamlessly with chat frameworks and, with minimal fine-tuning, produces emotionally expressive speech guided by implicit contextual cues.
In podcast generation, it surpasses existing systems including MoonCast, Zipvoice-Dialogue, and MOSS-TTSD in objective intelligibility, speaker-turn reliability, and perceived naturalness with context-consistent prosody.
Our demos are available at [this https url](https://fireredteam.github.io/demos/firered_tts_2).
-->
当前的对话生成方法通常需要在合成之前获得完整的对话文本, 并生成包含所有声音的单一不可分割的音频, 使其不适用于交互式聊天; 此外, 它们还存在合成不稳定, 说话人切换不准确, 韵律不连贯等问题.

在本工作中, 我们提出了 ***FireRedTTS-2***, 一种面向多说话人对话生成的长形式流式 TTS 系统, 能够生成稳定自然的语音, 实现可靠的说话人切换和上下文感知的韵律.
我们提出了一种新式的 12.5Hz 流式语音 Tokenizer 加速训练和推理, 延长最大对话长度, 编码更丰富的语义信息以稳定文本到 Token 建模并支持高保真的流式生成, 适用于实时应用.
我们采用了文本-语音交错格式, 将带说话人标签的文本和对齐的语音 Token 以时间顺序拼接, 并用 Dual-Transformer 建模: 大的 Decoder-Only Transformer 预测第一层的 Token, 小的负责完成后续层.

实验结果表明 ***FireRedTTS-2*** 与聊天框架无缝集成, 仅需微调, 就可以由隐式上下文信息引导产生富有情感色彩的语音.
在播客生成时, 它在客观可理解性, 说话人切换, 感知的自然度, 上下文一致性的韵律等方面超越了现有系统 (如 MoonCast, ZipVoice-Dialogue, MOSS-TTSD).
我们的演示示例可在[此处](https://fireredteam.github.io/demos/firered_tts_2)获得.

## 1·Introduction: 引言

Large language model (LLM) based text-to-speech (TTS) systems can generate natural-sounding speech with zero-shot voice cloning and are widely used for monologue applications like video dubbing.
These systems typically follow one of two modeling paradigms: an autoregressive, decoder-only transformer that predicts speech tokens (**FireRedTTS**[^Guo2024Fireredtts], **FireRedTTS-1S**[^Guo2025Fireredtts-1s], **CosyVoice**[^Du2024Cosyvoice], **CosyVoice2**[^Du2024Cosyvoice2], **CosyVoice3**[^Du2025Cosyvoice3], **IndexTTS**[^Deng2025Indextts], **Spark-TTS**[^Wang2025Spark-TTS]), or a non-autoregressive flow-matching model that produces mel-spectrograms directly from text (**F5-TTS**[^Chen2024F5-TTS], **E2-TTS**[^Eskimez2024E2-TTS]).
While these monologue TTS systems can be adapted to dialogue generation by segmenting dialogue text and synthesizing each fragment independently (**Step-Audio**[^Huang2025Step-Audio], **AudioGPT**[^Huang2023AudioGPT], **PodAgent**[^Xiao2025PodAgent]), this strategy ignores preceding text and speech context, leading to a loss of conversational coherence.

Recent works have extended TTS system to two-speaker dialogue generation, which can be grouped into three categories based on how text and speech are organized across turns:
(1) splitting the dialogue text into two parallel channels and synthesizing a single mixed speech track containing both voices, which can naturally handles overlapping speech and generate interjections effects(**CoMoMix**[^Zhang2024CoVoMix], **CoMoMix2**[^Zhang2025CoVoMix2]);
(2) concatenating the dialogue text in chronological order with each utterance prefixed by a speaker label, which likewise produces a mixed speech track (**MoonCast**[^Ju2025MoonCast], [^Darefsky2024Parakeet], **ZipVoice-Dialogue**[^Zhu2025ZipVoice-Dialog], **MOSS-TTSD**[^Team2025Text], **VibeVoice**[^Peng2025VibeVoice]);
(3) interleaving the text and speech of each utterance [^Schalkwyk2025Crossing].

Approaches (1) and (2) require the complete dialogue text before synthesis and yield a single inseparable mixed speech, limiting their suitability for interactive scenarios such as chat, whereas (3) supports flexible sentence-by-sentence generation, suitable for both interactive chat and podcast production.

In this work, we present ***FireRedTTS-2***, a long-form, streaming TTS system for multi-speaker dialogue and podcast generation that delivers stable, natural speech, reliable speaker switching, and context-aware prosody.
A new streaming 12.5Hz speech tokenizer accelerates training and inference, lengthens the effective dialogue context, encodes richer semantics to stabilize text-to-token modeling and supports high-fidelity streaming generation for real-time applications.
We adopt an interleaved text–speech format by concatenating speaker-labeled text with speech tokens in chronological order, and model it with a dual-transformer architecture: a large decoder-only network predicts tokens at the first layer, while a smaller network refines the subsequent layers.
Experimental results show that ***FireRedTTS-2*** integrates seamlessly with chat frameworks and, with minimal fine-tuning, produces emotionally expressive speech guided by implicit context.
In podcast generation, it surpasses the state of the art systems including **MoonCast**[^Ju2025MoonCast], **ZipVoice-Dialogue**[^Zhu2025ZipVoice-Dialog], and **MOSS-TTSD**[^Team2025Text] in objective intelligibility, speaker-turn reliability, and perceived naturalness, while maintaining prosody consistent with long-range context.

## 2·Methodology: 方法

<--
As shown in [Fig.01](#img:tts_model_framework), ***FireRedTTS-2*** consists of a newly developed speech tokenizer and a text-to-speech model with perception to previous text and speech context.
-->
如[图 01](#img:tts_model_framework) 所示, ***FireRedTTS-2*** 由一个新开发的语音分词器和一个能够感知先前文本和语音上下文的文本转语音模型组成.

> ![](Images/2025.09.02_FireRedTTS-2_Fig.01.png)
>
> <a id="img:tts_model_framework">
> <!--
> Fig.01: An overview of ***FireRedTTS-2***, including:
> (a) a new speech tokenizer with a 12.5Hz frame rate and enhanced semantic information,
> (b) a text-to-speech model using a dual-transformer architecture with interleaved text–speech input, enabling sentence-by-sentence generation and contextually coherent prosody.
> -->
> 图 01: ***FireRedTTS-2*** 的概览, 包括:
> (a) 一个新的语音分词器, 具有 12.5Hz 帧率和增强的语义信息,
> (b) 一个文本转语音模型, 使用 Dual-Transformer 架构, 采用交错的文本-语音输入, 实现逐句生成和上下文连贯的韵律.
> </a>

### Speech Tokenizer: 语音分词器

<!--
We design our speech tokenizer to enhance dialogue modeling, with a focus on long, multi-speaker speech sequence.
To make such sequences tractable, we reduce the frame rate to 12.5Hz, half that of most open-source tokenizers (**CosyVoice**[^Du2024Cosyvoice], **CosyVoice2**[^Du2024Cosyvoice2], **CosyVoice3**[^Du2025Cosyvoice3], **X-Codec**[^Ye2025Codec], **X-Codec2**[^Ye2025Llasa], **SpeechTokenizer**[^Zhang2023SpeechTokenizer]).
We further employ semantic injection and supervision to simplify text-to-token modeling, which has been shown to improve synthesis stability (**XY-Tokenizer**[^Team2025Text], **X-Codec**[^Ye2025Codec], **X-Codec2**[^Ye2025Llasa], **SpeechTokenizer**[^Zhang2023SpeechTokenizer], **Mimi**[^D{\'e}fossez2024Moshi]).
In addition, the tokenizer supports high-fidelity streaming generation for real-time applications.
-->
我们设计了语音分词器, 以便增强对话建模, 着重于长时间的多说话人语音序列.
为了使得这样的序列可以处理, 我们将帧率降低到 12.5 Hz, 是大多数开源分词器的一半 (**CosyVoice**[^Du2024Cosyvoice], **CosyVoice2**[^Du2024Cosyvoice2], **CosyVoice3**[^Du2025Cosyvoice3], **X-Codec**[^Ye2025Codec], **X-Codec2**[^Ye2025Llasa], **SpeechTokenizer**[^Zhang2023SpeechTokenizer]).
我们进一步采用语义注入和监督以简化文本到 Token 建模, 这种做法已被证明可以提升合成稳定性 (**XY-Tokenizer**[^Team2025Text], **X-Codec**[^Ye2025Codec], **X-Codec2**[^Ye2025Llasa], **SpeechTokenizer**[^Zhang2023SpeechTokenizer], **Mimi**[^D{\'e}fossez2024Moshi]).
此外, 该分词器支持高保真的流式生成以适用于实时应用.

<!--
As illustrated in [Fig.01](#img:tts_model_framework)(a), our speech tokenizer employs a pretrained **Whisper**[^Radford2022Whisper] encoder to extract semantic features from the 16kHz input speech.
These semantic features are encoded by an adapter and then concatenated with acoustic features from a trainable acoustic encoder structurally identical to the Whisper encoder.
The combined features undergo 4 times downsampling from 50Hz to 12.5Hz and are discretized by a residual vector quantizer (RVQ) (**SoundStream**[^Zeghidour2021SoundStream]) with 16 layers, each containing 2048 code entries.
The quantized features are upsampled to 50Hz and fed to a semantic decoder to predict the original semantic features derived from the pretrained Whisper encoder.
The same upsampled features are also used by a **Vocos**[^Siuzdak2023Vocos]-based acoustic decoder to reconstruct the waveform.
Depending on the reception fields of its inner convolution and attention layers, the acoustic decoder can be implemented as either streaming or non-streaming.
-->
如[图 01](#img:tts_model_framework)(a) 所示, 我们的语音分词器采用预训练 **Whisper**[^Radford2022Whisper] 编码器从 16kHz 的输入语音中提取语义特征.
这些语义特征通过一个适配器 `SSLAdaptor` 进行编码, 然后和从可训练的声学编码器获得的声学特征拼接, 该编码器的结构和 Whisper 编码器相同.
拼接后的特征经过四倍下采样从 50Hz 到 12.5Hz, 并通过具有 16 个码本, 每个码本具有 2048 码元的残差向量量化器 (**SoundStream**[^Zeghidour2021SoundStream]) 进行离散化.
量化后的特征上采样到 50 Hz 然后输入到语义解码器以预测从预训练 Whisper 编码器得到的原始的语义特征.
上采样后的特征也输入到基于 **Vocos**[^Siuzdak2023Vocos] 的声学解码器来重构波形.
根据其内部卷积层和注意力层的感受野特性，声学解码器可以实现为流式或非流式模式.

<!--
To balance generalization capability and speech quality, we train our speech tokenizer in two stages similar to **MOSS-TTSD**[^Team2025Text].
First, the acoustic decoder is implemented as non-streaming and optimized to predict 16kHz speech.
We use approximately 500k hours of speech data and train the model for 320k steps on 32 H800 GPUs, with each sample randomly cropped to 6 seconds.
For the final 35k steps, we incorporate the perceptual loss [^Ye2025Llasa], [^Parker2024Scaling] to further improve semantic details.
In the second stage, we freeze the encoding part and replace the acoustic decoder with a fully streaming variant that predicts 24kHz speech.
We continue to train the speech tokenizer on a subset of 60k hours high-fidelity speech data for 80k steps.
-->
为了平衡泛化能力和语音质量, 我们以类似于 **MOSS-TTSD**[^Team2025Text] 的两阶段方式训练语音分词器.
首先, 声学解码器以非流式实现, 然后优化用于预测 16kHz 语音.
我们使用约 500K 小时语音数据并在 32 个 H800 GPU 上训练 320K 步, 每个样本随机裁剪为 6 秒.
对于最后 35K 步, 我们将感知损失 [^Ye2025Llasa], [^Parker2024Scaling] 纳入其中, 以进一步提高语义细节.

在第二个阶段, 我们冻结编码部分, 将声学解码器替换为全流式变体以预测 24kHz 语音.
我们在 60K 小时高保真语音数据上继续训练语音分词器 80K 步.

### Text-to-Speech Model: 文本转语音模型

<--
Building on the new speech tokenizer, we employ a dual-transformer architecture akin to [^Schalkwyk2025Crossing], [^D{\'e}fossez2024Moshi] that operates on a text–speech interleaved sequence, enabling flexible sentence-by-sentence generation and reducing first-packet latency.
As illustrated in [Fig.01](#img:tts_model_framework)(b), each dialogue text is prefixed with a speaker tag (e.g., `[S1]`) and concatenated with its corresponding speech tokens; these segments are then joined in temporal order to form sequences such as `[S1]<text><audio>[S2]<text><audio>[S3]<text><audio>...`.
Existing approaches ([^Darefsky2024Parakeet], **MOSS-TTSD**[^Team2025Text]) model multi-layer speech tokens using the delay-pattern[^Copet2023MusicGen]: for $N$ token layers, the $i^\text{th}$ layer is shifted $i-1$ timesteps to the right, and $N$ prediction heads predict these shifted layers in parallel.
This design has two main drawbacks: first, at each timestep the model has only partial access to the speech tokens from previous steps due to the rightward shifts, weakening contextual conditioning; second, obtaining the complete set of $N$ layer tokens for the first timestep requires $N$ autoregressive steps, resulting in high latency.
To overcome these issues, we adopt a dual-transformer architecture comprising a backbone transformer that processes the text–speech interleaved sequence and predicts the first-layer tokens, and a smaller decoder transformer that generates remaining token layers.
Both transformers are based on **Qwen2.5**[^Ahmed2025Qwen] structure.
At each timestep, the decoder consumes both the predicted first layer token and the backbone’s hidden states, which provide complete contextual information.
Comparing with the delay-pattern, it requires one auto-regressive inference step of the backbone transformer and $N-1$ steps of the smaller decoder, reducing computation and first-packet latency.
Moreover, our speech tokenizer produces high-fidelity speech in a streaming manner without requiring separate token-to-speech modules, simplifying the overall system.

建立在新式语音分词器上, 我们采用 Dual-Transformer 架构 (类似 [^Schalkwyk2025Crossing], [^D{\'e}fossez2024Moshi]) 来处理文本-语音交错序列, 实现灵活的逐句生成并减少首包延迟.
如[图 01](#img:tts_model_framework)(b) 所示, 每个对话文本用说话人标签作为前缀, 例如 `[S1]`, 然后和对应的语音 Token 进行拼接, 这些片段按时间顺序组成序列, 例如 `[S1]<text><audio>[S2]<text><audio>[S3]<text><audio>...`.
现有方法 ([^Darefsky2024Parakeet], **MOSS-TTSD**[^Team2025Text]) 使用 Delay-Pattern [^Copet2023MusicGen] 建模多层语音 Token: 对于 $N$ 个 Token 层, 第 $i$ 层


The text-to-speech model is optimized with the following loss function:

$$
\begin{aligned}
\mathcal{L}_{loss}=2*((1-\lambda_{decoder})\mathcal{L}_{backbone}+\lambda_{decoder}\mathcal{L}_{decoder})+\lambda_{text}\mathcal{L}_{text}
\end{aligned}
$$

Here, $\mathcal{L}_{backbone}$ and $\mathcal{L}_{decoder}$ denote the cross-entropy loss of the backbone and decoder transformer respectively.
To improve training efficiency, we optimize the decoder transformer only on 1/8 of the speech segments in the interleaved sequence.
Additionally, we incorporate a cross-entropy loss for the textual part ($\mathcal{L}_{text}$) to stabilize training.
In our experiments, we set $\lambda_{text} = 0.01$ and $\lambda_{decoder} = 0.6$.

To enable the model with dialogue generation capability, we adopt a three-stage curriculum training process utilized in **CoMoMix**[^Zhang2024CoVoMix], **MOSS-TTSD**[^Team2025Text], comprising pretraining, post-training, and supervised fine-tuning (SFT).
The pretraining stage leverages 1.1M hours of monologue speech data and trains the model for 2 epochs to build foundational text-to-speech ability.
Subsequently, we post-train ***FireRedTTS-2*** for 5 epochs on 300k hours of multi-speaker dialogue data, with each dialogue containing 2 to 5 speakers, to enable robust multi-speaker dialogue generation.
Finally, the SFT stage is applied to tailor the model to specific voices with minimal data.

## 3·Downstream Applications: 下游应用

***FireRedTTS-2*** excels at both monologue and dialogue speech generation.
For monologues, it offers competitive zero-shot voice cloning suitable for tasks like video dubbing.
For dialogues generation, it surpasses monologue TTS systems due to its perception of text and speech context, producing speech with coherent prosody.
Compared with existing dialogue TTS systems, it supports sentence-by-sentence generation, enabling both interactive chat and offline podcast production.
Across both modes, ***FireRedTTS-2*** can be tailored to specific application requirements with minimal data, demonstrating strong flexibility.

### Voice Cloning

Our speech tokenizer captures both semantic and acoustic information, enabling fine-grained acoustic modeling.
Paired with large-scale pretraining on monologue speech, it allows ***FireRedTTS-2*** to deliver robust zero-shot voice cloning.
Given a speech prompt and its transcript, we concatenate the prompt transcript, the target text to be synthesized, and the discretized prompt speech tokens, then feed this sequence into the text-to-speech model to autoregressively generate new speech tokens.
The generated tokens are appended to the prompt tokens and decoded into a waveform by the speech tokenizer’s decoder, after which the portion corresponding to the original prompt is removed.

### Interactive Chat

Current interactive chat frameworks (**Step-Audio**[^Huang2025Step-Audio], **AudioGPT**[^Huang2023AudioGPT]) typically rely on monologue TTS systems, which lack perception of prior user queries and system responses, often resulting in inconsistent emotion and prosody.
This can be partially mitigated with explicit instructions such as emotion labels, but it requires additional fine-tuning of the text LLM and adds unnecessary complexity.

![Images/2025.09.02_FireRedTTS-2_Fig.02.png](Images/2025.09.02_FireRedTTS-2_Fig.02.png)

<a id="img:tts2_for_chat">Integration of ***FireRedTTS-2*** into interactive chat scenarios.</a>

As shown in Figure [img:tts2_for_chat](#img:tts2_for_chat), ***FireRedTTS-2*** can be seamlessly integrated into existing chat frameworks without modifying other modules.
To address the inconsistency issue, we fine-tune the post-trained ***FireRedTTS-2*** to infer and adjust emotion and prosody from implicit contextual cues.
Specifically, we curate a 15-hour speech corpus of a distinctive female voice expressing six emotions: surprise, sadness, happiness, concern, apology, and anger.
Then we emulate conversational context by first generating text context with a text LLM and then synthesizing it into speech.
After fine-tuning, ***FireRedTTS-2*** dynamically shifts emotion and tone in response to preceding chat history, delivering a near-human interactive experience.

### Podcast Generation

Subsequent post-training on dialogue corpus equips ***FireRedTTS-2*** with conversational generation abilities, making it well-suited for podcast generation.
Compared with conventional segmenting approaches that utlizes monologue TTS systems, it simplifies the workflow and can synthesizes contextually coherent prosody.
Moreover, it generates dialogue speech sentence by sentence, providing greater flexibility for editing and post-processing.

![Images/2025.09.02_FireRedTTS-2_Fig.03.png](Images/2025.09.02_FireRedTTS-2_Fig.03.png)

<a id="img:tts2_for_zeroshot_podcast">Zero-shot podcast generation of ***FireRedTTS-2***.</a>

As shown in Figure [img:tts2_for_zeroshot_podcast](#img:tts2_for_zeroshot_podcast), ***FireRedTTS-2*** can perform zero-shot podcast generation by placing two dialogue turns as prompt context and then generating subsequent turns one by one.
It currently supports 3 minutes dialogues with 4 speakers and can be easily scaled to longer conversations with more speakers by extending training corpus.
It can also be tailored to specific speakers with minimal data.
In this work, we collected approximately 50 hours of dialogue speech featuring a male and a female podcast host and fine-tuned the post-trained model for 15 epochs.
Once customized,  ***FireRedTTS-2*** delivers stable synthesis, accurate speaker transitions, and contextually coherent prosody that matches the hosts’ distinctive speaking styles.

## 4·Results: 结果

### Speech Tokenizer Evaluation

We evaluate our speech tokenizer on intelligibility, speaker similarity, and speech quality of the reconstructed speech using the LibriSpeech test-clean set, which contains 2,620 utterances at 16 kHz.
Speech intelligibility is measured by word error rate (WER) using a HuBERT-based automatic speech recognition (ASR) system\footnote{\url{https://huggingface.co/facebook/hubert-large-ls960-ft}}.
Speech quality is assessed with speaker similarity (SPK-SIM) computed by the WavLM-Large model \footnote{\url{https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification}}, Short-Time Objective Intelligibility (STOI), Perceptual Evaluation of Speech Quality (PESQ), and UTMOS \footnote{\url{https://github.com/tarepan/SpeechMOS}}.
We compare our speech tokenizer with other methods that likewise incorporate semantic injection and supervision, including **XY-Tokenizer**[^Team2025Text], **XCodec2**[^Ye2025Llasa], **SpeechTokenizer**[^Zhang2023SpeechTokenizer], and Mimi[^D{\'e}fossez2024Moshi]; results are reported in Table [tab:codec_cmp](#tab:codec_cmp).

<a id="tab:codec_cmp">
Comparison between different speech tokenizers.
Best results are marked in bold.
</a>

Table [tab:codec_cmp](#tab:codec_cmp) shows that our speech tokenizer achieves the highest intelligibility, which we attribute to the semantic injection with explicit supervision.
It also ranks first or second on speaker similarity and speech quality metrics, even at a 12.5Hz frame rate, thanks to a larger quantizer that reduces quantization error and a **Vocos**[^Siuzdak2023Vocos]-based acoustic decoder.
However, it trails Mimi on the PESQ metrics, likely because Mimi uses a massive, purely English training corpus that more closely matches the test set.
It also scores lower on UTMOS than Xcodec2, due to its lower 12.5Hz frame rate.
Overall, these results verify that our speech tokenizer can produce high-quality speech despite its low 12.5Hz frame rate.

### Voice Cloning Evaluation

We evaluate ***FireRedTTS-2*** for voice cloning on the "Test-ZH" and "Test-EN" sets from the Seed-TTS-eval[^Anastassiou2024Seed-TTS] benchmark, using the model after the pretraining stage.
We compare it against popular monologue TTS systems, including **Seed-TTS**[^Anastassiou2024Seed-TTS], **F5-TTS**[^Chen2024F5-TTS], **MaskGCT**[^Wang2024MaskGCT], **Spark-TTS**[^Wang2025Spark-TTS], and the CosyVoice series(**CosyVoice**[^Du2024Cosyvoice], **CosyVoice2**[^Du2024Cosyvoice2], **CosyVoice3**[^Du2025Cosyvoice3]).
Results are reported in Table [tab:exp_zeroshot_tts](#tab:exp_zeroshot_tts).

<a id="tab:exp_zeroshot_tts">
The objective evaluation on Seed-TTS test set.
Best results are marked in bold.
</a>

Our pretrained ***FireRedTTS-2*** achieves 1.14\% CER on Mandarin and 1.95\% WER on English, closely matching the best results of 1.12\% CER and 1.83\% WER.
We attribute this to enhanced semantic information in the speech tokens, which strengthens text-to-token modeling.
For speaker similarity, it aligns with human recordings in Mandarin but trails in English, likely due to limited voice diversity in the English training data.
Moreover, systems such as Seed-TTS inject timbre through dedicated diffusion or flow-matching modules, further boosting similarity.
Compared with previous FireRedTTS-1S, it delivers better English WER and speaker similarity, with a slight drop on Mandarin that may stem from the halved frame rate of speech tokenizer.
However, as noted in **FireRedTTS-1S**[^Guo2025Fireredtts-1s], objective metrics cannot faithfully reflect TTS performance due to limited test set coverage and imprecise evaluation tools, and speech with more expressive prosody tends to be rated less intelligible, while plainer ones typically score higher; we therefore place greater weight on the following subjective evaluations.

### Interactive Chat Evaluation

To assess the chat fine-tuned ***FireRedTTS-2***’s ability to infer and adjust synthesis emotions from implicit contextual cues, we built a test set with 30 test cases for each of six emotions: surprise, sadness, happiness, concern, apology, and anger.
For each test case, we use the **Qwen3**[^Yang2025Qwen3] model to generate a text query–response pair that implicitly conveyed the target emotion.
The text query was then synthesized into speech using random voices, and ***FireRedTTS-2*** produced the speech response.
We manually label the emotion of generated speech response and calculate the emotion control accuracy.
The results in Table [tab:exp_finetune_chatbot_instruct](#tab:exp_finetune_chatbot_instruct) show that ***FireRedTTS-2*** can infer appropriate emotions from implicit contextual cues by leveraging preceding text and speech context, thereby enabling a more human-like chat experience and validating the effectiveness of our approach.

> <a id="tab:exp_finetune_chatbot_instruct">Tab.</a>: Emotion control accuracy of ***FireRedTTS-2*** after fine-tuning for interactive chat scenario.

### Podcast Generation Evaluation

To evaluate zero-shot podcast generation, we curated two two-speaker podcast evaluation sets: dialogue-zh and dialogue-en, containing 100 Mandarin and 115 English dialogues, respectively.
Each dialogue test set spans 4 to 10 turns, totaling 1.67 hours for Mandarin and 2.35 hours for English.
For each dialogue, we use the first two turns as the prompt and generate the remaining turns with the post-trained ***FireRedTTS-2*** model.

We assess the generated dialogues using three objective metrics: intelligibility, speaker similarity, and Mel-cepstral distortion (MCD)\footnote{\url{https://github.com/chenqi008/pymcd}}.
For intelligibility, we use Whisper-large-v3\footnote{\url{https://huggingface.co/openai/whisper-large-v3}} to compute word error rate (WER) for English and Paraformer-zh\footnote{\url{https://huggingface.co/funasr/paraformer-zh}} to compute character error rate (CER) for Mandarin.
Speaker similarity is measured with the WavLM-Large model\footnote{\url{https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification}}.
For subjective evaluation, we conduct a comparative mean opinion score (CMOS) test in which raters choose the more natural synthesized dialogue between ***FireRedTTS-2*** and competing models.
We compare against open-source dialogue TTS systems including MoonCast, ZipVoice-Dialog, and MOSS-TTSD\footnote{We were unable to generate the test set with Dia and Sesame due to instability and therefore exclude them from comparison.}.
Because MoonCast, ZipVoice-Dialog, and MOSS-TTSD produce a single mixed-track dialogue containing both voices, we use **Pyannote**[^Plaquet2023Powerset], [^Bredin2023pyannote.audio] to segment each speaker before evaluation.
The results are listed in Table [tab:exp_zeroshot_dialogue](#tab:exp_zeroshot_dialogue).

<a id="tab:exp_zeroshot_dialogue">Objective and subjective evaluation of zero-shot podcast generation.</a>

Table [tab:exp_zeroshot_dialogue](#tab:exp_zeroshot_dialogue) shows that ***FireRedTTS-2*** delivers the most stable synthesis, achieving the lowest WER/CER on dialogue-zh and dialogue-en, suggesting that our lower frame rate and semantically enhanced speech tokenizer enable robust modeling of long speech sequences.
It also attains the highest speaker similarity, reflecting strong cross-turn voice cloning and reliable speaker transitions, and the lowest MCD, indicating minimal deviation from ground truth; CMOS results further confirm its contextually coherent naturalness and highlight the effectiveness of the dual-transformer’s context-learning capabilities.

![Images/2025.09.02_FireRedTTS-2_Fig.04.png](Images/2025.09.02_FireRedTTS-2_Fig.04.png)

<a id="img:exp_feihua_cmos">Subjective preference results between ***FireRedTTS-2*** fine-tuned on two podcast speakers and ground truth recordings. "Win": ***FireRedTTS-2*** synthesis is more natural than ground truth dialogue speech; "Even": indistinguishable; "Fail": ground truth is more natural.</a>

We also evaluate the intelligibility and naturalness of the ***FireRedTTS-2*** model fine-tuned on two podcast speakers using the dialogue-zh set.
For intelligibility, the fine-tuned model maintains stable synthesis performance and achieves a lower CER of 1.66\% than in zero-shot mode.
For naturalness, we conduct a subjective test in which raters are asked to choose the more natural sample between the ***FireRedTTS-2*** synthesis and the ground truth dialogue.
As shown in Figure [img:exp_feihua_cmos](#img:exp_feihua_cmos), our synthesis is preferred as more natural in 28\% of cases and judged equally natural in another 28\%, matching or surpassing real recordings in 56\% of trials and indicating that ***FireRedTTS-2*** can produce human-like podcast speech.

## 5·Conclusions: 结论

In this work, we present ***FireRedTTS-2***, a conversational TTS system for dialogue-centric applications such as interactive chat and podcast generation.
It comprises a newly developed speech tokenizer and a text-to-speech model.
The tokenizer operates at a low 12.5Hz frame rate and encodes richer semantics, shortening speech sequences and facilitating robust text-to-token modeling; it also supports high-fidelity streaming decoding, making the system well suited to real-time use.
The text-to-speech model adopts a dual-transformer architecture and a text–speech interleaved format to enable flexible sentence-by-sentence generation with first-packet latency under 100 ms, serving both real-time interaction and offline podcast production.
Experiments show that, in monologue settings, ***FireRedTTS-2*** offers strong zero-shot voice cloning comparable to conventional monologue TTS systems; in dialogue settings, it can be integrated seamlessly into existing frameworks without adjusting other modules and produces emotionally expressive speech inferred from implicit contextual cues.
In zero-shot podcast generation, it delivers more stable synthesis, more accurate speaker transitions, and more coherent prosody, outperforming state-of-the-art dialogue generation models.
Furthermore, it can be tailored to two specific podcast voices, producing dialogue speech that is difficult to distinguish from human recordings.

## References: 参考文献

[^Guo2024Fireredtts]: Fireredtts: A Foundation Text-to-Speech Framework for Industry-Level Generative Speech Applications. arXiv:2409.03283.
[^Guo2025Fireredtts-1s]: Fireredtts-1s: An Upgraded Streamable Foundation Text-to-Speech System. arXiv:2503.20499.
[^Du2024Cosyvoice]: Cosyvoice: A Scalable Multilingual Zero-Shot Text-to-Speech Synthesizer Based on Supervised Semantic Tokens. arXiv:2407.05407.
[^Du2024Cosyvoice2]: [**CosyVoice2**: Scalable Streaming Speech Synthesis with Large Language Models](2024.12.13_CosyVoice2.md). ArXiv:2412.10117v3.
[^Du2025Cosyvoice3]: Cosyvoice 3: Towards in-the-Wild Speech Generation via Scaling-Up and Post-Training. arXiv:2505.17589.
[^Deng2025Indextts]: Indextts: An Industrial-Level Controllable and Efficient Zero-Shot Text-to-Speech System. arXiv:2502.05512.
[^Wang2025Spark-TTS]: [**Spark-TTS**: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens](2025.03.03_Spark-TTS.md). ArXiv:2503.01710v1.
[^Chen2024F5-TTS]: [**F5-TTS**: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching](../FlowMatching/2024.10.09_F5-TTS.md). ArXiv:2410.06885v3.
[^Eskimez2024E2-TTS]: [**E2 TTS**: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS](../FlowMatching/2024.06.26_E2_TTS.md). ArXiv:2406.18009v2/SLT2024.
[^Huang2025Step-Audio]: Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction. arXiv:2502.11946.
[^Huang2023AudioGPT]: AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head. arXiv:2304.12995.
[^Xiao2025PodAgent]: PodAgent: A Comprehensive Framework for Podcast Generation. Findings of the Association for Computational Linguistics 2025.
[^Zhang2024CoVoMix]: CoVoMix: Advancing Zero-Shot Speech Generation for Human-Like Multi-Talker Conversations. Advances in Neural Information Processing Systems 2024.
[^Zhang2025CoVoMix2]: CoVoMix2: Advancing Zero-Shot Dialogue Generation With Fully Non-Autoregressive Flow Matching. arXiv:2506.00885.
[^Ju2025MoonCast]: MoonCast: High-Quality Zero-Shot Podcast Generation. arXiv:2503.14345.
[^Darefsky2024Parakeet]: Parakeet.
[^Zhu2025ZipVoice-Dialog]: ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation With Flow Matching. arXiv:2507.09318.
[^Team2025Text]: Text to Spoken Dialogue Generation.
[^Peng2025VibeVoice]: VibeVoice Technical Report. arXiv:2508.19205.
[^Schalkwyk2025Crossing]: Crossing the Uncanny Valley of Conversational Voice..
[^Ye2025Codec]: Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model. Proceedings of the AAAI Conference on Artificial Intelligence 2025.
[^Ye2025Llasa]: Llasa: Scaling Train-Time and Inference-Time Compute for Llama-Based Speech Synthesis. arXiv:2502.04128.
[^Zhang2023SpeechTokenizer]: [**Speechtokenizer**: Unified Speech Tokenizer for Speech Large Language Models](../Tokenizers/2023.08.31_SpeechTokenizer.md). ArXiv:2308.16692/ICLR2024.
[^D{\'e}fossez2024Moshi]: Moshi: A Speech-Text Foundation Model for Real-Time Dialogue. arXiv:2410.00037.
[^Radford2022Whisper]: [**Whisper**: Robust Speech Recognition via Large-Scale Weak Supervision](../-ASR/2022.12.06_Whisper.md). ArXiv:2212.04356/ICML2023.
[^Zeghidour2021SoundStream]: [**SoundStream**: An End-to-End Neural Audio Codec](../Tokenizers/2021.07.07_SoundStream.md). ArXiv:2107.03312/TASLP2021.
[^Siuzdak2023Vocos]: Vocos: Closing the Gap Between Time-Domain and Fourier-Based Neural Vocoders for High-Quality Audio Synthesis. The Twelfth International Conference on Learning Representations 2023.
[^Parker2024Scaling]: Scaling Transformers for Low-Bitrate High-Quality Speech Coding. The Thirteenth International Conference on Learning Representations 2024.
[^Copet2023MusicGen]: [**MusicGen**: Simple and Controllable Music Generation](../Music/2023.06.08_MusicGen.md). ArXiv:2306.05284/NeurIPS2023.
[^Ahmed2025Qwen]: Qwen 2.5: A Comprehensive Review of the Leading Resource-Efficient LLM With Potentioal to Surpass All Competitors. Authorea Preprints 2025.
[^Anastassiou2024Seed-Tts]: Seed-Tts: A Family of High-Quality Versatile Speech Generation Models. arXiv:2406.02430.
[^Wang2024MaskGCT]: [**MaskGCT**: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer](2024.09.01_MaskGCT.md). ArXiv:2409.00750v3.
[^Yang2025Qwen3]: Qwen3 Technical Report. arXiv:2505.09388.
[^Plaquet2023Powerset]: {Powerset Multi-Class Cross Entropy Loss for Neural Speaker Diarization}. Proc. INTERSPEECH 2023 2023.
[^Bredin2023pyannote.audio]: {pyannote.audio 2.1 Speaker Diarization Pipeline: Principle, Benchmark, and Recipe}. Proc. INTERSPEECH 2023 2023.