# FireRedTTS-2: Towards Long Conversational Speech Generation for Podcast and Chatbot

<details>
<summary>基本信息</summary>

- 标题: "FireRedTTS-2: Towards Long Conversational Speech Generation for Podcast and Chatbot."
- 作者:
  - 01 Kun Xie
  - 02 Feiyu Shen
  - 03 Junjie Li
  - 04 Fenglong Xie
  - 05 Xu Tang
  - 06 Yao Hu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.02020v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2509.02020v1](PDF/2025.09.02_2509.02020v1_FireRedTTS-2__Towards_Long_Conversational_Speech_Generation_for_Podcast_and_Chatbot.pdf)
  - [ArXiv:2509.02020v2](PDF/2025.09.04_2509.02020v2_FireRedTTS-2__Towards_Long_Conversational_Speech_Generation_for_Podcast_and_Chatbot.pdf)
  - [Publication] #TODO

</details>

## Abstract

Current dialogue generation approaches typically require the complete dialogue text before synthesis and produce a single, inseparable speech containing all voices, making them unsuitable for interactive chat; moreover, they suffer from unstable synthesis, inaccurate speaker transitions, and incoherent prosody.

In this work, we present FireRedTTS‑2, a long-form streaming TTS system for multi-speaker dialogue generation, delivering stable, natural speech with reliable speaker switching and context-aware prosody.
A new 12.5Hz streaming speech tokenizer accelerates training and inference, extends maximum dialogue length, encodes richer semantics to stabilize text-to-token modeling and supports high-fidelity streaming generation for real-time applications.
We adopt a text–speech interleaved format, concatenating speaker-labeled text with aligned speech tokens in chronological order, and model it with a dual-transformer: a large decoder-only transformer predicts tokens at the first layer, and a smaller one completes subsequent layers.
Experimental results show that FireRedTTS‑2 integrates seamlessly with chat frameworks and, with minimal fine-tuning, produces emotionally expressive speech guided by implicit contextual cues.
In podcast generation, it surpasses existing systems including MoonCast, Zipvoice-Dialogue, and MOSS-TTSD in objective intelligibility, speaker-turn reliability, and perceived naturalness with context-consistent prosody.
Our demos are available at \url{https://fireredteam.github.io/demos/firered_tts_2}.

## 1·Introduction

\label{sec:intro}

Large language model (LLM) based text-to-speech (TTS) systems can generate natural-sounding speech with zero-shot voice cloning and are widely used for monologue applications like video dubbing.

These systems typically follow one of two modeling paradigms: an autoregressive, decoder-only transformer that predicts speech tokens[^Guo2024Fireredtts], [^Guo2025Fireredtts-1s], [^Du2024Cosyvoice], [^Du2024Cosyvoice], [^Du2025Cosyvoice], [^Deng2025Indextts], [^Wang2025Spark-TTS], or a non-autoregressive flow-matching model that produces mel-spectrograms directly from text[^Chen2024F5-TTS], [^Eskimez2024E2].

While these monologue TTS systems can be adapted to dialogue generation by segmenting dialogue text and synthesizing each fragment independently[^Huang2025Step-Audio], [^Huang2023AudioGPT], [^Xiao2025PodAgent], this strategy ignores preceding text and speech context, leading to a loss of conversational coherence.

Recent works have extended TTS system to two-speaker dialogue generation, which can be grouped into three categories based on how text and speech are organized across turns: (1) splitting the dialogue text into two parallel channels and synthesizing a single mixed speech track containing both voices, which can naturally handles overlapping speech and generate interjections effects[^Zhang2024CoVoMix], [^Zhang2025CoVoMix2]; (2) concatenating the dialogue text in chronological order with each utterance prefixed by a speaker label, which likewise produces a mixed speech track[^Ju2025MoonCast], [^Darefsky2024Parakeet], [^Zhu2025ZipVoice-Dialog], [^Team2025Text], [^Peng2025VibeVoice]; and (3) interleaving the text and speech of each utterance[^Schalkwyk2025Crossing].

Approaches (1) and (2) require the complete dialogue text before synthesis and yield a single inseparable mixed speech, limiting their suitability for interactive scenarios such as chat, whereas (3) supports flexible sentence-by-sentence generation, suitable for both interactive chat and podcast production.

In this work, we present FireRedTTS‑2, a long‑form, streaming TTS system for multi‑speaker dialogue and podcast generation that delivers stable, natural speech, reliable speaker switching, and context‑aware prosody.

A new streaming 12.5Hz speech tokenizer accelerates training and inference, lengthens the effective dialogue context, encodes richer semantics to stabilize text‑to‑token modeling and supports high-fidelity streaming generation for real-time applications.

We adopt an interleaved text–speech format by concatenating speaker‑labeled text with speech tokens in chronological order, and model it with a dual‑transformer architecture: a large decoder‑only network predicts tokens at the first layer, while a smaller network refines the subsequent layers.

Experimental results show that FireRedTTS‑2 integrates seamlessly with chat frameworks and, with minimal fine‑tuning, produces emotionally expressive speech guided by implicit context.

In podcast generation, it surpasses the state of the art systems including MoonCast[^Ju2025MoonCast], ZipVoice-Dialogue[^Zhu2025ZipVoice-Dialog], and MOSS-TTSD[^Team2025Text] in objective intelligibility, speaker‑turn reliability, and perceived naturalness, while maintaining prosody consistent with long‑range context.
