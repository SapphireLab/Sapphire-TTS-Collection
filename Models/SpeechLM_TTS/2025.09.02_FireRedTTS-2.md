# FireRedTTS-2: Towards Long Conversational Speech Generation for Podcast and Chatbot

<details>
<summary>基本信息</summary>

- 标题: "FireRedTTS-2: Towards Long Conversational Speech Generation for Podcast and Chatbot."
- 作者:
  - 01 Kun Xie
  - 02 Feiyu Shen
  - 03 Junjie Li
  - 04 Fenglong Xie
  - 05 Xu Tang
  - 06 Yao Hu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.02020v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2509.02020v1](PDF/2025.09.02_2509.02020v1_FireRedTTS-2__Towards_Long_Conversational_Speech_Generation_for_Podcast_and_Chatbot.pdf)
  - [ArXiv:2509.02020v2](PDF/2025.09.04_2509.02020v2_FireRedTTS-2__Towards_Long_Conversational_Speech_Generation_for_Podcast_and_Chatbot.pdf)
  - [Publication] #TODO

</details>

## Abstract

Current dialogue generation approaches typically require the complete dialogue text before synthesis and produce a single, inseparable speech containing all voices, making them unsuitable for interactive chat; moreover, they suffer from unstable synthesis, inaccurate speaker transitions, and incoherent prosody.

In this work, we present FireRedTTS‑2, a long-form streaming TTS system for multi-speaker dialogue generation, delivering stable, natural speech with reliable speaker switching and context-aware prosody.
A new 12.5Hz streaming speech tokenizer accelerates training and inference, extends maximum dialogue length, encodes richer semantics to stabilize text-to-token modeling and supports high-fidelity streaming generation for real-time applications.
We adopt a text–speech interleaved format, concatenating speaker-labeled text with aligned speech tokens in chronological order, and model it with a dual-transformer: a large decoder-only transformer predicts tokens at the first layer, and a smaller one completes subsequent layers.
Experimental results show that FireRedTTS‑2 integrates seamlessly with chat frameworks and, with minimal fine-tuning, produces emotionally expressive speech guided by implicit contextual cues.
In podcast generation, it surpasses existing systems including MoonCast, Zipvoice-Dialogue, and MOSS-TTSD in objective intelligibility, speaker-turn reliability, and perceived naturalness with context-consistent prosody.
Our demos are available at \url{https://fireredteam.github.io/demos/firered_tts_2}.

## 1·Introduction

\label{sec:intro}

Large language model (LLM) based text-to-speech (TTS) systems can generate natural-sounding speech with zero-shot voice cloning and are widely used for monologue applications like video dubbing.

These systems typically follow one of two modeling paradigms: an autoregressive, decoder-only transformer that predicts speech tokens[^Guo2024Fireredtts], [^Guo2025Fireredtts-1s], [^Du2024Cosyvoice], [^Du2024Cosyvoice], [^Du2025Cosyvoice], [^Deng2025Indextts], [^Wang2025Spark-TTS], or a non-autoregressive flow-matching model that produces mel-spectrograms directly from text[^Chen2024F5-TTS], [^Eskimez2024E2].

While these monologue TTS systems can be adapted to dialogue generation by segmenting dialogue text and synthesizing each fragment independently[^Huang2025Step-Audio], [^Huang2023AudioGPT], [^Xiao2025PodAgent], this strategy ignores preceding text and speech context, leading to a loss of conversational coherence.

Recent works have extended TTS system to two-speaker dialogue generation, which can be grouped into three categories based on how text and speech are organized across turns: (1) splitting the dialogue text into two parallel channels and synthesizing a single mixed speech track containing both voices, which can naturally handles overlapping speech and generate interjections effects[^Zhang2024CoVoMix], [^Zhang2025CoVoMix2]; (2) concatenating the dialogue text in chronological order with each utterance prefixed by a speaker label, which likewise produces a mixed speech track[^Ju2025MoonCast], [^Darefsky2024Parakeet], [^Zhu2025ZipVoice-Dialog], [^Team2025Text], [^Peng2025VibeVoice]; and (3) interleaving the text and speech of each utterance[^Schalkwyk2025Crossing].

Approaches (1) and (2) require the complete dialogue text before synthesis and yield a single inseparable mixed speech, limiting their suitability for interactive scenarios such as chat, whereas (3) supports flexible sentence-by-sentence generation, suitable for both interactive chat and podcast production.

In this work, we present FireRedTTS‑2, a long‑form, streaming TTS system for multi‑speaker dialogue and podcast generation that delivers stable, natural speech, reliable speaker switching, and context‑aware prosody.

A new streaming 12.5Hz speech tokenizer accelerates training and inference, lengthens the effective dialogue context, encodes richer semantics to stabilize text‑to‑token modeling and supports high-fidelity streaming generation for real-time applications.

We adopt an interleaved text–speech format by concatenating speaker‑labeled text with speech tokens in chronological order, and model it with a dual‑transformer architecture: a large decoder‑only network predicts tokens at the first layer, while a smaller network refines the subsequent layers.

Experimental results show that FireRedTTS‑2 integrates seamlessly with chat frameworks and, with minimal fine‑tuning, produces emotionally expressive speech guided by implicit context.

In podcast generation, it surpasses the state of the art systems including MoonCast[^Ju2025MoonCast], ZipVoice-Dialogue[^Zhu2025ZipVoice-Dialog], and MOSS-TTSD[^Team2025Text] in objective intelligibility, speaker‑turn reliability, and perceived naturalness, while maintaining prosody consistent with long‑range context.

## 2·FireRedTTS-2

As shown in Figure [img:tts_model_framework](#img:tts_model_framework), FireRedTTS-2 consists of a newly developed speech tokenizer and a text-to-speech model with perception to previous text and speech context.

![](image/tts_model_framework.png)

<a id="img:tts_model_framework">An overview of FireRedTTS-2, including: (a) a new speech tokenizer with a 12.5Hz frame rate and enhanced semantic information, and (b) a text-to-speech model using a dual-transformer architecture with interleaved text–speech input, enabling sentence-by-sentence generation and contextually coherent prosody.</a>

### Speech Tokenizer

We design our speech tokenizer to enhance dialogue modeling, with a focus on long, multi-speaker speech sequence.

To make such sequences tractable, we reduce the frame rate to 12.5Hz, half that of most open-source tokenizers[^Du2024Cosyvoice], [^Du2024Cosyvoice], [^Du2025Cosyvoice], [^Ye2025Codec], [^Ye2025Llasa], [^Zhang2023Speechtokenizer].

We further employ semantic injection and supervision to simplify text-to-token modeling, which has been shown to improve synthesis stability[^Team2025Text], [^Ye2025Codec], [^Ye2025Llasa], [^Zhang2023Speechtokenizer], [^D{\'e}fossez2024Moshi].

In addition, the tokenizer supports high-fidelity streaming generation for real-time applications.

As illustrated in Figure [img:tts_model_framework](#img:tts_model_framework)(a), our speech tokenizer employs a pretrained Whisper[^Radford2023Robust] encoder to extract semantic features from the 16kHz input speech.

These semantic features are encoded by an adapter and then concatenated with acoustic features from a trainable acoustic encoder structurally identical to the Whisper encoder.

The combined features undergo 4 times downsampling from 50Hz to 12.5Hz and are discretized by a residual vector quantizer (RVQ)[^Zeghidour2021Soundstream] with 16 layers, each containing 2048 code entries.

The quantized features are upsampled to 50Hz and fed to a semantic decoder to predict the original semantic features derived from the pretrained Whisper encoder.

The same upsampled features are also used by a Vocos[^Siuzdak2023Vocos]-based acoustic decoder to reconstruct the waveform.

Depending on the reception fields of its inner convolution and attention layers, the acoustic decoder can be implemented as either streaming or non-streaming.

To balance generalization capability and speech quality, we train our speech tokenizer in two stages similar to[^Team2025Text].

First, the acoustic decoder is implemented as non-streaming and optimized to predict 16kHz speech.

We use approximately 500k hours of speech data and train the model for 320k steps on 32 H800 GPUs, with each sample randomly cropped to 6 seconds.

For the final 35k steps, we incorporate the perceptual loss[^Ye2025Llasa], [^Parker2024Scaling] to further improve semantic details.

In the second stage, we freeze the encoding part and replace the acoustic decoder with a fully streaming variant that predicts 24kHz speech.

We continue to train the speech tokenizer on a subset of 60k hours high-fidelity speech data for 80k steps.

### Text-to-Speech Model

Building on the new speech tokenizer, we employ a dual-transformer architecture akin to [^Schalkwyk2025Crossing], [^D{\'e}fossez2024Moshi] that operates on a text–speech interleaved sequence, enabling flexible sentence-by-sentence generation and reducing first-packet latency.

As illustrated in Figure [img:tts_model_framework](#img:tts_model_framework)(b), each dialogue text is prefixed with a speaker tag (e.g., "[S1]") and concatenated with its corresponding speech tokens; these segments are then joined in temporal order to form sequences such as "[S1]<text><audio>[S2]<text><audio>[S3]<text><audio>...".

Existing approaches[^Darefsky2024Parakeet], [^Team2025Text] model multi-layer speech tokens using the delay-pattern[^Copet2023Simple]: for $N$ token layers, the $i^\text{th}$ layer is shifted $i-1$ timesteps to the right, and $N$ prediction heads predict these shifted layers in parallel.

This design has two main drawbacks: first, at each timestep the model has only partial access to the speech tokens from previous steps due to the rightward shifts, weakening contextual conditioning; second, obtaining the complete set of $N$ layer tokens for the first timestep requires $N$ autoregressive steps, resulting in high latency.

To overcome these issues, we adopt a dual-transformer architecture comprising a backbone transformer that processes the text–speech interleaved sequence and predicts the first-layer tokens, and a smaller decoder transformer that generates remaining token layers.

Both transformers are based on Qwen2.5[^Ahmed2025Qwen] structure.

At each timestep, the decoder consumes both the predicted first layer token and the backbone’s hidden states, which provide complete contextual information.

Comparing with the delay-pattern, it requires one auto-regressive inference step of the backbone transformer and $N-1$ steps of the smaller decoder, reducing computation and first-packet latency.

Moreover, our speech tokenizer produces high-fidelity speech in a streaming manner without requiring separate token-to-speech modules, simplifying the overall system.

The text-to-speech model is optimized with the following loss function:

$$\begin{aligned}
\mathcal{L}_{loss}=2*((1-\lambda_{decoder})\mathcal{L}_{backbone}+\lambda_{decoder}\mathcal{L}_{decoder})+\lambda_{text}\mathcal{L}_{text}
\end{aligned}$$

Here, $\mathcal{L}_{backbone}$ and $\mathcal{L}_{decoder}$ denote the cross-entropy loss of the backbone and decoder transformer respectively.

To improve training efficiency, we optimize the decoder transformer only on 1/8 of the speech segments in the interleaved sequence.

Additionally, we incorporate a cross-entropy loss for the textual part ($\mathcal{L}_{text}$) to stabilize training.

In our experiments, we set $\lambda_{text} = 0.01$ and $\lambda_{decoder} = 0.6$.

To enable the model with dialogue generation capability, we adopt a three-stage curriculum training process utilized in [^Zhang2024CoVoMix], [^Team2025Text], comprising pretraining, post-training, and supervised fine-tuning (SFT).

The pretraining stage leverages 1.1M hours of monologue speech data and trains the model for 2 epochs to build foundational text-to-speech ability.

Subsequently, we post-train FireRedTTS-2 for 5 epochs on 300k hours of multi-speaker dialogue data, with each dialogue containing 2 to 5 speakers, to enable robust multi-speaker dialogue generation.

Finally, the SFT stage is applied to tailor the model to specific voices with minimal data.

## 3·Downstream Applications

FireRedTTS-2 excels at both monologue and dialogue speech generation.

For monologues, it offers competitive zero-shot voice cloning suitable for tasks like video dubbing.

For dialogues generation, it surpasses monologue TTS systems due to its perception of text and speech context, producing speech with coherent prosody.

Compared with existing dialogue TTS systems, it supports sentence-by-sentence generation, enabling both interactive chat and offline podcast production.

Across both modes, FireRedTTS-2 can be tailored to specific application requirements with minimal data, demonstrating strong flexibility.

### Voice Cloning

Our speech tokenizer captures both semantic and acoustic information, enabling fine-grained acoustic modeling.

Paired with large-scale pretraining on monologue speech, it allows FireRedTTS-2 to deliver robust zero-shot voice cloning.

Given a speech prompt and its transcript, we concatenate the prompt transcript, the target text to be synthesized, and the discretized prompt speech tokens, then feed this sequence into the text-to-speech model to autoregressively generate new speech tokens.

The generated tokens are appended to the prompt tokens and decoded into a waveform by the speech tokenizer’s decoder, after which the portion corresponding to the original prompt is removed.

### Interactive Chat

Current interactive chat frameworks[^Huang2025Step-Audio], [^Huang2023AudioGPT] typically rely on monologue TTS systems, which lack perception of prior user queries and system responses, often resulting in inconsistent emotion and prosody.

This can be partially mitigated with explicit instructions such as emotion labels, but it requires additional fine-tuning of the text LLM and adds unnecessary complexity.

![](image/tts2_for_chat.png)

<a id="img:tts2_for_chat">Integration of FireRedTTS-2 into interactive chat scenarios.</a>

As shown in Figure [img:tts2_for_chat](#img:tts2_for_chat), FireRedTTS-2 can be seamlessly integrated into existing chat frameworks without modifying other modules.

To address the inconsistency issue, we fine-tune the post-trained FireRedTTS-2 to infer and adjust emotion and prosody from implicit contextual cues.

Specifically, we curate a 15-hour speech corpus of a distinctive female voice expressing six emotions: surprise, sadness, happiness, concern, apology, and anger.

Then we emulate conversational context by first generating text context with a text LLM and then synthesizing it into speech.

After fine-tuning, FireRedTTS-2 dynamically shifts emotion and tone in response to preceding chat history, delivering a near-human interactive experience.

### Podcast Generation

Subsequent post-training on dialogue corpus equips FireRedTTS-2 with conversational generation abilities, making it well-suited for podcast generation.

Compared with conventional segmenting approaches that utlizes monologue TTS systems, it simplifies the workflow and can synthesizes contextually coherent prosody.

Moreover, it generates dialogue speech sentence by sentence, providing greater flexibility for editing and post-processing.

![](image/tts2_for_zeroshot_podcast.png)

<a id="img:tts2_for_zeroshot_podcast">Zero-shot podcast generation of FireRedTTS-2.</a>

As shown in Figure [img:tts2_for_zeroshot_podcast](#img:tts2_for_zeroshot_podcast), FireRedTTS-2 can perform zero-shot podcast generation by placing two dialogue turns as prompt context and then generating subsequent turns one by one.

It currently supports 3 minutes dialogues with 4 speakers and can be easily scaled to longer conversations with more speakers by extending training corpus.

It can also be tailored to specific speakers with minimal data.

In this work, we collected approximately 50 hours of dialogue speech featuring a male and a female podcast host and fine-tuned the post-trained model for 15 epochs.

Once customized,  FireRedTTS-2 delivers stable synthesis, accurate speaker transitions, and contextually coherent prosody that matches the hosts’ distinctive speaking styles.
