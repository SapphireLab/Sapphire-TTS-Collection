# FireRedTTS-2: Towards Long Conversational Speech Generation for Podcast and Chatbot

<details>
<summary>基本信息</summary>

- 标题: "FireRedTTS-2: Towards Long Conversational Speech Generation for Podcast and Chatbot."
- 作者:
  - 01 Kun Xie
  - 02 Feiyu Shen
  - 03 Junjie Li
  - 04 Fenglong Xie
  - 05 Xu Tang
  - 06 Yao Hu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.02020v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2509.02020v1](PDF/2025.09.02_2509.02020v1_FireRedTTS-2__Towards_Long_Conversational_Speech_Generation_for_Podcast_and_Chatbot.pdf)
  - [ArXiv:2509.02020v2](PDF/2025.09.04_2509.02020v2_FireRedTTS-2__Towards_Long_Conversational_Speech_Generation_for_Podcast_and_Chatbot.pdf)
  - [Publication] #TODO

</details>

## Abstract

Current dialogue generation approaches typically require the complete dialogue text before synthesis and produce a single, inseparable speech containing all voices, making them unsuitable for interactive chat; moreover, they suffer from unstable synthesis, inaccurate speaker transitions, and incoherent prosody.

In this work, we present FireRedTTS‑2, a long-form streaming TTS system for multi-speaker dialogue generation, delivering stable, natural speech with reliable speaker switching and context-aware prosody.
A new 12.5Hz streaming speech tokenizer accelerates training and inference, extends maximum dialogue length, encodes richer semantics to stabilize text-to-token modeling and supports high-fidelity streaming generation for real-time applications.
We adopt a text–speech interleaved format, concatenating speaker-labeled text with aligned speech tokens in chronological order, and model it with a dual-transformer: a large decoder-only transformer predicts tokens at the first layer, and a smaller one completes subsequent layers.
Experimental results show that FireRedTTS‑2 integrates seamlessly with chat frameworks and, with minimal fine-tuning, produces emotionally expressive speech guided by implicit contextual cues.
In podcast generation, it surpasses existing systems including MoonCast, Zipvoice-Dialogue, and MOSS-TTSD in objective intelligibility, speaker-turn reliability, and perceived naturalness with context-consistent prosody.
Our demos are available at \url{https://fireredteam.github.io/demos/firered_tts_2}.

## 1·Introduction

\label{sec:intro}

Large language model (LLM) based text-to-speech (TTS) systems can generate natural-sounding speech with zero-shot voice cloning and are widely used for monologue applications like video dubbing.

These systems typically follow one of two modeling paradigms: an autoregressive, decoder-only transformer that predicts speech tokens[^Guo2024Fireredtts], [^Guo2025Fireredtts-1s], [^Du2024Cosyvoice], [^Du2024Cosyvoice], [^Du2025Cosyvoice], [^Deng2025Indextts], [^Wang2025Spark-TTS], or a non-autoregressive flow-matching model that produces mel-spectrograms directly from text[^Chen2024F5-TTS], [^Eskimez2024E2].

While these monologue TTS systems can be adapted to dialogue generation by segmenting dialogue text and synthesizing each fragment independently[^Huang2025Step-Audio], [^Huang2023AudioGPT], [^Xiao2025PodAgent], this strategy ignores preceding text and speech context, leading to a loss of conversational coherence.

Recent works have extended TTS system to two-speaker dialogue generation, which can be grouped into three categories based on how text and speech are organized across turns: (1) splitting the dialogue text into two parallel channels and synthesizing a single mixed speech track containing both voices, which can naturally handles overlapping speech and generate interjections effects[^Zhang2024CoVoMix], [^Zhang2025CoVoMix2]; (2) concatenating the dialogue text in chronological order with each utterance prefixed by a speaker label, which likewise produces a mixed speech track[^Ju2025MoonCast], [^Darefsky2024Parakeet], [^Zhu2025ZipVoice-Dialog], [^Team2025Text], [^Peng2025VibeVoice]; and (3) interleaving the text and speech of each utterance[^Schalkwyk2025Crossing].

Approaches (1) and (2) require the complete dialogue text before synthesis and yield a single inseparable mixed speech, limiting their suitability for interactive scenarios such as chat, whereas (3) supports flexible sentence-by-sentence generation, suitable for both interactive chat and podcast production.

In this work, we present FireRedTTS‑2, a long‑form, streaming TTS system for multi‑speaker dialogue and podcast generation that delivers stable, natural speech, reliable speaker switching, and context‑aware prosody.

A new streaming 12.5Hz speech tokenizer accelerates training and inference, lengthens the effective dialogue context, encodes richer semantics to stabilize text‑to‑token modeling and supports high-fidelity streaming generation for real-time applications.

We adopt an interleaved text–speech format by concatenating speaker‑labeled text with speech tokens in chronological order, and model it with a dual‑transformer architecture: a large decoder‑only network predicts tokens at the first layer, while a smaller network refines the subsequent layers.

Experimental results show that FireRedTTS‑2 integrates seamlessly with chat frameworks and, with minimal fine‑tuning, produces emotionally expressive speech guided by implicit context.

In podcast generation, it surpasses the state of the art systems including MoonCast[^Ju2025MoonCast], ZipVoice-Dialogue[^Zhu2025ZipVoice-Dialog], and MOSS-TTSD[^Team2025Text] in objective intelligibility, speaker‑turn reliability, and perceived naturalness, while maintaining prosody consistent with long‑range context.

## 2·FireRedTTS-2

As shown in Figure [img:tts_model_framework](#img:tts_model_framework), FireRedTTS-2 consists of a newly developed speech tokenizer and a text-to-speech model with perception to previous text and speech context.

![](image/tts_model_framework.png)

<a id="img:tts_model_framework">An overview of FireRedTTS-2, including: (a) a new speech tokenizer with a 12.5Hz frame rate and enhanced semantic information, and (b) a text-to-speech model using a dual-transformer architecture with interleaved text–speech input, enabling sentence-by-sentence generation and contextually coherent prosody.</a>

### Speech Tokenizer

We design our speech tokenizer to enhance dialogue modeling, with a focus on long, multi-speaker speech sequence.

To make such sequences tractable, we reduce the frame rate to 12.5Hz, half that of most open-source tokenizers[^Du2024Cosyvoice], [^Du2024Cosyvoice], [^Du2025Cosyvoice], [^Ye2025Codec], [^Ye2025Llasa], [^Zhang2023Speechtokenizer].

We further employ semantic injection and supervision to simplify text-to-token modeling, which has been shown to improve synthesis stability[^Team2025Text], [^Ye2025Codec], [^Ye2025Llasa], [^Zhang2023Speechtokenizer], [^D{\'e}fossez2024Moshi].

In addition, the tokenizer supports high-fidelity streaming generation for real-time applications.

As illustrated in Figure [img:tts_model_framework](#img:tts_model_framework)(a), our speech tokenizer employs a pretrained Whisper[^Radford2023Robust] encoder to extract semantic features from the 16kHz input speech.

These semantic features are encoded by an adapter and then concatenated with acoustic features from a trainable acoustic encoder structurally identical to the Whisper encoder.

The combined features undergo 4 times downsampling from 50Hz to 12.5Hz and are discretized by a residual vector quantizer (RVQ)[^Zeghidour2021Soundstream] with 16 layers, each containing 2048 code entries.

The quantized features are upsampled to 50Hz and fed to a semantic decoder to predict the original semantic features derived from the pretrained Whisper encoder.

The same upsampled features are also used by a Vocos[^Siuzdak2023Vocos]-based acoustic decoder to reconstruct the waveform.

Depending on the reception fields of its inner convolution and attention layers, the acoustic decoder can be implemented as either streaming or non-streaming.

To balance generalization capability and speech quality, we train our speech tokenizer in two stages similar to[^Team2025Text].

First, the acoustic decoder is implemented as non-streaming and optimized to predict 16kHz speech.

We use approximately 500k hours of speech data and train the model for 320k steps on 32 H800 GPUs, with each sample randomly cropped to 6 seconds.

For the final 35k steps, we incorporate the perceptual loss[^Ye2025Llasa], [^Parker2024Scaling] to further improve semantic details.

In the second stage, we freeze the encoding part and replace the acoustic decoder with a fully streaming variant that predicts 24kHz speech.

We continue to train the speech tokenizer on a subset of 60k hours high-fidelity speech data for 80k steps.

### Text-to-Speech Model

Building on the new speech tokenizer, we employ a dual-transformer architecture akin to [^Schalkwyk2025Crossing], [^D{\'e}fossez2024Moshi] that operates on a text–speech interleaved sequence, enabling flexible sentence-by-sentence generation and reducing first-packet latency.

As illustrated in Figure [img:tts_model_framework](#img:tts_model_framework)(b), each dialogue text is prefixed with a speaker tag (e.g., "[S1]") and concatenated with its corresponding speech tokens; these segments are then joined in temporal order to form sequences such as "[S1]<text><audio>[S2]<text><audio>[S3]<text><audio>...".

Existing approaches[^Darefsky2024Parakeet], [^Team2025Text] model multi-layer speech tokens using the delay-pattern[^Copet2023Simple]: for $N$ token layers, the $i^\text{th}$ layer is shifted $i-1$ timesteps to the right, and $N$ prediction heads predict these shifted layers in parallel.

This design has two main drawbacks: first, at each timestep the model has only partial access to the speech tokens from previous steps due to the rightward shifts, weakening contextual conditioning; second, obtaining the complete set of $N$ layer tokens for the first timestep requires $N$ autoregressive steps, resulting in high latency.

To overcome these issues, we adopt a dual-transformer architecture comprising a backbone transformer that processes the text–speech interleaved sequence and predicts the first-layer tokens, and a smaller decoder transformer that generates remaining token layers.

Both transformers are based on Qwen2.5[^Ahmed2025Qwen] structure.

At each timestep, the decoder consumes both the predicted first layer token and the backbone’s hidden states, which provide complete contextual information.

Comparing with the delay-pattern, it requires one auto-regressive inference step of the backbone transformer and $N-1$ steps of the smaller decoder, reducing computation and first-packet latency.

Moreover, our speech tokenizer produces high-fidelity speech in a streaming manner without requiring separate token-to-speech modules, simplifying the overall system.

The text-to-speech model is optimized with the following loss function:

$$\begin{aligned}
\mathcal{L}_{loss}=2*((1-\lambda_{decoder})\mathcal{L}_{backbone}+\lambda_{decoder}\mathcal{L}_{decoder})+\lambda_{text}\mathcal{L}_{text}
\end{aligned}$$

Here, $\mathcal{L}_{backbone}$ and $\mathcal{L}_{decoder}$ denote the cross-entropy loss of the backbone and decoder transformer respectively.

To improve training efficiency, we optimize the decoder transformer only on 1/8 of the speech segments in the interleaved sequence.

Additionally, we incorporate a cross-entropy loss for the textual part ($\mathcal{L}_{text}$) to stabilize training.

In our experiments, we set $\lambda_{text} = 0.01$ and $\lambda_{decoder} = 0.6$.

To enable the model with dialogue generation capability, we adopt a three-stage curriculum training process utilized in [^Zhang2024CoVoMix], [^Team2025Text], comprising pretraining, post-training, and supervised fine-tuning (SFT).

The pretraining stage leverages 1.1M hours of monologue speech data and trains the model for 2 epochs to build foundational text-to-speech ability.

Subsequently, we post-train FireRedTTS-2 for 5 epochs on 300k hours of multi-speaker dialogue data, with each dialogue containing 2 to 5 speakers, to enable robust multi-speaker dialogue generation.

Finally, the SFT stage is applied to tailor the model to specific voices with minimal data.
