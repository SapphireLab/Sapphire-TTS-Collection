# KALL-E

<details>
<summary>基本信息</summary>

- 标题: "Autoregressive Speech Synthesis with Next-Distribution Prediction"
- 作者:
  - 01 Xinfa Zhu (NPU@ASLP)
  - 02 Wenjie Tian (NPU@ASLP)
  - 03 Lei Xie (NPU@ASLP)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.16846)
  - [Publication]
  - [Github]
  - [Demo](https://zxf-icpc.github.io/kalle/)
- 文件:
  - [ArXiv](../SpeechLM/_PDF/2412.16846v1__KALL-E__Autoregressive_Speech_Synthesis_with_Next-Distribution_Prediction.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

We introduce ***KALL-E***, a novel autoregressive (AR) language modeling approach with next-distribution prediction for text-to-speech (TTS) synthesis.
Unlike existing methods, ***KALL-E*** directly models and predicts the continuous speech distribution conditioned on text without relying on VAE- or diffusion-based components.
Specifically, we use WaveVAE to extract continuous speech distributions from waveforms instead of using discrete speech tokens.
A single AR language model predicts these continuous speech distributions from text, with a Kullback-Leibler divergence loss as the constraint.
Experimental results show that ***KALL-E*** outperforms open-source implementations of YourTTS, VALL-E, NaturalSpeech 2, and CosyVoice in terms of naturalness and speaker similarity in zero-shot TTS scenarios.
Moreover, ***KALL-E*** demonstrates exceptional zero-shot capabilities in emotion and accent cloning.
Importantly, ***KALL-E*** presents a more straightforward and effective paradigm for using continuous speech representations in TTS.
Audio samples are available at: [this https URL](https://zxf-icpc.github.io/kalle/).

</td><td>

我们介绍 ***KALL-E***, 一种新颖的自回归语言建模方法, 通过下一分布预测来实现文本转语音合成.
和现有方法不同, ***KALL-E*** 直接建模并预测基于文本条件化的连续语音分布, 而不依赖于变分自编码器或扩散模型组件.
具体来说, 我们使用 WaveVAE 来从波形中提取连续语音分布而不是使用离散语音 Token.
单个自回归语言模型从文本预测这些连续语音分布, 以 Kullback-Leibler 散度损失作为约束.

实验结果表明 ***KALL-E*** 在零样本文本转语音场景下的自然度和说话人相似性优于 YourTTS, VALL-E, NaturalSpeech2, CosyVoice 的开源实现.
此外, ***KALL-E*** 在情感和口音克隆方面也展现出了卓越的零样本能力.
重要的是, ***KALL-E*** 提供了一个更加直接和有效的在文本转语音中使用连续语音表示的范式.
音频示例可在[此链接](https://zxf-icpc.github.io/kalle/) 获得.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

The past decade has seen remarkable advancements in speech synthesis driven by the development of neural networks ([^01], [^02]).
Early text-to-speech (TTS) systems employ cascaded pipelines, combining acoustic models and vocoders, with Mel spectrograms serving as intermediate representations (**Tacotron**[^03], **FastSpeech**[^04], **DurIAN**[^05], **FastSpeech2**[^06], **Parallel Tacotron**[^07]).
Later innovations have shifted towards end-to-end TTS modeling, enabling high-quality speech synthesis (**VITS**[^08], **Glow-WaveGAN**[^09], [^10], **VITS2**[^11]).
However, due to the inherent one-to-many mapping nature of TTS, these systems continue to suffer from over-smoothing issues ([^12], **GenerSpeech**[^13]).
Powered by large language models (LLMs) (**VALL-E**[^14], **SPEAR-TTS**[^15], **UniAudio**[^16], [^17], **Seed-TTS**[^18], **CosyVoice**[^19], [^20], [^21]), diffusion models (**NaturalSpeech2**[^22], **NaturalSpeech3**[^23], **FlashSpeech**[^24], **SimpleSpeech**[^25], **E2 TTS**[^26], **E3 TTS**[^27], **MaskGCT**[^28]) and large-scale corpora (**LibriLight**[^29], **wenetSpeech4TTS**[^30], **Emilia**[^31]), current state-of-the-art (SOTA) TTS systems have achieved unprecedented levels of naturalness and diversity, including capabilities for zero-shot voice cloning.

</td><td>

</td></tr>
<tr><td>

Typical LLM-based TTS frameworks (**VALL-E**[^14], **UniAudio**[^16]) rely on speech tokenizers (**EnCodec**[^32], **SoundStream**[^33]) to quantize continuous speech waveforms into discrete tokens, which are then modeled autoregressively.
While significant efforts have been made to improve speech tokenizers (**SpeechTokenizer**[^34], **Single-Codec**[^35], **XCodec**[^36], **WavTokenizer**[^37]), a fundamental trade-off persists between bitrate and the preservation of speech components (**NaturalSpeech2**[^22], **MELLE**[^38]).
Some tokenizers (**EnCodec**[^32], **SoundStream**[^33], **SpeechTokenizer**[^34]) employing multiple discrete tokens per speech frame capture richer acoustic information but significantly increase sequence length, making language modeling challenging.
Conversely, tokenizers ([^39], **CosyVoice**[^19]) producing low-bitrate sequences simplify language modeling but result in lossy representations lacking acoustic detail.
Unlike text, speech waveforms are continuous in nature, which inherently makes it hard to achieve an ideal speech tokenizer that retains all acoustic nuances at a limited bit rate.

</td><td>

</td></tr>
<tr><td>

Recent works (**Spectron**[^40], **MELLE**[^38], [^41]) have explored continuous speech representations within AR language modeling frameworks to overcome the limitations of speech tokenization.
Continuous representations are considered nearly lossless carriers of speech information.
However, as highlighted in **MELLE**[^38], the key challenges of using continuous speech representations in AR language models lie in the training objective and sampling mechanism.
**MELLE** addresses these challenges by introducing a VAE-like latent sampling module into an AR language model to predict Mel spectrograms, while other works ([^42], [^43]) leverage diffusion-based heads for continuous representation prediction in visual and multimodal generation tasks.

</td><td>

</td></tr>
<tr><td>

In this work, we propose ***KALL-E***, a novel AR speech synthesis framework with next-distribution prediction.
***KALL-E*** first extracts continuous speech distributions via WaveVAE and predicts them directly through an AR language model, bypassing the need for VAE or diffusion heads and eliminating the inherent dilemma associated with speech tokenizers.
To tackle the challenge of the training objective, we replace the traditional cross-entropy loss with a Kullback-Leibler (KL) divergence loss for next-distribution prediction, supplemented by a binary cross-entropy (BCE) loss for stop prediction.
For the sampling mechanism, we employ a straightforward reparameterization technique to sample from the predicted speech distributions, effectively addressing the challenge of the sampling mechanism.

</td><td>

</td></tr>
<tr><td>

We evaluate ***KALL-E*** on the **LibriTTS**[^44] corpus and compare it with open-source implementations of several popular zero-shot TTS systems, including **YourTTS**[^45], **VALL-E**[^14], **NaturalSpeech2**[^22], and **CosyVoice**[^19].
Following established benchmarks, we use the **LibriTTS test-clean set** for zero-shot TTS evaluation, the **ESD**[^46] corpus for zero-shot emotion cloning, and the **VCTK**[^47] corpus for accent cloning.
Experimental results demonstrate that ***KALL-E*** achieves competitive performance with these TTS systems on objective metrics while surpassing them on subjective metrics.
Moreover, ***KALL-E*** exhibits exceptional capabilities in zero-shot emotion and accent cloning despite training on a modest 500-hour dataset.

</td><td>

</td></tr></table>

## ~~2·Related Works: 相关工作~~

## 3·Methodology: 方法

### Problem Formulation: Next-Distribution Prediction<br>问题形式化: 下一分布预测

<table><tr><td width="50%">

***KALL-E*** regards zero-shot TTS as a conditional language modeling task achieved through next-distribution prediction.
As illustrated in [Figure.01](#Fig.01), ***KALL-E*** consists of a WaveVAE model and an AR language model.
The WaveVAE model extracts continuous speech distributions from speech waveforms.
Using well-established continuous distributions as the training targets, ***KALL-E*** directly predicts these distributions via the AR language model.
Finally, the predicted speech distributions are converted back into waveforms by the WaveVAE decoder.

</td><td>

***KALL-E*** 将零样本 TTS 视为条件化语言建模任务, 通过下一分布预测实现.
如[图 01](#Fig.01) 所示, ***KALL-E*** 由 WaveVAE 模型和 AR 语言模型组成.

WaveVAE 模型从语音波形中提取连续语音分布.
使用良好建立的连续分布作为训练目标, ***KALL-E*** 直接通过自回归语言模型预测这些分布.
最后, 预测的语音分布通过 WaveVAE 解码器转换回波形.

</td></tr>
<tr><td>

Given a `<text, speech>` pair, the text tokenizer encodes the input text into a sequence of text tokens, $x=\{x_0, x_1, ..., x_L\}$, where $L$ is the number of text tokens.
The WaveVAE encoder extracts continuous speech distributions $z=\{z_0, z_1, ..., z_T\}$ from the corresponding speech waveform, where $T$ denotes the number of frames in continuous speech distributions, and $z_i$ is sampled from $\mathcal{N}(Z_\mu^i, Z_\sigma^i)$.
***KALL-E*** is trained to predict $z$ from $x$ autoregressively.
Specifically, at each AR step $t$, ***KALL-E*** predicts the next speech distribution $z_t$ conditioned on the text prompt $x$ and the previously generated speech distributions $z_{<t}$.
This can be formulated as:

$$
p(z|x;\theta) =\prod_{t=0}^{T}p(z_{t}|z_{<t},x;\theta),
$$

where $z_{<t}$ represents the previously predicted speech distributions $\{z_0, z_1, ..., z_{t-1}\}$ and $\theta$ represents the parameters of the AR language model.

</td><td>

给定 `<文本, 语音>` 对, 文本 Tokenizer 将输入文本编码为文本 Token 序列 $x=\{x_0, x_1, ..., x_L\}$, 其中 $L$ 为文本 Token 数量.
WaveVAE 编码器从对应的语音波形中提取连续的语音分布 $z=\{z_0, z_1, ..., z_T\}$, 其中 $T$ 表示连续语音分布的帧数, 且 $z_i$ 由 $\mathcal{N}(Z_\mu^i, Z_\sigma^i)$ 采样得到.

***KALL-E*** 被训练用于从 $x$ 自回归地预测 $z$.
具体来说, 在每个自回归步骤 $t$ 中, ***KALL-E*** 以文本提示 $x$ 和之前生成的语音分布 $z_{<t}$ 为条件预测下一个语音分布 $z_t$.

这可以被形式化为:

$$
p(z|x;\theta) =\prod_{t=0}^{T}p(z_{t}|z_{<t},x;\theta),
$$

- $z_{<t}$ 表示之前预测的语音分布 $\{z_0, z_1, ..., z_{t-1}\}$
- $\theta$ 表示 AR 语言模型的参数

</td></tr></table>

### WaveVAE

<table><tr><td width="50%">

To enable the AR language model to predict continuous speech distributions, we first employ WaveVAE to learn these distributions in an unsupervised manner.
Inspired by the **Glow-WaveGAN series**(**Glow-WaveGAN**[^09], **Glow-WaveGAN2**[^48], **Glow-WaveGAN3**[^49]), WaveVAE is a **Variational Auto-Encoder (VAE)**[^50] and consists of an encoder and a decoder, where the encoder maps the inputs $w$ into latent representations $z$, and the decoder reconstructs $w$ from $z$.
The process is formulated as follows:

$$
z = Enc(w) \sim q(z|w),
$$

$$
\hat{w} = Dec(z) \sim p(w|z),
$$

where $w$ means the input waveform, and $q(z|w)$ represents the latent distribution of speech, which is used to reconstruct waveform $\hat{w}$ from $p(w|z)$ via the decoder.

</td><td>

为了实现自回归语言模型预测连续语音分布, 我们首先采用 WaveVAE 以无监督的方式学习这些分布.
受到 **Glow-WaveGAN 系列** (**Glow-WaveGAN**[^09], **Glow-WaveGAN2**[^48], **Glow-WaveGAN3**[^49]), WaveVAE 是一个**变分自编码器**[^50] 由编码器和解码器组成, 其中编码器将输入 $w$ 映射到潜在表示 $z$, 解码器从 $z$ 重构 $w$.

过程可以形式化为:

$$
z = Enc(w) \sim q(z|w),
$$

$$
\hat{w} = Dec(z) \sim p(w|z),
$$

- $w$ 表示输入波形;
- $q(z|w)$ 表示语音的潜在分布, 用于通过解码器重构波形 来自 $p(w|z)$ 的 $\hat{w}$;

</td></tr>
<tr><td>

As for the detailed architecture of the proposed WaveVAE, the encoder consists of a stack of down-sampling dilated convolution layers with residual blocks, which effectively capture abstract features in the speech waveform.
After encoding, the produced mean and variance are interpreted as the parameters of the learned latent distribution $q(z|w) = \mathcal{N}(Z_\mu, Z_\sigma)$.
The decoder mirrors the encoder's architecture but uses transposed convolution layers with residual blocks to up-sample the latent representation $z$ back into the waveform $\hat{w}$.
Additionally, we integrate advanced techniques, such as the Snake activation function from **BigVGAN**[^51], to enhance the WaveVAE decoder's performance.

</td><td>

在所提出的 WaveVAE 的详细结构中, 编码器由下采样膨胀卷积层和残差块堆叠而成, 有效捕获语音波形中的抽象特征.
编码完成后, 所产生的均值和方差被解释为学习到的潜在分布的参数 $q(z|w) = \mathcal{N}(Z_\mu, Z_\sigma)$.
解码器镜像了编码器的架构, 但是用转置卷积层和残差块来上采样潜在表示 $z$ 回到波形 $\hat{w}$.
此外, 我们集成了一些高级技术, 如 **BigVGAN**[^51] 中的蛇形激活函数, 以增强 WaveVAE 解码器的性能.

</td></tr>
<tr><td>

To train WaveVAE, we use four loss functions:
(1) a Kullback-Leibler divergence loss $\mathcal{L}_{KL}$ between latent speech distribution $z$ and prior Gaussian distribution $\mathcal{N}(0, 1)$;
(2) a reconstruction loss $\mathcal{L}_{recon}$ between predicted mel-spectrogram and ground-truth mel-spectrogram;
(3) a discriminator loss $\mathcal{L}_{disc}$ including the multi-period discriminator loss[^52] and multi-resolution discriminator loss[^53];
(4) a feature matching loss $\mathcal{L}_{fm}$ between the feature map extracted from intermediate layers in each discriminator[^54].

These losses work collaboratively to optimize WaveVAE, and the overall loss training objective is defined as:

$$
\mathcal{L}_{waave} = \lambda_{kl} \mathcal{L}_{KL} + \lambda_{recon} \mathcal{L}_{recon} + \lambda_{disc} \mathcal{L}_{disc} + \lambda_{fm} \mathcal{L}_{fm},
$$

where $\lambda_{kl}$, $\lambda_{recon}$, $\lambda_{disc}$, $\lambda_{fm}$ are the respective weighting factors for each loss component.

</td><td>

为了训练 WaveVAE, 我们使用了四个损失函数:
1. 潜在语音分布 $z$ 和先验高斯分布 $\mathcal{N}(0, 1)$ 之间的 Kullback-Leibler 散度损失 $\mathcal{L}_{KL}$;
2. 预测的梅尔频谱和真实频谱之间的重构损失;
3. 包含多周期判别器损失[^52] 和多分辨率判别器损失[^53] 的判别器损失 $\mathcal{L}_{disc}$;
4. 每个判别器中间层提取的特征图之间的特征匹配损失 $\mathcal{L}_{fm}$.

这些损失共同用于优化 WaveVAE, 总的训练损失目标定义为:

$$
\mathcal{L}_{waave} = \lambda_{kl} \mathcal{L}_{KL} + \lambda_{recon} \mathcal{L}_{recon} + \lambda_{disc} \mathcal{L}_{disc} + \lambda_{fm} \mathcal{L}_{fm},
$$

</td></tr></table>

### Autoregressive Language Model: 自回归语言模型

<table><tr><td width="50%">

With the assistance of WaveVAE, ***KALL-E*** employs a causal transformer decoder as the language model to autoregressively predict the continuous speech distribution.
Specifically, input text tokens $x$, augmented with an `<EOS>` token, are first converted into embeddings by the text embedding layer.
Simultaneously, a linear layer projects the sampled speech distribution $z$ into the dimension of the language model.
The language model, consisting of blocks of multi-head self-attention and feed-forward layers, takes the concatenation of text and speech embeddings as input to model the dependency between semantic and acoustic information.

</td><td>

有 WaveVAE 的帮助, ***KALL-E*** 采用因果 Transformer 解码器作为语言模型, 以自回归方式预测连续语音分布.
具体地, 输入文本 Token $x$, 添加 `<EOS>` Token 后首先通过文本嵌入层转换为嵌入.
同时, 线性层将采样的语音分布 $z$ 映射到语言模型的维度.
语言模型由多头自注意力和前馈层的块组成, 将文本和语音嵌入的拼接作为输入, 用于建模语义和声学信息之间的依赖.

</td></tr>
<tr><td>

At each time step $t$, the output of the language model, $o_t$, is subsequently processed by a linear layer to predict the mean $\hat{Z}_{\mu}$ and variance $\hat{Z}_{\sigma}$ of target speech distribution.
These parameters are then used to sample the predicted speech distribution for the subsequent AR step.

</td><td>

在每个时间步 $t$, 语言模型的输出 $o_t$, 之后被线性层处理用于预测目标语音分布的均值 $\hat{Z}_{\mu}$ 和方差 $\hat{Z}_{\sigma}$.
这些参数用于采样后续自回归步骤的预测语音分布.

</td></tr>
<tr><td>

The training objective for the AR language model consists of two components: (1) a Kullback-Leibler divergence loss $\mathcal{L}_{KL}$ between predicted and ground-truth speech distributions; (2) a binary cross-entropy loss $\mathcal{L}_{stop}$ for stop prediction.
Following the design of **Tacotron**[^03] and other AR TTS models (**TransformerTTS**[^55], **SpeechT5**[^56]), we employ a linear layer to project the output of the LM to logits, which are then used to calculate the BCE loss for stop prediction.
The overall training objective for the AR language model is formulated as follows:

$$
\mathcal{L}_{LM} = \mathcal{L}_{KL} + \lambda_{stop} \mathcal{L}_{stop},
$$

where $\lambda_{stop}$ is a hyperparameter that balances the stop prediction loss with the KL divergence loss.

</td><td>

自回归语言模型的训练目标由两部分组成:
1. 预测和真实语音分布之间的 Kullback-Leibler 散度损失 $\mathcal{L}_{KL}$;
2. 停止预测的二元交叉熵损失 $\mathcal{L}_{stop}$.

遵循 **Tacotron**[^03] 和其他自回归 TTS 模型 (**TransformerTTS**[^55], **SpeechT5**[^56]) 的设计, 我们采用线性层将语言模型的输出映射为 logits, 然后用于计算停止预测的 BCE 损失.
总训练目标形式化为:

$$
\mathcal{L}_{LM} = \mathcal{L}_{KL} + \lambda_{stop} \mathcal{L}_{stop},
$$

- $\lambda_{stop}$ 为用于平衡停止预测损失和 KL 散度损失的超参数.

</td></tr></table>

### Inference: 推理

<table><tr><td width="50%">

During inference, ***KALL-E*** is capable of both unconditional and conditional text-to-speech synthesis.
For unconditional TTS, ***KALL-E*** autoregressively generates the target speech distribution directly from the provided input text.
Thanks to the reparameterization technique used in the sampling process, ***KALL-E*** can produce diverse speech outputs with varying speaker timbres and speaking styles for the same input text when performing batch inference.

For conditional TTS, such as in the case of zero-shot TTS, ***KALL-E*** achieves this through an in-context learning mechanism, similar to **VALL-E** and **MELLE**.
Specifically, given the text content $x$ for synthesis, along with the text transcription $\hat{x}$ and speech distribution $\hat{z}$ of acoustic prompt, ***KALL-E*** autoregressively generates the target speech distribution $z$ of the corresponding content while preserving the acoustic characteristics of the prompt speech.
This process is formulated by maximizing the likelihood probability: $p(z|\hat{x},x,\hat{z};\theta)$.

</td><td>

在推理时, ***KALL-E*** 能够处理非条件化和条件化的文本转语音合成.
- 非条件化 TTS: ***KALL-E*** 直接从提供的输入文本生成目标语音分布.
  由于采样时使用重参数化技术, 使用批量推理时, ***KALL-E*** 可以对相同的输入文本生成具有不同说话人音色和说话风格的多样化输出.
- 条件化 TTS (如零样本 TTS): ***KALL-E*** 通过类似 **VALL-E** 和 **MELLE** 的上下文学习机制来实现.
  具体来说, 给定待合成的文本内容 $x$, 以及声学提示的语音分布 $\hat{z}$ 和文本转录 $\hat{x}$, ***KALL-E*** 以自回归方式生成对应内容的目标语音分布 $z$, 同时保留提示语音的声学特征.
  这一过程可以被形式化为最大化似然概率: $p(z|\hat{x},x,\hat{z};\theta)$.

</td></tr></table>

## 4·Experiments: 实验

### Dataset

<table><tr><td width="50%">

We conduct experiments on the open-source **LibriTTS corpus**[^44], a high-quality speech dataset derived from the **LibriSpeech corpus**[^57].
LibriTTS contains approximately 585 hours of 24 kHz speech data from over 2,400 speakers, making it suitable for high-fidelity speech synthesis.
For evaluation, we use the LibriTTS test-clean set for zero-shot TTS performance evaluation.
Additionally, we utilize the **ESD corpus**[^46] for zero-shot emotion cloning evaluation and the **VCTK corpus**[^47] for zero-shot accent cloning evaluation.

</td><td>

</td></tr></table>

### Implement Details

<table><tr><td width="50%">

For WaveVAE, the encoder follows the architecture of **Glow-WaveGAN**[^09], while the decoder refers to the settings of **BigVGAN**[^51].
The latent dimension of the speech distribution $z$ is set to 64, with a frame rate of 100 Hz.
The AR language model comprises approximately 445M parameters and 24 **LLaMA**[^58] layers, each with 16 attention heads.
The input embedding dimension for the language model is set to 1024.
The WaveVAE model is trained with a total batch size of 64 for 2,000k steps, while the AR language model is trained with a total batch size of 256 for 32k steps.

</td><td>

</td></tr></table>

### Comparison Systems

<table><tr><td width="50%">

To assess the performance of ***KALL-E***, we compare it with the open-source implementation of several popular zero-shot TTS models.
Note that the training data used for these models might be different.

- **YourTTS**[^45]: A non-autoregressive TTS model based on VITS, enhanced with a pre-t
- rained speaker encoder to improve zero-shot TTS capabilities.
We use the released checkpoint [Github](https://github.com/Edresson/YourTTS).
- **VALL-E**[^14]: A codec language model approach for zero-shot TTS synthesis that uses audio codec codes as intermediate representations.
We use the open-source checkpoint [Github](https://github.com/open-mmlab/Amphion/tree/main/egs/tts/VALLE) from Amphion [^59].
- **NaturalSpeech2**[^22]: A zero-shot TTS system that incorporates a neural audio codec with continuous latent vectors and a latent diffusion model for non-autoregressive speech generation.
We use the open-source checkpoint [Github](https://github.com/open-mmlab/Amphion/tree/main/egs/tts/NaturalSpeech2) from Amphion [^59].
- **CosyVoice**[^19]: a scalable zero-shot speech synthesis system that integrates language and flow-matching models.
We use their released checkpoint [Github](https://github.com/FunAudioLLM/CosyVoice).

</td><td>

</td></tr></table>

### Evaluation Metrics

<table><tr><td width="50%">

We employ subjective and objective evaluations to assess the performance of synthetic speech.
For subjective evaluation, we use the Mean Opinion Score (MOS) to evaluate the naturalness of the synthetic speech, Similarity Mean Opinion Score (SMOS) to assess speaker similarity between the synthetic and prompt speech, Comparative Mean Opinion Scores (CMOS) for emotion or accent preference according to reference speech.
Each evaluation has at least 10 participants.
The rating scale is as follows: bad = 1, poor = 2, fair = 3, good = 4, great = 5, with half-point increments.
For objective evaluation, we follow **Seed-TTS**[^18] [Github](https://github.com/BytedanceSpeech/seed-tts-eval/tree/main) and measure word error rate (WER) and speaker similarity (SIM).
Specifically, we use **Whisper-Large-V3**[^60] to compute the WER of synthetic speech.
For SIM, we employ **WavLM-Large**[^61] fine-tuned on a speaker verification task to extract speaker embeddings and calculate the cosine similarity between the synthetic speech and reference clips.

</td><td>

</td></tr></table>

## 5·Results: 结果

### Zero-shot TTS

<table><tr><td width="50%">

As shown in Table~\ref{tab:zero-shot_tts}, ***KALL-E*** outperforms YourTTS, VALL-E, NaturalSpeech 2, and CosyVoice in MOS, demonstrating its ability to synthesize highly natural speech in zero-shot scenarios.
Moreover, ***KALL-E*** surpasses these systems by a notable margin in SMOS, highlighting its superior capability in cloning speaker timbre and other speech characteristics.
This advantage can be attributed to the use of continuous speech distributions, which capture finer-grained acoustic details compared to discrete representations used in comparison systems.

In objective evaluations, CosyVoice achieves the best WER, with ***KALL-E*** obtaining a higher WER compared to CosyVoice and NaturalSpeech 2.
This discrepancy is likely due to occasional issues in ***KALL-E***’s generated speech, such as missed or duplicated words, pointing to areas where synthesis robustness can be further improved.
Despite this, ***KALL-E*** achieves the highest SIM among all systems, corroborating its strength in zero-shot voice cloning and aligning with subjective evaluation results.

</td><td>

</td></tr></table>

### Zero-shot Emotion and Accent Cloning

<table><tr><td width="50%">

To further assess the zero-shot capabilities of ***KALL-E***, we conduct experiments on emotion cloning and accent cloning.
As shown in Table~\ref{tab:esd_tts}, all comparison models exhibit performance degradation in zero-shot emotional voice cloning, underscoring the inherent challenge of this task.
However, ***KALL-E*** demonstrates superior performance compared to its results on the LibriTTS test-clean set.
This observation highlights the richness of continuous speech distributions in capturing acoustic details.
Notably, the CMOS results indicate that ***KALL-E*** achieves the best performance in zero-shot emotion cloning among all models.

In zero-shot accent cloning, as depicted in Table~\ref{tab:vctk_tts}, NaturalSpeech 2 and CosyVoice achieve low WER, whereas YourTTS, VALL-E, and ***KALL-E*** exhibit relatively higher WER.
We find that accented pronunciation adversely affects the clarity of synthetic speech produced by YourTTS, VALL-E, and ***KALL-E***.
Despite this, ***KALL-E*** achieves the best results in accent preference as reflected in CMOS evaluations.
Furthermore, ***KALL-E*** records the highest SIM score, affirming its effectiveness in preserving speaker timbre during zero-shot accent cloning.

Additionally, ***KALL-E*** can generate diverse speech with various speaker timbres and speaking styles in an unconditional TTS manner.
We show this ability on our online demo page.

</td><td>

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

In this study, we propose ***KALL-E***, an AR language model approach for TTS with next-distribution prediction.
By leveraging WaveVAE to extract continuous speech distributions, ***KALL-E*** predicts these distributions conditioned on text input through an AR language model.
We demonstrate the superior in-context learning capability of ***KALL-E*** in zero-shot scenarios.
Additionally, ***KALL-E*** excels in cloning the emotion and accent during synthesis, offering diverse outputs in sampling-based inference.

Despite these advancements, ***KALL-E*** has several limitations.
First, like other AR TTS systems, it still faces challenges in synthesis robustness.
Specifically, some words may be unclear, missed, or duplicated in the generated speech.
Second, the inference efficiency can be improved.
Although the current system operates at a frame rate of 100 Hz, reducing the frame rate to a lower frame rate, such as 12.5 Hz, could improve computational efficiency.
Lastly, the LibriTTS corpus used for training is relatively small, and the performance of ***KALL-E*** could benefit from leveraging larger datasets, such as **LibriLight**[^29], **WenetSpeech4TTS**[^30] or **Emilia**[^31].

</td><td>

</td></tr></table>

### Future Work

#### Scaling law

<table><tr><td width="50%">

We observe that when ***KALL-E*** is trained on a 10-hour corpus, the generated speech lacks intelligibility.
However, when scaled to a 500-hour corpus, such as LibriTTS, the intelligibility and naturalness of the synthesized speech improve significantly.
We believe that by scaling up to tens of thousands of hours of speech data, the performance of ***KALL-E*** could be further enhanced, enabling more realistic and expressive speech synthesis.

</td><td>

</td></tr></table>

#### Sampling mechanism

<table><tr><td width="50%">

In discrete token-based language models, the sampling mechanism significantly affects the quality, diversity, and coherence of the generated token sequence.
Standard techniques, such as top-k sampling, top-p sampling, and additional parameters like temperature and repetition penalty, can be adjusted to optimize performance.
However, there is limited research on effective sampling from continuous speech representations.
Investigating and developing new sampling strategies for continuous latent distributions will be key to improving ***KALL-E***'s generation capabilities.

</td><td>

</td></tr></table>

#### Spoken dialogue system

<table><tr><td width="50%">

According to **WavChat**[^62], most speech comprehension systems rely on continuous speech representations for capturing fine-grained speech details, whereas most speech generation systems typically use discrete speech representations due to next-token prediction.
However, in AR language models, the output is intrinsically equal to the input, meaning the output and input representations should be the same type.
***KALL-E***'s success in using continuous speech distributions for speech generation suggests a potential pathway for integrating continuous representations into spoken dialogue systems, addressing the current divide between comprehension and generation models.

</td><td>

</td></tr></table>

## References: 参考文献

[^01]: [**Survey**: Neural Text-to-Speech Synthesis.]() Springer2023.
[^02]: [**Survey**: Towrads](../../Surveys/S20241209.md) ArXiv:2412.06602.
[^03]: [**Tacotron**: A Fully End-to-End Text-to-Speech Synthesis Model.](../Acoustic/2017.03.29_Tacotron.md) InterSpeech2017.
[^04]: fastspeech
[^05]: durian
[^06]: fs2
[^07]: ptaco
[^08]: vits
[^09]: glow-wavegan
[^10]: clone
[^11]: vits2
[^12]: oversmoothing
[^13]: generspeech
[^14]: [**VALL-E**: Neural Codec Language Models Are Zero-Shot Text to Speech Synthesizers.](../../SpeechLM_TTS/2023.01.05_VALL-E.md023.01.05_VALL-E.md) ArXiv:2301.02111.
[^15]: speartts
[^16]: [**UniAudio**: Towards Universal Audio Generation with Large Language Models.](../SpeechLM/ST2S/2023.10.01_UniAudio.md) ICML2024.
[^17]: unistyle
[^18]: seedtts
[^19]: cosyvoice
[^20]: dkguo
[^21]: touchTTS
[^22]: ns2
[^23]: ns3
[^24]: flashsspech
[^25]: simplespeech
[^26]: e2tts
[^27]: e3tts
[^28]: maskgct
[^29]: librilight
[^30]: wenetspeech4tts
[^31]: Emilia
[^32]: [**EnCodec**: High Fidelity Neural Audio Compression.](../Tokenizers/2022.10.24_EnCodec.md) ArXiv:2210.13438.
[^33]: soundstream
[^34]: speechtokenizer
[^35]: singlecodec
[^36]: xcodec
[^37]: wavtokenizer
[^38]: melle
[^39]: vectok
[^40]: Spectron
[^41]: contokenizer
[^42]: kaiminghe
[^43]: ms_diff
[^44]: libritts
[^45]: yourtts
[^46]: ESD
[^47]: vctk
[^48]: glow-wavegan2
[^49]: glow-wavegan3
[^50]: vae
[^51]: bigvgan
[^52]: mpd
[^53]: mrd
[^54]: featuremap
[^55]: transformertts
[^56]: speecht5
[^57]: Librispeech
[^58]: llama
[^59]: amphion
[^60]: [**Whisper**: Robust Speech Recognition via Large-Scale Weak Supervision.](../-ASR/2022.12.06_Whisper.md) ICML2023.
[^61]: wavlm
[^62]: wavchat
