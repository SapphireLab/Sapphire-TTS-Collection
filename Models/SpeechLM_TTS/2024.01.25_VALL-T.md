# VALL-T

<details>
<summary>基本信息</summary>

- 标题: "VALL-T: Decoder-Only Generative Transducer for Robust and Decoding-Controllable Text-to-Speech"
- 作者:
  - 01 Chenpeng Du (杜晨鹏)
  - 02 Yiwei Guo (郭奕玮)
  - 03 Hankun Wang (王翰坤)
  - 04 Yifan Yang (杨亦凡)
  - 05 Zhikang Niu (牛志康)
  - 06 Shuai Wang (王帅)
  - 07 Hui Zhang
  - 08 Xie Chen (陈谐)
  - 09 Kai Yu (俞凯)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2401.14321)
  - [Publication](https://doi.org/10.1109/ICASSP49660.2025.10890943) ICASSP2025
  - [Github](https://github.com/cpdu/vallt)
  - [Demo](http://cpdu.github.io/vallt)
- 文件:
  - [ArXiv](../SpeechLM/_PDF/2401.14321v5__VALL-T__Decoder-Only_Generative_Transducer_for_Robust_and_Decoding-Controllable_Text-to-Speech.pdf)
  - [Publication] #TODO

</details>

## Abstract·摘要

<table><tr><td width="50%">

Recent TTS models with decoder-only Transformer architecture, such as **SPEAR-TTS** and **VALL-E**, achieve impressive naturalness and demonstrate the ability for zero-shot adaptation given a speech prompt.
However, such decoder-only TTS models lack monotonic alignment constraints, sometimes leading to hallucination issues such as mispronunciation, word skipping and repeating.
To address this limitation, we propose ***VALL-T***, a generative Transducer model that introduces shifting relative position embeddings for input phoneme sequence, explicitly indicating the monotonic generation process while maintaining the architecture of decoder-only Transformer.
Consequently, ***VALL-T*** retains the capability of prompt-based zero-shot adaptation and demonstrates better robustness against hallucinations with a relative reduction of 28.3% in the word error rate.
The audio samples are available at http://cpdu.github.io/vallt.

</td><td>

最近, 采用仅解码器 Transformer 架构的文本转语音模型, 如 **SPEAR-TTS** 和 **VALL-E**, 在自然性方面取得了显著进展, 并且展示了在给定语音提示的情况下进行零样本适应的能力.
然而, 这些仅解码器的TTS模型缺乏单调对齐约束, 有时会导致幻觉问题, 如发音错误、单词跳过或重复等.

为了解决这一限制, 我们提出了 ***VALL-T***, 一种生成型Transducer模型, 它为输入的音素序列引入了相对位置嵌入的位移, 从而明确表示单调生成过程, 同时保持仅解码器Transformer的架构.
因此, ***VALL-T*** 保留了基于提示的零样本适应能力, 并且在抗幻觉方面表现得更为稳健, 单词错误率相对减少了28.3%.

此外, ***VALL-T*** 在解码过程中对齐的可控性使其能够使用未转录的语音提示, 甚至是在未知语言中.
它还通过利用对齐的上下文窗口, 能够合成较长的语音.

音频样本可以通过访问 [http://cpdu.github.io/vallt](http://cpdu.github.io/vallt) 获取.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

Text-to-speech (TTS) synthesis is a monotonic sequence-to-sequence task, maintaining a strict order between the input phoneme sequence and the output speech sequence.
Moreover, the output speech sequence is at frame-level and one phoneme may correspond to multiple frames of speech, so the output sequence is significantly longer than its corresponding input phoneme sequence.
Typical non-autoregressive neural text-to-speech models, such as **FastSpeech2**[^01], **GradTTS**[^02], **UniCATS**[^03] and **NaturalSpeech2**[^04], integrate an explicit duration prediction module.
Prior to training, the target duration is conventionally derived using the Viterbi forced alignment algorithm.

Over the past two years, utilizing discrete speech tokens for speech generation is proposed in **GSLM**[^05] and **VQTTS**[^06], paving the way for integrating cutting-edge language modeling techniques into TTS systems.
Inspired by exceptional strides in natural language processing driven by decoder-only large Transformer models like **GPT-3**[^07] and the **LLAMA2**[^08], **Tortoise-TTS**[^09], **SPEAR-TTS**[^10] and **VALL-E**[^11] adopted the decoder-only architecture for TTS, achieving remarkable naturalness.
**SPEAR-TTS** and **VALL-E** also have the ability to perform zero-shot speaker adaptation through in-context learning and auto-regressive (AR) continuation from a given speech prompt.
Furthermore, these decoder-only TTS models, unlike the previous non-autoregressive neural TTS models, circumvent explicit duration modeling and the requirement for phoneme durations obtained before hand.
This characteristic offers convenience and simplifies training process, especially when training on large scale datasets.
However, the implicit duration modeling within these systems lacks the monotonic alignment constraints, often leading to hallucination issues like mispronunciation, word skipping and repeating.

In this work, we introduce ***VALL-T***, a decoder-only generative Transducer model that combines the best of both worlds.
It eliminates the need for obtaining phoneme duration before hand while still maintains the monotonic alignment constraint.

**Transducer**[^12], also known as RNN-T, is an existing training scheme designed specifically for monotonic sequence-to-sequence task.
Typical Transducer model adopts a modularized architecture, composed of an encoder, a prediction network and a joint network, and has demonstrated success in automatic speech recognition (ASR) [^13] as a classification task.

During inference, ***VALL-T*** allows us to explicitly guide the monotonic generation process by shifting the relative positions from left to right and dramatically alleviate the hallucination issue in decoder-only TTS.

To the best of our knowledge, this is the first work that implements Transducer with a decoder-only Transformer architecture.
***VALL-T*** presents several advantages compared to previous TTS models:

- ***VALL-T*** introduces monotonic alignment constraints without altering the decoder-only architecture, leading to a better robustness against hallucination.
- ***VALL-T*** is capable of forced alignment given paired phoneme and speech sequences.
- The alignment between phoneme and speech sequences is aware during inference by tracking the timing of shifting the relative positions.

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

The **Transducer** model [^12], also known as **RNN-T**, is designed for monotonic sequence-to-sequence tasks and comprises three components: an encoder, a prediction network, and a joint network.
Here, the prediction network is an auto-regressive network, such as RNN and LSTM.
Transducer model also introduces a special output token called blank, denoted as $\varnothing$, which signifies the alignment boundary between output and input sequence.
We define $\mathcal{Y}$ as the vocabulary of output tokens and $\bar{\mathcal{Y}}=\mathcal{Y}\cup \{\varnothing\}$ as the extended vocabulary.
Also, we denote the lengths of the input sequence $\bm{x}$ and output sequence $\bm{y}$ as $T$ and $U$ and the size of the extended vocabulary $\bar{\mathcal{Y}}$ as $\bar{V}$.

In the training phase, the encoder and prediction network encode the two sequences $\bm{x}$ and $\bm{y}$ respectively, yielding encoded hidden sequences $\bm{f}$ and $\bm{g}$.
Then the joint network combines $\bm{f}$ and $\bm{g}$ at all possible positions to predict the corresponding next output token, constructing an alignment grid.
Each path $\bar{\bm{y}}$ from the bottom left corner to the top right corner of the grid represents an alignment between $\bm{x}$ and $\bm{y}$, with a length of $T+U$.
The training criterion of Transducer model is to maximize the probability of $\mathbf{Pr}(\bm{y}|\bm{x})$, which is the summation of the probabilities of all possible alignment paths $\bar{\bm{y}}$.

In the inference phase, the prediction network auto-regressively predicts the next token, conditioning on the sliced input hidden vectors that slide from $\bm{f}_0$ to $\bm{f}_{T-1}$ whenever the blank token $\varnothing$ emerges.

</td><td>

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

Modularized Transducer model has demonstrated significant success in ASR [^13].
Nevertheless, its suitability for generation tasks is limited.
Typically, the joint network is a small network, comprising only one or a few linear projection layers, and the prediction network is LSTM or Transformer.
This architecture introduces a limitation wherein the input condition is not incorporated into the generation process until it reaches the joint network.
Worse still, the joint network is too small to effectively integrate input conditions into the generation process.
Moreover, the modularized Transducer model utilizes slicing to denote specific positions.
Consequently, the joint network is unable to explicitly perceive the input context, further making difficulties in achieving satisfactory performance for conditional generation tasks.

Therefore, ***VALL-T*** integrates the encoder, the prediction network and the joint network into one single decoder-only Transformer architecture and leverages relative position embedding to denote the corresponding positions.
We discuss the training and inference details below.

</td><td>

</td></tr>
<tr><td>

### Training

We use a decoder-only architecture for ***VALL-T***.
Similar to the approach in the previous work **VALL-E**, we concatenate the input phoneme and output speech tokens along the time axis and present them to the model as a unified sequence.
Unlike traditional RNN and LSTM architectures, the Transformer lacks a specific time order for input tokens, relying instead on position embeddings to indicate their positions.
The position indices for the input sequence range from $0$ to $T-1$ and are converted into position embeddings through a sinusoidal function [^14].
Similarly, the output sequence adopts position indices from $0$ to $U$, including an additional \texttt{<sos>} token at the beginning.
Following **VALL-E**, we utilize a triangular attention mask for the output sequence, facilitating auto-regressive generation.
This mask ensures that each speech token attends to only previously generated tokens, maintaining a proper sequential order in the output.

Beyond the typical absolute position indices starting from 0, we introduce additional relative position indices in ***VALL-T*** for input tokens.
The relative position index $0$ specifies the current phoneme under synthesis.
The phonemes to its left are assigned negative position indices starting from $-1$, while those to its right are assigned positive position indices starting from $1$.
These relative position indices are converted to relative position embeddings with a same sinusoidal function as the absolute position indices.
The resulting absolute and relative position embeddings are added to the input phoneme embeddings and subsequently presented to the decoder-only Transformer.
By adopting this approach, the model gains awareness of the phoneme presently undergoing synthesis, specifically the one assigned a relative position of $0$, and the phonemes serving as its preceding and subsequent contexts.

The blank token $\varnothing$ serves as a marker denoting the end of each phoneme's generation.
Consequently, the output projection following the decoder-only Transformer projects the hidden sequence into the extended vocabulary size of $\bar{V}$.
The projected hidden sequence, with a length of $U+1$, undergoes a Softmax function to yield a sequence representing the output distribution.

Transducer loss is computed on top of an alignment grid in the shape of $T \times (U+1)$.
Different from the method for constructing the alignment grid in the modularized Transducer, ***VALL-T*** treats the output sequence of the decoder-only model as a column within this grid, corresponding to the phoneme indicated by the relative position index 0.
Through iterating over all possible relative positions, as illustrated in Figure \ref{fig:vallt}, the alignment grid is constructed.
This enables us to compute the Transducer loss on top of the grid to maximize the probability of $p(\bm{y}|\bm{x})$.

</td><td>

</td></tr>
<tr><td>

### Monotonic auto-regressive inference

Let us first consider the auto-regressive inference process without a speech prompt.
Initially, the relative position $0$ is designated to the first phoneme, starting the speech generation from the \texttt{<sos>} token.
The model then auto-regressively produces speech tokens based on the input phoneme tokens and previously generated speech tokens until the blank token $\varnothing$ emerges.
The emergence of $\varnothing$ denotes the completion of the first phoneme's generation and triggers a shift in relative positions.
We iteratively conduct the above process until the appearance of $\varnothing$ for the last phoneme, indicating the conclusion of the entire generation process for the input phoneme sequence.

Since the model is encouraged to generate speech tokens for the phoneme assigned relative position $0$ by Transducer loss during training, the step-by-step shifting operation during decoding facilitates the monotonic generation process and consequently enhance the robustness against hallucination.

Next, we consider the integration of the speech prompt for zero-shot speaker adaptation.
Following the approach used in **VALL-E**, the phoneme transcript of the speech prompt is placed at the start of the input sequence, while the speech prompt itself is positioned at the beginning of the output sequence.
The two sequences are followed by the input phonemes to be generated and their corresponding output speech tokens respectively.
Given that the speech prompt are provided, we assign the relative position 0 to the first phoneme right after the prompt transcript, as shown in Figure \ref{fig:vallt}, and perform speech continuation.
Likewise, the relative positions undergo a shift each time $\varnothing$ emerges, repeating until the generation for the final phoneme is completed.

</td><td>

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

***VALL-T*** is compatible with any speech tokenizers.
In our experiments, we use the **EnCodec**[^15] speech tokenizer, following **VALL-E**, to ensure a fair comparison between our model and **VALL-E** and eliminate other factors that could potentially affect our analysis.
The speech tokens are extracted in 50Hz and the sampling rate of output waveforms is 16k.
It comprises 8 residual vector quantization (RVQ) indices for each frame.
We also follow the approach introduced in **VALL-E** that predicts the sequence of the first RVQ index with the auto-regressive models and then predicts the remaining 7 RVQ indices conditioned on the first RVQ index with a separate non-auto-regressive (NAR) model.
Both the input and output sequences are encoded with **BPE**[^16] algorithm to shorten sequence lengths and diminish GPU memory consumption.
***VALL-T*** adopts an identical architecture to **VALL-E**, containing 12 layers of Transformer blocks.
Each block comprises 12 attention heads and has a hidden dimension of 1024.

We use **LibriTTS**[^17] dataset in our experiments, which is a multi-speaker transcribed English speech dataset.
Its training set consists of approximately 580 hours of speech data from 2,306 speakers.
We train our model for 40 epochs using a **ScaledAdam**[^18] optimizer.
The learning rate scheduler is **Eden**[^18] with a base learning rate of $0.05$, an epoch scheduling factor of 4 and a step scheduling factor of 5000.
We use 8 A800 GPUs for the training.

</td><td>

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

### Alignment Analysis

We first do alignment analysis to check if relative position embedding in ***VALL-T*** indicates the alignment as expected.
Given the speech $\bm{y}$ and its transcript $\bm{x}$, we iterate over all relative positions and calculate the alignment grid $\bm{p}$ of output distributions in the shape of $T\times (U+1)$.
Then we calculate the forward variables, backward variables and posterior probabilities accordingly.
The definitions of forward variable, backward variables, and posterior probabilities have been introduced in **Transducer**[^12].

In Figure \ref{fig:alignment}, we illustrate an example of the forward variable, backward variable, and posterior probability for ***VALL-T***, with darker colors indicating lower values.
The values are plotted on a logarithmic scale.
In Figure \ref{alpha} and \ref{beta}, we can see a faint bright line on the diagonal of the two graphs.

Pixel-wise summing the values from Figure \ref{alpha} and Figure \ref{beta} produces Figure \ref{prob}, which represents the posterior probability.
The diagonal line becomes much clearer in this composite figure, indicating that ***VALL-T*** correctly models the alignment between the input and output sequences with relative position embeddings.
Accordingly, ***VALL-T*** is capable of forced alignment, where the most probable path from the bottom-left corner to the top-right corner in the posterior probability map serves as the alignment path.
The alignment path for this example is depicted in Figure \ref{path}.
Since ground-truth labels for alignment are unavailable, our alignment analysis here only focuses on qualitative aspects.

</td><td>

</td></tr>
<tr><td>

### Evaluation on Zero-Shot TTS

In this section, we conduct an evaluation of our models on zero-shot TTS task.
The task refers to synthesizing speech in the voices of unseen speakers given speech prompts and their corresponding transcripts.
Our test set uses a same test set as in **UniCATS**[^03], containing 500 utterances and involving 37 speakers from the LibriTTS test set.
Each speaker is assigned a specific speech prompt.
Before assessing the performance of our models, we conduct speech resynthesis using our Encodec to evaluate the speech tokenizer.
We also do an experiment named ``NAR resynthesis''.
In this experiment, we send the ground-truth first RVQ index to the NAR model for predicting the remaining 7 RVQ indices.
Then, we convert all the 8 RVQ indices to waveform using the Encodec decoder.
The purpose of the NAR resynthesis experiment is to demonstrate the performance degradation introduced by the NAR model, so we can better analyze the results of the entire pipelines, where the AR models are the primary focus of our paper.

The baselines of this experiment include two models.
One is the popular decoder-only TTS model **VALL-E** and another is the recently proposed TTS model with a modularized Transducer architecture called "Transduce and Speak" [^19].
The main evaluation metric in this paper is the word error rate (WER).
In our evaluation process, we first synthesize speech for the test set, and then perform speech recognition using a well-known ASR model, **Whisper**[^20] [URL](https://huggingface.co/openai/whisper-medium).
The transcripts obtained from the ASR model are then compared to the ground-truth input text to calculate the word error rate.

Table \ref{tab:tts} shows that ***VALL-T*** attains significant lower WER than baselines, which is a 28.3\% relative reduction when compared to **VALL-E** and is only 0.41 higher than NAR resynthesis, suggesting the robustness of ***VALL-T***.

Additionally, we present the mel-cepstral distortion (MCD), Speaker Embedding Cosine Similarity (SECS) and Mean Opinion Score (MOS) for naturalness and speaker similarity.
Note that the SECS here is evaluated between the generated speech and the provided speech prompt, not the corresponding ground-truth speech, since our goal is to emulate solely the voice of the given prompt.
It is obtained using a [pretrained speaker verification model [URL]](https://github.com/resemble-ai/Resemblyzer).
In MOS tests, 15 listeners were tasked with rating each utterance on a scale from 1 to 5, with higher scores indicating better naturalness and similarity.
We can observe that ***VALL-T*** performs comparably to or even better than the baselines on most of these metrics when WER dramatically decreases.

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

In this research, we present ***VALL-T***, a decoder-only generative Transducer model designed to improve the robustness and controllability of TTS models.
***VALL-T*** incorporates monotonic alignment constraints into the decoder-only TTS framework, enabling implicit modeling of phoneme durations.
Threfore, this model eliminates the need for acquiring phoneme durations before training.
***VALL-T*** supports forced alignment given input phonemes and the corresponding output speech by searching the best path on the posterior probability map.

</td><td>

</td></tr></table>

## References: 参考文献

[^01]: [FastSpeech2](../Acoustic/2020.06.08_FastSpeech2.md)
[^02]: [Grad-TTS](../Acoustic/2021.05.13_Grad-TTS.md)
[^03]: [UniCATS](../Diffusion/2023.06.13_UniCATS.md)
[^04]: [NaturalSpeech2](../Diffusion/2023.04.18_NaturalSpeech2.md)
[^05]: [GSLM](../SpeechLM/PureSpeechLM/2021.02.01_GSLM.md)
[^06]: [VQTTS](../E2E/2022.04.02_VQTTS.md)
[^07]: [GPT-3](../TextLM/2020.05.28_GPT-3.md)
[^08]: [LLaMA2](../TextLM/2023.07.18_LLaMA2.md)
[^09]: [TorToise-TTS](../Diffusion/2023.05.12_TorToise-TTS.md)
[^10]: [SPEAR-TTS](../../SpeechLM_TTS/2023.02.07_SPEAR-TTS.md.02.07_SPEAR-TTS.md)
[^11]: [VALL-E](2023.01.05_VALL-E.md)
[^12]: [Transducer/RNN-T](../-ASR/2012.11.14_RNN-T.md)
[^13]: Streaming End-To-End Speech Recognition for Mobile Devices.
[^14]: [Transformer](../_Basis/2017.06.12_Transformer.md)
[^15]: [EnCodec](../../SpeechCodec/2022.10.24_EnCodec.md)
[^16]: [BPE](../_Basis/2015.08.31_BPE.md)
[^17]: [LibriTTS](../../Datasets/2019.04.05_LibriTTS.md)
[^18]: [ScaleAdam/Eden/Zipformer](../../Modules/Optimization/2023.10.17_ScaledAdam.md)
[^19]: Transduce and Speak: Neural Transducer for Text-To-Speech with Semantic Token Prediction.
[^20]: [Whisper](../-ASR/2022.12.06_Whisper.md)
