# VoiceCraft

<details>
<summary>基本信息</summary>

- 标题: "VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild"
- 作者:
  - 01 Puyuan Peng
  - 02 Po-Yao Huang
  - 03 Daniel Li
  - 04 Abdelrahman Mohamed
  - 05 David Harwath
- 链接:
  - [ArXiv](https://arxiv.org/abs/2403.16973)
  - [Publication](https://doi.org/10.18653/v1/2024.acl-long.673) ACL 2024
  - [Github](https://github.com/jasonppy/VoiceCraft)
  - [Demo](https://jasonppy.github.io/VoiceCraft_web)
- 文件:
  - [ArXiv](../SpeechLM/_PDF/2403.16973v3__VoiceCraft__Zero-Shot_Speech_Editing_&_TTS_in_the_Wild.pdf)
  - [Publication](../SpeechLM/_PDF/2403.16973p0__VoiceCraft__ACL2024.pdf)

</details>

## Abstract: 摘要

We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts1.
VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence.
On speech editing
tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including [VALL-E (2023)](ST2S/2023.01.05_VALL-E.md-E.md) and the popular commercial model [XTTS-v2 (2023)](2024.06.07_X../SpeechLM/2024.06.07_XTTS.md
Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models and real recordings.
In particular, for speech editing evaluation, we introduce a high quality, challenging, and realistic dataset named REALEDIT.
We encourage readers to listen to the demos at [this http url](https://jasonppy.github.io/VoiceCraft_web).

## 1.引言

We introduce VOICECRAFT, a Transformer-based neural codec language model (NCLM) that per-forms infilling generation of neural speech codec to-kens autoregressively conditioned on bidirectional context.
VOICECRAFT achieves state-of-the-art(SotA) performance on both speech editing (shown in Fig.1) and zero-shot TTS.
Our method is based on a two-step token rearrangement procedure that consists of a causal masking step and delayed stacking step.
The causal masking technique is in-spired by the success of causal masked multimodal　model in joint text-image modeling (Aghajanyan et al.,2022), and our proposed technique works for speech codec sequences, which enables autore-gressive generation with bidirectional context.
In addition, we further integrate causal masking with delayed stacking (Kharitonov et al., 2021a; Copet et al., 2023) as our proposed token rearrangement procedure, to ensure efficient multi-codebook modeling.

To evaluate speech editing, we manually crafted a first-of-its-kind, realistic, and challenging dataset named REALEDIT.
REALEDIT consists of310real world speech editing examples, with wave-forms sourced from audiobooks (Zen et al., 2019),YouTube videos (Chen et al., 2021a), and Spo-tify podcasts (Clifton et al., 2020), and duration ranging from5seconds to12seconds.
To create the target transcripts, the transcripts of the source speech are edited in such a way that the edited transcripts remain grammatically correct and are semantically coherent.
The dataset is designed to cover a wide range of editing scenarios, includ-ing insertion, deletion, substitution, and multi-span editing, with the length of the edited text ranging from1word to16words.
Compared to commonly used speech synthesis evaluation datasets that only contain audiobooks such as VCTK (Yamagishi et al., 2019), LJSpeech (Ito and Johnson, 2017),and LibriTTS (Zen et al., 2019), REALEDITis　more challenging in that the recordings have di-verse content, accents, speaking styles, recording conditions, and background sounds.
We believe that the realism and diversity of REALEDITmakes it a reliable indicator of the practicality of speech editing models in the real world.

In the subjective human listening tests, VOICE-CRAFTsignificantly outperforms prior SotA speech editing model on REALEDIT.
Importantly,the edited speech produced by VOICECRAFTis nearly indistinguishable from the original unedited recording in terms of naturalness.
We found that VOICECRAFT generalizes well to zero-shot TTS without any finetuning, achieving SotA perfor-mance on a dataset comprised of audiobooks and YouTube videos, outperforming strong baselines including reproduced [VALL-E (2023)](ST2S/2023.01.05_VALL-E.md-E.md) and the popular commercial model XTTS v2 (CO-QUI, 2023).

In summary, our contributions are:
1. We introduce VOICECRAFT, a neural codec language model for speech editing that gener-ates synthesized speech that is nearly indistin-guishable from in-the-wild recordings accord-ing to human listeners.
We also release the code and model weights for VOICECRAFT.
1. We show that VOICECRAFT generalizes well to zero-shot TTS without finetuning.
2. We release a high quality, challenging, and realistic speech editing evaluation dataset REALEDIT.


## 2.相关工作

### 2.1.NCLM & Zero-Shot TTS

Tokenizing speech signals into sequences of learnable, discrete units and then training a language model on the resulting unit sequences was initially proposed in the context of textless NLP (Hsu et al., 2021; Lakhotia et al.,2021; Kharitonov et al., 2021b; Nguyen et al.,2022), where the goal is to perform NLP tasks directly on spoken utterances without the need to first transcribe the speech into text. Recently,NCLMs that operates on tokens from Residual vec-tor quantization (RVQ)-based models (Zeghidour et al., 2021; Defossez et al., 2022) attract increased attention due to its high quality generation. For example, AudioLM (Borsos et al., 2022a) exhibits strong performance on long-term coherent speech continuation. Zero-shot TTS is a task where a model needs to synthesize speech in a target voice which was unseen during training, given only the target transcript and a short reference recording of the target voice. Framing zero-shot TTS as transcript-conditioned speech continuation, [VALL-E (2023)](ST2S/2023.01.05_VALL-E.md-E.md) and [SPEAR-TTS (2023)](ST2S/2023.02.07_SPEAR-TTS.mdTS.md) are the first applications of NCLMs on this task, significantly outperforming non-NCLM approaches. Zhang et al. (2023) extends VALL-E to cross-lingual TTS. Guo et al. (2022); Yang et al.(2023); Liu et al. (2023); Ji et al. (2023); Lyth and King (2024) adapt NCLMs style-controlled speech synthesis. Song et al. (2024); Du et al. (2024b) en-hance phoneme alignment in NCLMs to reduce er-ror. Wang et al. (2023b) proposes a unified NCLM for both speech generation and recognition tasks.
Borsos et al. (2023) proposes an efficient parallel decoding method. NCLMs have also been suc-cessfully applied to other audio domains. Kreuk et al. (2022) applies NCLM to sound effects gener-ation, and Agostinelli et al. (2023); Donahue et al.(2023); Garcia et al. (2023); Copet et al. (2023) use NCLMs for music generation.
Speech editing.This task requires a model to alter words or phrases within an utterance to match a target transcript, but the regions of the original speech not targeted for editing must re-main unchanged (see Fig. 1 for an example). Early methods achieve text-guided speech insertion and substitution by combining a single speaker TTS model and a voice conversion model to generate desired speech segment, which is then concate-nated with unedited part (Jin et al., 2017). Since the generation is not conditioned on the unedited part of the speech, the result sounds unnatural due to prosody mismatch and boundary artifacts (Morri-son et al., 2021). More recent speech editing mod-els have attempted to condition their generation on surrounding speech context. Tan et al. (2021)uses two unidirectional LSTM models with bidirec-tional fusion. Wang et al. (2022); Bai et al. (2022);Borsos et al. (2022b) uses the masked reconstruc-tion objective with Convolutional or Transformer models to further improve contextualization. Flu-entSpeech (Jiang et al., 2023b) is a diffusion-based speech editing model that achieves SotA perfor-mance on speech editing on LibriTTS and VCTK.
The research community starts to investigate the possibility of having a unified model for both zero-shot TTS and speech editing. Yin et al. (2022);Jiang et al. (2023a) propose modular models for the two tasks, while our model is end-to-end. Con-current work SpeechX (Wang et al., 2023c) adapt VALL-E by prompt tuning for a range of tasks including speech editing and zero-shot TTS, but no human evaluation is conducted in their paper.
Concurrent work UniCATS (Du et al., 2024a) is a diffusion-based modular model for the two tasks.
However their model is only evaluated on masked speech reconstruction of span length less than 2 sec-onds, while our model is evaluated on as much as16 words editing. Voicebox (Le et al., 2023) is a re-cent flow matching based model capable of a wide range of tasks including speech editing and zero-shot TTS. However the speech editing capability is not evaluated in their paper, and only shown in their demo page. We therefore compare our model’s edit-ing results with Voicebox’s on our demo page using on the same examples from their demo page.

## 3.方法

VOICECRAFTcasts both sequence infilling (for speech editing) and continuation (for zero-shot TTS) as a simple left-to-right language mod-eling by rearranging neural codec’s output to-kens. The rearrangement involves two steps: (1)causal masking (§3.1) to enable autoregressive continuation/infilling with bidirectional context and (2) delayed stacking (§3.2) to ensure efficient multi-codebook modeling. VOICECRAFTemploys decoder-only Transformers and is trained with an autoregressive sequence prediction (§3.3). We in-troduce the inference setup for speech editing and zero-shot TTS in §3.4.

### 3.1.Rearrangement Step 1: Causal Masking

As shown on the left hand side of Fig. 2, given a continuous speech waveform as input, we first use Encodec (Defossez et al., 2022) to quantize it into aTbyKcodec matrixX, whereTis the number of temporal frames, andKis the number of RVQ codebooks.Xcan be written as(X1, · · · , XT),whereXtis a vector of lengthKrepresenting the codes from different codebooks at time stept, and we assume that code from codebookkmodels the residual from codebookk − 1. During training,our goal is to randomly mask some span of tokens(Xt0, . . . , Xt1), and then autoregressively predict these masked tokens conditioned on all of the un-masked tokens. This is a problem whent1< T, be-cause we cannot condition on future outputs when performing autoregressive generation. We need to modify the masking onXso that it is causal,by moving the span to be masked to the end of the sequence, so that when infilling these tokens the model can condition on both past and future unmasked tokens (Aghajanyan et al., 2022).The procedure outlined above can be trivially ex-tended to multiple masked spans by simply moving all masked spans to the end of the sequence. The number of spans to be maskednis sampled from Poison(λ), and then for each span, we sample a span lengthl ∼ Uniform(1, L). Finally, we ran-domly select the locations of the spans withinX under the constraint that they do not overlap with each other. The selectednspans are then replaced with mask tokens〈M1〉, · · · , 〈Mn〉. The original tokens within these masked spans are moved to the end of the sequenceX, with each span preceded by its corresponding mask token.
Consider this example: letX = (X1, . . . , X6)and imagine we wish to mask a single span from X2toX4. The original sequenceXis rearranged intoY = (Y1; 〈M1〉; Y2; 〈M1〉; Y3; ), whereY1=(X1),Y2= (X5, X6), andY3= (X2, X3, X4).We callY1andY2the unmasked spans, andY3the masked span. An end of span orEOStoken is added to the end of each masked span (in this example at the end ofY3), and an end of utterance orEOUtoken is added to the end of the utterance (i.e.Y2). For simplicity, we do not explicitly denote these special tokens and assume they are part of the spans.

### 3.2.Rearrangement Step 2: Delayed Stacking

### 3.3.Modeling

### 3.4.Inference

## 4.RealDict: A Realistic & Challenging Speech Editing Dataset

## 5.实验

## 6.结论

## 7.局限性

## 8.

## R.参考文献
