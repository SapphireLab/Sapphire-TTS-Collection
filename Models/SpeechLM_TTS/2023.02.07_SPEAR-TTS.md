# SPEAR-TTS

<details>
<summary>基本信息</summary>

- 标题: "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision"
- 作者:
  - 01 Eugene Kharitonov - Google
  - 02 Damien Vincent - Google
  - 03 Zalan Borsos - Google
  - 04 Raphael Marinier - Google
  - 05 Sertan Girgin - Google
  - 06 Olivier Pietquin - Google
  - 07 Matt Sharifi - Google
  - 08 Marco Tagliasacchi - Google
  - 09 Neil Zeghidour - Google
- 链接:
  - [ArXiv](https://arxiv.org/abs/2302.03540)
  - [Publication](https://doi.org/10.1162/tacl_a_00618)
  - [Github]
  - [Demo](https://google-research.github.io/seanet/speartts/examples/)
- 文件:
  - [ArXiv](../SpeechLM/_PDF/2302.03540v1__SPEAR-TTS__Speak_Read_&_Prompt__High-Fidelity_TTS_with_Minimial_Supervision.pdf)
  - [Publication](../SpeechLM/_PDF/2302.03540p0__SPEAR-TTS__TACL2023.pdf)

</details>

## Abstract: 摘要

<table><tr><td width="50%">

We introduce ***SPEAR-TTS***, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision.
By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to "reading") and from semantic tokens to low-level acoustic tokens ("speaking").
Decoupling these two tasks enables training of the "speaking" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the "reading" component.
To control the speaker identity, we adopt example prompting, which allows ***SPEAR-TTS*** to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels.
Our experiments demonstrate that ***SPEAR-TTS*** achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.

</td><td>

我们介绍了 ***SPEAR-TTS***, 这是一个多说话人的文本转语音系统, 可以在最小监督下进行训练.
通过结合两种离散的语音表示, 我们将文本转语音视为两个序列到序列任务的组合:
- 从文本到高级语义 Token (类似于 "阅读")
- 从语义 Token 到低级声学 Token (类似于 "说话")

解耦这两个任务使得可以使用丰富的仅音频数据训练 "说话" 模块, 并解锁了预训练和回译的高效组合, 以减少训练 "阅读" 组件时对平行数据的需求.
为了控制说话人身份, 我们采用了示例提示, 这使得 ***SPEAR-TTS*** 能够仅使用 3 秒的短样本泛化到未见过的说话人, 而无需任何显式的说话人表示或说话人 ID 标签.
我们的实验表明 ***SPEAR-TTS*** 在使用仅 15 分钟的平行数据时, 字符错误率与最先进的方法相当, 同时在主观测试中, 其自然度和声学质量与真实语音相当.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

Training a text-to-speech (TTS) system typically requires hundreds of hours of parallel data in the form of transcribed utterances.
As a consequence, TTS is only available for "high-resource" languages.
Moreover, the audio generated by such systems is only as diverse as the parallel data that they are trained on, which should contain many speakers, with various accents, of diverse demographics, and heterogeneous recording conditions.
At the same time, for most languages, including low-resource ones, audio-only speech data can be relatively abundant online, present in the forms of audiobooks, podcasts, radio and TV shows.

In this paper, we investigate how audio-only data can be leveraged to reduce the need for supervision in training TTS systems.
We introduce \ours{}, SPEAR stands for speak, read and prompt, a multi-speaker TTS system that can be trained with as little as 15 minutes of parallel data from a single speaker.
Moreover, \ours{} can synthesize a new voice using only 3 seconds of speech, without any speaker labels or explicit speaker representation.
At its core, \ours{} leverages recent advances in the "textless" modeling of spoken language~\citep{Lakhotia2021,Dunbar2021,Polyak2021,Kreuk2021,Kharitonov2022,Nguyen2022,Borsos2022}.
These methods represent continuous audio waveforms as sequences of tokens from a finite vocabulary, casting speech generation as a language modeling task.
In particular, AudioLM~\citep{Borsos2022} combines two types of discrete tokens: high-level semantic tokens and low-level acoustic tokens, which that can be mapped to audio.
Using these representations, we cast the TTS problem as a "translation" from text transcripts to acoustic tokens with semantic token representations serving as a pivot "language"~\cite{Utiyama2007}.
This way, TTS is reduced to a composition of two sequence-to-sequence (seq2seq) tasks: translating text to semantic tokens, and translating semantic to acoustic tokens.

The key benefit of splitting the TTS task into these two sub-tasks is that the supervision needed to learn how to map text into the intermediate semantic token representation ("reading") and how to produce speech from it ("speaking") become decoupled.
While the "reading" stage relies on parallel text-audio data, the audio tokens used to train the "speaking" component are produced by self-supervised audio models and therefore can be extracted from a massive amount of unlabeled speech data.
As a result, the quality and diversity of the generated speech become independent from the available parallel data.

Casting each stage of \ours{} as a seq2seq problem allows us to use standard Transformer models~\citep{Vaswani2017} and makes it easy to tap into the vast pool of ideas developed by the machine translation community to reduce the need for supervision.
Specifically, we combine BART/T5-style pretraining~\citep{Lewis2020,Raffel2020} with backtranslation~\citep{Sennrich2016} to significantly reduce the amount of parallel supervision required to train \ours{}.

To control the voice used by \ours{} when producing an utterance, we leverage an example prompting mechanism that is closely related to prompting in textual language models~\citep{Brown2020}.
Here we condition the "speaking"  model with an audio clip representing the target voice, steering it to use this voice when generating the utterance.
This feature can simplify building controllable multi-speaker TTS systems for languages where only single-speaker parallel data is available.

Modeling speech synthesis with seq2seq models enables using stochastic sampling at inference, which allows generating outputs of diverse quality for the same input.
We exploit that to improve the synthesized audio quality by proposing a sampling scheme based on an objective quality metric.

Our experimental study on English speech shows that, by combining pretraining and backtranslation over a large dataset --- 551 hours from LibriTTS~\cite{Zen2019} --- with just 15 minutes of parallel data from a single speaker --- LJSpeech~\cite{ljspeech} --- \ours{}
(a) generates speech with high fidelity to the input transcript --- CER 1.92\% on LibriSpeech test-clean~\cite{Panayotov2015};
(b) synthesizes speech with diverse voices, (c) reliably reproduces the voice of an unseen speaker, when using a 3 second example from the target speaker;
(d) achieves high acoustic quality, comparable to that of the ground-truth utterances (MOS 4.96 vs.\ 4.92).
Note: Samples produced by \ours can be found on [the demo site](https://google-research.github.io/seanet/speartts/examples/).

Overall, our approach to building TTS using massive self-supervised pretraining and backtranslation of discrete speech representations considerably differs from how existing TTS systems are implemented~\cite{Shen2018,Kong2020,Ren2020,Kim2021,Ao2022,valle}, significantly reducing the costs related to data collection and potentially providing high-quality multi-speaker TTS for languages that are not well covered today.

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

Our work relates to several research directions that we overview in this Section.

</td><td>

</td></tr>
<tr><td>

**Discretized Speech Processing**

The work of \citet{Lakhotia2021} on generative spoken language modeling (GSLM) pioneered the use of language models on discretized speech representations.
The main tasks \citet{Lakhotia2021} focuses on are unconstrained speech generation and speech continuation.
Their work became a foundation for a range of applications and extensions, including emotion transfer \cite{Kreuk2021}, prosody \cite{Kharitonov2022} and dialog \cite{Nguyen2022} modeling.
\ours{} is based on AudioLM~\citep{Borsos2022}, a recent development in this line of work that achieves a superior quality in spoken language modeling as well as a high audio quality.
SPEAR-TTS extends AudioLM along several dimensions. We adapt it to the text-to-speech scenario by conditioning it with a transcript input and show that by combining pretraining and back-translation, we can dramatically reduce the amount
of supervision required to train a high-fidelity TTS system. Finally, SPEAR-TTS explicitly incorporates a prompt-like mechanism for a voice control.

</td><td>

</td></tr>
<tr><td>



</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td><td>

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td><td>

</td></tr></table>
