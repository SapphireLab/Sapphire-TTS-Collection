# MixedG2P-T5: G2p-Free Speech Synthesis for Mixed-Script Texts Using Speech Self-Supervised Learning and Language Model

<details>
<summary>基本信息</summary>

- 标题: "MixedG2P-T5: G2p-Free Speech Synthesis for Mixed-Script Texts Using Speech Self-Supervised Learning and Language Model."
- 作者:
  - 01 Joonyong Park
  - 02 Daisuke Saito
  - 03 Nobuaki Minematsu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.01391v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2509.01391v1](PDF/2025.09.01_2509.01391v1_MixedG2P-T5__G2p-Free_Speech_Synthesis_for_Mixed-Script_Texts_Using_Speech_Self-Supervised_Learning_and_Language_Model.pdf)
  - [Publication] #TODO

</details>

## Abstract

This study presents a novel approach to voice synthesis that can substitute the traditional grapheme-to-phoneme (G2P) conversion by using a deep learning-based model that generates discrete tokens directly from speech.
Utilizing a pre-trained voice SSL model, we train a T5 encoder to produce pseudo-language labels from mixed-script texts (e.g., containing Kanji and Kana).
This method eliminates the need for manual phonetic transcription, reducing costs and enhancing scalability, especially for large non-transcribed audio datasets.
Our model matches the performance of conventional G2P-based text-to-speech systems and is capable of synthesizing speech that retains natural linguistic and paralinguistic features, such as accents and intonations.\\

## 1·Introduction

\vspace{-1mm}

Speech synthesis refers to the technology by which machines automatically generate speech audio signals and is commonly known as text-to-speech (TTS).

With the advancement of deep learning, speech synthesis models have demonstrated performance that significantly surpasses traditional methods[^Borgholt2022Brief].

These models typically convert input text into acoustic feature vectors through an encoder, and subsequently generate Mel-spectrograms using techniques such as attention mechanisms or variational inference, which are then transformed into speech by a vocoder[^Shen2018Natural], [^Kim2021Conditional].

The model learns the correspondence between audio samples and their respective “input representations.”

Constructing such deep learning-based speech synthesis systems requires accurately labeled data corresponding to spoken utterances.

In conventional approaches, phonemes are typically generated from sample text using grapheme-to-phoneme (G2P) conversion, which are then input into the speech synthesis model.

In the case of Japanese, where texts often contain a mix of kanji and kana, phonemes are generated from the mixed-script input, which are subsequently used to synthesize speech.

Specifically, some methods rely on rule-based systems to assign required TTS information—such as accent and prosody—based on morphological analysis, while others adopt neural G2P models using CTC or encoder-decoder structures to model the alignment between text and phoneme sequences of differing lengths.

Such transcription tasks are largely conducted manually.

While it is possible to incorporate additional information—such as accents and syllable durations—by referring to pronunciation or accent dictionaries, two major challenges remain in building G2P systems.

The first is the cost associated with data construction.

Generating phonetic elements requires various resources, including pronunciation and accent dictionaries and linguistic rules.

Since these supplementary inputs cannot be derived solely from raw text, they must be individually integrated into the system, thereby incurring high annotation costs.

The second challenge lies in the limited support for multilingual text.

When dealing with texts that include multiple languages, it becomes difficult for a single G2P model to provide adequate coverage.

This necessitates the development of separate models for each language, which further increases costs.

Additionally, pronunciation errors can arise when the same character is pronounced differently depending on the language, presenting a significant difficulty in multilingual speech synthesis.

To address these challenges, this study aims to develop a G2P-free, multilingual-capable speech synthesis model by utilizing discrete representations derived from speech self-supervised learning (SSL) models.

To achieve this goal, the following key aspects are investigated.

-  Performance comparison between input representation using SSL model and conventional G2P representation 
-  Implementation of G2P for mixed Kanji and Kana situations using discrete representation 

## 2·Related Works

\vspace{-1mm}

![](images/GSLM_jpenv4.pdf)

<a id="">(a) Architecture of GSLM and (b) Application to the Japanese language</a>

To address the aforementioned challenges in speech synthesis—particularly the reliance on costly and language-dependent grapheme-to-phoneme (G2P) conversion—recent studies have explored the use of discrete tokens, which are directly extracted from raw audio using speech self-supervised learning (SSL) models trained on large-scale unlabeled speech corpora.

Unlike conventional phoneme representations, these discrete tokens encode not only linguistic and semantic content, but also speaker-specific paralinguistic features such as accentuation, intonation, and prosody.

This rich and compact representation is especially advantageous because it avoids the information loss typically incurred during the intermediate conversion of speech into text.

A prominent framework that leverages this representation for speech synthesis is the Generative Spoken Language Model (GSLM)[^Lakhotia2021On].

GSLM establishes a three-stage pipeline: (1) an encoder that converts the input speech waveform into a sequence of discrete symbols via quantized SSL embeddings, (2) an optional unsupervised language model (uLM) that captures long-range dependencies across the symbol sequence, and (3) a decoder that reconstructs the speech waveform from the symbolic input.

Encoders such as wav2vec 2.0[^Baevski2020Wav2vec] and HuBERT[^Hsu2021Hu{BERT] are commonly employed, offering robust and generalizable speech representations.

These discrete tokens act as a learned alternative to phonemes or graphemes, forming a flexible intermediate layer between raw audio and synthesis.

As depicted in Figure~1(a), the encoder module transforms the continuous speech signal into a compressed symbolic form using a k-means clustering layer trained on SSL features.

The decoder module, typically a neural vocoder or sequence-to-sequence model such as Tacotron 2[^Shen2018Natural], learns to reconstruct the speech waveform from these symbolic representations.

Interestingly, it has been demonstrated (Figure~1(b)) that this symbolic representation is not strictly language-specific: once the encoder-decoder pipeline is trained on one language, it can be transferred to other languages through fine-tuning, enabling zero-shot or low-resource language synthesis.

This cross-lingual transferability opens the door to G2P-free synthesis across many languages, even those lacking well-defined phonemic resources.
