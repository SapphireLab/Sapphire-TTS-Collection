# MixedG2P-T5: G2p-Free Speech Synthesis for Mixed-Script Texts Using Speech Self-Supervised Learning and Language Model

<details>
<summary>基本信息</summary>

- 标题: "MixedG2P-T5: G2p-Free Speech Synthesis for Mixed-Script Texts Using Speech Self-Supervised Learning and Language Model."
- 作者:
  - 01 Joonyong Park
  - 02 Daisuke Saito
  - 03 Nobuaki Minematsu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.01391v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2509.01391v1](PDF/2025.09.01_2509.01391v1_MixedG2P-T5__G2p-Free_Speech_Synthesis_for_Mixed-Script_Texts_Using_Speech_Self-Supervised_Learning_and_Language_Model.pdf)
  - [Publication] #TODO

</details>

## Abstract

This study presents a novel approach to voice synthesis that can substitute the traditional grapheme-to-phoneme (G2P) conversion by using a deep learning-based model that generates discrete tokens directly from speech.
Utilizing a pre-trained voice SSL model, we train a T5 encoder to produce pseudo-language labels from mixed-script texts (e.g., containing Kanji and Kana).
This method eliminates the need for manual phonetic transcription, reducing costs and enhancing scalability, especially for large non-transcribed audio datasets.
Our model matches the performance of conventional G2P-based text-to-speech systems and is capable of synthesizing speech that retains natural linguistic and paralinguistic features, such as accents and intonations.\\

## 1·Introduction

\vspace{-1mm}

Speech synthesis refers to the technology by which machines automatically generate speech audio signals and is commonly known as text-to-speech (TTS).

With the advancement of deep learning, speech synthesis models have demonstrated performance that significantly surpasses traditional methods[^Borgholt2022Brief].

These models typically convert input text into acoustic feature vectors through an encoder, and subsequently generate Mel-spectrograms using techniques such as attention mechanisms or variational inference, which are then transformed into speech by a vocoder[^Shen2018Natural], [^Kim2021Conditional].

The model learns the correspondence between audio samples and their respective “input representations.”

Constructing such deep learning-based speech synthesis systems requires accurately labeled data corresponding to spoken utterances.

In conventional approaches, phonemes are typically generated from sample text using grapheme-to-phoneme (G2P) conversion, which are then input into the speech synthesis model.

In the case of Japanese, where texts often contain a mix of kanji and kana, phonemes are generated from the mixed-script input, which are subsequently used to synthesize speech.

Specifically, some methods rely on rule-based systems to assign required TTS information—such as accent and prosody—based on morphological analysis, while others adopt neural G2P models using CTC or encoder-decoder structures to model the alignment between text and phoneme sequences of differing lengths.

Such transcription tasks are largely conducted manually.

While it is possible to incorporate additional information—such as accents and syllable durations—by referring to pronunciation or accent dictionaries, two major challenges remain in building G2P systems.

The first is the cost associated with data construction.

Generating phonetic elements requires various resources, including pronunciation and accent dictionaries and linguistic rules.

Since these supplementary inputs cannot be derived solely from raw text, they must be individually integrated into the system, thereby incurring high annotation costs.

The second challenge lies in the limited support for multilingual text.

When dealing with texts that include multiple languages, it becomes difficult for a single G2P model to provide adequate coverage.

This necessitates the development of separate models for each language, which further increases costs.

Additionally, pronunciation errors can arise when the same character is pronounced differently depending on the language, presenting a significant difficulty in multilingual speech synthesis.

To address these challenges, this study aims to develop a G2P-free, multilingual-capable speech synthesis model by utilizing discrete representations derived from speech self-supervised learning (SSL) models.

To achieve this goal, the following key aspects are investigated.

-  Performance comparison between input representation using SSL model and conventional G2P representation 
-  Implementation of G2P for mixed Kanji and Kana situations using discrete representation 

## 2·Related Works

\vspace{-1mm}

![](images/GSLM_jpenv4.pdf)

<a id="">(a) Architecture of GSLM and (b) Application to the Japanese language</a>

To address the aforementioned challenges in speech synthesis—particularly the reliance on costly and language-dependent grapheme-to-phoneme (G2P) conversion—recent studies have explored the use of discrete tokens, which are directly extracted from raw audio using speech self-supervised learning (SSL) models trained on large-scale unlabeled speech corpora.

Unlike conventional phoneme representations, these discrete tokens encode not only linguistic and semantic content, but also speaker-specific paralinguistic features such as accentuation, intonation, and prosody.

This rich and compact representation is especially advantageous because it avoids the information loss typically incurred during the intermediate conversion of speech into text.

A prominent framework that leverages this representation for speech synthesis is the Generative Spoken Language Model (GSLM)[^Lakhotia2021On].

GSLM establishes a three-stage pipeline: (1) an encoder that converts the input speech waveform into a sequence of discrete symbols via quantized SSL embeddings, (2) an optional unsupervised language model (uLM) that captures long-range dependencies across the symbol sequence, and (3) a decoder that reconstructs the speech waveform from the symbolic input.

Encoders such as wav2vec 2.0[^Baevski2020Wav2vec] and HuBERT[^Hsu2021Hu{BERT] are commonly employed, offering robust and generalizable speech representations.

These discrete tokens act as a learned alternative to phonemes or graphemes, forming a flexible intermediate layer between raw audio and synthesis.

As depicted in Figure~1(a), the encoder module transforms the continuous speech signal into a compressed symbolic form using a k-means clustering layer trained on SSL features.

The decoder module, typically a neural vocoder or sequence-to-sequence model such as Tacotron 2[^Shen2018Natural], learns to reconstruct the speech waveform from these symbolic representations.

Interestingly, it has been demonstrated (Figure~1(b)) that this symbolic representation is not strictly language-specific: once the encoder-decoder pipeline is trained on one language, it can be transferred to other languages through fine-tuning, enabling zero-shot or low-resource language synthesis.

This cross-lingual transferability opens the door to G2P-free synthesis across many languages, even those lacking well-defined phonemic resources.

## 3·Research Approach

![](images/T5_model_ronbun.png)

<a id="">Architecture of the Pseudo-Language Label Prediction Model</a>

<a id="tab:corpus">Datasets used for training, fine-tuning, and applying each model</a>

In this work, we extend this idea by using the encoder of GSLM not only for decoding speech but also for constructing a pseudo-language label predictor, which can infer symbolic representations directly from raw text.

The goal is to replace the G2P module with a learned mapping from text, which may contain mixed scripts, such as Kanji and Kana in Japanese, to symbolic tokens that function analogously to phonemes.

Once these pseudo-language labels are predicted, they can be fed into the spectral predictor to produce natural-sounding synthesized speech.

The pseudo-language label predictor is built on top of the T5 architecture[^Raffel2019Exploring], a powerful Transformer-based sequence-to-sequence model known for its success in a wide range of natural language processing tasks.

T5’s encoder-decoder structure allows it to handle inputs and outputs of varying lengths, making it suitable for converting raw text into sequences of symbolic labels.

Moreover, T5 is pretrained on a massive multilingual corpus, which equips it with broad linguistic understanding and generalization capabilities.

In our setting, it enables the conversion of Japanese text—including diverse orthographic patterns—to meaningful discrete symbol sequences.

The architecture of this label predictor is illustrated in Figure~2.

By leveraging this T5-based predictor, we eliminate the need for hand-crafted phoneme dictionaries, morphological analyzers, or accent dictionaries typically required in G2P pipelines.

While prior studies have explored the use of T5 for grapheme-to-phoneme conversion tasks, they remain constrained to alphabetic scripts and conventional G2P paradigms[^Řezáčková2021T5g2p], [^Ao2022S}peech{T}5].

Specifically, their work assumes a clear one-to-one correspondence between characters and phonemes, and does not address the combined demonstration of mixed-script languages.

Also, multilingual G2P systems have also been proposed, but they still operate within a text-based inference thus does not have additional prosodic features essential for natural-sounding speech synthesis, such as accents and durations, that can be obtained from SSL models[^Sokolov2019Neural], [^Vesik2020One], [^Zhu2022ByT5].

Therefore, this study demonstrate a fully G2P-free speech synthesis pipeline that leverages SSL-derived tokens and a T5-based pseudo-language label predictor to handle the structural and phonological intricacies of mixed-script languages, such as Japanese kanji and kana. 
