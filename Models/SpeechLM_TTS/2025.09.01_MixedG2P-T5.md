# MixedG2P-T5: G2p-Free Speech Synthesis for Mixed-Script Texts Using Speech Self-Supervised Learning and Language Model

<details>
<summary>基本信息</summary>

- 标题: "MixedG2P-T5: G2p-Free Speech Synthesis for Mixed-Script Texts Using Speech Self-Supervised Learning and Language Model."
- 作者:
  - 01 Joonyong Park
  - 02 Daisuke Saito
  - 03 Nobuaki Minematsu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.01391v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2509.01391v1](PDF/2025.09.01_2509.01391v1_MixedG2P-T5__G2p-Free_Speech_Synthesis_for_Mixed-Script_Texts_Using_Speech_Self-Supervised_Learning_and_Language_Model.pdf)
  - [Publication] #TODO

</details>

## Abstract

This study presents a novel approach to voice synthesis that can substitute the traditional grapheme-to-phoneme (G2P) conversion by using a deep learning-based model that generates discrete tokens directly from speech.
Utilizing a pre-trained voice SSL model, we train a T5 encoder to produce pseudo-language labels from mixed-script texts (e.g., containing Kanji and Kana).
This method eliminates the need for manual phonetic transcription, reducing costs and enhancing scalability, especially for large non-transcribed audio datasets.
Our model matches the performance of conventional G2P-based text-to-speech systems and is capable of synthesizing speech that retains natural linguistic and paralinguistic features, such as accents and intonations.\\

## 1·Introduction

\vspace{-1mm}

Speech synthesis refers to the technology by which machines automatically generate speech audio signals and is commonly known as text-to-speech (TTS).

With the advancement of deep learning, speech synthesis models have demonstrated performance that significantly surpasses traditional methods[^Borgholt2022Brief].

These models typically convert input text into acoustic feature vectors through an encoder, and subsequently generate Mel-spectrograms using techniques such as attention mechanisms or variational inference, which are then transformed into speech by a vocoder[^Shen2018Natural], [^Kim2021Conditional].

The model learns the correspondence between audio samples and their respective “input representations.”

Constructing such deep learning-based speech synthesis systems requires accurately labeled data corresponding to spoken utterances.

In conventional approaches, phonemes are typically generated from sample text using grapheme-to-phoneme (G2P) conversion, which are then input into the speech synthesis model.

In the case of Japanese, where texts often contain a mix of kanji and kana, phonemes are generated from the mixed-script input, which are subsequently used to synthesize speech.

Specifically, some methods rely on rule-based systems to assign required TTS information—such as accent and prosody—based on morphological analysis, while others adopt neural G2P models using CTC or encoder-decoder structures to model the alignment between text and phoneme sequences of differing lengths.

Such transcription tasks are largely conducted manually.

While it is possible to incorporate additional information—such as accents and syllable durations—by referring to pronunciation or accent dictionaries, two major challenges remain in building G2P systems.

The first is the cost associated with data construction.

Generating phonetic elements requires various resources, including pronunciation and accent dictionaries and linguistic rules.

Since these supplementary inputs cannot be derived solely from raw text, they must be individually integrated into the system, thereby incurring high annotation costs.

The second challenge lies in the limited support for multilingual text.

When dealing with texts that include multiple languages, it becomes difficult for a single G2P model to provide adequate coverage.

This necessitates the development of separate models for each language, which further increases costs.

Additionally, pronunciation errors can arise when the same character is pronounced differently depending on the language, presenting a significant difficulty in multilingual speech synthesis.

To address these challenges, this study aims to develop a G2P-free, multilingual-capable speech synthesis model by utilizing discrete representations derived from speech self-supervised learning (SSL) models.

To achieve this goal, the following key aspects are investigated.

-  Performance comparison between input representation using SSL model and conventional G2P representation 
-  Implementation of G2P for mixed Kanji and Kana situations using discrete representation 
