# MixedG2P-T5: G2p-Free Speech Synthesis for Mixed-Script Texts Using Speech Self-Supervised Learning and Language Model

<details>
<summary>基本信息</summary>

- 标题: "MixedG2P-T5: G2p-Free Speech Synthesis for Mixed-Script Texts Using Speech Self-Supervised Learning and Language Model."
- 作者:
  - 01 Joonyong Park
  - 02 Daisuke Saito
  - 03 Nobuaki Minematsu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.01391v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2509.01391v1](PDF/2025.09.01_2509.01391v1_MixedG2P-T5__G2p-Free_Speech_Synthesis_for_Mixed-Script_Texts_Using_Speech_Self-Supervised_Learning_and_Language_Model.pdf)
  - [Publication] #TODO

</details>

## Abstract

This study presents a novel approach to voice synthesis that can substitute the traditional grapheme-to-phoneme (G2P) conversion by using a deep learning-based model that generates discrete tokens directly from speech.
Utilizing a pre-trained voice SSL model, we train a T5 encoder to produce pseudo-language labels from mixed-script texts (e.g., containing Kanji and Kana).
This method eliminates the need for manual phonetic transcription, reducing costs and enhancing scalability, especially for large non-transcribed audio datasets.
Our model matches the performance of conventional G2P-based text-to-speech systems and is capable of synthesizing speech that retains natural linguistic and paralinguistic features, such as accents and intonations.\\
