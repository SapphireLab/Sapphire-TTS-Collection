# Matcha-TTS

<details>
<summary>基本信息</summary>

- 标题: "Matcha-TTS: A Fast TTS Architecture with Conditional Flow Matching"
- 作者:
  - 01 Shivam Mehta
  - 02 Ruibo Tu
  - 03 Jonas Beskow
  - 04 Eva Szekely
  - 05 Gustav Eje Henter
- 链接:
  - [ArXiv](https://arxiv.org/abs/2309.03199v2)
  - [Publication](https://doi.org/10.1109/ICASSP48485.2024.10448291)
  - [Github](https://github.com/shivammehta25/Matcha-TTS)
  - [Demo](https://shivammehta25.github.io/Matcha-TTS/)
- 文件:
  - [ArXiv:2309.03199v1](PDF/2023.09.06_2309.03199v1__Matcha-TTS__A_Fast_TTS_Architecture_With_Conditional_Flow_Matching.pdf)
  - [ArXiv:2309.03199v2](PDF/2024.01.09_2309.03199v2__Matcha-TTS__A_Fast_TTS_Architecture_With_Conditional_Flow_Matching.pdf)
  - [Publication](PDF/2024.03.18_2309.03199p0__Matcha-TTS__ICASSP2024.pdf)

</details>

## Abstract: 摘要

We introduce ***Matcha-TTS***, a new encoder-decoder architecture for speedy TTS acoustic modelling, trained using optimal-transport conditional flow matching (OT-CFM).
This yields an ODE-based decoder capable of high output quality in fewer synthesis steps than models trained using score matching.
Careful design choices additionally ensure each synthesis step is fast to run.
The method is probabilistic, non-autoregressive, and learns to speak from scratch without external alignments.
Compared to strong pre-trained baseline models, the ***Matcha-TTS*** system has the smallest memory footprint, rivals the speed of the fastest model on long utterances, and attains the highest mean opinion score in a listening test.

## 1·Introduction: 引言

Diffusion probabilistic models (DPMs) ([^Song2019Generative]) are currently setting new standards in deep generative modelling on continuous-valued data-generation tasks such as image synthesis ([^Dhariwal2021Diffusion], [^Rombach2022High-Resolution]), motion synthesis ([^Alexanderson2023Listen,], [^Mehta2023D}iff-{Ttsg]), and speech synthesis (**WaveGrad**[^Chen2020WaveGrad], **WaveGrad2**[^Chen2021WaveGrad2], **Grad-TTS**[^Popov2021Grad-TTS], **Diff-TTS**[^Jeong2021Diff-TTS], **DiffWave**[^Kong2020DiffWave]) -- the topic of this paper.
DPMs define a diffusion process which transforms the \emph{data} (a.k.a.\ \emph{target}) distribution to a \emph{prior} (a.k.a.\ \emph{source}) distribution, e.g., a Gaussian.
They then learn a \emph{sampling process} that reverses the diffusion process.
The two processes can be formulated as forward- and reverse-time stochastic differential equations (SDEs) [^Song2020Score].
Solving a reverse-time SDE initial value problem  generates samples from the learnt data distribution.
Furthermore, each reverse-time SDE has a corresponding ordinary differential equation (ODE), called the probability flow ODE ([^Song2020Score], **InterFlow**[^Albergo2022InterFlow]), which describes (and samples from) the exact same distribution as the SDE.
The probability flow ODE is a deterministic process for turning source samples into data samples, similar to continuous-time normalising flows (CNF) [^Chen2018Neural], but without the need to backpropagate through expensive ODE solvers or approximate the reverse ODE using adjoint variables [^Chen2018Neural].

The SDE formulation of DPMs is trained by approximating the score function (the gradients of the log probability density) of the data distribution [^Song2020Score].
The training objective takes the form of a mean squared error (MSE) which can be derived from an evidence lower bound (ELBO) on the likelihood.
This is fast and simple and, unlike typical normalising flow models, does not impose any restrictions on model architecture.
But whilst they allow efficient training without numerical SDE/ODE solvers, DPMs suffer from slow synthesis speed, since each sample requires numerous iterations (steps), computed in sequence, to accurately solve the SDE.
Each such step requires that an entire neural network be evaluated.
This slow synthesis speed has long been the main practical issue with DPMs.

This paper introduces ***Matcha-TTS***, a probabilistic and non-autoregressive, fast-to-sample-from TTS acoustic model based on continuous normalising flows.
We call our approach Matcha-TTS because it uses flow matching for TTS, and because the name sounds similar to "matcha tea", which some people prefer over Taco(tron)s.

There are two main innovations:
- To begin with, we propose an improved encoder-decoder TTS architecture that uses a combination of 1D CNNs and Transformers in the decoder.
This reduces memory consumption and is fast to evaluate, improving synthesis speed.
- Second, we train these models using **optimal-transport conditional flow matching (OT-CFM)**[^Lipman2022FM],
which is a new method to learn ODEs that sample from a data distribution.
Compared to conventional CNFs and score-matching probability flow ODEs, OT-CFM defines simpler paths from source to target, enabling accurate synthesis in fewer steps than DPMs.

Experimental results demonstrate that both innovations accelerate synthesis, reducing the trade-off between speed and synthesis quality.
Despite being fast and lightweight, Matcha-TTS learns to speak and align without requiring an external aligner.
Compared to strong pre-trained baseline models, Matcha-TTS achieves fast synthesis with better naturalness ratings.
Audio examples and code are provided at [url](https://shivammehta25.github.io/Matcha-TTS/).

## 2·Background

### Recent encoder-decoder TTS architectures

DPMs have been applied to numerous speech-synthesis tasks with impressive results, including waveform generation (**WaveGrad**[^Chen2020WaveGrad], **DiffWave**[^Kong2020DiffWave]) and end-to-end TTS (**WaveGrad2**[^Chen2021WaveGrad2]).
**Diff-TTS**[^Jeong2021Diff-TTS] was first to apply DPMs for acoustic modelling.
Shortly after,  **Grad-TTS**[^Popov2021Grad-TTS] conceptualised the diffusion process as an SDE.
Although these models, and descendants like **Fast Grad-TTS**[^Vovk2022FastGrad-TTS], are non-autoregressive, **TorToiSe-TTS**[^Betker2023TorToiSe-TTS] demonstrated DPMs in an autoregressive TTS model with quantised latents.

The above models -- like many modern TTS acoustic models -- use an encoder-decoder architecture with Transformer blocks in the encoder.
Many models, e.g., **FastSpeech**[^Ren2019FastSpeech] and **FastSpeech2**[^Ren2020FastSpeech2], use sinusoidal position embeddings for positional dependences.
This has been found to generalise poorly to long sequences [^Press2022Train].
**Glow-TTS**[^Kim2020Glow-TTS], **VITS**[^Kim2021VITS], and **Grad-TTS**[^Popov2021Grad-TTS] instead use relative positional embeddings [^Shaw2018Self-Attention].
Unfortunately, these treat inputs outside a short context window as a "bag of words", often resulting in unnatural prosody.
LinearSpeech [^Zhang2021L}inear{S}peech] instead employed rotational position embeddings (RoPE) **RoFormer/RoPE**[^Su2021RoFormer], which have computational and memory advantages over relative embeddings and generalise to longer distances [^Wennberg2021Case], [^Press2022Train].
Matcha-TTS thus uses Transformers with RoPE in the encoder, reducing RAM use compared to Grad-TTS.
We believe ours is the first SDE or ODE-based TTS method to use RoPE.

Modern TTS architectures also differ in terms of decoder network design.
The normalising-flow based methods **Glow-TTS**[^Kim2020Glow-TTS] and  **OverFlow**[^Mehta2022OverFlow] use dilated 1D-convolutions.
DPM-based methods like **Diff-TTS**[^Jeong2021Diff-TTS], **ProDiff**[^Huang2022ProDiff] likewise use 1D convolutions to synthesise mel spectrograms.
**Grad-TTS**[^Popov2021Grad-TTS], in contrast, uses a U-Net with 2D-convolutions.
This treats mel spectrograms as images and implicitly assumes translation invariance in both time and frequency.
However, speech mel-spectra are not fully translation-invariant along the frequency axis, and 2D decoders generally require more memory as they introduce an extra dimension to the tensors.
Meanwhile, non-probabilistic models like FastSpeech 1 and 2 have demonstrated that decoders with (1D) Transformers can learn long-range dependencies and fast, parallel synthesis.
Matcha-TTS also uses Transformers in the decoder, but in a 1D U-Net design inspired by the 2D U-Nets in the Stable Diffusion image-generation model [^Rombach2022High-Resolution].

Whilst some TTS systems, e.g., **FastSpeech**[^Ren2019FastSpeech], rely on externally-supplied alignments, most systems are capable of learning to speak and align at the same time, although it has been found to be important to encourage or enforce monotonic alignments ([^Watts2019Where], **Neural HMM TTS**[^Mehta2021Neural]) for fast and effective training.
One mechanism for this is monotonic alignment search (MAS), used by, e.g., **Glow-TTS**[^Kim2020Glow-TTS] and **VITS**[^Kim2021VITS].
**Grad-TTS**[^Popov2021Grad-TTS], in particular, uses a MAS-based mechanism which they term \emph{prior loss} to quickly learn to align input symbols with output frames.
These alignments are also used to train a deterministic duration predictor minimising MSE in the log domain.
Matcha-TTS uses these same methods for alignment and duration modelling.
Finally, Matcha-TTS differs by using \emph{snake beta} activations from **BigVGAN**[^Lee2022BigVGAN] in all decoder feedforward layers.

### Flow matching and TTS

Currently, some of the highest-quality TTS systems either utilise DPMs (**Grad-TTS**[^Popov2021Grad-TTS], **TorToiSe-TTS**[^Betker2023TorToiSe-TTS]) or discrete-time normalising flows (**VITS**[^Kim2021VITS], **OverFlow**[^Mehta2022OverFlow]), with continuous-time flows being less explored.
**FM**[^Lipman2022FM] recently introduced a framework for synthesis using ODEs that unifies and extends probability flow ODEs and CNFs.
They were then able to present an efficient approach to learn ODEs for synthesis, using a simple vector-field regression loss called \emph{conditional flow matching} (CFM), as an alternative to learning score functions for DPMs or using numerical ODE solvers at training time like classic CNFs [^Chen2018Neural].
Crucially, by leveraging ideas from optimal transport, CFM can be set up to yield ODEs that have simple vector fields that change little during the process of mapping samples from the source distribution onto the data distribution, since it essentially just transports probability mass along straight lines.
This technique is called \emph{OT-CFM};
**RectifiedFlow**[^Liu2022RectifiedFlow] represent concurrent work with a similar idea.
The simple paths mean that the ODE can be solved accurately using few discretisation steps, i.e., accurate model samples can be drawn with fewer neural-network evaluations than DPMs, enabling much faster synthesis for the same quality.

CFM is a new technique that differs from earlier approaches to speed up SDE/ODE-based TTS, which most often were based on distillation (e.g., **ProDiff**[^Huang2022ProDiff], **Fast Grad-TTS**[^Vovk2022FastGrad-TTS], **CoMoSpeech**[^Ye2023CoMoSpeech]).
Prior to Matcha-TTS, the only public preprint on CFM-based acoustic modelling was the **VoiceBox**[^Le2023VoiceBox] model from Meta.
Voicebox (VB) is a system that performs various text-guided speech-infilling tasks based on large-scale training data, with its English variant (VB-En) being trained on 60k hours of proprietary data.
VB differs substantially from Matcha-TTS:
VB performs TTS, denoising, and text-guided acoustic infilling trained using a combination of masking and CFM, whereas Matcha-TTS is a pure TTS model trained solely using OT-CFM.
VB uses convolutional positional encoding with **AliBi**[^Press2022Train] self-attention bias, whilst our text encoder uses RoPE.
In contrast to VB, we train on standard data and make code and checkpoints publicly available.
VB-En consumes 330M parameters, which is 18 times larger than the Matcha-TTS model in our experiments.
Also, VB uses external alignments for training whereas Matcha-TTS learns to speak without them.

## 3·Method

We now outline flow-matching training (in \cref{ssec:cfm}) and then (in \cref{ssec:matcha}) give details on our proposed TTS architecture.

### Optimal-transport conditional flow matching

We here give a high-level overview of flow matching, first introducing the probability-density path generated by a vector field and then leading into the OT-CFM objective used in our proposed method.
Notation and definitions mainly follow **FM**[^Lipman2022FM].
Let $\x$ denote an observation in the data space $\real^d$, sampled from a complicated, unknown data distribution $q(\x)$.
A \emph{probability density path} is a time-dependent probability density function, $p_t: [0,1]\times \real^d \rightarrow \real>0$.
One way to generate samples from the data distribution $q$ is to construct a probability density path $p_t$, where $t \in [0,1]$ and $p_0(\x) = \mathcal{N}(\x; \boldsymbol{0},\boldsymbol{I})$ is a prior distribution, such that $p_1(\x)$ approximates the data distribution $q(\x)$.
For example, CNFs first define a vector field $\boldsymbol{v}_t: [0, 1] \times \real^d \rightarrow \real^d$, which generates the flow $\phi_t: [0, 1] \times \real^d \rightarrow \real^d$ through the ODE

$$
\begin{aligned}
\tfrac{d}{dt}\phi_t(\x)
& = \boldsymbol{v}_t(\phi_t(\x))\text{;}
\qquad\phi_0(\x) = \x.
\end{aligned}
$$

This generates the path $p_t$ as the marginal probability distribution of the data points.
We can sample from the approximated data distribution $p_1$ by solving the initial value problem in Eq.\ \eqref{eq:ode}.
Suppose there exists a known vector field $\boldsymbol{u}_t$ that generates a probability path $p_t$ from $p_0$ to $p_1\approx q$.
The flow matching loss is

$$
\begin{aligned}
\mathcal{L}_{\mathrm{FM}}(\theta) & = \mathbb{E}_{t, p_t(\x)}\Vert \boldsymbol{u}_t(\x) - \boldsymbol{v}_t(\x; \theta) \Vert^2
\end{aligned}
$$

where $t\sim\mathbb{U}[0,1]$ and $\boldsymbol{v}_t(\x; \theta)$ is a neural network with parameters $\theta$.
Nevertheless, flow matching is intractable in practice because it is non-trivial to get access to the vector field $\boldsymbol{u}_t$ and the target probability $p_t$.
Therefore, conditional flow matching instead considers

$$
\begin{aligned}
\mathcal{L}_{\mathrm{CFM}} (\theta) & = \mathbb{E}_{t, q(\x_1),p_t(\x|\x_1)}\Vert \boldsymbol{u}_t(\x\vert \x_1) - \boldsymbol{v}_t(\x; \theta) \Vert^2.
\end{aligned}
$$

This replaces the intractable marginal probability densities and the vector field with conditional probability densities and conditional vector fields.
Crucially, these are in general tractable and have closed-form solutions, and one can furthermore show that $\mathcal{L}_{\mathrm{CFM}}(\theta)$ and $\mathcal{L}_{\mathrm{FM}}(\theta)$ both have identical gradients with respect to $\theta$ **FM**[^Lipman2022FM].

Matcha-TTS is trained using **optimal-transport conditional flow matching (OT-CFM)**[^Lipman2022FM], which is a CFM variant with particularly simple gradients.
The OT-CFM loss function can be written

$$
\begin{aligned}
\mathcal{L} (\theta) & =
\mathbb{E}_{t, q(\x_1),p_0(\x_0)}
\Vert\boldsymbol{u}^{\mathrm{OT}}_t(\phi^{\mathrm{OT}}_t(\x)| \x_1)- \boldsymbol{v}_t(\phi^{\mathrm{OT}}_t(\x) | \boldsymbol{\mu}; \theta) \Vert^2
\text{,}
\end{aligned}
$$

defining $\phi^{\mathrm{OT}}_t(\x) = (1 - (1-\sigma_{\mathrm{min}})t)\x_0 + t \x_1$ as the flow from $\x_0$ to $\x_1$ where each datum $\x_1$ is matched to a random sample $\x_0\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I})$ as in **FM**[^Lipman2022FM].
Its gradient vector field -- whose expected value is the target for the learning -- is then $\boldsymbol{u}^{\mathrm{OT}}_t(\phi^{\mathrm{OT}}_t(\x_0)\vert \x_1) = \x_1-(1-\sigma_{\mathrm{min}})\x_0$, which is linear, time-invariant, and only depends on $\x_0$ and $\x_1$.
These properties enable easier and faster training, faster generation, and better performance compared to DPMs.
In the case of Matcha-TTS, $\x_1$ are acoustic frames and $\boldsymbol{\mu}$ are the conditional mean values of those frames, predicted from text using the architecture described in the next section.
$\sigma_{\mathrm{min}}$ is a hyperparameter with a small value (\texttt{1e-4} in our experiments).

### Proposed architecture

> <a id="fig: model architecture overview">![](architectural_overview.pdf)</a>
> Fig.01: Overview of the proposed approach at synthesis time.

Matcha-TTS is a non-autoregressive encoder-decoder architecture for neural TTS.
An overview of the architecture is provided in \cref{fig: model architecture overview}.
Text encoder and duration predictor architectures follow **Glow-TTS**[^Kim2020Glow-TTS], **Grad-TTS**[^Popov2021Grad-TTS], but use rotational position embeddings (**RoFormer/RoPE**[^Su2021RoFormer]) instead of relative ones.
Alignment and duration-model training follow use MAS and the prior loss $\mathcal{L}_{\mathrm{enc}}$ as described in **Grad-TTS**[^Popov2021Grad-TTS].
The predicted durations, rounded up, are used to upsample (duplicate) the vectors output by the encoder to obtain $\boldsymbol{\mu}$, the predicted average acoustic features (e.g., mel-spectrogram) given the text and the chosen durations.
This mean is used to condition the decoder that predicts the vector field $\boldsymbol{v}_t(\phi^{\mathrm{OT}}_t(\x_0) | \boldsymbol{\mu}; \theta)$ used for synthesis, but is not used as the mean for the initial noise samples $\x_0$ (unlike Grad-TTS).

\cref{fig: decoder architecture} shows the Matcha-TTS decoder architecture.
Inspired by [^Rombach2022High-Resolution], it is a U-Net containing 1D convolutional residual blocks to downsample and upsample the inputs,
with the flow-matching step $t\in[0,1]$ embedded as in **Grad-TTS**[^Popov2021Grad-TTS].
Each residual block is followed by a Transformer block, whose feedforward nets use snake beta activations (**BigVGAN**[^Lee2022BigVGAN]).
These Transformers do not use any position embeddings, since between-phone positional information already has been baked in by the encoder, and the convolution and downsampling operations act to interpolate these between frames within the same phone and distinguish their relative positions from each other.
This decoder network is significantly faster to evaluate and consumes less memory than the 2D convolutional-only U-Net used by **Grad-TTS**[^Popov2021Grad-TTS].

## 4·Experiments

To validate the proposed approach we compared it to three pre-trained baselines in several experiments, including a listening test.
All experiments were performed on NVIDIA RTX 3090 GPUs.
See \href{\webpageurl}{\webpageurltext} for audio and code.

> <a id="fig: decoder architecture">![](decoder_architecture.pdf)</a>
> Fig.02: Matcha-TTS decoder (the flow-prediction network in \cref{fig: model architecture overview}).

### Data and systems

We performed our experiments on the standard split of the LJ Speech dataset\footnote{\customurl{https://keithito.com/LJ-Speech-Dataset/}} (a female US English native speaker reading public-domain texts), training a version of the Matcha-TTS architecture on this data.
We used the same encoder and duration predictor (i.e., the same hyperparameters) as **Grad-TTS**[^Popov2021Grad-TTS], just different position embeddings in the encoder.
Our trained flow-prediction network (decoder) used two downsampling blocks, followed by two midblocks and two upsampling blocks, as shown in \cref{fig: decoder architecture}.
Each block had one Transformer layer with hidden dimensionality 256, 2 heads, attention dimensionality 64, and `snakebeta' activations (**BigVGAN**[^Lee2022BigVGAN]).
Phonemizer\footnote{\customurl{https://github.com/bootphon/phonemizer}} [^Bernard2021P}honemizer] with the \texttt{espeak-ng} backend was used to convert input graphemes to IPA phones.
We trained for 500k updates on 2 GPUs with batch size 32 and learning rate \texttt{1e-4}, labelling our trained system **MAT**.

MAT was compared to three widely used neural TTS baseline approaches with pre-trained checkpoints available for LJ Speech, namely **Grad-TTS**[^Popov2021Grad-TTS]([Github](https://github.com/huawei-noah/Speech-Backbones/tree/main/Grad-TTS)) (label **GRAD**), a strong DPM-based acoustic model, FastSpeech2 (**FS2**), a fast non-probabilistic acoustic model, and **VITS** ([Github](https://github.com/jaywalnut310/vits)), a strong probabilistic end-to-end TTS system with discrete-time normalising flows.
The baselines used the official checkpoints from the respective linked repositories.
For FS2, which does not provide an official implementation, we instead used the checkpoint from Meta's FairSeq ([Github](https://github.com/facebookresearch/fairseq)).
To decouple the effects of CFM training from those due to the new architecture, we also trained the GRAD architecture using the OT-CFM objective instead, using the same optimiser hyperparameters as for MAT.
This produced the ablation labelled **GCFM**.
For all acoustic models (i.e., all systems except VITS), we used the pre-trained **HiFi-GAN**[^Kong2020HiFi-GAN]
LJ Speech checkpoint `LJ_V1` ([Github](https://github.com/jik876/hifi-gan/)) for waveform generation, with a denoising filter as introduced in **WaveGlow**[^Prenger2018WaveGlow] at a strength of \texttt{2.5e-4}.
As a top line, our experiments also included vocoded held-out speech, labelled **VOC**.%

<a id="tab:results"></a>
Conditions in the evaluation (with the NFE for ODE-based methods) and their number of parameters, minimum GPU RAM needed to train (GiB), real-time factor (including vocoding time) on the test set, ASR WER in percent, and mean opinion score with 95\%-confidence interval.
The best TTS condition in each column is bold.
The parameter count and RTF for VOC pertain to the vocoder.

ODE-based models, e.g., DPMs, allow trading off speed against quality.
We therefore evaluated synthesis from the trained ODE-based systems with a different number of steps for the ODE solver.
Like **Grad-TTS**[^Popov2021Grad-TTS], we used the first-order Euler forward ODE-solver, where the number of steps is equal to the number of function (i.e., neural-network) evaluations, commonly abbreviated \emph{NFE}.
This gave rise to multiple \emph{conditions} for some systems.
We labelled these conditions **MAT-$\boldsymbol{n**$}, **GRAD-$\boldsymbol{n**$}, and **GCFM-$\boldsymbol{n**$}, $n$ being the NFE.
We used NFE 10 or less, since **Grad-TTS**[^Popov2021Grad-TTS] reported that NFE 10 and 100 gave the same MOS for Grad-TTS (NFE 50 is the official code default).
All conditions used a temperature of 0.667 for synthesis, similar to **Grad-TTS**[^Popov2021Grad-TTS].
\cref{tab:results} provides an overview of the conditions in the evaluation.

### Evaluations, results, and discussion

We evaluated our approach both objectively and subjectively.
First we measured parameter count and maximum memory use during training (at batch size 32 and fp16) of all systems, with results listed in \cref{tab:results}.
We see that MAT is approximately the same size as GRAD/GCFM, and smaller than all other systems.
In particular, it is smaller than VITS also after adding the vocoder (13.9M parameters) to the MAT parameter count.
More importantly, it uses less memory than all baselines, which (more than parameter count) is the main limiter on how large and powerful models that can be trained.

After training the systems, we assessed the synthesis speed and intelligibility of the different conditions, by computing the real time factor (RTF) mean and standard deviation when synthesising the test set, and evaluating the word error rate (WER) when applying the **Whisper** \texttt{medium}[^Radford2022Whisper] ASR system to the results, since the WERs of strong ASR systems correlate well with intelligibility [^Taylor2021Confidence].
The results, in \cref{tab:results}, suggest that MAT is the most intelligible system, even using only two synthesis steps.
MAT is also much faster than VITS, equally fast or faster than GRAD/GCFM at the same NFE, and only slightly slower than FS2 when at the fastest setting.

To evaluate the naturalness of the synthesised audio we ran a mean opinion score (MOS) listening test.
We selected 40 utterances (4 groups of 10) of different lengths from the test set and synthesised each utterance using all conditions, loudness-normalising every stimulus using EBU R128.
80 subjects (self-reported as native English speakers using headphones) were crowdsourced through \href{https://prolific.co/}{Prolific} to listen to and rate these stimuli.
For each stimulus, listeners were asked "How natural does the synthesised speech sound?", and provided responses on an integer rating scale from 1 ("Completely unnatural") to 5 ("Completely natural") adopted from the Blizzard Challenge [^Prahallad2013B}lizzard].
Each group of 10 utterances was evaluated by 20 listeners, who were paid \textsterling 3 for a median completion time of 13 mins.
Inattentive listeners were filtered out and replaced in exactly the same way as in **OverFlow**[^Mehta2022OverFlow].
In the end we obtained 800 ratings for each condition.
The resulting MOS values, along with confidence intervals based on a normal approximation, are listed in \cref{tab:results}.
We note that, since MOS values depend on many variables external to stimulus quality, e.g., listener demographics and instructions (see [^Chiang2023Why], [^Kirkland2023Stuck]), they should not be treated as an absolute metric.
Comparing our MOS values to other papers is thus unlikely to be meaningful.%

> <a id="fig: rtf slopes">![](time_vs_txtlength.pdf)</a>
> Scatterplot of prompt length vs.\ synthesis time for acoustic models.
> Regression lines show as curves due to the log-log axes.

Applying $t$-tests to all pairs of conditions, all differences were found to be statistically significant at the $\alpha=0.05$ level except the pairs (MAT-10,MAT-4), (MAT-4,VITS), (VITS,MAT-2), (MAT-2,GCFM-4), and (GCFM-4,GRAD-10).
This means that MAT always had significantly better rated naturalness than GRAD for the same NFE, and always surpassed FS2.
Both the new architecture and training method contributed to the naturalness improvement, since MAT-4>GCFM-4>GRAD-4.
The fact that GRAD-10 was much better than GRAD-4 whilst MAT-10 and MAT-4 performed similarly suggests that GRAD requires many steps for good synthesis quality, whereas MAT reached a good level in fewer steps.
Finally, VITS performed similarly to MAT-2 and MAT-4 in terms of MOS.
MAT-10, although close to MAT-4 in rating, was significantly better than VITS.
For any given $n$, MAT-$n$ always scored higher than any system with equal or faster RTF.
In summary, Matcha-TTS achieved similar or better naturalness than all comparable baselines.

Finally, we evaluated how synthesis speed scaled with utterance length for the different models, by generating 180 sentences of different lengths using a GPT-2\footnote{\customurl{https://huggingface.co/gpt2}} model and plotting wall-clock synthesis time in \cref{fig: rtf slopes}, also fitting least-squares regression lines to the data.
The results show that MAT-2 synthesis speed becomes competitive with FS2 at longer utterances, with MAT-4 not far behind.
The major contributor to this appears to be the new architecture (since GRAD-4 and GCFM-4 both are much slower), and the gap from MAT to GRAD only grows with longer utterances.

## 5·Conclusions and Future Work

We have introduced Matcha-TTS, a fast, probabilistic, and high-quality ODE-based TTS acoustic model trained using conditional flow matching.
The approach is non-autoregressive, memory efficient, and jointly learns to speak and align.
Compared to three strong pre-trained baselines, Matcha-TTS provides superior speech naturalness and can match the speed of the fastest model on long utterances.
Our experiments show that both the new architecture and the new training contribute to these improvements.
Compelling future work includes making the model multi-speaker, adding probabilistic duration modelling, and applications to challenging, diverse data such as spontaneous speech [^Sz{\'e}kely2019Spontaneous].

## References

[^Song2019Generative]: Generative Modeling by Estimating Gradients of the Data Distribution. Proc. NeurIPS 2019.
[^Dhariwal2021Diffusion]: Diffusion Models Beat {GAN}s on Image Synthesis. Proc. NeurIPS 2021.
[^Rombach2022High-Resolution]: High-Resolution Image Synthesis With Latent Diffusion Models. Proc. CVPR 2022.
[^Alexanderson2023Listen,]: Listen, Denoise, Action! {A}udio-Driven Motion Synthesis With Diffusion Models. ACM Trans. Graph. 2023.
[^Mehta2023D}iff-{Ttsg]: {D}iff-{Ttsg}: Denoising Probabilistic Integrated Speech and Gesture Synthesis. Proc. SSW 2023.
[^Chen2020WaveGrad]: [**WaveGrad**: Estimating Gradients for Waveform Generation](../Vocoder/2020.09.02_WaveGrad.md). ArXiv:2009.00713v2/ICLR2021.
[^Chen2021WaveGrad2]: [**WaveGrad2**: Iterative Refinement for Text-to-Speech Synthesis](../Vocoder/2021.06.17_WaveGrad2.md). ArXiv:2106.09660v2/InterSpeech2021.
[^Popov2021Grad-TTS]: [**Grad-TTS**: A Diffusion Probabilistic Model for Text-to-Speech](../Acoustic/2021.05.13_Grad-TTS.md). ArXiv:2105.06337v2/ICML2021.
[^Jeong2021Diff-TTS]: [**Diff-TTS**: A Denoising Diffusion Model for Text-to-Speech](../Acoustic/2021.04.03_Diff-TTS.md). ArXiv:2104.01409v1/InterSpeech2021.
[^Kong2020DiffWave]: [**DiffWave**: A Versatile Diffusion Model for Audio Synthesis](../Vocoder/2020.09.21_DiffWave.md). ArXiv:2009.09761v3/ICLR2021Oral.
[^Song2020Score]: [Score-Based Generative Modeling through Stochastic Differential Equations](../Diffusion/_2020.11.26_Song2020Score.md). ArXiv:2011.13456v2/ICLR2021Oral.
[^Albergo2022InterFlow]: [**InterFlow**: Building Normalizing Flows with Stochastic Interpolants](_2022.09.30_InterFlow.md). ArXiv:2209.15571v3/ICLR2023.
[^Chen2018Neural]: Neural Ordinary Differential Equations. Proc. NeurIPS 2018.
[^Lipman2022FM]: [**FM**: Flow Matching for Generative Modeling](_2022.10.06_Flow_Matching.md). ArXiv:2210.02747v2.
[^Vovk2022FastGrad-TTS]: [**Fast Grad-TTS**: Towards Efficient Diffusion-Based Speech Generation on CPU](../Acoustic/2022.09.18_Fast_Grad-TTS.md). InterSpeech2022.
[^Betker2023TorToiSe-TTS]: [**TorToiSe-TTS**: Better Speech Synthesis Through Scaling](../Diffusion/2023.05.12_TorToise-TTS.md). ArXiv:2305.07243v2.
[^Ren2019FastSpeech]: [**FastSpeech**: Fast Robust and Controllable Text to Speech](../Acoustic/2019.05.22_FastSpeech.md). ArXiv:1905.09263/NeurIPS2019.
[^Ren2020FastSpeech2]: [**FastSpeech2**: Fast and High-Quality End-to-End Text-to-Speech](../Acoustic/2020.06.08_FastSpeech2.md). ArXiv:2006.04558/ICLR2021.
[^Press2022Train]: Train Short, Test Long: Attention With Linear Biases Enables Input Length Extrapolation. Proc. ICLR 2022.
[^Kim2020Glow-TTS]: [**Glow-TTS**: A Generative Flow for Text-to-Speech via Monotonic Alignment Search](../Acoustic/2020.05.22_Glow-TTS.md). ArXiv:2005.11129v2/NeurIPS2020.
[^Kim2021VITS]: [**VITS**: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech](../E2E/2021.06.11_VITS.md). ArXiv:2106.06103/ICML2021.
[^Shaw2018Self-Attention]: Self-Attention With Relative Position Representations. Proc. NAACL 2018.
[^Zhang2021L}inear{S}peech]: {L}inear{S}peech: Parallel Text-to-Speech With Linear Complexity. Proc. Interspeech 2021.
[^Su2021RoFormer]: [**RoFormer/RoPE**: Enhanced Transformer with Rotary Position Embedding](../_Basis/2021.04.20_RoFormer.md). ArXiv:2104.09864v5/Neurocomputing2024.
[^Wennberg2021Case]: The Case for Translation-Invariant Self-Attention in {T}ransformer-Based Language Models. Proc. ACL-IJCNLP Vol. 2 2021.
[^Mehta2022OverFlow]: [**OverFlow**: Putting flows on top of neural transducers for better TTS](../Acoustic/2022.11.13_OverFlow.md). ArXiv:2211.06892v2/InterSpeech2023.
[^Huang2022ProDiff]: [**ProDiff**: Progressive Fast Diffusion Model For High-Quality Text-to-Speech](../Diffusion/2022.07.13_ProDiff.md). ArXiv:2207.06389v1/ACM Multimedia 2022.
[^Watts2019Where]: Where Do the Improvements Come From in Sequence-to-Sequence Neural {TTS}?. Proc. SSW 2019.
[^Mehta2021Neural]: [**Neural HMM TTS**: Neural HMMs Are All You Need (for High-Quality Attention-Free TTS)](../Acoustic/2021.08.30_Neural_HMM_TTS.md). ArXiv:2108.13320v6.
[^Lee2022BigVGAN]: [**BigVGAN**: Large-Scale Generative Adversarial Networks for High-Fidelity Audio Synthesis](../Vocoder/2022.06.09_BigVGAN.md). ArXiv:2206.04658/ICLR2023Poster.
[^Liu2022RectifiedFlow]: [**RectifiedFlow**: Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow](_2022.09.07_RectifiedFlow.md). ArXiv:2209.03003v1.
[^Ye2023CoMoSpeech]: [**CoMoSpeech**: One-Step Speech and Singing Voice Synthesis via Consistency Model](../Consistency/2023.05.11_CoMoSpeech.md). ArXiv:2305.06908v4/ACM MM2023.
[^Le2023VoiceBox]: [**VoiceBox**: Text-Guided Multilingual Universal Speech Generation at Scale](2023.06.23_Voicebox.md). ArXiv:2306.15687/NeurIPS2023Poster.
[^Bernard2021P}honemizer]: {P}honemizer: Text to Phones Transcription for Multiple Languages in {P}ython. J. Open Source Softw. 2021.
[^Kong2020HiFi-GAN]: [**HiFi-GAN**: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis](../Vocoder/2020.10.12_HiFi-GAN.md). ArXiv:2010.05646/NeurIPS2020.
[^Prenger2018WaveGlow]: [**WaveGlow**: A Flow-based Generative Network for Speech Synthesis](../Vocoder/2018.10.31_WaveGlow.md). ArXiv:1811.00002v1/ICASSP2019.
[^Radford2022Whisper]: [**Whisper**: Robust Speech Recognition via Large-Scale Weak Supervision](../-ASR/2022.12.06_Whisper.md). ArXiv:2212.04356/ICML2023.
[^Taylor2021Confidence]: Confidence Intervals for {ASR}-Based {TTS} Evaluation. Proc. Interspeech 2021.
[^Prahallad2013B}lizzard]: The {B}lizzard {C}hallenge 2013 -- {I}ndian Language Task. Proc. Blizzard Challenge Workshop 2013.
[^Chiang2023Why]: Why We Should Report the Details in Subjective Evaluation of {TTS} More Rigorously. Proc. Interspeech 2023.
[^Kirkland2023Stuck]: Stuck in the {MOS} Pit: A Critical Analysis of {MOS} Test Methodology in {TTS} Evaluation. Proc. SSW 2023.
[^Sz{\'e}kely2019Spontaneous]: Spontaneous Conversational Speech Synthesis From Found Data. Proc. Interspeech 2019.