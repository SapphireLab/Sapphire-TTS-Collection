# DiFlow-TTS: Discrete Flow Matching With Factorized Speech Tokens for Low-Latency Zero-Shot Text-to-Speech

<details>
<summary>基本信息</summary>

- 标题: "DiFlow-TTS: Discrete Flow Matching With Factorized Speech Tokens for Low-Latency Zero-Shot Text-to-Speech."
- 作者:
  - 01 Ngoc-Son Nguyen
  - 02 Hieu-Nghia Huynh-Nguyen
  - 03 Thanh V. T. Tran
  - 04 Truong-Son Hy
  - 05 Van Nguyen
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.09631v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2509.09631v1](PDF/2025.09.11_2509.09631v1_DiFlow-TTS__Discrete_Flow_Matching_With_Factorized_Speech_Tokens_for_Low-Latency_Zero-Shot_Text-to-Speech.pdf)
  - [ArXiv:2509.09631v2](PDF/2025.09.12_2509.09631v2_DiFlow-TTS__Discrete_Flow_Matching_With_Factorized_Speech_Tokens_for_Low-Latency_Zero-Shot_Text-to-Speech.pdf)
  - [Publication] #TODO

</details>

## Abstract

Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that mimics the voice of an unseen speaker using only a short reference sample, requiring not only speaker adaptation but also accurate modeling of prosodic attributes.
Recent approaches based on language models, diffusion, and flow matching have shown promising results in zero-shot TTS, but still suffer from slow inference and repetition artifacts.
Discrete codec representations have been widely adopted for speech synthesis, and recent works have begun to explore diffusion models in purely discrete settings, suggesting the potential of discrete generative modeling for speech synthesis.
However, existing flow-matching methods typically embed these discrete tokens into a continuous space and apply continuous flow matching, which may not fully leverage the advantages of discrete representations.
To address these challenges, we introduce *DiFlow-TTS*, which, to the best of our knowledge, is the first model to explore purely Discrete Flow Matching for speech synthesis. *DiFlow-TTS* explicitly models factorized speech attributes within a compact and unified architecture.
It leverages in-context learning by conditioning on textual content, along with prosodic and acoustic attributes extracted from a reference speech, enabling effective attribute cloning in a zero-shot setting.
In addition, the model employs a factorized flow prediction mechanism with distinct heads for prosody and acoustic details, allowing it to learn aspect-specific distributions.
Experimental results demonstrate that *DiFlow-TTS* achieves promising performance in several key metrics, including naturalness, prosody, preservation of speaker style, and energy control.
It also maintains a compact model size and achieves low-latency inference, generating speech up to 25.8 times faster than the latest existing baselines.
Code and audio samples are available on our demo page\footnote{\url{https://diflow-tts.github.io}}.
