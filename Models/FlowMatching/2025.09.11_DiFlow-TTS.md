# DiFlow-TTS: Discrete Flow Matching With Factorized Speech Tokens for Low-Latency Zero-Shot Text-to-Speech

<details>
<summary>基本信息</summary>

- 标题: "DiFlow-TTS: Discrete Flow Matching With Factorized Speech Tokens for Low-Latency Zero-Shot Text-to-Speech."
- 作者:
  - 01 Ngoc-Son Nguyen
  - 02 Hieu-Nghia Huynh-Nguyen
  - 03 Thanh V. T. Tran
  - 04 Truong-Son Hy
  - 05 Van Nguyen
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.09631v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2509.09631v1](PDF/2025.09.11_2509.09631v1_DiFlow-TTS__Discrete_Flow_Matching_With_Factorized_Speech_Tokens_for_Low-Latency_Zero-Shot_Text-to-Speech.pdf)
  - [ArXiv:2509.09631v2](PDF/2025.09.12_2509.09631v2_DiFlow-TTS__Discrete_Flow_Matching_With_Factorized_Speech_Tokens_for_Low-Latency_Zero-Shot_Text-to-Speech.pdf)
  - [Publication] #TODO

</details>

## Abstract

Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that mimics the voice of an unseen speaker using only a short reference sample, requiring not only speaker adaptation but also accurate modeling of prosodic attributes.
Recent approaches based on language models, diffusion, and flow matching have shown promising results in zero-shot TTS, but still suffer from slow inference and repetition artifacts.
Discrete codec representations have been widely adopted for speech synthesis, and recent works have begun to explore diffusion models in purely discrete settings, suggesting the potential of discrete generative modeling for speech synthesis.
However, existing flow-matching methods typically embed these discrete tokens into a continuous space and apply continuous flow matching, which may not fully leverage the advantages of discrete representations.
To address these challenges, we introduce *DiFlow-TTS*, which, to the best of our knowledge, is the first model to explore purely Discrete Flow Matching for speech synthesis. *DiFlow-TTS* explicitly models factorized speech attributes within a compact and unified architecture.
It leverages in-context learning by conditioning on textual content, along with prosodic and acoustic attributes extracted from a reference speech, enabling effective attribute cloning in a zero-shot setting.
In addition, the model employs a factorized flow prediction mechanism with distinct heads for prosody and acoustic details, allowing it to learn aspect-specific distributions.
Experimental results demonstrate that *DiFlow-TTS* achieves promising performance in several key metrics, including naturalness, prosody, preservation of speaker style, and energy control.
It also maintains a compact model size and achieves low-latency inference, generating speech up to 25.8 times faster than the latest existing baselines.
Code and audio samples are available on our demo page\footnote{\url{https://diflow-tts.github.io}}.

## 1·Introduction

Zero-shot TTS has seen significant advancements in recent years, aiming to generate high-quality speech that accurately mimics the voice of previously unseen speakers using only a few seconds of reference, with applications in personalized virtual assistants, accessibility tools for low-resource languages, and content creation.

Although large-scale generative models have made substantial progress in this area, several challenges remain. 

Recent studies have explored the application of language modeling [^Zhang2023Speak], [^Han2024Vall-E], [^Meng2025Autoregressive], [^Song2024Ella-V], [^Chen2024Vall-E], [^Peng2024V}oice{C}raft], [^Ji2024M}obile{S}peech], [^Wang2025Spark-TTS], [^Chen2025Neural], particularly through autoregressive (AR) approaches.

A pioneering work of this is VALL-E [^Chen2025Neural], which represents speech as discrete codec tokens and treats these tokens analogously to text, thus reformulating TTS as a conditional codec language modeling task.

Although such models demonstrate strong performance in terms of speech naturalness and speaker similarity, they typically require training on large-scale datasets to be effective.

Moreover, their autoregressive nature results in slow inference and introduces common artifacts, such as unintended repetition of content from the reference speech or missing initial words from the input text.

![](figures/overall-model.pdf)

<a id="fig:overview">Overview of DiLow-TTS.

The model decomposes the speech prompt into timbre, prosody, and acoustic tokens using a codec encoder.

Input text is processed by PCM to generate content tokens and embeddings.

The Factorized Discrete Flow Denoiser generates prosody, and acoustic tokens conditioned the content embeddings, speaker embedding, and the discrete prosody and acoustic tokens derived from speech prompt.

A codec decoder reconstructs the final waveform.</a>

To overcome these limitations, non-autoregressive (NAR) approaches have been developed, enabling faster generation through parallel decoding.

Among these, diffusion-based [^Kang2023ZET-Speech], [^Shen2024NaturalSpeech], [^Ju2024N}atural{S}peech], [^Lee2025Di{TT}o-{Tts] and flow-matching-based [^Kim2023P-Flow], [^Le2023Voicebox], [^Mehta2024Matcha-TTS], [^Eskimez2024E2], [^Chen2024F5-TTS] models have emerged as effective generative frameworks for TTS, striking a better balance between synthesis quality and inference efficiency.

These models typically operate in the mel-spectrogram domain, which preserves rich acoustic detail and enables in-context learning via target speech prompting, leading to improved speaker similarity.

To further reduce inference complexity, several works have adopted discrete codec tokens as intermediate representations.

For example, R-VC [^Zuo2025Rhythm] introduces a shortcut flow matching mechanism that allows generation in a few steps and even one step, significantly reducing inference latency.

Similarly, OZSpeech [^Hieu2025OZS}peech] achieves single-pass decoding by incorporating a prior as the initial stage and introducing an anchor loss during training to facilitate one-step sampling.

However, zero-shot TTS is not solely about capturing speaker identity; it also requires accurate modeling of prosodic characteristics from the reference speech.

A natural solution is to factorize the reference speech into attributes such as prosody, content, and acoustic details, and to model each of these components explicitly.

While OZSpeech attempts to decompose prompt speech into attribute representations and model them through flow matching, it does so in an implicit manner: treating all attributes equally and relying on the flow matching process to learn the separation implicitly.

This lack of explicit supervision results in entangled representations, which hinders the modelâ€™s ability to learn disentangled and controllable factors.

Consequently, the expressiveness and naturalness of synthesized speech are significantly limited.

Based on these observations, we pose the following question: ***Q1: How can we explicitly model factorized speech tokens in a compact and unified framework?*** 

In particular, recent efforts to adapt discrete codec tokens to generative paradigms have sparked a growing interest in applying diffusion models within fully discrete settings [^Ye2025Emotional], [^Ye2025Shushing!].

In contrast, flow-matching models have predominantly followed a single strategy: embedding discrete data into a continuous space followed by continuous flow matching [^Du2024Cosyvoice], [^Hieu2025OZS}peech], [^Wang2025Discl-Vc], [^Zuo2025Enhancing].

Although Discrete Flow Matching (DFM) [^Gat2024Discrete] has emerged as a promising generative paradigm in domains such as language, vision, and bioinformatics [^Shaul2025Flow], [^Yadav2025Retro], [^Fuest2025Maskflow], its application to speech synthesis is still limited.

This motivates a second question:
***Q2: Can purely Discrete Flow Matching achieve state-of-the-art zero-shot TTS performance while maintaining both naturalness and fidelity in synthesized speech?***

In this study, we propose **Di**screte **Flow**

Matching with Factorized Speech Tokens for Zero-Shot **T**ext-**T**o-**S**peech (DiFlow-TTS), as illustrated in Fig. [fig:overview](#fig:overview), to directly address the aforementioned challenges and research questions.

To explicitly model factorized speech attributes within a compact and unified framework, we propose the Phoneme-Content Mapper (PCM), which maps phoneme sequences to discrete speech tokens that represent the content of the utterance.

This module generates content embeddings that align closely with the semantic structure of the speech.

These embeddings, along with auditory attributes extracted from the reference speech prompt, are then used to condition a Factorized Discrete Flow Denoiser (FDFD) module, allowing it to effectively clone the reference's speaking style.

Crucially, we design the model with separate prediction heads for the probability velocity of distinct speech aspects, specifically prosody, and acoustic details, allowing it to learn aspect-specific distributions explicitly.

As a result, the model achieves improved naturalness, expressiveness, and fidelity in the synthesized speech.

Our main contributions are as follows:

-  We propose a novel DFM model, the first to the best of our knowledge to explore purely discrete flow matching to speech synthesis.

Our approach leverages in-context learning by conditioning on target text content representations from PCM, and auditory attributes of the reference speech prompt, enabling effective attribute cloning in a zero-shot setting and high-quality speech generation in a fully non-autoregressive manner.

-  We introduce a factorized flow prediction mechanism in which DFM jointly models multiple speech aspects, including prosody and fine-grained acoustic details, through distinct prediction heads.

This architecture enables the model to explicitly learn aspect-specific distributions within a unified framework.

-  Experimental results demonstrate that DiFlow-TTS is both compact and efficient in model size, surpassing several strong zero-shot TTS baselines in terms of naturalness, prosody, energy control, and speaker style preservation.

It also maintains a compact model size and achieves low-latency inference, generating speech up to 25.8 times faster than strong baselines.

## 2·Related Work

A growing trend in speech synthesis focuses on converting raw waveforms into discrete token representations using vector-quantized variational autoencoders (VQ-VAE), which was first introduced by [^Oord2017Neural] in the field of computer vision and later adapted to speech synthesis.

These tokenized representations have demonstrated greater naturalness and robustness compared to conventional mel-spectrogram-based approaches.

To effectively model sequences of discrete speech tokens, recent efforts have adapted large language models (LLMs) from the natural language processing (NLP) domain [^Zhang2023Speak], [^Chen2024Vall-E], [^Han2024Vall-E], [^Du2024Cosyvoice], [^Peng2024V}oice{C}raft], [^Meng2025Autoregressive], [^Chen2025Neural], [^Wang2025Spark-TTS].

A notable example is VALL-E [^Chen2025Neural], which leverages a pre-trained neural codec to encode speech into discrete codec tokens and reformulates zero-shot TTS as a conditional codec language modeling task.

During inference, it performs autoregressive (AR) continuation from the acoustic tokens of a short speech prompt, enabling high-fidelity speaker-consistent voice synthesis.

Although autoregressive models achieve impressive quality, they are inherently limited by slow inference speeds.

This limitation has prompted a shift toward non-autoregressive (NAR) paradigms [^Shen2024NaturalSpeech], [^Ju2024N}atural{S}peech], [^Du2024UniCATS], [^Lee2025Di{TT}o-{Tts], [^Jia2025Di{TAR].

For example, NaturalSpeech 2 [^Shen2024NaturalSpeech] uses diffusion [^Ho2020Denoising], [^Song2021Score-Based] to generate discrete acoustic tokens as continuous features, and NaturalSpeech 3 [^Ju2024N}atural{S}peech] further factorizes speech into subspaces of content, prosody, and acoustic details, employing multiple diffusion models to independently capture various acoustic characteristics.

In parallel, flow matching [^Lipman2023Flow], [^Liu2023Flow] has gained attention as a promising generative technique, producing strong results in various domains.

However, most existing speech-related flow matching applications operate in a continuous space [^Mehta2024Matcha-TTS], [^Guan2024Reflow-TTS], [^Yao2025Stablevc], [^Zuo2025Rhythm], [^Zuo2025Enhancing], [^Hieu2025OZS}peech], requiring either a pure mel-spectrogram or discrete tokens to be embedded into continuous representations prior to generation.

This leads to increased computational overhead and latency during inference, which diminishes the efficiency benefits of flow-based approaches.

An emerging line of research seeks to extend iterative refinement techniques to discrete spaces by modeling generation dynamics using Markov chains.

Discrete-space generative models have already proven effective in domains such as natural language [^Lou2024Discrete], [^Shi2024Simplified], [^Sahoo2024Simple], proteins [^Campbell2024Generative], [^Yi2025All-Atom], vision [^Austin2021Structured], [^Chang2022MaskGIT], [^Shi2024Simplified], [^Fuest2025Maskflow], code [^Gat2024Discrete], and even graphs [^Qin2025DeFoG].

Although discrete diffusion models have recently been applied to speech synthesis [^Ye2025Emotional], [^Ye2025Shushing!], the use of discrete flow matching [^Gat2024Discrete] to model speech tokens remains largely unexplored, particularly in zero-shot TTS scenarios.

In this work, we propose a DFM framework tailored for zero-shot TTS, aiming to harness the efficiency of discrete modeling without compromising quality.

## 3·Method

Figure~[fig:components](#fig:components) illustrates the overall framework of DiFlow-TTS, which comprises three main modules: (a) *Speech Tokenization*, (b) *Phoneme-Content Mapper*, and (c) *Factorized Discrete Flow Denoiser*.

In the following sections, we describe each module in detail.

![](figures/components.pdf)

<a id="fig:components">The detailed components of DiFlow TTS.

The architecture consists of three main components: (a) *Speech Tokenization*, which extracts discrete tokens and a speaker embedding from a raw speech; (b) *Phoneme-Content Mapper*, which maps input phonemes to discrete content tokens and generates the corresponding content embeddings; and (c) *Factorized Discrete Flow Denoiser*, which performs discrete flow matching conditioned on the content embeddings, speaker embedding, and the discrete prosody and acoustic tokens derived from the reference speech prompt.</a>

### Preliminaries

#### Notions.

We denote a sequence $x$ as an array of $L$ tokens $(x^1, x^2, \dots, x^L)$ belonging to a discrete space $\mathcal{D} = [v]^L$, where each token is selected from a vocabulary of size $v$, with $[v] = \{1, \dots, v\}$.

We also define an extended space $\mathcal{D}' = [v]^{nL}$ as the concatenation of $n$ such sequences, each of length $L$.

To express the distributions of the point mass in these sequences, we use the delta function $\delta_y(x) = \prod_{i=1}^N \delta_{y^i}(x^i)$, where $\delta_{y^i}(x^i) = 1$ if $x^i = y^i$, and $0$ otherwise.

#### Discrete Flow Matching.

In DFM models, the goal is to transform the source samples $\mathbf{x}_0 \sim p$ to the target samples $\mathbf{x}_1 \sim q$.

We elaborate on the source and target distributions as well as the probability velocity in the following paragraphs.

**Source Distribution**: Following [^Gat2024Discrete], we instantiate the source distribution $p$ to assign all probability mass to sequences in which every token is the mask token \verb|[MASK]|, that is, $p(x) = \delta_{\verb|[MASK]|}(x)$.

This implies that the source distribution places all probability mass in the sequence where every token is the mask token \verb|[MASK]|.

**Target Distribution**:
In conventional DFM settings, the target sequence $\mathbf{x}_1$ is treated as a monolithic sequence.

In contrast, we propose to factorize $\mathbf{x}_1$ into two structured components that are learned jointly.

This formulation allows us to construct a probability velocity over a structured target space composed of two parts.

To this end, we define the target distribution $q$ as follows:

\begin{definition}

Let $\mathbf{x}_1^{p} \sim q_p$ and $\mathbf{x}_1^{a} \sim q_a$ denote the random variables corresponding to the prosody and acoustic details sequences, respectively.

These sequences are in spaces $[v]^{mL}$ and $[v]^{kL}$.

The full target sequence is then defined as $\mathbf{x}_1 = \mathbf{x}_1^{p} \oplus \mathbf{x}_1^{a} \in [v]^{(m+k)L}$, where $\oplus$ denotes the concatenation of the sequence.

Assuming the independence between the two components, the joint target distribution is factorized as $q(x) = q_p(x^p) \cdot q_a(x^a)$, where $x = x^p \oplus x^a$.
\end{definition}

**Probability Velocity**: We define a scheduler $\kappa_t \in [0,1]$, which is a monotonically increasing function with boundary conditions $\kappa_0 = 0$ and $\kappa_1 = 1$, where $t \in [0, 1]$ denotes the continuous time variable.

Following [^Gat2024Discrete], we consider a conditional probability path known as the *mixture path*, which linearly interpolates between the source and target distributions:
$p_t(\mathbf{x}^i | \mathbf{x}_0, \mathbf{x}_1) = (1 - \kappa_t)\delta_{\mathbf{x}_0}(\mathbf{x}^i) + \kappa_t\delta_{\mathbf{x}_1}(\mathbf{x}^i)$, where $p_t(\mathbf{x}|\mathbf{x}_0,\mathbf{x}_1) = \prod_{i=1}^N p_t(\mathbf{x}^i | \mathbf{x}_0,\mathbf{x}_1)$.

This defines a time-dependent probability on the space of tokens $[v]$ conditioned on the pair $\mathbf{x}_0, \mathbf{x}_1$, where the probability mass gradually shifts from the source token to the target token as $\kappa_t$ increases.

This *conditional probability path* is governed by the probability velocity $\mathbf{u}_t$, defined as:

$$\label{eq:u_t_denoiser}
\mathbf{u}^i_t(\mathbf{x}^i,\mathbf{x}_t) = \frac{\dot{\kappa}_t}{1-\kappa_t}\left[p_{1|t}(\mathbf{x}^i|\mathbf{x}_t, \mathbf{c}; \theta) - \delta_{\mathbf{x}_t}(\mathbf{x}^i)\right],
$$

where $\dot{\kappa}_t$ is the time derivative of the scheduler $\kappa_t$, and $\mathbf{c} = (\mathbf{r}, \mathbf{h}_c, \mathbf{s})$ denotes the set of conditioning variables, including reference speech prompt $\mathbf{r}$, content embedding $\mathbf{h}_c$, and speaker embedding $\mathbf{s}$.

The term $p_{1|t}(\mathbf{x}^i|\mathbf{x}_t, \mathbf{c}; \theta)$ is modeled by a probability denoiser $f_{\theta}$ with learnable parameters $\theta$ and is defined as:

$$
p_{1|t}(\mathbf{x}^i|\mathbf{x}_t, \mathbf{c}; \theta) = \sum_{\mathbf{x}_0, \mathbf{x}_1} \delta_{\mathbf{x}_1}(\mathbf{x}^i) \, p_t(\mathbf{x}_0, \mathbf{x}_1|\mathbf{x}_t, \mathbf{c}; \theta),
\label{eq:p1|t}
$$

where $p_t(\mathbf{x}_0, \mathbf{x}_1|\mathbf{x}_t, \mathbf{c}; \theta)$ is the posterior distribution $\mathbf{x}_1$ given a partially corrupted sequence $\mathbf{x}_t$ and condition $\mathbf{c}$.

### Speech Tokenization

The *Speech Tokenization* module converts a raw input speech waveform into discrete token sequences.

In our framework, we employ FaCodec [^Ju2024N}atural{S}peech] as the speech tokenizer and speaker embedding extractor.

FaCodec factorizes the original speech signal $\mathcal{A}$ into distinct components representing prosody, content, acoustic details, and speaker identity:

$$
\mathbf{x}^p, \mathbf{x}^c, \mathbf{x}^a, \mathbf{s} = \text{CodecEncoder}(\mathcal{A}),
\label{eq:tokenizer}
$$

where $\mathbf{x}^p \in [v]^{mL}$, $\mathbf{x}^c \in [v]^{nL}$, and $\mathbf{x}^a \in [v]^{kL}$ denote the discrete token sequences for prosody, content, and acoustic details, respectively, and $\mathbf{s} \in \mathbb{R}^{D_{\text{spk}}}$ is the embedding of the speaker.

Here, $m$, $n$, and $k$ represent the number of quantizers used for each attribute, $L$ denotes the temporal length of the sequences, and $v$ is the vocabulary size.

### Phoneme-Content Mapper

The *Phoneme-Content Mapper* (PCM) aligns and transforms the phonemes derived from the text prompt into the corresponding content tokens generated by the speech tokenizer, along with the corresponding embeddings of the content.

Given a transcript text, we first use an open-source grapheme-to-phoneme converter\footnote{\url{https://github.com/Kyubyong/g2p/tree/master}} to obtain the textual phoneme sequence $\mathbf{P} = (\mathbf{P}_1, \mathbf{P}_2, \dots, \mathbf{P}_N)$ with $N$ phoneme tokens.

A phoneme encoder then processes $\mathbf{P}$ into a sequence of $N$ phoneme embeddings $\mathbf{p} = (\mathbf{p}_1, \mathbf{p}_2, \dots, \mathbf{p}_N) \in \mathbb{R}^{N \times D'}$. 

To align phonemes with discrete content speech tokens, we employ a Duration Predictor that estimates the duration $\mathbf{d} = (\mathbf{d}_1, \mathbf{d}_2, \dots, \mathbf{d}_N)$ (i.e., the number of aligned speech tokens) for each phoneme.

This produces an integer-based alignment that maps each phoneme to a variable-length span in the speech-token sequence.

These alignments are used by a Length Regulator to upsample the phoneme embeddings, matching the length of the discrete content token sequence: 
$$\mathbf{p}_{\text{up}} = (\underbrace{\mathbf{p}_1,..., \mathbf{p}_1}_{\mathbf{d}_1\ \text{times}}, \underbrace{\mathbf{p}_2,..., \mathbf{p}_2}_{\mathbf{d}_2\ \text{times}},..., \underbrace{\mathbf{p}_N,..., \mathbf{p}_N}_{\mathbf{d}_N\ \text{times}}
) \in \mathbb{R}^{L \times D'}$$. 

The upsampled sequence is then fed into a Content Predictor, consisting of several Feed-Forward Transformer (FFT) layers.

These layers extract $n$ contextual representations in a hierarchical manner, with each layer modeling dependencies conditioned on the preceding ones.

The resulting hidden states $\mathbf{h} \in \mathbb{R}^{n \times L \times D'}$ are processed by two branches: a projection layer $\mathcal{H}{\varrho}(\cdot)$ that outputs content embeddings and a content head $\mathcal{G}{\varphi}(\cdot)$ that produces logits to predict discrete content tokens:

$$

\begin{split}
\mathbf{h}_c &= \mathcal{H}_{\varrho}(\mathbf{h}) \in \mathbb{R}^{n \times L \times D}, \\
p(\mathbf{x}^c | \mathbf{h}; \varphi) &= \mathcal{G}_{\varphi}(\mathbf{h}) \in \mathbb{R}^{n \times L \times v},
\end{split}

\label{eq:h_c}
$$

where $v$ is the vocabulary size for the discrete content tokens, and $D$ is the hidden dimension of the content embeddings.

### Factorized Discrete Flow Denoiser

The *Factorized Discrete Flow Denoiser (FDFD)* aims to generate the prosody and acoustic sequences of the target speech by leveraging Discrete Flow Matching and in-context learning, conditioned on a set of contextual inputs.

In the following, we detail the key elements of this module.

#### Contextual Modeling.

We now elaborate on the construction of the conditioning context $\mathbf{c}$ introduced in Eq.~\eqref{eq:u_t_denoiser} and describe how it is integrated into our framework.

Given a reference speech prompt $\mathbf{r}$, we decompose it as shown in Eq.~\eqref{eq:tokenizer} into a prosody token sequence $\mathbf{r}^p \in [v]^{mL_p}$, an acoustic token sequence $\mathbf{r}^a \in [v]^{kL_p}$, and a speaker embedding $\mathbf{s} \in \mathbb{R}^{D_{\text{spk}}}$, where $L_p$ denotes the temporal length of the reference prompt, and $D_{\text{spk}}$ is the hidden dimension of the speaker embedding.

Likewise, the current denoising input $\mathbf{x}_t \in [v]^{(m+k)L}$ is split into prosody tokens $\mathbf{x}_t^p \in [v]^{mL}$ and acoustic tokens $\mathbf{x}_t^a \in [v]^{kL}$.

We then use prosody and acoustic embedders, denoted $\mathcal{E}_{p}(\cdot)$ and $\mathcal{E}_a(\cdot)$, to convert these sequences into hidden representations:

$$

\begin{split}
\mathbf{e}^p_r &= \mathcal{E}_p(\mathbf{r}^p) \in \mathbb{R}^{m \times L_p \times D}, \quad
\mathbf{e}^p_t = \mathcal{E}_p(\mathbf{x}^p_t) \in \mathbb{R}^{m \times L \times D}, \\
\mathbf{e}^a_r &= \mathcal{E}_a(\mathbf{r}^a) \in \mathbb{R}^{k \times L_p \times D}, \quad
\mathbf{e}^a_t = \mathcal{E}_a(\mathbf{x}^a_t) \in \mathbb{R}^{k \times L \times D}.
\end{split}

$$

To enrich the modeling of prosody and acoustic information, we further incorporate the embedding of content $\mathbf{h}_c$ obtained from Eq~\eqref{eq:h_c} and the embedding of speakers $\mathbf{s}$ extracted from the reference speech prompt described above.

Specifically, for each attribute, the reference embedding $\mathbf{e}^i_r$ is concatenated with its corresponding corrupted embedding $\mathbf{e}^i_t$, where $i \in \{p, c, a\}$ denotes prosody, content, and acoustic details, respectively.

For the corrupted content embedding $\mathbf{e}^c_t$, we directly use the content representation: $\mathbf{e}^c_t = \mathbf{h}_c \in \mathbb{R}^{n \times L \times D}$.

Since content information is not required for the reference branch, we set $\mathbf{e}^c_r$ to a zero-valued placeholder $\mathbf{h}_{\text{zeros}} \in \mathbb{R}^{n \times L_p \times D}$ to maintain consistency in the number of quantizer streams.

The final concatenated embeddings for each attribute type are defined as:

$$

\begin{split}
\mathbf{e}_p &= \mathbf{e}^p_r \oplus \mathbf{e}^p_t \in \mathbb{R}^{m \times (L_p + L) \times D}, \\
\mathbf{e}_c &= \mathbf{e}^c_r \oplus \mathbf{e}^c_t \in \mathbb{R}^{n \times (L_p + L) \times D},\\
\mathbf{e}_a &= \mathbf{e}^a_r \oplus \mathbf{e}^a_t \in \mathbb{R}^{k \times (L_p + L) \times D}.
\end{split}

$$

To help the model distinguish among attribute types, we introduce learnable attribute-type embeddings: $\mathbf{g}_p$, $\mathbf{g}_c$, and $\mathbf{g}_a$, each in $\mathbb{R}^{1 \times 1 \times D}$, corresponding to prosody, content, and acoustic details attributes, respectively.

These are broadcast and added to their respective embeddings to inject attribute-type awareness.

The resulting embeddings are then concatenated along the temporal dimension as follows:

$$
\mathbf{e} =  \left[(\mathbf{e}_p + \mathbf{g}_p)\oplus (\mathbf{e}_c + \mathbf{g}_c) \oplus (\mathbf{e}_a + \mathbf{g}_a)\right],
$$

where $\mathbf{e} \in \mathbb{R}^{(m + n + k) \times (L_p + L) \times D}$.

We reshape the combined embedding $\mathbf{e}$ by permuting its axes to flatten the dimension of the quantizer, resulting in a tensor of shape $\mathbb{R}^{(L_p + L) \times (m + n + k)D}$.

This reshaped embedding is then projected into the hidden dimension of the model $D$ using a learable projection layer $\mathcal{G}_{\xi}(\cdot)$:

$$
\mathbf{z} = \mathcal{G}_{\xi}(**Permute**(\mathbf{e})) \in \mathbb{R}^{(L_p + L) \times D}.
$$

The resulting sequence is passed through a neural network $f_\psi: \mathbb{R}^{(L_p + L) \times D} \to \mathbb{R}^{(L_p + L) \times (m + k)D}$, implemented with Diffusion Transformer (DiT) blocks [^Peebles2023Scalable].

In parallel, the timestep $\mathbf{t}$ is embedded into $\mathbb{R}^D$ and added to the speaker embedding $\mathbf{s}$, which is also projected into $\mathbb{R}^D$, to form a global conditioning vector.

This vector is fed into a multilayer perceptron (MLP), which outputs scaling and shifting parameters used for feature-wise affine modulation, enabling speaker-aware adaptation.

After residual addition, the final transformation is applied, comprising layer normalization followed by feature-wise affine modulation conditioned on the global conditioning vector, and a linear projection to $(m + k)D$.

We then discard the reference portion and permute the result to yield the final hidden representation:

$$
\mathbf{h}_{p,a} = **Permute**(**Discard**(f_\psi(\mathbf{z}))) \in \mathbb{R}^{(m + k) \times L \times D}.
\label{eq:h_pa}
$$

#### Factorized Flow Prediction.

Inspired by the Multi-head Attention mechanism [^Vaswani2017Attention], which enables the model to jointly attend to information from different representation subspaces, we propose a factorized flow prediction mechanism based on multi-head prediction.

In this design, FDFD simultaneously models multiple aspects of speech, specifically prosody and acoustic details.

Formally, we define two parallel heads: the *prosody head* $f_{\phi}(\cdot)$ and the *acoustic head* $f_{\omega}(\cdot)$, which independently predict probability distributions corresponding to prosody and acoustic attributes.

We begin by slicing the representation $\mathbf{h}_{p,a}$ obtained from Eq~\eqref{eq:h_pa} into two parts: the prosody representation $\mathbf{h}_p \in \mathbb{R}^{m \times L \times D}$ and the acoustic representation $\mathbf{h}_a \in \mathbb{R}^{k \times L \times D}$.

Each component is processed by its respective head, $f_\phi(\cdot)$ and $f_\omega(\cdot)$, producing logits of the shapes $\mathbb{R}^{m \times L \times v}$ and $\mathbb{R}^{k \times L \times v}$, respectively.

These logits correspond to the categorical distributions predicted over the discrete token vocabulary for each aspect.

Finally, the two outputs are concatenated along the dimension of the quantizer, producing a unified tensor of shape $\mathbb{R}^{(m+k) \times L \times v}$.

This tensor serves as the estimated posterior distribution over $\mathbf{x}_1$, as defined in Eq.~\eqref{eq:p1|t}.

#### Overall Factorized Discrete Flow Denoiser.

We define the overall model $f_{\theta}$ as a composition of three main components: a *neural network* $f_{\psi}$, a *prosody head* $f_{\phi}$, and an *acoustic head* $f_{\omega}$.

Formally, the model can be expressed as $f_{\theta} = (f_{\phi} \oplus f_{\omega}) \circ f_{\psi}$, where $\circ$ denotes composition.

### Training Objectives

The overall training objective consists of three loss components corresponding to different modules in our framework.

First, we optimize the *Duration Predictor* using a loss of the Mean Squared Error (MSE) on a logarithmic scale, denoted as $\mathcal{L}_{dur}$, which compares the predicted and ground-truth durations.

Second, for the *Content Predictor* defined in Eq.~\eqref{eq:h_c}, we use a cross-entropy loss $\mathcal{L}_c$ between the predicted logits and the discrete content tokens obtained from the ground truth.

Third, for the *Factorized Discrete Flow Denoiser* module, we learn a probabilistic denoiser $p_{1|t}$ trained to recover masked tokens under varying masking ratios.

The objective is to minimize the cross-entropy loss:
$$
\mathcal{L}_{FDFD}(\theta) = -\sum_{i \in \mathcal{T}}\mathbb{E}_{t \sim \mathcal{U}[0,1],(\mathbf{x}_0,\mathbf{x_1}),\mathbf{x}_t}\log p_{1|t}(\mathbf{x}_1^i|\mathbf{x}_t, \mathbf{c}; \theta), 
$$
where $\mathcal{T} = [(m+k)L]$, $\mathbf{x}_t \sim p_t(\mathbf{x}|\mathbf{x}_0, \mathbf{x}_1), \mathbf{x}_0 \sim p$, and $\mathbf{x}_1 \sim q$.

Finally, the total loss is expressed as:

$$
\mathcal{L} = \lambda_{dur} \mathcal{L}_{dur} + \lambda_c \mathcal{L}_c + \lambda_{FDFD} \mathcal{L}_{FDFD},
\label{eq:total_loss}
$$

where $\lambda_{dur}$, $\lambda_c$, and $\lambda_{FDFD}$ are hyperparameters that control the relative importance of each loss term.
