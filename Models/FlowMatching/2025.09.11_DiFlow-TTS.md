# DiFlow-TTS: Discrete Flow Matching With Factorized Speech Tokens for Low-Latency Zero-Shot Text-to-Speech

<details>
<summary>基本信息</summary>

- 标题: "DiFlow-TTS: Discrete Flow Matching With Factorized Speech Tokens for Low-Latency Zero-Shot Text-to-Speech."
- 作者:
  - 01 Ngoc-Son Nguyen
  - 02 Hieu-Nghia Huynh-Nguyen
  - 03 Thanh V. T. Tran
  - 04 Truong-Son Hy
  - 05 Van Nguyen
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.09631v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2509.09631v1](PDF/2025.09.11_2509.09631v1_DiFlow-TTS__Discrete_Flow_Matching_With_Factorized_Speech_Tokens_for_Low-Latency_Zero-Shot_Text-to-Speech.pdf)
  - [ArXiv:2509.09631v2](PDF/2025.09.12_2509.09631v2_DiFlow-TTS__Discrete_Flow_Matching_With_Factorized_Speech_Tokens_for_Low-Latency_Zero-Shot_Text-to-Speech.pdf)
  - [Publication] #TODO

</details>

## Abstract

Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that mimics the voice of an unseen speaker using only a short reference sample, requiring not only speaker adaptation but also accurate modeling of prosodic attributes.
Recent approaches based on language models, diffusion, and flow matching have shown promising results in zero-shot TTS, but still suffer from slow inference and repetition artifacts.
Discrete codec representations have been widely adopted for speech synthesis, and recent works have begun to explore diffusion models in purely discrete settings, suggesting the potential of discrete generative modeling for speech synthesis.
However, existing flow-matching methods typically embed these discrete tokens into a continuous space and apply continuous flow matching, which may not fully leverage the advantages of discrete representations.
To address these challenges, we introduce *DiFlow-TTS*, which, to the best of our knowledge, is the first model to explore purely Discrete Flow Matching for speech synthesis. *DiFlow-TTS* explicitly models factorized speech attributes within a compact and unified architecture.
It leverages in-context learning by conditioning on textual content, along with prosodic and acoustic attributes extracted from a reference speech, enabling effective attribute cloning in a zero-shot setting.
In addition, the model employs a factorized flow prediction mechanism with distinct heads for prosody and acoustic details, allowing it to learn aspect-specific distributions.
Experimental results demonstrate that *DiFlow-TTS* achieves promising performance in several key metrics, including naturalness, prosody, preservation of speaker style, and energy control.
It also maintains a compact model size and achieves low-latency inference, generating speech up to 25.8 times faster than the latest existing baselines.
Code and audio samples are available on our demo page\footnote{\url{https://diflow-tts.github.io}}.

## 1·Introduction

Zero-shot TTS has seen significant advancements in recent years, aiming to generate high-quality speech that accurately mimics the voice of previously unseen speakers using only a few seconds of reference, with applications in personalized virtual assistants, accessibility tools for low-resource languages, and content creation.

Although large-scale generative models have made substantial progress in this area, several challenges remain. 

Recent studies have explored the application of language modeling [^Zhang2023Speak], [^Han2024Vall-E], [^Meng2025Autoregressive], [^Song2024Ella-V], [^Chen2024Vall-E], [^Peng2024V}oice{C}raft], [^Ji2024M}obile{S}peech], [^Wang2025Spark-TTS], [^Chen2025Neural], particularly through autoregressive (AR) approaches.

A pioneering work of this is VALL-E [^Chen2025Neural], which represents speech as discrete codec tokens and treats these tokens analogously to text, thus reformulating TTS as a conditional codec language modeling task.

Although such models demonstrate strong performance in terms of speech naturalness and speaker similarity, they typically require training on large-scale datasets to be effective.

Moreover, their autoregressive nature results in slow inference and introduces common artifacts, such as unintended repetition of content from the reference speech or missing initial words from the input text.

![](figures/overall-model.pdf)

<a id="fig:overview">Overview of DiLow-TTS.

The model decomposes the speech prompt into timbre, prosody, and acoustic tokens using a codec encoder.

Input text is processed by PCM to generate content tokens and embeddings.

The Factorized Discrete Flow Denoiser generates prosody, and acoustic tokens conditioned the content embeddings, speaker embedding, and the discrete prosody and acoustic tokens derived from speech prompt.

A codec decoder reconstructs the final waveform.</a>

To overcome these limitations, non-autoregressive (NAR) approaches have been developed, enabling faster generation through parallel decoding.

Among these, diffusion-based [^Kang2023ZET-Speech], [^Shen2024NaturalSpeech], [^Ju2024N}atural{S}peech], [^Lee2025Di{TT}o-{Tts] and flow-matching-based [^Kim2023P-Flow], [^Le2023Voicebox], [^Mehta2024Matcha-TTS], [^Eskimez2024E2], [^Chen2024F5-TTS] models have emerged as effective generative frameworks for TTS, striking a better balance between synthesis quality and inference efficiency.

These models typically operate in the mel-spectrogram domain, which preserves rich acoustic detail and enables in-context learning via target speech prompting, leading to improved speaker similarity.

To further reduce inference complexity, several works have adopted discrete codec tokens as intermediate representations.

For example, R-VC [^Zuo2025Rhythm] introduces a shortcut flow matching mechanism that allows generation in a few steps and even one step, significantly reducing inference latency.

Similarly, OZSpeech [^Hieu2025OZS}peech] achieves single-pass decoding by incorporating a prior as the initial stage and introducing an anchor loss during training to facilitate one-step sampling.

However, zero-shot TTS is not solely about capturing speaker identity; it also requires accurate modeling of prosodic characteristics from the reference speech.

A natural solution is to factorize the reference speech into attributes such as prosody, content, and acoustic details, and to model each of these components explicitly.

While OZSpeech attempts to decompose prompt speech into attribute representations and model them through flow matching, it does so in an implicit manner: treating all attributes equally and relying on the flow matching process to learn the separation implicitly.

This lack of explicit supervision results in entangled representations, which hinders the modelâ€™s ability to learn disentangled and controllable factors.

Consequently, the expressiveness and naturalness of synthesized speech are significantly limited.

Based on these observations, we pose the following question: ***Q1: How can we explicitly model factorized speech tokens in a compact and unified framework?*** 

In particular, recent efforts to adapt discrete codec tokens to generative paradigms have sparked a growing interest in applying diffusion models within fully discrete settings [^Ye2025Emotional], [^Ye2025Shushing!].

In contrast, flow-matching models have predominantly followed a single strategy: embedding discrete data into a continuous space followed by continuous flow matching [^Du2024Cosyvoice], [^Hieu2025OZS}peech], [^Wang2025Discl-Vc], [^Zuo2025Enhancing].

Although Discrete Flow Matching (DFM) [^Gat2024Discrete] has emerged as a promising generative paradigm in domains such as language, vision, and bioinformatics [^Shaul2025Flow], [^Yadav2025Retro], [^Fuest2025Maskflow], its application to speech synthesis is still limited.

This motivates a second question:
***Q2: Can purely Discrete Flow Matching achieve state-of-the-art zero-shot TTS performance while maintaining both naturalness and fidelity in synthesized speech?***

In this study, we propose **Di**screte **Flow**

Matching with Factorized Speech Tokens for Zero-Shot **T**ext-**T**o-**S**peech (DiFlow-TTS), as illustrated in Fig. [fig:overview](#fig:overview), to directly address the aforementioned challenges and research questions.

To explicitly model factorized speech attributes within a compact and unified framework, we propose the Phoneme-Content Mapper (PCM), which maps phoneme sequences to discrete speech tokens that represent the content of the utterance.

This module generates content embeddings that align closely with the semantic structure of the speech.

These embeddings, along with auditory attributes extracted from the reference speech prompt, are then used to condition a Factorized Discrete Flow Denoiser (FDFD) module, allowing it to effectively clone the reference's speaking style.

Crucially, we design the model with separate prediction heads for the probability velocity of distinct speech aspects, specifically prosody, and acoustic details, allowing it to learn aspect-specific distributions explicitly.

As a result, the model achieves improved naturalness, expressiveness, and fidelity in the synthesized speech.

Our main contributions are as follows:

-  We propose a novel DFM model, the first to the best of our knowledge to explore purely discrete flow matching to speech synthesis.

Our approach leverages in-context learning by conditioning on target text content representations from PCM, and auditory attributes of the reference speech prompt, enabling effective attribute cloning in a zero-shot setting and high-quality speech generation in a fully non-autoregressive manner.

-  We introduce a factorized flow prediction mechanism in which DFM jointly models multiple speech aspects, including prosody and fine-grained acoustic details, through distinct prediction heads.

This architecture enables the model to explicitly learn aspect-specific distributions within a unified framework.

-  Experimental results demonstrate that DiFlow-TTS is both compact and efficient in model size, surpassing several strong zero-shot TTS baselines in terms of naturalness, prosody, energy control, and speaker style preservation.

It also maintains a compact model size and achieves low-latency inference, generating speech up to 25.8 times faster than strong baselines.
