# DiFlow-TTS: Discrete Flow Matching With Factorized Speech Tokens for Low-Latency Zero-Shot Text-to-Speech

<details>
<summary>基本信息</summary>

- 标题: "DiFlow-TTS: Discrete Flow Matching With Factorized Speech Tokens for Low-Latency Zero-Shot Text-to-Speech."
- 作者:
  - 01 Ngoc-Son Nguyen
  - 02 Hieu-Nghia Huynh-Nguyen
  - 03 Thanh V. T. Tran
  - 04 Truong-Son Hy
  - 05 Van Nguyen
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.09631v2)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2509.09631v1](PDF/2025.09.11_2509.09631v1_DiFlow-TTS__Discrete_Flow_Matching_With_Factorized_Speech_Tokens_for_Low-Latency_Zero-Shot_Text-to-Speech.pdf)
  - [ArXiv:2509.09631v2](PDF/2025.09.12_2509.09631v2_DiFlow-TTS__Discrete_Flow_Matching_With_Factorized_Speech_Tokens_for_Low-Latency_Zero-Shot_Text-to-Speech.pdf)
  - [Publication] #TODO

</details>

## Abstract

Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that mimics the voice of an unseen speaker using only a short reference sample, requiring not only speaker adaptation but also accurate modeling of prosodic attributes.
Recent approaches based on language models, diffusion, and flow matching have shown promising results in zero-shot TTS, but still suffer from slow inference and repetition artifacts.
Discrete codec representations have been widely adopted for speech synthesis, and recent works have begun to explore diffusion models in purely discrete settings, suggesting the potential of discrete generative modeling for speech synthesis.
However, existing flow-matching methods typically embed these discrete tokens into a continuous space and apply continuous flow matching, which may not fully leverage the advantages of discrete representations.
To address these challenges, we introduce *DiFlow-TTS*, which, to the best of our knowledge, is the first model to explore purely Discrete Flow Matching for speech synthesis. *DiFlow-TTS* explicitly models factorized speech attributes within a compact and unified architecture.
It leverages in-context learning by conditioning on textual content, along with prosodic and acoustic attributes extracted from a reference speech, enabling effective attribute cloning in a zero-shot setting.
In addition, the model employs a factorized flow prediction mechanism with distinct heads for prosody and acoustic details, allowing it to learn aspect-specific distributions.
Experimental results demonstrate that *DiFlow-TTS* achieves promising performance in several key metrics, including naturalness, prosody, preservation of speaker style, and energy control.
It also maintains a compact model size and achieves low-latency inference, generating speech up to 25.8 times faster than the latest existing baselines.
Code and audio samples are available on our demo page\footnote{\url{https://diflow-tts.github.io}}.

## 1·Introduction

Zero-shot TTS has seen significant advancements in recent years, aiming to generate high-quality speech that accurately mimics the voice of previously unseen speakers using only a few seconds of reference, with applications in personalized virtual assistants, accessibility tools for low-resource languages, and content creation.

Although large-scale generative models have made substantial progress in this area, several challenges remain. 

Recent studies have explored the application of language modeling [^Zhang2023Speak], [^Han2024Vall-E], [^Meng2025Autoregressive], [^Song2024Ella-V], [^Chen2024Vall-E], [^Peng2024V}oice{C}raft], [^Ji2024M}obile{S}peech], [^Wang2025Spark-TTS], [^Chen2025Neural], particularly through autoregressive (AR) approaches.

A pioneering work of this is VALL-E [^Chen2025Neural], which represents speech as discrete codec tokens and treats these tokens analogously to text, thus reformulating TTS as a conditional codec language modeling task.

Although such models demonstrate strong performance in terms of speech naturalness and speaker similarity, they typically require training on large-scale datasets to be effective.

Moreover, their autoregressive nature results in slow inference and introduces common artifacts, such as unintended repetition of content from the reference speech or missing initial words from the input text.

![](figures/overall-model.pdf)

<a id="fig:overview">Overview of DiLow-TTS.

The model decomposes the speech prompt into timbre, prosody, and acoustic tokens using a codec encoder.

Input text is processed by PCM to generate content tokens and embeddings.

The Factorized Discrete Flow Denoiser generates prosody, and acoustic tokens conditioned the content embeddings, speaker embedding, and the discrete prosody and acoustic tokens derived from speech prompt.

A codec decoder reconstructs the final waveform.</a>

To overcome these limitations, non-autoregressive (NAR) approaches have been developed, enabling faster generation through parallel decoding.

Among these, diffusion-based [^Kang2023ZET-Speech], [^Shen2024NaturalSpeech], [^Ju2024N}atural{S}peech], [^Lee2025Di{TT}o-{Tts] and flow-matching-based [^Kim2023P-Flow], [^Le2023Voicebox], [^Mehta2024Matcha-TTS], [^Eskimez2024E2], [^Chen2024F5-TTS] models have emerged as effective generative frameworks for TTS, striking a better balance between synthesis quality and inference efficiency.

These models typically operate in the mel-spectrogram domain, which preserves rich acoustic detail and enables in-context learning via target speech prompting, leading to improved speaker similarity.

To further reduce inference complexity, several works have adopted discrete codec tokens as intermediate representations.

For example, R-VC [^Zuo2025Rhythm] introduces a shortcut flow matching mechanism that allows generation in a few steps and even one step, significantly reducing inference latency.

Similarly, OZSpeech [^Hieu2025OZS}peech] achieves single-pass decoding by incorporating a prior as the initial stage and introducing an anchor loss during training to facilitate one-step sampling.

However, zero-shot TTS is not solely about capturing speaker identity; it also requires accurate modeling of prosodic characteristics from the reference speech.

A natural solution is to factorize the reference speech into attributes such as prosody, content, and acoustic details, and to model each of these components explicitly.

While OZSpeech attempts to decompose prompt speech into attribute representations and model them through flow matching, it does so in an implicit manner: treating all attributes equally and relying on the flow matching process to learn the separation implicitly.

This lack of explicit supervision results in entangled representations, which hinders the modelâ€™s ability to learn disentangled and controllable factors.

Consequently, the expressiveness and naturalness of synthesized speech are significantly limited.

Based on these observations, we pose the following question: ***Q1: How can we explicitly model factorized speech tokens in a compact and unified framework?*** 

In particular, recent efforts to adapt discrete codec tokens to generative paradigms have sparked a growing interest in applying diffusion models within fully discrete settings [^Ye2025Emotional], [^Ye2025Shushing!].

In contrast, flow-matching models have predominantly followed a single strategy: embedding discrete data into a continuous space followed by continuous flow matching [^Du2024Cosyvoice], [^Hieu2025OZS}peech], [^Wang2025Discl-Vc], [^Zuo2025Enhancing].

Although Discrete Flow Matching (DFM) [^Gat2024Discrete] has emerged as a promising generative paradigm in domains such as language, vision, and bioinformatics [^Shaul2025Flow], [^Yadav2025Retro], [^Fuest2025Maskflow], its application to speech synthesis is still limited.

This motivates a second question:
***Q2: Can purely Discrete Flow Matching achieve state-of-the-art zero-shot TTS performance while maintaining both naturalness and fidelity in synthesized speech?***

In this study, we propose **Di**screte **Flow**

Matching with Factorized Speech Tokens for Zero-Shot **T**ext-**T**o-**S**peech (DiFlow-TTS), as illustrated in Fig. [fig:overview](#fig:overview), to directly address the aforementioned challenges and research questions.

To explicitly model factorized speech attributes within a compact and unified framework, we propose the Phoneme-Content Mapper (PCM), which maps phoneme sequences to discrete speech tokens that represent the content of the utterance.

This module generates content embeddings that align closely with the semantic structure of the speech.

These embeddings, along with auditory attributes extracted from the reference speech prompt, are then used to condition a Factorized Discrete Flow Denoiser (FDFD) module, allowing it to effectively clone the reference's speaking style.

Crucially, we design the model with separate prediction heads for the probability velocity of distinct speech aspects, specifically prosody, and acoustic details, allowing it to learn aspect-specific distributions explicitly.

As a result, the model achieves improved naturalness, expressiveness, and fidelity in the synthesized speech.

Our main contributions are as follows:

-  We propose a novel DFM model, the first to the best of our knowledge to explore purely discrete flow matching to speech synthesis.

Our approach leverages in-context learning by conditioning on target text content representations from PCM, and auditory attributes of the reference speech prompt, enabling effective attribute cloning in a zero-shot setting and high-quality speech generation in a fully non-autoregressive manner.

-  We introduce a factorized flow prediction mechanism in which DFM jointly models multiple speech aspects, including prosody and fine-grained acoustic details, through distinct prediction heads.

This architecture enables the model to explicitly learn aspect-specific distributions within a unified framework.

-  Experimental results demonstrate that DiFlow-TTS is both compact and efficient in model size, surpassing several strong zero-shot TTS baselines in terms of naturalness, prosody, energy control, and speaker style preservation.

It also maintains a compact model size and achieves low-latency inference, generating speech up to 25.8 times faster than strong baselines.

## 2·Related Work

A growing trend in speech synthesis focuses on converting raw waveforms into discrete token representations using vector-quantized variational autoencoders (VQ-VAE), which was first introduced by [^Oord2017Neural] in the field of computer vision and later adapted to speech synthesis.

These tokenized representations have demonstrated greater naturalness and robustness compared to conventional mel-spectrogram-based approaches.

To effectively model sequences of discrete speech tokens, recent efforts have adapted large language models (LLMs) from the natural language processing (NLP) domain [^Zhang2023Speak], [^Chen2024Vall-E], [^Han2024Vall-E], [^Du2024Cosyvoice], [^Peng2024V}oice{C}raft], [^Meng2025Autoregressive], [^Chen2025Neural], [^Wang2025Spark-TTS].

A notable example is VALL-E [^Chen2025Neural], which leverages a pre-trained neural codec to encode speech into discrete codec tokens and reformulates zero-shot TTS as a conditional codec language modeling task.

During inference, it performs autoregressive (AR) continuation from the acoustic tokens of a short speech prompt, enabling high-fidelity speaker-consistent voice synthesis.

Although autoregressive models achieve impressive quality, they are inherently limited by slow inference speeds.

This limitation has prompted a shift toward non-autoregressive (NAR) paradigms [^Shen2024NaturalSpeech], [^Ju2024N}atural{S}peech], [^Du2024UniCATS], [^Lee2025Di{TT}o-{Tts], [^Jia2025Di{TAR].

For example, NaturalSpeech 2 [^Shen2024NaturalSpeech] uses diffusion [^Ho2020Denoising], [^Song2021Score-Based] to generate discrete acoustic tokens as continuous features, and NaturalSpeech 3 [^Ju2024N}atural{S}peech] further factorizes speech into subspaces of content, prosody, and acoustic details, employing multiple diffusion models to independently capture various acoustic characteristics.

In parallel, flow matching [^Lipman2023Flow], [^Liu2023Flow] has gained attention as a promising generative technique, producing strong results in various domains.

However, most existing speech-related flow matching applications operate in a continuous space [^Mehta2024Matcha-TTS], [^Guan2024Reflow-TTS], [^Yao2025Stablevc], [^Zuo2025Rhythm], [^Zuo2025Enhancing], [^Hieu2025OZS}peech], requiring either a pure mel-spectrogram or discrete tokens to be embedded into continuous representations prior to generation.

This leads to increased computational overhead and latency during inference, which diminishes the efficiency benefits of flow-based approaches.

An emerging line of research seeks to extend iterative refinement techniques to discrete spaces by modeling generation dynamics using Markov chains.

Discrete-space generative models have already proven effective in domains such as natural language [^Lou2024Discrete], [^Shi2024Simplified], [^Sahoo2024Simple], proteins [^Campbell2024Generative], [^Yi2025All-Atom], vision [^Austin2021Structured], [^Chang2022MaskGIT], [^Shi2024Simplified], [^Fuest2025Maskflow], code [^Gat2024Discrete], and even graphs [^Qin2025DeFoG].

Although discrete diffusion models have recently been applied to speech synthesis [^Ye2025Emotional], [^Ye2025Shushing!], the use of discrete flow matching [^Gat2024Discrete] to model speech tokens remains largely unexplored, particularly in zero-shot TTS scenarios.

In this work, we propose a DFM framework tailored for zero-shot TTS, aiming to harness the efficiency of discrete modeling without compromising quality.

## 3·Method

Figure~[fig:components](#fig:components) illustrates the overall framework of DiFlow-TTS, which comprises three main modules: (a) *Speech Tokenization*, (b) *Phoneme-Content Mapper*, and (c) *Factorized Discrete Flow Denoiser*.

In the following sections, we describe each module in detail.

![](figures/components.pdf)

<a id="fig:components">The detailed components of DiFlow TTS.

The architecture consists of three main components: (a) *Speech Tokenization*, which extracts discrete tokens and a speaker embedding from a raw speech; (b) *Phoneme-Content Mapper*, which maps input phonemes to discrete content tokens and generates the corresponding content embeddings; and (c) *Factorized Discrete Flow Denoiser*, which performs discrete flow matching conditioned on the content embeddings, speaker embedding, and the discrete prosody and acoustic tokens derived from the reference speech prompt.</a>

### Preliminaries

#### Notions.

We denote a sequence $x$ as an array of $L$ tokens $(x^1, x^2, \dots, x^L)$ belonging to a discrete space $\mathcal{D} = [v]^L$, where each token is selected from a vocabulary of size $v$, with $[v] = \{1, \dots, v\}$.

We also define an extended space $\mathcal{D}' = [v]^{nL}$ as the concatenation of $n$ such sequences, each of length $L$.

To express the distributions of the point mass in these sequences, we use the delta function $\delta_y(x) = \prod_{i=1}^N \delta_{y^i}(x^i)$, where $\delta_{y^i}(x^i) = 1$ if $x^i = y^i$, and $0$ otherwise.

#### Discrete Flow Matching.

In DFM models, the goal is to transform the source samples $\mathbf{x}_0 \sim p$ to the target samples $\mathbf{x}_1 \sim q$.

We elaborate on the source and target distributions as well as the probability velocity in the following paragraphs.

**Source Distribution**: Following [^Gat2024Discrete], we instantiate the source distribution $p$ to assign all probability mass to sequences in which every token is the mask token \verb|[MASK]|, that is, $p(x) = \delta_{\verb|[MASK]|}(x)$.

This implies that the source distribution places all probability mass in the sequence where every token is the mask token \verb|[MASK]|.

**Target Distribution**:
In conventional DFM settings, the target sequence $\mathbf{x}_1$ is treated as a monolithic sequence.

In contrast, we propose to factorize $\mathbf{x}_1$ into two structured components that are learned jointly.

This formulation allows us to construct a probability velocity over a structured target space composed of two parts.

To this end, we define the target distribution $q$ as follows:

\begin{definition}

Let $\mathbf{x}_1^{p} \sim q_p$ and $\mathbf{x}_1^{a} \sim q_a$ denote the random variables corresponding to the prosody and acoustic details sequences, respectively.

These sequences are in spaces $[v]^{mL}$ and $[v]^{kL}$.

The full target sequence is then defined as $\mathbf{x}_1 = \mathbf{x}_1^{p} \oplus \mathbf{x}_1^{a} \in [v]^{(m+k)L}$, where $\oplus$ denotes the concatenation of the sequence.

Assuming the independence between the two components, the joint target distribution is factorized as $q(x) = q_p(x^p) \cdot q_a(x^a)$, where $x = x^p \oplus x^a$.
\end{definition}

**Probability Velocity**: We define a scheduler $\kappa_t \in [0,1]$, which is a monotonically increasing function with boundary conditions $\kappa_0 = 0$ and $\kappa_1 = 1$, where $t \in [0, 1]$ denotes the continuous time variable.

Following [^Gat2024Discrete], we consider a conditional probability path known as the *mixture path*, which linearly interpolates between the source and target distributions:
$p_t(\mathbf{x}^i | \mathbf{x}_0, \mathbf{x}_1) = (1 - \kappa_t)\delta_{\mathbf{x}_0}(\mathbf{x}^i) + \kappa_t\delta_{\mathbf{x}_1}(\mathbf{x}^i)$, where $p_t(\mathbf{x}|\mathbf{x}_0,\mathbf{x}_1) = \prod_{i=1}^N p_t(\mathbf{x}^i | \mathbf{x}_0,\mathbf{x}_1)$.

This defines a time-dependent probability on the space of tokens $[v]$ conditioned on the pair $\mathbf{x}_0, \mathbf{x}_1$, where the probability mass gradually shifts from the source token to the target token as $\kappa_t$ increases.

This *conditional probability path* is governed by the probability velocity $\mathbf{u}_t$, defined as:

$$\label{eq:u_t_denoiser}
\mathbf{u}^i_t(\mathbf{x}^i,\mathbf{x}_t) = \frac{\dot{\kappa}_t}{1-\kappa_t}\left[p_{1|t}(\mathbf{x}^i|\mathbf{x}_t, \mathbf{c}; \theta) - \delta_{\mathbf{x}_t}(\mathbf{x}^i)\right],
$$

where $\dot{\kappa}_t$ is the time derivative of the scheduler $\kappa_t$, and $\mathbf{c} = (\mathbf{r}, \mathbf{h}_c, \mathbf{s})$ denotes the set of conditioning variables, including reference speech prompt $\mathbf{r}$, content embedding $\mathbf{h}_c$, and speaker embedding $\mathbf{s}$.

The term $p_{1|t}(\mathbf{x}^i|\mathbf{x}_t, \mathbf{c}; \theta)$ is modeled by a probability denoiser $f_{\theta}$ with learnable parameters $\theta$ and is defined as:

$$
p_{1|t}(\mathbf{x}^i|\mathbf{x}_t, \mathbf{c}; \theta) = \sum_{\mathbf{x}_0, \mathbf{x}_1} \delta_{\mathbf{x}_1}(\mathbf{x}^i) \, p_t(\mathbf{x}_0, \mathbf{x}_1|\mathbf{x}_t, \mathbf{c}; \theta),
\label{eq:p1|t}
$$

where $p_t(\mathbf{x}_0, \mathbf{x}_1|\mathbf{x}_t, \mathbf{c}; \theta)$ is the posterior distribution $\mathbf{x}_1$ given a partially corrupted sequence $\mathbf{x}_t$ and condition $\mathbf{c}$.

### Speech Tokenization

The *Speech Tokenization* module converts a raw input speech waveform into discrete token sequences.

In our framework, we employ FaCodec [^Ju2024N}atural{S}peech] as the speech tokenizer and speaker embedding extractor.

FaCodec factorizes the original speech signal $\mathcal{A}$ into distinct components representing prosody, content, acoustic details, and speaker identity:

$$
\mathbf{x}^p, \mathbf{x}^c, \mathbf{x}^a, \mathbf{s} = \text{CodecEncoder}(\mathcal{A}),
\label{eq:tokenizer}
$$

where $\mathbf{x}^p \in [v]^{mL}$, $\mathbf{x}^c \in [v]^{nL}$, and $\mathbf{x}^a \in [v]^{kL}$ denote the discrete token sequences for prosody, content, and acoustic details, respectively, and $\mathbf{s} \in \mathbb{R}^{D_{\text{spk}}}$ is the embedding of the speaker.

Here, $m$, $n$, and $k$ represent the number of quantizers used for each attribute, $L$ denotes the temporal length of the sequences, and $v$ is the vocabulary size.

### Phoneme-Content Mapper

The *Phoneme-Content Mapper* (PCM) aligns and transforms the phonemes derived from the text prompt into the corresponding content tokens generated by the speech tokenizer, along with the corresponding embeddings of the content.

Given a transcript text, we first use an open-source grapheme-to-phoneme converter\footnote{\url{https://github.com/Kyubyong/g2p/tree/master}} to obtain the textual phoneme sequence $\mathbf{P} = (\mathbf{P}_1, \mathbf{P}_2, \dots, \mathbf{P}_N)$ with $N$ phoneme tokens.

A phoneme encoder then processes $\mathbf{P}$ into a sequence of $N$ phoneme embeddings $\mathbf{p} = (\mathbf{p}_1, \mathbf{p}_2, \dots, \mathbf{p}_N) \in \mathbb{R}^{N \times D'}$. 

To align phonemes with discrete content speech tokens, we employ a Duration Predictor that estimates the duration $\mathbf{d} = (\mathbf{d}_1, \mathbf{d}_2, \dots, \mathbf{d}_N)$ (i.e., the number of aligned speech tokens) for each phoneme.

This produces an integer-based alignment that maps each phoneme to a variable-length span in the speech-token sequence.

These alignments are used by a Length Regulator to upsample the phoneme embeddings, matching the length of the discrete content token sequence: 
$$\mathbf{p}_{\text{up}} = (\underbrace{\mathbf{p}_1,..., \mathbf{p}_1}_{\mathbf{d}_1\ \text{times}}, \underbrace{\mathbf{p}_2,..., \mathbf{p}_2}_{\mathbf{d}_2\ \text{times}},..., \underbrace{\mathbf{p}_N,..., \mathbf{p}_N}_{\mathbf{d}_N\ \text{times}}
) \in \mathbb{R}^{L \times D'}$$. 

The upsampled sequence is then fed into a Content Predictor, consisting of several Feed-Forward Transformer (FFT) layers.

These layers extract $n$ contextual representations in a hierarchical manner, with each layer modeling dependencies conditioned on the preceding ones.

The resulting hidden states $\mathbf{h} \in \mathbb{R}^{n \times L \times D'}$ are processed by two branches: a projection layer $\mathcal{H}{\varrho}(\cdot)$ that outputs content embeddings and a content head $\mathcal{G}{\varphi}(\cdot)$ that produces logits to predict discrete content tokens:

$$

\begin{split}
\mathbf{h}_c &= \mathcal{H}_{\varrho}(\mathbf{h}) \in \mathbb{R}^{n \times L \times D}, \\
p(\mathbf{x}^c | \mathbf{h}; \varphi) &= \mathcal{G}_{\varphi}(\mathbf{h}) \in \mathbb{R}^{n \times L \times v},
\end{split}

\label{eq:h_c}
$$

where $v$ is the vocabulary size for the discrete content tokens, and $D$ is the hidden dimension of the content embeddings.

### Factorized Discrete Flow Denoiser

The *Factorized Discrete Flow Denoiser (FDFD)* aims to generate the prosody and acoustic sequences of the target speech by leveraging Discrete Flow Matching and in-context learning, conditioned on a set of contextual inputs.

In the following, we detail the key elements of this module.

#### Contextual Modeling.

We now elaborate on the construction of the conditioning context $\mathbf{c}$ introduced in Eq.~\eqref{eq:u_t_denoiser} and describe how it is integrated into our framework.

Given a reference speech prompt $\mathbf{r}$, we decompose it as shown in Eq.~\eqref{eq:tokenizer} into a prosody token sequence $\mathbf{r}^p \in [v]^{mL_p}$, an acoustic token sequence $\mathbf{r}^a \in [v]^{kL_p}$, and a speaker embedding $\mathbf{s} \in \mathbb{R}^{D_{\text{spk}}}$, where $L_p$ denotes the temporal length of the reference prompt, and $D_{\text{spk}}$ is the hidden dimension of the speaker embedding.

Likewise, the current denoising input $\mathbf{x}_t \in [v]^{(m+k)L}$ is split into prosody tokens $\mathbf{x}_t^p \in [v]^{mL}$ and acoustic tokens $\mathbf{x}_t^a \in [v]^{kL}$.

We then use prosody and acoustic embedders, denoted $\mathcal{E}_{p}(\cdot)$ and $\mathcal{E}_a(\cdot)$, to convert these sequences into hidden representations:

$$

\begin{split}
\mathbf{e}^p_r &= \mathcal{E}_p(\mathbf{r}^p) \in \mathbb{R}^{m \times L_p \times D}, \quad
\mathbf{e}^p_t = \mathcal{E}_p(\mathbf{x}^p_t) \in \mathbb{R}^{m \times L \times D}, \\
\mathbf{e}^a_r &= \mathcal{E}_a(\mathbf{r}^a) \in \mathbb{R}^{k \times L_p \times D}, \quad
\mathbf{e}^a_t = \mathcal{E}_a(\mathbf{x}^a_t) \in \mathbb{R}^{k \times L \times D}.
\end{split}

$$

To enrich the modeling of prosody and acoustic information, we further incorporate the embedding of content $\mathbf{h}_c$ obtained from Eq~\eqref{eq:h_c} and the embedding of speakers $\mathbf{s}$ extracted from the reference speech prompt described above.

Specifically, for each attribute, the reference embedding $\mathbf{e}^i_r$ is concatenated with its corresponding corrupted embedding $\mathbf{e}^i_t$, where $i \in \{p, c, a\}$ denotes prosody, content, and acoustic details, respectively.

For the corrupted content embedding $\mathbf{e}^c_t$, we directly use the content representation: $\mathbf{e}^c_t = \mathbf{h}_c \in \mathbb{R}^{n \times L \times D}$.

Since content information is not required for the reference branch, we set $\mathbf{e}^c_r$ to a zero-valued placeholder $\mathbf{h}_{\text{zeros}} \in \mathbb{R}^{n \times L_p \times D}$ to maintain consistency in the number of quantizer streams.

The final concatenated embeddings for each attribute type are defined as:

$$

\begin{split}
\mathbf{e}_p &= \mathbf{e}^p_r \oplus \mathbf{e}^p_t \in \mathbb{R}^{m \times (L_p + L) \times D}, \\
\mathbf{e}_c &= \mathbf{e}^c_r \oplus \mathbf{e}^c_t \in \mathbb{R}^{n \times (L_p + L) \times D},\\
\mathbf{e}_a &= \mathbf{e}^a_r \oplus \mathbf{e}^a_t \in \mathbb{R}^{k \times (L_p + L) \times D}.
\end{split}

$$

To help the model distinguish among attribute types, we introduce learnable attribute-type embeddings: $\mathbf{g}_p$, $\mathbf{g}_c$, and $\mathbf{g}_a$, each in $\mathbb{R}^{1 \times 1 \times D}$, corresponding to prosody, content, and acoustic details attributes, respectively.

These are broadcast and added to their respective embeddings to inject attribute-type awareness.

The resulting embeddings are then concatenated along the temporal dimension as follows:

$$
\mathbf{e} =  \left[(\mathbf{e}_p + \mathbf{g}_p)\oplus (\mathbf{e}_c + \mathbf{g}_c) \oplus (\mathbf{e}_a + \mathbf{g}_a)\right],
$$

where $\mathbf{e} \in \mathbb{R}^{(m + n + k) \times (L_p + L) \times D}$.

We reshape the combined embedding $\mathbf{e}$ by permuting its axes to flatten the dimension of the quantizer, resulting in a tensor of shape $\mathbb{R}^{(L_p + L) \times (m + n + k)D}$.

This reshaped embedding is then projected into the hidden dimension of the model $D$ using a learable projection layer $\mathcal{G}_{\xi}(\cdot)$:

$$
\mathbf{z} = \mathcal{G}_{\xi}(**Permute**(\mathbf{e})) \in \mathbb{R}^{(L_p + L) \times D}.
$$

The resulting sequence is passed through a neural network $f_\psi: \mathbb{R}^{(L_p + L) \times D} \to \mathbb{R}^{(L_p + L) \times (m + k)D}$, implemented with Diffusion Transformer (DiT) blocks [^Peebles2023Scalable].

In parallel, the timestep $\mathbf{t}$ is embedded into $\mathbb{R}^D$ and added to the speaker embedding $\mathbf{s}$, which is also projected into $\mathbb{R}^D$, to form a global conditioning vector.

This vector is fed into a multilayer perceptron (MLP), which outputs scaling and shifting parameters used for feature-wise affine modulation, enabling speaker-aware adaptation.

After residual addition, the final transformation is applied, comprising layer normalization followed by feature-wise affine modulation conditioned on the global conditioning vector, and a linear projection to $(m + k)D$.

We then discard the reference portion and permute the result to yield the final hidden representation:

$$
\mathbf{h}_{p,a} = **Permute**(**Discard**(f_\psi(\mathbf{z}))) \in \mathbb{R}^{(m + k) \times L \times D}.
\label{eq:h_pa}
$$

#### Factorized Flow Prediction.

Inspired by the Multi-head Attention mechanism [^Vaswani2017Attention], which enables the model to jointly attend to information from different representation subspaces, we propose a factorized flow prediction mechanism based on multi-head prediction.

In this design, FDFD simultaneously models multiple aspects of speech, specifically prosody and acoustic details.

Formally, we define two parallel heads: the *prosody head* $f_{\phi}(\cdot)$ and the *acoustic head* $f_{\omega}(\cdot)$, which independently predict probability distributions corresponding to prosody and acoustic attributes.

We begin by slicing the representation $\mathbf{h}_{p,a}$ obtained from Eq~\eqref{eq:h_pa} into two parts: the prosody representation $\mathbf{h}_p \in \mathbb{R}^{m \times L \times D}$ and the acoustic representation $\mathbf{h}_a \in \mathbb{R}^{k \times L \times D}$.

Each component is processed by its respective head, $f_\phi(\cdot)$ and $f_\omega(\cdot)$, producing logits of the shapes $\mathbb{R}^{m \times L \times v}$ and $\mathbb{R}^{k \times L \times v}$, respectively.

These logits correspond to the categorical distributions predicted over the discrete token vocabulary for each aspect.

Finally, the two outputs are concatenated along the dimension of the quantizer, producing a unified tensor of shape $\mathbb{R}^{(m+k) \times L \times v}$.

This tensor serves as the estimated posterior distribution over $\mathbf{x}_1$, as defined in Eq.~\eqref{eq:p1|t}.

#### Overall Factorized Discrete Flow Denoiser.

We define the overall model $f_{\theta}$ as a composition of three main components: a *neural network* $f_{\psi}$, a *prosody head* $f_{\phi}$, and an *acoustic head* $f_{\omega}$.

Formally, the model can be expressed as $f_{\theta} = (f_{\phi} \oplus f_{\omega}) \circ f_{\psi}$, where $\circ$ denotes composition.

### Training Objectives

The overall training objective consists of three loss components corresponding to different modules in our framework.

First, we optimize the *Duration Predictor* using a loss of the Mean Squared Error (MSE) on a logarithmic scale, denoted as $\mathcal{L}_{dur}$, which compares the predicted and ground-truth durations.

Second, for the *Content Predictor* defined in Eq.~\eqref{eq:h_c}, we use a cross-entropy loss $\mathcal{L}_c$ between the predicted logits and the discrete content tokens obtained from the ground truth.

Third, for the *Factorized Discrete Flow Denoiser* module, we learn a probabilistic denoiser $p_{1|t}$ trained to recover masked tokens under varying masking ratios.

The objective is to minimize the cross-entropy loss:
$$
\mathcal{L}_{FDFD}(\theta) = -\sum_{i \in \mathcal{T}}\mathbb{E}_{t \sim \mathcal{U}[0,1],(\mathbf{x}_0,\mathbf{x_1}),\mathbf{x}_t}\log p_{1|t}(\mathbf{x}_1^i|\mathbf{x}_t, \mathbf{c}; \theta), 
$$
where $\mathcal{T} = [(m+k)L]$, $\mathbf{x}_t \sim p_t(\mathbf{x}|\mathbf{x}_0, \mathbf{x}_1), \mathbf{x}_0 \sim p$, and $\mathbf{x}_1 \sim q$.

Finally, the total loss is expressed as:

$$
\mathcal{L} = \lambda_{dur} \mathcal{L}_{dur} + \lambda_c \mathcal{L}_c + \lambda_{FDFD} \mathcal{L}_{FDFD},
\label{eq:total_loss}
$$

where $\lambda_{dur}$, $\lambda_c$, and $\lambda_{FDFD}$ are hyperparameters that control the relative importance of each loss term.

## 4·Experiments

\input{tables/overall_results_3s}
\input{tables/mos_results}
\input{tables/latency_results}
\input{tables/ablation_components}
\input{tables/ablation_nfe}

### Experimental Setup

#### Dataset.

We use the *LibriTTS* dataset [^Zen2019LibriTTS], which contains multi-speaker English audio recordings, for training purposes.

For evaluation, we utilize the *LibriSpeech test-clean* dataset [^Panayotov2015Librispeech]. 
Additional details are provided in the *Dataset Details* section of the Supplementary Material.

#### Evaluation Metrics.

To evaluate model performance, we use a range of *objective evaluation metrics* targeting various aspects: naturalness and speech quality is measured with UTMOS; speaker similarity is evaluated using SIM-O and SIM-R; robustness is reflected by the word error rate (WER); and prosody accuracy and error are analyzed through pitch and energy metrics.

In addition, we assess model latency using the real-time factor (RTF).

More information on these metrics is available in the *Metrics Details* section of the Supplementary Material.

To complement these objective evaluations, we perform a *subjective assessment* based on the Mean Opinion Score (MOS) protocol.

In this evaluation, 30 listeners rate synthesized utterances on a scale from 1 to 5 based on naturalness, intelligibility, and speaker similarity to the reference prompt.

#### Implementation Details \& Baselines.

Please refer to the *Implementation Details* section of the Supplementary Material for a comprehensive description.

Additionally, we compare our model with previous zero-shot TTS baselines, with further information provided in the *Baselines Details* section of the Supplementary Material.

### Main Results

#### Comparison Results.

Table~[tab:overall-results-3s](#tab:overall-results-3s) presents the performance of DiFlow-TTS with 128 function evaluations (NFE) using 3-second audio prompts, compared to baseline methods.

DiFlow-TTS, along with OZSpeech, achieves state-of-the-art (SOTA) performance in terms of WER, demonstrating the effectiveness of our PCM in preserving the linguistic content of synthesized speech.

In particular, DiFlow-TTS is the runner-up in naturalness and speech quality, as measured by UTMOS, despite being trained on only 470 hours of speech data, which is significantly less (1.1$\times$ to 212.8$\times$) than other baselines.

It trails SOTA SparkTTS by just 0.33 UTMOS points and is within 0.11 of ground truth, highlighting the strength of our FDFD module in capturing prosodic and acoustic nuances even under limited data conditions.

For speaker similarity, DiFlow-TTS performs slightly below other SOTA methods, specifically ranking second in SIM-R, while SIM-O shows no clear advantage over baseline models.

This may be due to the relatively simple speaker conditioning mechanism in our DiT blocks, which could be enhanced with more advanced strategies.

For prosody reconstruction, DiFlow-TTS sets new SOTA in pitch and energy metrics.

These findings further confirm the ability of the FDFD module to model fine-grained prosodic attributes with high fidelity.

To gain further insight into speech quality, we report the subjective MOS evaluations in Table~[tab:mos-results](#tab:mos-results).

These results are consistent with the findings in Table~[tab:overall-results-3s](#tab:overall-results-3s).

Overall, DiFlow-TTS consistently ranks as the second-best model across all MOS dimensions, providing strong evidence of its well-balanced performance in generating natural and intelligible speech with high speaker similarity.

These results are particularly notable given the model's data efficiency during training.

#### Model Size \& Latency Analysis.

Table~[tab:latency-results](#tab:latency-results) compares the model size and latency between DiFlow-TTS and the baseline systems for a 3-second audio prompt.

DiFlow-TTS achieves a RTF of 0.066 and a model size of 164M parameters with 16 NFE, ranking second in both speed and size.

In comparison, OZSpeech, with only 1 NFE, is marginally faster (by 0.04s) and slightly smaller (144M parameters), but it exhibits significantly lower UTMOS and speaker similarity scores.

Compared to other baselines, DiFlow-TTS is 3.9$\times$ to 25.8$\times$ faster in terms of RTF and 2$\times$ to 5.1$\times$ smaller in model size, while still maintaining competitive performance in speech naturalness, intelligibility, and pronunciation accuracy.

These results demonstrate that DiFlow-TTS strikes a strong balance between speed, compactness, and the quality of the speech.

### Ablation Study

#### Effect of each component.

To assess the impact of each component in DiFlow-TTS, we perform an ablation study by systematically removing or modifying key elements: (1) removing the attribute-type embeddings used to distinguish prosody, content, and acoustic streams; (2) excluding the speaker embedding from the conditioning process, i.e., not injecting it into the DiT blocks; (3) disabling the use of content embeddings in the FDFD module; and (4) replacing the multi-head prediction architecture with a single-head prediction.

As shown in Table~[tab:ablation_components](#tab:ablation_components), we observe a slight degradation in all metrics except UTMOS when the attribute-type embeddings are removed.

This suggests that while these embeddings enhance overall fidelity and prosody modeling, they may introduce minor redundancies that subtly affect perceived naturalness.

A more pronounced decline in speaker similarity and prosody-related metrics is observed when speaker embedding is excluded from the FDFD module.

This highlights that prosody is not only content-dependent but also strongly influenced by speaker identity; without speaker conditioning, the FDFD module produces extraneous prosodic variations, resulting in reduced speaker adaptation and overall synthesis quality.

When content embeddings from the PCM branch are removed from FDFD, we observe substantial degradation across metrics related to naturalness and speaker similarity.

This demonstrates the critical role of content embeddings in conditioning FDFD to generate appropriate prosody and support speaker adaptation.

Lastly, replacing the multi-head prediction mechanism with a single-head alternative leads to minor performance drops across all metrics, indicating that the multi-head design enhances prediction diversity and robustness in prosody and acoustic modeling.

#### Effect of NFE.

We investigate the impact of varying NFE from 1 to 128 on DiFlow-TTS performance to explore the trade-off between inference efficiency and synthesis quality, as presented in Table~[tab:ablation_nfe](#tab:ablation_nfe).

Although increasing NFE yields only marginal improvements in WER and speaker similarity, it significantly enhances UTMOS scores, indicating that the FDFD module benefits from additional refinement steps to generate more natural speech.

In particular, performance stabilizes around 32 NFE, with optimal audio quality observed at 64 NFE, and only marginal improvements beyond this point.

Although RTF naturally increases with NFE, the overall latency remains competitive.

At 8 NFE, DiFlow-TTS is only slower than the 1 NFE setting in OZSpeech, yet it achieves 6.0$\times$ to 39.6$\times$ speedups over other baselines.

At this setting, DiFlow-TTS delivers sufficiently strong performance in speech naturalness, intelligibility, and pronunciation accuracy, even if it does not outperform all baselines, as reported in Table~[tab:latency-results](#tab:latency-results).

These results demonstrate its effective trade-off between quality and efficiency.

## 5·Conclusion

In this work, we introduced DiFlow-TTS, a novel zero-shot text-to-speech system that leverages purely discrete flow matching with factorized speech token modeling.

By explicitly disentangling prosody and acoustic details and predicting them through distinct heads within a unified non-autoregressive framework, DiFlow-TTS achieves strong performance across naturalness, speaker similarity, prosody accuracy, and synthesis speed.

Our design effectively integrates in-context learning and content conditioning via a lightweight architecture, offering a favorable trade-off between quality and latency.

Experimental results demonstrate that DiFlow-TTS outperforms several strong baselines despite being trained on a significantly smaller dataset.

Future work includes enabling multilingual and emotional synthesis and integrating visual lip cues for dubbing to DiFlow-TTS.

\bibliography{aaai2026}

\end{document}

## References

[^Zhang2023Speak]: Speak Foreign Languages With Your Own Voice: Cross-Lingual Neural Codec Language Modeling. arXiv:2303.03926.
[^Han2024Vall-E]: VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment. arXiv:2406.07855.
[^Meng2025Autoregressive]: Autoregressive Speech Synthesis Without Vector Quantization. Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) 2025.
[^Song2024Ella-V]: ELLA-V: Stable Neural Codec Language Modeling With Alignment-Guided Sequence Reordering. 
[^Chen2024Vall-E]: VALL-E 2: Neural Codec Language Models Are Human Parity Zero-Shot Text to Speech Synthesizers. 
[^Peng2024V}oice{C}raft]: {V}oice{C}raft: Zero-Shot Speech Editing and Text-to-Speech in the Wild. Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) 2024.
[^Ji2024M}obile{S}peech]: {M}obile{S}peech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech. Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) 2024.
[^Wang2025Spark-Tts]: Spark-Tts: An Efficient LLM-Based Text-to-Speech Model With Single-Stream Decoupled Speech Tokens. arXiv:2503.01710.
[^Chen2025Neural]: Neural Codec Language Models Are Zero-Shot Text to Speech Synthesizers. IEEE Transactions on Audio, Speech and Language Processing 2025.
[^Kang2023ZET-Speech]: ZET-Speech: Zero-Shot Adaptive Emotion-Controllable Text-to-Speech Synthesis With Diffusion and Style-Based Models. Interspeech 2023 2023.
[^Shen2024NaturalSpeech]: NaturalSpeech 2: Latent Diffusion Models Are Natural and Zero-Shot Speech and Singing Synthesizers. The Twelfth International Conference on Learning Representations 2024.
[^Ju2024N}atural{S}peech]: {N}atural{S}peech 3: Zero-Shot Speech Synthesis With Factorized Codec and Diffusion Models. Proceedings of the 41st International Conference on Machine Learning 2024.
[^Lee2025Di{TT}o-{Tts]: Di{TT}o-{Tts}: Diffusion Transformers for Scalable Text-to-Speech Without Domain-Specific Factors. The Thirteenth International Conference on Learning Representations 2025.
[^Kim2023P-Flow]: P-Flow: A Fast and Data-Efficient Zero-Shot {TTS} Through Speech Prompting. Thirty-Seventh Conference on Neural Information Processing Systems 2023.
[^Le2023Voicebox]: Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale. Thirty-Seventh Conference on Neural Information Processing Systems 2023.
[^Mehta2024Matcha-Tts]: Matcha-Tts: A Fast TTS Architecture With Conditional Flow Matching. ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2024.
[^Eskimez2024E2]: E2 Tts: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS. 2024 IEEE Spoken Language Technology Workshop (SLT) 2024.
[^Chen2024F5-Tts]: F5-Tts: A Fairytaler That Fakes Fluent and Faithful Speech With Flow Matching. 
[^Zuo2025Rhythm]: Rhythm Controllable and Efficient Zero-Shot Voice Conversion via Shortcut Flow Matching. Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) 2025.
[^Hieu2025OZS}peech]: {OZS}peech: One-Step Zero-Shot Speech Synthesis With Learned-Prior-Conditioned Flow Matching. Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) 2025.
[^Ye2025Emotional]: Emotional Face-to-Speech. Forty-Second International Conference on Machine Learning 2025.
[^Ye2025Shushing!]: Shushing! Let's Imagine an Authentic Speech From the Silent Video. arXiv:2503.14928.
[^Du2024Cosyvoice]: Cosyvoice 2: Scalable Streaming Speech Synthesis With Large Language Models. arXiv:2412.10117.
[^Wang2025Discl-Vc]: Discl-Vc: Disentangled Discrete Tokens and in-Context Learning for Controllable Zero-Shot Voice Conversion. arXiv:2505.24291.
[^Zuo2025Enhancing]: Enhancing Expressive Voice Conversion With Discrete Pitch-Conditioned Flow Matching Model. ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2025.
[^Gat2024Discrete]: Discrete Flow Matching. Advances in Neural Information Processing Systems 2024.
[^Shaul2025Flow]: Flow Matching With General Discrete Paths: A Kinetic-Optimal Perspective. The Thirteenth International Conference on Learning Representations 2025.
[^Yadav2025Retro]: RETRO SYNFLOW: Discrete Flow Matching for Accurate and Diverse Single-Step Retrosynthesis. arXiv:2506.04439.
[^Fuest2025Maskflow]: Maskflow: Discrete Flows for Flexible and Efficient Long Video Generation. arXiv:2502.11234.
[^Oord2017Neural]: Neural Discrete Representation Learning. Proceedings of the 31st International Conference on Neural Information Processing Systems 2017.
[^Du2024UniCATS]: UniCATS: A Unified Context-Aware Text-to-Speech Framework With Contextual VQ-Diffusion and Vocoding. Proceedings of the AAAI Conference on Artificial Intelligence 2024.
[^Jia2025Di{TAR]: Di{TAR}: Diffusion Transformer Autoregressive Modeling for Speech Generation. Forty-Second International Conference on Machine Learning 2025.
[^Ho2020Denoising]: Denoising Diffusion Probabilistic Models. Advances in Neural Information Processing Systems 2020.
[^Song2021Score-Based]: Score-Based Generative Modeling Through Stochastic Differential Equations. International Conference on Learning Representations 2021.
[^Lipman2023Flow]: Flow Matching for Generative Modeling. The Eleventh International Conference on Learning Representations 2023.
[^Liu2023Flow]: Flow Straight and Fast: Learning to Generate and Transfer Data With Rectified Flow. The Eleventh International Conference on Learning Representations 2023.
[^Guan2024Reflow-Tts]: Reflow-Tts: A Rectified Flow Model for High-Fidelity Text-to-Speech. ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2024.
[^Yao2025Stablevc]: Stablevc: Style Controllable Zero-Shot Voice Conversion With Conditional Flow Matching. Proceedings of the AAAI Conference on Artificial Intelligence 2025.
[^Lou2024Discrete]: Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution. Proceedings of the 41st International Conference on Machine Learning 2024.
[^Shi2024Simplified]: Simplified and Generalized Masked Diffusion for Discrete Data. The Thirty-Eighth Annual Conference on Neural Information Processing Systems 2024.
[^Sahoo2024Simple]: Simple and Effective Masked Diffusion Language Models. The Thirty-Eighth Annual Conference on Neural Information Processing Systems 2024.
[^Campbell2024Generative]: Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows With Applications to Protein Co-Design. Icml 2024.
[^Yi2025All-Atom]: All-Atom Inverse Protein Folding Through Discrete Flow Matching. Forty-Second International Conference on Machine Learning 2025.
[^Austin2021Structured]: Structured Denoising Diffusion Models in Discrete State-Spaces. Advances in Neural Information Processing Systems 2021.
[^Chang2022MaskGIT]: MaskGIT: Masked Generative Image Transformer. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2022.
[^Qin2025DeFoG]: DeFoG: Discrete Flow Matching for Graph Generation. Proceedings of the 42nd International Conference on Machine Learning (ICML) 2025.
[^Peebles2023Scalable]: Scalable Diffusion Models With Transformers. Proceedings of the IEEE/CVF International Conference on Computer Vision 2023.
[^Vaswani2017Attention]: Attention Is All You Need. Advances in Neural Information Processing Systems 2017.
[^Zen2019LibriTTS]: LibriTTS: A Corpus Derived From LibriSpeech for Text-to-Speech. Interspeech 2019 2019.
[^Panayotov2015Librispeech]: Librispeech: An ASR Corpus Based on Public Domain Audio Books. 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2015.