# MeanAudio: Fast and Faithful Text-to-Audio Generation With Mean Flows

<details>
<summary>基本信息</summary>

- 标题: "MeanAudio: Fast and Faithful Text-to-Audio Generation With Mean Flows."
- 作者:
  - 01 Xiquan Li
  - 02 Junxi Liu
  - 03 Yuzhe Liang
  - 04 Zhikang Niu
  - 05 Wenxi Chen
  - 06 Xie Chen
- 链接:
  - [ArXiv](https://arxiv.org/abs/2508.06098v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2508.06098v1](PDF/2025.08.08_2508.06098v1_MeanAudio__Fast_and_Faithful_Text-to-Audio_Generation_With_Mean_Flows.pdf)
  - [Publication] #TODO

</details>

## Abstract

Recent developments in diffusion- and flow- based models have significantly advanced Text-to-Audio Generation (TTA). 
While achieving great synthesis quality and controllability, current TTA systems still suffer from slow inference speed, which significantly limits their practical applicability.

This paper presents MeanAudio, a novel MeanFlow-based model tailored for fast and faithful text-to-audio generation. 
Built on a Flux-style latent transformer, MeanAudio regresses the average velocity field during training, enabling fast generation by mapping directly from the start to the endpoint of the flow trajectory.
By incorporating classifier-free guidance (CFG) into the training target, MeanAudio incurs no additional cost in the guided sampling process. 
To further stabilize training, we propose an instantaneous-to-mean curriculum with flow field mix-up, which encourages the model to first learn the foundational instantaneous dynamics, and then gradually adapt to mean flows. 
This strategy proves critical for enhancing training efficiency and generation quality.
Experimental results demonstrate that MeanAudio achieves state-of-the-art performance in single-step audio generation. 
Specifically, it achieves a real time factor (RTF) of 0.013 on a single NVIDIA RTX 3090, yielding a 100x speedup over SOTA diffusion-based TTA systems. 
Moreover, MeanAudio also demonstrates strong performance in multi-step generation, enabling smooth and coherent transitions across successive synthesis steps.

## 1·Introduction

Text-to-Audio Generation (TTA) [^Liu2023AudioLDM], [^Ghosal2023Text-to-Audio], [^Huang2023Make-an-Audio] aims to synthesize diverse auditory content from textual prompts. 
By translating language into sound, TTA models unlock a broad spectrum of real-world applications, including virtual reality, gaming, film post-production, and human-computer interaction. 

Recent years have witnessed significant progress in TTA systems, with advancements in architectural design [^Huang2023Make-an-Audio], [^Hai2025EzAudio], [^Hung2024TangoFlux], [^Haji-Ali2024Taming], [^Evans2025Stable], [^Lee2024Etta], [^Valle2025Fugatto], data scaling [^Kong2024Improving], [^Yuan2025Sound-VECaps], [^Liu2024AudioLDM], [^Haji-Ali2024Taming], training objectives [^Majumder2024Tango], [^Liao2024Baton] substantially improving model's generation quality and controllability.  
However, despite these advances, TTA models still suffer from slow inference speed, where they typically require seconds to minutes to synthesis an audio sample. 

Such latency, stemming from the iterative sampling process in flow and diffusion-based models, not only impedes deployment in time-sensitive scenarios such as virtual assistants and interactive gaming, but also hinders the creative workflow of sound creators. 

> <a id="fig:performance">![](figs/performance.pdf)</a>

> MeanAudio achieves state-of-the-art single-step generation performance with a real-time factor (RTF) of 0.013, offering a 100x speedup over existing diffusion-based TTA systems.
> 
> It also demonstrates strong performance in multi-step generation, despite using only 120M parameters.

To accelerate the inference speed of TTA models, recent studies [^Liu2024AudioLCM], [^Liu2025FlashAudio], [^Saito2025SoundCTM], [^Bai2023ConsistencyTTA] have primarily focused on diffusion distillation [^Song2023Consistency].

In this paradigm, the number of diffusion sampling steps are reduced by distilling a pretrained teacher model into a few-step student generator. 
As such, the student network learns to synthesize audio in few steps by modeling the flow trajectory of their multi-step teachers. 
While these models have achieved promising performance in single-step and few-step audio generation, they are inherently limited by the rigid consistency constraints and their reliance on teacher models. 
Moreover, the distillation-based approaches are often computationally expensive, as online methods require holding 2-3 full models in memory at the same time, and offline methods rely on large-scale generation and storage of teacher trajectories before training.

In this paper, we present MeanAudio, a MeanFlow-based [^Geng2025Mean] fast and faithful text-to-audio generator which achieves strong performance in both few-step and multi-step generation. 
Built upon a Flux-style [^BlackForestLabs2024Flux] latent transformer, MeanAudio regresses the average velocity field during training, enabling high quality audio synthesis with only one function evaluation (1-NFE). 
To support guided generation without additional inference cost, we incorporate classifier-free guidance (CFG) [^Ho2022Classifier-Free] into the target velocity field.

To further stabilize training, we propose an instantaneous-to-mean learning curriculum with flow field mix-up. 
This advanced training scheme anchors the model in the foundational instantaneous velocity field, which significantly improves efficiency and enhances performance in both single- and multi-step inference.

Experimental results show that MeanAudio achieves state-of-the-art (SOTA) performance in single-step TTA generation with a real-time factor (RTF) of 0.016,  corresponding to 100x speedup over SOTA diffusion-based TTA system, GenAU, which requires 200 sampling steps.

In addition, it delivers competitive performance in multi-step generation, while maintaining a lightweight architecture with only 120M parameters and being trainable within three days on four NVIDIA RTX 3090 GPUs.

Our main contributions are summarized as follow: 

-  **MeanFlow for Audio Generation:** 
To the best of our knowledge, this is the first study to apply MeanFlow in text-to-audio generation. 
Our model is efficient and self-contained, requiring no established teacher for distillation while being trainable with consumer-grade GPUs.    

-  **Stable Training Curriculum:**

We introduce an instantaneous-to-mean curriculum with flow field mix-up that stabilizes training.

This scheme also proves critical for improving training efficiency and generation quality.

-  
**State-of-the-Art Performance:**
We present MeanAudio, a fast and faithful text-to-audio generator that achieves SOTA performance in single-step audio synthesis, while also delivering strong results in multi-step generation.
To promote further research, we will open-source all the training codes and model weights. 
% Our architectural design, experimental results, and ablation studies provide a foundation for building stronger and faster TTA models using average velocity fields.

## 2·Related Work

### Text to Audio Generation

Text to Audio Generation (TTA) focuses on generating sound based on text inputs.

Current TTA models are often based on the Latent Diffusion Model (LDM) [^Rombach2022High-Resolution] architecture.

Among all, AudioLDM [^Liu2023AudioLDM] pioneered this by training a U-Net to perform denoising conditioned on CLAP [^Wu2023Large-Scale] embeddings.

Tango [^Ghosal2023Text-to-Audio] further improves the instruction following abilities by using the Large Language Model (LLM) FLAN-T5 [^Chung2024Scaling] as the text encoder.

More recently, Stable-Audio-Open [^Evans2025Stable] adopted the Diffusion Transformer (DiT) [^Peebles2023Scalable] to generate variable-length, full-band audio samples.

Meanwhile, LAFMA [^Guan2024Lafma] and TangoFlux [^Hung2024TangoFlux] integrated Flow Matching [^Liu2023Flow] to improve the efficiency and fidelity of audio generation.  

Additionally, recent work has explored Reinforcement Learning (RL) [^Liao2024Baton], [^Majumder2024Tango], [^Wang2025T2a-Feedback] and Retrieval-Augmented Generation (RAG) [^Yuan2024Retrieval-Augmented], [^Yang2024Audiobox] to enhance generation controllability and output quality.

While these models achieve high fidelity and controllability, they often suffer from slow inference due to the iterative sampling process inherent to diffusion- and flow-based models.

### TTA with Inference Acceleration

To accelerate TTA inference, ConsistencyTTA first integrates Consistency Distillation [^Song2023Consistency] on TTA Latent Diffusion Models, where a teacher model, Tango [^Majumder2024Tango] provides supervision to train a distilled few-step student generator.

Subsequently, AudioLCM [^Liu2024AudioLCM] enriches this framework by employing a multi-step Ordinary Differential Equation (ODE) solver, while SoundCTM [^Saito2025SoundCTM] introduces a novel feature distance to enable flexible single-step and multi-step generation. 
In parallel, Presto [^Novack2025Presto!] proposes a dual-faceted distillation strategy that reduces both the number of sampling steps and model parameters to improve inference efficiency.

FlashAudio [^Liu2025FlashAudio] and AudioTurbo [^Zhao2025AudioTurbo] explored the use of Rectified Flow [^Liu2023Flow] and Rectified Diffusion [^Wang2024Rectified] to learn straight generative paths for rapid audio synthesis.

Meanwhile, Stable-Audio-Small [^Novack2025Fast] employs contrastive post-training with adversarial loss to construct a compact few-step generator.

In this work, we investigate the use of MeanFlow to develop a TTA model that achieves strong performance in both single-step and multi-step generation.

## 3·Preliminaries

### Conditional Flow Matching

Flow Matching [^Liu2023Flow], [^Lipman2023Flow], [^Albergo2023Building] is a powerful generative model that learns to match the flows between two probabilistic distributions. 
Given data $x \sim p_\text{data}(x)$, prior $\epsilon \sim p_\text{prior}(\epsilon)$, the optimal transport flow path can be constructed as: $x_t = (1-t)x + t \epsilon$, and the conditional velocity is thus given by $v_t = \frac{dx_t}{dt} = \epsilon - x$.  

At training time, the objective is to find a neural network $f_\theta$ which minimizes the conditional flow matching loss [^Lipman2023Flow]: 

$$
\label{eq:fm_loss}
\mathcal{L}_\text{CFM} = \mathbb{E}_{t, x, \epsilon}\|f_\theta (t, x_t) - v_t\|^2
$$

During sampling, we randomly draw noise $\epsilon$ from $p_\text{prior}$ and solve the ordinary differential equation (ODE) defined below:
$$
dx_t = -f_\theta (t, x_t)dt
$$
The solution is thus given by $x_r = x_t - \int_r^tf_\theta(\tau, x_\tau)d\tau$, where $r$ denotes another time step. 
During implementation, we can use numerical methods (e.g.

Euler Method) to approximate this integration. 

### Mean Flows for Generative Modeling

To accelerate the inference speed of Flow Matching, Mean Flows [^Geng2025Mean] proposed to regress the average velocity field during training, allowing high-quality single-step generation. 
Specifically, given a time interval $[r, t]$, the average velocity within it is defined as:
$u(x_t, r, t) \triangleq \frac{1}{t-r}\int_r^t v(x_\tau, \tau) d\tau$. 
By differentiating both sides with respect to $t$ and re-arranging terms, we obtain the *Mean Flow Identity*, which describes the relation between $v$ and $u$: 

$$

u(x_t, r, t) = v_t - (t-r)\frac{d}{dt}u(x_t, r, t)
$$

We then encourage $f_\theta$ to satisfy this identity by minimizing the mean flow objective: 

$$
\label{eq:mf_loss}
\mathcal{L}_\text{MF}= \mathbb{E}_{t, r, x, \epsilon} \| f_\theta(x_t, r, t) - \text{sg}(u_\text{tgt})\|^2 
$$

Where $u_\text{tgt}=v_t  - (t-r)\frac{d}{dt}f_\theta(x_t, r, t)$, and $\text{sg}(\cdot)$ denotes the stop-gradient operation.  
Note that this total derivative can be expanded by its partial components, corresponding to a Jacobian-Vector Product (JVP): 
$\frac{d}{dt}f_\theta(x_t, r, t) = v_t\partial_x f_\theta+ \partial_t f_\theta$. 
When $r=t$, the mean flow objective becomes the vanilla flow matching objective. 

During sampling, the time integral in CFM can be replaced by the average velocity, leading to: 
$$
x_r = x_t - (t-r)f_\theta(x_t, r, t)
$$
In particular, in single-step generation, we have: 
$
x_0 = x_1 - f_\theta(x_1, 0, 1)
$
where $x_1 = \epsilon \sim p_\text{prior}(\epsilon)$. 
