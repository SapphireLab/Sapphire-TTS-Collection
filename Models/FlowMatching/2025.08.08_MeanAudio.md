# MeanAudio: Fast and Faithful Text-to-Audio Generation With Mean Flows

<details>
<summary>基本信息</summary>

- 标题: "MeanAudio: Fast and Faithful Text-to-Audio Generation With Mean Flows."
- 作者:
  - 01 Xiquan Li
  - 02 Junxi Liu
  - 03 Yuzhe Liang
  - 04 Zhikang Niu
  - 05 Wenxi Chen
  - 06 Xie Chen
- 链接:
  - [ArXiv](https://arxiv.org/abs/2508.06098v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2508.06098v1](PDF/2025.08.08_2508.06098v1_MeanAudio__Fast_and_Faithful_Text-to-Audio_Generation_With_Mean_Flows.pdf)
  - [Publication] #TODO

</details>

## Abstract

Recent developments in diffusion- and flow- based models have significantly advanced Text-to-Audio Generation (TTA). 
While achieving great synthesis quality and controllability, current TTA systems still suffer from slow inference speed, which significantly limits their practical applicability.

This paper presents MeanAudio, a novel MeanFlow-based model tailored for fast and faithful text-to-audio generation. 
Built on a Flux-style latent transformer, MeanAudio regresses the average velocity field during training, enabling fast generation by mapping directly from the start to the endpoint of the flow trajectory.
By incorporating classifier-free guidance (CFG) into the training target, MeanAudio incurs no additional cost in the guided sampling process. 
To further stabilize training, we propose an instantaneous-to-mean curriculum with flow field mix-up, which encourages the model to first learn the foundational instantaneous dynamics, and then gradually adapt to mean flows. 
This strategy proves critical for enhancing training efficiency and generation quality.
Experimental results demonstrate that MeanAudio achieves state-of-the-art performance in single-step audio generation. 
Specifically, it achieves a real time factor (RTF) of 0.013 on a single NVIDIA RTX 3090, yielding a 100x speedup over SOTA diffusion-based TTA systems. 
Moreover, MeanAudio also demonstrates strong performance in multi-step generation, enabling smooth and coherent transitions across successive synthesis steps.

## 1·Introduction

Text-to-Audio Generation (TTA) [^Liu2023AudioLDM], [^Ghosal2023Text-to-Audio], [^Huang2023Make-an-Audio] aims to synthesize diverse auditory content from textual prompts. 
By translating language into sound, TTA models unlock a broad spectrum of real-world applications, including virtual reality, gaming, film post-production, and human-computer interaction. 

Recent years have witnessed significant progress in TTA systems, with advancements in architectural design [^Huang2023Make-an-Audio], [^Hai2025EzAudio], [^Hung2024TangoFlux], [^Haji-Ali2024Taming], [^Evans2025Stable], [^Lee2024Etta], [^Valle2025Fugatto], data scaling [^Kong2024Improving], [^Yuan2025Sound-VECaps], [^Liu2024AudioLDM], [^Haji-Ali2024Taming], training objectives [^Majumder2024Tango], [^Liao2024Baton] substantially improving model's generation quality and controllability.  
However, despite these advances, TTA models still suffer from slow inference speed, where they typically require seconds to minutes to synthesis an audio sample. 

Such latency, stemming from the iterative sampling process in flow and diffusion-based models, not only impedes deployment in time-sensitive scenarios such as virtual assistants and interactive gaming, but also hinders the creative workflow of sound creators. 

> <a id="fig:performance">![](figs/performance.pdf)</a>

> MeanAudio achieves state-of-the-art single-step generation performance with a real-time factor (RTF) of 0.013, offering a 100x speedup over existing diffusion-based TTA systems.
> 
> It also demonstrates strong performance in multi-step generation, despite using only 120M parameters.

To accelerate the inference speed of TTA models, recent studies [^Liu2024AudioLCM], [^Liu2025FlashAudio], [^Saito2025SoundCTM], [^Bai2023ConsistencyTTA] have primarily focused on diffusion distillation [^Song2023Consistency].

In this paradigm, the number of diffusion sampling steps are reduced by distilling a pretrained teacher model into a few-step student generator. 
As such, the student network learns to synthesize audio in few steps by modeling the flow trajectory of their multi-step teachers. 
While these models have achieved promising performance in single-step and few-step audio generation, they are inherently limited by the rigid consistency constraints and their reliance on teacher models. 
Moreover, the distillation-based approaches are often computationally expensive, as online methods require holding 2-3 full models in memory at the same time, and offline methods rely on large-scale generation and storage of teacher trajectories before training.

In this paper, we present MeanAudio, a MeanFlow-based [^Geng2025Mean] fast and faithful text-to-audio generator which achieves strong performance in both few-step and multi-step generation. 
Built upon a Flux-style [^BlackForestLabs2024Flux] latent transformer, MeanAudio regresses the average velocity field during training, enabling high quality audio synthesis with only one function evaluation (1-NFE). 
To support guided generation without additional inference cost, we incorporate classifier-free guidance (CFG) [^Ho2022Classifier-Free] into the target velocity field.

To further stabilize training, we propose an instantaneous-to-mean learning curriculum with flow field mix-up. 
This advanced training scheme anchors the model in the foundational instantaneous velocity field, which significantly improves efficiency and enhances performance in both single- and multi-step inference.

Experimental results show that MeanAudio achieves state-of-the-art (SOTA) performance in single-step TTA generation with a real-time factor (RTF) of 0.016,  corresponding to 100x speedup over SOTA diffusion-based TTA system, GenAU, which requires 200 sampling steps.

In addition, it delivers competitive performance in multi-step generation, while maintaining a lightweight architecture with only 120M parameters and being trainable within three days on four NVIDIA RTX 3090 GPUs.

Our main contributions are summarized as follow: 

-  **MeanFlow for Audio Generation:** 
To the best of our knowledge, this is the first study to apply MeanFlow in text-to-audio generation. 
Our model is efficient and self-contained, requiring no established teacher for distillation while being trainable with consumer-grade GPUs.    

-  **Stable Training Curriculum:**

We introduce an instantaneous-to-mean curriculum with flow field mix-up that stabilizes training.

This scheme also proves critical for improving training efficiency and generation quality.

-  
**State-of-the-Art Performance:**
We present MeanAudio, a fast and faithful text-to-audio generator that achieves SOTA performance in single-step audio synthesis, while also delivering strong results in multi-step generation.
To promote further research, we will open-source all the training codes and model weights. 
% Our architectural design, experimental results, and ablation studies provide a foundation for building stronger and faster TTA models using average velocity fields.
