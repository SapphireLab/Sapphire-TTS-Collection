# MeanAudio: Fast and Faithful Text-to-Audio Generation With Mean Flows

<details>
<summary>基本信息</summary>

- 标题: "MeanAudio: Fast and Faithful Text-to-Audio Generation With Mean Flows."
- 作者:
  - 01 Xiquan Li
  - 02 Junxi Liu
  - 03 Yuzhe Liang
  - 04 Zhikang Niu
  - 05 Wenxi Chen
  - 06 Xie Chen
- 链接:
  - [ArXiv](https://arxiv.org/abs/2508.06098v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2508.06098v1](PDF/2025.08.08_2508.06098v1_MeanAudio__Fast_and_Faithful_Text-to-Audio_Generation_With_Mean_Flows.pdf)
  - [Publication] #TODO

</details>

## Abstract

Recent developments in diffusion- and flow- based models have significantly advanced Text-to-Audio Generation (TTA). 
While achieving great synthesis quality and controllability, current TTA systems still suffer from slow inference speed, which significantly limits their practical applicability.

This paper presents MeanAudio, a novel MeanFlow-based model tailored for fast and faithful text-to-audio generation. 
Built on a Flux-style latent transformer, MeanAudio regresses the average velocity field during training, enabling fast generation by mapping directly from the start to the endpoint of the flow trajectory.
By incorporating classifier-free guidance (CFG) into the training target, MeanAudio incurs no additional cost in the guided sampling process. 
To further stabilize training, we propose an instantaneous-to-mean curriculum with flow field mix-up, which encourages the model to first learn the foundational instantaneous dynamics, and then gradually adapt to mean flows. 
This strategy proves critical for enhancing training efficiency and generation quality.
Experimental results demonstrate that MeanAudio achieves state-of-the-art performance in single-step audio generation. 
Specifically, it achieves a real time factor (RTF) of 0.013 on a single NVIDIA RTX 3090, yielding a 100x speedup over SOTA diffusion-based TTA systems. 
Moreover, MeanAudio also demonstrates strong performance in multi-step generation, enabling smooth and coherent transitions across successive synthesis steps.

## 1·Introduction

Text-to-Audio Generation (TTA) [^Liu2023AudioLDM], [^Ghosal2023Text-to-Audio], [^Huang2023Make-an-Audio] aims to synthesize diverse auditory content from textual prompts. 
By translating language into sound, TTA models unlock a broad spectrum of real-world applications, including virtual reality, gaming, film post-production, and human-computer interaction. 

Recent years have witnessed significant progress in TTA systems, with advancements in architectural design [^Huang2023Make-an-Audio], [^Hai2025EzAudio], [^Hung2024TangoFlux], [^Haji-Ali2024Taming], [^Evans2025Stable], [^Lee2024Etta], [^Valle2025Fugatto], data scaling [^Kong2024Improving], [^Yuan2025Sound-VECaps], [^Liu2024AudioLDM], [^Haji-Ali2024Taming], training objectives [^Majumder2024Tango], [^Liao2024Baton] substantially improving model's generation quality and controllability.  
However, despite these advances, TTA models still suffer from slow inference speed, where they typically require seconds to minutes to synthesis an audio sample. 

Such latency, stemming from the iterative sampling process in flow and diffusion-based models, not only impedes deployment in time-sensitive scenarios such as virtual assistants and interactive gaming, but also hinders the creative workflow of sound creators. 

> <a id="fig:performance">![](figs/performance.pdf)</a>

> MeanAudio achieves state-of-the-art single-step generation performance with a real-time factor (RTF) of 0.013, offering a 100x speedup over existing diffusion-based TTA systems.
> 
> It also demonstrates strong performance in multi-step generation, despite using only 120M parameters.

To accelerate the inference speed of TTA models, recent studies [^Liu2024AudioLCM], [^Liu2025FlashAudio], [^Saito2025SoundCTM], [^Bai2023ConsistencyTTA] have primarily focused on diffusion distillation [^Song2023Consistency].

In this paradigm, the number of diffusion sampling steps are reduced by distilling a pretrained teacher model into a few-step student generator. 
As such, the student network learns to synthesize audio in few steps by modeling the flow trajectory of their multi-step teachers. 
While these models have achieved promising performance in single-step and few-step audio generation, they are inherently limited by the rigid consistency constraints and their reliance on teacher models. 
Moreover, the distillation-based approaches are often computationally expensive, as online methods require holding 2-3 full models in memory at the same time, and offline methods rely on large-scale generation and storage of teacher trajectories before training.

In this paper, we present MeanAudio, a MeanFlow-based [^Geng2025Mean] fast and faithful text-to-audio generator which achieves strong performance in both few-step and multi-step generation. 
Built upon a Flux-style [^BlackForestLabs2024Flux] latent transformer, MeanAudio regresses the average velocity field during training, enabling high quality audio synthesis with only one function evaluation (1-NFE). 
To support guided generation without additional inference cost, we incorporate classifier-free guidance (CFG) [^Ho2022Classifier-Free] into the target velocity field.

To further stabilize training, we propose an instantaneous-to-mean learning curriculum with flow field mix-up. 
This advanced training scheme anchors the model in the foundational instantaneous velocity field, which significantly improves efficiency and enhances performance in both single- and multi-step inference.

Experimental results show that MeanAudio achieves state-of-the-art (SOTA) performance in single-step TTA generation with a real-time factor (RTF) of 0.016,  corresponding to 100x speedup over SOTA diffusion-based TTA system, GenAU, which requires 200 sampling steps.

In addition, it delivers competitive performance in multi-step generation, while maintaining a lightweight architecture with only 120M parameters and being trainable within three days on four NVIDIA RTX 3090 GPUs.

Our main contributions are summarized as follow: 

-  **MeanFlow for Audio Generation:** 
To the best of our knowledge, this is the first study to apply MeanFlow in text-to-audio generation. 
Our model is efficient and self-contained, requiring no established teacher for distillation while being trainable with consumer-grade GPUs.    

-  **Stable Training Curriculum:**

We introduce an instantaneous-to-mean curriculum with flow field mix-up that stabilizes training.

This scheme also proves critical for improving training efficiency and generation quality.

-  
**State-of-the-Art Performance:**
We present MeanAudio, a fast and faithful text-to-audio generator that achieves SOTA performance in single-step audio synthesis, while also delivering strong results in multi-step generation.
To promote further research, we will open-source all the training codes and model weights. 
% Our architectural design, experimental results, and ablation studies provide a foundation for building stronger and faster TTA models using average velocity fields.

## 2·Related Work

### Text to Audio Generation

Text to Audio Generation (TTA) focuses on generating sound based on text inputs.

Current TTA models are often based on the Latent Diffusion Model (LDM) [^Rombach2022High-Resolution] architecture.

Among all, AudioLDM [^Liu2023AudioLDM] pioneered this by training a U-Net to perform denoising conditioned on CLAP [^Wu2023Large-Scale] embeddings.

Tango [^Ghosal2023Text-to-Audio] further improves the instruction following abilities by using the Large Language Model (LLM) FLAN-T5 [^Chung2024Scaling] as the text encoder.

More recently, Stable-Audio-Open [^Evans2025Stable] adopted the Diffusion Transformer (DiT) [^Peebles2023Scalable] to generate variable-length, full-band audio samples.

Meanwhile, LAFMA [^Guan2024Lafma] and TangoFlux [^Hung2024TangoFlux] integrated Flow Matching [^Liu2023Flow] to improve the efficiency and fidelity of audio generation.  

Additionally, recent work has explored Reinforcement Learning (RL) [^Liao2024Baton], [^Majumder2024Tango], [^Wang2025T2a-Feedback] and Retrieval-Augmented Generation (RAG) [^Yuan2024Retrieval-Augmented], [^Yang2024Audiobox] to enhance generation controllability and output quality.

While these models achieve high fidelity and controllability, they often suffer from slow inference due to the iterative sampling process inherent to diffusion- and flow-based models.

### TTA with Inference Acceleration

To accelerate TTA inference, ConsistencyTTA first integrates Consistency Distillation [^Song2023Consistency] on TTA Latent Diffusion Models, where a teacher model, Tango [^Majumder2024Tango] provides supervision to train a distilled few-step student generator.

Subsequently, AudioLCM [^Liu2024AudioLCM] enriches this framework by employing a multi-step Ordinary Differential Equation (ODE) solver, while SoundCTM [^Saito2025SoundCTM] introduces a novel feature distance to enable flexible single-step and multi-step generation. 
In parallel, Presto [^Novack2025Presto!] proposes a dual-faceted distillation strategy that reduces both the number of sampling steps and model parameters to improve inference efficiency.

FlashAudio [^Liu2025FlashAudio] and AudioTurbo [^Zhao2025AudioTurbo] explored the use of Rectified Flow [^Liu2023Flow] and Rectified Diffusion [^Wang2024Rectified] to learn straight generative paths for rapid audio synthesis.

Meanwhile, Stable-Audio-Small [^Novack2025Fast] employs contrastive post-training with adversarial loss to construct a compact few-step generator.

In this work, we investigate the use of MeanFlow to develop a TTA model that achieves strong performance in both single-step and multi-step generation.

## 3·Preliminaries

### Conditional Flow Matching

Flow Matching [^Liu2023Flow], [^Lipman2023Flow], [^Albergo2023Building] is a powerful generative model that learns to match the flows between two probabilistic distributions. 
Given data $x \sim p_\text{data}(x)$, prior $\epsilon \sim p_\text{prior}(\epsilon)$, the optimal transport flow path can be constructed as: $x_t = (1-t)x + t \epsilon$, and the conditional velocity is thus given by $v_t = \frac{dx_t}{dt} = \epsilon - x$.  

At training time, the objective is to find a neural network $f_\theta$ which minimizes the conditional flow matching loss [^Lipman2023Flow]: 

$$
\label{eq:fm_loss}
\mathcal{L}_\text{CFM} = \mathbb{E}_{t, x, \epsilon}\|f_\theta (t, x_t) - v_t\|^2
$$

During sampling, we randomly draw noise $\epsilon$ from $p_\text{prior}$ and solve the ordinary differential equation (ODE) defined below:
$$
dx_t = -f_\theta (t, x_t)dt
$$
The solution is thus given by $x_r = x_t - \int_r^tf_\theta(\tau, x_\tau)d\tau$, where $r$ denotes another time step. 
During implementation, we can use numerical methods (e.g.

Euler Method) to approximate this integration. 

### Mean Flows for Generative Modeling

To accelerate the inference speed of Flow Matching, Mean Flows [^Geng2025Mean] proposed to regress the average velocity field during training, allowing high-quality single-step generation. 
Specifically, given a time interval $[r, t]$, the average velocity within it is defined as:
$u(x_t, r, t) \triangleq \frac{1}{t-r}\int_r^t v(x_\tau, \tau) d\tau$. 
By differentiating both sides with respect to $t$ and re-arranging terms, we obtain the *Mean Flow Identity*, which describes the relation between $v$ and $u$: 

$$

u(x_t, r, t) = v_t - (t-r)\frac{d}{dt}u(x_t, r, t)
$$

We then encourage $f_\theta$ to satisfy this identity by minimizing the mean flow objective: 

$$
\label{eq:mf_loss}
\mathcal{L}_\text{MF}= \mathbb{E}_{t, r, x, \epsilon} \| f_\theta(x_t, r, t) - \text{sg}(u_\text{tgt})\|^2 
$$

Where $u_\text{tgt}=v_t  - (t-r)\frac{d}{dt}f_\theta(x_t, r, t)$, and $\text{sg}(\cdot)$ denotes the stop-gradient operation.  
Note that this total derivative can be expanded by its partial components, corresponding to a Jacobian-Vector Product (JVP): 
$\frac{d}{dt}f_\theta(x_t, r, t) = v_t\partial_x f_\theta+ \partial_t f_\theta$. 
When $r=t$, the mean flow objective becomes the vanilla flow matching objective. 

During sampling, the time integral in CFM can be replaced by the average velocity, leading to: 
$$
x_r = x_t - (t-r)f_\theta(x_t, r, t)
$$
In particular, in single-step generation, we have: 
$
x_0 = x_1 - f_\theta(x_1, 0, 1)
$
where $x_1 = \epsilon \sim p_\text{prior}(\epsilon)$. 

## 4·MeanAudio

> <a id="fig:mean_audio">![](figs/MeanAudio.pdf)</a>

> Model architecture overview: MeanAudio combines $N_1$ multi-modal (MMDiT) blocks and $N_2$ single-modal (DiT) blocks to construct the flow transformer. 
>     It leverages the joint attention in multi-modal blocks to integrate FLAN-T5’s fine-grained text embeddings, and employs AdaLN to inject CLAP’s global conditioning. 
>     \vspace{-0.3cm}
>     % MeanAudio regresses the average velocity field in the latent space during training, enabling fast and faithful audio generation during inference. 
>     % Conditioned on the textual prompt and timestep embeddings, MeanAudio regresses the average velocity field in the latent space for fast and faithful audio generation. 
>     % Based on a Flux-style latent flow transformer, MeanAudio regress the average velocity field during training, enabling fast and faithful audio generation during inference.

As illustrated in Figure [fig:mean_audio](#fig:mean_audio), MeanAudio employs a Flux-Style flow transformer to learn mean velocity in the latent space conditioned on the textual prompt and timestep embeddings.

To stabilize training, we introduce an instantaneous-to-mean learning curriculum with flow field mix-up, guiding the model to learn both short and long displacements along the trajectory. 
In this section, we first detail the architectural design of MeanAudio, then present methods to accelerate its inference speed. 

### Audio Encoding

Following prior works [^Liu2023AudioLDM], [^Ghosal2023Text-to-Audio], we model the generative process in the latent space to improve computational efficiency.

Specifically, we apply the short-time Fourier transform (STFT) to audio waveforms and extract the magnitude component as mel spectrograms [^Stevens1937Scale].

These spectrograms are then encoded into latent representations $x$ using a pretrained variational autoencoder (VAE) [^Kingma2013Auto-Encoding].

During inference, the generated latents are decoded back into spectrograms via the VAE and subsequently converted to audio waveforms using a pretrained vocoder [^Lee2023BigVGAN].

We employ a 1D convolution-based VAE for its superior capacity to model frequency- and length-variable representations.

For a 10-second audio input, the autoencoder produces a latent sequence of 312 tokens, each with a hidden dimension of 20.

\begin{algorithm}
\caption{**MeanAudio Training**}
\label{alg:meanaudio_training}

\begin{algorithmic}[1]
\State **Input:**

Encoded audio latent: $x$, textual conditions $**C**$, flow transformer $f_\theta$. 
\State Sample $t, r \sim \text{lognorm}(\mu, \sigma)$, $\epsilon \sim \mathcal{N}(0, I)$

\State $x_t \gets (1 - t) \cdot x + t \cdot \epsilon$
\State $v_t \gets \epsilon - x$
\State Compute model output $f_\theta(x_t, r, t)$ and its derivative $\frac{df_\theta}{dt}$ via JVP: \\
\hskip1em $f_\theta(x_t, r, t), \frac{df_\theta (x_t, r, t)}{dt}\gets \text{JVP}(f_\theta, (x_t, r, t), (v, 0, 1))$

\State Compute the guided instantaneous velocity estimate:
\State \hskip1em $v_t^\text{cfg} \gets \omega \cdot v_t + \kappa \cdot f_\theta(x_t, t, t, \varnothing) + (1 - w - \kappa) \cdot f_\theta(x_t, t, t, **C**)$
\State Compute the guided average velocity estimate:
\State \hskip1em $u_{\text{tgt}}^\text{cfg} \gets  v_t^\text{cfg} - (t - r) \cdot \frac{d}{dt}f_\theta (x_t, r, t)$
\State $\text{loss} \gets \|f_\theta(x_t, r, t) - \text{sg}(u_{\text{tgt}})\| ^2$
\State **Output:** loss
\end{algorithmic}

\end{algorithm}

\begin{algorithm}
\caption{**MeanAudio Inference**}
\label{alg:meanaudio_inference}

\begin{algorithmic}[1]
\State **Input:**

Trained network $f_\theta$, textual conditions $**C**$
\State Sample noise: $\epsilon \sim \mathcal{N}(0, I)$
\State $x \gets \epsilon$

\For{$i = 0$ to $N-1$}
\State $x = x - (t_{i+1} - t_{i}) \cdot f_\theta(x, t_i, t_{i+1}, **C**)$
\EndFor

\State **Output:** $x$
\end{algorithmic}

\end{algorithm}

### Flow Transformer

Inspired by Flux [^BlackForestLabs2024Flux], we adopt the MMDiT [^Esser2024Scaling] and DiT [^Peebles2023Scalable] blocks to build MeanAudio. 
Specifically, we combine $N_1$ multi-modal transformer blocks with audio/text branches and $N_2$ audio-only DiT blocks to construct our flow transformer.

To further improve the training stability and generation quality, we employ several changes: Firstly, we use ConvMLP rather than vanilla MLPs in the audio stream of MeanAudio.

ConvMLP uses 1D convolutions (kernel size = 3 and padding = 1) rather than linear layers, demonstrating stronger performance in capturing local temporal structure [^Cheng2024Taming].

Secondly, we apply rotary positional embedding (RoPE) [^Su2024Roformer] on the queries and keys in both the audio and text branches. 
Unlike absolute position embeddings, RoPE models the relative distances and is beneficial for variable-length audio generation
Thirdly, we use RMSNorm 

with learnable scales in attention calculation to enable stable and efficient training. 

### Model Conditioning

MeanAudio is conditioned on textual prompt and time steps to render faithful audio signals. 
For text conditioning, we use FLAN-T5 [^Chung2024Scaling] and CLAP [^Wu2023Large-Scale] to extract caption embeddings. 

FLAN-T5 is an instruction-tuned large-language model (LLM) capable of producing fine-grained token embeddings. 
Meanwhile, CLAP is pre-trained on large-scale audio-text dataset and can offer global acoustic-aligned text embeddings. 
Denote $y_\text{T5} \in \mathbb{R}^{N\times d_\text{T5}}$ as the embedding extracted by FLAN-T5, where $N$ and $d_\text{T5}$ represent the number of tokens and model's output dimension.

We feed $y_\text{T5}$ to the text branch of the MMDiT to fuse the detailed textual information into audio branch via multi-modal joint attention.

Furthermore, let $c_\text{CLAP} \in \mathbb{R}^{1\times d_\text{CLAP}}$ represent the global text embedding obtained from the CLAP text encoder (CLAP$_\text{T}$).

We project this embedding through an MLP and combine it with the extracted timestep features to form the global condition $c = t_\text{emb} + r_\text{emb} + c'_\text{CLAP}$. 
This global information is then injected into the model via the scales and biases of adaptive layer normalization (AdaLN) layers.

While FLAN-T5 can capture fine-grained textual details at the token level, CLAP contributes holistic, audio-grounded semantic information.

Together, they provide rich and balanced conditions that improve both the fidelity and semantic alignment of the generated audio. 

### Integrated Classifier-Free Guidance

Classifier-free guidance (CFG) [^Ho2022Classifier-Free] is the widely adopted technique for guiding generative models.

However, using CFG during sampling doubles the number of function evaluations (NFE), as both class-conditional and unconditional model outputs should be computed.

To eliminate the additional cost associated with guided sampling, MeanAudio integrates CFG into the training target, following [^Geng2025Mean]. 
Specifically, define $v_t^\text{cfg}$ as the estimated instantaneous velocity field with guidance, which can be expressed as follows: 
$$
v_t^\text{cfg} = \omega v_t + \underbrace{\kappa f_\theta(x_t, t, t | **C**)}_{\text{cls. conditional}} + \underbrace{(1-\omega-\kappa)f_\theta(x_t, t, t|\varnothing)}_{\text{cls. unconditional}}
$$
Here, $\kappa$ is a mixing factor which combines both class-conditional and unconditional predictions into the guided field, resulting in an effective guidance scale of $\omega' = \frac{\omega}{1-\kappa}$. 
Similarly, we also expose the trainable network $f_\theta$ with class-unconditional inputs, where we randomly drop $**C**$ with 10\% probability, following [^Ho2022Classifier-Free]. 
By replacing $v_t$ with $v_t^\text{cfg}$ in $u_\text{tgt}$, we obtain the average velocity target with guidance, which can be formulated as: 
$$
u_\text{tgt}^\text{cfg} = v^\text{cfg}_t -  (t-r)\frac{d}{dt}f_\theta(x_t, r, t)
$$
By regressing $u_\text{tgt}^{\text{cfg}}$, MeanAudio directly learns the guidance during training, thus avoiding the need for an additional forward pass during generation.

The training and inference procedure of MeanAudio are illustrated in Algorithm [alg:meanaudio_training](#alg:meanaudio_training) and [alg:meanaudio_inference](#alg:meanaudio_inference). 

> <a id="tab:main_results"></a>
> Main results on AudioCaps test set.

The single-step generation results are in *italic*.

The best mulit-step and single-step generation results are **bolded**, the second-best results are \underline{underlined}. 
$\ast$: The parameter count refers to the diffusion backbone, excluding the text encoders, VAE, and vocoders.
$\dag$: The real-time factor (RTF) is evaluated over 100 generations samples on an NVIDIA RTX 3090 GPU.
$\ddag$: For open-source models, we evaluated the performance using official code and checkpoint. 
$\diamondsuit$: These models integrated CFG into training, resulting in *True*

NFEs, for other models, NFE should actually be doubled. 
\vspace{-0.3cm}

> <a id="fig:flow_trajctory">![](figs/Flow_trajactory.pdf)</a>

> % Illustration of our training curriculum, which encourages the model to first learn the foundational instantaneous velocity $v_t$, and then gradually adapt to mean flows $u(x_t, r, t)$ along the trajectory. 
>     Illustration of the underlying instantaneous field $v_t$, which models tangents of the flow trajectory, and the average velocity field $u(x_t, r, t)$, which captures long displacements. 
>     Our training curriculum encourages the model to first learn the foundational instantaneous dynamics, then gradually adapt to mean flows for fast and faithful generation.

### Stabilizing Flow Fields

Although the *Mean Flow Identity* provides an effective training target for learning fast single-step generation, we found that directly modeling audio latent with Eq. [eq:mf_loss](#eq:mf_loss) results in unstable training, slow convergence and poor multi-step generation.

This may be due to several factors:
Firstly, the MF objective defined in Eq.~[eq:mf_loss](#eq:mf_loss) focuses solely on learning the average velocity, which may cause the model to neglect the underlying instantaneous field that serves as the foundation for mean flow [^Peng2025Flow-Anchored].

Secondly, the training target $u^\text{cfg}_\text{tgt}$ is defined by the model’s own derivative.

However, a randomly initialized model may fail to provide effective guidance at the beginning, resulting in slow convergence. 

To address this issue, we propose an instantaneous-to-mean curriculum with flow field mixup to improve training stability and efficiency. 
As illustrated in Figure [fig:flow_trajctory](#fig:flow_trajctory), our curriculum comprises two stages: In the first stage, the model is trained on large-scale, weakly-labeled audio-text datasets to learn the instantaneous velocity field, whose loss is defined in Eq. [eq:fm_loss](#eq:fm_loss).

This stage establishes a strong initialization, allowing the model to first capture the underlying foundational dynamics. 
In the second stage, the model is fine-tuned on a smaller, high-quality dataset to learn the mean velocity. 

In this stage, we adopt the strategy from [^Geng2025Mean], where we blend the instantaneous and average fields by randomly setting $r = t$.

As illustrated in Eq.~[eq:mf_loss](#eq:mf_loss), this operation degenerates the MF objective into standard flow matching.

As such, the network can provide an effective derivative approximation during fine-tuning by leveraging the knowledge acquired in pre-training.

Furthermore, by combining two flows, the model can stably adapt to average field for fast few-step generation, while preserving the multi-step synthesis performance through adherence to instantaneous velocity.

We demonstrate the effectiveness of our training curriculum in the experimental section. 

## 5·Experiments

### Datasets

The datasets we used to train MeanAudio include AudioCaps [^Kim2019Audiocaps], Clotho [^Drossos2020Clotho], and WavCaps [^Mei2024WavCaps].

AudioCaps contains approximately 50,000 10-second audio clips sourced from AudioSet [^Gemmeke2017Audio].

Clotho consists of around 4,000 audio clips ranging from 15 to 30 seconds in duration.

WavCaps comprises 400,000 audio samples collected from multiple sources, including BBC Sound Effects,

FreeSound

, 
SoundBible

and AudioSet-Strong [^Hershey2021Benefit].

While AudioCaps and Clotho provide high quality human-annotated textual captions, WavCaps is weakly labeled using ChatGPT [^Schulman2022Introducing]. 
During training, we truncate all audios into 10 seconds.

For long audios ($>$20s), we crop at most 5 non-overlapping 10s segments. 
We use the test split of AudioCaps as the evaluation set, which contains 957 audio clips.

Each audio of test set is paired with 5 textual captions, and we randomly select 1 caption for audio generation. 
To avoid data leakage, we carefully filtered out all audio samples which overlap with the test split from the training dataset. 
In total, we collect approximately 884k audio-text pairs for training, with a total duration of 2454 hours. 

> <a id="tab:ablation-curriculum"></a>
> Ablation study of the training curriculum. 
% The best single-step and multi-step generation results are **bolded**.
$\dag$: ALL denotes that all four datasets are used together.

AC: only the AudioCaps dataset is used for training. $\ddag: $Hours $\times$ GPU denote training hours on one NVIDIA RTX 3090 GPU. 
\vspace{-0.3cm}

### Metrics

We evaluate model's performance using standard TTA evaluation metrics: 
Fréchet Distance (FD), Fréchet Audio Distance (FAD), Kullback–Leibler Divergence (KL), Inception Score (IS) and CLAP score. 
Among them, FD and FAD measure the distance between the generated audio distribution and the real audio distribution.

A low FD indicates that the generated audio is realistic and closely resembles the reference audio.

KL evaluates how semantically similar the generated audio is to the reference audio.

IS measures the diversity and quality of the generated samples, and CLAP score\footnote{The CLAP score is calculated based on the checkpoint:  \url{https://huggingface.co/lukewys/laion_clap/blob/main/music_speech_audioset_epoch_15_esc_89.98.pt}} measures how the generated audio align with the textual prompt. 
FD, IS and KL are calculated based on the state-of-the-art audio tagger PANNs [^Kong2020Panns], while FAD is calculated by VGGish [^Hershey2017CNN]. 
To further evaluate model's inference speed, we also report the Real-time Factor (RTF) of the system, which denotes the ratio between the total time a system takes to synthesize an audio and the duration of the audio. 

### Implementation Details

We use $N_1=4$ multi-modal blocks and $N_2=8$ single-modal blocks to construct MeanAudio.

The hidden dimension of transformer is set to 448, and the network has a total of 120M parameters. 
For integrated classifier-free guidance, We set $\omega=0.3$ and $\kappa=0.9$, resulting in an effective guidance scale of $\frac{\omega}{1-\kappa}=3$. 
We sample timesteps $(t,r)$ according to a logit-normal distribution [^Esser2024Scaling], with $\mu=0.4$ and $\sigma=1$. 
Given a sampled pair, we assign the larger value to $t$ and the smaller value to $r$.

Additionally, 75\% of the samples are randomly set with $r=t$ to mix the vanilla instantaneous flow field with mean flow field, as described in the previous section. 

Following the curriculum described in the previous section, we first pre-train MeanAudio on WavCaps, AudioCaps and Clotho for 200,000 iterations, and then fine-tune it on AudioCaps for another 200,000 iterations. 
During both stages, we use a learning rate of 1e-4 with a linear warm-up of 1,000 steps.

A step decay schedule reduces the learning rate to 10\% of its original value at 80\% and 90\% of total steps.

Batch sizes are set to 256 and 72 for pre-training and fine-tuning, respectively.

All experiments are conducted on four NVIDIA RTX 3090 GPUs, requiring approximately 24 hours for pre-training and 29 hours for fine-tuning.

### Main Results

We compare the performance of MeanAudio with other TTA models under both few-step and multi-step generation. 
For multi-step ($\geq 10$ NFEs) generation, we compare with state-of-the-art diffusion and flow-based models. 
For few-step generation ($< 10$ NFEs) , we compare with SOTA accelerated TTA models. 

As illustrated in Table [tab:main_results](#tab:main_results), for single-step generation, MeanAudio shows the best performance across all metrics.

Specifically, compared to SOTA, it improves FAD by $23.3\%$, FD by $22.2 \%$, KL by $7.7\%$, IS by $6.8 \%$ and CLAP score by $8.9\%$, respectively. 
Remarkably, its single-step generation performance even surpasses some multi-step baselines that require hundreds of function evaluations.

Moreover, MeanAudio achieves the fastest real-time factor (RTF) of 0.013, representing a 100x speedup over the best-performing diffusion-based model, GenAU, which has an RTF of 1.612.

As the number of sampling steps increases, MeanAudio’s generation quality also improves, revealing a trade-off between inference speed and output fidelity.

Its two-step and five-step generations consistently outperform previous state-of-the-art methods, highlighting its strong few-step generation capabilities.

For multi-step generation, MeanAudio achieves SOTA results in FAD, FD, and CLAP score, indicating better alignment with both the ground-truth audio and the textual prompt. 
It is worth noting that MeanAudio contains only 120M parameters, whereas other TTA systems typically exceed 500M.

### Ablation Studies

\paragraph{Instantaneous-to-Mean Curriculum.}

We conducted a detailed ablation study to investigate the effectiveness of our proposed training curriculum. 
For all experiments, we train the model for approximately 260 epochs to ensure a fair comparison.

Firstly, we evaluate the performance of MeanAudio trained with the standard flow matching objective, which we denote as MeanAudio$_\text{PT}$. 
As shown in Table~[tab:ablation-curriculum](#tab:ablation-curriculum), MeanAudio$_\text{PT}$ delivers strong performance in multi-step generation but performs poorly under single-step settings. 
For instance, it achieves a CLAP score of 0.327 with 25 function evaluations (NFE), while its single-step synthesis yields only $-$0.007. 
This large performance gap arises because the vanilla flow matching objective guides the model only toward instantaneous velocity field and small displacements. 

Secondly, we trained MeanAudio using the mixed flow objective as described in the previous section but without initializing from MeanAudio$_\text{PT}$.

We refer to this variant as MeanAudio$_\text{MF}$. 
For this scenario, we conduct two experiments: training solely on AudioCaps (AC), and training on the combined data from all four datasets.

As illustrated in Figure [tab:ablation-curriculum](#tab:ablation-curriculum), training with mixed flow field significantly improves MeanAudio's single-step generation performance, as the model learns also the average velocity necessary for few-step generation. 
As data scales up, MeanAudio$_\text{MF}$'s generation performance steadily increases. 
Specifically, its single-step CLAP score increases from 0.28 to 0.30, confirming that model can benefit from more diverse training data. 
However, we find that training is difficult to converge, requiring approximately five times more GPU hours compared to training with the standard flow matching objective.

Moreover, MeanAudio$_\text{MF}$ consistently lags behind MeanAudio$_\text{PT}$ in multi-step generation, except for the CLAP score. 
For instance, it only achieves an IS of 10.50, compared to 11.12 of MeanAudio$_\text{{PT}}$. 
These results suggest that learning mean flows from scratch is computationally expensive, as the model lacks prior knowledge of the underlying instantaneous velocity.

Furthermore, it fails to achieve strong multi-step generation performance, indicating that a robust initialization is necessary to accelerate and refine model training.

Finally, our proposed training curriculum yields the best performance in both single-step and multi-step generation, while using significantly less GPU resources. 
This indicates that by pre-training on a large dataset with the vanilla flow matching loss, MeanAudio can first acquire knowledge of foundational the instantaneous velocity field. 
During fine-tuning, the pre-constructed instantaneous field can help the model learn both two fields together, delivering better convergence speed and generation performance. 

**Flow Field Mix-up. **

We then investigated the effectiveness of the flow field mix-up by changing the ratio of $r=t$.

Remember that this equals to the percentage of using the vanilla flow matching objective during the fine-tuning stage.

As illustrated in Table [tab:ablation-flowratio](#tab:ablation-flowratio), when the flow ratio $r=t$ is set to 0 (using only the mean flow objective during fine-tuning), training becomes highly unstable, and convergence is very slow.

Setting the ratio to 25\% stabilizes the training, but we observe only a modest improvement in multi-step generation performance, with IS increasing by just 1.8\%. 
As the flow ratio further increases, convergence accelerates, and generation performance also improves in both single- and multi-step settings. 
Specifically, when the ratio increases from 25\% to 75\%, the single-step CLAP score increases by 30.9\%, and the multi-step IS score improves by 44.6\%. 
These results suggest that in audio latent modeling, jointly learning the instantaneous and mean velocity fields can stabilize training and enhance performance in both single-step and multi-step generation, as the two flow fields complement each other. 

**Integrated Classifier Free Guidance.**

We finally conduct an ablation study across different guidance scales to evaluate the effectiveness of using the classifier-free guidance (CFG).

As shown in Table~[tab:ablation-cfgscale](#tab:ablation-cfgscale), increasing the CFG scale from 1 (no guidance) to 3 leads to substantial improvements in both single-step and multi-step generation.

Specifically, for single-step generation (NFE=1), CLAP and IS increase by 59.6\% and 63.8\%, respectively.

For multi-step generation (NFE=25), the CLAP score improves by 47.9\% and IS by 90.1\%.

These results confirm that incorporating CFG could enhance both perceptual quality and semantic alignment.

Further increasing the guidance scale to 4 yields marginal improvements in FD, but slightly degrades CLAP, IS and KL, suggesting a trade-off between generation diversity and prompt adherence. 
Note that our CFG is integrated into training and incurs no additional cost during sampling.

> <a id="tab:ablation-flowratio"></a>
> Ablation study of the flow mix-up ratio.

> <a id="tab:ablation-cfgscale"></a>
> Ablation study of the CFG scale. 
\vspace{-0.3cm}

## 6·Conclusion

In this work, we present MeanAudio, a novel text-to-audio generator that achieves fast and faithful synthesis by leveraging the Mean Flows objective.

Based on a Flux-style latent transformer, MeanAudio regresses the average velocity field during training, enabling fast generation by directly mapping from the start to the endpoint along the trajectory. 
By incorporating the classifier-free guidance into the training target, MeanAudio avoids the additional guided sampling costs while maintaining strong prompt alignment.

To further improve training stability and generation quality, we introduce an instantaneous-to-mean curriculum with flow field mix-up, which allows the model to benefit from both short- and long-range displacement modeling.

Extensive experiments demonstrate that MeanAudio achieves state-of-the-art performance in both single-step and multi-step settings, with up to 100x faster inference compared to diffusion-based baselines.

Our ablation studies further validate the importance of the proposed training curriculum. 

\bibliography{aaai2026}

\end{document}
