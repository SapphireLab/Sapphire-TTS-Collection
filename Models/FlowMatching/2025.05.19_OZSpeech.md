# OZSpeech: One-Step Zero-Shot Speech Synthesis With Learned-Prior-Conditioned Flow Matching

<details>
<summary>基本信息</summary>

- 标题: "OZSpeech: One-Step Zero-Shot Speech Synthesis With Learned-Prior-Conditioned Flow Matching."
- 作者:
  - 01 Hieu-Nghia Huynh-Nguyen
  - 02 Ngoc Son Nguyen
  - 03 Huynh Nguyen Dang
  - 04 Thieu Vo
  - 05 Truong-Son Hy
  - 06 Van Nguyen
- 链接:
  - [ArXiv](https://arxiv.org/abs/2505.12800v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2505.12800v1](PDF/2025.05.19_2505.12800v1_OZSpeech__One-Step_Zero-Shot_Speech_Synthesis_With_Learned-Prior-Conditioned_Flow_Matching.pdf)
  - [Publication] #TODO

</details>

## Abstract

Text-to-speech (TTS) systems have seen significant advancements in recent years, driven by improvements in deep learning and network architectures.
Viewing the output speech as a data distribution, previous approaches often employ traditional speech representations, such as waveforms or spectrograms, within the Flow Matching framework.
However, these methods have limitations, including overlooking various speech attributes and incurring high computational costs due to additional constraints introduced during training.
To address these challenges, we introduce *OZSpeech*, the first TTS method to explore optimal transport conditional flow matching with one-step sampling and a learned prior as the condition, effectively disregarding preceding states and reducing the number of sampling steps.
Our approach operates on disentangled, factorized components of speech in token format, enabling accurate modeling of each speech attribute, which enhances the TTS system's ability to precisely clone the prompt speech.
Experimental results show that our method achieves promising performance over existing methods in content accuracy, naturalness, prosody generation, and speaker style preservation.
Code and audio samples are available at our demo page \footnote{\url{https://ozspeech.github.io/OZSpeech_Web/}}.

## 1·Introduction

Text-to-speech (TTS) has numerous real-world applications, such as voice-based virtual assistants, assistive screen readers for the visually impaired, and reading aids for people with dyslexia, to name a few.

Most TTS systems focus on synthesizing speech that matches a speaker in a set of speakers seen during training.

Recent studies tackle a more challenging problem of converting text into speech that follows the acoustic characteristics of a prompt spoken by a speaker not seen during training.

This problem is called zero-shot TTS.

In recent years, remarkable progress has been achieved in the research of Zero-shot TTS models. 
These advancements have demonstrated the impressive capabilities of such models, with their synthesized outputs often approaching a quality level that is virtually indistinguishable from human speech. 
The body of research on Zero-Shot TTS can be broadly divided into two primary categories, each aligned with a dominant methodological paradigm in the field: autoregressive models and diffusion-based models.

Prominent examples of the autoregressive approach are VALL-E [^Chen2025Neural] and its variants [^Chen2024Vall-E], [^Zhang2023Speak], [^Han2024Vall-E], [^Meng2024Autoregressive], [^Song2024Ella-V], [^Peng2024V}oice{C}raft], [^Ji2024M}obile{S}peech], which have significantly advanced Zero-Shot TTS by integrating language modeling techniques and employing disentangled speech units as input and output tokens. 
This innovative framework has paved the way for the potential convergence of Zero-Shot TTS with large language models (LLMs), enabling the creation of efficient, multimodal systems which are capable of generating text, speech, and other modalities in a flexible and scalable manner. 
However, as with other LLM-based systems, autoregressive models are susceptible to the issue of the non-deterministic sampling process, potentially leading to infinite repetition, which remains a critical challenge in applications requiring high levels of precision and reliability.

In contrast, diffusion-based models, as demonstrated by state-of-the-art (SOTA) TTS systems such as E2 TTS [^Eskimez2024E2] and other related approaches [^Le2023Voicebox], [^Vyas2023Audiobox], [^Shen2024NaturalSpeech], [^Ju2024N}atural{S}peech], have emerged as powerful generative frameworks capable of producing high-quality, natural-sounding audio.

This approach has proven particularly effective in specialized tasks such as in-filling and speech editing. 
Nevertheless, diffusion-based models face limitations in real-time applications due to the computational inefficiency of their multi-step sampling processes. 

These constraints underscore the trade-offs inherent in the diffusion-based paradigm, particularly in scenarios that demand low-latency performance.

Distillation methods for diffusion-based models have been explored to address the multi-step sampling challenge, with Consistency Models [^Song2023Consistency] introducing one-to-one mapping functions that transform intermediate states along the Ordinary Differential Equation (ODE) trajectory directly to their origin. 
This approach reduces sampling steps to one while maintaining output quality but requires access to a full range of $t \in [0, 1]$ to approximate trajectories, demanding extensive training steps. 
As an alternative, Shortcut Models [^Frans2024One] condition the network on noise level and step size, enabling faster generation with fewer training steps by using only a subset of $t$ values. 
However, this method is computationally intensive due to additional constraints introduced during training, making it more resource-demanding than Consistency Models.

To capitalize on the strengths and mitigate the limitations of the aforementioned approaches, we propose OZSpeech (**O**ne-step **Z**ero-shot **Speech**

Synthesis with Learned-Prior-Conditioned Flow Matching), a novel Zero-Shot TTS system.

Our model leverages optimal transport conditional flow matching [^Lipman2023Flow] (OT-CFM), a class of diffusion-based models. 
We reformulate the original OT-CFM to enable single-step sampling, where the vector field estimator regresses the trajectories of all pairs of initial points from the learned prior distribution, rather than conventional Gaussian noise, to their respective target distributions.

By minimizing the distance between the initial points and their origins while implicitly learning the optimal $t$ for each prior, this approach eliminates the need to access a comprehensive range of $t$ values or compute additional constraints, thereby ensuring high-fidelity synthesized speech.

The key contributions of this paper are as follows:

\begin{compactitem}

-  We propose a reformulated OT-CFM framework that effectively initializes the starting points of the flow matching process using samples from a learned prior distribution.

This prior is optimized to closely approximate the target distribution, enabling one-step sampling with minimal errors.

Our framework requires only a single training run without the need for an extensive distillation stage.

-  We propose a simple yet effective network architecture to learn prior-distributed codes.

-  Compared to previous methods, our model yields multi-fold improvement in WER and latency, achieving significant reduction in model size while striking a balance with acoustical quality.

In addition, while previous models suffer from increasing noise level in the audio prompts, OZSpeech's WER remains stable, highlighting the excellent noise-tolerant intelligibility of our method.

Our model requires significantly less computation, with inference speed being $2.7 - 6.5$ times faster than the other methods.

Our model is only 29\%-71\% the size of the other models.
% Our model outperforms SOTA methods on a range of objective evaluation metrics yet have significantly smaller size and shorter inference time.

We achieve model size reduction of 76\% and faster inference time by 70\% compared to SOTA methods, respectively.

\end{compactitem}

## 2·Related Work

Zero-Shot TTS enables the generation of speech in an unseen speaker's voice using only a few seconds of audio as a prompt; this process is often termed voice mimicking.

Advances in large-scale generative models have driven significant progress in this field.

One prominent development is the adoption of diffusion models [^Ho2020Denoising], [^Song2021Score-Based], which have demonstrated remarkable performance [^Kang2023ZET-Speech], [^Tran2023Sten-TTS], [^Shen2024NaturalSpeech], [^Ju2024N}atural{S}peech].

Another approach, flow matching [^Lipman2023Flow], [^Liu2023Flow], has further advanced the state-of-the-art by delivering strong results with reduced inference times [^Kim2023P-Flow], [^Mehta2024Matcha-TTS], [^Eskimez2024E2], [^Chen2024F5-TTS].

Additionally, a key innovation in Zero-Shot TTS is the use of discrete tokens, often derived from neural codecs [^Wang2023Neural], [^Kharitonov2023Speak,], [^Chen2024Vall-E], [^Ju2024NaturalSpeech], [^Du2024UniCATS].

Neural codecs are designed to learn discrete speech and audio tokens, often referred to as acoustic tokens, while preserving reconstruction quality and maintaining a low bitrate.

SoundStream [^Zeghidour2022SoundStream] is a well-known example that employs a vector-quantized variational autoencoder (VQ-VAE), which was first introduced by [^Oord2017Neural] in the field of computer vision, and later adapted to TTS, to disentangle continuous data into discrete tokens.

It comprises multiple residual vector quantizers to compress speech into multiple tokens, which serve as intermediate representations for speech generation.

A significant breakthrough in this area, inspired by the success of LLMs in natural language processing, is VALL-E [^Chen2025Neural], a pioneering work in this domain.

VALL-E represents speech as discrete codec codes using an off-the-shelf neural codec and redefines TTS as a conditional codec language modeling task.

This approach has sparked further research and development in the field [^Kharitonov2023Speak,], [^Zhang2023Speak], [^Chen2024Vall-E], [^Han2024Vall-E], [^Du2024Cosyvoice].
