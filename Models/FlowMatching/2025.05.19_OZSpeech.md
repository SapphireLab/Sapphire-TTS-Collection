# OZSpeech: One-Step Zero-Shot Speech Synthesis With Learned-Prior-Conditioned Flow Matching

<details>
<summary>基本信息</summary>

- 标题: "OZSpeech: One-Step Zero-Shot Speech Synthesis With Learned-Prior-Conditioned Flow Matching."
- 作者:
  - 01 Hieu-Nghia Huynh-Nguyen
  - 02 Ngoc Son Nguyen
  - 03 Huynh Nguyen Dang
  - 04 Thieu Vo
  - 05 Truong-Son Hy
  - 06 Van Nguyen
- 链接:
  - [ArXiv](https://arxiv.org/abs/2505.12800v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2505.12800v1](PDF/2025.05.19_2505.12800v1_OZSpeech__One-Step_Zero-Shot_Speech_Synthesis_With_Learned-Prior-Conditioned_Flow_Matching.pdf)
  - [Publication] #TODO

</details>

## Abstract

Text-to-speech (TTS) systems have seen significant advancements in recent years, driven by improvements in deep learning and network architectures.
Viewing the output speech as a data distribution, previous approaches often employ traditional speech representations, such as waveforms or spectrograms, within the Flow Matching framework.
However, these methods have limitations, including overlooking various speech attributes and incurring high computational costs due to additional constraints introduced during training.
To address these challenges, we introduce *OZSpeech*, the first TTS method to explore optimal transport conditional flow matching with one-step sampling and a learned prior as the condition, effectively disregarding preceding states and reducing the number of sampling steps.
Our approach operates on disentangled, factorized components of speech in token format, enabling accurate modeling of each speech attribute, which enhances the TTS system's ability to precisely clone the prompt speech.
Experimental results show that our method achieves promising performance over existing methods in content accuracy, naturalness, prosody generation, and speaker style preservation.
Code and audio samples are available at our demo page \footnote{\url{https://ozspeech.github.io/OZSpeech_Web/}}.

## 1·Introduction

Text-to-speech (TTS) has numerous real-world applications, such as voice-based virtual assistants, assistive screen readers for the visually impaired, and reading aids for people with dyslexia, to name a few.

Most TTS systems focus on synthesizing speech that matches a speaker in a set of speakers seen during training.

Recent studies tackle a more challenging problem of converting text into speech that follows the acoustic characteristics of a prompt spoken by a speaker not seen during training.

This problem is called zero-shot TTS.

In recent years, remarkable progress has been achieved in the research of Zero-shot TTS models. 
These advancements have demonstrated the impressive capabilities of such models, with their synthesized outputs often approaching a quality level that is virtually indistinguishable from human speech. 
The body of research on Zero-Shot TTS can be broadly divided into two primary categories, each aligned with a dominant methodological paradigm in the field: autoregressive models and diffusion-based models.

Prominent examples of the autoregressive approach are VALL-E [^Chen2025Neural] and its variants [^Chen2024Vall-E], [^Zhang2023Speak], [^Han2024Vall-E], [^Meng2024Autoregressive], [^Song2024Ella-V], [^Peng2024V}oice{C}raft], [^Ji2024M}obile{S}peech], which have significantly advanced Zero-Shot TTS by integrating language modeling techniques and employing disentangled speech units as input and output tokens. 
This innovative framework has paved the way for the potential convergence of Zero-Shot TTS with large language models (LLMs), enabling the creation of efficient, multimodal systems which are capable of generating text, speech, and other modalities in a flexible and scalable manner. 
However, as with other LLM-based systems, autoregressive models are susceptible to the issue of the non-deterministic sampling process, potentially leading to infinite repetition, which remains a critical challenge in applications requiring high levels of precision and reliability.

In contrast, diffusion-based models, as demonstrated by state-of-the-art (SOTA) TTS systems such as E2 TTS [^Eskimez2024E2] and other related approaches [^Le2023Voicebox], [^Vyas2023Audiobox], [^Shen2024NaturalSpeech], [^Ju2024N}atural{S}peech], have emerged as powerful generative frameworks capable of producing high-quality, natural-sounding audio.

This approach has proven particularly effective in specialized tasks such as in-filling and speech editing. 
Nevertheless, diffusion-based models face limitations in real-time applications due to the computational inefficiency of their multi-step sampling processes. 

These constraints underscore the trade-offs inherent in the diffusion-based paradigm, particularly in scenarios that demand low-latency performance.

Distillation methods for diffusion-based models have been explored to address the multi-step sampling challenge, with Consistency Models [^Song2023Consistency] introducing one-to-one mapping functions that transform intermediate states along the Ordinary Differential Equation (ODE) trajectory directly to their origin. 
This approach reduces sampling steps to one while maintaining output quality but requires access to a full range of $t \in [0, 1]$ to approximate trajectories, demanding extensive training steps. 
As an alternative, Shortcut Models [^Frans2024One] condition the network on noise level and step size, enabling faster generation with fewer training steps by using only a subset of $t$ values. 
However, this method is computationally intensive due to additional constraints introduced during training, making it more resource-demanding than Consistency Models.

To capitalize on the strengths and mitigate the limitations of the aforementioned approaches, we propose OZSpeech (**O**ne-step **Z**ero-shot **Speech**

Synthesis with Learned-Prior-Conditioned Flow Matching), a novel Zero-Shot TTS system.

Our model leverages optimal transport conditional flow matching [^Lipman2023Flow] (OT-CFM), a class of diffusion-based models. 
We reformulate the original OT-CFM to enable single-step sampling, where the vector field estimator regresses the trajectories of all pairs of initial points from the learned prior distribution, rather than conventional Gaussian noise, to their respective target distributions.

By minimizing the distance between the initial points and their origins while implicitly learning the optimal $t$ for each prior, this approach eliminates the need to access a comprehensive range of $t$ values or compute additional constraints, thereby ensuring high-fidelity synthesized speech.

The key contributions of this paper are as follows:

\begin{compactitem}

-  We propose a reformulated OT-CFM framework that effectively initializes the starting points of the flow matching process using samples from a learned prior distribution.

This prior is optimized to closely approximate the target distribution, enabling one-step sampling with minimal errors.

Our framework requires only a single training run without the need for an extensive distillation stage.

-  We propose a simple yet effective network architecture to learn prior-distributed codes.

-  Compared to previous methods, our model yields multi-fold improvement in WER and latency, achieving significant reduction in model size while striking a balance with acoustical quality.

In addition, while previous models suffer from increasing noise level in the audio prompts, OZSpeech's WER remains stable, highlighting the excellent noise-tolerant intelligibility of our method.

Our model requires significantly less computation, with inference speed being $2.7 - 6.5$ times faster than the other methods.

Our model is only 29\%-71\% the size of the other models.
% Our model outperforms SOTA methods on a range of objective evaluation metrics yet have significantly smaller size and shorter inference time.

We achieve model size reduction of 76\% and faster inference time by 70\% compared to SOTA methods, respectively.

\end{compactitem}

## 2·Related Work

Zero-Shot TTS enables the generation of speech in an unseen speaker's voice using only a few seconds of audio as a prompt; this process is often termed voice mimicking.

Advances in large-scale generative models have driven significant progress in this field.

One prominent development is the adoption of diffusion models [^Ho2020Denoising], [^Song2021Score-Based], which have demonstrated remarkable performance [^Kang2023ZET-Speech], [^Tran2023Sten-TTS], [^Shen2024NaturalSpeech], [^Ju2024N}atural{S}peech].

Another approach, flow matching [^Lipman2023Flow], [^Liu2023Flow], has further advanced the state-of-the-art by delivering strong results with reduced inference times [^Kim2023P-Flow], [^Mehta2024Matcha-TTS], [^Eskimez2024E2], [^Chen2024F5-TTS].

Additionally, a key innovation in Zero-Shot TTS is the use of discrete tokens, often derived from neural codecs [^Wang2023Neural], [^Kharitonov2023Speak,], [^Chen2024Vall-E], [^Ju2024NaturalSpeech], [^Du2024UniCATS].

Neural codecs are designed to learn discrete speech and audio tokens, often referred to as acoustic tokens, while preserving reconstruction quality and maintaining a low bitrate.

SoundStream [^Zeghidour2022SoundStream] is a well-known example that employs a vector-quantized variational autoencoder (VQ-VAE), which was first introduced by [^Oord2017Neural] in the field of computer vision, and later adapted to TTS, to disentangle continuous data into discrete tokens.

It comprises multiple residual vector quantizers to compress speech into multiple tokens, which serve as intermediate representations for speech generation.

A significant breakthrough in this area, inspired by the success of LLMs in natural language processing, is VALL-E [^Chen2025Neural], a pioneering work in this domain.

VALL-E represents speech as discrete codec codes using an off-the-shelf neural codec and redefines TTS as a conditional codec language modeling task.

This approach has sparked further research and development in the field [^Kharitonov2023Speak,], [^Zhang2023Speak], [^Chen2024Vall-E], [^Han2024Vall-E], [^Du2024Cosyvoice].

## 3·Method

> <a id="fig:overall-architecture">![](figs/overall-2.pdf)</a>

> Overview of OZSpeech: (a) The overall architecture: The text prompt is converted to phonemes and then into prior codes via the Prior Codes Generator.
> 
> Simultaneously, the audio prompt is encoded into codes using the FACodec Encoder.
> 
> These codes are concatenated along the sequence dimension and fed into the OT-CFM Vector Field Estimator, which generates codes preserving the text content and acoustic attributes.
> 
> Finally, the FACodec Decoder converts them into output speech. (b) The Prior Codes Generator $f_\psi(\cdot)$ produces sequences of phoneme-aligned codes. 
>     (c) The Vector Field Estimator refines these codes with the prosody and acoustic details from the acoustic prompt.
> 
> Before being fed through $v_\theta(\cdot, \cdot)$, six sequences of codes are first enhanced via Quantizer Embedding, which serves as an identifier for each sequence within the hidden space.
> 
> These embeddings are then folded along the hidden dimension and processed by the network to estimate the velocity of the prior codes. 
>     % The input sequence is subsequently enriched with Position Embedding and Time Embedding before being processed by the network to estimate the velocity of the prior codes.

### Problem Statement

\label{sec:problem-statement}

In this paper, we consider the problem of generating speech from given text and acoustic prompt such that conditions for the outputs are met. 
Viewing the synthesized speech as a data distribution, denoted as $\text{x}_1 \sim p_1(x)$, previous methods often construct the output data distribution from a noise distribution $\text{x}_0 \sim p_0(x)$. 
However, we propose constructing the output data distribution from a feasible intermediate state candidate $\text{x}_{\text{pr}} \sim p_{\text{prior}}(x)$ instead of $\text{x}_0$, thereby disregarding preceding states and reducing the number of sampling steps. 
To achieve this, we undertake the following steps:

\begin{compactitem}

-  **Prior Code Generation:**

We design an effective method for generating prior codes to produce $**x**_{\text{pr}}$ (see Section [sec:prior-codes-gen](#sec:prior-codes-gen)).

-  **Vector Field Estimation:**

We develop a vector field estimator to approximate $v_{\theta}$ , facilitating the transition from $\text{x}_{\text{pr}}$ to $\text{x}_{1}$ (see Section [sec:osfm](#sec:osfm)).

-  **Waveform Decomposition via FACodec:**

We employ FACodec [^Ju2024N}atural{S}peech], a neural codec disentangler framework, to decompose the waveform into distinct components, including speaker identity and sequences of codes encoding prosody, content, and acoustic 
details.

This decomposition enables precise control over the aspects of speech to be preserved or modified.
\end{compactitem}

### Prior Codes Generation Modeling

\label{sec:prior-codes-gen}

Our key contribution to prior code generation is that the process follows a hierarchical structure: each code sequence generation depends on the preceding code sequences, while the condition for the first code sequence is initialized based on phoneme embeddings.

To achieve this, we implement a cascaded neural network where specific decoder layers generate the respective code sequences in the hierarchy (shown in Fig.~[fig:overall-architecture](#fig:overall-architecture)b).

Formally, the Prior Codes Generator $\mathbf{f}_{\psi}(\cdot)$ is modeled as:

$$
\label{eq:pcg}
p(\mathbf{q}_{1:6} \mid \mathbf{p}; \psi) = p(\mathbf{q}_{1} \mid \mathbf{p}; f_{\psi}^1) \prod_{j=2}^{6} p(\mathbf{q}_{j} \mid \mathbf{q}_{j-1}; f_{\psi}^j),
$$

where \(\mathbf{q}_j\) is the \(j\)-th code sequence from the Feed-Forward Transformer (FFT) decoder layer \(f_{\psi}^j\) and \(\mathbf{p}\) represents phoneme embeddings as the initial condition. 
The Prior Loss \(\mathcal{L}_{\text{prior}}\) minimizes the negative logarithm of the joint probability in Eq.~ \eqref{eq:pcg}, ensuring content code sequences are learned effectively by conditioning on phonemes, while the others, including prosody and acoustic details, converge towards the mean representations.

The Prior Codes Generator produces semantically meaningful codes, reducing the distance between \(\mathbf{x}_{\text{pr}}\) and \(\mathbf{x}_1\), allowing the Vector Field Estimator \(\mathbf{v}_{\theta}(\cdot, \cdot)\) to approximate vectors from a mean distribution rather than generating them from pure noise.

To align the input phonemes with their corresponding output code sequences, we employ a neural network functioning as a Duration Predictor, as introduced in [^Ren2019FastSpeech]. 
Briefly, the Duration Predictor estimates the duration (i.e., the number of acoustic tokens) for each input phoneme. 
The phoneme embeddings are duplicated accordingly before being passed through the decoder of the Prior Codes Generator.

We define the loss function used to train the Duration Predictor as $\mathcal{L}_{dur}$, which aims to minimize the mean squared error between the predicted and ground truth durations on a logarithmic scale.

### One-Step Optimal Transport Flow Matching for Zero-Shot TTS

\label{sec:osfm}

**One-Step Optimal Transport Flow Matching Formulation**.

We rectify the OT-CFM paradigm, simultaneously introduced by [^Lipman2023Flow] and [^Liu2023Flow] and first adapted to text generation by [^Hu2024Flow].

Our approach involves constructing a vector field that regresses the velocity of non-Gaussian distribution and data distribution pairs, where the initial distribution closely approximates the target distribution.

To this end, we reformulate the original flow matching loss equation (details provided in [appendix:fm](#appendix:fm)) to explicitly account for the discrepancies between the non-Gaussian initial distribution and the target distribution.

Let $\mathbf{x}_t$ denote a linear probability path, starting from a purely noisy initial point $\mathbf{x}_0 \sim \mathcal{N}(0, I)$ and progressing towards a data point $\mathbf{x}_1 \sim \mathcal{D}$.

It is defined as $\mathbf{x}_t = t\mathbf{x}_1 + (1-t)\mathbf{x}_0$,

where $t \sim \mathcal{U}(0, 1)$ denotes the interpolation parameter. 
Based on this, the initial point $\mathbf{x}_0$ can be derived as $\mathbf{x}_0 = \frac{t\mathbf{x}_1 - \mathbf{x}_t}{1 - t}$. 
Thus, the OT-CFM objective, as presented in Eq.~\eqref{eq:orgin-cfm}, can be reformulated as follows:

$$
\label{eq:ot-cfm-adjusted}
\mathcal{L}_{CFM}(\theta) = \mathbb{E}_{t,\mathbf{x}_0,\mathbf{x}_1}
\left\lVert
\mathbf{v}_\theta(\mathbf{x}_t, t) - \frac{\mathbf{x}_1 - \mathbf{x}_t}{1 - t}
\right\rVert^2.
$$

We assume that $\mathbf{x}_t$ can be estimated via $\mathbf{f}_{\psi}(\cdot)$.

Under this assumption, $\mathbf{x}_t$ is treated as a learnable state, which we denote as $\mathbf{x}_{\text{pr}}$, while $t$ is regarded as an unknown interpolation parameter, represented as a prior-dependent time variable $\tau$. 
Consequently, Eq.~([eq:ot-cfm-adjusted](#eq:ot-cfm-adjusted)) can be reformulated as follows:

$$
\label{eq:ot-cfm-adjusted-1}

\begin{split}
\mathcal{L}_{\text{CFM}}(\theta) = \mathbb{E}_{\mathbf{x}_{\text{pr}}, \mathbf{x}_1}
\left\lVert
\mathbf{v}_\theta(\mathbf{x}_{\text{pr}}, \tau) - \frac{\mathbf{x}_1 - \mathbf{x}_{\text{pr}}}{1 - \tau}
\right\rVert^2.
\end{split}

$$

This paradigm is similar to the original OT-CFM in its objective of regressing the velocity between the initial prior $\mathbf{x}_0$ and data $\mathbf{x}_1$ pairs. 
However, unlike the original approach, it does not access the distribution of $\mathbf{x}_0$ during training.

Therefore, it also does not enforce $\mathbf{x}_0$ to follow a normal distribution.

Furthermore, as the prior distribution $\mathbf{x}_{\text{pr}}$ approaches the target distribution $\mathbf{x}_1$, both the number of sampling steps and the magnitude of each step are substantially reduced. 
This convergence enables the sampling process to be efficiently performed in as few as a single step.\\[8pt]
**Vector Field Estimator Modeling**. 

To model $\mathbf{v}_{\theta}(\cdot,\cdot)$, we define the latent representations of the prior, target, and estimated target distributions as $\mathbf{x}_{\text{pr}}$, $\mathbf{x}_1$, and $\mathbf{\tilde{x}}_1$, respectively, where $\mathbf{x}_{\text{pr}}, \mathbf{x}_1, \mathbf{\tilde{x}}_1 \in \mathbb{R}^{6 \times N \times D}$. 
These terms represent six quantizer categories with sequence length $N$ and feature dimensionality $D$. 
Inspired by diffusion-based text generation models like Diffuseq [^Gong2023DiffuSeq], Difformer [^Gao2024Empowering], and FlowSeq [^Hu2024Flow], we condition $\mathbf{x}_{\text{pr}}$ on an acoustic prompt to guide $\mathbf{v}_{\theta}(\cdot,\cdot)$ in transferring speech attributes other than content.

Specifically, only prosody and acoustic detail codes serve as prompts, while the content codes are masked to prevent undesired content transfer.

The input to $\mathbf{v}_{\theta}(\cdot,\cdot)$ is then formulated as the concatenated representation:

$$
\label{eq:z-prior}
\mathbf{z_{\text{pr}}} = **Concat**(\mathbf{y}_{\text{mask}}, \mathbf{x}_{\text{pr}} + \epsilon), \quad  \epsilon \sim \mathcal{N}(0,I),
$$

where $\mathbf{z}_\text{pr} \in \mathbb{R}^{6 \times L \times D}$, with $L = M + N$ representing the total sequence length of the concatenated input.
$\mathbf{y}_\text{mask}$ denotes the content-masking representation of the acoustic prompt.

Random Gaussian noise $\epsilon$ is added to the latent representation of $\mathbf{x}_{\text{pr}}$ to ensure the robustness and diversity of the model.

The diagram of the Vector Field Estimator is shown in Fig.~[fig:overall-architecture](#fig:overall-architecture)c.\\[8pt]
**Folding Mechanism and Quantizer Encoding**.

Our data representation consists of six sequences of quantizer embeddings, making direct sequence modeling with Transformers challenging. 
Previous works using Neural Codec for audio discretization, such as VALL-E [^Chen2025Neural], VALL-E 2 [^Chen2024Vall-E], and *NaturalSpeech3* [^Ju2024N}atural{S}peech], model each quantizer sequence independently. 
While effective, this approach incurs high computational costs and long generation times due to sequential processing.

To mitigate these inefficiencies, we propose modeling all six quantizers simultaneously by folding them along the hidden dimension. 
Let $\mathcal{F}(\cdot)$ be the folding function, defined as a composition of two transformations: $\mathcal{G}: \mathbb{R}^{6 \times L \times D} \to \mathbb{R}^{L \times 6D}$ and $\mathcal{H}: \mathbb{R}^{L \times 6D} \to \mathbb{R}^{L \times D'}$.

Thus, $\mathcal{F}$ is expressed as:
$\mathcal{F} \triangleq \mathcal{H} \circ \mathcal{G}: \mathbb{R}^{6 \times L \times D} \to \mathbb{R}^{L \times D'}.
$
Within this framework, the function $\mathcal{F}(\cdot)$ permutes and reshapes the input tensor to sequentially align the component quantizers in the hidden space, following the prescribed order of prosody, content, and acoustic details. 
Subsequently, it adjusts the tensor's dimensionality accordingly.

In addition, we propose a quantizer encoding mechanism designed to identify specific quantizers within the hidden space. 
This mechanism operates in conjunction with the $\mathcal{F}(\cdot)$ function. 
The quantizer encoding is formally defined as follows:

$$
\label{eq:vq-encode}
\mathcal{Q}(x) = x + **Dup**(\omega, L), \quad x \in \mathbb{R}^{6 \times L \times D}.
$$

Here, the quantizer encoding function $\mathcal{Q}(\cdot)$ integrates the input tensor $\mathbb{R}^{6 \times L \times D}$ with the embedding $**Dup**(\omega, L)$.

In this context, $\omega \in \mathbb{R}^{6 \times 1 \times D}$ plays a role as the identifier for the latent representation of the quantizers, while the function $**Dup**(\cdot, \cdot)$ duplicates the identifier along the sequence length $L$.

With $\mathcal{Q}(\cdot)$, we aim to prevent the model from confusing the quantizers with each other, even when they are simultaneously modeled within a single sequence.

Consequently, the latent representation $\mathbf{z}_{\text{pr}}$ is transformed into $\breve{\mathbf{z}}_{\text{pr}}$, which subsequently serves as the direct input to the function $\mathbf{v}_{\theta}(\cdot, \cdot)$.

The transformation is defined as $\breve{\mathbf{z}}_{\text{pr}} = (\mathcal{F} \circ \mathcal{Q})(\mathbf{z}_{\text{pr}})$.\\[8pt]

**Anchor Loss**.

To optimize generative model performance on discrete token data and stabilize training, we use Anchor Loss \(\mathcal{L}_{anchor}\) as a regularization term for embeddings.

Initially introduced in diffusion models for discrete data [^Gao2024Empowering] and later adapted for flow matching models [^Hu2024Flow], \(\mathcal{L}_{anchor}\) measures the difference between an intermediate state \(\mathbf{x}_t\) and the ground truth \(\mathbf{x}_1\). 
It prevents embedding collapse, reduces distances between states, and enables efficient sampling. 
In this study, \(\mathcal{L}_{\text{anchor}}\) minimizes the negative log-likelihood of the joint probability (Eq.~\eqref{eq:anchor-prob}).

Let $\mathbf{e}_{\phi}(\cdot)=[e_1,e_2,\dots,e_V] \in \mathbb{R}^{V \times D}$ denotes the embedding lookup function with the vocabulary size of $V$.

Given $\mathcal{A}^{6 \times L}=\{\alpha_{i,j} \mid i=1 \dots 6; j=1 \dots L\}$, where $\alpha_{i,j} \in \{1 \dots V \}$,  represents a quantizer element of an $\mathbf{x}_1$ with the sequence length of $L$, the embedding of $\mathcal{A}^{6 \times L}$ can be expressed as $\mathbf{e}_{\phi}(\mathcal{A}^{6 \times L})=\{\varepsilon_{i,j} \mid i=1 \dots 6; j=1 \dots L\}$, where $\varepsilon_{i,j} \in \mathbb{R}^D$.

Thus, the joint probability approximating the target distribution $\mathbf{z}_1$, conditioned on the estimated sequences $\tilde{\mathbf{z}}_{1}$, can be expressed as follows:

$$
\label{eq:anchor-prob}
p(\mathbf{z}_1 \mid \tilde{\mathbf{z}}_1;\phi) = \prod_{i=1}^{6} \prod_{j=1}^{L} p(\varepsilon_{i,j} \mid \tilde{\varepsilon}_{i,j}; \mathbf{e}_{\phi}),
$$

where $\tilde{\mathbf{z}}_{1}$ is the approximation of $\mathbf{z}_{1}$, deduced using the triplet consisting of the estimated vector $\mathbf{v}_{\theta}(\breve{\mathbf{z}}_{\text{pr}}, \tau)$, the prior state $\mathbf{z}_{\text{pr}}$, and the corresponding interpolate parameter $\tau$.

The function $\mathcal{F}^{-1}$ denotes the reverse function of $\mathcal{F}$.

This relationship is expressed as $\tilde{\mathbf{z}}_1 = \mathbf{z}_{\text{pr}} + (1 - \tau)\mathcal{F}^{-1}(\mathbf{v}_{\theta}(\breve{\mathbf{z}}_{\text{pr}}, \tau))$.\\[8pt]

**Total loss**.

We define the total loss function used in our joint training method as $\mathcal{L}_{total} = \mathcal{L}_{prior} + \mathcal{L}_{dur} + \mathcal{L}_{CFM} + \mathcal{L}_{anchor}$.

The loss functions \(\mathcal{L}_{prior}\) and \(\mathcal{L}_{dur}\) set the training objective for the Prior Codes Generator, whereas \(\mathcal{L}_{CFM}\) and \(\mathcal{L}_{anchor}\) are designed to construct the vector field and distill the sampling steps, respectively.

## 4·Experiments

> <a id="tab:overall-results"></a>
> Performance evaluation on the *LibriSpeech test-clean* across different audio prompt lengths. **Bold** indicates the best result, and \underline{underline} indicates the second-best result. ($\uparrow$) indicates that higher values are better, while ($\downarrow$) indicates that lower values are better. $[\spadesuit]$ means reproduced results. $[\bigstar]$ and $[\clubsuit]$ mean results inferred from official and ufficial checkpoints, respectively.

Abbreviation: LT (LibriTTS), E (Emilia), GS (GigaSpeech).

### Experiment Setup

**Dataset**.

We employ the *LibriTTS* dataset [^Zen2019LibriTTS], which comprises multi-speaker English audio recordings of training data. 
For benchmarking purposes, we use the *LibriSpeech test-clean* [^Panayotov2015Librispeech] dataset. 
More detailed information is provided in Appendix [sec:data](#sec:data).\\[8pt]
**Evaluation Metrics**.

To assess model performance, we employ the following objective evaluation metrics for each criterion: speech quality, quantified by UTMOS; speaker similarity, measured using SIM-O and SIM-R; robustness, indicated by WER; and prosody accuracy and error, analyzed through pitch and energy.

Additionally, we employ NFE and RTF metrics to measure the latency of the sampling process.

More details on the evaluation metrics can be found in Appendix [sec:metrics](#sec:metrics).\\[8pt]
**Baselines**.

We compare our model with previous zero-shot TTS baselines.

Further details regarding baselines are available in Appendix [appendix:baselines](#appendix:baselines).

### Main Results

> <a id="tab:size-rtf-results"></a>
> Comparison of model size and latency for 3s audio prompt length.

Column **\#Params** indicates the total number of parameters required for end-to-end synthesis, with the first value representing the parameters of the zero-shot model (trainable) and the second value corresponding to those of the neural codec or vocoder component (frozen).

Table [tab:overall-results](#tab:overall-results) shows the performance of OZSpeech and representative baseline methods for 1s, 3s, and 5s audio prompt lengths.  OZSpeech establishes a new SOTA on WER across all audio prompt lengths, demonstrating superior content preserving capability through a *multi-fold* reduction in WER.

For example, OZSpeech reduces WER by a factor of $1.8 - 6.8$ over the other methods for 5s prompt length.

Some of these models, such as F5-TTS, are trained on substantially more training data (F5-TTS is trained on 95,000 hours of speech whereas OZSpeech is trained on 500 hours).

Compared to the next-best method, OZSpeech yields a relative reduction in WER by 58\%, 44\%, and 44\% for  1s, 3s, and 5s audio prompt lengths, respectively.  Additionally, while the WER scores of all baseline methods are sensitive to prompt length, OZSpeech maintains a consistent WER regardless of prompt length.

For pitch and energy accuracies and errors, which indicate the prosody reconstruction ability of TTS systems, OZSpeech consistently ranks as the best or second-best performer across different prompt lengths.

For the remaining metrics (UTMOS, SIM-O, and SIM-R), our method in overall does not exhibit an obvious performance advantage over the baseline models. (We note that these models also experience trade-offs between different metrics.) However, our goal is to enhance the balance between intelligibility (i.e. content accuracy) and acoustical/perceptual quality while maintaining low latency and small model size.%However, our goal is to enhance the balance between intelligibility (i.e. content accuracy), model size, sampling speed, and speech synthesis capability without much compromise in speech perceptual qualities. % Pros: WER improvement is multifold.

Sampling speed is fast.

Model size is smallest

Our UTMOS scores show a rather small degradation compared to some of the baselines, particularly VALL-E and VoiceCraft.

This is largely due to differences in the neural codecs' trade-offs between acoustic and semantic representations.

EnCodec (VoiceCraft’s codec) primarily relies on acoustic codes, while SpeechTokenizer (VALL-E’s codec in this experiment) incorporates one semantic sequence alongside acoustic codes.

In contrast, FaCodec (OZSpeech’s codec) strives to balance both representations.

However, our focus is on optimizing the trade-off between sampling speed and speech synthesis quality.
% This could unlock new potential for non-autoregressive models, as the number of network forwards is significantly smaller compared to related non-autoregressive baselines.

We also retrain F5-TTS with 500~hours of the LibriTTS dataset using the official code for 1 million steps following its guideline, however the resulting WER exceeds 0.95 across all settings, so we exclude this retrained checkpoint from Table~[tab:overall-results](#tab:overall-results)  and instead use the officially released checkpoint, which is trained on 95,000 hours of data.

The poor retraining results suggest that this method, which is based on traditional OT-CFM, requires a much larger, more diverse dataset for robustness.

In contrast, neural codec-based models remain effective with limited data, likely due to extensive pre-training of the neural codec module on massive datasets.

Thus, traditional OT-CFM methods like F5-TTS are unsuitable for low-resource languages.

Table [tab:size-rtf-results](#tab:size-rtf-results) compares the model sizes and latency of OZSpeech and baseline models.

OZSpeech is the smallest model of all, being only 29\%-71\% the size of the other models.

When considering only the trainable part of the models, the number of trainable parameters of OZSpeech is only 17\%-43\% that of the other models.

For the NFE metric, our model uses only a single sampling step, significantly reducing computation compared to NaturalSpeech 2 and F5-TTS, which require 200 and 32 steps, respectively, to achieve optimal performance.

As a result, in terms of inference speed represented by the RTF metric, OZSpeech is almost 3 times faster than the next fastest model, F5-TTS.

### Ablation Study

> <a id="tab:prompting-strategy"></a>
> Comparison of two prompting strategies during training: *First Segment* and *Arbitrary Segment*.

Table [tab:prompting-strategy](#tab:prompting-strategy) compares the performance of two prompting strategies: *First Segment* and *Arbitrary Segment*.

The former generates prompts using the initial portion of the ground truth, whereas the latter selects random audio segments from the ground truth to form the prompts.

The results clearly show that the *Arbitrary Segment* strategy outperforms the *First Segment* strategy across all metrics.

In the *First Segment* setting, the model seems to overfit in that it is forced to transfer the prompt to the beginning of the target.

In contrast, the *Arbitrary Segment* setting hides the position of the prompt, allowing it to smoothly transfer attributes from the prompt to the target.

Consequently, we adopt the *Arbitrary Segment* approach for our training.

This experiment also shows that the *Arbitrary Segment* approach improves robustness by exposing the model to a more diverse range of speech contexts, leading to better generalization in zero-shot speech synthesis.

### Noise Tolerance Analysis

> <a id="tab:noisy-test"></a>
> Performance evaluation on noisy audio prompts.

The noisy prompts are derived from the *LibriSpeech test-clean* dataset with additive noise augmentation.

The prompts are 3-second long.

The checkpoints of each model trained on *LibriTTS*—except for VoiceCraft, which was trained on *GigaSpeech*—are used without re-training the models to include noisy samples. $[\blacklozenge]$ means fine-tuned results on noisy prompts.

This table highlights a vulnerable use case where speech prompts contain noise, assessing the tolerance of these models.

SNR$=\infty$ indicates the prompts are directly obtained from *LibriSpeech test-clean* dataset and these results are simply copied from Table~[tab:overall-results](#tab:overall-results).

Unlike previous studies, we propose to investigate the effects of noisy prompts in TTS.

Table [tab:noisy-test](#tab:noisy-test) evaluates the tolerance of each model on noisy prompts.

We conduct *zero-shot* testing of all models in this scenario, where zero-shot in this context means that the models trained on their original training dataset, typically with clean prompts, are directly tested on noisy prompts.

For this, we generate three sets of noisy prompts at SNRs of 0dB, 6dB, and 12dB, respectively. $\text{SNR} =\infty$ refers to prompts directly sourced from *LibriSpeech test-clean* dataset, with metric values directly replicated from Table~[tab:overall-results](#tab:overall-results).

Overall, all baseline methods are highly sensitive to noise in the audio prompts, experiencing significant degradation in all metrics as prompt SNR decreases.

OZSpeech also shows similar sensitivity except for WER, which experiences either no or negligible degradation across all prompt SNR levels.

VALL-E seems to be the most vulnerable to noise, where the WER increases by almost 2.7 times at the least noisy setting, SNR = 12dB.

At SNR = 0dB, VALL-E  becomes almost unintelligible with a 93\% WER.

The WER results highlight the robust intelligibility of OZSpeech, even in noisy prompt conditions. 

Although OZSpeech performs sub-optimally in non-WER metrics with the original clean prompts, it surpasses all baseline models in UTMOS.

This improvement is largely attributed to the significant performance drop observed in the baseline models.

Mixed results are observed when comparing all models on the remaining metrics.% (speaker similarity, pitch, and energy)

Next, we further fine-tune OZSpeech with both original and noisy prompts, where noisy prompts occur with a probability of 0.8.

The noisy prompts are constructed by mixing the original prompts with random noise at different SNRs, drawn from a uniform distribution over the [0dB, 15dB] range.

We leveraged the QUT-NOISE database [^Dean2010Qut-Noise-Timit] as our noise dataset.

When tested with clean prompts (SNR = $\infty$), there is either no change or minimal changes in OZSpeech's performance across all metrics before and after fine-tuning.

In noisy prompt conditions, WER remains unaffected by fine-tuning while the other metrics are significantly improved across all SNR levels.

With decreasing SNR, fine-tuning generally yields increasingly larger improvements in all non-WER metrics.

We have empirically demonstrated the feasibility of the noise-aware training approach, which aims to synthesize noise-free speech conditioned by noisy prompts.

This approach enables Zero-Shot TTS models to implicitly remove noise from given codec codes while preserving key attributes of speech.

Consequently, neural codec-based Zero-shot TTS systems, which have traditionally been vulnerable and sensitive to noisy prompts, exhibit enhanced robustness, particularly against adversarial attacks.

	% UTMOS: all models suffer significant degradation, with larger degradation as the prompts get noisier (smaller SNR).
	% SIM-O: lag just behind  VoiceCraft, outperforms others
	% SIM-R: While VALL-E is the best performer with OZSpeech lags just behind for a marginal value of 0.01 for original prompt, it quickly degrades as the prompts get noisier.

OZSpeech becomes the best performer for all 3 noisy scenarios. 
	% PESQ: With original prompts, there is marginal difference among all methods.

With noisy prompts,  VALL-E gets noticeable boost in PESQ while the other methods stay fairly the same. 

## 5·Conclusion

We propose OZSpeech, an effective and efficient zero-shot TTS model that employs flow matching with a single sampling step from a learned prior instead of random noise.

The model strikes a balance between synthesized speech intelligibility and acoustical quality.

In particular, OZSpeech yields a multi-fold improvement in WER compared to existing baseline methods with some trade-off in the acoustical quality.

Furthermore, unlike other methods, OZSpeech achieves a consistent WER across different audio prompt's lengths and noise levels.

With a single-step sampling approach and a novel prior learning module that learns an effective starting point for the sampling process, our model requires significantly less computation, with inference speed being $2.7 - 6.5$ times faster than the other methods.

In addition, our model size is only 29\%-71\% that of the other models.

OZSpeech achieves competitive results even over models that are trained on much larger training sets.% These advantages form a a low-latency and efficient Zero=shot TTS system that 

In future work, we plan to enhance OZSpeech by integrating adaptive noise filtering techniques and expanding its capability to support multilingual and multimodal zero-shot speech synthesis, enabling more versatile applications in real-world scenarios.

## 6·Background

### FACodec

\label{appendix:FACodec}

Factorized neural speech codec, named FACodec, [^Ju2024N}atural{S}peech] was proposed as a codec disentangler and timbre extractor.

It separates the original speech waveform into distinct aspects: content, prosody, acoustic details, and timbre.

Specifically, the speech input $x \in \mathbb{R}^C$ is processed through a speech encoder, $f_{enc}$, comprising several convolutional blocks to produce a pre-quantization latent representation:

$$
\label{eq:f_enc}
h = f_{enc}(x) \in \mathbb{R}^{T \times D},
$$

where $T$ and $D$ denote the downsampled timeframes and the latent dimension, respectively.

Subsequently, three factorized vector quantizers (FVQs) are employed to tokenize $h$ into distinct discrete sequences, capturing detailed representations of speech attributes such as content, prosody, and acoustic details.

Let $\mathcal{Q}_p$, $\mathcal{Q}_c$, and $\mathcal{Q}_a$ denote the FVQs for prosody, content, and acoustic details, respectively.

Each FVQ comprises a certain number of quantizers, defined $\mathcal{Q}_i = \{q_i^j\}^{N_i}_{j=1}$ where $i \in \{p,c, a\}$, $q_i^j \in \mathbb{R}^d$ represents the $j$-th quantizer corresponding to the $i$-th attribute, with a hidden dimension $d$, and its codebook size of 1024.

The number of quantizers for each attribute is $N_p=1, N_c=2, N_a=3$.

Thus, the output consists of a total of six sequences of discrete codes:

$$
\label{z}
z = **Concat**(f_p(h), f_c(h), f_a(h)) \in \mathbb{R}^{T \times 6},
$$

where $f_p(h) \in \mathbb{R}^{T \times 1}$, $f_c(h) \in \mathbb{R}^{T \times 2}$, and $f_a(h) \in \mathbb{R}^{T \times 3}$ are functions that map the latent representation $h$ into discrete codes representing the speech attributes, which are then concatenated into a unified representation $z$.

The timbre attribute is extracted by passing $h$ through several Conformer blocks [^Gulati2020Conformer] combined with a temporal pooling layer, which converts $h$ into a timbre-specific representation:

$$
\label{z_t}
z_t = **TemporalPooling**(**Conformer**(h)) \in \mathbb{R}^D.
$$

After obtaining $z$ and $z_t$, the neural codec decoder $f_{dec}$ combines them to reconstruct the waveform:

$$
\label{eq:f_dec}
y = f_{dec}(z, z_t).
$$

Inspired by Eq.~\eqref{eq:f_dec}, which takes $z$ and $z_t$ as inputs and is pre-trained on a large-scale, multi-speaker dataset, ensuring robust zero-shot TTS capabilities, our approach aims to build a system that generates a six-sequence representation $\tilde{z} \in \mathbb{R}^{T \times 6}$, which is forced to lie within the subspaces of the pre-trained FACodec.

This representation captures prosody, content, and acoustic details in a manner consistent with $z$.

Subsequently, $\tilde{z}$ is fed into $f_{dec}$, alongside $z_t$, obtained using Eq.~\eqref{z_t}, to synthesize the speech output $\tilde{y}$.

### Flow Matching

\label{appendix:fm}

We present the fundamental principles of Flow Matching (FM) upon which our model is built.

FM aims to construct a probability path $x_t \sim p_t(x)$, from a known source distribution $x_0 \sim p_0(x)$ (typically a Gaussian distribution) to a target distribution $x_1 \sim p_1(x)$.

Specifically, FM is formulated as a regression objective for training a velocity field (also called a vector field), which models the instantaneous velocities of samples at time $t$ (also known as the flow).

This velocity field is then used to transform the source distribution $p_0$ into the target distribution $p_1$ along the probability path $p_t$.

Formally, the flow of $x$ along the trajectory is defined by an ordinary differential equation (ODE):

$$
\label{eq:ode_flow}
\frac{d}{dt}d\psi_t(x) = \mathbf{v}_{t}(\psi_t(x);\theta),\quad \psi_0(x) = x,
$$

where $t \sim \mathcal{U}[0,1]$, $\psi_t: [0,1] \times \mathbb{R}^d \to \mathbb{R}^d$ represents a time-dependent flow describing the position of the point $x$ at time $t$, and $\mathbf{v}_{t}: [0,1] \times \mathbb{R}^d \to \mathbb{R}^d$ is the time-dependent velocity field modeled by a neural network with parameters $\theta$.

Given $x_t := \psi_t(x_0)$, the velocity field $\mathbf{v}_t$ creates a probability path $p_t$ such that $x_t \sim p_t$ for $x_0 \sim p_0$.

Under this formulation, the objective is to regress velocity field $\mathbf{v}_t$ predicted by the neural network parameterized by $\theta$ to a target velocity field $\mathbf{u}_t$ in order to generate the desired probability path $p_t$.

This is achieved by minimizing the Flow Matching (FM) loss:

$$
\label{eq:fm}

\begin{split}
\mathcal{L}_{FM}(\theta) = \mathbb{E}_{t,x_t}
\Big\|
\mathbf{v}_t(x_t;\theta) - \mathbf{u}_t(x_t)
\Big\|^2,
\end{split}

$$

where $t \sim \mathcal{U}[0,1],~x_t \sim p_t$.

In practice, $\mathcal{L}_{FM}(\theta)$ is rarely implemented due to the complexity of $\mathbf{u}_t$ and the lack of prior knowledge of $p_t$, $\mathbf{u}_t$, and the target distribution $p_1$, which makes it an obstacle to directly calculate $\mathbf{u}_t(x_t)$.

A feasible approach to address this issue is to simplify the loss by constructing the probability path $p_t$ conditioned on real data $x_1$ from the training dataset.

This path is also known as *conditional optimal transport* path.

Following [^Lipman2023Flow], a random variable $x_t \sim p_t$ can be expressed as a linear combination of $x_0 \sim \mathcal{N}(x |0,I)$ and $x_1 \sim p_1$:

$$
x_t = tx_1 + (1-t)x_0 \sim p_t,
$$

Thus, the probability path $p_t(x|x_1)=\mathcal{N}(x|tx_1,(1-t)^2I)$.

Given $x_t$ represents conditional random variables, the conditional velocity field can be derived from $\frac{d}{dt}x_t = \mathbf{u}_t(x_t|x_1)$ as $\mathbf{u}_t(x_t|x_1) = x_1 - x_0$.

Using this, we can formulate a tractable and simplified version of the Flow Matching loss \eqref{eq:fm}, referred to as the Conditional Flow Matching (CFM) loss.

This formulation encourages straighter trajectories between the source and target distributions and is expressed as follows:

$$
\label{eq:orgin-cfm}

\begin{split}
\mathcal{L}_{CFM}(\theta)   & = \mathbb{E}_{t,x_0,x_1}
\Big\|
\mathbf{v}_t(x_t;\theta) - \mathbf{u}_t(x_t|x_1)
\Big\|^2 \\
& = \mathbb{E}_{t,x_0,x_1}
\Big\|
\mathbf{v}_t(x_t;\theta) - (x_1 - x_0)
\Big\|^2,
\end{split}

$$

where $t \sim \mathcal{U}[0,1],~x_0 \sim \mathcal{N}(x |0,I),~x_1 \sim p_1$.

Once the training of the vector field $\mathbf{v}_t$ is complete, solving the ODE \eqref{eq:ode_flow} at discretized time steps until $t=1$ allows us to generate novel samples $x_1$ that approximate the target distribution $p_1$.

## 7·Method Details

\label{sec:training}

**Prompting Trick during Training**.

As outlined in Section [sec:osfm](#sec:osfm), incorporating an acoustic prompt is essential for generating $**x**_1$.

This process involves transferring prosody and acoustic detail attributes from the prompt to the output quantizers. 
A significant challenge arises in preparing prompt-target pairs that exhibit similar attributes, as mismatches can lead to degraded performance.

To address this issue, we leverage ground truth quantizers, utilizing them as both the prompt and the target during training. 
Specifically, we randomly select and clone a segment of 1 $\sim$ 3 seconds from the ground truth data to serve as the prompt at each training step. 
This approach ensures a high degree of similarity between the prompt and target, facilitating more effective attribute transfer and enhancing the quality of the generated output.\\[8pt]
**Losses Computing Strategy**.

Let the velocity of $\breve{\mathbf{z}}_{\text{pr}}$ along the path progressing toward the corresponding $\mathbf{z}_1$ be represented as $\mathbf{v}_{\theta}^{1:L}(\breve{\mathbf{z}}_{\text{pr}}, \tau)$, where $L$ denotes the length of the entire output sequence of $\mathbf{v}_{\theta}(\cdot,\cdot)$.

Our goal is to compute the drift of $\mathbf{x}_{\text{pr}}$ for generating $\mathbf{x}_1$ only; it is unnecessary to backpropagate gradients over the entire output sequence, which includes the concatenation of acoustic prompt $**y**$ and $**x**_{\text{prior}}$ velocities.

To address this, $\mathbf{v}_{\theta}^{1:L}(\breve{\mathbf{z}}_{\text{pr}}, \tau)$ is truncated by excluding the velocity components associated with the acoustic prompt $\mathbf{y}$, where $M$ denotes its length.

The resulting truncated velocity, $\mathbf{v}_{\theta}^{M:L}(\breve{\mathbf{z}}_{\text{pr}}, \tau)$, is then used in subsequent operations, including loss computation, to ensure computational efficiency while maintaining the focus on the target velocity for $\mathbf{x}_{\text{pr}}$.

As a result, Eq. (\eqref{eq:ot-cfm-adjusted-1}) is rewritten as:

$$
\label{eq:ot-cfm-adjusted-2}
\mathcal{L}_{CFM}(\theta) = \mathbb{E}_{\mathbf{x}_1, \mathbf{x}_{\text{pr}}}
\left\lVert
\mathbf{v}_{\theta}^{M:L}(\breve{\mathbf{z}}_{\text{pr}}, \tau) - \frac{\mathbf{x}_1 - \mathbf{x}_{\text{pr}}}{1 - \tau}
\right\rVert^2.
$$

Consequently, the Anchor Loss $\mathcal{L}_{anchor}$ approximating the target distribution $\mathbf{x}_1$, conditioned on the $\tilde{\mathbf{x}}_1$ is formulated:

$$
\label{eq:anchor-loss-2}
\mathcal{L}_{anchor}(\phi) = \mathbb{E}_{\mathbf{x}_1, \tilde{\mathbf{x}}_{1}} \left[ - \mathbf{log} p(\mathbf{x}_1 \mid \tilde{\mathbf{x}}_1;\phi) \right],
$$

where, $\tilde{\mathbf{x}}_1$ is computed as follows:

$$
\label{eq:x1-est-2}
\tilde{\mathbf{x}}_1 = \mathbf{x}_{\text{pr}} + (1 - \tau)\mathcal{F}^{-1}(\mathbf{v}_{\theta,M:L}(\breve{\mathbf{z}}_{\text{pr}}, \tau)).
$$
