# IntMeanFlow: Few-Step Speech Generation With Integral Velocity Distillation

<details>
<summary>基本信息</summary>

- 标题: "IntMeanFlow: Few-Step Speech Generation With Integral Velocity Distillation."
- 作者:
  - 01 Wei Wang
  - 02 Rong Cao
  - 03 Yi Guo
  - 04 Zhengyang Chen
  - 05 Kuan Chen
  - 06 Yuanyuan Huo
- 链接:
  - [ArXiv](https://arxiv.org/abs/2510.07979v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2510.07979v1](PDF/2025.10.09_2510.07979v1_IntMeanFlow__Few-Step_Speech_Generation_With_Integral_Velocity_Distillation.pdf)
  - [Publication] #TODO

</details>

## Abstract

Flow-based generative models have greatly improved text-to-speech (TTS) synthesis quality, but inference speed remains limited by the iterative sampling process and multiple function evaluations (NFE).
The recent MeanFlow model accelerates generation by modeling average velocity instead of instantaneous velocity.
However, its direct application to TTS encounters challenges, including GPU memory overhead from Jacobian-vector products (JVP) and training instability due to self-bootstrap processes.
To address these issues, we introduce IntMeanFlow, a framework for few-step speech generation with integral velocity distillation.
By approximating average velocity with the teacher’s instantaneous velocity over a temporal interval, IntMeanFlow eliminates the need for JVPs and self-bootstrap, improving stability and reducing GPU memory usage.
We also propose the Optimal Step Sampling Search (O3S) algorithm, which identifies the model-specific optimal sampling steps, improving speech synthesis without additional inference overhead.
Experiments show that IntMeanFlow achieves 1-NFE inference for token-to-spectrogram and 3-NFE for text-to-spectrogram tasks while maintaining high-quality synthesis.
Demo samples are available\footnote{\url{https://vvwangvv.github.io/intmeanflow/}}.
