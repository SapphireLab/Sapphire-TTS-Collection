# IntMeanFlow: Few-Step Speech Generation With Integral Velocity Distillation

<details>
<summary>基本信息</summary>

- 标题: "IntMeanFlow: Few-Step Speech Generation With Integral Velocity Distillation."
- 作者:
  - 01 Wei Wang
  - 02 Rong Cao
  - 03 Yi Guo
  - 04 Zhengyang Chen
  - 05 Kuan Chen
  - 06 Yuanyuan Huo
- 链接:
  - [ArXiv](https://arxiv.org/abs/2510.07979v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2510.07979v1](PDF/2025.10.09_2510.07979v1__IntMeanFlow__Few-Step_Speech_Generation_With_Integral_Velocity_Distillation.pdf)
  - [Publication] #TODO

</details>

## Abstract

Flow-based generative models have greatly improved text-to-speech (TTS) synthesis quality, but inference speed remains limited by the iterative sampling process and multiple function evaluations (NFE).
The recent MeanFlow model accelerates generation by modeling average velocity instead of instantaneous velocity.
However, its direct application to TTS encounters challenges, including GPU memory overhead from Jacobian-vector products (JVP) and training instability due to self-bootstrap processes.
To address these issues, we introduce ***IntMeanFlow***, a framework for few-step speech generation with integral velocity distillation.
By approximating average velocity with the teacher’s instantaneous velocity over a temporal interval, ***IntMeanFlow*** eliminates the need for JVPs and self-bootstrap, improving stability and reducing GPU memory usage.
We also propose the Optimal Step Sampling Search (O3S) algorithm, which identifies the model-specific optimal sampling steps, improving speech synthesis without additional inference overhead.
Experiments show that ***IntMeanFlow*** achieves 1-NFE inference for token-to-spectrogram and 3-NFE for text-to-spectrogram tasks while maintaining high-quality synthesis.
Demo samples are [available](https://vvwangvv.github.io/intmeanflow/).

## 1·Introduction

Text-to-speech (TTS) generation has made significant progress in recent years, with models achieving near-human-level, zero-shot synthesis capabilities (**CosyVoice2**[^Du2024CosyVoice2], **CosyVoice3**[^Du2025CosyVoice3], **F5-TTS**[^Chen2024F5-TTS], **XTTS**[^Casanova2024XTTS], **YourTTS**[^Casanova2021YourTTS], **Seed-TTS**[^Anastassiou2024Seed-TTS], **ZipVoice**[^Zhu2025ZipVoice]).
The rise of flow-based generative models (**Flow Matching**[^Lipman2022FM]) has contributed to this advancement, offering promising results across various fields, including image synthesis (**InterFlow**[^Albergo2022InterFlow], **RectifiedFlow**[^Liu2022RectifiedFlow]), video (****PyramidalFM**[^Jin2024PyramidalFM]) and music generation (**MusicFlow**[^Prajwal2024MusicFlow]).
These models learn to map data distributions to a latent space, enabling high-quality generation.
However, flow-based models often face a trade-off between sampling quality and efficiency, as their iterative sampling process can lead to slow inference and high computational costs.
To address this, recent efforts in the image domain, such as consistency models (**AudioLCM**[^Liu2024AudioLCM], **MusicCM**[^Fei2024MusicCM], **CM**[^Song2023CM], **sCM**[^Lu2024sCM], **MultiStepCM**[^Heek2024MultiStepCM], **FACM**[^Peng2025FACM]) and **Shortcut Models**[^Frans2024ShortcutModel], have been proposed to reduce the number of function evaluations~(NFE) while maintaining high-quality generation results.

Among these approaches, **MeanFlow**[^Geng2025MeanFlow] has emerged as a promising solution to the trade-off between sampling quality and efficiency.
By modeling averaged velocity instead of instantaneous velocity, MeanFlow enables more efficient sampling without compromising output quality.
While it has shown success in image generation and audio generation (**MeanAudio**[^Li2025MeanAudio]), applying MeanFlow to TTS introduces several challenges.
First, the training process of MeanFlow relies on a self-bootstrap mechanism, which requires mixing with instantaneous velocity guidance similar to flow matching.
The strength of this guidance can significantly impact model performance, and without it, the model is prone to collapse.
Second, MeanFlow involves the Jacobian-vector product (JVP), a computationally intensive operation that consumes substantial GPU memory.
Additionally, JVP is not natively supported by certain custom CUDA operators or torch-native operations, such as flash attention, complicating the adaptation of existing models to the MeanFlow framework.
These memory and compatibility challenges make training large-scale TTS models with MeanFlow infeasible.

To address these challenges, we propose ***IntMeanFlow***, a framework for few-step speech generation that leverages integral velocity distillation.
Building on the core motivation of MeanFlow, ***IntMeanFlow*** enables the model to learn averaged velocity instead of instantaneous velocity.
Motivated by the observation that the quality of speech generated by flow-based models plateaus after a certain NFE, we approximate the average velocity over a temporal interval $[t,r]$ by dividing the accumulated displacement between discrete time steps by the interval duration.
We further introduce an initialization strategy to leverage pretrained flow-matching models, enabling smoother migration from existing models.
By eliminating the need for self-bootstrap and reliance on the Jacobian-vector product (JVP), ***IntMeanFlow*** ensures improved stability, reduced GPU memory consumption during training, and better compatibility with existing models, simplifying the adaptation process.

Additionally, based on the empirical observation that denser sampling near noisier timesteps leads to better generation quality, we introduce the Optimal Step Sampling Search (O3S) algorithm.
O3S automatically identifies model-specific near-optimal sampling steps, using a customizable quality metric and a ternary search algorithm.
This improves the generation quality without introducing additional inference overhead.

We conduct experiments on models from two widely used approaches in flow-based TTS: (1) **CosyVoice2**[^Du2024CosyVoice2], which integrates a language model (LM) followed by a flow model converting time-aligned tokens into mel-spectrograms (token2mel), and (2) **F5-TTS**[^Chen2024F5-TTS], where a flow model directly converts raw text embeddings into mel-spectrograms (text2mel), learning time alignment inherently.
Experimental results show that ***IntMeanFlow*** achieves 1-NFE inference for the token2mel task in **CosyVoice2** and 3-NFE for the text2mel task in F5-TTS, maintaining high-quality synthesis.

Our contributions can be summarized as follows:
-  We propose a distillation framework that applies MeanFlow to TTS, overcoming training memory overhead and stability issues associated with its direction application to TTS.
-  We introduce a search algorithm to identify model-specific sampling steps, improving generation efficiency without increasing inference overhead.
-  We evaluate our method on two popular flow-based TTS models, achieving 1-NFE inference for token2mel and 3-NFE for text2mel tasks, while maintaining high-quality synthesis.

## 2·Related Works

### Distillation based on Direct Metric Optimization

Consistency and shortcut models are commonly used to reduce NFE while maintaining output quality.
However, applying these methods to text-to-speech (TTS) has shown limited success due to task differences.
Notable exceptions, such as **DMOSpeech**[^Li2024DMOSpeech] and **DMOSpeech2**[^Li2025DMOSpeech2], reduce NFE in TTS through distillation and reinforcement learning.
However, these methods rely on auxiliary models that complicate the training process, as well as a fixed time sampling schedule during distillation, which reduces flexibility during inference.
In contrast, our approach eliminates the need for auxiliary models and additional loss functions.
And we do not employ a fixed step sampling strategy during training, which simplifies the training process, and improves flexibility during inference.

### MeanFlow

MeanFlow introduces a framework for one-step generative modeling by defining average velocity as displacement over a time interval.
Unlike Flow Matching, which models instantaneous velocity, MeanFlow focuses on learning average velocity and establishes an analytical relationship between average and instantaneous velocities through the MeanFlow Identity.
This approach provides a clear training objective without relying on heuristic constraints, and it operates independently of score estimation, pretraining, or distillation.
However, its dependence on Jacobian-vector products (JVPs) to compute the time derivative introduces computational overhead and limits scalability, especially with custom operators lacking JVP support.

## 3·Methodology

### IntMeanFlow: MeanFlow Distillation via Integral Velocity

***IntMeanFlow*** extends the principles of MeanFlow by focusing on learning the averaged velocity over a time interval, rather than the instantaneous velocity at individual time steps.
This approach retains the coarse-to-fine nature of MeanFlow, where smaller intervals are emphasized during training to capture fine-grained details, while broader temporal dynamics are learned more gradually.
In the distillation process, the student model is guided by a flow-matching teacher model.
The teacher model transforms an initial distribution $p_0$ (e.g., Gaussian noise) into a target distribution $p_1$ using a time-dependent vector field $v(z_t, t; \theta)$.
The state evolution $z_t$ is governed by the following ordinary differential equation (ODE):

$$
\frac{d}{dt} z_t = v(z_t, t; \theta), \quad z_0 \sim p_0, \quad z_1 \sim p_1, \quad t \in [0, 1]
$$

The teacher’s loss function is given by:

$$
L_{\text{CFM}} = \mathbb{E}_{t, p_0(z_0), q(z_1)} \left[ \left\| v(z_t, t; \theta) - (z_1 - z_0) \right\|^2 \right]
$$

While the teacher learns to model the instantaneous velocity $v(z_t, t; \theta)$, the student model is tasked with learning the averaged velocity over a time interval $[t, r]$, defined as:

$$
\bar{v}(z_t, t, r) = \frac{z_r - z_t}{r - t}
$$

where $z_t$ and $z_r$ are the states of the system at times $t$ and $r$, respectively. $z_r$ is computed iteratively during inference as described below, and is not a direct parameter of the velocity function $v$.
The student model is trained to approximate this averaged velocity using the teacher’s instantaneous velocity.
To achieve this, we perform iterative sampling during distillation.
The interval $[t, r]$ is discretized into $n$ subintervals, with time steps $t_0 = t, t_1, \dots, t_n = r$.
At each step, the teacher model evolves the state according to the discrete approximation of the ODE:

$$
z_{t_{k+1}} = z_{t_k} + (t_{k+1} - t_k) \cdot v(z_{t_k}, t_k; \theta)
$$

where $t_0 = t$, $t_n = r$, and $t_1, t_2, \dots, t_{n-1}$ are intermediate time steps.
The total displacement over the interval $[t, r]$ is given by:

$$
\Delta z^{\text{teacher}} = \sum_{k=0}^{n-1} \left( z_{t_{k+1}} - z_{t_k} \right) = \sum_{k=0}^{n-1} (t_{k+1} - t_k) \cdot v(z_{t_k}, t_k; \theta)
$$

This discrete displacement approximates the integral of the instantaneous velocity $v(z_t, t; \theta)$ over $[t, r]$, which is the continuous process the student is aiming to model.
To approximate the averaged velocity, the displacement is normalized by the interval length:

$$
\bar{v}_{\text{teacher}}(z_t, t, r) = \frac{\Delta z^{\text{teacher}}}{r - t}
$$

The continuous form of the averaged velocity is given by:

$$
\bar{v}(z_t, t, r) = \frac{1}{r - t} \int_t^r v(z_\tau, \tau; \theta) \, d\tau
$$

Thus, the teacher’s discrete displacement serves as a numerical approximation of this integral.
Finally, the student model is trained to minimize the distillation loss:

$$
L_{\text{distill}} = \mathbb{E}_{t, r} \left[ \left\| u_{\text{student}}(z_t, t, r) - \bar{v}_{\text{teacher}}(z_t, t, r) \right\|^2 \right]
$$

where $u_{\text{student}}(z_t, t, r)$ is the predicted velocity from the student model, and $\bar{v}_{\text{teacher}}(z_t, t, r)$ is the target velocity from the teacher.
The student model learns to predict the averaged velocity by following the teacher’s guidance, which approximates the integral of the instantaneous velocity via iterative sampling.

> <a id="fig:placeholder">![](imgs/imf.pdf)</a>
>
> Fig.01: Illustration of ***IntMeanFlow***: The student model learns the averaged velocity from the instantaneous velocities provided by the teacher model at multiple states.

### Optimal Step Sampling Searching~(O3S)

In flow-based speech generation, previous work has shown that denser sampling at noisier steps leads to improved speech quality (**F5-TTS**[^Chen2024F5-TTS], **Fast F5-TTS**[^Zheng2025FastF5-TTS]).
To satisfy varying NFE requirements, earlier methods have used continuous functions or hard-coded discrete step schedules.
However, instead of relying on a fixed schedule, this work optimizes the steps sampling specifically for each model's inference process.
As shown in [Fig.02](#fig:o3s), we observe through experiments that speech quality, as a function of sampling step position, exhibits near-convex behavior.
This finding motivates the introduction of the Optimal Sampling Step Search (O3S) algorithm, which optimizes the distribution of a fixed number of sampling steps across the inference interval $[0, 1]$.

The core idea behind O3S is to optimize the placement of each sampling step using ternary search.
To achieve this, we fix all but one of the sampling steps and apply ternary search to optimize the remaining one.
This process is repeated for each step, and the optimization continues until no further improvement is observed on a development set.
O3S thus identifies the optimal distribution of sampling steps, enhancing speech quality without increasing NFE.

The pipeline for O3S is outlined in [Algorithm.01](#alg:o3s).
The metric function $\mathcal{L}$ takes a set of sampling steps $T$ and a development set for inference, computing a pre-defined metric on the generated audio.
In this work, we use speaker similarity as the metric.

> <a id="alg:o3s">![](Images/2025.10.09_IntMeanFlow_Algo.01.png)</a>
>
> Alg.01: Optimal Step Sampling Search (O3S) Algorithm.

> <a id="fig:o3s">![](imgs/convex.pdf)</a>
>
> Fig.02: When fixing all but one of the sampling steps, the speaker similarity metric exhibits near-convex behavior.

### Initialization Strategy for IntMeanFlow

To adapt flow-matching models to ***IntMeanFlow***, we introduce an additional parameter $r$.
Both $t$ and $r$ are passed through the same embedding network, concatenated, and projected back to the feature space of $t$ using a linear mapping $\mathbf{W}$.

Let the embeddings for $t$ and $r$ be $\mathbf{e}_t = \mathcal{E}(t)$ and $\mathbf{e}_r = \mathcal{E}(r)$.
The concatenated and mapped embeddings $\mathbf{e}_{t,r}$, $\mathbf{e'}_{t,r}$ are:
$$
\mathbf{e}_{t, r} = [\mathbf{e}_t, \mathbf{e}_r]; \mathbf{e'}_{t,r} = \mathbf{W} \mathbf{e}_{t, r}
$$

To preserve the original model behavior, $\mathbf{W}$ is initialized as:
$$
\mathbf{W} =\begin{bmatrix}D_{diag} & 0 \end{bmatrix}
$$

where $D_{diag}$ indicates a diagonal matrix.
This ensures the model to behave like the original model at initialization.

## 4·Experiments

### Experiment Setup

We conduct experiments based on **F5-TTS** for the text2mel task and **CosyVoice2** for the token2mel task.
During distillation with ***IntMeanFlow***, the student model learns the vector field from the teacher with **classifier-free guidance (CFG)**[^Ho2022CFG], while the student avoids using CFG during training to reduce inference overhead.
For F5-TTS, we use the **Seed-TTS**[^Anastassiou2024Seed-TTS] test set ([Github](https://github.com/BytedanceSpeech/seed-tts-eval)); for **CosyVoice2**, we use the **LibriSpeech-PC**[^Meister2023LibriSpeech-PC] test set.

Performance is evaluated on a cross-sentence task using a variety of metrics.
For objective evaluation, we report Word Error Rate (WER) and SIM-o (speaker similarity).
WER is computed using Whisperlarge-v3 for English transcription and **Paraformer**-zh[^Gao2022Paraformer] for Chinese transcription.
For SIM-o, we extract speaker embeddings using a **WavLM**-based[^Chen2021WavLM] speaker verification model and compute the cosine similarity between synthesized and prompt human speech.
Real-time factors(RTF) are measured on NVIDIA A100 GPU.
For CMOS, speech quality is rated from 3 (worse) to + 3 (better) compared to human reference.
For SMOS, similarity between synthesized and prompt speech is rated on a 1–5 scale, with higher scores indicating better quality.
Additionally, we report **UTMOS**[^Saeki2022UTMOS] scores, which are evaluated using an open-source MOS prediction model.
To complement UTMOS, we also apply the recent **Uni-Versa-Ext** (UV.MOS)[^Wang2025Uni-VERSA-Ext], which has demonstrated a high correlation with human-provided MOS scores ([Github](https://huggingface.co/vvwangvv/universa-ext_wavlm-base_5metric)).

### Results on Text2Mel Task

F5-TTS maps text embeddings to mel spectrograms in the text2mel task using a diffusion transformer.
Following **F5-TTS**[^Chen2024F5-TTS], We train small models on the **LibriTTS**[^Zen2019LibriTTS] dataset and base and medium models on the processed 95k-hour **Emilia**[^He2024Emilia] dataset.
Teachers' inference has a CFG rate of 3.0 while student does not apply CFG during inference.
Experimental results, shown in [Tab.01](#tab:seedtts-test), demonstrate that distillation with ***IntMeanFlow*** significantly improves NFE and RTF, with only a slight compromise in performance.

> <a id="tab:seedtts-test"></a>
> Tab.01: Text2Mel results on Seed-TTS *test-en* and *test-zh*.
> Bold indicates the best result, underlined the second-best.
> All methods are based on F5-TTS.
> IMF stands for ***IntMeanFlow***.
> Medium models have 592M parameters, base models 336M, and small models 158M.

> <a id="tab:cosyvoice-librispeech-pc"></a>
> Tab.02: Token2Mel Results on LibriSpeech-PC: RTF values are reported only for the flow module.

The relationship between NFE and speech quality are illustrated in [Fig.03](#fig:nfe).
We do not compare with MeanFlow for the text2mel task, as **F5-TTS** requires training with a batch size of 1 due to JVP overhead.
Performance of MeanFlow is discussed in "Results on Token2Mel Task" section.

> <a id="fig:nfe">![](imgs/NFE.pdf)</a>
>
> Fig.03: NFE vs. WER (\%) and SIM-o for the Flow Matching teacher and ***IntMeanFlow*** student models

#### Effectiveness of the O3S Algorithm

The comparison between lines 6 and 7 on *test-en* and lines 16 and 17 on *test-zh* demonstrates the effectiveness of applying the sampling steps optimized by O3S.
O3S consistently leads to improvements in both subjective and objective metrics by a large margin.

#### Impact of Teacher NFE on Training Time and Performance

Using a larger teacher NFE during distillation can significantly increase training time, as shown in [Tab.03](#tab:train_time).
To examine the trade-off between teacher NFE and student performance, we conduct ablation experiments.
A comparison of lines 10, 11, and 12 on *test-en*, and lines 18, 19, and 20 on *test-zh*, reveals that while a smaller teacher NFE results in minimal degradation in subjective metrics, objective metrics show noticeable performance loss, which is a common challenge in TTS evaluation.
Synthesized speech from lines 10 and 20 demonstrates unnatural pronunciation and degraded fluency due to the excessively small teacher NFE.

> <a id="tab:train_time"></a>
>
> Tab.03: Teacher NFE and training time per step.

#### Impact of Teacher Size on Student Performance

The size of the teacher model does not affect the inference efficiency of the student model.
Therefore, we examine whether a larger teacher can improve student performance, particularly when paired with a smaller, more practically efficient student model.
Comparing lines 11 and 12, we observe that increasing the teacher's size leads to improvements in both objective and subjective metrics for the student.
These improvements significantly surpass the baseline performance reported in line 4.

### Results on Token2Mel Task

In this experiment, we use the CosyVoice 2 architecture for flow-based speech generation.
Unlike F5-TTS, where the flow operates on raw text embeddings, the flow in CosyVoice 2 processes time-aligned semantics, simplifying the task and enabling 1-NFE inference after distillation.
We perform distillation using the default model configuration from the [official repository](https://github.com/FunAudioLLM/CosyVoice)
and its pre-trained checkpoint as the teacher model.
Since CosyVoice 2 is trained on a proprietary dataset that is unavailable to us, we use LibriTTS for distillation.
The vanilla MeanFlow model, as shown in line 18, yields good WER and SIM-o results but leads to degraded speech quality in both neural and human MOS evaluations, likely due to the absence of a teacher model and instability in the training process.
The results in line 9 demonstrate that ***IntMeanFlow*** effectively distills the teacher’s capabilities while reducing NFE, even when trained on a significantly smaller dataset.

## 5·Conclusions

In this work, we introduce the ***IntMeanFlow*** framework for efficient few-step speech generation through distillation.
We also propose an optimal step sampling search algorithm to identify model-specific sampling steps during inference.
Our experiments, conducted on two widely used TTS models, **F5-TTS** for the text2mel task and **CosyVoice2** for the token2mel task.
Results demonstrate that ***IntMeanFlow*** achieves 3-NFE for the text2mel task and 1-NFE for the token2mel task.
This results in 10 times acceleration in terms of RTF, with only minimal degradation in performance.
Additionally, we propose an efficient initialization strategy that facilitates the seamless adaptation of existing flow-matching models to ***IntMeanFlow***, improving both the practical applicability of the framework.

## References

[^Du2024CosyVoice2]: [**CosyVoice2**: Scalable Streaming Speech Synthesis with Large Language Models](../SpeechLM_TTS/2024.12.13_CosyVoice2.md). ArXiv:2412.10117v3.
[^Du2025CosyVoice3]: [**CosyVoice3**: Towards In-the-wild Speech Generation via Scaling-Up and Post-training](../SpeechLM_TTS/2025.05.23_CosyVoice3.md). ArXiv:2505.17589v2.
[^Chen2024F5-TTS]: [**F5-TTS**: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching](2024.10.09_F5-TTS.md). ArXiv:2410.06885v3.
[^Casanova2024XTTS]: [**XTTS**: A Massively Multilingual Zero-Shot Text-to-Speech Model](../SpeechLM_TTS/2024.06.07_XTTS.md). ArXiv:2406.04904v1.
[^Casanova2021YourTTS]: [**YourTTS**: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for Everyone](../E2E/2021.12.04_YourTTS.md). ArXiv:2112.02418v4/ICML2022.
[^Anastassiou2024Seed-TTS]: [**Seed-TTS**: A Family of High-Quality Versatile Speech Generation Models](../SpeechLM_TTS/2024.06.04_Seed-TTS.md). ArXiv:2406.02430v1.
[^Zhu2025ZipVoice]: [**ZipVoice**: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching](2025.06.16_ZipVoice.md). ArXiv:2506.13053v3/ASRU2025.
[^Lipman2022FM]: [**FM**: Flow Matching for Generative Modeling](_2022.10.06_Flow_Matching.md). ArXiv:2210.02747v2.
[^Albergo2022InterFlow]: [**InterFlow**: Building Normalizing Flows with Stochastic Interpolants](_2022.09.30_InterFlow.md). ArXiv:2209.15571v3/ICLR2023.
[^Liu2022RectifiedFlow]: [**RectifiedFlow**: Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow](_2022.09.07_RectifiedFlow.md). ArXiv:2209.03003v1.
[^Jin2024PyramidalFM]: [**PyramidalFM**: Pyramidal Flow Matching for Efficient Video Generative Modeling](_2024.10.08_PyramidalFM.md). ArXiv:2410.05954v2/ICLR2025.
[^Prajwal2024MusicFlow]: [**MusicFlow**: Cascaded Flow Matching for Text Guided Music Generation](../-Music/2024.10.27_MusicFlow.md). ArXiv:2410.20478v1.
[^Liu2024AudioLCM]: [**AudioLCM**: Text-to-Audio Generation with Latent Consistency Models](../Consistency/2024.06.01_AudioLCM.md). ArXiv:2406.00356v2.
[^Fei2024MusicCM]: [**MusicCM**: Music Consistency Models](../-Music/2024.04.20_MusicCM.md). ArXiv:2404.13358v1.
[^Song2023CM]: [**CM**: Consistency Models](../Consistency/_2023.03.02_CM.md). ArXiv:2303.01469v2/ICML2023.
[^Lu2024sCM]: [**sCM**: Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models](../Consistency/_2024.10.14_sCM.md). ArXiv:2410.11081v2/ICLR2025Oral.
[^Heek2024MultiStepCM]: [**MultiStepCM**: Multistep Consistency Models](../Consistency/_2024.03.11_MultiStepCM.md). ArXiv:2403.06807v3.
[^Peng2025FACM]: [**FACM**: Flow-Anchored Consistency Models](../Consistency/_2025.07.04_FACM.md). ArXiv:2507.03738v1.
[^Frans2024ShortcutModel]: [**ShortcutModel**: One Step Diffusion via Shortcut Models](../Diffusion/_2024.10.16_ShortcutModel.md). ArXiv:2410.12557v3.
[^Geng2025MeanFlow]: [**MeanFlow**: Mean Flows for One-step Generative Modeling](_2025.05.19_MeanFlow.md). ArXiv:2505.13447v1.
[^Li2025MeanAudio]: [**MeanAudio**: Fast and Faithful Text-to-Audio Generation with Mean Flows](2025.08.08_MeanAudio.md). ArXiv:2508.06098v1.
[^Li2024DMOSpeech]: [**DMOSpeech**: Direct Metric Optimization via Distilled Diffusion Model in Zero-Shot Speech Synthesis](../Diffusion/2024.10.14_DMOSpeech.md). ArXiv:2410.11097v2.
[^Li2025DMOSpeech2]: [**DMOSpeech2**: Reinforcement Learning for Duration Prediction in Metric-Optimized Speech Synthesis](../Diffusion/2025.07.20_DMOSpeech2.md). ArXiv:2507.14988v1.
[^Zheng2025FastF5-TTS]: [**Fast F5-TTS**: Accelerating Flow-Matching-Based Text-to-Speech via Empirically Pruned Step Sampling](2025.05.26_Fast_F5-TTS.md). ArXiv:2505.19931v2.
[^Ho2022CFG]: [**CFG**: Classifier-Free Diffusion Guidance](../Diffusion/_2022.07.26_Classifier-Free_Guidance.md). ArXiv:2207.12598v1/NeurIPS2021Workshop.
[^Meister2023LibriSpeech-PC]: [**LibriSpeech-PC**: Benchmark for Evaluation of Punctuation and Capitalization Capabilities of end-to-end ASR Models](../../Datasets/2023.10.04_LibriSpeech-PC.md). ArXiv:2310.02943v1.
[^Gao2022Paraformer]: [**Paraformer**: Fast and Accurate Parallel Transformer for Non-Autoregressive End-to-End Speech Recognition](../-ASR/2022.06.16_Paraformer.md). ArXiv:2206.08317v3/InterSpeech2022.
[^Chen2021WavLM]: [**WavLM**: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](../Tokenizers/2021.10.26_WavLM.md). ArXiv:2110.13900v5/JSTSP2022.
[^Saeki2022UTMOS]: [**UTMOS**: UTokyo-SaruLab System for VoiceMOS Challenge 2022](../../Evaluations/2022.04.05_UTMOS.md). ArXiv:2204.02152/InterSpeech2022.
[^Wang2025Uni-VERSA-Ext]: [**Uni-VERSA-Ext**: Improving Speech Enhancement with Multi-Metric Supervision from Learned Quality Assessment](../../Evaluations/2025.06.13_Uni-VERSA-Ext.md). ArXiv:2506.12260v2/ASRU2025.
[^Zen2019LibriTTS]: [**LibriTTS**: A Corpus Derived from LibriSpeech for Text-to-Speech](../../Datasets/2019.04.05_LibriTTS.md). ArXiv:1904.02882v1/InterSpeech2019.
[^He2024Emilia]: [**Emilia**: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation](../../Datasets/2024.07.07_Emilia.md). ArXiv:2407.05361v1/2501.15907v1/SLT2024.