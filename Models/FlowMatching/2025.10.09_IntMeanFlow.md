# IntMeanFlow: Few-Step Speech Generation With Integral Velocity Distillation

<details>
<summary>基本信息</summary>

- 标题: "IntMeanFlow: Few-Step Speech Generation With Integral Velocity Distillation."
- 作者:
  - 01 Wei Wang
  - 02 Rong Cao
  - 03 Yi Guo
  - 04 Zhengyang Chen
  - 05 Kuan Chen
  - 06 Yuanyuan Huo
- 链接:
  - [ArXiv](https://arxiv.org/abs/2510.07979v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2510.07979v1](PDF/2025.10.09_2510.07979v1_IntMeanFlow__Few-Step_Speech_Generation_With_Integral_Velocity_Distillation.pdf)
  - [Publication] #TODO

</details>

## Abstract

Flow-based generative models have greatly improved text-to-speech (TTS) synthesis quality, but inference speed remains limited by the iterative sampling process and multiple function evaluations (NFE).
The recent MeanFlow model accelerates generation by modeling average velocity instead of instantaneous velocity.
However, its direct application to TTS encounters challenges, including GPU memory overhead from Jacobian-vector products (JVP) and training instability due to self-bootstrap processes.
To address these issues, we introduce IntMeanFlow, a framework for few-step speech generation with integral velocity distillation.
By approximating average velocity with the teacher’s instantaneous velocity over a temporal interval, IntMeanFlow eliminates the need for JVPs and self-bootstrap, improving stability and reducing GPU memory usage.
We also propose the Optimal Step Sampling Search (O3S) algorithm, which identifies the model-specific optimal sampling steps, improving speech synthesis without additional inference overhead.
Experiments show that IntMeanFlow achieves 1-NFE inference for token-to-spectrogram and 3-NFE for text-to-spectrogram tasks while maintaining high-quality synthesis.
Demo samples are available\footnote{\url{https://vvwangvv.github.io/intmeanflow/}}.

## 1·Introduction

\label{sec:intro}

Text-to-speech (TTS) generation has made significant progress in recent years, with models achieving near-human-level, zero-shot synthesis capabilities[^Du2024CosyVoice], [^Du2025CosyVoice], [^Chen2024F5-TTS], [^Casanova2024Xtts], [^Casanova2022Yourtts], [^Anastassiou2024Seed-TTS], [^Zhu2025ZipVoice].

The rise of flow-based generative models[^Lipman2022Flow] has contributed to this advancement, offering promising results across various fields, including image synthesis[^Albergo2023Building], [^Liu2023Flow], video[^Jin2024Pyramidal] and music generation[^Prajwal2024MusicFlow].

These models learn to map data distributions to a latent space, enabling high-quality generation.

However, flow-based models often face a trade-off between sampling quality and efficiency, as their iterative sampling process can lead to slow inference and high computational costs.

To address this, recent efforts in the image domain, such as consistency models[^Liu2024Audiolcm], [^Fei2024Music], [^Song2023Consistency], [^Lu2024Simplifying,], [^Heek2024Multistep], [^Peng2025Flow-Anchored] and shortcut models[^Frans2025One], have been proposed to reduce the number of function evaluations~(NFE) while maintaining high-quality generation results.

Among these approaches, MeanFlow[^Geng2025Mean] has emerged as a promising solution to the trade-off between sampling quality and efficiency.

By modeling averaged velocity instead of instantaneous velocity, MeanFlow enables more efficient sampling without compromising output quality.

While it has shown success in image generation and audio generation[^Li2025MeanAudio], applying MeanFlow to TTS introduces several challenges.

First, the training process of MeanFlow relies on a self-bootstrap mechanism, which requires mixing with instantaneous velocity guidance similar to flow matching.

The strength of this guidance can significantly impact model performance, and without it, the model is prone to collapse.

Second, MeanFlow involves the Jacobian-vector product (JVP), a computationally intensive operation that consumes substantial GPU memory.

Additionally, JVP is not natively supported by certain custom CUDA operators or torch-native operations, such as flash attention, complicating the adaptation of existing models to the MeanFlow framework.

These memory and compatibility challenges make training large-scale TTS models with MeanFlow infeasible.

To address these challenges, we propose IntMeanFlow, a framework for few-step speech generation that leverages integral velocity distillation.

Building on the core motivation of MeanFlow, IntMeanFlow enables the model to learn averaged velocity instead of instantaneous velocity.

Motivated by the observation that the quality of speech generated by flow-based models plateaus after a certain NFE, we approximate the average velocity over a temporal interval $[t,r]$ by dividing the accumulated displacement between discrete time steps by the interval duration.

We further introduce an initialization strategy to leverage pretrained flow-matching models, enabling smoother migration from existing models.

By eliminating the need for self-bootstrap and reliance on the Jacobian-vector product (JVP), IntMeanFlow ensures improved stability, reduced GPU memory consumption during training, and better compatibility with existing models, simplifying the adaptation process.

Additionally, based on the empirical observation that denser sampling near noisier timesteps leads to better generation quality, we introduce the Optimal Step Sampling Search (O3S) algorithm.

O3S automatically identifies model-specific near-optimal sampling steps, using a customizable quality metric and a ternary search algorithm.

This improves the generation quality without introducing additional inference overhead.

We conduct experiments on models from two widely used approaches in flow-based TTS: (1) CosyVoice2[^Du2024CosyVoice], which integrates a language model (LM) followed by a flow model converting time-aligned tokens into mel-spectrograms (token2mel), and (2) F5-TTS[^Chen2024F5-TTS], where a flow model directly converts raw text embeddings into mel-spectrograms (text2mel), learning time alignment inherently.

Experimental results show that IntMeanFlow achieves 1-NFE inference for the token2mel task in CosyVoice2 and 3-NFE for the text2mel task in F5-TTS, maintaining high-quality synthesis.

Our contributions can be summarized as follows:

\begin{enumerate}[topsep=0pt]

-  We propose a distillation framework that applies MeanFlow to TTS, overcoming training memory overhead and stability issues associated with its direction application to TTS.

-  We introduce a search algorithm to identify model-specific sampling steps, improving generation efficiency without increasing inference overhead.

-  We evaluate our method on two popular flow-based TTS models, achieving 1-NFE inference for token2mel and 3-NFE for text2mel tasks, while maintaining high-quality synthesis.
\end{enumerate}
