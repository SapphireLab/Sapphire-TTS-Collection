# Flamed-TTS

<details>
<summary>基本信息</summary>

- 标题: "Flamed-Tts: Flow Matching Attention-Free Models for Efficient Generating and Dynamic Pacing Zero-Shot Text-to-Speech."
- 作者:
  - 01 Hieu-Nghia Huynh-Nguyen
  - 02 Huynh Nguyen Dang
  - 03 Ngoc-Son Nguyen
  - 04 Van Nguyen
- 链接:
  - [ArXiv](https://arxiv.org/abs/2510.02848v1)
  - [Publication](https://github.com/flamed-tts/Flamed-TTS)
  - [Github]()
  - [Demo](https://flamed-tts.github.io)
- 文件:
  - [ArXiv:2510.02848v1](PDF/2025.10.03_2510.02848v1__Flamed-Tts__Flow_Matching_Attention-Free_Models_for_Efficient_Generating_and_Dynamic_Pacing_Zero-Shot_Text-to-Speech.pdf)
  - [Publication] #TODO

</details>

## Abstract

Zero-shot Text-to-Speech (TTS) has recently advanced significantly, enabling models to synthesize speech from text using short, limited-context prompts.
These prompts serve as voice exemplars, allowing the model to mimic speaker identity, prosody, and other traits without extensive speaker-specific data.
Although recent approaches incorporating language models, diffusion, and flow matching have proven their effectiveness in zero-shot TTS, they still encounter challenges such as unreliable synthesis caused by token repetition or unexpected content transfer, along with slow inference and substantial computational overhead.
Moreover, temporal diversity—crucial for enhancing the naturalness of synthesized speech—remains largely underexplored.
To address these challenges, we propose ***Flamed-TTS***, a novel zero-shot TTS framework that emphasizes low computational cost, low latency, and high speech fidelity alongside rich temporal diversity.
To achieve this, we reformulate the flow matching training paradigm and incorporate both discrete and continuous representations corresponding to different attributes of speech.
Experimental results demonstrate that ***Flamed-TTS*** surpasses state-of-the-art models in terms of intelligibility, naturalness, speaker similarity, acoustic characteristics preservation, and dynamic pace.
Notably, ***Flamed-TTS*** achieves the best WER of 4\% compared to the leading zero-shot TTS baselines, while maintaining low latency in inference and high fidelity in generated speech.
Code and audio samples are available at [our demo page](https://flamed-tts.github.io).

## 1·Introduction

In recent years, zero-shot TTS models have undergone significant development, achieving substantial advancements in performance.
Research efforts have primarily focused on improving the quality of synthesized speech, with particular emphasis on enhancing naturalness, speaker similarity, and intelligibility.
Consequently, these models have produced synthesized speech that closely approximates the quality of human speech, often rendering it nearly indistinguishable from natural vocal speech.
The research landscape of zero-shot TTS systems can be categorized into two main groups based on tokenization methodologies, each aligned with a dominant methodological paradigm: discrete-valued models and continuous-valued models.

Discrete-valued TTS models typically rely on external tokenizers—commonly referred to as neural codecs (**SoundStream**[^Zeghidour2021SoundStream], **EnCodec**[^Defossez2022EnCodec])—to convert continuous speech signals into sequences of discrete tokens via residual vector quantization (RVQ).
Representative models, such as **VALL-E**[^Wang2023VALL-E] and its extensions (**VALL-E 2**[^Chen2024VALL-E2], **VALL-E X**[^Zhang2023VALL-EX], **VALL-E R**[^Han2024VALL-ER], **ELLA-V**[^Song2024ELLA-V], **VoiceCraft**[^Peng2024VoiceCraft], **Spark-TTS**[^Wang2025Spark-TTS]), utilize these discrete representations within autoregressive architectures to enable zero-shot speech synthesis.
More recently, diffusion-based frameworks (**NaturalSpeech3**[^Ju2024NaturalSpeech3], **OZSpeech**[^Huynh2025OZSpeech]) have also been applied to discrete token modeling.
Despite their success, these approaches face notable limitations: RVQ may introduce information loss due to coarse quantization, and autoregressive (AR) models are susceptible to sampling errors such as token repetition.
These limitations raise questions regarding the necessity and efficiency of employing discrete representations and large-scale transformer architectures in high-fidelity zero-shot TTS systems.

In contrast, continuous-valued models (**VoiceBox**[^Le2023VoiceBox], **E2-TTS**[^Eskimez2024E2-TTS], **F5-TTS**[^Chen2024F5-TTS]) operate directly on mel-spectrograms and generate speech through in-context learning approach to implicitly model speaker identity and prosody from arbitrary speech prompts.
By bypassing external tokenization, these models reduce error accumulation and tend to produce more natural and speaker-consistent outputs.
However, the effectiveness of in-context learning requires large and diverse datasets, resulting in high computational demands.
While continuous-valued models offer improved synthesis quality, discrete-valued approaches can benefit from the modular structure and scalability of neural codecs pre-trained on large corpora, which enable explicit control over speech factors.

In addition, reducing computational costs in generative models has garnered significant attention and achieved notable progress.
In the domain of zero-shot TTS, several studies (**OZSpeech**[^Huynh2025OZSpeech], **SFM**[^Yang2025SFM], **RapFlow-TTS**[^Park2025RapFlow-TTS], **ZipVoice**[^Zhu2025ZipVoice]) have focused on minimizing computational demands by decreasing the number of sampling steps.
These efforts have yielded promising results, facilitating low-latency, real-world zero-shot TTS applications.

To address the aforementioned challenges while introducing an innovative approach to reduce latency and enhance temporal naturalness, we propose ***Flamed-TTS*** (**Fl**ow Matching **A**ttention-Free **M**odels for **E**fficient Generating and **D**ynamic Pacing Zero-shot **T**ext-**t**o-**S**peech).
Unlike prior work, ***Flamed-TTS*** does not focus on reducing latency by decreasing the number of sampling steps but instead prioritizes the modeling of flow matching training paradigm, eliminating the attention mechanism to improve efficiency.
Our observation reveals that compact zero-shot TTS models based on non-autoregressive (NAR) transformer architectures align input phonemes with corresponding discrete-valued tokens to a certain extent, achieving competitive intelligibility but often producing synthesized speech of suboptimal quality.
This observation inspires the development of a novel zero-shot TTS system, wherein discrete-valued tokens (also referred to as codes) are generated in a single forward pass using a compact Transformer neural network.
These tokens serve as the prior distribution for generating continuous-valued representations (also known as latent vectors) through a flow matching-based training paradigm, thereby improving naturalness.
We hypothesize that semantic features are effectively captured and encoded within the prior distribution.
Consequently, we eliminate the multi-head self-attention module, which is designed to model global relationships or semantic features, in the flow matching vector field estimator (also termed the Denoiser), significantly reducing computational complexity.

Additionally, due to their nature, AR TTS models excel at generating temporally diverse speech, with phoneme durations varying across runs and pauses emerging spontaneously in generated speech.
This allows the synthesized speech to approach human-level temporal diversity.
However, many real-world TTS systems rely on NAR architectures, which have become de facto standards in practical applications.
These models typically employ a Duration Predictor and Length Regulator **FastSpeech**[^Ren2019FastSpeech] to temporally align input phonemes with the corresponding generated speech signal.
While these components have proven effective for phoneme-to-speech alignment, they formulate duration estimation as a regression problem, producing a fixed duration for each phoneme.
This deterministic approach fails to capture the inherent variability of natural human speech, which features dynamic pacing intermittent silent pauses.
As a result, it limits the naturalness and expressiveness of the synthesized output.
Several prior works have investigated probabilistic duration modeling (**VITS**[^Kim2021VITS], **VITS2**[^Kong2023VITS2], [^Mehta2024Should]), showing promising improvements in speech naturalness.
However, such models offer only a partial solution, as both duration-varying phonemes and silent segments—prevalent in human speech—jointly contribute to temporal naturalness.
In this work, we adopt a probabilistic duration modeling mechanism, termed the *Duration Generator*, which probabilistically samples a duration for each phoneme, and introduce a *Silence Generator*, which inserts silences into spoken sequences to model pauses.
This enhances the naturalness of synthesized speech.
Both modules are formulated as probabilistic processes.

The key contributions of this paper are listed as follows:
-  We propose **Flow Matching Attention-Free Models**, a variant of **DiT**[^Peebles2022DiT] and **Optimal Conditional Transport Flow Matching**[^Lipman2022FM], designed to enhance the naturalness of synthesized speech by regressing a vector field from a semantically enriched prior distribution to the data distribution.
Consequently, this approach eliminates the need for self-attention—traditionally used to model semantic relationships—during the iterative sampling process, while preserving intelligibility.
-  We propose a novel joint modeling method termed **Probabilistic Duration \& Silence Generator** for both phoneme and silence durations to promote dynamic pacing in the synthesized speech, resulting in improved naturalness.
% We propose **Probabilistic Silence Generator**, which is utilized along with *Probabilistic Duration Generator* to improve the naturalness and enable the dynamic pace in generated speech.
-  Compared to prior works, ***Flamed-TTS*** achieves the best WER  while delivering comparable UTMOS and speaker similarity (SIM-O \& SIM-R) scores, all within a remarkably low-latency, compact neural architecture.
Specifically, our approach yields a **1.25× to 8×** reduction in WER compared to all baselines and demonstrates up to a **40\%** improvement in UTMOS over models trained on equivalently sized datasets.
Remarkably, it also achieves up to **106$\times$** faster inference speed than competing baselines.

## 2·Related Work

Zero-shot TTS aims to synthesize speech in the voice of an unseen speaker without any fine-tuning or supervised adaptation.
Given a reference speech prompt, the model captures the speaker’s vocal characteristics and generates new speech that reflects those traits while matching a given text prompt.
Many approaches have been proposed for zero-shot TTS, including diffusion-based models (**ZET-Speech**[^Kang2023ZET-Speech], **STEN-TTS**[^Tran2023STEN-TTS], **NaturalSpeech2**[^Shen2023NaturalSpeech2], **NaturalSpeech3**[^Ju2024NaturalSpeech3]) and flow-matching techniques (**P-Flow**[^Kim2023P-Flow], **Matcha-TTS**[^Mehta2023Matcha-TTS], **E2-TTS**[^Eskimez2024E2-TTS], **F5-TTS**[^Chen2024F5-TTS], **OZSpeech**[^Huynh2025OZSpeech]), which have demonstrated remarkable performance.
However, these models often suffer from inefficiency during inference, prompting the development of various optimization methods to improve speed and scalability.

One solution to improve efficiency is to adopt smaller backbones. **Small-E**[^Lemerle2024Small-E] proposed Small-E, a model that replaces the Transformer architecture with various recurrent modules such as **RWKV**[^Peng2023RWKV], **Mamba**[^Gu2023Mamba], and **Gated Linear Attention**[^Yang2023GLA].
These alternatives alleviate the quadratic complexity of self-attention and significantly enhance inference speed.
Similarly, **MobileSpeech**[^Ji2024MobileSpeech] introduced the MobileSpeech framework, designed for fast and high-fidelity zero-shot TTS on mobile devices.
Their approach not only reduces model size but also leverages a mask-based parallel generation strategy to accelerate audio synthesis.

An alternative approach is to enhance the flow-matching algorithm.
ZipVoice **ZipVoice**[^Zhu2025ZipVoice] utilizes Zipformer **Zipformer**[^Yao2023Zipformer] as the backbone and introduces a flow distillation method to reduce the number of sampling steps required during inference.
**SFM**[^Yang2025SFM] proposes a mechanism to improve the efficiency and quality of flow-matching-based TTS models by constructing intermediate states along the flow-matching paths—rather than starting from pure noise—using coarse output representations from a weak generator.
**RapFlow-TTS**[^Park2025RapFlow-TTS] adopts consistency flow matching, enabling the model to learn to produce consistent outputs along a straightened trajectory more effectively.

Besides the methods above, there are also various techniques outside the zero-shot TTS domain that aim to reduce computational costs.
One-step and consistency models (**CM-TTS**[^Li2024Cm-TTS], **CoMoSpeech**[^Ye2023CoMoSpeech]) significantly reduce inference time by collapsing multi-step diffusion into a single step without sacrificing audio quality.
Shortcut models further improve efficiency by learning to skip multiple diffusion steps at once using self-consistency losses.
Additionally, autoregressive models (**MELLE**[^Meng2024MELLE], **ARDiT**[^Liu2024ARDiT]) operate directly in the continuous domain which eliminates the need for discrete vector quantization.
These approaches simplify the architecture and improve inference speed while maintaining competitive fidelity, offering promising directions that could be incorporated into future zero-shot TTS systems.

## 3·Method

This section is divided into three main subsections:
-  **Overall Architecture**: This subsection presents the end-to-end pipeline, detailing the process from input (text and acoustic prompt) to synthesized speech.
-  **Probabilistic Duration and Silence Generation**: This subsection describes our proposed temporal dynamics modeling approach, aiming to produce realistic dynamic pacing in the synthesized speech.
-  **Attention-Free Flow Matching Models**: This subsection introduces a novel training paradigm where the attention mechanism is removed from the iterative denoising process in Flow Matching.
We leverage **FACodec**[^Ju2024NaturalSpeech3], a neural codec-based framework that decomposes speech waveforms into disentangled components including speaker identity and code sequences capturing prosody, content, and acoustic details.
Specifically, for each input waveform, FACodec compresses the waveform into a latent representation, which plays the role of the continuous-valued representations, and disentangles it into six sequences of discrete-valued tokens (or codes): one for prosody, two for content, and three for acoustic details.
In this work, we use these six code sequences to model a semantically enriched prior distribution, which is then used to initialize the starting points in the flow matching process for generating fine-grained continuous-valued representations.

### Overall Architecture

> <a id="fig:overall-architecture">![](Images/2025.10.03_Flamed-TTS_Fig.01.png)</a>
> Fig.01: Overview of ***Flamed-TTS***.
> The input speech prompt is first processed by the *Codec Encoder*, which produces six latent codes: one for prosody, two for content, and three for acoustic details.
> These encoded representations are then duplicated based on the durations predicted by the *Duration Generator*, while the *Silence Generator* inserts silences after each phoneme.
> The *Code Decoder* then generates predicted codes for the text prompt, conditioned on both the encoded phonemes and the latent representation of the reference speech.
> These predicted codes are converted into embeddings and merged before being passed through the *Denoiser*, where flow matching is performed.
> Finally, the output embeddings are fed into the *Codec Decoder* to synthesize the final speech waveform.

We propose a compact yet effective Zero-shot TTS framework, illustrated in detail in [Fig.01](#fig:overall-architecture).
Our framework consists of two key components, including *Code Generator* and *Denoiser*.
First, input phonemes are mapped to discrete-valued tokens by the *Code Generator*.
The corresponding hidden representations of those tokens are then fed into the *Denoiser*, which produces continuous-valued representations.
Finally, these fine-grained tokens are converted into a waveform using the Codec Decoder.

The *Code Generator* comprises four components: the *Phoneme Encoder*, *Duration Generator*, *Silence Generator*, and *Code Decoder*.
The *Phoneme Encoder* converts input phonemes into hidden representations, which are expanded according to durations predicted by the *Probabilistic Duration Generator*.
Simultaneously, the *Silence Generator* predicts optional silences, allowing zero-duration outputs when pauses are unsuitable.
Detailed formulations are provided in the next section.

After the hidden representations of the input phonemes have been temporally expanded and silences have been inserted, the resulting phoneme-encoded sequence is fed into the *Code Decoder* to generate sequences of codes.
At this stage, speech prompts are also incorporated to capture prosodic and acoustic attributes.
We adopt the hierarchical architecture proposed by **OZSpeech**[^Huynh2025OZSpeech] to map the phoneme-encoded input sequence to code sequences.
In contrast to prior work, which omits speech prompts during this modeling stage, we modify the *Code Decoder*—as illustrated in Fig.~[fig:qdecoder-denoiser](#fig:qdecoder-denoiser)a—to integrate speech prompts into the code generation process.
The *Code Decoder* models the conditional distribution over the six-level code sequence $\mathbf{q}_{1:6}$ given the encoded phoneme sequence $\mathcal{P}$, the speech prompt $\mathbf{p}$, and decoder parameters $\psi$, formulated as:

$$\begin{aligned}
\label{eq:q-decoder}
p(\mathbf{q}_{1:6} \mid \mathcal{P}; \mathbf{p}; \psi) &= p(\mathbf{q}_{1} \mid \mathcal{P}; \mathbf{p}_{1}; \mathcal{F}_{\psi}^1)  \nonumber\\
&\times \prod_{j=2}^{6} p(\mathbf{q}_{j} \mid \mathbf{q}_{j-1}; \mathbf{p}_{j}; \mathcal{F}_{\psi}^j),
\end{aligned}$$

where $\mathbf{q}_j$ denotes the $j$-th code sequence generated by the Feed-Forward Transformer (FFT) decoder layer(s) $\mathcal{F}_{\psi}^j$, and $\mathbf{p}_j$ represents the speech prompt at the same hierarchical level as the target $\mathbf{q}_j$.
To predict $\mathbf{q}_j$, the inputs $\mathbf{p}_j$ and $\mathbf{q}_{j-1}$ are concatenated and fed into $\mathcal{F}_{\psi}^j$.
After processing, the output segment corresponding to $\mathbf{s}_j$ is discarded, and the remaining portion is taken as the estimated $\mathbf{q}_j$.
The Prior Loss $\mathcal{L}_{prior}$ is designed to minimize the negative log-likelihood of the joint distribution defined in \eqref{eq:q-decoder}, guiding the model to effectively learn content code sequences conditioned on phonemes, while the other aspects, including prosody and acoustic details, are encouraged to mimic the corresponding speech prompt.

After obtaining the six code sequences, we adopt the code encoding and folding mechanism proposed by **OZSpeech**[^Huynh2025OZSpeech] to reshape the data from the space $B \times 6 \times L \times D$ into $B \times L \times 6D$.
This transformed representation is then passed through CNN layers to further compress it into the space $B \times L \times D'$, as illustrated in Fig.[fig:qdecoder-denoiser](#fig:qdecoder-denoiser).
This compressed representation serves as the initial input for the *Denoiser*, which generates fine-grained continuous-valued representations without relying on any additional conditioning.
The formulation and modeling details of the *Denoiser* will be discussed in a later section.

![](figs/q_decoder_denoiser_cropped.pdf)

> <a id="fig:qdecoder-denoiser">![](Images/2025.10.03_Flamed-TTS_Fig.02.png)</a>
> Fig.02: Code Decoder architecture.
> With the encoded phonemes, the codes are generated gradually by corresponding *FFT Blocks* with condition of prior code of speech prompt.
> These synthesized codes are combined by a CNN module.
> This embedding is finally finetuned by the *Denoiser* with flow matching algorithm.

### Probabilistic Duration and Silence Generation

Existing NAR-based TTS models typically use a duration predictor as a regressor, which takes the encoded phoneme sequence as input and predicts the corresponding durations, trained using mean squared error (MSE) loss in the log-duration domain.
In this paper, we treat this module as a probabilistic component and additionally introduce the *Silence Generator*, which is also modeled probabilistically.
Both modules are trained using the optimal transport conditional flow matching paradigm, with the objective of enabling variation in their log-domain outputs across different runs.
More specifically, given an encoded phoneme sequence $\mathcal{P} \in \mathbb{R}^{L \times D}$, we model the *Duration Generator* inspired by [^Mehta2024Should] using the following objective:

$$
\label{eq:durgen}
\mathcal{L}_{dur}(\phi) = \mathbb{E}_{t,\mathcal{P},\mathbf{d}_0,\mathbf{d}_1}
\left\lVert
\mathbf{\mathcal{D}}_\phi(\mathbf{d}_t,\mathcal{P},t) - (\mathbf{d}_1 - \mathbf{d}_0)
\right\rVert^2,
$$

where $\mathcal{D}_{\phi}$ denotes *Duration Generator* neural network parameterized by $\phi$, and $\mathbf{d}_t = t\mathbf{d}_1 + (1 - t)\mathbf{d}_0$, with $\mathbf{d}_t, \mathbf{d}_1, \mathbf{d}_0 \in \mathbb{R}^{L \times 1}$ and $t \sim \mathcal{U}(0, 1)$.
The vectors $\mathbf{p}$ and $\mathbf{d}_t$ are concatenated along the hidden dimension and projected into the space $\mathbb{R}^D$ before being passed through the neural network.
Thanks to this probabilistic training framework, the *Duration Generator* learns to denoise purely noisy inputs $\mathbf{d}_0 \in \mathbb{R}^1$ into log-scale duration values, conditioned on the encoded phoneme sequence $\mathcal{P} \in \mathbb{R}^{L \times D}$.

Similarly, the *Silence Generator* is modeled as:

$$
\label{eq:silgen}
\mathcal{L}_{sil}(\xi) = \mathbb{E}_{t,\mathcal{P},\mathbf{s}_0,\mathbf{s}_1}
\left\lVert
\mathbf{\mathcal{S}}_\xi(\mathbf{s}_t,\mathcal{P},t) - (\mathbf{s}_1 - \mathbf{s}_0)
\right\rVert^2,
$$

where $\mathcal{S}_\xi$ refers to the *Silence Generator*, a neural network parameterized by $\xi$, and $\mathbf{s}_t = t\mathbf{s}_1 + (1 - t)\mathbf{s}_0$, with $\mathbf{s}_t, \mathbf{s}_1, \mathbf{s}_0 \in \mathbb{R}^{L \times 1}$, and $t \sim \mathcal{U}(0, 1)$.
Following the same design as the *Duration Generator*, we use the concatenated representation of $\mathbf{s}_t$ and $\mathcal{P}$ as the input to the network.

To insert silences after each phoneme, we first prepend a special silent phoneme symbol $\verb|[SIL]|$ to the beginning of the phoneme sequence.
This encoded $\verb|[SIL]|$ token is then appended to the end of each phoneme in the sequence.
The detailed sampling and expanding algorithm is described as in Algorithm 1, provided in Supplementary Material.
The algorithm constrains the minimum duration of a phoneme to one, whereas silence may have a minimum duration of zero.
This design enables the *Silence Generator* to flexibly insert silent segments without affecting the temporal structure of the speech.

### Flow Matching Attention-Free Models

**Optimal Transport Conditional Flow Matching** paradigm[^Lipman2022FM] has emerged as the de facto approach to train generative models (e.g., DiT) by guiding samples from a prior distribution (typically Gaussian) toward a target data point $\mathbf{X}_1$.
For training, a random time $t \sim \mathcal{U}(0,1)$ is selected, and a noise sample $\mathbf{X}_0 \sim \mathcal{N}(0, \mathbf{I})$ is used to construct an intermediate state $x_t$.
The model learns to predict the velocity $v_t$ that moves $x_t$ toward $x_1$.
The intermediate sample follows a linear or optimal transport path:

$$
\mathbf{x}_t = t \mathbf{x}_1 + (1 - (1 - \sigma_{\min})t) \mathbf{x}_0,
$$

with the ground truth velocity given by:

$$
\mathbf{u}_t = \frac{d\mathbf{x}_t}{dt} = \mathbf{x}_1 - (1 - \sigma_{\min}) \mathbf{x}_0.
$$

Notably, the target velocity $\mathbf{u}_t$ remains independent of the chosen time step $t$.
Let $\theta$ denote the model parameters and $\mathbf{c}$ a condition; the model predicts the velocity as $\mathbf{v}_{\theta}(\mathbf{x}_t, \mathbf{c}, t)$.
Training involves minimizing the MSE between the predicted and true velocities:

$$
\mathbb{E}_{\mathbf{x}_0,\mathbf{x}_1,t,\mathbf{c}} \left\lVert \mathbf{v}_{\theta}(\mathbf{x}_t, \mathbf{c}, t) - \mathbf{u}_t \right\rVert^2.
$$

Although pure noise $\mathbf{x}_0 \sim \mathcal{N}(0, I)$ is commonly used as the initial input for most generative models, it constitutes a suboptimal prior due to the absence of semantic information.
We hypothesize that, under such conditions, generative models are compelled to rely heavily on attention mechanisms to capture global dependencies and infer high-level semantic representations.
Conversely, if the model is provided with a semantically enriched prior, the necessity for attention-based operations may be substantially reduced.
In this work, we empirically demonstrate this hypothesis in the context of zero-shot TTS synthesis.
Notably, our approach removes the reliance on attention mechanisms while outperforming existing methods that adopt the original training paradigm of flow matching.

Let $\mathbf{x}_{\text{pr}}$ denote the semantically enriched prior distribution.
The initial point $\mathbf{x}_0$ is then defined as:

$$
\mathbf{x}_0' = \mathbf{x}_\mathbf{pr} + \tau \cdot \epsilon,
$$

where $\epsilon \sim \mathcal{N}(0, \mathbf{I})$, and $\tau$ is a scalar hyperparameter controlling the noise scale.
The addition of noise to the prior serves to enhance the diversity of the resulting vector field, promoting better generalization during training.
Throughout our experiments, we set $\tau = 1$ by default.
The remaining components of the optimal transport flow matching training paradigm—such as computing the intermediate sample $\mathbf{x}_t$ and estimating the target velocity $\mathbf{u}_t$—are retained from the original formulation.
The training objective of our proposed method, referred to as *Flow Matching Attention-Free Models*, is defined as follows, where $\mathbf{s}$ denotes the speaker identity:

$$
\mathcal{L}_**CFM**(\theta) =  \mathbb{E}_{\mathbf{x}_\mathbf{pr},\mathbf{x}_1, t,\mathbf{s}, \tau, \epsilon} \left\lVert \mathbf{v}_{\theta}(\mathbf{x}_t, \mathbf{s}, t) - (\mathbf{x}_1 - \mathbf{x}_0') \right\rVert^2.
$$

We adopt the DiT architecture [^Peebles2022DiT] as the backbone for the vector field estimator, with a key modification: the multi-head self-attention modules are replaced by a lightweight ConvNeXt module.
This architectural change significantly reduces the computational complexity from $\mathcal{O}(L^2 \cdot d)$ to $\mathcal{O}(L \cdot k^2 \cdot d)$, where $L$ denotes the sequence length, $k$ is the convolutional kernel size, and $d$ is the hidden dimensionality (i.e., the number of channels).

### Total loss

We formulate the total loss function as:

$$
\mathcal{L}_{total} = \mathcal{L}_{prior} + \mathcal{L}_{dur} + \mathcal{L}_{sil} + \mathcal{L}_{CFM} + \mathcal{L}_{anchor}.
$$

Here, the loss functions $\mathcal{L}_{prior}$, $\mathcal{L}_{dur}$, and $\mathcal{L}_{sil}$ define the training objectives for the Code Generator, while $\mathcal{L}_{CFM}$ and $\mathcal{L}_{anchor}$ are designed to guide the construction of the vector field via the Denoiser.
Specifically, $\mathcal{L}_{CFM}$ is the flow matching loss, and $\mathcal{L}_{\text{anchor}}$ is an auxiliary loss term to stabilize the training process and is expressed as:

$$
\mathcal{L}_{anchor} = \left\lVert \mathbf{\tilde{x}}_1 - \mathbf{x}_1 \right\rVert^2
$$

where $\space \mathbf{\tilde{x}}_1 = \mathbf{x}_t + (1-t) \cdot \mathbf{v}_\theta(\mathbf{x}_t,\mathbf{s},t)$.

## 4·Experiments

### Experimental Setup

#### Dataset

We employ the **LibriTTS**[^Zen2019LibriTTS] dataset for training, which comprises multi-speaker English audio recordings.
For evaluation, we use the **LibriSpeech**[^Panayotov2015LibriSpeech] test-clean dataset as a standard benchmark.

#### Evaluation Metrics

We evaluate model performance across multiple dimensions, employing not only conventional metrics such as speech quality (UTMOS), speaker similarity (SIM-O and SIM-R), content accuracy (WER), prosodic features, generation efficiency, but also proposing a range of metrics to assess temporal diversity.
Prosodic features, including pitch and energy, are analyzed for accuracy and error trends.
To assess generation efficiency, we report the Number of Function Evaluations (NFE) and Real-Time Factor (RTF).
Temporal diversity is evaluated using Speech Rate, Mean Phoneme Duration (MPhD), Number of Pauses (\#Pauses), and Mean Pause Duration (MPaD).
Additional evaluation details are provided in Appendix Supplementary Material.

#### Baselines

We compare our method with existing zero-shot TTS systems.
A detailed overview of baseline models is presented in Supplementary Material.

### Main Results

**Overall Results**

> <a id="tab:overall-results">![](Images/2025.10.03_Flamed-TTS_Tab.01.png)</a>
> Performance evaluation on the *LibriSpeech test-clean* across different audio prompt lengths.
> **Bold** indicates the best result, and \underline{underline} indicates the second-best result.
> ($\uparrow$) indicates that higher values are better, while ($\downarrow$) indicates that lower values are better.
> $[\heartsuit]$ means reproduced results.
> $[\diamondsuit]$ and $[\spadesuit]$ mean results inferred from official and ufficial checkpoints, respectively.
> Abbreviation: VB (VoxBox), LT (LibriTTS), GS (GigaSpeech).

[Tab.01](#tab:overall-results) compares ***Flamed-TTS*** with baseline models under 1-, 3-, and 5-second prompt conditions. ***Flamed-TTS*** consistently outperforms all baselines across key metrics, including WER, prosody, and energy measures ($\mathbf{F0}_\mathbf{ACC}, \mathbf{EN}_\mathbf{ACC}, \mathbf{F0}_\mathbf{RMSE}, \mathbf{EN}_\mathbf{RMSE}$), achieving a WER as low as 4\% despite using 18–200× less training data than models like VoiceCraft and SparkTTS, which show 2.5–4.75× higher (worse) WERs.
These results emphasize the effectiveness of phoneme-to-speech alignment mechanisms (e.g., duration predictors) in improving zero-shot TTS performance, as seen in models such as NaturalSpeech 2, OZSpeech, and ***Flamed-TTS***.
In terms of naturalness, ***Flamed-TTS*** ranks second in UTMOS at longer prompt lengths, and it also achieves strong speaker similarity, ranking second in SIM-R and maintaining high SIM-O scores.
Overall, these findings support our design choice of removing the attention mechanism from the *Denoiser*, demonstrating that attention is not essential for achieving intelligibility, naturalness, and speaker consistency in high-fidelity zero-shot TTS.

#### Latency Comparison

> <a id="tab:size-rtf-results">![](Images/2025.10.03_Flamed-TTS_Tab.02.png)</a>
> Comparison of model size and latency for a 3-second audio prompt.
> The **\#Params** column shows the total parameters needed for end-to-end synthesis, with the first number indicating the parameters of the zero-shot model (trainable) and the second number representing the parameters of the neural codec or vocoder component (frozen).

[Tab.02](#tab:size-rtf-results) compares the model sizes and inference latencies of ***Flamed-TTS*** against previous baselines. ***Flamed-TTS*** is the most lightweight among all evaluated models, with a parameter count equal to 29\% the size of the largest model, VoiceCraft with Encodec. ***Flamed-TTS*** demonstrates substantial efficiency in inference speed, achieving nearly 10$\times$ smaller RTF compared to F5-TTS—a model that shares the same number of sampling steps (32) and training paradigm (flow matching) but retains attention mechanisms.
Furthermore, in comparison to OZSpeech, which is specifically designed for single-step sampling, ***Flamed-TTS*** exhibits comparable latency.
Even with 16 sampling steps, its RTF remains close to that of OZSpeech, highlighting the efficiency of its attention-free denoising architecture.

#### Temporal Diversity Analysis

> <a id="tab:temp-results">![](Images/2025.10.03_Flamed-TTS_Tab.03.png)</a>
> Comparison of temporal diversity across models using a 5-second audio prompt.
> We report both the mean and standard deviation of Speech Rate, MPhD, \#Pauses, and MPaD to reflect variations in the temporal domain.
> All models were trained on the same dataset—*LibriTTS* (500 hours).</a>

[Tab.03](#tab:temp-results) analyzes temporal diversity across models.
Compared to baselines with deterministic duration predictors (e.g., NaturalSpeech 2, OZSpeech), ***Flamed-TTS*** exhibits greater diversity and naturalness.
It generates slower speech rates and higher MPhD, both with higher variability.
Notably, its number of pauses and MPaD are 4× and 5× higher, respectively, indicating more spontaneous, human-like speech.
F5-TTS performs similarly in temporal diversity, suggesting its architecture and training paradigm remain effective in this regard.
However, it lags behind ***Flamed-TTS*** in WER due to the lack of phoneme-speech alignment.
These findings highlight that employing *Probabilistic Duration and Silence Generators* can help close the dynamic pacing gap between AR and NAR TTS models, while also enhancing phoneme-to-speech alignment for improved intelligibility.

### Ablation Study

#### NFE Evaluation

> <a id="tab:nfe-effect">![](Images/2025.10.03_Flamed-TTS_Tab.04.png)</a>
> Performance evaluation on the *LibriSpeech test-clean* across different NFE using 5-second audio prompts.
> The noise scaling factor $\tau$ is set to 0.3 by default.

We evaluate the performance of ***Flamed-TTS*** under varying numbers of function evaluations (NFE).
Results from [Tab.04](#tab:nfe-effect) show that SIM-O and SIM-R peak at NFE=16, while WER increases slightly (by ~1\%) at NFE=4 but remains stable around 4\% as NFE increases.
In contrast, UTMOS consistently improves with higher NFE, indicating enhanced naturalness with more denoising steps.

#### Noise Scaling Effect

> <a id="tab:tau-effect">![](Images/2025.10.03_Flamed-TTS_Tab.05.png)</a>
> Performance evaluation on the *LibriSpeech test-clean* across different noise scaling factor $\tau$ using 5-second audio prompts.
> The NFE is set to 64 as default.

In contrast to NFE, increasing the noise scaling factor $\tau$ does not consistently improve performance.
As shown in [Tab.05](#tab:tau-effect), the model achieves optimal results across all metrics at $\tau = 0.3$.
From $\tau = 0.0$ to $\tau = 0.3$, UTMOS, SIM-O, and SIM-R show modest improvements; however, further increases in $\tau$ beyond 0.3 result in performance degradation.

**Model Size Comparison**

> <a id="tab:model-size-results">![](Images/2025.10.03_Flamed-TTS_Tab.06.png)</a>
> Comparison of two ***Flamed-TTS*** model sizes: Base (143M parameters) and Small (76M parameters), evaluated on the *LibriSpeech test-clean* dataset.
> Both models were trained on the 500-hour *LibriTTS* training dataset.
> The NFE and $\tau$ are set to 128 and 0.3, respectively, as defaults.

[Tab.06](#tab:model-size-results) compares the performance of ***Flamed-TTS***-Base (143M parameters) and ***Flamed-TTS***-Small (76M parameters).
Despite a nearly 50\% reduction in model size, the Small variant achieves comparable performance to the Base model across all metrics except UTMOS.
Specifically, WER and speaker similarity remain largely unchanged, while naturalness shows a noticeable decline, with UTMOS scores dropping by approximately 4.5–6\%.

## 5·Conclusion

In this paper, we present ***Flamed-TTS***, a novel zero-shot TTS framework trained under the optimal transport conditional flow matching paradigm.
Unlike prior models following the same training strategy, ***Flamed-TTS*** eliminates the attention mechanism—a primary source of inference latency—by leveraging a semantically enriched prior as the initial condition in the iterative denoising process.
In this setup, the vector field estimator is treated purely as a denoiser, focusing solely on enhancing acoustic features.
This design not only preserves competitive performance but also substantially reduces latency.
Additionally, we introduce *Probabilistic Silence Generation*, in combination with *Probabilistic Duration Generation*, to improve temporal diversity.
Together, these components enable NAR TTS models to generate more spontaneous and human-like speech.

## References

[^Zeghidour2021SoundStream]: [**SoundStream**: An End-to-End Neural Audio Codec](../Tokenizers/2021.07.07_SoundStream.md). ArXiv:2107.03312v1/TASLP2021.
[^Defossez2022EnCodec]: [**EnCodec**: High Fidelity Neural Audio Compression](../Tokenizers/2022.10.24_EnCodec.md). ArXiv:2210.13438/TMLR2023.
[^Wang2023VALL-E]: [**VALL-E**: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers](../SpeechLM_TTS/2023.01.05_VALL-E.md). ArXiv:2301.02111/TASLP2025.
[^Chen2024VALL-E2]: [**VALL-E 2**: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers](../SpeechLM_TTS/2024.06.08_VALL-E_2.md). ArXiv:2406.05370v2.
[^Zhang2023VALL-EX]: [**VALL-E X**: Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling](../SpeechLM_TTS/2023.03.07_VALL-E_X.md). ArXiv:2303.03926.
[^Han2024VALL-ER]: [**VALL-E R**: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment](../SpeechLM_TTS/2024.06.12_VALL-E_R.md). ArXiv:2406.07855.
[^Song2024ELLA-V]: [**ELLA-V**: Stable Neural Codec Language Modeling with Alignment-Guided Sequence Reordering](../SpeechLM_TTS/2024.01.14_ELLA-V.md). ArXiv:2401.07333v1.
[^Peng2024VoiceCraft]: [**VoiceCraft**: Zero-Shot Speech Editing and Text-to-Speech in the Wild](../SpeechLM_TTS/2024.03.25_VoiceCraft.md). ArXiv:2403.16973v3/ACL2024.
[^Wang2025Spark-TTS]: [**Spark-TTS**: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens](../SpeechLM_TTS/2025.03.03_Spark-TTS.md). ArXiv:2503.01710v1.
[^Ju2024NaturalSpeech3]: [**NaturalSpeech3/FACodec**: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models](../Diffusion/2024.03.05_NaturalSpeech3.md). ArXiv:2403.03100v3/ICML2024Oral.
[^Huynh2025OZSpeech]: [**OZSpeech**: One-Step Zero-Shot Speech Synthesis with Learned-Prior-Conditioned Flow Matching](2025.05.19_OZSpeech.md). ArXiv:2505.12800v1/ACL2025.
[^Le2023VoiceBox]: [**VoiceBox**: Text-Guided Multilingual Universal Speech Generation at Scale](2023.06.23_Voicebox.md). ArXiv:2306.15687/NeurIPS2023Poster.
[^Eskimez2024E2-TTS]: [**E2 TTS**: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS](2024.06.26_E2_TTS.md). ArXiv:2406.18009v2/SLT2024.
[^Chen2024F5-TTS]: [**F5-TTS**: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching](2024.10.09_F5-TTS.md). ArXiv:2410.06885v3.
[^Yang2025SFM]: [**SFM**: Shallow Flow Matching for Coarse-to-Fine Text-to-Speech Synthesis](2025.05.18_SFM.md). ArXiv:2505.12226v1.
[^Park2025RapFlow-TTS]: [**RapFlow-TTS**: Rapid and High-Fidelity Text-to-Speech with Improved Consistency Flow Matching](2025.06.20_RapFlow-TTS.md). ArXiv:2506.16741v1/InterSpeech2025.
[^Zhu2025ZipVoice]: [**ZipVoice**: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching](2025.06.16_ZipVoice.md). ArXiv:2506.13053v3/ASRU2025.
[^Ren2019FastSpeech]: [**FastSpeech**: Fast Robust and Controllable Text to Speech](../Acoustic/2019.05.22_FastSpeech.md). ArXiv:1905.09263/NeurIPS2019.
[^Kim2021VITS]: [**VITS**: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech](../E2E/2021.06.11_VITS.md). ArXiv:2106.06103/ICML2021.
[^Kong2023VITS2]: [**VITS2**: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design](../E2E/2023.07.31_VITS2.md). ArXiv:2307.16430v1/InterSpeech2023.
[^Mehta2024Should]: [Should you use a probabilistic duration model in TTS? Probably! Especially for spontaneous speech](2024.06.08_Mehta2024Should.md). ArXiv:2406.05401v1/InterSpeech2024.
[^Peebles2022DiT]: [**DiT**: Scalable Diffusion Models with Transformers](../Diffusion/_2022.12.19_DiT.md). ArXiv:2212.09748v2/ICCV2023Oral.
[^Lipman2022FM]: [**FM**: Flow Matching for Generative Modeling](_2022.10.06_Flow_Matching.md). ArXiv:2210.02747v2.
[^Kang2023ZET-Speech]: [**ZET-Speech**: Zero-Shot Adaptive Emotion-Controllable Text-to-Speech Synthesis with Diffusion and Style-based Models](../Diffusion/2023.05.23_ZET-Speech.md). ArXiv:2305.13831v1/InterSpeech2023.
[^Tran2023STEN-TTS]: [**STEN-TTS**: Improving Zero-shot Cross-Lingual Transfer for Multi-Lingual TTS with Style-Enhanced Normalization Diffusion Framework](../Diffusion/2023.08.20_STEN-TTS.md). InterSpeech2023.
[^Shen2023NaturalSpeech2]: [**NaturalSpeech2**: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers](../Diffusion/2023.04.18_NaturalSpeech2.md). ArXiv:2304.09116v3/ICLR2024Spotlight.
[^Kim2023P-Flow]: [**P-Flow**: A Fast and Data-Efficient Zero-Shot TTS through Speech Prompting](2023.09.22_P-Flow.md). NeurIPS2023.
[^Mehta2023Matcha-TTS]: [**Matcha-TTS**: A Fast TTS Architecture with Conditional Flow Matching](2023.09.06_Matcha-TTS.md). ArXiv:2309.03199v2/ICASSP2024.
[^Lemerle2024Small-E]: [**Small-E**: Small Language Model with Linear Attention for Efficient Speech Synthesis](../SpeechLM_TTS/2024.06.06_Small-E.md). ArXiv:2406.04467v2/InterSpeech2024.
[^Peng2023RWKV]: [**RWKV**: Reinventing RNNs for the Transformer Era](../_Basis/2023.05.22_RWKV.md). ArXiv:2305.13048v2.
[^Gu2023Mamba]: [**Mamba**: Linear-Time Sequence Modeling with Selective State Spaces](../_Basis/2023.12.01_Mamba.md). ArXiv:2312.00752v2.
[^Yang2023GLA]: [**GLA**: Gated Linear Attention Transformers with Hardware-Efficient Training](../../Modules/Attention/2023.12.11_GLA.md). ArXiv:2312.06635v6.
[^Ji2024MobileSpeech]: [**MobileSpeech**: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech](../SpeechLM/2024.02.14_MobileSpeech.md). ArXiv:2402.09378v2/ACL2024.
[^Yao2023Zipformer]: [**Zipformer**: A faster and better encoder for automatic speech recognition](../-ASR/2023.10.17_Zipformer.md). ArXiv:2310.11230v4/ICLR2024.
[^Li2024CM-TTS]: [**CM-TTS**: Enhancing Real Time Text-to-Speech Synthesis Efficiency through Weighted Samplers and Consistency Models](../Consistency/2024.03.31_CM-TTS.md). ArXiv:2404.00569v1/NAACL2024.
[^Ye2023CoMoSpeech]: [**CoMoSpeech**: One-Step Speech and Singing Voice Synthesis via Consistency Model](../Consistency/2023.05.11_CoMoSpeech.md). ArXiv:2305.06908v4/ACM MM2023.
[^Meng2024MELLE]: [**MELLE**: Autoregressive Speech Synthesis without Vector Quantization](../SpeechLM_TTS/2024.07.11_MELLE.md). ArXiv:2407.08551v2.
[^Liu2024ARDiT]: [**ARDiT**: Autoregressive Diffusion Transformer for Text-to-Speech Synthesis](../Diffusion/2024.06.08_ARDiT.md). ArXiv:2406.05551v1.
[^Zen2019LibriTTS]: [**LibriTTS**: A Corpus Derived from LibriSpeech for Text-to-Speech](../../Datasets/2019.04.05_LibriTTS.md). ArXiv:1904.02882v1/InterSpeech2019.
[^Panayotov2015LibriSpeech]: [**LibriSpeech**: An ASR Corpus Based on Public Domain Audio Books](../../Datasets/2015.04.19_LibriSpeech.md). IEEE@ICASSP2015.