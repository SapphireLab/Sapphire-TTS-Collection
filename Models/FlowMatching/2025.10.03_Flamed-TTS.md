# Flamed-Tts: Flow Matching Attention-Free Models for Efficient Generating and Dynamic Pacing Zero-Shot Text-to-Speech

<details>
<summary>基本信息</summary>

- 标题: "Flamed-Tts: Flow Matching Attention-Free Models for Efficient Generating and Dynamic Pacing Zero-Shot Text-to-Speech."
- 作者:
  - 01 Hieu-Nghia Huynh-Nguyen
  - 02 Huynh Nguyen Dang
  - 03 Ngoc-Son Nguyen
  - 04 Van Nguyen
- 链接:
  - [ArXiv](https://arxiv.org/abs/2510.02848v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2510.02848v1](PDF/2025.10.03_2510.02848v1_Flamed-Tts__Flow_Matching_Attention-Free_Models_for_Efficient_Generating_and_Dynamic_Pacing_Zero-Shot_Text-to-Speech.pdf)
  - [Publication] #TODO

</details>

## Abstract

Zero-shot Text-to-Speech (TTS) has recently advanced significantly, enabling models to synthesize speech from text using short, limited-context prompts.
These prompts serve as voice exemplars, allowing the model to mimic speaker identity, prosody, and other traits without extensive speaker-specific data.
Although recent approaches incorporating language models, diffusion, and flow matching have proven their effectiveness in zero-shot TTS, they still encounter challenges such as unreliable synthesis caused by token repetition or unexpected content transfer, along with slow inference and substantial computational overhead.
Moreover, temporal diversity—crucial for enhancing the naturalness of synthesized speech—remains largely underexplored.
To address these challenges, we propose **Flamed-TTS**, a novel zero-shot TTS framework that emphasizes low computational cost, low latency, and high speech fidelity alongside rich temporal diversity.
To achieve this, we reformulate the flow matching training paradigm and incorporate both discrete and continuous representations corresponding to different attributes of speech.
Experimental results demonstrate that *Flamed-TTS* surpasses state-of-the-art models in terms of intelligibility, naturalness, speaker similarity, acoustic characteristics preservation, and dynamic pace.
Notably, *Flamed-TTS* achieves the best WER of 4\% compared to the leading zero-shot TTS baselines, while maintaining low latency in inference and high fidelity in generated speech.
Code and audio samples are available at our demo page \footnote{\url{https://flamed-tts.github.io}}.

## 1·Introduction

In recent years, zero-shot TTS models have undergone significant development, achieving substantial advancements in performance.

Research efforts have primarily focused on improving the quality of synthesized speech, with particular emphasis on enhancing naturalness, speaker similarity, and intelligibility.

Consequently, these models have produced synthesized speech that closely approximates the quality of human speech, often rendering it nearly indistinguishable from natural vocal speech.

The research landscape of zero-shot TTS systems can be categorized into two main groups based on tokenization methodologies, each aligned with a dominant methodological paradigm: discrete-valued models and continuous-valued models.

Discrete-valued TTS models typically rely on external tokenizers—commonly referred to as neural codecs [^Zeghidour2022SoundStream], [^D{\'e}fossez2023High]—to convert continuous speech signals into sequences of discrete tokens via residual vector quantization (RVQ).

Representative models, such as VALL-E and its extensions [^Chen2025Neural], [^Chen2024Vall-E], [^Zhang2023Speak], [^Han2024Vall-E], [^Song2024Ella-V], [^Peng2024V}oice{C}raft], [^Wang2025Spark-TTS], utilize these discrete representations within autoregressive architectures to enable zero-shot speech synthesis.

More recently, diffusion-based frameworks [^Ju2024N}atural{S}peech], [^Huynh-Nguyen2025OZSpeech] have also been applied to discrete token modeling.

Despite their success, these approaches face notable limitations: RVQ may introduce information loss due to coarse quantization, and autoregressive (AR) models are susceptible to sampling errors such as token repetition.

These limitations raise questions regarding the necessity and efficiency of employing discrete representations and large-scale transformer architectures in high-fidelity zero-shot TTS systems.

In contrast, continuous-valued models [^Le2023Voicebox], [^Eskimez2024E2], [^Chen2024F5-TTS] operate directly on mel-spectrograms and generate speech through in-context learning approach to implicitly model speaker identity and prosody from arbitrary speech prompts.

By bypassing external tokenization, these models reduce error accumulation and tend to produce more natural and speaker-consistent outputs.

However, the effectiveness of in-context learning requires large and diverse datasets, resulting in high computational demands.

While continuous-valued models offer improved synthesis quality, discrete-valued approaches can benefit from the modular structure and scalability of neural codecs pre-trained on large corpora, which enable explicit control over speech factors.

In addition, reducing computational costs in generative models has garnered significant attention and achieved notable progress.

In the domain of zero-shot TTS, several studies [^Huynh-Nguyen2025OZSpeech], [^Yang2025Shallow], [^Park2025RapFlow-TTS], [^Zhu2025ZipVoice] have focused on minimizing computational demands by decreasing the number of sampling steps.

These efforts have yielded promising results, facilitating low-latency, real-world zero-shot TTS applications.

To address the aforementioned challenges while introducing an innovative approach to reduce latency and enhance temporal naturalness, we propose *Flamed-TTS* (**Fl**ow Matching **A**ttention-Free **M**odels for **E**fficient Generating and **D**ynamic Pacing Zero-shot **T**ext-**t**o-**S**peech).

Unlike prior work, *Flamed-TTS* does not focus on reducing latency by decreasing the number of sampling steps but instead prioritizes the modeling of flow matching training paradigm, eliminating the attention mechanism to improve efficiency.

Our observation reveals that compact zero-shot TTS models based on non-autoregressive (NAR) transformer architectures align input phonemes with corresponding discrete-valued tokens to a certain extent, achieving competitive intelligibility but often producing synthesized speech of suboptimal quality.

This observation inspires the development of a novel zero-shot TTS system, wherein discrete-valued tokens (also referred to as codes) are generated in a single forward pass using a compact Transformer neural network.

These tokens serve as the prior distribution for generating continuous-valued representations (also known as latent vectors) through a flow matching-based training paradigm, thereby improving naturalness.

We hypothesize that semantic features are effectively captured and encoded within the prior distribution.

Consequently, we eliminate the multi-head self-attention module, which is designed to model global relationships or semantic features, in the flow matching vector field estimator (also termed the Denoiser), significantly reducing computational complexity.

Additionally, due to their nature, AR TTS models excel at generating temporally diverse speech, with phoneme durations varying across runs and pauses emerging spontaneously in generated speech.

This allows the synthesized speech to approach human-level temporal diversity.

However, many real-world TTS systems rely on NAR architectures, which have become de facto standards in practical applications.

These models typically employ a Duration Predictor and Length Regulator [^Ren2019FastSpeech] to temporally align input phonemes with the corresponding generated speech signal.

While these components have proven effective for phoneme-to-speech alignment, they formulate duration estimation as a regression problem, producing a fixed duration for each phoneme.

This deterministic approach fails to capture the inherent variability of natural human speech, which features dynamic pacing intermittent silent pauses.

As a result, it limits the naturalness and expressiveness of the synthesized output.

Several prior works have investigated probabilistic duration modeling [^Kim2021Conditional], [^Kong2023Vits2], [^Mehta2024Should], showing promising improvements in speech naturalness.

However, such models offer only a partial solution, as both duration-varying phonemes and silent segments—prevalent in human speech—jointly contribute to temporal naturalness.

In this work, we adopt a probabilistic duration modeling mechanism, termed the *Duration Generator*, which probabilistically samples a duration for each phoneme, and introduce a *Silence Generator*, which inserts silences into spoken sequences to model pauses.

This enhances the naturalness of synthesized speech.

Both modules are formulated as probabilistic processes.

The key contributions of this paper are listed as follows:

-  We propose **Flow Matching Attention-Free Models**, a variant of DiT [^Peebles2023Scalable] and Optimal Conditional Transport Flow Matching [^Lipman2023Flow], designed to enhance the naturalness of synthesized speech by regressing a vector field from a semantically enriched prior distribution to the data distribution.

Consequently, this approach eliminates the need for self-attention—traditionally used to model semantic relationships—during the iterative sampling process, while preserving intelligibility.

-  We propose a novel joint modeling method termed **Probabilistic Duration \& Silence Generator** for both phoneme and silence durations to promote dynamic pacing in the synthesized speech, resulting in improved naturalness.
% We propose **Probabilistic Silence Generator**, which is utilized along with *Probabilistic Duration Generator* to improve the naturalness and enable the dynamic pace in generated speech.

-  Compared to prior works, *Flamed-TTS* achieves the best WER  while delivering comparable UTMOS and speaker similarity (SIM-O \& SIM-R) scores, all within a remarkably low-latency, compact neural architecture.

Specifically, our approach yields a **1.25× to 8×** reduction in WER compared to all baselines and demonstrates up to a **40\%** improvement in UTMOS over models trained on equivalently sized datasets.

Remarkably, it also achieves up to **106$\times$** faster inference speed than competing baselines.

![](figs/flame-tts.drawio.pdf)

<a id="fig:overall-architecture">Overview of Flamed-TTS.

The input speech prompt is first processed by the *Codec Encoder*, which produces six latent codes: one for prosody, two for content, and three for acoustic details.

These encoded representations are then duplicated based on the durations predicted by the *Duration Generator*, while the *Silence Generator* inserts silences after each phoneme.

The *Code Decoder* then generates predicted codes for the text prompt, conditioned on both the encoded phonemes and the latent representation of the reference speech.

These predicted codes are converted into embeddings and merged before being passed through the *Denoiser*, where flow matching is performed.

Finally, the output embeddings are fed into the *Codec Decoder* to synthesize the final speech waveform.</a>

## 2·Related Work

Zero-shot TTS aims to synthesize speech in the voice of an unseen speaker without any fine-tuning or supervised adaptation.

Given a reference speech prompt, the model captures the speaker’s vocal characteristics and generates new speech that reflects those traits while matching a given text prompt.

Many approaches have been proposed for zero-shot TTS, including diffusion-based models [^Kang2023ZET-Speech], [^Tran2023Sten-TTS], [^Shen2024NaturalSpeech], [^Ju2024N}atural{S}peech] and flow-matching techniques [^Kim2023P-Flow], [^Mehta2024Matcha-TTS], [^Eskimez2024E2], [^Chen2024F5-TTS], [^Huynh-Nguyen2025OZSpeech], which have demonstrated remarkable performance.

However, these models often suffer from inefficiency during inference, prompting the development of various optimization methods to improve speed and scalability.

One solution to improve efficiency is to adopt smaller backbones. [^Lemerle2024Small-E] proposed Small-E, a model that replaces the Transformer architecture with various recurrent modules such as RWKV [^Peng2023Rwkv], Mamba [^Gu2024Mamba], and Gated Linear Attention [^Yang2024Gated].

These alternatives alleviate the quadratic complexity of self-attention and significantly enhance inference speed.

Similarly, [^Ji2024M}obile{S}peech] introduced the MobileSpeech framework, designed for fast and high-fidelity zero-shot TTS on mobile devices.

Their approach not only reduces model size but also leverages a mask-based parallel generation strategy to accelerate audio synthesis.

An alternative approach is to enhance the flow-matching algorithm.

ZipVoice [^Zhu2025ZipVoice] utilizes Zipformer [^Yao2024Zipformer] as the backbone and introduces a flow distillation method to reduce the number of sampling steps required during inference. [^Yang2025Shallow] proposes a mechanism to improve the efficiency and quality of flow-matching-based TTS models by constructing intermediate states along the flow-matching paths—rather than starting from pure noise—using coarse output representations from a weak generator.

RapFlow-TTS [^Park2025RapFlow-TTS] adopts consistency flow matching, enabling the model to learn to produce consistent outputs along a straightened trajectory more effectively.

Besides the methods above, there are also various techniques outside the zero-shot TTS domain that aim to reduce computational costs.

One-step and consistency models [^Li2024Cm-TTS], [^Ye2023CoMoSpeech] significantly reduce inference time by collapsing multi-step diffusion into a single step without sacrificing audio quality.

Shortcut models further improve efficiency by learning to skip multiple diffusion steps at once using self-consistency losses.

Additionally, autoregressive models [^Meng2024Autoregressive], [^Liu2024Autoregressive] operate directly in the continuous domain which eliminates the need for discrete vector quantization.

These approaches simplify the architecture and improve inference speed while maintaining competitive fidelity, offering promising directions that could be incorporated into future zero-shot TTS systems.
