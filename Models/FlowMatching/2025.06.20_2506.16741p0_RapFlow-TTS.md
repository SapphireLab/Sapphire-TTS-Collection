# RapFlow-TTS

<details>
<summary>基本信息</summary>

- 标题: "RapFlow-TTS: Rapid and High-Fidelity Text-to-Speech with Improved Consistency Flow Matching."
- 作者:
  - 01 Hyun Joon Park,
  - 02 Jeongmin Liu,
  - 03 Jin Sob Kim,
  - 04 Jeong Yeol Yang,
  - 05 Sung Won Han,
  - 06 Eunwoo Song
- 链接:
  - [ArXiv](https://arxiv.org/abs/2506.16741)
  - [Publication]()
  - [Github](https://github.com/naver-ai/RapFlow-TTS)
  - [Demo](https://tts-demo.github.io/rap.github.io/)
- 文件:
  - [ArXiv](_PDF/2025.06.20_2506.16741v1__RapFlow-TTS__Rapid_and_High-Fidelity_Text-to-Speech_with_Improved_Consistency_Flow_Matching.pdf)
  - [Publication] #TODO InterSpeech2025

</details>

## 摘要

<!--
We introduce ***RapFlow-TTS***, a rapid and high-fidelity TTS acoustic model that leverages velocity consistency constraints in Flow Matching (FM) training.
Although ordinary differential equation (ODE)-based TTS generation achieves natural-quality speech, it typically requires a large number of generation steps, resulting in a trade-off between quality and inference speed.
To address this challenge, ***RapFlow-TTS*** enforces consistency in the velocity field along the FM-straightened ODE trajectory, enabling consistent synthetic quality with fewer generation steps.
Additionally, we introduce techniques such as time interval scheduling and adversarial learning to further enhance the quality of the few-step synthesis.
Experimental results show that ***RapFlow-TTS*** achieves high-fidelity speech synthesis with a 5- and 10-fold reduction in synthesis steps than the conventional FM- and score-based approaches, respectively.
-->

本文提出了 ***RapFlow-TTS***，一个在**流匹配 (Flow Matching, FM)** 训练中利用速度一致性约束的快速且高保真的 TTS 声学模型.
尽管基于常微分方程的 TTS 生成可以实现自然质量的语音, 但往往需要大量的生成步骤, 从而需要在质量和推理速度之间进行权衡.

为了解决这一挑战, ***RapFlow-TTS*** 在流匹配直化的 ODE 轨迹上强制在速度场中保持一致性, 从而实现用更少的生成步骤获得一致的合成质量.
此外, 我们引入了如时间区间调度和对抗训练等技术以进一步增强少步合成的质量.
实验结果表明, ***RapFlow-TTS*** 能够实现高保真语音合成, 其生成步骤比传统的基于流匹配和得分的模型减少了 5-10 倍.

## 1·引言

Text-to-speech (TTS), also known as speech synthesis, aims to synthesize high-fidelity speech, given an input text \cite{wang2017tacotron, shen2018natural, kim2020glow}.
Among the various generative modeling approaches \cite{ren2019fastspeech, miao2020flow, li2019neural, ren2020fastspeech, jeong2021diff}, ordinary differential equations (ODE)-based models \cite{popov2021grad, park2024dex} have become strong solutions for outstanding TTS.
One representative method involves diffusion models using stochastic differential equations (SDE) \cite{song2020score, karras2022elucidating}, e.g., Grad-TTS \cite{popov2021grad}.
It trains a score network with an SDE-based diffusion process and solves the probability flow ODE for speech synthesis.
However, ODE trajectories obtained by diffusion models are complicated \cite{lipman2022flow, tong2023improving}, requiring numerous steps to generate high-quality speech \cite{guo2024voiceflow}.
Since TTS functions as a user interaction channel, the slow inference speed caused by numerous diffusion steps is a major drawback of diffusion-based TTS.

To resolve this limitation, researchers investigated improved ODE-based models \cite{lipman2022flow, tong2023improving, liu2022flow, song2023consistency} for TTS.
For example, Comospeech \cite{ye2023comospeech} applied consistency distillation \cite{song2023consistency} to transfer the knowledge of a diffusion teacher model to a consistent student model.
As the consistency constraint maps any point on the ODE trajectory to the endpoint, i.e., the target data, the student model can consistently produce high-quality speech regardless of the number of diffusion steps.
However, the complexity of diffusion-based ODE trajectories limited the effective construction of consistency models along those trajectories.

Meanwhile, flow matching (FM), which learns the vector field for the ODE and probability path, is another approach for improving ODE-based models.
Using an optimal transport plan that makes the probability path linear in time \cite{lipman2022flow, tong2023improving}, the ODE trajectory tends to be straight, making the few-step performance of FM superior to that of the diffusion models.
In TTS frameworks, Matcha-TTS \cite{mehta2024matcha} and VoiceFlow \cite{guo2024voiceflow} adopted the FM-based straight ODE trajectory, demonstrating high-quality speech synthesis with even fewer generation steps than conventional diffusion-based models.
However, completely generalizing the real distribution to a straight ODE trajectory is difficult; thus, the quality of the few-step synthesized speech has not yet reached that of the ground truth.
In summary, previous ODE-based TTS approaches failed to simultaneously satisfy both aspects, i.e., inference speed and speech synthesis quality.

In this study, we present ***RapFlow-TTS***, a rapid and high-fidelity TTS acoustic model that effectively addresses the limitations above.
Inspired by advancements in image generation, we adopt consistency FM \cite{yang2024consistency}, a novel FM method that explicitly enforces self-consistency in the velocity field.
By imposing consistency constraints on the velocity values, consistency FM directly defines straight flows from different time points to a common endpoint.
This allows ***RapFlow-TTS*** to learn how to produce consistent outputs along the straight trajectory more effectively, unlike diffusion-based methods with complicated trajectories, yielding a more effective consistency model.
Consequently, high-quality speech synthesis is achievable with only a few generation steps.
To the best of our knowledge, ***RapFlow-TTS*** is the first TTS framework based on consistency FM.

Furthermore, we propose several improved training techniques to enhance the performance, including shared dropout, Huber loss, time-interval scheduling, and adversarial learning, none of which have been explored in the context of consistency FM.
Experimental results show that the proposed ***RapFlow-TTS*** can synthesize more natural speech in just 2 steps, reducing generation steps by 5 to 10 times compared to previous ODE-based methods.
Our code and demos are available at [Github.IO](https://tts-demo.github.io/rap.github.io/).

## 2·背景

## 3·方法

## 4·实验

## 5·结果

## 6·结论

## 参考文献