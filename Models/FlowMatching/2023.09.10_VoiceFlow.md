# VoiceFlow

<details>
<summary>基本信息</summary>

- 标题: "VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching"
- 作者:
  - 01 Yiwei Guo (X-LANCE Lab, Shanghai Jiao Tong University, China)
  - 02 Chenpeng Du (X-LANCE Lab, Shanghai Jiao Tong University, China)
  - 03 Ziyang Ma (X-LANCE Lab, Shanghai Jiao Tong University, China)
  - 04 Xie Chen (X-LANCE Lab, Shanghai Jiao Tong University, China)
  - 05 Kai Yu (X-LANCE Lab, Shanghai Jiao Tong University, China)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2309.05027)
  - [Publication](https://doi.org/10.1109/ICASSP48485.2024.10445948)
  - [Github](https://github.com/X-LANCE/VoiceFlow-TTS)
  - [Demo](https://cantabile-kwok.github.io/VoiceFlow/)
- 文件:
  - [ArXiv v2](_PDF/2024.01.16_2309.05027v2__VoiceFlow__Efficient_Text-to-Speech_with_Rectified_Flow_Matching.pdf)
  - [ArXiv v3](_PDF/2024.09.01_2309.05027v3__VoiceFlow__Efficient_Text-to-Speech_with_Rectified_Flow_Matching.pdf)
  - [Publication] #TODO

</details>

## 摘要

Although diffusion models in text-to-speech have become a popular choice due to their strong generative ability, the intrinsic complexity of sampling from diffusion models harms their efficiency.
Alternatively, we propose ***VoiceFlow***, an acoustic model that utilizes a rectified flow matching algorithm to achieve high synthesis quality with a limited number of sampling steps.
***VoiceFlow*** formulates the process of generating mel-spectrograms into an ordinary differential equation conditional on text inputs, whose vector field is then estimated.
The rectified flow technique then effectively straightens its sampling trajectory for efficient synthesis.
Subjective and objective evaluations on both single and multi-speaker corpora showed the superior synthesis quality of ***VoiceFlow*** compared to the diffusion counterpart.
Ablation studies further verified the validity of the rectified flow technique in ***VoiceFlow***.

## 1·引言

Modern text-to-speech (TTS) has witnessed tremendous progress by adopting different types of advanced generative algorithms, such as TTS models with GANs~\cite{binkowski2019high,kim2021conditional}, normalizing flows~\cite{kim2020glow,valle2021flowtron,kim2021conditional}, self-supervised features~\cite{du2022vqtts,du2023speaker} or denoising diffusion models~\cite{popov2021grad,liu2022diffsinger,liu2023diffvoice,du2023unicats}.
Among them, diffusion-based TTS models recently received growing attention because of their high synthesis quality, such as GradTTS~\cite{popov2021grad} and DiffVoice~\cite{liu2023diffvoice}.
They also show versatile functionalities such as conditional generation~\cite{kim2022guided,guo2023emodiff}, speech editing~\cite{tae2021editts,liu2023diffvoice, du2023unicats} and speaker adaptation~\cite{liu2023diffvoice, du2023unicats}.
By estimating the score function $\nabla \log p_t(\bm x)$ of a stochastic differential equation (SDE), diffusion models are stable to train~\cite{dhariwal2021diffusion}.
They generate realistic samples by numerically solving the reverse-time SDE or the associated probability-flow ordinary differential equation (ODE).

However, a major drawback of diffusion models lies in their efficiency.
Regardless of SDE or ODE sampling methods, diffusion models typically require numerous steps to generate a satisfying sample, causing a large latency in inference.
Some efforts have been made to mitigate this issue and improve the speed-quality tradeoff in diffusion-based TTS models, usually by extra mathematical tools or knowledge distillation.
Fast GradTTS\cite{vovk2022fast} adopts maximum likelihood SDE solver~\cite{popov2022diffusionbased}, progressive distillation~\cite{salimans2022progressive} and denoising diffusion GAN~\cite{xiao2022tackling} to accelerate diffusion sampling.
FastDiff~\cite{huang2022fastdiff} optimizes diffusion noise schedules inspired by BDDM~\cite{lam2022bddm}.
ProDiff~\cite{huang2022prodiff} also uses a progressive distillation technique to halve the sampling steps from DDIM~\cite{song2021denoising} teacher iteratively.
LightGrad~\cite{chen2023lightgrad} adopts DPM-Solver~\cite{lu2022dpm} to explicitly derive a solution of probability-flow ODE.
A concurrent work, CoMoSpeech~\cite{ye2023comospeech}, integrates the consistency model~\cite{song2023consistency} as a special type of diffusion distillation.
These models successfully decrease the necessary number of sampling steps in diffusion models to some extent.
However, due to the intricate nature of the diffusion process, the speed-quality tradeoff still exists and is hard to overcome.

Despite denoising diffusion, another branch in the family of differential-equation-based generative models began to arise recently, namely the flow matching generative models~\cite{lipman2022flow,liu2022flow,tong2023improving}.
% Different from diffusion models which models the score function of a specific SDE, flow matching aims to model the vector field implied by an arbitrary ODE directly.
While diffusion models learn the score function of a specific SDE, flow matching aims to model the vector field implied by an arbitrary ODE directly.
A neural network is used for approximating the vector field, and the ODE can also be numerically solved to obtain data samples.
The design of such ODE and vector field often considers linearizing the sampling trajectory and minimizing the transport cost~\cite{tong2023improving}.
As a result, flow matching models have simpler formulations and fewer constraints but better quality.
VoiceBox~\cite{le2023voicebox} shows the potential of flow matching in fitting large-scale speech data, and LinDiff~\cite{liu2023boosting} shares a similar concept in the study of vocoders.
More importantly, the rectified flow~\cite{liu2022flow} technique in flow matching models further straightens the ODE trajectory in a concise way.
By training a flow matching model again but with its own generated samples, the sampling trajectory of rectified flow theoretically approaches a straightforward line, which improves the efficiency of sampling.
In essence, rectified flow matching achieves good sample quality even with a very limited number of sampling steps.
As a side note, its ODE nature also makes flow matching extensible for knowledge distillation similar in previous diffusion-based works~\cite{liu2022flow}.

Inspired by these, we propose to utilize rectified flow matching in the TTS acoustic model for the first time in literature.
We construct an ODE to flow between noise distribution and mel-spectrogram while conditioning it with phones and duration.
An estimator learns to model the underlying vector field.
Then, a flow rectification process is applied, where we generate samples from the trained flow matching model to train itself again.
In this way, our model is able to generate decent mel-spectrograms with much fewer steps.
We name our model ***VoiceFlow***.
To fully investigate its ability, we experiment both on the single-speaker benchmark LJSpeech and the larger multi-speaker dataset LibriTTS.
The results show that ***VoiceFlow*** outperforms the diffusion baseline in a sufficient number of sampling steps.
In a highly limited budget such as two steps, ***VoiceFlow*** still maintains a similar performance while the diffusion model cannot generate reasonable speech.
Therefore, ***VoiceFlow*** achieves better efficiency and speed-quality tradeoff while sampling.
The code and audio samples are available online at [Github.IO](https://cantabile-kwok.github.io/VoiceFlow).

## 2·背景

### 2.1·流匹配生成式模型

Denote the data distribution as $p_1(\bm x_1)$ and some tractable prior distribution as $p_0(\bm x_0)$.
Most generative models work by finding a way to map samples $\bm x_0\sim p_0(\bm x_0)$ to data $\bm x_1$.
Particularly, diffusion models manually construct a special SDE, and then estimate the score function of the probability path $p_t(\bm x_t)$ yielded by it.
Sampling is tackled by solving either the reverse-time SDE or probability-flow ODE alongside this probability path.
Flow matching generative models, on the other hand, model the probability path $p_t(\bm x_t)$ directly~\cite{lipman2022flow}.
Consider an arbitrary ODE
$$
\text{d}x_t = v_t (x_t)\text{d}t,
\tag{01}
$$

with $\bm v_t(\cdot)$ named the vector field and $t\in[0,1]$.
This ODE is associated with a probability path $p_t(\bm x_t)$ by the continuity equation $\frac{\dd }{\dd t} \log p_t(\bm x) + \operatorname{div}(p_t(\bm x)\bm v_t(\bm x)) = 0$.

However, the design of the vector field needs to be instantiated before practically applied.
\cite{lipman2022flow} proposes the method of constructing a conditional probability path with a data sample $\bm x_1$.
Suppose this probability path is $p_t(\bm x\mid \bm x_1)$, with boundary condition $p_{t=0}(\bm x\mid \bm x_1) = p_0(\bm x)$ and $p_{t=1}(\bm x\mid\bm x_1) = \mathcal N(\bm x\mid \bm x_1, \sigma^2\bm I)$ for sufficiently small $\sigma$.
By the continuity equation, there is an associated vector field $\bm v_t(\bm x\mid \bm x_1)$.
It is proven that estimating the conditional vector field by neural network $\bm u_\theta$ is equivalent, in the sense of expectation, to estimating the unconditional vector field, i.e.
$$
\begin{align}
    &\min_{\theta}\mathbb E_{t,p_t(\bm x)}\|\bm u_\theta(\bm x,t) - \bm v_t(\bm x)\|^2\\
    \equiv &\min_{\theta}\mathbb E_{t,p_1(\bm x_1),p_t(\bm x\mid \bm x_1)}\|\bm u_\theta(\bm x,t) - \bm v_t(\bm x\mid \bm x_1)\|^2.
    \label{eq:fm-target}
\end{align}
$$

Then, by designing a simple conditional probability path $p_t(\bm x\mid \bm x_1)$ and the corresponding $\bm v_t(\bm x\mid \bm x_1)$, one can easily draw samples from $p_t(\bm x\mid \bm x_1)$ and minimize Eq.\eqref{eq:fm-target}.
For example, \cite{lipman2022flow} uses the Gaussian path $p_t(\bm x\mid \bm x_1)=\mathcal N(\bm x\mid \bm \mu_t(\bm x_1),\sigma_t(\bm x_1)^2\bm I)$ and linear vector field $\bm v_t(\bm x\mid \bm x_1)=\frac{\sigma'_t(\bm x_1)}{\sigma_t(\bm x_1)}(\bm x-\bm \mu_t(\bm x_1))+\bm \mu_t'(\bm x_1)$.

Meanwhile, this conditioning technique can be further generalized, i.e. any condition $z$ for $p_t(\bm x\mid z)$ can lead to the same form of optimization target like Eq.\eqref{eq:fm-target}.
Thus, \cite{tong2023improving} proposes to additionally condition on a noise sample $\bm x_0$ to form a probability path $p_t(\bm x\mid \bm x_0, \bm x_1)=\mathcal N(\bm x\mid t\bm x_1 + (1-t)\bm x_0, \sigma^2 \bm I)$.
The conditional vector field therefore becomes $\bm v_t(\bm x\mid \bm x_0, \bm x_1)=\bm x_1-\bm x_0$, which is a constant straight line towards $\bm x_1$.
In this formulation, training the generative model only requires the following steps:
- Sample $\bm x_1$ from data and $\bm x_0$ from any noise distribution $p_0(\bm x_0)$;
- Sample a time $t\in[0,1]$ and then $\bm x_t \sim \mathcal N(t\bm x_1 + (1-t)\bm x_0, \sigma^2 \bm I)$;
- Apply gradient descent on loss $\|\bm u_\theta(\bm x,t) - (\bm x_1 - \bm x_0)\|^2$.

This is often referred to as the "conditional flow matching" algorithm, which is proven to outperform diffusion-based models with deep correlation to the optimal transport theory~\cite{tong2023improving}.

### 2.2·Rectified Flow 提升采样效率

The notion of rectified flow is proposed in \cite{liu2022flow}.
It is a simple but mathematically solid approach to improve the sampling efficiency of flow matching models.
The flow matching model here has the same formulation as that of \cite{tong2023improving}, which is conditioned on both $\bm x_1$ and $\bm x_0$.
Suppose a flow matching model is trained to generate data $\bm {\hat x}_1$ from noise $\bm x_0$ by the ODE in Eq.\eqref{eq:ODE}. In other words, $\bm x_0$ and $\bm {\hat x}_1$ are a pair of the starting and ending points of the ODE trajectory.
Then, this flow matching model is trained again, but conditions $\bm v_t(\bm x\mid \bm x_0,\bm x_1)$ and $p_t(\bm x\mid \bm x_0,\bm x_1)$ on the given pair $(\bm x_0, \bm {\hat x}_1)$ instead of independently sampling $\bm x_0, \bm x_1$.
This flow rectification step can be iterated multiple times, denoted by the recursion $\left(\bm z_0^{k+1}, \bm z_1^{k+1}\right) = \operatorname{FM}\left(\bm z_0^{k}, \bm z_1^k\right)$, with $\operatorname{FM}$ the flow matching model and $(\bm z_0^0, \bm z_1^0)=(\bm x_0, \bm x_1)$ the independently drawn noise and data samples.

Intuitively, rectified flow ``rewires" the sampling trajectory of flow matching models to become more straight.
Because the ODE trajectories cannot intersect when being solved, most likely the trajectory cannot be as straight as the conditional vector field in training.
However, by training the flow matching model again on the endpoints of the same trajectory, the model learns to find a shorter path to connect these noise and data.
This straightening tendency is theoretically guaranteed in \cite{liu2022flow}.
By rectifying the trajectories, flow matching models will be able to sample data more efficiently with fewer steps of ODE simulation.

## 3·方法

## 4·实验

## 5·结果

## 6·结论

## 参考文献
