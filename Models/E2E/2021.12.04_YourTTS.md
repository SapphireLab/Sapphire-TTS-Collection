# YourTTS

<details>
<summary>基本信息</summary>

- 标题: "YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for Everyone"
- 作者:
  - 01 Edresson Casanova
  - 02 Julian Weber
  - 03 Christopher Shulby
  - 04 Arnaldo Candido Junior
  - 05 Eren Golge
  - 06 Moacir Antonelli Ponti
- 链接:
  - [ArXiv](https://arxiv.org/abs/2112.02418)
  - [Publication](https://proceedings.mlr.press/v162/casanova22a.html)
  - [Github](https://github.com/coqui-ai/TTS)
  - [Demo](https://edresson.github.io/YourTTS/)
- 文件:
  - [ArXiv](_PDF/2112.02418v4__YourTTS__Towards_Zero-Shot_Multi-Speaker_TTS_and_Zero-Shot_Voice_Conversion_for_Everyone.pdf)
  - [Publication](_PDF/2112.02418p0__YourTTS__ICML2022.pdf)

</details>

## Abstract: 摘要

<table><tr><td width="50%">

***YourTTS*** brings the power of a multilingual approach to the task of zero-shot multi-speaker TTS.
Our method builds upon the **VITS** model and adds several novel modifications for zero-shot multi-speaker and multilingual training.
We achieved state-of-the-art (SOTA) results in zero-shot multi-speaker TTS and results comparable to SOTA in zero-shot voice conversion on the **VCTK dataset**.
Additionally, our approach achieves promising results in a target language with a single-speaker dataset, opening possibilities for zero-shot multi-speaker TTS and zero-shot voice conversion systems in low-resource languages.
Finally, it is possible to fine-tune the ***YourTTS*** model with less than 1 minute of speech and achieve state-of-the-art results in voice similarity and with reasonable quality.
This is important to allow synthesis for speakers with a very different voice or recording characteristics from those seen during training.

</td><td>

***YourTTS*** 将多语言方法的能力引入到零样本多说话人文本转语音任务中.
我们的方法基于 **VITS** 模型, 并添加了数种新式修改用于零样本多说话人和多语言训练.
我们在零样本多说话人文本转语音中获得了 SoTA 结果, 并在 **VCTK 数据集**上的零样本声音转换任务中取得了和 SoTA 相匹配的结果.
此外, 我们的方法在单说话人数据集上目标语言获得了具有前景的结果, 开辟了在低资源语言中实现零样本多说话人文本转语音和零样本声音转换系统的可能性.
最后, 我们可以用不到一分钟的语音对 ***YourTTS*** 模型进行微调, 并在声音相似度和可接受的质量方面取得 SoTA 结果.
这对于合成与训练时显著不同的声音或录音特征的说话人是至关重要的.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

Text-to-Speech (TTS) systems have significantly advanced in recent years with deep learning approaches, allowing successful applications such as speech-based virtual assistants.
Most TTS systems were tailored from a single speaker's voice, but there is current interest in synthesizing voices for new speakers (not seen during training), employing only a few seconds of speech.
This approach is called zero-shot multi-speaker TTS (ZS-TTS) as in [^01], [^02], [^03], **SC-GlowTTS**[^04].

ZS-TTS using deep learning was first proposed by [^05] which extended the **DeepVoice3** method[^06].
Meanwhile, **Tacotron2**[^07] was adapted using external speaker embeddings extracted from a trained speaker encoder using a generalized end-to-end loss (GE2E)[^08], allowing for speech generation that resembles the target speaker[^01].
Similarly, **Tacotron2** was used with a different speaker embeddings methods[^02], with LDE embeddings[^09] to improve similarity and naturalness of speech for unseen speakers[^10].
The authors also showed that a gender-dependent model improves the similarity for unseen speakers[^02].
In this context, Attentron[^03] proposed a fine-grained encoder with an attention mechanism for extracting detailed styles from various reference samples and a coarse-grained encoder.
As a result of using several reference samples, they achieved better voice similarity for unseen speakers.
ZSM-SS [^11] is a Transformer-based architecture with a normalization architecture and an external speaker encoder based on Wav2vec 2.0 [^12].
The authors conditioned the normalization architecture with speaker embeddings, pitch, and energy.
Despite promising results, the authors did not compare the proposed model with any of the related works mentioned above.
**SC-GlowTTS**[^04] was the first application of flow-based models in ZS-TTS.
It improved voice similarity for unseen speakers in training with respect to previous studies while maintaining comparable quality.

Despite these advances, the similarity gap between observed and unobserved speakers during training is still an open research question.
ZS-TTS models still require a considerable amount of speakers for training, making it difficult to obtain high-quality models in low-resource languages.
Furthermore, according to [^13], the quality of current ZS-TTS models is not sufficiently good, especially for target speakers with speech characteristics that differ from those seen in training.
Although **SC-GlowTTS**[^04] achieved promising results with only 11 speakers from the VCTK dataset [^14], when one limits the number and variety of training speakers, it also further hinders the model generalization for unseen voices.

In parallel with the ZS-TTS, multilingual TTS has also evolved aiming at learning models for multiple languages at the same time~ [^15], [^16], [^17], [^18].
Some of these models are particularly interesting as they allow for code-switching, i.e., changing the target language for some part of a sentence, while keeping the same voice [^17].
This can be useful in ZS-TTS as it allows using of speakers from one language to be synthesized in another language.

In this paper, we propose ***YourTTS*** with several novel ideas focused on zero-shot multi-speaker and multilingual training.
We report state-of-the-art zero-shot multi-speaker TTS results, as well as results comparable to SOTA in zero-shot voice conversion for the VCTK dataset.

Our novel zero-shot multi-speaker TTS approach includes the following contributions:

- State-of-the-art results in the English Language;
- The first work proposing a multilingual approach in the zero-shot multi-speaker TTS scope;
- Ability to do zero-shot multi-speaker TTS and zero-shot Voice Conversion with promising quality and similarity in a target language using only one speaker in the target language during model training;
- Require less than 1 minute of speech to fine-tune the model for speakers who have voice/recording characteristics very different from those seen in model training, and still achieve good similarity and quality results.

The audio samples for each of our experiments are available on the [demo web-site](https://edresson.github.io/YourTTS/).
For reproducibility, our source-code is available at the [Coqui TTS [Github]](https://github.com/coqui-ai/TTS), as well as the model checkpoints of all experiments at [YourTTS [Github]](https://github.com/Edresson/YourTTS).

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

***YourTTS*** builds upon **VITS**[^19], but includes several novel modifications for zero-shot multi-speaker and multilingual training.
First, unlike previous work (**SC-GlowTTS**[^04], **VITS**[^19]), in our model we used raw text as input instead of phonemes.
This allows more realistic results for languages without good open-source grapheme-to-phoneme converters available.

</td><td>

***YourTTS*** 建立在 **VITS**[^19] 之上, 但对零样本多说话人和多语言训练进行了一些新修改.
首先, 与之前的工作 (**SC-GlowTTS**[^04], **VITS**[^19]) 不同, 我们使用原始文本作为输入, 而不是音素.
这使得对于没有可用的开源字素转音素转换器 (G2P) 的语言, 可以获得更真实的结果.

</td></tr>
<tr><td>

As in previous works, e.g., **VITS**[^19], we use a Transformer-based text encoder (**Glow-TTS**[^20], **SC-GlowTTS**[^04]).
However, for multilingual training, we concatenate 4-dimensional trainable language embeddings into the embeddings of each input character.
In addition, we also increased the number of Transformer blocks to 10 and the number of hidden channels to 196.
As a decoder, we use a stack of 4 affine coupling layers (**RealNVP**[^21]) each layer is itself a stack of 4 **WaveNet** residual blocks[^22], as in **VITS** model.

</td><td>

与之前的工作 (如 **VITS**[^19]) 一样, 我们使用基于 Transformer 的文本编码器 (**Glow-TTS**[^20], **SC-GlowTTS**[^04]).
然而, 为了进行多语言训练, 我们将 4 维可训练语言嵌入拼接到每个输入字符的嵌入中.
此外, 我们还增加了 Transformer 块的数量到 10, 隐藏通道的数量到 196.
对于解码器部分, 我们使用 4 层仿射耦合层 (**RealNVP**[^21]) 的堆叠, 每层都是 4 个 **WaveNet** 残差块的堆叠, 与 **VITS** 模型相同.

</td></tr>
<tr><td>

As a vocoder we use the **HiFi-GAN**[^23] version 1 with the discriminator modifications introduced by **VITS**[^19].
Furthermore, for efficient end2end training, we connect the TTS model with the vocoder using a **Variational Autoencoder (VAE)**[^24].
For this, we use the Posterior Encoder proposed by **VITS**[^19].
The Posterior Encoder consists of 16 non-causal WaveNet residual blocks (**WaveGlow**[^25], **Glow-TTS**[^20]).
As input, the Posterior Encoder receives a linear spectrogram and predicts a latent variable, this latent variable is used as input for the vocoder and for the flow-based decoder, thus, no intermediate representation (such as mel-spectrograms) is necessary.
This allows the model to learn an intermediate representation; hence, it achieves superior results to a two-stage approach system in which the vocoder and the TTS model are trained separately (**VITS**[^19]).
Furthermore, to enable our model to synthesize speech with diverse rhythms from the input text, we use the stochastic duration predictor proposed in **VITS**[^19].

</td><td>

声码器我们使用 **HiFi-GAN** 版本 1, 判别器使用 **VITS**[^19] 引入的修改.
此外, 为了高效端到端训练, 我们将 TTS 模型和声码器使用变分自编码器[^24]连接起来.
为此, 我们使用 **VITS**[^19] 提出的后验编码器.
后验编码器由 16 个非因果 WaveNet 残差块组成 (**WaveGlow**[^25], **Glow-TTS**[^20]).

作为输入, 后验编码器接收线性频谱图, 预测隐变量, 这个隐变量作为声码器和流式解码器的输入, 因此, 不需要中间表示 (如梅尔频谱图).
这使得模型可以学习中间表示, 因此, 它相比两阶段方法系统单独训练声码器和 TTS 模型能取得更好的结果 (**VITS**[^19]).
此外, 为了使得模型能够从输入文本中合成具有多样的韵律的语音, 我们使用 **VITS**[^19] 提出的随机时长预测器.

</td></tr>
<tr><td>

***YourTTS*** during training and inference is illustrated in [Figure.01](#Fig.01), where `++` indicates concatenation, red connections mean no gradient will be propagated by this connection, and dashed connections are optional.
We omit the **HiFi-GAN** discriminator networks for simplicity.

</td><td>

***YourTTS*** 在训练和推理过程中如[图 01](#Fig.01) 所示, `++` 表示拼接, 红色连线表示不会传播梯度, 虚线表示可选.
我们为了简单起见省略了 **HiFi-GAN** 判别器网络.

</td></tr>
<tr><td colspan="2">

![](Images/2021.12.04_YourTTS_Fig.01.png)

<a id="Fig.01">Figure.01</a>: YourTTS diagram depicting (a) training procedure and (b) inference procedure.

</td></tr>
<tr><td>

To give the model zero-shot multi-speaker generation capabilities we condition all affine coupling layers of the flow-based decoder, the posterior encoder, and the vocoder on external speaker embeddings.
We use global conditioning (**WaveNet**[^22]) in the residual blocks of the coupling layers as well as in the posterior encoder.
We also sum the external speaker embeddings with the text encoder output and the decoder output before we pass them to the duration predictor and the vocoder, respectively.
We use linear projection layers to match the dimensions before element-wise summations (see [Figure.01](#Fig.01)).

</td><td>

为了赋予模型零样本多说话人生成能力, 我们将基于流的解码器, 后验编码器, 和声码器的所有仿射耦合层都条件在外部说话人嵌入上.
我们在耦合层的残差块和后验编码器中使用全局条件化 (**WaveNet**[^22]).
我们还将外部说话人嵌入和文本编码器输出和解码器输出相加, 并将它们分别传递给时长预测器和声码器.
我们使用线性投影层来匹配维度, 然后进行逐元素求和 (见[图 01](#Fig.01)).

</td></tr>
<tr><td>

Also, inspired by [^26], we investigated **Speaker Consistency Loss (SCL)** in the final loss.
In this case, a pre-trained speaker encoder is used to extract speaker embeddings from the generated audio and ground truth on which we maximize the cosine similarity.
Formally, let $\phi(\cdot)$ be a function outputting the embedding of a speaker, $cos\_sim$ be the cosine similarity function, $\alpha$ a positive real number that controls the influence of the SCL in the final loss, and $n$ the batch size, the SCL is defined as follows:

$$
L_{SCL} = \frac{- \alpha }{n} \cdot \sum_{i}^{n}{~cos\_sim(\phi(g_{i}), \phi(h_{i}))},
$$

where $g$ and $h$ represent, respectively, the ground truth and the generated speaker audio.

</td><td>

受到 [^26] 的启发, 我们研究了最终损失中的**说话人一致性损失 (SCL)**.
在这种情况下, 我们使用预训练的说话人编码器从生成的音频和真实音频上提取说话人嵌入, 并最大化余弦相似度.
形式上, 令 $\phi(\cdot)$ 为输出说话人嵌入的函数, $cos\_sim$ 为余弦相似度函数, $\alpha$ 为正实数, 控制 SCL 在最终损失中的影响, $n$ 为批大小, SCL 定义如下:

$$
L_{SCL} = \frac{- \alpha }{n} \cdot \sum_{i}^{n}{~cos\_sim(\phi(g_{i}), \phi(h_{i}))},
$$

</td></tr>
<tr><td>

During training, the Posterior Encoder receives linear spectrograms and speaker embeddings as input and predicts a latent variable $z$.
This latent variable and speaker embeddings are used as input to the GAN-based vocoder generator which generates the waveform.
For efficient end-to-end vocoder training, we randomly sample constant length partial sequences from $z$ as in **HiFi-GAN**[^23], **GAN-TTS**[^27], **FastSpeech2**[^28], **VITS**[^19].
The Flow-based decoder conditions the latent variable $z$ and speaker embeddings with respect to a $P_{Zp}$ prior distribution.
To align the $P_{Zp}$ distribution with the output of the text encoder, we use the **Monotonic Alignment Search (MAS)** (**Glow-TTS**[^20], **VITS**[^19]).
The stochastic duration predictor receives as input speaker embeddings, language embeddings and the duration obtained through MAS.
To generate human-like rhythms of speech, the objective of the stochastic duration predictor is a variational lower bound of the log-likelihood of the phoneme (pseudo-phoneme in our case) duration.

</td><td>

在训练时, 后验编码器接收线性频谱图和说话人嵌入作为输入, 预测隐变量 $z$.
这个隐变量和说话人嵌入作为基于 GAN 的声码器生成器的输入, 生成波形.
为了高效的端到端声码器训练, 我们随机从 $z$ 中采样固定长度的部分序列, 与 **HiFi-GAN**[^23], **GAN-TTS**[^27], **FastSpeech2**[^28], **VITS**[^19] 相同.
基于流的解码器将隐变量 $z$ 和说话人嵌入条件在 $P_{Zp}$ 先验分布上.
为了对齐 $P_{Zp}$ 分布和文本编码器输出, 我们使用 **单调对齐搜索 (MAS)** (**Glow-TTS**[^20], **VITS**[^19]).
随机时长预测器接收说话人嵌入, 语言嵌入和通过 MAS 获得的时长作为输入.
为了生成类似人类的语音韵律, 随机时长预测器的目标是对音素 (本工作为伪音素) 时长的对数似然的变分下界.

</td></tr>
<tr><td>

During inference, MAS is not used.
Instead, $P_{Zp}$ distribution is predicted by the text encoder and the duration is sampled from random noise through the inverse transformation of the stochastic duration predictor and then, converted to integer.
In this way, a latent variable $z_p$ is sampled from the distribution $P_{Zp}$.
The inverted Flow-based decoder receives as input the latent variable $z_p$ and the speaker embeddings, transforming the latent variable $z_p$ into the latent variable $z$ which is passed as input to the vocoder generator, thus obtaining the synthesized waveform.

</td><td>

在推理时, MAS 未被使用.
相反, $P_{Zp}$ 分布由文本编码器预测, 时长从随机噪声采样通过随机时长预测器的逆变换然后转换为整数.
这样, 隐变量 $z_p$ 被采样自 $P_{Zp}$ 分布.
逆向的基于流的解码器接收隐变量 $z_p$ 和说话人嵌入作为输入, 将隐变量 $z_p$ 转换为隐变量 $z$, 作为声码器生成器的输入, 获得合成波形.

</td></tr></table>

## 4·Experiments: 实验

### Speaker Encoder

<table><tr><td width="50%">

As speaker encoder, we use the H/ASP model [^29] publicly available, that was trained with the Prototypical Angular [^30] plus Softmax loss functions in the VoxCeleb 2 [^31] dataset.
This model was chosen for achieving state-of-the-art results in VoxCeleb 1 [^32] test subset.
In addition, we evaluated the model in the test subset of Multilingual LibriSpeech (MLS) [^33] using all languages.
This model reached an average Equal Error Rate (EER) of 1.967 while the speaker encoder used in the **SC-GlowTTS** paper[^04] reached an EER of 5.244.

</td><td>

</td></tr></table>

### Audio Datasets

<table><tr><td width="50%">

We investigated 3 languages, using one dataset per language to train the model.
For all datasets, pre-processing was carried out in order to have samples of similar loudness and to remove long periods of silence.
All the audios to 16Khz and applied voice activity detection (VAD) using [Webrtcvad toolkit [Github]](https://github.com/wiseman/py-webrtcvad) to trim the trailing silences.
Additionally, we normalized all audio to -27dB using the RMS-based normalization from the Python package [ffmpeg-normalize [Github]](https://github.com/slhck/ffmpeg-normalize).

</td><td>

</td></tr>
<tr><td>

**English**

VCTK [^14] dataset, which contains 44 hours of speech and 109 speakers, sampled at 48KHz.
We divided the VCTK dataset into: train, development (containing the same speakers as the train set) and test.
For the test set, we selected 11 speakers that are neither in the development nor the training set; following the proposal by [^01] and **SC-GlowTTS**[^04], we selected 1 representative from each accent totaling 7 women and 4 men (speakers 225, 234, 238, 245, 248, 261, 294, 302, 326, 335 and 347).
Furthermore, in some experiments we used the subsets \textit{train-clean-100} and \textit{train-clean-360} of the LibriTTS dataset [^34] seeking to increase the number of speakers in the training of the models.

</td><td>

</td></tr>
<tr><td>

**Portuguese**

TTS-Portuguese Corpus [^35], a single-speaker dataset of the Brazilian Portuguese language with around 10 hours of speech, sampled at 48KHz.
As the authors did not use a studio, the dataset contains ambient noise.
We used the FullSubNet model [^36] as denoiser and resampled the data to 16KHz.
For development we randomly selected 500 samples and the rest of the dataset was used for training.

</td><td>

</td></tr>
<tr><td>

**French**

fr\_FR set of the M-AILABS dataset [^37], which is based on [LibriVox [URL]](https://librivox.org/).
It consists of 2 female (104h) and 3 male speakers (71h) sampled at 16KHz.

</td><td>

</td></tr>
<tr><td>

To evaluate the zero-shot multi-speaker capabilities of our model in English, we use the 11 VCTK speakers reserved for testing.
To further test its performance outside of the VCTK domain, we select 10 speakers (5F/5M) from subset \textit{test-clean} of LibriTTS dataset [^34].
For Portuguese we select samples from 10 speakers (5F/5M) from the Multilingual LibriSpeech (MLS) [^33] dataset.
For French, no evaluation dataset was used, due to the reasons described in Section \ref{sec:results}.
Finally, for speaker adaptation experiments, to mimic a more realistic setting, we used 4 speakers from the Common Voice dataset [^38].

</td><td>

</td></tr></table>

### Experimental setup

<table><tr><td width="50%">

We carried out four training experiments with ***YourTTS***:
- System 1: using VCTK dataset (monolingual);
- System 2: using both VCTK and TTS-Portuguese datasets (bilingual);
- System 3: using VCTK, TTS-Portuguese and M-AILABS french datasets (trilingual);
- System 4: starting with the model obtained in experiment 3 we continue training with 1151 additional English speakers from both LibriTTS partitions \textit{train-clean-100} and \textit{train-clean-360}.

</td><td>

</td></tr>
<tr><td>

To accelerate training, in every experiment, we use transfer learning.
In experiment 1, we start from a model trained 1M steps on LJSpeech [^39] and continue the training for 200K steps with the VCTK dataset.
However, due to the proposed changes, some layers of the model were randomly initialized due to the incompatibility of the shape of the weights.
For experiments 2 and 3, training is done by continuing from the previous experiment for approximately 140k steps, learning one language at a time.
In addition, for each of the experiments a fine-tuning was performed for 50k steps using the Speaker Consistency Loss (SCL), described in section \ref{sec:TTSModel}, with $\alpha = 9$.
Finally, for experiment 4, we continue training from the model from experiment 3 fine-tuned with the Speaker Consistency Loss.
Note that, although the latest works in ZS-TTS [^02], [^03], **SC-GlowTTS**[^04] only use the VCTK dataset, this dataset has a limited number of speakers (109) and little variety of recording conditions.
Thus, after training with VCTK only, in general, ZS-TTS models do not generalize satisfactorily to new speakers where recording conditions or voice characteristics are very different than those seen in the training [^13].

The models were trained using an NVIDIA TESLA V100 32GB with a batch size of 64.
For the TTS model training and for the discrimination of vocoder HiFi-GAN we use the AdamW optimizer [^40] with betas 0.8 and 0.99, weight decay 0.01 and an initial learning rate of 0.0002 decaying exponentially by a gamma of 0.999875[^41].
For the multilingual experiments, we use weighted random sampling [^41] to guarantee a language balanced batch.

</td><td>

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

In this paper, we evaluate synthesized speech quality using a Mean Opinion Score (MOS) study, as in [^42].
To compare the similarity between the synthesized voice and the original speaker, we calculate the Speaker Encoder Cosine Similarity (SECS) (**SC-GlowTTS**[^04]) between the speaker embeddings of two audios extracted from the speaker encoder.
It ranges from -1 to 1, and a larger value indicates a stronger similarity [^02].
Following previous works [^03], **SC-GlowTTS**[^04], we compute SECS using the speaker encoder of the Resemblyzer [^43] package, allowing for comparison with those studies.
We also report the Similarity MOS (Sim-MOS) following the works of [^01], [^03], and **SC-GlowTTS**[^04].

</td><td>

</td></tr>
<tr><td>

Although the experiments involve 3 languages, due to the high cost of the MOS metrics, only two languages were used to compute such metrics: English, which has the largest number of speakers, and Portuguese, which has the smallest number.
In addition, following the work of **SC-GlowTTS**[^04] we present such metrics only for speakers unseen during training.
MOS scores were obtained with [rigorous crowdsourcing [URL]](https://www.definedcrowd.com/evaluation-of-experience/).
For the calculation of MOS and the Sim-MOS in the English language, we use 276 and 200 native English contributors, respectively.
For the Portuguese language, we use 90 native Portuguese contributors for both metrics.

</td><td>

</td></tr>
<tr><td>

During evaluation we use the fifth sentence of the VCTK dataset (i.e., speakerID\_005.txt) as reference audio for the extraction of speaker embeddings, since all test speakers uttered it and because it is a long sentence (20 words).
For the LibriTTS and MLS Portuguese, we randomly draw one sample per speaker considering only those with 5 seconds or more, to guarantee a reference with sufficient duration.
For the calculation of MOS, SECS, and Sim-MOS in English, we select 55 sentences randomly from the \textit{test-clean} subset of the LibriTTS dataset, considering only sentences with more than 20 words.
For Portuguese we use the translation of these 55 sentences.
During inference, we synthesize 5 sentences per speaker in order to ensure coverage of all speakers and a good number of sentences.
As ground truth for all test subsets, we randomly select 5 audios for each of the test speakers.
For the SECS and Sim-MOS ground truth, we compared such randomly selected 5 audios per speaker with the reference audios used for the extraction of speaker embeddings during synthesis of the test sentences.

</td><td>

</td></tr>
<tr><td>

Table \ref{tab:results} shows MOS and Sim-MOS with 95\% confidence intervals and SECS for all of our experiments in English for the datasets VCTK and LibriTTS and in Portuguese with the Portuguese sub-set of the dataset MLS.

</td></tr></table>

### VCTK Dataset

<table><tr><td width="50%">

For the VCTK dataset, the best similarity results were obtained with experiments 1 (monolingual) and 2 + SCL (bilingual).
Both achieved the same SECS and a similar Sim-MOS.
According to the Sim-MOS, the use of SCL did not bring any improvements; however, the confidence intervals of all experiments overlap, making this analysis inconclusive.
On the other hand, according to SECS, using SCL improved the similarity in 2 out of 3 experiments.
Also, for experiment 2, both metrics agree on the positive effect of SCL in similarity.

Another noteworthy result is that SECS for all of our experiments on the VCTK dataset are higher than the ground truth.
This can be explained by characteristics of the VCTK dataset itself which has, for example, significant breathing sounds in most audios.
The speaker encoder may not be able to handle these features, hereby lowering the SECS of the ground truth.
Overall, in our best experiments with VCTK, the similarity (SECS and Sim-MOS) and quality (MOS) results are similar to the ground truth.
Our results in terms of MOS match the ones reported by the **VITS** article[^19].
However, we show that with our modifications, the model manages to maintain good quality and similarity for unseen speakers.
Finally, our best experiments achieve superior results in similarity and quality when compared to [^03], **SC-GlowTTS**[^04]; therefore, achieving the SOTA in the VCTK dataset for zero-shot multi-speaker TTS.

</td><td>

</td></tr></table>

### LibriTTS Dataset

<table><tr><td width="50%">

We achieved the best LibriTTS similarity in experiment 4.
This result can be explained by the use of more speakers ($\sim1.2$k) than any other experiments ensuring a broader coverage of voice and recording condition diversity.
On the other hand, MOS achieved the best result for the monolingual case.
We believe that this was mainly due to the quality of the training datasets.
Experiment 1 uses VCTK dataset only, which has higher quality when compared to other datasets added in the other experiments.

</td><td>

</td></tr></table>

### Portuguese MLS Dataset

<table><tr><td width="50%">

For the Portuguese MLS dataset, the highest MOS metric was achieved by experiment 3+SCL, with MOS 4.11$\pm$0.07, although the confidence intervals overlap with the other experiments.
It is interesting to observe that the model trained in Portuguese with a single-speaker dataset of medium quality, manages to reach a good quality in the zero-shot multi-speaker synthesis.
Experiment 3 is the best experiment according to Sim-MOS (3.19$\pm$0.10) however, with an overlap with other ones considering the confidence intervals.
In this dataset, Sim-MOS and SECS do not agree: based on the SECS metric, the model with higher similarity was obtained in experiment 4+SCL.
We believe this is due to the variety in the LibriTTS dataset.
The dataset is also composed of audiobooks, therefore tending to have similar recording characteristics and prosody to the MLS dataset.
We believe that this difference between SECS and Sim-MOS can be explained by the confidence intervals of Sim-MOS.
Finally, Sim-MOS achieved in this dataset is relevant, considering that our model was trained with only one male speaker in the Portuguese language.

Analyzing the metrics by \textbf{gender}, the MOS for experiment 4 considering only male and female speakers are respectively 4.14 $\pm$ 0.11 and 3.79 $\pm$ 0.12.
Also, the Sim-MOS for male and female speakers are respectively 3.29 $\pm$ 0.14 and 2.84 $\pm$ 0.14.
Therefore, the performance of our model in Portuguese is affected by gender.
We believe that this happened because our model was not trained with female Portuguese speakers.
Despite that, our model was able to produce female speech in the Portuguese language.
The Attentron model achieved a Sim-MOS of 3.30$\pm$0.06 after being trained with approximately 100 speakers in the English language.
Considering confidence intervals, our model achieved a similar Sim-MOS even when seeing only one male speaker in the target language.
Hence, we believe that our approach can be the solution for the development of zero-shot multi-speaker TTS models in low-resourced languages.

Including \textbf{French} (i.e., experiment 3) appear to have improved both quality and similarity (according to SECS) in Portuguese.
The increase in quality can be explained by the fact that the M-AILABS French dataset has better quality than the Portuguese corpus; consequently, as the batch is balanced by language, there is a decrease in the amount of lower quality speech in the batch during model training.
Also, increase in similarity can be explained by the fact that TTS-Portuguese is a single speaker dataset and with the batch balancing by language in experiment 2, half of the batch is composed of only one male speaker.
When French is added, then only a third of the batch will be composed of the Portuguese speaker voice.

</td><td>

</td></tr></table>

### Speaker Consistency Loss

<table><tr><td width="50%">

The use of Speaker Consistency Loss (SCL) improved similarity measured by SECS.
On the other hand, for the Sim-MOS the confidence intervals between the experiments are inconclusive to assert that the SCL improves similarity.
Nevertheless, we believe that SCL can help the generalization in recording characteristics not seen in training.
For example, in experiment 1, the model did not see the recording characteristics of the LibriTTS dataset in training but during testing on this dataset, both the SECS and Sim-MOS metrics showed an improvement in similarity thanks to SCL.
On the other hand, it seems that using SCL slightly decreases the quality of generated audio.
We believe this is because with the use of SCL, our model learns to generate recording characteristics present in the reference audio, producing more distortion and noise.
However, it should be noted that in our tests with high-quality reference samples, the model is able to generate high-quality speech.

</td><td>

</td></tr></table>

### Zero-Shot Voice Conversion

<table><tr><td width="50%">

As in the **SC-GlowTTS**[^04] model, we do not provide any information about the speaker’s identity to the encoder, so the distribution predicted by the encoder is forced to be speaker independent.
Therefore, ***YourTTS*** can convert voices using the model’s Posterior Encoder, decoder and the HiFi-GAN Generator.
Since we conditioned ***YourTTS*** with external speaker embeddings, it enables our model to mimic the voice of unseen speakers in a zero-shot voice conversion setting.

In [^44], the authors reported the MOS and Sim-MOS metrics for the AutoVC [^45] and NoiseVC [^44] models for 10 VCTK speakers not seen during training.
To compare our results, we selected 8 speakers (4M/4F) from the VCTK test subset.
Although [^44] uses 10 speakers, due to gender balance, we were forced to use only 8 speakers.

Furthermore, to analyze the generalization of the model for the Portuguese language, and to verify the result achieved by our model in a language where the model was trained with only one speaker, we used the 8 speakers (4M/4F) from the test subset of the MLS Portuguese dataset.
Therefore, in both languages we use speakers not seen in the training.
Following [^45] for a deeper analysis, we compared the transfer between male, female and mixed gender speakers individually.
During the analysis, for each speaker, we generate a transfer in the voice of each of the other speakers, choosing the reference samples randomly, considering only samples longer than 3 seconds.
In addition, we analyzed voice transfer between English and Portuguese speakers.
We calculate the MOS and the Sim-MOS as described in Section \ref{sec:results}.
However, for the calculation of the sim-MOS when transferring between English and Portuguese (pt-en and en-pt), as the reference samples are in one language and the transfer is done in another language, we used evaluators from both languages (58 and 40, respectively, for English and Portuguese).

Table \ref{tb:vc} presents the MOS and Sim-MOS for these experiments.
Samples of the zero-shot voice conversion are present in the [demo page](https://edresson.github.io/YourTTS/).

</td><td>

</td></tr></table>

#### Intra-Lingual Results

<table><tr><td width="50%">

For zero-shot voice conversion from one English-speaker to another English-speaker (en-en) our model achieved a MOS of 4.20$\pm$0.05 and a Sim-MOS of 4.07$\pm$0.06.

For comparison in [^44] the authors reported the MOS and Sim-MOS results for the AutoVC [^45] and NoiseVC [^44] models.
For 10 VCTK speakers not seen during training, the AutoVC model achieved a MOS of $3.54\pm 1.08$  and a Sim-MOS of $1.91\pm 1.34$.

Note: The authors presented the results in a graph without the actual figures, so the MOS scores reported here are approximations calculated considering the length in pixels of those graphs.

On the other hand, the NoiseVC model achieved a MOS of $3.38\pm 1.35$ and a Sim-MOS of $3.05\pm 1.25$.
Therefore, our model achieved results comparable to the SOTA in zero-shot voice conversion in the VCTK dataset.
Alhtough the model was trained with more data and speakers, the similarity results of the VCTK dataset in Section \ref{sec:results} indicate that the model trained with only the VCTK dataset (experiment 1) presents a better similarity than the model explored in this Section (experiment 4).
Therefore, we believe that ***YourTTS*** can achieve a result very similar or even superior in zero-shot voice conversion when being trained and evaluated using only the VCTK dataset.

For zero-shot voice conversion from one Portuguese speaker to another Portuguese speaker our model achieved a MOS of 3.64 $\pm$ 0.09 and a Sim-MOS of 3.43 $\pm$ 0.09.

We note that our model performs significantly worse in voice transfer similarity between female speakers (3.35 $\pm$ 0.19) compared to transfers between male speakers (3.80 $\pm$ 0.15).
This can be explained by the lack of female speakers for the Portuguese language during the training of our model.
Again, it is remarkable that our model manages to approximate female voices in Portuguese without ever having seen a female voice in that language.

</td><td>

</td></tr></table>

#### Cross-Lingual Results

<table><tr><td width="50%">

Apparently, the transfer between English and Portuguese speakers works as well as the transfer between Portuguese speakers.
However, for the transfer of a Portuguese speaker to an English speaker (pt-en) the MOS scores drop in quality.
This was especially due to the low quality of voice conversion from Portuguese male speakers to English female speakers.
In general, as discussed above, due to the lack of female speakers in the training of the model, the transfer to female speakers achieves poor results.
In this case, the challenge is even greater as it is necessary to convert audios from a male speaker in Portuguese to the voice of a English female speaker.

In English, during conversions, the speaker's gender did not significantly influence the model's performance.
However, for transfers involving Portuguese, the absence of female voices in the training of the model hindered generalization.

</td><td>

</td></tr></table>

### Speaker Adaptation

<table><tr><td width="50%">

The different recording conditions are a challenge for the generalization of the zero-shot multi-speaker TTS models.
Speakers who have a voice that differs greatly from those seen in training also become a challenge [^13].
Nevertheless, to show the potential of our model for adaptation to new speakers/recording conditions, we selected samples from 20 to 61 seconds of speech for 2 Portuguese and 2 English speakers (1M/1F) in the Common Voice [^38] dataset.
Using these 4 speakers, we perform fine-tuning on the checkpoint from experiment 4 with Speaker Consistency Loss individually for each speaker.

During fine-tuning, to ensure that multilingual synthesis is not impaired, we use all the datasets used in experiment 4.
However, we use Weighted random sampling [^41] to guarantee that samples from adapted speakers appear in a quarter of the batch.
The model is trained that way for 1500 steps.
For evaluation, we use the same approach described in Section \ref{sec:results}.

Table \ref{tb:spk_adpt} shows the gender, total duration in seconds and number of samples used during the training for each speaker, and the metrics SECS, MOS and Sim-MOS for the ground truth (GT), zero-shot multi-speaker TTS mode (ZS), and the fine-tuning (FT) with speaker samples.

In general, our model's fine-tuning with less than 1 minute of speech from speakers who have recording characteristics not seen during training achieved very promising results, significantly improving similarity in all experiments.

In English, the results of our model in zero-shot multi-speaker TTS mode are already good and after fine-tuning both male and female speakers achieved Sim-MOS comparable to the ground truth.
The fine-tuned model achieves greater SECS than the ground truth, which was already observed in previous experiments.
We believe that this phenomenon can be explained by the model learning to copy the recording characteristics and reference sample's distortions, giving an advantage over other real speaker samples.

In Portuguese, compared to zero-shot, fine-tuning seems to trade a bit of naturalness for a much better similarity.
For the male speaker, the Sim-MOS increased from 3.35$\pm$0.12 to 4.19$\pm$0.07 after fine-tuning with just 31 seconds of speech for that speaker.
For the female speaker, the similarity improvement was even more impressive, going from 2.77$\pm$0.15 in zero-shot mode to 4.43$\pm$0.06 after the fine-tuning with just 20 seconds of speech from that speaker.

Although our model manages to achieve high similarity using only seconds of the target speaker's speech, Table \ref{tb:spk_adpt} seems to presents a direct relationship between the amount of speech used and the naturalness of speech (MOS).
With approximately 1 minute of speech in the speaker's voice our model can copy the speaker's speech characteristics, even increasing the naturalness compared to zero-shot mode.
On the other hand, using 44 seconds or less of speech reduces the quality/naturalness of the generated speech when compared to the zero-shot or ground truth model.
Therefore, although our model shows good results in copying the speaker's speech characteristics using only 20 seconds of speech, more than 45 seconds of speech are more adequate to allow higher quality.
Finally, we also noticed that voice conversion improves significantly after fine-tuning the model, mainly in Portuguese and French where few speakers are used in training.

</td><td>

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

In this work, we presented ***YourTTS***, which achieved SOTA results in zero-shot multi-speaker TTS and zero-shot voice conversion in the VCTK dataset.
Furthermore, we show that our model can achieve promising results in a target language using only a single speaker dataset.
Additionally, we show that for speakers who have both a voice and recording conditions that differ greatly from those seen in training, our model can be adjusted to a new voice using less than 1 minute of speech.

However, our model exhibits some limitations.
For the TTS experiments in all languages, our model presents instability in the stochastic duration predictor which, for some speakers and sentences, generates unnatural durations.
We also note that mispronunciations occur for some words, especially in Portuguese.
Unlike [^35], [^46], **VITS**[^19], we do not use phonetic transcriptions, making our model more prone to such problems.
For Portuguese voice conversion, the speaker's gender significantly influences the model's performance, due to the absence of female voices in training.
For Speaker Adaptation, although our model shows good results in copying the speaker's speech characteristics using only 20 seconds of speech, more than 45 seconds of speech are more adequate to allow higher quality.

In future work, we intend to seek improvements to the duration predictor of the ***YourTTS*** model as well as training in more languages.
Furthermore, we intend to explore the application of this model for data augmentation in the training of automatic speech recognition models in low-resource settings.

</td><td>

</td></tr></table>

## References: 参考文献

[^01]: jia2018transfer
[^02]: cooper2020zero
[^03]: choi2020attentron
[^04]: [SC-GlowTTS](../Flow/2021.04.02_SC-GlowTTS.md)
[^05]: arik2018neural
[^06]: [DeepVoice3](../Acoustic/2017.10.20_DeepVoice3.md)
[^07]: [Tacotron2](../Acoustic/2017.12.16_Tacotron2.md)
[^08]: ge2e
[^09]: cai2018exploring
[^10]: snyder2018x
[^11]: kumar21c_interspeech
[^12]: baevski2020wav2vec
[^13]: [**Survey by Tan et al.(2021)**](../../Surveys/2021.06.29__Survey__A_Survey_on_Neural_Speech_Synthesis_(63P).md)
[^14]: veaux2016superseded
[^15]: cao2019end
[^16]: zhang2019learning
[^17]: nekvinda2020one
[^18]: li2021light
[^19]: [VITS](2021.06.11_VITS.md)
[^20]: [Glow-TTS](../Acoustic/2020.05.22_Glow-TTS.md)
[^21]: [RealNVP](../_Basis/2016.05.27_RealNVP.md)
[^22]: [WaveNet](../Vocoder/2016.09.12_WaveNet.md)
[^23]: [HiFi-GAN](../Vocoder/2020.10.12_HiFi-GAN.md)
[^24]: kingma2013auto
[^25]: [WaveGlow](../Vocoder/2018.10.31_WaveGlow.md)
[^26]: xin21_interspeech
[^27]: [GAN-TTS](../Vocoder/2019.09.25_GAN-TTS.md)
[^28]: [FastSpeech2](../Acoustic/2020.06.08_FastSpeech2.md)
[^29]: heo2020clova
[^30]: chung2020in
[^31]: chung2018voxceleb2
[^32]: nagrani2017voxceleb
[^33]: PratapXSSC20
[^34]: zen2019libritts
[^35]: casanova2020ttsportuguese
[^36]: Hao_2021
[^37]: mailabs
[^38]: ardila2020common
[^39]: ito2017lj
[^40]: loshchilov2017decoupled
[^41]: paszke2017automatic
[^42]: mos
[^43]: Jemine2019Master
[^44]: wang2021noisevc
[^45]: qian2019autovc
[^46]: casanova2020end
