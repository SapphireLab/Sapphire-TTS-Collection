# OpenVoice

<details>
<summary>基本信息</summary>

- 标题: OpenVoice: Versatile Instant Voice Cloning
- 作者:
  1. Zengyi Qin
  2. Wenliang Zhao
  3. Xumin Yu
  4. Xin Sun
- 机构:
  1. MIT
  2. MyShell.ai
  3. 清华大学
- 时间:
  - 预印时间: 2023.12.03 ArXiv v1
  - 预印时间: 2023.12.13 ArXiv v2
  - 预印时间: 2023.12.16 ArXiv v3
  - 预印时间: 2023.12.21 ArXiv v4
  - 预印时间: 2024.01.02 ArXiv v5 (基于此版本)
  - 预印时间: 2024.08.18 ArXiv v6
  - 更新笔记: 2024.09.16
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2312.01479)
  - [DOI]()
  - [Github](https://github.com/myshell-ai/OpenVoice)
  - [Demo](https://research.myshell.ai/open-voice)
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: 7
- 引用: ?
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

We introduce **OpenVoice**, a versatile instant voice cloning approach that requires only a short audio clip from the reference speaker to replicate their voice and generate speech in multiple languages. **OpenVoice** represents a significant advancement in addressing the following open challenges in the field: 
1. **Flexible Voice Style Control**. **OpenVoice** enables granular control over voice styles, including emotion,accent, rhythm, pauses, and intonation, in addition to replicating the tone color of the reference speaker. The voice styles are not directly copied from and constrained by the style of the reference speaker. Previous approaches lacked the ability to flexibly manipulate voice styles after cloning. 
2. **Zero-Shot Cross-Lingual Voice Cloning**. **OpenVoice** achieves zero-shot cross-lingual voice cloning for languages not included in the massive-speaker training set. Unlike previous approaches,which typically require extensive massive-speaker multi-lingual (MSML) dataset 2 for all languages, **OpenVoice** can clone voices into a new language without any massive-speaker training data for that language. 

**OpenVoice** is also computationally efficient, costing tens of times less than commercially available APIs that offer even inferior performance. To foster further research in the field, we have made the source code3and trained model publicly accessible. We also provide qualitative results in our demo website4. Prior to its public release, our internal version of **OpenVoice** was used tens of millions of times by users worldwide between May and October 2023, serving as the backend of MyShell.ai.

## 1.Introduction: 引言

**Instant voice cloning (IVC)** in **text-to-speech (TTS)** synthesis means the TTS model can clone the voice of any reference speaker given a short audio sample without additional training on the reference speaker.
It is also referred to as Zero-shot TTS.
IVC enables the users to flexibly customize the generated voice and exhibits tremendous value in a wide variety of real-world applications, such as media content creation, customized chatbots, and multi-modal interaction between humans and computers or large language models.

An abundant of previous work has been done in IVC.
Examples of auto-regressive approaches include [2023.01.05_VALL-E](../Speech_LLM/2023.01.05_VALL-E.md) and [XTTS (2023)](), which extract the acoustic tokens or speaker embedding from the reference audio as a condition for the auto-regressive model.
Then the auto-regressive model sequentially generate acoustic tokens, which are then decoded to raw audio waveform.
While these methods can clone the tone color, they do not allow users to flexibly manipulate other important style parameters such as emotion, accent, rhythm, pauses and intonation.
Also, auto-regressive models are relatively computationally expensive and has relatively slow inference speed.
Examples of non-autoregressive approach include [2021.12.04_YourTTS](2021.12.04_YourTTS.md) and the recently developed [2023.06.23_VoiceBox](../Speech_LLM/2023.06.23_VoiceBox.md), which demonstrate significantly faster inference speed but are still unable to provide flexible control over style parameters besides tone color.
Another common disadvantage of the existing methods is that they typically require a huge MSML dataset in order to achieve cross-lingual voice clone.
Such combinatorial data requirement can limit their flexibility to include new languages.
In addition, since the voice cloning research [8,16] by tech giants are mostly closed-source, there is not a convenient way for the research community to step on their shoulders and push the field forward.

We present **OpenVoice**, a flexible instant voice cloning approach targeted at the following key problems in the field:
- In addition to cloning the tone color, how to have flexible control of other important style parameters such as emotion, accent, rhythm, pauses and intonation? These features are crucial for generating in-context natural speech and conversations, rather than monotonously narrating the input text.
Previous approaches [2,3,16] can only clone the monotonous tone color and style from the reference speaker but do not allow flexible manipulation of styles.
- How to enable zero-shot cross-lingual voice cloning in a simple way.
We put forward two aspects of zero-shot capabilities that are important but not solved by previous studies:
  - If the language of the reference speaker is not presented in the MSML dataset, can the model clone their voice?
  - If the language of the generated speech is not presented in the MSML dataset, can the model clone the reference voice and generate speech in that language?

  In previous studies [18,8], the language of the reference speaker and the generated language by the model should both exist in great quantity in the MSML dataset.
But what if neither of them exist?
- How to realize super-fast speed real-time inference without downgrading the quality, which is crucial for massive commercial production environment.

To address the first two problems, **OpenVoice** is designed to decouple the components in a voice as much as possible.
The generation of language, tone color, and other important voice features are made independent of each other, enabling flexible manipulation over individual voice styles and language types.
This is achieved without labeling any voice style in the MSML training set.
We would like to clarify that **the zero-shot cross-lingual task in this study is different from that in VALLE-X** [18].
In VALLE-X, data for all languages need to be included in the MSML training set, and the model cannot generalize to an unseen language outside the MSML training set.
By comparison, **OpenVoice** is designed to generalize to completely unseen languages outside the MSML training set.
The third problem is addressed by default, since the decoupled structure reduces requirement on model size and computational complexity.
We do not require a large model to learn everything.
Also, we avoid auto-regressive or diffusion components to speed up the inference.

Our internal version of **OpenVoice** before this public release has been used tens of millions of times by users worldwide between May and October 2023.
It powers the instant voice cloning backend of MyShell.ai and has witnessed several hundredfold user growth on this platform.
To facilitate the research progress in the field, we explain the technology in great details and make the source code with model weights publicly available.

## 2.Related Works: 相关工作

## 3.Methodology: 方法

The technical approach is simple to implement but surprisingly effective.
We first present the intuition behind **OpenVoice**, then elaborate on the model structure and training.

这种技术方法易于实施但很有效果.
我们首先展示算法背后的直觉, 然后详细阐述模型结构和训练.

### 3.1.Intuition

The Hard.
It is obvious that simultaneously cloning the tone color for any speaker, enabling flexible control of all other styles, and adding new language with little effort could be very challenging.
It requires a huge amount of combinatorial datasets where the controlled parameters intersect, and pairs of data that only differ in one attribute, and are well-labeled, as well as a relatively large-capacity model to fit the dataset.

难处: 显然同时克隆任意说话人的音色, 实现对其他风格的灵活控制, 且只需少许努力就能添加新语言是非常具有挑战性的. 这需要大量的组合数据集, 其中受控参数相交, 且成对数据只有一个属性不同, 且标记良好, 以及需要一个相对大容量的模型来拟合数据集.

The Easy.
We also notice that in regular single-speaker TTS, as long as voice cloning is not required, it is relatively easy to add control over other style parameters and add a new language.
For example, recording a single-speaker dataset with 10K short audio samples with labeled emotions and intonation is sufficient to train a single-speaker TTS model that provides control over emotion and intonation.
Adding a new language or accent is also straightforward by including another speaker in the dataset.

易处: 在常规的单说话人语音识别中,只要不要求声音克隆, 添加对其他风格参数的控制和添加一种新的语言相对容易. 例如用带有情绪和语调标签的 10K 短音频样本记录一个单说话人数据集足以训练一个能提供情感和语调控制的单说话人 TTS 模型. 可以通过在数据集中添加其他说话人来添加一种新语言或口音.

The intuition behind **OpenVoice** is to decouple the IVC task into separate subtasks where every subtask is much easier to achieve compared to the coupled task.
The cloning of tone color is fully decoupled from the control over all remaining style parameters and languages.
We propose to use a base speaker TTS model to control the style parameters and languages, and use a tone color converter to embody the reference tone color into the generated voice.

OpenVoice 背后的直觉是将 IVC 任务解耦为单独的子任务, 每个子任务都要更容易完成. 音色克隆和其他的风格参数, 语言控制解耦. 建议使用一个基笨的说话人 TTS 模型以控制风格参数和语言, 并使用音色转换器将参考音色体现到生成的语音中.

### 3.2.模型结构

We illustrate the model structure in [2023.12.03_OpenVoice](#FIG01).
The two main components of **OpenVoice** are the base speaker TTS model and the tone color converter.
The base speaker TTS model is a single-speaker or multi-speaker model, which allows control over the style parameters (e.g., emotion, accent, rhythm, pauses and intonation), accent and language.
The voice generated by this model is passed to the tone color converter, which changes the tone color of the base speaker into that of the reference speaker.

![](../../2023.12.03_OpenVoice_Fig.01.png)

OpenVoice 的两个主要部分是基础说话人 TTS 模型和音色转换器.
基础说话人 TTS 模型是一个允许风格参数 (情感, 口音, 节奏, 停顿和语调), 口音和语言控制的单说话人/多说话人模型. 由此模型生成的声音传递给音色转换器, 将基础说话人的音色转化为参考说话人.

**Base Speaker TTS Model**.
The choice of the base speaker TTS model is very flexible.
For example,the VITS [6] model can be modified to accept style and language embedding in its text encoder and duration predictor.
Other choices such as InstructTTS [17] can also accept style prompts.
It is also possible to use commercially available (and cheap) models such as Microsoft TTS, which accepts speech synthesis markup language (SSML) that specifies the emotion, pauses and articulation.
One can even skip the base speaker TTS model, and read the text by themselves in whatever styles and languages they desire.
In our **OpenVoice** implementation, we used the VITS [6] model by default, but other choices are completely feasible.
We denote the outputs of the base model as $\mathbf{X}(L_I, S_I, C_I)$, where the three parameters represent the language, styles and tone color respectively.
Similarly, the speech audio from the reference speaker is denoted as $\mathbf{X}(L_O, S_O, C_O)$.

基础说话人语音合成模型: 这一模型的选择十分灵活.
- VITS 模型的文本编码器和时长预测器可以修改为接受风格和语言嵌入.
- 其他选择如 InstructTTS 同样接受风格提示.
- 还可以使用商业可用且便宜的模型如 Microsoft TTS, 接受指定情绪, 停顿和发音的 SSML.
- 甚至可以跳过基本的说话人 TTS 模型, 并自行阅读那些他们想要的任意风格和语言的文本.
- 在我们的实现中, 我们使用 VITS 模型作为默认, 但其他选择也完全可行.
- 我们将基础模型的输出记为 $\mathbf{X}(L_I, S_I, C_I)$, 其中三个参数分别表示语言, 风格和音色.
- 类似地, 参考说话人的语音音频表示为 $\mathbf{X}(L_O, S_O, C_O)$.

**Tone Color Converter**.

音色转换器

The tone color converter is an encoder-decoder structure with a invertible normalizing flow [12] in the middle.
The encoder is an 1D convolutional neural network that takes the short-time Fourier transformed spectrum of $\mathbf{X}(L_I, S_I, C_I)$ as input.
All convolutions are single-strided.
The feature maps outputted by the encoder are denoted as $\mathbf{Y}(L_I, S_I, C_I)$.
The tone color extractor is a simple 2D convolutional neural network that operates on the mel-spectrogram of the input voice and outputs a single feature vector that encodes the tone color information.
We apply it on $\mathbf{X}(L_I, S_I, C_I)$ to obtain vector $v(C_I)$, then apply it on $\mathbf{X}(L_O, S_O, C_O)$ to obtain vector $v(C_O)$.

音色转换器是一种中间具有可逆规范化流的编码器-解码器结构.
编码器是一维卷积神经网络, 接受短时傅里叶变换谱作为输入, 所有的卷积是单步的. 由编码器输出的特征图记为 $Y(L_I, S_I, C_I)$.
音色提取器是一个简单的二维卷积神经网络, 它对输入语音的梅尔频谱进行操作, 并输出一个编码音色信息的单个特征向量.
我们将之应用到基础说话人模型的输出和参考说话人音频的输入上获得相应向量

The normalizing flow layers take $\mathbf{Y}(L_I, S_I, C_I)$ and $v(C_I)$ as input and outputs a feature representation $\mathbf{Z}(L_I, S_I)$ that eliminates the tone color information but preserves all remaining style properties.
The feature $\mathbf{Z}(L_I, S_I)$ is aligned with International Phonetic Alphabet (IPA) [1] along the time dimension.
Details about how such feature representation is learned will be explained in the next section.
Then we apply the normalizing flow layers in the inverse direction, which takes $\mathbf{Z}(L_I, S_I)$ and $v(C_O)$ as input and outputs $\mathbf{Y}(L_I, S_I, C_O)$.
This is a critical step where the tone colorCOfrom the reference speaker is embodied into the feature maps.
Then the $\mathbf{Y}(L_I, S_I, C_O)$ is decoded into raw waveforms $\mathbf{X}(L_I, S_I, C_O)$ by HiFi-Gan [7] that contains a stack of transposed 1D convolutions.
The entire model in our **OpenVoice** implementation is feed-forward without any auto-regressive component.
The tone color converter is conceptually similar to voice conversion [14,11], but with different emphasis on its functionality, inductive bias on its model structure and training objectives.
The flow layers in the tone color converter are structurally similar to the flow-based TTS methods [6,5] but with different functionalities and training objectives.

规范化流层将 $\mathbf{Y}(L_I, S_I, C_I)$ 和 $v(C_I)$ 作为输入, 并输出一个特征表示 $\mathbf{Z}(L_I, S_I)$, 该表示消除了音色信息，但保留了所有剩余的风格属性. 
特征 $\mathbf{Z}(L_I, S_I)$ 沿时间维度与国际音标对齐. 关于如何学习这样的特征表示的详细信息将在下一节中解释.
然后我们以反方向应用规范化流层，它将 $\mathbf{Z}(L_I, S_I)$ 和 $v(C_O)$ 作为输入，并输出 $\mathbf{Y}(L_I, S_I, C_O)$. 这是关键步骤，其中参考说话人的音色 $C_O$ 被嵌入到特征图中.
然后 $\mathbf{Y}(L_I, S_I, C_O)$ 被 HiFi-GAN 解码为原始波形 $\mathbf{X}(L_I, S_I, C_O)$, 其中包含一系列转置的一维卷积.
**OpenVoice**实现中的整个模型是前馈的, 没有任何自回归组件. 音色转换器在功能, 模型结构上的归纳偏差和训练目标上与语音转换概念上相似, 但强调功能性不同.
音色转换器中的流层在结构上与基于流的 TTS 方法相似, 但具有不同的功能和训练目标.

**Alternative Ways and Drawbacks**.
Although there are alternative ways [4,9,14] to extract $\mathbf{Z}(L_I, S_I)$, we empirically found that the proposed approach achieves the best audio quality.
One can use HuBERT [4] to extract discrete or continuous acoustic units [14] to eliminate tone color information, but we found that such method also eliminates emotion and accent from the input speech.
When the input is an unseen language, this type of method also has issues preserving the natural pronunciation of the phonemes.
We also studied another approach [9] that carefully constructs information bottleneck to only preserve speech content, but we observed that this method is unable to completely eliminate the tone color.

**替代方法和缺点**

尽管有其他方法来提取 $\mathbf{Z}(L_I, S_I)$, 但我们经验上发现所提方法实现了最佳的音频质量.
可以使用 HuBERT 来提取离散或连续的声学单元以消除音色信息, 但我们发现这种方法也会消除输入音频的情感和口音. 当输入是没见过的语言时, 这类方法在保留音素的自然发音方面也存在问题.
我们同样研究了另一种方法, 仔细构造信息瓶颈仅保留语音内容, 但我们观察到该方法无法完全消除音色.

**Remark on Novelty**.
**OpenVoice** does not intend to invent the submodules in the model structure.
Both the base speaker TTS model and the tone color converter borrow the model structure from existing work [5,6].
The contribution of **OpenVoice** is the decoupled framework that seperates the voice style and language control from the tone color cloning.
This is very simple, but very effective, especially when one wants to control styles, accents or generalize to new languages.
If one wanted to have the same control on a coupled framework such as XTTS [3], it could require a tremendous amount of data and computing, and it is relatively hard to fluently speak every language.
In **OpenVoice**, as long as the single-speaker TTS speaks fluently, the cloned voice will be fluent.
Decoupling the generation of voice styles and language from the generation of tone color is the core philosophy of **OpenVoice**.
We also provided our insights of using flow layers in tone color converter, and the importance of choosing a universal phoneme system in language generalization in our experiment section.

**创新型说明**
OpenVoice 不打算在模型结构中创建子模块.
其基础说话人 TTS 模型和音色转换器都借用了现有工作的模型结构.
OpenVoice 的贡献是从音色克隆中分离语音风格与语言控制的解耦框架.
这非常简单, 但非常有效, 尤其是在控制风格, 口音或泛化到新语言时.
如果需要在耦合框架如 XTTS 中拥有相同的控制, 将需要大量的数据和计算, 而且相对难以流畅地讲每种语言.
在 OpenVoice 中只要单个说话人 TTS 能流畅地说话, 那么克隆的声音也将流畅.
讲声音风格和语言从音色生成中解耦时 OpenVoice 的核心哲学.
我们同样提供了在音色转化器中使用流层的见解, 并且在实验部分中进行语言泛化时选择通用音素系统的重要性.

### 3.3.训练

In order to train the base speaker TTS model, we collected audio samples from two English speakers (American and British accents), one Chinese speaker and one Japanese speaker.
There are 30K sentences in total, and the average sentence length is 7s.
The English and Chinese data has emotion classification labels.
We modified the VITS [6] model and input the emotion categorical embedding, language categorical embedding and speaker id into the text encoder, duration predictor and flow layers.
The training follows the standard procedure provided by the authors of VITS [6].
The trained model is able to change the accent and language by switching between different base speakers, and read the input text in different emotions.
We also experimented with additional training data and confirmed that rhythm, pauses and intonation can be learned in exactly the same way as emotions.

为了训练基础说话人 TTS 模型, 我们收集了来自两位英语说话人 (美国和英国口音), 一位汉语说话人和一位日语说话人.
总计 30K 句子, 平均句子长度为 7 秒.
英语和汉语数据拥有情感分类标签.
我们修改 VITS 模型并讲情感类别嵌入, 语言类别嵌入和说话人编号输入到文本编码器, 时长预测器和流层.
训练遵循 VITS 作者提供的标准流程.
训练好的模型能够通过切换不同的基础说话人来修改口音和语言, 并以不同情感阅读输入文本.
我们同样使用额外训练数据实验并确认节奏, 停顿和语调能够和情感一样进行学习.

In order to train the tone color converter, we collected 300K audio samples from 20K individuals.
Around 180K samples are English, 60K samples are Chinese and 60K samples are Japanese.
This is what we called the MSML dataset.
The training objectives of the tone color converter is two-fold.

为了训练音色转换器, 我们收集了来自 20K 个个体的 300K 音频样本.
大约 180K 样本是英语, 60K 样本是汉语, 60K 样本是日语.
这也是为什么称之为 MSML 数据集.
音色转化器的训练目标有两个方面.

First, we require the encoder-decoder to produce natural sound.
During training, we feed the encoder output directly to the decoder, and supervised the generated waveform using the original waveform with mel-spectrogram loss and HiFi-GAN [7] loss.
We will not detail here as it has been well explained by previous literature [7, 6].

第一个方面, 要求编码器解码器能够生成自然的声音.
在训练时, 我们将编码器输出直接传递给解码器, 并对生成波形和原始波形采用梅尔频谱损失和 HiFi-GAN 损失以监督学习的方式进行训练.
此处不详细介绍, 因为有其他文献进行了详细介绍.

Second, we require flow layers to eliminate as much tone color information as possible from the audio features.
During training, for each audio sample, its text is converted to a sequence of phonemes in IPA [1], and each phoneme is represented by a learnable vector embedding.
The sequence of vector embedding is passed to a transformer [15] encoder to produce the feature representation of the text content.
Denote this feature as $\mathbf{L} \in \mathbb{R}^{c\times l}$, where $c$ is the number of feature channels and $l$ is the number of phonemes in the input text.
The audio waveform is processed by the encoder and flow layers to produce the feature representation $\mathbf{Z} \in \mathbb{R}^{c\times t}$, where $t$ is the length of the features along the time dimension.
Then we align $L$ with $Z$ along the time dimension using dynamic time warping [13, 10] (an alternative is monotonic alignment [5, 6]) to produce $\hat{\mathbf{L}} \in \mathbb{R}^{c\times t}$, and minimize the KL-divergence between $\hat{\mathbf{L}}$ an $\mathbf{Z}$.
Since $\hat{\mathbf{L}}$ does not contain any tone color information, the minimization objective would encourage the flow layers to remove tone color information from their output $\mathbf{Z}$.
The flow layers are conditioned on the tone color information from the tone color encoder, which further helps the flow layers to identify what information needs to be eliminated.
In addition, we do not provide any style or language information for the flow layers to condition on, which prevents the flow layers to eliminate information other than tone color.
Since the flow layers are invertible, conditioning them on a new piece of tone color information and running its inverse process can add the new tone color back to the feature representations, which are then decoded to the raw waveform with the new tone color embodied.

第二个方面, 要求流层能够尽可能地从音频特征中消除音色信息.
在训练时, 对于每个音频样本, 其文本转化为 IPA 音素序列, 每个音素表示为一个可学习的向量嵌入.
向量嵌入序列传递给 Transformer 编码器用于生成文本内容的特征表示.
将之记为 $\mathbf{L} \in \mathbb{R}^{c\times l}$, 其中 $c$ 为特征通道数, $l$ 为输入文本的音素数.
音频波形通过编码器和流层处理以生成特征表示 $\mathbf{Z} \in \mathbb{R}^{c\times t}$, 其中 $t$ 是特征沿时间维度的长度.
然后使用动态时间规整 (单调对齐的替代) 将 $\mathbf{L}$ 和 $\mathbf{Z}$ 沿着时间维度对齐用于产生 $\hat{\mathbf{L}}$, 并最小化 $\hat{\mathbf{L}}$ 和 $\mathbf{Z}$ 之间的 KL 散度.
因为 $\hat{\mathbf{L}}$ 不包含任何音色信息, 最小化目标将会鼓励流层从输出中移除音色信息.
流层被音色编码器的音色信息条件化, 进一步帮助流层识别哪些信息需要消除.
此外我们不为流层提供任何风格或语言信息进行条件化, 防止流层消除音色之外的信息.
因为流层可逆, 用新的音色信息片段条件化并运行逆过程可以将新的音色添加回特征表示, 然后解码为带有新的音色信息的原始波形.

## 4.Experiments: 实验

The evaluation of voice cloning is hard to be objective for several reasons.
First, different research studies (e.g., [8], [2]) usually have different training and test sets.
The numerical comparison could be intrinsically unfair.
Even their metrics such as Mean Opinion Score can be evaluated by crowd-sourcing, the diversity and difficulty of the test set would significantly influence the results.
For example, if many samples in the test set are neural voices that concentrate on the mean of human voice distributions, then it is relatively easy for most methods to achieve good voice cloning results.
Second, different studies usually have different training sets, where the scale and diversity would have considerable influence of the results.
Third, different studies can have different focus on their core functionalities.
**OpenVoice** mainly aims at tone color cloning, flexible control over style parameters,and making cross-lingual voice clone easy even without massive-speaker data for a new language.
These are different from the objectives of previous work on voice cloning or zero-shot TTS.
Therefore,instead of comparing numerical scores with existing methods, we mainly focus on analyzing the qualitative performance of **OpenVoice** itself, and make the audio samples publicly available for relevant researchers to freely evaluate.

**Accurate Tone Color Cloning**.
We build a test set of reference speakers selected from celebrities,game characters and anonymous individuals.
The test set covers a wide voice distributions including both expressive unique voices and neutral samples in human voice distribution.
With any of the 4 base speakers and any of the reference speaker, **OpenVoice** is able to accurately clone the reference tone color and generate speech in multiple languages and accents.
We invite the readers to this website5for qualitative results.

**Flexible Control on Voice Styles**.
A premise for the proposed framework to flexibly control the speech styles is that the tone color converter is able to only modify the tone color and preserves all other styles and voice properties.
In order to confirm this, we use both our base speaker model and the Microsoft TTS with SSML to generate a speech corpus of 1K samples with diverse styles (emotion,accent, rhythm, pauses and intonation) as the base voices.
After converting to the reference tone color,we observed that all styles are well-preserved.
In rare cases, the emotion will be slightly neutralized,and one way that we found to solve this problem is to replace the tone color embedding vector of this particular sentence with the average vector of multiple sentences with different emotions from the same base speaker.
This gives less emotion information to the flow layers so that they do not eliminate the emotion.
Since the tone color converter is able to preserve all the styles from the base voice, controlling the voice styles becomes very straightforward by simply manipulating the base speaker TTS model.
The qualitative results are publicly available on this website6.

**Cross-Lingual Voice Clone with Ease**.
**OpenVoice** achieves near zero-shot cross-lingual voice cloning without using any massive-speaker data for an unseen language.
It does require a base speaker of the language, which can be achieved with minimum difficulty with the off-the-shelf models and datasets.
On our website7, we provide an abundant of samples that demonstrates the cross-lingual voice clone capabilities of the proposed approach.
The cross-lingual capabilities are two-fold:•When the language of the reference speaker is unseen in the MSML dataset, the model is able to accurately clone the tone color of the reference speaker.•When the language of the generated speech is unseen in the MSML dataset, the model is able to clone the reference voice and speak in that language, as long as the base speaker TTS supports that language.


**Fast Inference with Low Cost**.
Since **OpenVoice** is a feed-forward structure without any auto-regressive component, it achieves very high inference speed.
Our experiment shows that a slightly optimized version of **OpenVoice** (including the base speaker model and the tone converter) is able achieve12×real-time performance on a single A10G GPU, which means it only takes 85ms to generate a one second speech.
Through detailed GPU usage analysis, we estimate that the upper bound is around 40× real-time, but we will leave this improvement as future work.

**Importance of IPA**.
We found that using IPA as the phoneme dictionary is crucial for the tone color converter to perform cross-lingual voice cloning.
As we detailed in Section 2.3, in training the tone color converter, the text is first converted into a sequence of phonemes in IPA, then each phoneme is represented by a learnable vector embedding.
The sequence of embedding is encoded with transformer layers and compute loss against the output of the flow layers, aiming to eliminate the tone color information.
IPA itself is a cross-lingual unified phoneme dictionary, which enables the flow layers to produce a language-neutral representation.
Even if we input a speech audio with unseen language to the tone color converter, it is still able to smoothly process the audio.
We also experimented with other types of phoneme dictionaries but the resulting tone color converter tend to mispronounce some phonemes in unseen languages.
Although the input audio is correct, there is a high likelihood that the output audio is problematic and sounds non-native.

## 5.讨论

**OpenVoice** demonstrates remarkable instance voice cloning capabilities and is more flexible than previous approaches in terms of voice styles and languages.
The intuition behind the approach is that it is relatively easy to train a base speaker TTS model to control the voice styles and languages,as long as we do not require the model to have the ability to clone the tone color of the reference speaker.
Therefore, we proposed to decouple the tone color cloning from the remaining voice styles and the language, which we believe is the foundational design principle of **OpenVoice**.
In order to facilitate future research, we make the source code and model weights publicly available.
