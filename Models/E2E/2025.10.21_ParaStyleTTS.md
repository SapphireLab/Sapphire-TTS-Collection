# ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation

<details>
<summary>基本信息</summary>

- 标题: "ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation."
- 作者:
  - 01 Haowei Lou
  - 02 Hye-Young Paik
  - 03 Wen Hu
  - 04 Lina Yao
- 链接:
  - [ArXiv](https://arxiv.org/abs/2510.18308v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2510.18308v1](PDF/2025.10.21_2510.18308v1_ParaStyleTTS__Toward_Efficient_and_Robust_Paralinguistic_Style_Control_for_Expressive_Text-to-Speech_Generation.pdf)
  - [Publication] #TODO

</details>

## Abstract

<table><tr><td width="50%">

Controlling speaking style in text-to-speech (TTS) systems has become a growing focus in both academia and industry.
While many existing approaches rely on reference audio to guide style generation, such methods are often impractical due to privacy concerns and limited accessibility.
More recently, large language models (LLMs) have been used to control speaking style through natural language prompts; however, their high computational cost, lack of interpretability, and sensitivity to prompt phrasing limit their applicability in real-time and resource-constrained environments.
In this work, we propose ParaStyleTTS, a lightweight and interpretable TTS framework that enables expressive style control from text prompts alone.
ParaStyleTTS features a novel two-level style adaptation architecture that separates prosodic and paralinguistic speech style modeling.
It allows fine-grained and robust control over factors such as emotion, gender, and age.
Unlike LLM-based methods, ParaStyleTTS maintains consistent style realization across varied prompt formulations and is well-suited for real-world applications, including on-device and low-resource deployment.
Experimental results show that ParaStyleTTS generates high-quality speech with performance comparable to state-of-the-art LLM-based systems while being 30x faster, using 8x fewer parameters, and requiring 2.5x less CUDA memory.
Moreover, ParaStyleTTS exhibits superior robustness and controllability over paralinguistic speaking styles, providing a practical and efficient solution for style-controllable text-to-speech generation.
Demo can be found at \url{https://parastyletts.github.io/ParaStyleTTS_Demo/}.
Code can be found at \url{https://github.com/haoweilou/ParaStyleTTS}.

</td><td>

</td></tr></table>

## 1·Introduction

<table><tr><td width="50%">

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="tab:model_comparison">![]()</a>

</td></tr>
<tr><td>

Tab.01: Comparison of style-controllable TTS models

</td><td>

</td></tr>
<tr><td>

Text-to-Speech (TTS) generation has made significant progress in recent years.
It is an essential component of human-computer interaction in applications such as virtual assistants, audiobooks, and accessibility tools.
Modern TTS systems aim not only to produce intelligible and natural, human-like speech but also need to support expressive and controllable generation that can generate speech with different speaking style.
Earlier TTS models such as Tacotron2[^Wang2017Tacotron], FastSpeech[^Ren2019Fastspeech], [^Ren2020Fastspeech], Glow-TTS[^Kim2020Glow-TTS], and VITS[^Kim2021Conditional] focused primarily on improving intelligibility and naturalness.
In particular, VITS introduces a fully end-to-end architecture that unifies the acoustic model and vocoder into a single neural network.
It enhances both audio quality and generation efficiency by removing the need for external modules.
Recent advances in stylized and controllable speech generation aim to enhance the expressiveness and flexibility of TTS models.
Some works have attempted to control prosodic style variations across different languages.
For instance, StyleSpeech[^Lou2024StyleSpeech] enables control tone in Chinese by disentangling tonal prosody styles during the text tokenization stage.
Similarly, LanStyleTTS[^Lou2025Generalized] proposes a similar approach to control language-specific prosody style and enables manipulation of tone and stress patterns across multiple languages.
However, beyond prosody styles, paralinguistic styles, such as emotion, age, and gender are also critical for speech generation.
These factors influence how speech is perceived and are essential for personalized applications such as voice assistants, storytelling and dialogue systems with emotion.
While StyleSpeech[^Lou2024StyleSpeech] and LanStyleTTS[^Lou2025Generalized] are effective at controlling prosodic styles, they are not well-suited for handling paralinguistic styles.
Their phoneme-level fusion of style and phoneme embeddings is tailored to prosody, which affects phoneme articulation, but lacks the flexibility to model higher-level, paralinguistic-related speaking styles such as speaker's emotion, age, and gender.
Recent advances in large language models (LLMs)[^Yao2024Survey] demonstrate strong capabilities in natural language understanding and text generation.
These strengths have motivated the use of LLMs in speech generation, particularly for controlling the paralinguistic styles of speech.
CosyVoice[^Du2024Cosyvoice] explores the use of LLMs to enable paralinguistic control in speech.
In CosyVoice, a descriptive style prompt (e.g., "a young woman speaking angrily") is concatenated with the text input and processed by an LLM.
The LLM encodes both content and style into a unified semantic embedding, which serves as conditioning for the speech decoder.
This enables the model to guide speech generation based on the implied paralinguistic styles in the prompt.
While this approach allows for flexible and expressive synthesis, it also introduces several limitations.
First, the speaking style and content are implicitly entangled by the LLM in an auto-regressive manner.
The black-box nature of LLMs limits interpretability, making it difficult to understand or control how style is applied in the generated speech.
Second, LLM-based models are computationally expensive, requiring substantial memory and inference time, which makes them unsuitable for real-time or on-device deployment.
Third, the lack of explicit control and transparency reduces the robustness of the TTS system which make the style of speech highly sensitive to the phrasing of the input prompt.
To address the limitations of high computational cost and limited interpretability in LLM-based approaches.
We propose ParaStyleTTS, a lightweight, controllable, and expressive TTS framework that enables rich style control through a novel two-level style modeling architecture.
Inspired by LanStyleTTS's use of prosody style tokens at phoneme level and VITS's end-to-end design, ParaStyleTTS introduces an end-to-end framework that is capable of controlling both prosodic and paralinguistic styles at the phoneme and sentence levels.
Designed for end-to-end training and inference, ParaStyleTTS achieves high-quality speech generation while offering improved interpretability and computational efficiency.
Key contributions of this work are as follows: 

-  We propose a novel two-level style-controllable TTS model that explicitly disentangles prosodic and paralinguistic styles, enabling fine-grained and interpretable control over speaking style in speech synthesis.

-  Our system is lightweight and computationally efficient, featuring an end-to-end architecture that supports expressive speech generation and is well-suited for real-time and edge-device deployment.

-  Extensive experiments show that ParaStyleTTS achieves robust and consistent style control across varied prompt formulations, with improved generalizability in real-world scenarios.

Experimental results show that our proposed method can generate high-quality speech with performance comparable to state-of-the-art LLM-based speech generation models while achieving 30x faster inference, 8x smaller model size, and 2.5x lower CUDA memory usage.

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="fig:ParaStyleTTS_train">![](figure/ParaStyleTTS_train.pdf)</a>

</td></tr>
<tr><td>

Fig.01: Training pipeline of ParaStyleTTS

</td><td>

</td></tr>
<tr><td>

</td><td>

</td></tr></table>
