# ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation

<details>
<summary>基本信息</summary>

- 标题: "ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation."
- 作者:
  - 01 Haowei Lou
  - 02 Hye-Young Paik
  - 03 Wen Hu
  - 04 Lina Yao
- 链接:
  - [ArXiv](https://arxiv.org/abs/2510.18308v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2510.18308v1](PDF/2025.10.21_2510.18308v1_ParaStyleTTS__Toward_Efficient_and_Robust_Paralinguistic_Style_Control_for_Expressive_Text-to-Speech_Generation.pdf)
  - [Publication] #TODO

</details>

## Abstract

<table><tr><td width="50%">

Controlling speaking style in text-to-speech (TTS) systems has become a growing focus in both academia and industry.
While many existing approaches rely on reference audio to guide style generation, such methods are often impractical due to privacy concerns and limited accessibility.
More recently, large language models (LLMs) have been used to control speaking style through natural language prompts; however, their high computational cost, lack of interpretability, and sensitivity to prompt phrasing limit their applicability in real-time and resource-constrained environments.
In this work, we propose ParaStyleTTS, a lightweight and interpretable TTS framework that enables expressive style control from text prompts alone.
ParaStyleTTS features a novel two-level style adaptation architecture that separates prosodic and paralinguistic speech style modeling.
It allows fine-grained and robust control over factors such as emotion, gender, and age.
Unlike LLM-based methods, ParaStyleTTS maintains consistent style realization across varied prompt formulations and is well-suited for real-world applications, including on-device and low-resource deployment.
Experimental results show that ParaStyleTTS generates high-quality speech with performance comparable to state-of-the-art LLM-based systems while being 30x faster, using 8x fewer parameters, and requiring 2.5x less CUDA memory.
Moreover, ParaStyleTTS exhibits superior robustness and controllability over paralinguistic speaking styles, providing a practical and efficient solution for style-controllable text-to-speech generation.
Demo can be found at \url{https://parastyletts.github.io/ParaStyleTTS_Demo/}.
Code can be found at \url{https://github.com/haoweilou/ParaStyleTTS}.

</td><td>

</td></tr></table>

## 1·Introduction

<table><tr><td width="50%">

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="tab:model_comparison">![]()</a>

</td></tr>
<tr><td>

Tab.01: Comparison of style-controllable TTS models

</td><td>

</td></tr>
<tr><td>

Text-to-Speech (TTS) generation has made significant progress in recent years.
It is an essential component of human-computer interaction in applications such as virtual assistants, audiobooks, and accessibility tools.
Modern TTS systems aim not only to produce intelligible and natural, human-like speech but also need to support expressive and controllable generation that can generate speech with different speaking style.
Earlier TTS models such as Tacotron2[^Wang2017Tacotron], FastSpeech[^Ren2019Fastspeech], [^Ren2020Fastspeech], Glow-TTS[^Kim2020Glow-TTS], and VITS[^Kim2021Conditional] focused primarily on improving intelligibility and naturalness.
In particular, VITS introduces a fully end-to-end architecture that unifies the acoustic model and vocoder into a single neural network.
It enhances both audio quality and generation efficiency by removing the need for external modules.
Recent advances in stylized and controllable speech generation aim to enhance the expressiveness and flexibility of TTS models.
Some works have attempted to control prosodic style variations across different languages.
For instance, StyleSpeech[^Lou2024StyleSpeech] enables control tone in Chinese by disentangling tonal prosody styles during the text tokenization stage.
Similarly, LanStyleTTS[^Lou2025Generalized] proposes a similar approach to control language-specific prosody style and enables manipulation of tone and stress patterns across multiple languages.
However, beyond prosody styles, paralinguistic styles, such as emotion, age, and gender are also critical for speech generation.
These factors influence how speech is perceived and are essential for personalized applications such as voice assistants, storytelling and dialogue systems with emotion.
While StyleSpeech[^Lou2024StyleSpeech] and LanStyleTTS[^Lou2025Generalized] are effective at controlling prosodic styles, they are not well-suited for handling paralinguistic styles.
Their phoneme-level fusion of style and phoneme embeddings is tailored to prosody, which affects phoneme articulation, but lacks the flexibility to model higher-level, paralinguistic-related speaking styles such as speaker's emotion, age, and gender.
Recent advances in large language models (LLMs)[^Yao2024Survey] demonstrate strong capabilities in natural language understanding and text generation.
These strengths have motivated the use of LLMs in speech generation, particularly for controlling the paralinguistic styles of speech.
CosyVoice[^Du2024Cosyvoice] explores the use of LLMs to enable paralinguistic control in speech.
In CosyVoice, a descriptive style prompt (e.g., "a young woman speaking angrily") is concatenated with the text input and processed by an LLM.
The LLM encodes both content and style into a unified semantic embedding, which serves as conditioning for the speech decoder.
This enables the model to guide speech generation based on the implied paralinguistic styles in the prompt.
While this approach allows for flexible and expressive synthesis, it also introduces several limitations.
First, the speaking style and content are implicitly entangled by the LLM in an auto-regressive manner.
The black-box nature of LLMs limits interpretability, making it difficult to understand or control how style is applied in the generated speech.
Second, LLM-based models are computationally expensive, requiring substantial memory and inference time, which makes them unsuitable for real-time or on-device deployment.
Third, the lack of explicit control and transparency reduces the robustness of the TTS system which make the style of speech highly sensitive to the phrasing of the input prompt.
To address the limitations of high computational cost and limited interpretability in LLM-based approaches.
We propose ParaStyleTTS, a lightweight, controllable, and expressive TTS framework that enables rich style control through a novel two-level style modeling architecture.
Inspired by LanStyleTTS's use of prosody style tokens at phoneme level and VITS's end-to-end design, ParaStyleTTS introduces an end-to-end framework that is capable of controlling both prosodic and paralinguistic styles at the phoneme and sentence levels.
Designed for end-to-end training and inference, ParaStyleTTS achieves high-quality speech generation while offering improved interpretability and computational efficiency.
Key contributions of this work are as follows: 

-  We propose a novel two-level style-controllable TTS model that explicitly disentangles prosodic and paralinguistic styles, enabling fine-grained and interpretable control over speaking style in speech synthesis.

-  Our system is lightweight and computationally efficient, featuring an end-to-end architecture that supports expressive speech generation and is well-suited for real-time and edge-device deployment.

-  Extensive experiments show that ParaStyleTTS achieves robust and consistent style control across varied prompt formulations, with improved generalizability in real-world scenarios.

Experimental results show that our proposed method can generate high-quality speech with performance comparable to state-of-the-art LLM-based speech generation models while achieving 30x faster inference, 8x smaller model size, and 2.5x lower CUDA memory usage.

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="fig:ParaStyleTTS_train">![](figure/ParaStyleTTS_train.pdf)</a>

</td></tr>
<tr><td>

Fig.01: Training pipeline of ParaStyleTTS

</td><td>

</td></tr>
<tr><td>

</td><td>

</td></tr></table>

## 2·Related Work

<table><tr><td width="50%">

Recent advances in style-controllable TTS models have aimed to enhance expressiveness, multilingual capabilities, and controllability over various aspects of speech such as prosody, emotion, and speaker identity.
Table~[Tab.01](#tab:model_comparison) provides a comparative overview of representative models, categorized by their control method and levels of style control.
VITS[^Kim2021Conditional] adopts a variational autoencoder framework that learns the speaking style of speakers from the training data and uses learned speaker embeddings to control the timbre and speaking style of the generated speech during inference.
While it achieves high naturalness and supports direct waveform generation through an end-to-end architecture, it lacks prompt-based controllability and disentangled style modeling.
Paralinguistic styles are typically entangled within the latent variables with limited interpretability and fine-grained control.
StyleSpeech[^Lou2024StyleSpeech] and LanStyleTTS[^Lou2025Generalized] introduce phoneme-level style control mechanisms by aligning prosodic style tokens with phonemes.
Each phoneme is associated with its own prosody style token, enabling fine-grained, interpretable control over prosodic features.
This approach is particularly effective for tonal languages such as Chinese.
However, these models are not end-to-end and rely on external vocoders, limiting their efficiency.
Moreover, they struggle to generalize to StyleSpeech[^Lou2024StyleSpeech] and LanStyleTTS[^Lou2025Generalized] introduce phoneme-level style control mechanisms by aligning prosodic style tokens with individual phonemes.
Each phoneme is associated with its own style token, enabling fine-grained and interpretable control over prosodic features.
This design is particularly effective for tonal languages such as Chinese, where pitch and intonation are linguistically meaningful.
However, these models lack the ability to control high-level paralinguistic styles such as emotion, age, or intent, which limits their expressiveness in broader speech generation scenarios.
More recent models, including Spark-TTS[^Wang2025Spark-TTS] and CosyVoice[^Du2024Cosyvoice], leverage either speech or text-based prompt with LLMs to control speaking style.
CosyVoice introduces a text-prompted system that enables paralinguistic control.
Specifically, it concatenates the style prompt and content into a single input sequence and relies on LLMs to convert this sequence into meaningful semantic tokens for decoding into stylized speech.
While this approach provides flexibility and multilingual generalization, it also introduces notable limitations.
Our experiments show that CosyVoice is highly sensitive to prompt phrasing.
For example, altering the prompt from “a female speaker” to “a female speaker is speaking Chinese” can lead to a speech generated with an incorrect speaking style.
In one case, the model generated speech with a female voice even when the prompt explicitly described a male speaker.
This suggests that the model may overfit to specific prompt formulations seen during training, resulting in poor generalization to compositionally complex or open-ended prompts.
A more systematic evaluation is warranted to assess the robustness and reliability of prompt-based style control in such models.

</td><td>

</td></tr></table>

## 3·Method

### 3.1·Text Tokenization

<table><tr><td width="50%">

\label{sec:text_tokenize} We adopt the IPA-based text tokenization method from LanStyleTTS [^Lou2025Generalized] to convert English and Chinese text into phonemes with accompanying prosody features such as stress (English) and tone (Chinese).
More specifically, each word in English is converted to ARPAbet phonemes using the CMU Pronouncing Dictionary[^{Carnegie Mellon University}2023Cmu], with stress markers extracted separately to form the style token sequence.
The phonemes are then mapped to IPA phonemes.
For Chinese, we use the \texttt{pypinyin} library to convert each character into its Pinyin form, which is then split into initials and finals.
The tone is stripped from the final and used as a style token, while the remaining components are mapped to IPA phonemes.
Our IPA dictionary comprises 81 phonemes, 5 tone markers for Chinese, and 3 stress markers for English.
In addition, we include three special tokens: \texttt{[START]} to denote the beginning of a sequence, \texttt{[END]} for the end of a sequence, and \texttt{[|]} as a word boundary marker.

</td><td>

</td></tr></table>

### 3.2·Token Encoder

<table><tr><td width="50%">

We first project the IPA tokens~$X$ and prosody style tokens~$S$ into a sequence of vector representations using two distinct embedding layers.
To preserve sequential information, sinusoidal positional encodings are added to the embeddings.
Then, IPA and prosody style embeddings are fed into Feed-Forward Transformer (FFT) blocks[^Ren2019Fastspeech] to leverage the self-attention to model long-range dependencies and contextual relationships across the token sequence.
Unlike the original Transformer architecture[^Vaswani2017Attention], which uses a two-layer fully connected network in its feed-forward submodule, we replace it with two one-dimensional convolutional layers to better capture local contextual dependencies between adjacent tokens.
Let $\mathbf{X} = [x_1, x_2, \ldots, x_L]$ denote the phoneme embedding sequence obtained from text tokenization.
We associate each phoneme $x_l$ with a corresponding prosodic style embedding $\mathbf{x}_t,\mathbf{s}^{\text{pho}}_t \in \mathbb{R}^{d_1}$, forming the phoneme-level prosody sequence: \[ \mathbf{S}^{\text{pho}} = [\mathbf{s}^{\text{pho}}_1, \mathbf{s}^{\text{pho}}_2, \ldots, \mathbf{s}^{\text{pho}}_L] \]

</td><td>

</td></tr></table>

### 3.3·Paralinguistic Encoder

<table><tr><td width="50%">

The sentence-level paralinguistic style embedding is denoted as $\mathbf{S}^{\text{para}} \in \mathbb{R}^{d_2}$, representing sentence-level paralinguistic characteristics such as emotion, age, gender, and accent.
To obtain this embedding, we employ a pre-trained MPNet model[^Song2020Mpnet] to encode descriptive paralinguistic prompts into $d_2$-dimensional embedding.
For each speech sample in our dataset, we construct a text prompt using the following template: 
\begin{quote} *"A [Age] [Gender] is speaking [Accent] with [Emotion] emotion.”* \end{quote}
It is then fed into MPNet to produce the paralinguistic prompt embedding $\mathbf{S}^{\text{para}}$, which is used to condition the TTS model and guide the generation of speech with the intended paralinguistic style.

</td><td>

</td></tr></table>

### 3.4·Style Adapter

<table><tr><td width="50%">

Style in speech generation is a broad concept that encompasses both prosodic features, such as pitch and tone, and paralinguistic styles, including gender, emotion, accent, and more.
In this research, we divide style into two categories with different levels of control.
One is the phoneme-level style, which captures fine-grained prosodic variations such as tone and stress at the level of individual phonemes.
This level strongly influences how each word is articulated.
Another one is sentence-level style, which represents global characteristics of the speech.
It includes emotion, age, gender, and accent.
While these features shape the overall impression of the speech, they exert less direct influence on phoneme realization.
To support effective control at both levels, we design specialized architectures tailored to the unique requirements of each level of style.

</td><td>

</td></tr></table>

#### 3.4.1·Prosody Style Adapter

<table><tr><td width="50%">

Given the phoneme embedding sequence $\mathbf{X} = [x_1, x_2, \ldots, x_L]$ and the phoneme-level prosody style sequence $\mathbf{S}^{\text{pho}} = [\mathbf{s}^{\text{pho}}_1, \mathbf{s}^{\text{pho}}_2, \ldots, \mathbf{s}^{\text{pho}}_L]$, we apply a lightweight adapter to inject prosodic features into the phoneme representations.
Following the design in LanStyleTTS[^Lou2025Generalized], we employ a Gated Tanh Unit (GTU) fusion mechanism, defined as: 

$$
\tilde{\mathbf{x}}_t = \tanh(W_1 x_t + b_1) \odot \sigma(W_2 \mathbf{s}^{\text{pho}}_t + b_2), \quad \forall t \in \{1, \ldots, L\}, 
$$

where $W_1$, $W_2$ are learnable projection weights, $b_1$, $b_2$ are biases, $\odot$ denotes element-wise multiplication, and $\sigma(\cdot)$ is the sigmoid function.
This formulation allows the prosody style to modulate the phoneme representation at a fine-grained level while preserving phonetic structure.

</td><td>

</td></tr></table>

#### 3.4.2·Paralinguistic Style Adapter

<table><tr><td width="50%">

Given that paralinguistic style typically remains consistent throughout a speech, it can influence both phoneme-level and sentence-level acoustic characteristics.
To capture these effects, we first apply two distinct linear layers to project paralinguistic prompt embedding $\mathbf{S}^{\text{para}} \in \mathbb{R}^{d_2}$ into phoneme-level~($\mathbf{S}^{\text{local}}$) and sentence-level~($\mathbf{S}^{\text{global}}$) paralinguistic style embedding.

$$
\mathbf{S}^{\text{local}} = W_{\text{local}} \mathbf{S}^{\text{para}} + b_{\text{local}}, \quad \mathbf{S}^{\text{global}} = W_{\text{global}} \mathbf{S}^{\text{para}} + b_{\text{global}}, 
$$

We adopt Feature-wise Linear Modulation (FiLM)[^Perez2018Film] to inject phoneme-level paralinguistic style embedding into the phoneme embeddings via sequence-wise conditioning.
Specifically, given the projected style embedding $\mathbf{s}^{\text{sent}}$, we compute the scaling and bias vectors as: 

$$
\gamma = W_\gamma \mathbf{S}^{\text{local}} + b_\gamma, \quad \beta = W_\beta \mathbf{s}^{\text{local}} + b_\beta, 
$$

where $W_\gamma, W_\beta \in \mathbb{R}^{d_1 \times d_2}$ are learnable projection matrices, and $b_\gamma, b_\beta \in \mathbb{R}^{d_1}$ are bias terms.
These parameters generate the modulation factors used to transform the phoneme embeddings: 

$$
\hat{\mathbf{x}}_t = \gamma \odot \tilde{\mathbf{x}}_t + \beta, \quad \forall t \in \{1, \ldots, L\}, 
$$

where $\odot$ denotes element-wise multiplication.
This FiLM-based adapter integrates paralinguistic style into each phoneme within the speech.
The sentence-level paralinguistic style embedding ($\mathbf{s}^{\text{global}}$) is applied in both training and inference stages to guide sentence-level style adaptation within the waveform decoder.
By conditioning on this embedding, ParaStyleTTS is able to impose consistent paralinguistic styles throughout the speech.

</td><td>

</td></tr></table>

### 3.5·Latent Embedding Learning

<table><tr><td width="50%">

\label{sec:latent_embed} We adopt the variational autoencoder (VAE) framework[^Kingma2019Introduction] with adversarial training[^Goodfellow2014Generative] and normalizing flows[^Papamakarios2021Normalizing] to model expressive latent representations and decode our waveform decoder due to its fully end-to-end training, non-autoregressive inference, and high-fidelity speech generation.
In our system, the decoder takes as input the style-integrated phoneme representations $\hat{\mathbf{X}} = [\hat{x}_1, \hat{x}_2, \ldots, \hat{x}_L]$, which are modulated by both phoneme-level prosody and sentence-level paralinguistic style.
To enforce global consistency, the sentence-level embedding $\mathbf{S}^{\text{global}}$ is concatenated with both the prior and posterior encodings before being passed through the normalizing flow layers.
The latent embedding $\mathbf{Z} \in \mathbb{R}^{N\times T}$ is first sampled from a Gaussian posterior using a variational autoencoder (VAE).
The posterior distribution is conditioned on the ground-truth spectrogram $\mathbf{Y}$ and the sentence-level style embedding $\mathbf{S}^{\text{global}}$, and is parameterized as: 

$$
\mathbf{Z} \sim q(\mathbf{Z} | \mathbf{Y}, \mathbf{S}^{\text{global}}) = \mathcal{N}(\boldsymbol{\mu}_{\text{post}}, \boldsymbol{\sigma}_{\text{post}}), 
$$

where $\boldsymbol{\mu}_{\text{post}}, \boldsymbol{\sigma}_{\text{post}}$ are predicted by a posterior encoder from the spectrogram and global style embedding.
To obtain a more expressive latent representation, we further apply a sequence of invertible normalizing flows to $\mathbf{z}$: 

$$
\mathbf{Z}_{\text{flow}} = f_{\text{flow}}(\mathbf{Z}; \theta_{\text{flow}}), 
$$

The prior distribution is defined over this transformed latent space and is modeled as a Gaussian conditioned on the style-integrated phoneme sequence $\hat{\mathbf{X}}$: 

$$
p(\mathbf{Z}_{\text{flow}} | \hat{\mathbf{X}}) = \mathcal{N}(\boldsymbol{\mu}_{\text{prior}}, \boldsymbol{\sigma}_{\text{prior}}).

$$

where \( \boldsymbol{\mu}_{\text{prior}}, \boldsymbol{\sigma}_{\text{prior}} \) are predicted by a prior encoder network, which takes as input the phoneme embeddings $\tilde{\mathbf{X}}$ and **local** style embedding $\mathbf{S}^{\text{local}}$.
These parameters define the expected distribution of latent speech features given the linguistic content and phoneme-level paralinguistic style.
The KL-divergence[^Kingma2013Auto-Encoding] between the transformed posterior sample and the prior is minimized during training: 

$$
\mathcal{L}_{\text{KL}} = D_{\text{KL}}\left(\mathbf{Z_{\text{flow}}} \parallel p(\mathbf{Z}_{\text{flow}} | \hat{\mathbf{X}})\right).

$$

This structure allows the model to capture a rich and flexible latent distribution that aligns with both the local and global style information.

</td><td>

</td></tr></table>

### 3.6·Duration Alignment and Modeling

<table><tr><td width="50%">

To align the phoneme sequence integrated with paralinguistic style embedding $\hat{\mathbf{X}} = [\hat{x}_1, \hat{x}_2, \ldots, \hat{x}_L]$ with the latent embedding~$\mathbf{Z}$ during training, we apply Monotonic Alignment Search (MAS)[^Kim2021Conditional] to compute a soft alignment matrix \( \mathbf{A} \in \mathbb{R}^{L \times T} \), where \( A_{t, j} \) represents the attention weight between the $t$-th phoneme and the $j$-th frame in $\mathbf{Z}$.
The duration \( d_t \) for each phoneme is estimated by summing the attention weights over the time axis: 

$$
d_t = \sum_{j=1}^{T} A_{t, j}, \quad \forall t \in \{1, \ldots, L\}.

$$

We integrate a Stochastic Duration Predictor (SDP) to learn to predict the log-duration distribution conditioned on the phoneme and style features.
During training, we minimize a log-domain Mean Squared Error (MSE) loss between the predicted and reference durations: 

$$
\mathcal{L}_{\text{dur}} = \frac{1}{L} \sum_{t=1}^{L} \left( \log(d_t + \epsilon) - \log(\hat{d}_t + \epsilon) \right)^2, 
$$

where \( \epsilon \) is a constant for numerical stability.
It allow the model to intrinsically learn phoneme durations during training, while also capturing duration variations influenced by paralinguistic styles.

</td><td>

</td></tr></table>

### 3.7·Training Objective

<table><tr><td width="50%">

The model is optimized using a combination of objectives adapted from the VITS framework[^Kim2021Conditional].
These include a reconstruction loss $\mathcal{L}_{\text{recon}}$, which measures the difference between the generated~$\hat{Y}$ and ground-truth spectrogram, an adversarial loss~$\mathcal{L}_{\text{adv}}$ to encourage realistic waveform generation through multi-period discriminators $D$ and a feature matching loss~$\mathcal{L}_{\text{fm}}$ to stabilize adversarial training by aligning discriminator's internal feature of real and generated speech: 

$$
\begin{aligned}
\mathcal{L}_{\text{recon}} &= \| \mathbf{Y} - \hat{\mathbf{Y}} \|_1 \\ \mathcal{L}_{\text{adv}} &= \mathbb{E}_{\hat{\mathbf{Y}}}[(D(\hat{\mathbf{Y}}) - 1)^2] \\ \mathcal{L}_{\text{fm}} &= \sum_{l=1}^{L} \| D^{(l)}(\mathbf{Y}) - D^{(l)}(\hat{\mathbf{Y}}) \|_1 \\ \mathcal{L_{\text{total}}} &= \mathcal{L_\text{fm}} + \mathcal{L_\text{KL}} + \mathcal{L_\text{dur}} + \mathcal{L_\text{recon}} + \mathcal{L_\text{adv}} 
\end{aligned}
$$

</td><td>

</td></tr></table>

### 3.8·Time Complexity Analysis

<table><tr><td width="50%">

To analyze the computational complexity of the overall architecture in ParaStyleTTS versus LLM-based paralinguistic style control models, we define $N$ as the length of the text sequence and $M$ as the length of the paralinguistic style prompt.
In ParaStyleTTS, the phoneme and prosody style tokens are encoded separately using transformer-based FFT blocks, followed by a Gated Tanh Unit (GTU) style adapter.
The time complexity is $\mathbf{O}(N^2)$.
Meanwhile, the paralinguistic style prompt is independently processed by a transformer-based MPNet encoder, contributing an additional $\mathbf{O}(M^2)$ in computation.
In total, this results in a combined time complexity of $\mathbf{O}(N^2 + M^2)$.
In contrast, LLM-style fusion approaches concatenate the text and paralinguistic tokens into a single sequence of length $N + M$, which is jointly encoded by a large transformer or LLM model.
This yields a total time complexity of $\mathbf{O}((N + M)^2)$.
As a result, LLM-style fusion introduces an additional cross-attention cost of $\mathbf{O}(NM)$, making it less computationally efficient.
\label{tab:overall_performance_comparision}

</td><td>

</td></tr></table>

## 4·Experiment

### 4.1·Datasets

<table><tr><td width="50%">

We conduct our experiments using a multilingual and multi-style speech corpus comprising both English and Chinese speech samples.
To ensure broad coverage of paralinguistic styles, we construct a composite dataset by combining several publicly available speech datasets.
The final training data consists of four sources.
The **Baker** dataset[^Databaker2020Chinese] is a single-speaker Mandarin Chinese corpus featuring a female voice.
The **LJSpeech** dataset[^Ito2017Lj] is a single-speaker English corpus, also featuring a female speaker, widely used in TTS research for clean and consistent English utterances.
The **Emotional Speech Dataset (ESD)**[^Zhou2021Seen] contributes a multilingual, multi-speaker emotional speech in both English and Chinese, covering five emotional categories and including both male and female speakers.
To further enrich stylistic diversity, we curate 16 stylized character speech captions from the **Genshin Impact** voice dataset to capture expressive speech across different age styles.
It covers **two languages** (English and Chinese), **two genders** (male and female), and **four age categories** (child, teenager, young adult, and adult).
Emotion labels span **five classes**: neutral, happy, sad, angry, and surprised.
The final training dataset has 86k speech samples, with around 108.30 hours of training data.
The dataset contains speech from 38 speakers.
More details about our dataset can be found in table~[Tab.08](#tab:dataset) in the appendix.

</td><td>

</td></tr></table>

### 4.2·Preprocessing

<table><tr><td width="50%">

All speech recordings are resampled to 22.05 kH.
To ensure the phoneme remains consistent across multilingual languages, we apply IPA-based phoneme tokenization uniformly across both English and Chinese using the text tokenization method described in Section~[sec:text_tokenize](#sec:text_tokenize).
For each speech-text pair, we pre-compute phoneme tokens and prosody-style tokens to serve as input to the TTS model.
In addition, we generate a paralinguistic style caption (e.g., "A young female is speaking English with happy emotion”), which is then encoded into a style embedding to guide paralinguistic style control.

</td><td>

</td></tr></table>

### 4.3·Training

<table><tr><td width="50%">

The model is trained on four NVIDIA V100 GPUs with a batch size of 32 for up to 700k steps.
We adopt the AdamW optimizer[^Loshchilov2017Decoupled], using the same hyperparameters and learning rate schedule as VITS[^Kim2021Conditional].

</td><td>

</td></tr>
<tr><td colspan="2">

<a id="tab:comparison">![]()</a>

</td></tr>
<tr><td>

Tab.02: Overall Performance Comparison

</td><td>

</td></tr>
<tr><td>

</td><td>

</td></tr></table>

### 4.4·Experiment Setting

<table><tr><td width="50%">

To evaluate the intelligibility and paralinguistic expressiveness of our system, we adopt both objective metrics and human perceptual testing.
For intelligibility assessment, we avoid using samples from our training set to ensure fair cross-model comparisons, as different baselines are trained on distinct datasets.
Instead, we generate speech for two standardized corpora: the 720 Harvard Sentences[^Electrical1969Ieee] in English and 400 phonetically balanced Mandarin TMNews sentences set from[^Chen2023Baspro], both widely recognized for evaluating speech systems.
The generated speech is transcribed using Whisper-Base[^Radford2023Robust], and Word Error Rate (WER) is computed by comparing the transcriptions against the ground-truth text.
Lower WER scores indicate higher intelligibility and transcription consistency.
To complement the objective evaluation, we conduct a Mean Opinion Score (MOS) listening test.
For each language, we randomly select ten generated samples from each model and present them to at least five bilingual listeners fluent in both English and Chinese.
The listeners are asked to rate each sample along two dimensions: intelligibility (I-MOS), which reflects how easily the speech content can be understood, and naturalness (N-MOS), which reflects how human-like and fluent the speech sounds.
To assess the expressiveness of paralinguistic styles, we use open-source speech analysis models to determine whether the generated speech is distinguishable by classifiers.
For emotion evaluation, we adopt Emotion2Vec[^Ma2023Emotion2vec], a state-of-the-art, open-source model for emotion recognition in speech.
We conduct classification experiments on generated samples with varying emotional labels to verify their perceptual separability.
For age and gender analysis, due to the lack of large-scale, open-source models, we train a lightweight paralinguistic style classifier based on CLAP[^Elizalde2023Clap].
This model evaluates whether age and gender styles in the generated speech can be reliably identified.
In addition to evaluating speech quality and expressiveness, we assess inference efficiency, model size, and CUDA memory usage across different TTS models to determine their suitability for edge device deployment.
All performance measurements are conducted on a single NVIDIA 3060 Ti GPU, a widely available consumer-grade device selected to reflect realistic and cost-effective deployment scenarios.
To ensure reliability, we generate the entire evaluation set comprising 1,120 sentences, including the 720 Harvard Sentences and 400 phonetically balanced Mandarin sentences.
Each sentence is processed individually with a batch size of one.
The average inference time and peak CUDA memory usage per sample are recorded.
These metrics provide a consistent and fair basis for comparing computational efficiency across models.

</td><td>

</td></tr></table>
