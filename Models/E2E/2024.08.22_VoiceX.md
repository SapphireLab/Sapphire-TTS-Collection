# VoiceX

<details>
<summary>基本信息</summary>

- 标题: VoiceX: A Text-To-Speech Framework for Custom Voices
- 作者:
  - 01 [Silvan Mertes](../../Authors/Silvan_Mertes.md)
  - 02 [Daksitha Withanage Don](../../Authors/Daksitha_Withanage_Don.md)
  - 03 [Otto Grothe](../../Authors/Otto_Grothe.md)
  - 04 [Johanna Kuch](../../Authors/Johanna_Kuch.md)
  - 05 [Ruben Schlagowski](../../Authors/Ruben_Schlagowski.md)
  - 06 [Elisabeth André](../../Authors/Elisabeth_André.md)
- 机构:
  - [德国奥格斯堡大学](../../Institutions/DEU-Augsburg_University_德国奥格斯堡大学.md) 
- 时间:
  - 预印时间: 2024.08.22 ArXiv v1
  - 更新笔记: 2024.08.23
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2408.12170)
  - [DOI]()
  - [Github]()
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: 5
- 引用: 31
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> Modern TTS systems are capable of creating highly realistic and natural-sounding speech. 
> Despite these developments, the process of customizing TTS voices remains a complex task, mostly requiring the expertise of specialists within the field.
> One reason for this is the utilization of deep learning models, which are characterized by their expansive, non-interpretable parameter spaces, restricting the feasibility of manual customization.
> In this paper, we present a novel human-in-the-loop paradigm based on an evolutionary algorithm for directly interacting with the parameter space of a neural TTS model.
> We integrated our approach into a user-friendly graphical user interface that allows users to efficiently create original voices. 
> Those voices can then be used with the backbone TTS model, for which we provide a Python API.
> Further, we present the results of a user study exploring the capabilities of VoiceX. We show that VoiceX is an appropriate tool for creating individual, custom voices.

## 1.Introduction: 引言

> Text-to-Speech (TTS) systems are increasingly becoming an integral part of our daily lives. 
> They seamlessly integrate into a variety of fields, like virtual agents, social media platforms, video games, and accessibility applications such as screen readers or navigation systems \cite{abdulrahman2022natural,mertes2021potential, zhang2023does,henderson2022screen,sangale2022literature}.
> The speech quality of those TTS systems is steadily increasing, and the corresponding models are modified and extended in a variety of ways to equip them with even more capabilities, such as expressing voice-level emotions \cite{triantafyllopoulos2023overview} or even coping with multiple languages \cite{shang2021incorporating}.
> One of those abilities that got heavily improved over the last years is the \emph{Multi-Speaker capability}. 
> There, TTS models are not restricted to the synthesizing speech of one single voice. 
> Instead, they are conditioned on multiple speakers' voices, so that they can produce a variety of different voice timbres \cite{casanova2022yourtts,chen2020multispeech,casanova2021sc,cooper2020zero}.
>
> However, although those approaches possess the ability to generate a wide array of voices for different tasks, enabling users to fine-tune these voices to meet specific requirements presents a notable challenge.
> Typically, a subset of voices, selected in advance by software engineers or sound designers, is made available to end-users.
> If a user really wants to create a \emph{custom} voice that totally fits a certain use-case or need, there is commonly no possibility.
> Even if the user has proficient technical knowledge, there is still another problem: in most state-of-the-art neural TTS models, the voice timbre is encoded in intermediate, latent representations inside the TTS model and gets implicitly learned during training.
> As such, it is mostly not interpretable and therefore difficult to adapt.
>
> In this paper, we present a human-in-the-loop paradigm to easily search through the latent speaker embedding space of the state-of-the-art TTS system \emph{VITS} \cite{kim2021conditional}.
> Therefore, we introduce \emph{VoiceX}, a tool which iteratively presents users with pairs of distinct voices. 
> Starting with an initial comparison, users are simply required to select the voice that ``fits better'' to their preference. 
> These selections act as a feedback mechanism, forming the fitness function for an underlying Evolution Strategy (ES) algorithm (see Figure \ref{fig:ea_scheme}). 
> Through this iterative process, VoiceX effectively navigates the complex space of voice timbres, refining the selections to converge towards a voice that aligns closely with the user's specific requirements.
> We integrated VoiceX into a human-in-the-loop web application to make voice customization easily accessible.
> The system is designed to be easy to use, keeping cognitive load at a minimum, and as such being suitable to a broad spectrum of users.
> We evaluated our system in a user study (N=65). 
> There, we tested if users are capable of creating voices of certain characteristics. 
> Specifically, as a proof-of-concept, we assessed whether VoiceX can be utilized to build voices that exhibit distinct personality traits. 
> As personality is a quite complex concept, we argue that it therefore serves as an appropriate characteristic to explore the capabilities of the system.
>
> The contributions of this work are as follows:
> - We present a novel human-in-the-loop paradigm for customizing TTS voices.
> - We publicly provide a web application that can be used to create new TTS voices.
> - We present a study examining the voice customization capabilities of the system.
> - We provide an open-source Python API for easy usage of the created voices in all sorts of applications.

## 2.Related Works: 相关工作

### 2.1.TTS Voice Customization

> To customize voices in TTS systems, the underlying models must support such modifications.
> Wang et al. presented \emph{Tacotron GST} \cite{wang2018style}, a modification of Tacotron 2 \cite{shen2018natural}. There, \emph{Global Style Tokens} are introduced, facilitating the direct manipulation of certain implicitly learned attributes, such as speech speed and speaking style.
> Similarly, Valle et al. introduced Mellotron \cite{valle2020mellotron}, which was conditioned on rhythm and continuous pitch contours, allowing for the direct modification of those features after training. 
> However, the number of features that can be adjusted in both those architectures is quite limited.
> Although in theory, Tacotron GST gives the possibility to modify the style token representations on a fine granular level, those representations then do not hold interpretable semantics -- making them difficult to manually adjust.
> The same goes for the \emph{VITS} model \cite{kim2021conditional}, which makes use of speaker embeddings. 
> As such, by altering these embeddings, different voices are synthesized.
> However, since speaker embeddings itself may not be inherently interpretable, manual adjustments remain a challenge.
> Overall, these approaches share a common limitation: customization options are either confined to a limited number of features, or they remain inaccessible due to the non-interpretable nature of latent representations.
> One tool that wraps various TTS systems in a way that different features can be steered by end users is \emph{So-to-Speak} \cite{szekely2023so}. 
> However, their approach is more directed towards model capability \emph{analysis} and audiovisual \emph{exploration}, while VoiceX breaks down the TTS system's adjustment possibilities to binary decision tasks, focusing on simple and efficient \emph{customization}.

### 2.2.Collaborative Voice Creation

> Contrary to the approaches above, some approaches have been presented in the past where customization of voice was performed not on an individual basis, but collaboratively, i.e., a group of people adapting a voice together. 
> For example, Harrison et al. presented the concept of \emph{Gibbs Sampling with People} (GSP), in which experiment participants were able to manipulate specific, interpretable voice features, such as pitch range and F0 perturbation, allowing the voice to convey emotional prosody \cite{harrison2020gibbs}. 
> That concept was followed up by van Rijn et al. in multiple studies where they extended the GSP paradigm to work with latent spaces of neural TTS systems to create emotional voice prototypes \cite{van2021exploring}, voices for human faces \cite{van2022voiceme}, and voices for robots \cite{van2024giving}.
> In contrast to VoiceX, all those approaches focus on collaborative adaptation, leading to more generic prototypes of voices instead of \emph{individually} customizing a voice to a \emph{single} user's preferences. 

### 2.3.Voice Cloning

> Voice cloning utilizes algorithms -- typically rooted in deep learning -- to replicate a particular individual's voice, aiming to mimic the speaker's vocal traits, intonations, and speech patterns. 
> Recent advances have brought a variety of approaches to voice cloning, mostly focusing on specific factors like real-time capabilities \cite{jemine2019real}, expressiveness \cite{neekhara2021expressive}, speech quality \cite{hu2023real}, or the ability to clone with little training data \cite{arik2018neural}.
> In contrast, the goal of creating \emph{custom} voices (as we aim for with VoiceX) is not to replicate, but to innovate, allowing for the generation of \emph{original} or \emph{novel} voices tailored to specific requirements or preferences.

## 3.Methodology: 方法

### 3.1.Backbone TTS Model

> We use \emph{VITS}  \cite{kim2021conditional}, a state-of-the-art neural TTS architecture, as backbone for the whole system. 
> Specifically, we use a model\footnote{The pretrained model was taken from \url{https://github.com/jaywalnut310/vits}.} trained on the VCTK dataset \cite{yamagishi2019cstr}, as that model has shown its ability to cover a wide variety of voices in related experiments \cite{van2022voiceme,van2024giving}.

### 3.2.Speaker Representation

> An integral part of the VITS model is the use of so-called \emph{speaker embeddings}. 
> Those embeddings are a 256-dimensional representation of the speaker's voice, and its semantics were implicitly learned during the training of the model.
> As such, feeding different speaker embeddings with the synthesis text produces varied voices. Therefore, by interpolating the speaker embedding space we can generate a broad variety of different voices.
> To reduce the search space of the following human-in-the-loop algorithm (as described in the next section), we use a principal component analysis (PCA).
> Specifically, we use the PCA model that has already been used by van Rijn et al. \cite{van2024giving}. Their model was fitted on the 110 speakers of the VCTK dataset. 
> We perform our human-in-the-loop algorithm solely on the first 10 PCA components of that model, which we inversely feed into the PCA model to obtain the 256-dimensional representation that the VITS model demands.

### 3.3.Voice Creation

#### Evolution Strategy (ES)

> The modification of the synthesized voice is achieved through the implementation of a $(1 + \lambda)$-ES \cite{beyer2002evolution}, where $\lambda$ represents the offspring size. 
> As such, one whole population consists of $1 + \lambda$ individuals. 
> One individual contains the first 10 PCA component values of one speaker embedding.
> In common evolutionary strategies, the phenotypes (which in our case would be voices synthesized using the speaker embeddings obtained through the individual PCA component sets) of the whole population would be evaluated by a fitness function to determine the best individuals. 
> For our human-in-the-loop approach, that fitness function gets replaced by human judgment, i.e., the user assesses the best individual by listening to the voices.
> A similar approach was followed by Ritschel et al. \cite{ritschel2019personalized} -- although they did not tackle the use-case of TTS customization. Rather, they used a human-in-the-loop ES to steer the customization of non-verbal affective sounds for social robots.
> We set $\lambda$ to $1$ -- which means that in total, one population consists of one parent individual and one offspring individual. 
> As such, the user must only listen to two voices per iteration and judge which one s/he likes better.
> After that decision has been made, the PCA set that produces the better results is selected as the parent for a new population.
> The offspring of the new population is produced by mutating that parent. 
> For mutation, we implemented an epsilon mutation strategy with $\epsilon = 0.2$, which means that in $80\%$ of the cases, the offspring is produced by an actual mutation of the parent, and in $20\%$ of the cases the offspring is just a random new individual. 
> Doing so prevents the whole ES loop from getting stuck in a local minimum. The mutation operation itself adds Gaussian noise to the PCA components, leading to a slightly altered voice.
> After the mutation took place, the new population (i.e., the best individual from the last iteration and newly formed individual) is transformed into speaker embeddings (by the inverse use of the PCA model), synthesized to speech (by the use of the VITS model), and the resulting phenotypes of the new population are again presented to the user.
> The overall loop continues until the user is satisfied with a voice.
> As related work \cite{van2022voiceme} has shown that the most important characteristic that steers voice customization decisions is the perceived gender of voices, we wanted to make sure that users can set that course right from the beginning of the interaction.
> As such, for the initial population, we use two predefined speaker embeddings: one that represents a stereotypical male voice, and one that represents a stereotypical female voice. 
> Figure \ref{fig:ea_scheme} depicts the overall scheme of our human-in-the-loop evolutionary algorithm.

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

> In this work, we presented \emph{VoiceX}, a system that uses a \emph{VITS} model as the backbone TTS engine and allows users to create custom voices in an efficient, user-friendly way.
> VoiceX is based on a human-in-the-loop evolutionary strategy, where the user's decision serves as the fitness function. 
> By integrating our approach into a web application, we provide an accessible, easy-to-use tool that everyone can use to create personalized voices.
> Additionally, we made a Python API publicly available, allowing for an integration of the created voices into all kinds of applications.
> A user study showed that VoiceX can be used to create voices of high quality, as indicated by a MOS of $4.32$.
>
> While semi-objective measures of the user study indicated that participants were not able to use the system to create voices that show a certain personality, users still had the impression that they were able to do so. 
> We discussed that this deviation between semi-objective and subjective measures might stem from the users not having a detailed imagination of what a voice with a certain personality trait sounds like. 
> However, the subjective measure of how creating a voice with a certain personality worked was extremely positive, indicating that participants could effectively apply the system to their own satisfaction. 
>
> Overall, we argue that the perception of being able to customize a voice in the desired way is the key requirement for personalized, individual voice customization. Therefore, we can conclude that VoiceX shows great potential for a variety of stakeholders and use cases.
