# Glow-WaveGAN

<details>
<summary>基本信息</summary>

- 标题: "Glow-WaveGAN: Learning Speech Representations from GAN-based Variational Auto-Encoder For High Fidelity Flow-based Speech Synthesis"
- 作者:
  - 01 Jian Cong (NPU@ASLP)
  - 02 Shan Yang (NPU@ASLP)
  - 03 Lei Xie (谢磊, NPU@ASLP)
  - 04 Dan Su (Tencent AI Lab)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2106.10831)
  - [Publication](https://doi.org/10.21437/interspeech.2021-414)
  - [Github]
  - [Demo](https://syang1993.github.io/glow_wavegan)
- 文件:
  - [ArXiv](_PDF/2106.10831v2__Glow-WaveGAN__Learning_Speech_Representations_from_GAN-based_Variational_Auto-Encoder_For_High_Fidelity_Flow-based_Speech_Synthesis.pdf)
  - [Publication](_PDF/2106.10831p0__Glow-WaveGAN__InterSpeech2021.pdf)

</details>

## Abstract: 摘要

<table><tr><td width="50%">

Current two-stage TTS framework typically integrates an acoustic model with a vocoder -- the acoustic model predicts a low resolution intermediate representation such as Mel-spectrum while the vocoder generates waveform from the intermediate representation.
Although the intermediate representation is served as a bridge, there still exists critical mismatch between the acoustic model and the vocoder as they are commonly separately learned and work on different distributions of representation, leading to inevitable artifacts in the synthesized speech.
In this work, different from using pre-designed intermediate representation in most previous studies, we propose to use VAE combining with GAN to learn a latent representation directly from speech and then utilize a flow-based acoustic model to model the distribution of the latent representation from text.
In this way, the mismatch problem is migrated as the two stages work on the same distribution.
Results demonstrate that the flow-based acoustic model can exactly model the distribution of our learned speech representation and the proposed TTS framework, namely ***Glow-WaveGAN***, can produce high fidelity speech outperforming the state-of-the-art GAN-based model.

</td><td>

现有两阶段 TTS 框架通常将声学模型和声码器集成在一起, 声学模型预测低分辨率中间表示 (如梅尔频谱) 而声码器从中间表示生成波形.
尽管中间表示作为桥梁, 但声学模型和声码器之间仍存在严重的不匹配, 因为它们通常是单独学习的, 并在不同的表示分布上工作, 导致合成语音中不可避免的伪影.
在这项工作中, 与大多数之前的研究中使用预设计的中间表示不同, 我们提出使用 VAE 和 GAN 结合来直接从语音中学习潜在表示, 然后使用基于 Flow 的声学模型来从文本建模隐表示的分布.
通过这种方式, 不匹配问题被迁移成两阶段都在同一分布上工作.
结果表明基于 Flow 的声学模型可以精确建模我们学习到的语音表示的分布, 而所提出的 TTS 框架, 名为 ***Glow-WaveGAN***, 能够产生高质量语音, 超越了最先进的基于 GAN 的模型.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td></tr></table>
