# JETS

<details>
<summary>基本信息</summary>

- 标题: "JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech"
- 作者: 
  - 01 Dan Lim
  - 02 Sunghee Jung
  - 03 Eesung Kim
- 链接: 
  - [ArXiv](https://arxiv.org/abs/2203.16852)
  - [Publication]() #TODO InterSpeech 2022
  - [Github](https://github.com/imdanboy/jets)
  - [Demo]()
- 文件: 
  - [ArXiv] #TODO
  - [Publication] #TODO

</details>

## Abstract: 摘要

In neural text-to-speech (TTS), two-stage system or a cascade of separately learned models have shown synthesis quality close to human speech. 
For example, [FastSpeech2](../TTS2_Acoustic/2020.06.08_FastSpeech2.md) transforms an input text to a mel-spectrogram and then [HiFi-GAN](../TTS3_Vocoder/2020.10.12_HiFi-GAN.md) generates a raw waveform from a mel-spectogram where they are called an acoustic feature generator and a neural vocoder respectively. 
However, their training pipeline is somewhat cumbersome in that it requires a fine-tuning and an accurate speech-text alignment for optimal performance. 
In this work, we present end-to-end text-to-speech (E2E-TTS) model which has a simplified training pipeline and outperforms a cascade of separately learned models. 
Specifically, our proposed model is jointly trained [FastSpeech2](../TTS2_Acoustic/2020.06.08_FastSpeech2.md) and [HiFi-GAN](../TTS3_Vocoder/2020.10.12_HiFi-GAN.md) with an alignment module. 
Since there is no acoustic feature mismatch between training and inference, it does not requires fine-tuning. 
Furthermore, we remove dependency on an external speech-text alignment tool by adopting an alignment learning objective in our joint training framework. 
Experiments on LJSpeech corpus shows that the proposed model outperforms publicly available, state-of-the-art implementations of ESPNet2-TTS on subjective evaluation (MOS) and some objective evaluations.

## 1.Introduction: 引言

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
