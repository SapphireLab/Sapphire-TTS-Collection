# EELE


<details>
<summary>基本信息</summary>

- 标题: EELE: Exploring Efficient and Extensible LoRA Integration in Emotional Text-to-Speech
- 作者:
  - 01 [Xin Qi](../../Authors/Xin_Qi.md)
  - 02 [Ruibo Fu](../../Authors/Ruibo_Fu.md)
  - 03 [Zhengqi Wen](../../Authors/Zhengqi_Wen.md)
  - 04 [Jianhua Tao](../../Authors/Jianhua_Tao.md)
  - 05 [Shuchen Shi](../../Authors/Shuchen_Shi.md)
  - 06 [Yi Lu](../../Authors/Yi_Lu.md)
  - 07 [Zhiyong Wang](../../Authors/Zhiyong_Wang.md)
  - 08 [Xiaopeng Wang](../../Authors/Xiaopeng_Wang.md)
  - 09 [Yuankun Xie](../../Authors/Yuankun_Xie.md)
  - 10 [Yukun Liu](../../Authors/Yukun_Liu.md)
  - 11 [Guanjun Li](../../Authors/Guanjun_Li.md)
  - 12 [Xuefei Liu](../../Authors/Xuefei_Liu.md)
  - 13 [Yongwei Li](../../Authors/Yongwei_Li.md)
- 机构:
  - 机构 
- 时间:
  - 预印时间: 2024.08.20 ArXiv v1
  - 更新笔记: 2024.08.21
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2408.10852)
  - [DOI]()
  - [Github]()
  - [Demo]()
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: ?
- 引用: ?
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> In the current era of Artificial Intelligence Generated Content (AIGC), a Low-Rank Adaptation (LoRA) method has emerged. It uses a plugin-based approach to learn new knowledge with lower parameter quantities and computational costs, and it can be plugged in and out based on the specific sub-tasks, offering high flexibility. However, the current application schemes primarily incorporate LoRA into the pre-introduced conditional parts of the speech models. This fixes the position of LoRA, limiting the flexibility and scalability of its application. 
> Therefore, we propose the Exploring Efficient and Extensible LoRA Integration in Emotional Text-to-Speech (EELE) method. Starting from a general neutral speech model, we do not pre-introduce emotional information but instead use the LoRA plugin to design a flexible adaptive scheme that endows the model with emotional generation capabilities. Specifically, we initially train the model using only neutral speech data. After training is complete, we insert LoRA into different modules and fine-tune the model with emotional speech data to find the optimal insertion scheme. Through experiments, we compare and test the effects of inserting LoRA at different positions within the model and assess LoRA's ability to learn various emotions, effectively proving the validity of our method. Additionally, we explore the impact of the rank size of LoRA and the difference compared to directly fine-tuning the entire model.

在当前人工智能生成内容（AIGC）时代，出现了一种低阶自适应（LoRA）方法。它使用基于插件的方法以较低的参数量和计算成本学习新知识，并且可以根据特定的子任务插入和插入，提供了很高的灵活性。然而，当前的应用方案主要将LoRA合并到预先引入的语音模型的条件部分中。这固定了LoRA的位置，限制了其应用程序的灵活性和可扩展性。因此，我们提出了在情感文本到语音（EELE）方法中探索有效且可扩展的LoRA集成。从一般的中性语音模型开始，我们不预先引入情感信息，而是使用LoRA插件来设计一个灵活的自适应方案，赋予模型情感生成能力。具体来说，我们最初只使用中性语音数据来训练模型。训练完成后，我们将LoRA插入到不同的模块中，并用情感语音数据对模型进行微调，以找到最佳插入方案。通过实验，我们比较和测试了在模型内不同位置插入LoRA的效果，并评估了LoRA学习各种情绪的能力，有效地证明了我们方法的有效性。此外，我们还探讨了LoRA的秩大小的影响以及与直接微调整个模型相比的差异。

## 1.Introduction: 引言

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论
