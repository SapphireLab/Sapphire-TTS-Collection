# EATS

<details>
<summary>基本信息</summary>

- 标题: "End-to-End Adversarial Text-to-Speech"
- 作者:
  - 01 Jeff Donahue
  - 02 Sander Dieleman
  - 03 Mikolaj Binkowski
  - 04 Erich Elsen
  - 05 Karen Simonyan
- 链接:
  - [ArXiv](https://arxiv.org/abs/2006.03575)
  - [Publication](https://openreview.net/forum?id=rsf1z-JSj87) ICLR2021Oral
  - [Github]()
  - [Demo](https://deepmind.com/research/publications/End-to-End-Adversarial-Text-to-Speech)
- 文件:
  - [ArXiv](_PDF/2006.03575v3__EATS__End-to-End_Adversarial_Text-to-Speech.pdf)
  - [Publication](_PDF/2006.03575p0__EATS__ICLR2021.pdf)

</details>

## Abstract: 摘要

<table><tr><td width="50%">

Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest.
In this work, we take on the challenging task of learning to synthesize speech from normalized text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs.
Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction.
It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram.
To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss.
The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.

</td><td>

现代文本转语音合成流程通常涉及多个处理阶段, 每个阶段都单独设计或学习.
在本工作中, 我们考虑从标准化文本或因素以端到端的方式学习合成语音的挑战性任务, 得到的模型可以直接在字符或因素输入序列上操作, 并产生原始语音音频输出.
我们提出的生成器是前馈的, 因此对于训练和推理都很高效, 使用基于 Token 长度预测的可微分的对齐方案.
它通过对抗反馈和预测损失的组合来学习生成高保真音频, 损失约束生成的音频粗略匹配真实音频的总时长和梅尔频谱图.
为了使得模型能够捕获生成音频中的时序变化, 我们在基于频谱图的预测损失中采用**软动态时间规整 (Soft Dynamic Time Warping, Soft DTW)**.
最终模型达到了 4 分的 MOS, 与依赖于多阶段训练和额外监督的 SoTA 模型相当.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

</td></tr></table>

## 3·Methodology: 方法

<table><tr><td width="50%">

</td></tr></table>

## 4·Experiments: 实验

<table><tr><td width="50%">

</td></tr></table>

## 5·Results: 结果

<table><tr><td width="50%">

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

</td></tr></table>
