# SSL-TTS

<details>
<summary>基本信息</summary>

- 标题: SSL-TTS: Leveraging Self-Supervised Embeddings and kNN Retrieval for Zero-Shot Multi-speaker TTS
- 作者:
  - 01 [Karl El Hajal](../../Authors/Karl_El_Hajal.md)
  - 02 [Ajinkya Kulkarni](../../Authors/Ajinkya_Kulkarni.md)
  - 03 [Enno Hermann](../../Authors/Enno_Hermann.md)
  - 04 [Mathew Magimai.-Doss](../../Authors/Mathew_Magimai.-Doss.md)
- 机构:
  - 机构 
- 时间:
  - 预印时间: 2024.08.20 ArXiv v1
  - 更新笔记: 2024.08.24
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2408.10771)
  - [DOI]()
  - [Github]()
  - [Demo](https://idiap.github.io/ssl-tts/)
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: 5
- 引用: 38
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

While recent zero-shot multispeaker text-to-speech (TTS) models achieve impressive results, they typically rely on extensive transcribed speech datasets from numerous speakers and intricate training pipelines. 
Meanwhile, self-supervised learning (SSL) speech features have emerged as effective intermediate representations for TTS. 
It was also observed that SSL features from different speakers that are linearly close share phonetic information while maintaining individual speaker identity, which enables straight-forward and robust voice cloning. 
In this study, we introduce SSL-TTS, a lightweight and efficient zero-shot TTS framework trained on transcribed speech from a single speaker. 
SSL-TTS leverages SSL features and retrieval methods for simple and robust zero-shot multi-speaker synthesis. 
Objective and subjective evaluations show that our approach achieves performance comparable to state-of-the-art models that require significantly larger training datasets. 
The low training data requirements mean that SSL-TTS is well suited for the development of multi-speaker TTS systems for low-resource domains and languages. 
We also introduce an interpolation parameter which enables fine control over the output speech by blending voices. 
Demo samples are available at https://idiap.github.io/ssl-tts/

## 1.Introduction: 引言

Neural text-to-speech (TTS) synthesis has advanced significantly in recent years, achieving a level of naturalness comparable to human speech and allowing for an increasingly expressive range of outputs ([Survey [1]](../../Surveys/2021.06.29_A_Survey_on_Neural_Speech_Synthesis_63P/_ToC.md); [VITS [2]](2021.06.11_VITS.md); [NaturalSpeech2 [3]](../Diffusion/2023.04.18_NaturalSpeech2.md); [StyleTTS2 [4]](../Speech_LLM/2023.06.13_StyleTTS2.md)).
Neural TTS systems can be categorized into two-stage and single-stage pipelines. 
Two-stage models convert phonemic features into acoustic features and then use a vocoder to generate waveforms. 
These models can suffer from error propagation and limitations due to their dependence on low-level features like mel-spectrograms ([Glow-TTS [5]](../TTS2_Acoustic/2020.05.22_Glow-TTS.md); [FastPitch [6]](../TTS2_Acoustic/2020.06.11_FastPitch.md); [Tacotron2 [7]](../TTS2_Acoustic/2017.12.16_Tacotron2.md)).
Single-stage models aim to address these issues by streamlining this process into an end-to-end framework ([VITS [2]](2021.06.11_VITS.md); [YourTTS [8]](2021.12.04_YourTTS.md); [HierSpeech [9]](2022.11.01_HierSpeech.md)), but they may face oversmoothing, mispronunciations, and reduced flexibility due to the lack of explicit linguistic information and entangled latent representations ([HierSpeech [9]](2022.11.01_HierSpeech.md); choi2023nansy).
Recent research combines the strengths of both approaches by using self-supervised learning (SSL) speech representations as intermediate elements in two-stage models \cite{choi2023nansy, wavthruvec, parrottts, comparative_study_ssl_tts}. 
These representations help improve word error rates, pronunciation of out-of-vocabulary words \cite{wavthruvec}, and robustness to noise \cite{zhu2023rep2wav}, leading to high-quality, natural speech often surpassing end-to-end models.
>
In practice, end-user applications may need multiple voices. 
Collecting high quality speech data and building a TTS model for each target speaker/voice is a challenging problem. 
So, there has been a growing interest in zero-shot multi-speaker TTS systems which can synthesize speech in an unseen speaker's voice based on short reference samples. 
State-of-the-art models such as [XTTS [15]](../Speech_LLM/2024.06.07_XTTS.md) and [HierSpeech++ [16]](../Speech_Neural_Codec/2023.11.21_HierSpeechpp.md) demonstrate impressive quality and similarity to unseen speakers. 
However, these models require end-to-end training on thousands of hours of transcribed audio data from a large number of speakers to generalize effectively.
>
Simultaneously, kNN-VC \cite{baas23_interspeech} has emerged as a promising any-to-any voice conversion method, leveraging SSL features for zero-shot conversion. 
It uses a kNN algorithm to match frames from the source speaker with the target speaker's representations, adjusting the speaker identity while preserving speech content. 
This approach is similar to retrieval-augmented generation (RAG) techniques used in deep generative models such as language models \cite{khandelwal2020knnlm} and image generators \cite{chen2022reimagen}. 
These methods can enhance accuracy, reliability, and enable style transfer by steering model outputs to mirror characteristics of a retrieval database \cite{chen2022reimagen, borgeaud2022improving}.
>
Building on these insights, we propose SSL-TTS, a lightweight framework for multi-speaker zero-shot TTS that leverages SSL features encapsulating both speaker and linguistic information.
In the first stage of the framework, text is converted to SSL embeddings, which are then matched with corresponding features from a target speaker by exploiting their linear relationship. 
Similar to kNN-VC, this modifies the target voice in a non-parametric manner and obviates the need for multi-speaker transcribed data for training. 
A waveform is finally generated from the converted features using a pre-trained vocoder. 
The only component that requires training is the Text-to-SSL model, which can be trained on transcribed data from a single speaker only. 
Furthermore, we introduce a linear interpolation parameter allowing for fine-grained control over the influence of the target style on the output.
We validate the approach by implementing the Text-to-SSL block using two different lightweight models, namely [Glow-TTS [5]](../TTS2_Acoustic/2020.05.22_Glow-TTS.md) and [GradTTS [21]](../TTS2_Acoustic/2021.05.13_Grad-TTS.md). 
We train them on transcribed speech from a single speaker, and compare them with state-of-the-art zero-shot multi-speaker models using objective metrics and subjective listening tests. 
The code and trained models will be made public upon publication, demo samples are available at https://idiap.github.io/ssl-tts/
>
This paper is organized as follows. 
The next section introduces the proposed framework. 
Section 3 details the implementation and evaluation setup. 
Section 4 presents and analyzes the results. 
Section 5 offers a discussion and concludes. 

## 2.Related Works: 相关工作

## 3.Methodology: 方法

The SSL-TTS framework, illustrated in Fig. \ref{fig:model_architecture}, begins with a Text-to-SSL model that generates source speaker features from text input. 
A kNN retrieval algorithm then matches these generated features to units in a target speaker's unit database, which contains features pre-extracted from the target speaker's recordings using a pre-trained, general-purpose SSL encoder. 
The selected target speaker features are linearly interpolated with the source speaker features to obtain the converted features. 
Finally, these converted features are decoded back into a waveform using a pre-trained vocoder.
>
**SSL encoder**: For this framework, we need an intermediate audio representation that meets the following criteria: 
(1) the features should encompass both linguistic and speaker-specific information; 
(2) features that are linearly close should exhibit similar phonetic properties while preserving speaker identity; 
(3) it should be possible to decode the features back to waveform without loss of information. 

Recent observations indicate that self-supervised models encode speech into general representations that meet these criteria \cite{dunbar_2022}. 
Therefore, such SSL representations are suitable for this framework.
>
**Text-to-SSL**: We train a Text-to-SSL model that generates corresponding SSL features from a given text input. 
Notably, this is the only component of our framework that requires audio data paired with text transcriptions for training. 
It is sufficient to train this model on the speech of a single speaker. 
>
**kNN**: To synthesize speech in a target speaker's voice, units (or frames) from the target speaker unit database are selected to replace corresponding frames from the source speaker features. 
The selection is performed by comparing source and target frames in terms of a linear distance metric. 
This process results in selected target speaker features which maintain the phonetic information while replacing the voice characteristics with those of the target speaker.
>
The source and target speaker features are then linearly interpolated with each other to obtain the converted features~\cite{khandelwal2020knnlm}. 
We use a variable parameter $\lambda$ which can be modified to change the degree of influence the target features have on the output. 
This allows for creating a blend of the source and target styles. 

$$
{y}_{\mathrm{converted}} = \lambda  \  {y}_{\mathrm{selected}} + (1-\lambda) \ {y}_{\mathrm{source}} 
$$

**Vocoder**: We employ a vocoder capable of decoding the SSL features back into a waveform. 
To ensure robust generalization, the vocoder should be pre-trained on a large and diverse dataset to maintain high-quality waveform reconstruction across different speakers and contexts. 

## 4.Experiments: 实验

### Model implementation

**SSL encoder**: We employ a pre-trained WavLM-Large encoder from [WavLM [23]](../Speech_Representaion/2021.10.26_WavLM.md) to derive representations from speech utterances. 
WavLM-Large fits the requirements we presented, and is specifically selected due to its effective audio reconstruction capabilities, obtained through training on masked speech denoising and prediction tasks \cite{wang23_ssw}.
Specifically, we utilize features extracted from the 6th layer of the model, which encapsulate both phonetic and speaker characteristics~\cite{baas23_interspeech,wang23_ssw}. 
Each utterance yields $T \times 1024$ features, where $T$ is the sequence length. 
Each frame represents a 25ms window with a 20ms hop between frames. 
These features are pre-extracted and cached prior to training or inference, eliminating the need to load the WavLM model during training or synthesis, assuming the target speaker is known.
>
**Text-to-SSL**: We evaluate two Text-to-SSL implementations: [Glow-TTS [5]](../TTS2_Acoustic/2020.05.22_Glow-TTS.md) and [GradTTS [21]](../TTS2_Acoustic/2021.05.13_Grad-TTS.md). 
GlowTTS employs a non-autoregressive architecture with a transformer-based text encoder, a duration predictor, and a flow-based decoder \cite{kingma2018glow}. 
GradTTS follows a similar architecture but uses a diffusion-based decoder \cite{song2021scorebased}. 
These models, which were originally designed to predict mel-spectrograms, are trained to encode text inputs into WavLM features corresponding to a single speaker's speech. 
We maintain the models' default configurations and only adjust their output dimension to 1024 channels to align with WavLM-Large features. 
For the GradTTS diffusion decoder, we use 100 iterations for synthesis. 
Both models are trained on the [LJSpeech dataset [27]](../../Datasets/2017.07.05_LJSpeech.md), which comprises 24~hours of single-speaker English speech. 
GlowTTS is trained for 650k steps, and GradTTS for 2M steps.
>
**kNN**: For target speaker feature retrieval, we perform a kNN algorithm similar to \cite{baas23_interspeech}. 
For each source frame, we compute its cosine distance with every target speaker frame within the unit database. 
We then select the $k$ closest units, and average them with uniform weighting.
We use $k = 4$ which was determined to be suitable across different amounts of target audio \cite{baas23_interspeech}.
>
**Vocoder**: We use a pre-trained [HiFi-GAN [28]](../TTS3_Vocoder/2020.10.12_HiFi-GAN.md) V1 model trained to reconstruct 16kHz waveforms from WavLM-Large layer 6 features. 
The model checkpoint, sourced from \cite{baas23_interspeech}, was trained on LibriSpeech train-clean-100 set which encompasses 100 hours of clean English speech, with 251 speakers, and 25 minutes of data per speaker. 
The prematched paradigm is employed, where the training set is reconstructed by selecting k-nearest neighbors for each utterance and training the vocoder to predict original waveforms from these prematched features.

### Baselines

We compare our models with the best performing open models for zero-shot multi-speaker TTS.
>
[YourTTS [8]](2021.12.04_YourTTS.md) builds on [VITS [2]](2021.06.11_VITS.md), adding elements for multilingual training and zero-shot multi-speaker capabilities. 
It uses the H/ASP speaker encoder \cite{chung2020in}, pre-trained on the VoxCeleb2 dataset \cite{chung18b_interspeech}, to extract a speaker embedding from reference utterances. 
This embedding conditions the model's duration predictor, flow-based decoder, posterior encoder, and vocoder. 
YourTTS is trained end-to-end on 529 hours of multilingual transcribed data from over 1000 speakers.
>
[XTTS [15]](../Speech_LLM/2024.06.07_XTTS.md) features a Vector Quantised-Variational AutoEncoder (VQ-VAE) that encodes mel-spectrograms into discrete codes, a GPT-2 encoder that predicts these audio codes from text tokens, and a HiFi-GAN-based decoder. 
The GPT-2 encoder is conditioned on speaker information using a Perceiver conditioner, which outputs 32 1024-dimensional embeddings from a mel-spectrogram. 
The decoder is also conditioned on a speaker embedding extracted using H/ASP. 
XTTS is trained end-to-end on 27,282 hours of transcribed speech data across 16 languages, including 14,513 hours of English speech.

[HierSpeech++ [16]](../Speech_Neural_Codec/2023.11.21_HierSpeechpp.md) comprises a text-to-vec module and a hierarchical speech synthesizer. 
The text-to-vec module generates massively multilingual speech (MMS) representations \cite{pratap2024scaling} from text inputs and prosody prompts. 
The hierarchical speech synthesizer produces a waveform from MMS features and a style prompt. 
Prosody and voice style representations are extracted from reference mel-spectrograms using style encoders comprising 1D convolutional networks, a multi-head self-attention temporal encoder, and a linear projection. 
HierSpeech++ is trained end-to-end on 2796 hours of transcribed English and Korean speech, encompassing 7299 speakers.

### Evaluation

We use the default checkpoints and configurations provided by the authors for each baseline model
YourTTS, XTTS: https://github.com/idiap/coqui-ai-TTS.
HierSpeech++: https://github.com/sh-lee-prml/HierSpeechpp. 

Since these models employ various speaker encoders to convert a reference utterance into a style embedding, we ensure a fair comparison by averaging the embeddings across all reference utterances for each target speaker. 
For zero-shot multi-speaker synthesis comparisons, we use the LibriSpeech test-clean dataset for target speaker reference utterances. 
It includes speech of varied quality from 20 male and 20 female speakers, with 8~minutes of speech per speaker. 
For each model, we synthesize 100 English sentences per speaker, selecting the sentences randomly from the FLoRes+ dataset \cite{costa2022no}, in accordance with the XTTS protocol. 
Tests are performed with $\lambda=1$.
>
**Objective analysis**: we evaluate each model's performance in terms of naturalness using [UTMOS [33]](../../Evaluations/UTMOS.md), intelligibility using the word error rate (WER) and phoneme error rate (PER) computed with the [Whisper [34]](../Speech_LLM/2022.12.06_Whisper.md)-Large v3 model, and speaker similarity using speaker encoder cosine similarity (SECS) with the ECAPA2 model \cite{thienpondt2023ecapa2}.
>
**Subjective evaluation**: we conduct a listening test to assess naturalness and similarity mean opinion scores (N-MOS and S-MOS). 
We randomly select utterances from 10 male and 10~female target speakers in the LibriSpeech test-clean dataset, choosing 3 synthesized sentences per speaker, totaling 60~utterances per model. 
Each utterance is rated by 10 raters on naturalness and similarity to a ground-truth recording, with scores ranging from 1 to 5 in 0.5 increments. 
We use Amazon Mechanical Turk, with raters required to be native English speakers based in the United States, having a HIT acceptance rate above 98\% and more than 500 approved HITs. 
Attention checks are implemented to filter out unreliable ratings.
>
**Model efficiency**: we compare the models based on the number of parameters, peak GPU memory usage during test sample synthesis, and real-time factor (RTF). 
These tests are performed using an NVIDIA RTX3090 GPU.
>
**Controllability**: to showcase this aspect of the framework, we perform an experiment using the interpolation parameter, computing the SECS of the model's output with the target speaker's ground truth data for various values of $\lambda$.

## 5.Results: 结果

Results are presented in Table \ref{tab:model_comp}. 
Objective metrics reveal that the SSL-TTS models demonstrate the best speaker similarity, XTTS excels in intelligibility, and HierSpeech++ achieves the highest naturalness. 
Subjective evaluations show that listeners rated HierSpeech++ highest for naturalness and similarity, while the SSL-TTS models and XTTS performed similarly. 
These models' results fall within each other's confidence intervals, suggesting comparable performance.
Regarding model efficiency, SSL-TTS models have the fewest parameters and lowest memory requirements among the top performers. 
Notably, GlowTTS-SSL requires $3\times$ less memory than HierSpeech++ with comparable speed. 
GradTTS-SSL's memory usage and RTF are higher due to the 100 iterations used in the diffusion decoder. 
Further, the SSL-TTS models are trained on 100$\times$ less transcribed data than HierSpeech++ and 1000$\times$ less data than XTTS.
>
The speaker similarity matrix (Figure \ref{fig:sim_matrix}) illustrates the results of the controllability experiment. 
We can observe that the similarity of the outputs to the target speaker gradually increases as $\lambda$ rises.
This demonstrates the framework's ability to blend source and target styles in a fine-grained manner and suggests the potential to combine multiple target styles.
>
We conduct ablation studies to evaluate the models' outputs with varying amounts of reference utterances. 
Figure \ref{fig:ljspeech_plot} compares outputs using kNN retrieval from different amounts of LJSpeech data. 
We find that approximately 30 seconds of reference utterances are needed to achieve suitable intelligibility, while naturalness improves up to 5 minutes, surpassing the model outputs without kNN retrieval. 
Figure \ref{fig:libri4077_plot} compares the SSL-TTS models to the baselines for different amounts of reference utterances from a target speaker. 
Similarly, about 30~seconds are required for suitable intelligibility, while similarity plateaus at around 1 minute. 
In contrast, the baselines benefit less from increasing the amount of reference utterances beyond 10 to 30 seconds.

## 6.Conclusions: 结论

State-of-the-art zero-shot multi-speaker TTS models rely on large datasets of transcribed speech from thousands of speakers for training. 
In this paper, we demonstrated that by combining SSL features and kNN retrieval methods, we can develop a lightweight TTS system that achieves a comparable level of naturalness and similarity to other approaches while requiring transcribed data from only a single speaker. 
However, there is a trade-off, with the other approaches performing better with just a few seconds of reference audio, while our method needs at least 30 seconds of reference audio, which is still a manageable requirement. 
We further showed that fine-grained control over the influence of the target style on the output can be achieved using an interpolation parameter. 
This indicates that this technique, which is originally inspired from other domains such as language modeling~\cite{khandelwal2020knnlm} and machine translation \cite{khandelwal2021knnmt}, also applies to TTS.
>
The simplicity of the training process is one of the main advantages of our approach, where only the Text-to-SSL model requires training, and it can be trained on transcribed data from a single speaker. 
This simplicity, in conjunction with the kNN approach's cross-lingual capability \cite{knnvc_followup}, is particularly appealing for extending the model to new languages and domains with fewer resources. 
This aspect is open for future work.
We also showed that the framework can be implemented using different Text-to-SSL model architectures, allowing for model swapping to leverage different benefits. 
Our implementations notably demonstrated efficiency in terms of parameters, memory usage, and runtime speed in the case of GlowTTS-SSL, even without optimizing the retrieval process. 
>
Typically, different speakers exhibit different pronunciation durations. 
In our framework, the duration aspect is determined by the Text-to-SSL model, and the target voice is modified through frame-by-frame selection, meaning that the duration of each utterance remains unchanged for different speakers. 
Interestingly, despite this limitation, the SSL-TTS models were rated comparably to other approaches in terms of similarity. 
Our future work will explore techniques, such as Urythmic~\cite{niekerk2023}, to address this limitation.
