# SSL-TTS

<details>
<summary>基本信息</summary>

- 标题: SSL-TTS: Leveraging Self-Supervised Embeddings and kNN Retrieval for Zero-Shot Multi-speaker TTS
- 作者:
  - Karl El Hajal
  - Ajinkya Kulkarni
  - Enno Hermann
  - Mathew Magimai.-Doss
- 机构:
  - 机构 
- 时间:
  - 预印时间: 2024.08.20 ArXiv v1
  - 更新笔记: 2024.08.24
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2408.10771)
  - [DOI]()
  - [Github]()
  - [Demo](https://idiap.github.io/ssl-tts/)
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: ?
- 引用: ?
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

> While recent zero-shot multispeaker text-to-speech (TTS) models achieve impressive results, they typically rely on extensive transcribed speech datasets from numerous speakers and intricate training pipelines. Meanwhile, self-supervised learning (SSL) speech features have emerged as effective intermediate representations for TTS. It was also observed that SSL features from different speakers that are linearly close share phonetic information while maintaining individual speaker identity, which enables straight-forward and robust voice cloning. In this study, we introduce SSL-TTS, a lightweight and efficient zero-shot TTS framework trained on transcribed speech from a single speaker. SSL-TTS leverages SSL features and retrieval methods for simple and robust zero-shot multi-speaker synthesis. Objective and subjective evaluations show that our approach achieves performance comparable to state-of-the-art models that require significantly larger training datasets. The low training data requirements mean that SSL-TTS is well suited for the development of multi-speaker TTS systems for low-resource domains and languages. We also introduce an interpolation parameter which enables fine control over the output speech by blending voices. 
> Demo samples are available at https://idiap.github.io/ssl-tts/

## 1.Introduction: 引言

> Neural text-to-speech (TTS) synthesis has advanced significantly in recent years, achieving a level of naturalness comparable to human speech and allowing for an increasingly expressive range of outputs \cite{tan2021survey, vits, shen2024naturalspeech, styletts2}.
> Neural TTS systems can be categorized into two-stage and single-stage pipelines. Two-stage models convert phonemic features into acoustic features and then use a vocoder to generate waveforms. These models can suffer from error propagation and limitations due to their dependence on low-level features like mel-spectrograms \cite{glowtts, fastpitch, tacotron2}.
> Single-stage models aim to address these issues by streamlining this process into an end-to-end framework \cite{vits, pmlr-v162-casanova22a, lee2022hierspeech}, but they may face oversmoothing, mispronunciations, and reduced flexibility due to the lack of explicit linguistic information and entangled latent representations \cite{lee2022hierspeech, choi2023nansy}.
> Recent research combines the strengths of both approaches by using self-supervised learning (SSL) speech representations as intermediate elements in two-stage models \cite{choi2023nansy, wavthruvec, parrottts, comparative_study_ssl_tts}. These representations help improve word error rates, pronunciation of out-of-vocabulary words \cite{wavthruvec}, and robustness to noise \cite{zhu2023rep2wav}, leading to high-quality, natural speech often surpassing end-to-end models.
>
> In practice, end-user applications may need multiple voices. Collecting high quality speech data and building a TTS model for each target speaker/voice is a challenging problem. So, there has been a growing interest in zero-shot multi-speaker TTS systems which can synthesize speech in an unseen speaker's voice based on short reference samples. State-of-the-art models such as XTTS \cite{casanova2024xtts} and HierSpeech++~\cite{lee2023hierspeechbridginggapsemantic} demonstrate impressive quality and similarity to unseen speakers. However, these models require end-to-end training on thousands of hours of transcribed audio data from a large number of speakers to generalize effectively.
>
> Simultaneously, kNN-VC \cite{baas23_interspeech} has emerged as a promising any-to-any voice conversion method, leveraging SSL features for zero-shot conversion. It uses a kNN algorithm to match frames from the source speaker with the target speaker's representations, adjusting the speaker identity while preserving speech content. This approach is similar to retrieval-augmented generation (RAG) techniques used in deep generative models such as language models \cite{khandelwal2020knnlm} and image generators \cite{chen2022reimagen}. These methods can enhance accuracy, reliability, and enable style transfer by steering model outputs to mirror characteristics of a retrieval database \cite{chen2022reimagen, borgeaud2022improving}.
>
> Building on these insights, we propose SSL-TTS, a lightweight framework for multi-speaker zero-shot TTS that leverages SSL features encapsulating both speaker and linguistic information.
> In the first stage of the framework, text is converted to SSL embeddings, which are then matched with corresponding features from a target speaker by exploiting their linear relationship. Similar to kNN-VC, this modifies the target voice in a non-parametric manner and obviates the need for multi-speaker transcribed data for training. A waveform is finally generated from the converted features using a pre-trained vocoder. The only component that requires training is the Text-to-SSL model, which can be trained on transcribed data from a single speaker only. Furthermore, we introduce a linear interpolation parameter allowing for fine-grained control over the influence of the target style on the output.
> We validate the approach by implementing the Text-to-SSL block using two different lightweight models, namely GlowTTS \cite{glowtts} and GradTTS \cite{pmlr-v139-popov21a-gradtts}. We train them on transcribed speech from a single speaker, and compare them with state-of-the-art zero-shot multi-speaker models using objective metrics and subjective listening tests. 
> The code and trained models will be made public upon publication, demo samples are available at https://idiap.github.io/ssl-tts/
>
> This paper is organized as follows. The next section introduces the proposed framework. Section 3 details the implementation and evaluation setup. Section 4 presents and analyzes the results. Section 5 offers a discussion and concludes. 

## 2.Related Works: 相关工作

## 3.Methodology: 方法

## 4.Experiments: 实验

## 5.Results: 结果

## 6.Conclusions: 结论

> State-of-the-art zero-shot multi-speaker TTS models rely on large datasets of transcribed speech from thousands of speakers for training. In this paper, we demonstrated that by combining SSL features and kNN retrieval methods, we can develop a lightweight TTS system that achieves a comparable level of naturalness and similarity to other approaches while requiring transcribed data from only a single speaker. However, there is a trade-off, with the other approaches performing better with just a few seconds of reference audio, while our method needs at least 30 seconds of reference audio, which is still a manageable requirement. We further showed that fine-grained control over the influence of the target style on the output can be achieved using an interpolation parameter. This indicates that this technique, which is originally inspired from other domains such as language modeling~\cite{khandelwal2020knnlm} and machine translation \cite{khandelwal2021knnmt}, also applies to TTS.
>
> The simplicity of the training process is one of the main advantages of our approach, where only the Text-to-SSL model requires training, and it can be trained on transcribed data from a single speaker. This simplicity, in conjunction with the kNN approach's cross-lingual capability \cite{knnvc_followup}, is particularly appealing for extending the model to new languages and domains with fewer resources. This aspect is open for future work.
> We also showed that the framework can be implemented using different Text-to-SSL model architectures, allowing for model swapping to leverage different benefits. Our implementations notably demonstrated efficiency in terms of parameters, memory usage, and runtime speed in the case of GlowTTS-SSL, even without optimizing the retrieval process. 
>
> Typically, different speakers exhibit different pronunciation durations. In our framework, the duration aspect is determined by the Text-to-SSL model, and the target voice is modified through frame-by-frame selection, meaning that the duration of each utterance remains unchanged for different speakers. Interestingly, despite this limitation, the SSL-TTS models were rated comparably to other approaches in terms of similarity. Our future work will explore techniques, such as Urythmic~\cite{niekerk2023}, to address this limitation.
