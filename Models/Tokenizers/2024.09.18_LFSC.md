# LFSC (Low Frame-rate Speech Codec)

<details>
<summary>基本信息</summary>

- 标题: "Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference"
- 作者:
  - 01 Edresson Casanova - NVIDIA Corporation
  - 02 Ryan Langman - NVIDIA Corporation
  - 03 Paarth Neekhara - NVIDIA Corporation
  - 04 Shehzeen Hussain - NVIDIA Corporation
  - 05 Jason Li - NVIDIA Corporation
  - 06 Subhankar Ghosh - NVIDIA Corporation
  - 07 Ante Jukic - NVIDIA Corporation
  - 08 Sang-gil Lee - NVIDIA Corporation
- 链接:
  - [ArXiv](https://arxiv.org/abs/2409.12117)
  - [Publication]() #TODO Submitted to ICASSP2025
  - [Github](https://github.com/NVIDIA/NeMo)
  - [Demo](https://edresson.github.io/Low-Frame-rate-Speech-Codec)
- 文件:
  - [ArXiv](_PDF/2409.12117v1__LFSC__Low_Frame-rate_Speech_Codec_A_Codec_Designed_for_Fast_High-Quality_Speech_LLM_Training_and_Inference.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Large language models (LLMs) have significantly advanced audio processing through audio codecs that convert audio into discrete tokens, enabling the application of language modeling techniques to audio data.
However, audio codecs often operate at high frame rates, resulting in slow training and inference, especially for autoregressive models.
To address this challenge, we present the ***Low Frame-rate Speech Codec (LFSC)***: a neural audio codec that leverages finite scalar quantization and adversarial training with large speech language models to achieve high-quality audio compression with a 1.89 kbps bitrate and 21.5 frames per second.
We demonstrate that our novel codec can make the inference of LLM-based text-to-speech models around three times faster while improving intelligibility and producing quality comparable to previous models.

</details>
<br>

大型语言模型通过将音频转换为离散 Tokens 的音频编解码器显著推进了音频处理技术, 使得语言建模技术能够应用于音频数据.
然而, 音频编解码器通常以高帧率运行, 导致训练和推理速度缓慢, 尤其是对于自回归模型.
为了应对这一挑战, 我们提出了 ***低帧率语音编解码器 (Low Frame-rate Speech Codec, LFSC)***: 一种利用有限标量量化和与大型语音语言模型进行对抗训练的神经音频编解码器, 以 1.89 kbps 的比特率和每秒 21.5 帧的速度实现高质量的音频压缩.
我们证明我们的新型编解码器可以使基于 LLM 的文本到语音模型的推理速度提高约三倍, 同时提高可理解性并产生与先前模型相媲美的质量.

## 1·Introduction: 引言

Audio codec is an important signal processing technique, that compresses audio signals into discrete codes and then uses these codes to reconstruct the original audio.
This technology has long held a central position in fields such as audio transmission and communication ([APCodec [1]](../SpeechCodec/2024.02.16_APCodec.md)).
Recently, it also has been applied to some downstream tasks.
For example, some researchers use the discrete codes generated by audio codecs combined with large language models (LLMs), to achieve impressive results in zero-shot text-to-speech (ZS-TTS)  ([AudioLM [2]](../SpeechLM/ST2S/2022.09.07_AudioLM.[VALL-E [3]](../SpeechLM/ST2S/2023.01.05_VALL-E.md)E.md); [SpeechTokenizer [4]](2023.08.31_SpeechTokenizer.md); [T5-TTS [5]](../SpeechLM/2024.06.25_T5-TTS.md)) and Speech-to-speech translation (S2ST) ([Textless Translatotron [6]](../S2ST/Textless_Translatotron.md); [UTUT [7]](../S2ST/UTUT.md); [Speech2S [8]](../S2ST/Speech2S.md)).

In recent years, Neural Audio Codecs (NACs) with raw waveform input and output have emerged, offering a balance between decoded audio quality and bitrate ([E2E Optimized Speech Coding with DNNs [9]](../_Full/2017.10.25_End-to-End_Optimized_Speech_Coding_with_DNNs.md); [VQ-VAE [10]](../../Modules/VQ/2017.11.02_VQ-VAE.md); [VQ-VAE+WaveNet [11]](VQ-VAE+WaveNet.md); [SoundStream [12]](../SpeechCodec/2021.07.07_SoundStream.md); [EnCodec [13]](../SpeechCodec/2022.10.24_EnCodec.md)).
For example, [SoundStream [12]](../SpeechCodec/2021.07.07_SoundStream.md), [EnCodec [13]](../SpeechCodec/2022.10.24_EnCodec.md), [DAC [14]](2023.06.11_Descript-Audio-Codec.md), and [APCodec [1]](../SpeechCodec/2024.02.16_APCodec.md) use the Residual Vector Quantization (RVQ) [Review [15]](../../Modules/VQ/A_Review_of_Vector_Quantization_Techniques.md) to encode audio at low bitrates while using losses from the [HiFi-GAN [16]](../Vocoder/2020.10.12_HiFi-GAN.md) vocoder to maintain audio fidelity.
Recent research has focused on improving existing NACs, particularly in the area of quantization strategies.
Researchers have worked on improving RVQ, as in [Language-Codec [17]](../SpeechCodec/2024.02.19_Language-Codec.md) and [Gull [18]](2024.04.07_Gull.md), or exploring new vector quantization approaches.
A notable example is the [Spectral Codec [19]](2024.06.07_Spectral_Codec.md), which encodes Mel-spectrograms using Finite Scalar Quantization (FSQ).
In [T5-TTS [5]](../SpeechLM/2024.06.25_T5-TTS.md), the authors applied a [Spectral Codec](2024.06.07_Spectral_Codec.md) to the TTS task, demonstrating that it can achieve high-quality audio without relying on techniques like delay patterns ([MusicGen [20]](../Music/2023.06.08_MusicGen.md)), which were necessary in previous RVQ-based codecs due to RVQ’s hierarchical architecture.
This feature is particularly beneficial for streaming, as it can significantly reduce latency for the prediction of the first audio chunk during inference.
Meanwhile, efforts have been made to introduce or disentangle semantic information during quantization to better suit specific tasks ([SpeechTokenizer [4]](2023.08.31_SpeechTokenizer.md); [TiCodec [21]](../Tokenizer/2023.09.15_TiCodec.md)).
Moreover, researchers have worked on improving model structures ([CBRC [22]](../SpeechCodec/2024.02.02_CBRC.md)), incorporating additional signal processing techniques in codecs ([Penguins [23]](Penguins.md)) and reducing the computational complexity ([LightCodec [24]](../SpeechCodec/2024.03.18_LightCodec.md)).

In parallel with our work, researchers have explored the NACs bitrate reduction ([APCodec_AP-BWE [25]](APCodec_AP-BWE.md); [WavTokenizer [26]](../Tokenizer/2024.08.29_WavTokenizer.md)).
In [APCodec_AP-BWE [25]](APCodec_AP-BWE.md), the authors integrated APCodec with a bandwidth extension model called [AP-BWE [27]](../_tmp/AP-BWE.md), which extends bandwidth from 8kHz to 48 kHz.
Their proposed model was able to achieve audio coding at 1 kbps bitrate and 25 frames per second (FPS).
[WavTokenizer [26]](../Tokenizer/2024.08.29_WavTokenizer.md) was able to encode audio at 0.9 kbps bitrate using a single codebook at 75 FPS, achieving audio quality comparable with previous SOTA NACs.

Despite these advancements, some recent works have overlooked the critical importance of low latency, which poses challenges for achieving real-time streamable inference ([HiFi-Codec [28]](2023.05.04_HiFi-Codec.md); [APCodec [1]](../SpeechCodec/2024.02.16_APCodec.md); [CBRC [22]](../SpeechCodec/2024.02.02_CBRC.md)).
For Speech LLMs, a key factor in achieving low latency is the number of audio frames produced by the codec per second of audio.
We use the term "Speech LLM" to denote models that integrate LLMs for speech tasks.

This is crucial because each frame requires the autoregressive model to perform a forward pass.
In scenarios where codes from multiple codebooks can be predicted in parallel, reducing the frame rate becomes even more important than lowering the bit rate.
However, most of the current literature focuses primarily on bitrate reduction, often neglecting the potential benefits of frame rate reduction.
For example, the SOTAs codecs DAC and [Spectral Codec](2024.06.07_Spectral_Codec.md) produce 86 FPS of audio making the inference of Speech LLM models trained with these codecs considerably slow because the Speech LLM needs to do at least 86 forward passes to predict one second of audio.
In this paper, we introduce the ***Low Frame-rate Speech Codec (LFSC)*** that reduces the frame rate by four times compared to [Spectral Codec](2024.06.07_Spectral_Codec.md) and DAC, achieving 21.5 FPS making it ideal for Speech LLM model training.

The key contributions of this work are as follows:

- We introduce ***Low Frame-rate Speech Codec***, a novel neural audio codec that compresses audio in 21.5 FPS with a bitrate of {1.89} kbps while maintaining high audio quality.
- To the best of our knowledge, we are the first to explore large speech language models as discriminators in NACs training, demonstrating significant improvements in low-frame rate scenarios.
- To demonstrate the effectiveness of our codec in delivering high-quality audio and reducing inference time for Speech LLMs, we trained and evaluated a state-of-the-art (SOTA) LLM-based TTS model, showcasing our codec's performance in practice.
- Our codec is publicly available in the [NeMo repository](https://github.com/NVIDIA/NeMo).

The audio samples for each of our experiments are available on the [demo website](https://edresson.github.io/Low-Frame-rate-Speech-Codec).

## ~~2·Related Works: 相关工作~~

## 3·Methodology: 方法

Low Frame-rate Speech Codec model is composed of a fully convolutional generator neural network and three discriminators.

The generator comprises an encoder, followed by vector quantization, and a [HiFi-GAN [16]](../Vocoder/2020.10.12_HiFi-GAN.md)-based decoder.
The encoder consists of five residual blocks, each block containing three residual layers similar to the multi-receptive field fusion (MRF) module proposed by [HiFi-GAN [16]](../Vocoder/2020.10.12_HiFi-GAN.md).
However, we use a dilation rate of 1 in the residual layers.
To reduce the frame rate to 21.5, each residual block is followed by a 1D convolutional layer with strides of [2, 2, 4, 8, 8] for the respective five blocks.
The decoder is based on the HiFi-GAN vocoder with upsampling rates of [8, 8, 4, 2, 2].
The encoder has 48 initial channels, which are doubled after each downsampling layer, while the decoder has 1024 initial channels, which are halved after each upsampling layer.
The encoder and decoder have 57.6M and 55.1M parameters, respectively.

For the discriminators, we utilize three neural networks, all employing a squared-GAN and feature-matching loss.
We adopt the multi-period discriminator proposed by [HiFi-GAN [16]](../Vocoder/2020.10.12_HiFi-GAN.md) and the multi-scale complex STFT discriminator proposed by [EnCodec [13]](../SpeechCodec/2022.10.24_EnCodec.md).
Additionally, inspired by [StyleTTS [29]](../TTS2_Acoustic/2022.05.30_StyleTTS.md), we proposed the use of Speech Language Models (SLMs) as a discriminator.
SLMs encode information ranging from acoustic to semantic aspects, which could benefit our model's training, especially in low frame rate settings where accurate pronunciation is difficult to achieve due to the high compression rate.
We adopted the 12-layer [WavLM [30]](../SpeechRepresentation/2021.10.26_WavLM.md), pre-trained on 94k hours of data, as the SLM.
During training, we resample the input audio to 16 kHz before feeding it into the WavLM model, extracting the intermediary layer features.
These features are then fed to a discriminative head composed of four 1D convolutional layers.
As in [WavLM [30]](../SpeechRepresentation/2021.10.26_WavLM.md), the SLM remains frozen during training.

For the vector quantization, we followed [Spectral Codec [19]](2024.06.07_Spectral_Codec.md) and used FSQ with eight codebooks and four dimensions per code.
However, due to the four times frame rate compression than [Spectral Codec [19]](2024.06.07_Spectral_Codec.md), we needed to increase the number of codes per codebook from 1000 to 2016\footnote{We have used codebook levels of [8, 7, 6, 6]}, for more details please check Section \ref{sec:ablations}. % We also found benefits in first training the model with FSQ disabled and then later fine-tuning the model with FSQ enabled, for more details please check Section \ref{sec:ablations}.

### Datasets

For codec training, we utilize the same strategy as [Spectral Codec [19]](2024.06.07_Spectral_Codec.md) to get 22.05kHz full-bandwidth audio data.
We run bandwidth estimation and apply a bandwidth filter of 11kHz to the English train subset of MLS, and to all languages of [Common Voice [31]](../../Datasets/CommonVoice.md) 13.
The Common Voice derived training set comprises 105 languages, totaling 2.7 million utterances, and 3.2k hours of audio from about one-hundred thousand speakers.
The MLS English training dataset consists of 6.2 million utterances and 25.5k hours of audio from 4329 speakers.

### Training Setup

We trained our codec in two phases.
First, we pre-trained the model with FSQ disabled, and then we fine-tuned it with FSQ enabled.
This approach was employed to accelerate the model's convergence when using different quantization techniques.
For both pre-training and fine-tuning, the model was trained for approximately 62,000 steps using 96 A100 GPUs with a batch size of 16.
The total accumulated batch size was 1,536, and the model processed roughly 95 million samples in each phase.
The training was conducted on 1.1-second audio excerpts.
We used the [Adam optimizer [32]](../../Modules/Optimization/2014.12.22_Adam.md) for both the generator and the discriminator, with $\beta_1 = 0.8$, $\beta_2 = 0.99$, and an initial learning rate of 2e-4, which decayed exponentially with a gamma of 0.998.

### Results & Discussion

We followed an evaluation strategy similar to [Spectral Codec [19]](2024.06.07_Spectral_Codec.md) and employed a combination of objective metrics.
For evaluating perceptual quality, we estimate Mean Opinion Scores (MOS) using [Torchaudio-Squim [33]](../_tmp/Torchaudio-Squim.md).
Time-domain accuracy is measured using [SI-SDR [34]](../../Evaluations/SI-SDR.md).
Spectral accuracy is assessed by calculating the L1 distance between log mel-spectrogram (Mel Dist.) and log magnitude STFT (STFT Dist.) features.
To measure the intelligibility of the codecs reconstruction we compute the character error rate (CER) between the **Whisper-Large V3** transcriptions of the ground truth audio and the reconstructed audio.

As the evaluation set, we reconstructed the [MLS [36]](../../Datasets/2020.12.07_MLS.md) dataset test set at a 44.1kHz sampling rate by redownloading the audiobooks and filtering out audio files with a bandwidth below 13kHz or a CER exceeding 10\%.
The CER was calculated using the **Whisper-Large V3**.
After filtering, we randomly selected 200 samples from each of the eight languages.
We chose the MLS test set for its multilingual nature and because we believe that these samples and speakers were not used in the training set of any of the evaluated codecs.
This choice is particularly relevant given that most contemporary Speech LLM models are primarily trained on audiobook-like data.
The dataset is available on our [demo page](https://edresson.github.io/Low-Frame-rate-Speech-Codec).

In addition, we assessed the models using the F10 and M10 speakers from the DAPS clear dataset, which had previously been used in the evaluation of the DAC model.
We included it to evaluate the models' performance on studio-quality audio. % Although this comparison may not be entirely fair, as the DAC model was trained on the DAPS dataset, we included it to evaluate the models' performance on studio-quality audio.

For evaluation, we selected three SOTA codecs that have been successfully applied in the training of Speech LLM models: [EnCodec [13]](../SpeechCodec/2022.10.24_EnCodec.md), [DAC [14]](2023.06.11_Descript-Audio-Codec.md), and [Spectral Codec [19]](2024.06.07_Spectral_Codec.md).
For Encodec and DAC, we utilized the publicly available checkpoints and we reduced the bitrate via codebook pruning, as done in the original papers.
For the [Spectral Codec](2024.06.07_Spectral_Codec.md), we used the same checkpoint employed in [T5-TTS [5]](../SpeechLM/2024.06.25_T5-TTS.md), as this model has already been evaluated in the context of Speech LLM model training.

Table \ref{tab:code-results} present the results of our evaluation on the 44.1kHz MLS test set and on F10 and M10 DAPS speakers.
For codecs operating at a sampling rate lower than 44.1kHz, the audio was resampled to match the target codec's sampling rate.
Additionally, during the computation of evaluation metrics, both the ground truth and reconstructed audio were downsampled to 22.05kHz to ensure a more fair comparison.

Our model achieved the highest perceptual quality score (Squim MOS) on the DAPS test set and the second-highest score on the MLS test set.
In terms of SI-SDR, our model outperformed the [Spectral Codec](2024.06.07_Spectral_Codec.md) across both evaluation sets and exceeded the performance of the 3kbps DAC on the DAPS test set.
However, it did not perform as well as the other models.
For Mel distance and STFT distance, our model demonstrated better results than both Encodec and the 3kbps DAC, although it performed slightly worse than the other models.
Regarding intelligibility (CER), our model surpassed the 3kbps DAC in both sets and outperformed Encodec and the 6kbps DAC in the CML test set.

These results suggest that our codec is competitive with SOTA codecs, despite its significantly lower bitrate and frame rate.
However, strong objective metrics alone do not guarantee superior performance in training Speech LLM models.
For instance, [T5-TTS [5]](../SpeechLM/2024.06.25_T5-TTS.md) demonstrated that an LLM-based TTS model trained using the [Spectral Codec](2024.06.07_Spectral_Codec.md) achieved better quality than the same model trained with a 7.75kbps DAC, even though the codec evaluation indicated that the 7.75kbps DAC outperformed the [Spectral Codec](2024.06.07_Spectral_Codec.md).
In Section \ref{sec:tts-exp}, we applied our codec in the training of a Speech LLM model and highlighted the advantages it offers over other codecs.

The DAC codec's balanced training across various bandwidths renders it robust and capable of producing high-quality output even with low-bandwidth audio.
We also evaluated the 7.75kbps DAC model using 22.05kHz audio resampled to 44.1kHz as input (resulting in a bandwidth of approximately 11kHz), on the MLS 44.1kHz test set.
When comparing DAC with real 44.1kHz input to the resampled 22.05kHz input, the latter approach yielded identical Squim MOS and Mel distance metrics, with slight improvements in SI-SDR (10.65 vs 10.54) and STFT distance (0.052 vs 0.054), though it exhibited a marginally higher CER (1.62 vs 1.57).
This feature is really interesting because it can be used to ensure a fair comparison with the [Spectral Codec](2024.06.07_Spectral_Codec.md) and our proposed codec, which were trained in 22.05 kHz audio.

### Ablation Study

We conduct an ablation study of our model, to show the effect of the WavLM-based discriminator and the codebook size of FSQ.
During the ablation study, we used the same steps and training approach described in Section \ref{sec:model}.
For the training without vector quantization, we have trained the model per 124k steps, which is the sum of the steps from the two training phases.

Table \ref{tab:ablations} presents our ablations study results.
The WavLM-based discriminator slightly improved the general quality (Squim MOS 4.43 vs 4.42).
It also enhanced speech intelligibility with a good margin (CER 2.09 vs 2.46).
However, it leads to a slightly worse time-domain accuracy (SI-SDR 5.04 vs 4.45) and STFT distance (0.061 vs 0.060).

The increase in codebook size from 1000 codes to 2016 produced the same score for general audio quality.
However, it improved the SI-SDR, STFT distance, and especially speech intelligibility (CER 2.61 vs 2.09).
Increasing the codebook size even more from 2016 to 4032 brings slightly better results for almost all the metrics.
In addition, using FSQ with 4032 codes our model was able to achieve the same general quality as the model trained without vector quantization and competitive performance in the other metrics.

## 4·ZS-TTS Study

### Experiments Setup

To evaluate how our codec performs in comparison to previous codecs in practice, we replicated the experiments outlined in [T5-TTS [5]](../SpeechLM/2024.06.25_T5-TTS.md).
We utilized the [T5-TTS [5]](../SpeechLM/2024.06.25_T5-TTS.md) model with context conditioning on the decoder due to its superior performance on the ZS-TTS task.

For training the TTS model, we employed the same datasets as those used in [T5-TTS [5]](../SpeechLM/2024.06.25_T5-TTS.md).
These datasets comprise 1.8k hours of English data from four sources: the train-clean 360 subset of [LibriTTS [37]](../../Datasets/2019.04.05_LibriTTS.md), [HiFiTTS [38]](../../Datasets/2021.04.03_Hi-Fi_TTS.md), a 1000-hour subset from [MLS [36]](../../Datasets/2020.12.07_MLS.md) train set, and a proprietary dataset featuring two speakers and totaling 63 hours.

The model was trained using three SOTA codecs: Encodec, DAC, [Spectral Codec](2024.06.07_Spectral_Codec.md), and two variants of our proposed codec.
Although models such as APCodec and WavTokenizer have recently demonstrated promising performance, they have not yet been explored in the context of Speech LLM training.
Considering that these codecs may require specific tricks to work properly in this context, we decided to focus on SOTA codecs that have been successfully applied in the training of Speech LLM models.

For training, we utilize a fixed context duration of three seconds, where context is an alternate utterance from the speaker of the target utterance.
We train each of our models with a batch size of 192 distributed across 32 NVIDIA A100 GPUs, for 300,000 steps.
The training is conducted using the AdamW optimizer with a fixed learning rate of 1e-4.
During inference, we use multinomial Top-k sampling with k equal to 80 and temperature equal to 0.85.

Following [T5-TTS [5]](../SpeechLM/2024.06.25_T5-TTS.md) and prior works, for the 24kHz 6kbps Encodec and 44.1kHz 7.75kbps DAC codecs, we utilized the delay pattern method ([MusicGen [20]](../Music/2023.06.08_MusicGen.md)) for the codec sequence.
For both the [Spectral Codec](2024.06.07_Spectral_Codec.md) and our proposed codec, we predicted all eight codes in parallel.
To ensure a fair comparison with the [Spectral Codec](2024.06.07_Spectral_Codec.md) and our proposed codec, and in accordance with [T5-TTS [5]](../SpeechLM/2024.06.25_T5-TTS.md), we extracted the codes from 22.05kHz audio resampled to 44.1kHz for the DAC model.

### Results & Discussion

To evaluate the performance of the T5-TTS model with various codecs, we followed the methodology proposed by [T5-TTS [5]](../SpeechLM/2024.06.25_T5-TTS.md).
We consider 200 utterances from the [VCTK dataset [39]](../../Datasets/2012.08.00_VCTK.md).
These sentences were generated using 20 random unseen speakers with 10 utterances per speaker from the VCTK dataset.
Consistent with [T5-TTS [5]](../SpeechLM/2024.06.25_T5-TTS.md), we used Character Error Rate (CER), Speaker Encoder Cosine Similarity (SECS), and Mean Opinion Score (MOS) as evaluation metrics.

CER was computed by comparing the transcription of the generated audio with the TTS input text, utilizing the Conformer-Transducer Large ASR model [HuggingFace](https://huggingface.co/nvidia/stt\_en\_conformer\_transducer\_large).
For SECS, we computed the cosine similarity between the speaker embeddings of the synthesized speech and the target speaker reference provided to the TTS model, using the WavLM speaker verification model [HuggingFace](https://huggingface.co/microsoft/wavlm-base-plus-sv) to extract the speaker embeddings.

For the MOS evaluation, 200 audio samples were generated for each model.
Each listener was presented with one sample and asked to rate it on a scale from 1 to 5, in 1-point intervals.
Each audio sample was evaluated by at least 10 independent listeners.

In addition to these metrics, we computed the inference real-time factor (RTF).
For better readability, we normalized the RTF values by setting the fastest model's RTF to 1 and adjusting the other models' RTF values accordingly.

Table \ref{tab:results-tts} presents the results of our ZS-TTS experiments.

The T5-TTS model trained using our proposed codec achieved better CER, showing that our codec improved the pronunciation accuracy in the TTS task.
Our proposed codecs achieved performance comparable to previous models in terms of MOS.
No significant differences were observed when comparing our codecs with DAC and [Spectral Codec](2024.06.07_Spectral_Codec.md).
In addition, the model achieved better speaker similarity, while bringing around 3 times speedup compared to other SOTA codecs.

Although, in Section \ref{sec:ablations}, our proposed codec using 4032 codebook size achieved better metrics than the proposed codec with 2016 codebook size, this improvement in performance did not generalize for the TTS downstream task.
Despite, the model 4032 codebook size achieving a better MOS, there is no significant difference.
In addition, the model with a smaller codebook size produced better performance in all the others metrics.
We believe that it happens because when the codebook size is smaller the model tends to generalize better.
A similar pattern was also reported in [XTTS [40]](../SpeechLM/2024.06.07_XTTS.md).

## 5·Conclusions: 结论

In this work, we presented the Low Frame-rate Speech Codec, which achieved high-quality audio compression at a bitrate of {1.89} kbps and 21.5 frames per second.
Furthermore, we demonstrated that a Speech LLM model trained with our codec achieved quality comparable to that of the same model trained with previous state-of-the-art audio codecs, while providing approximately threefold speedup.

In future work, we intend to explore our approach in the 44 kHz audio codec training and also investigate the application of our codec to other audio domains, such as music and sound effects.

在这项工作中, 我们介绍了 ***低帧率语音编解码器 (Low Frame-rate Speech Codec, LFSC)***, 它在 1.89 kbps 的比特率和每秒 21.5 帧的速度下实现了高质量的音频压缩.
此外我们证明使用我们的编解码器训练的语音 LLM 模型在质量上与使用先前最先进的音频编解码器训练的相同模型相当, 同时提供了大约三倍的加速.

在未来的工作中, 我们计划在 44 kHz 音频编解码器训练中探索我们的方法, 并研究将我们的编解码器应用于其他音频领域, 如音乐和音效.