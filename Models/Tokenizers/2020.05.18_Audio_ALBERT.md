Audio ALBERT: A Lite BERT for Self-supervised Learning of Audio Representation
Po-Han Chi, Pei-Hung Chung, Tsung-Han Wu, Chun-Cheng Hsieh, Yen-Hao Chen, Shang-Wen Li, Hung-yi Lee
For self-supervised speech processing, it is crucial to use pretrained models as speech representation extractors. In recent works, increasing the size of the model has been utilized in acoustic model training in order to achieve better performance. In this paper, we propose Audio ALBERT, a lite version of the self-supervised speech representation model. We use the representations with two downstream tasks, speaker identification, and phoneme classification. We show that Audio ALBERT is capable of achieving competitive performance with those huge models in the downstream tasks while utilizing 91\% fewer parameters. Moreover, we use some simple probing models to measure how much the information of the speaker and phoneme is encoded in latent representations. In probing experiments, we find that the latent representations encode richer information of both phoneme and speaker than that of the last layer.
Comments:	Accepted by IEEE Spoken Language Technology Workshop 2021
Subjects:	Audio and Speech Processing (eess.AS); Computation and Language (cs.CL); Sound (cs.SD)
Cite as:	arXiv:2005.08575 [eess.AS]
 	(or arXiv:2005.08575v5 [eess.AS] for this version)

https://doi.org/10.48550/arXiv.2005.08575
Focus to learn more
Submission history
From: Pohan Chi [view email]
[v1] Mon, 18 May 2020 10:42:44 UTC (1,192 KB)
[v2] Tue, 26 May 2020 05:35:28 UTC (1,192 KB)
[v3] Mon, 22 Feb 2021 00:56:58 UTC (1,905 KB)
[v4] Tue, 23 Mar 2021 17:51:46 UTC (1,905 KB)
[v5] Mon, 3 May 2021 09:33:31 UTC (1,900 KB)