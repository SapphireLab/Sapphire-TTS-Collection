# TaDiCodec: Text-Aware Diffusion Speech Tokenizer for Speech Language Modeling

<details>
<summary>基本信息</summary>

- 标题: "TaDiCodec: Text-Aware Diffusion Speech Tokenizer for Speech Language Modeling."
- 作者:
  - 01 Yuancheng Wang
  - 02 Dekun Chen
  - 03 Xueyao Zhang
  - 04 Junan Zhang
  - 05 Jiaqi Li
  - 06 Zhizheng Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2508.16790v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2508.16790v1](_PDF/2025.08.22_2508.16790v1_TaDiCodec__Text-Aware_Diffusion_Speech_Tokenizer_for_Speech_Language_Modeling.pdf)
  - [Publication] #TODO

</details>

## Abstract

Speech tokenizers serve as foundational components for speech language models, yet current designs exhibit several limitations, including: 
1) dependence on multi-layer residual vector quantization structures or high frame rates,
2) reliance on auxiliary pre-trained models for semantic distillation, and
3) requirements for complex two-stage training processes.
In this work, we introduce the ***T**ext-**a**ware **Di**ffusion Transformer Speech **Codec*** (*\myname{*}), a novel approach designed to overcome these challenges.
\myname{} employs end-to-end optimization for quantization and reconstruction through a diffusion autoencoder, while integrating text guidance into the diffusion decoder to enhance reconstruction quality and achieve optimal compression.
\myname{} achieves an extremely low frame rate of **6.25 Hz** and a corresponding bitrate of **0.0875 kbps** with a **single-layer codebook** for 24 kHz speech, 
while maintaining superior performance on critical speech generation evaluation metrics such as Word Error Rate (WER), speaker similarity (SIM), and speech quality (UTMOS).
Notably, \myname{} employs a single-stage, end-to-end training paradigm, and obviating the need for auxiliary pre-trained models.
We also validate the compatibility of \myname{} in language model based zero-shot text-to-speech with both autoregressive modeling and masked generative modeling, demonstrating its effectiveness and efficiency for speech language modeling, as well as a significantly small *reconstruction-generation gap*.
We will open source our code and model checkpoints.
Audio samples are are available at \url{https:/tadicodec.github.io/}.
We release code and model checkpoints at \url{https:/github.com/HeCheng0625/Diffusion-Speech-Tokenizer}.

## 1·Introduction

Recent advances have been made in both large language model (LLM)-based text-to-speech (TTS) systems[^Wang2023Neural], [^Anastassiou2024Seed-TTS], [^Du2024CosyVoice], [^Guo2024FireRedTTS], [^Ye2025Llasa], [^Wang2025Spark-TTS], [^Wang2024Maskgct], [^Kharitonov2023Speak,], [^Peng2024VoiceCraft] and spoken language models[^D{\'e}fossez2024Moshi], [^Zeng2024Scaling], [^Zeng2024GLM-4-Voice], [^Li2025Baichuan-Audio], [^Huang2025Step-Audio], [^Fang2024Llama-Omni], [^Wang2024Freeze-Omni], [^Ding2025Kimi-Audio], [^Xu2025Qwen2.].

At the core of these systems lies the speech tokenizer, which converts continuous speech signals into discrete token sequences, thereby enabling the application of textual LLM paradigms to speech modeling. 
Beyond this, speech tokenizers play a fundamental role in bridging the text and speech modalities, forming the basis for cross-modal learning, alignment, and generation.

However, most existing speech tokenizers are suboptimal for speech language modeling.

Prior works (*e.g.*, EnCodec[^D{\'e}fossez2022High], SoundStream[^Zeghidour2021Soundstream], DAC[^Kumar2024High-Fidelity]) 
primarily target speech signal compression and transmission, 
relying on multi-layer residual vector quantization (RVQ) and operating at high frame rates and bitrates. 
Such configurations make modeling with language models challenging and inefficient.

More recently, several studies[^Xin2024Bigcodec], [^Ji2024Wavtokenizer], [^Wang2025Spark-TTS], [^Ye2025Llasa], [^Parker2024Scaling] have explored techniques for single-layer speech tokenizers.

However, these approaches still fall short in reconstruction quality compared to RVQ-based tokenizers and often maintain high token rates (typically exceeding 50 tokens per second).

Moreover, they usually depend on complex loss designs and adversarial training.

Additionally, many of these models primarily optimize for acoustic-level reconstruction, resulting in discrete representations that lack semantic richness, making them suboptimal for language model modeling and causing *reconstruction-generation gap*.

Recent studies[^Du2024CosyVoice], [^Anastassiou2024Seed-TTS], [^Ye2025Codec], [^D{\'e}fossez2024Moshi], [^Li2025DualCodec], [^Guo2024FireRedTTS], [^Zeng2024Scaling] emphasize that effective speech tokens for language modeling should exhibit low frame rates and semantic richness, 
which criteria that directly shape the design of modern speech tokenizers. 
To achieve this, several works[^Zhang2023Speechtokenizer], [^D{\'e}fossez2024Moshi], [^Li2025DualCodec], [^Ye2025Codec] decompose speech into semantic and acoustic tokens by distilling features from speech self-supervised learning (SSL) models[^Chen2022Wavlm], [^Baevski2020Wav2vec], [^Hsu2021Hubert], [^Chiu2022Self-Supervised]. 
In this framework, semantic tokens exhibit improved alignment with textual representations, thereby facilitating more effective language modeling.

However, preserving reconstruction quality often requires RVQ, along with intricate loss functions, adversarial objectives, and the integration of external SSL models.

An alternative line of work, including systems such as CosyVoice[^Du2024CosyVoice], SeedTTS[^Anastassiou2024Seed-TTS], FireRedTTS[^Guo2024FireRedTTS], and Vevo[^Zhang2025Vevo], adopts a two-stage design: first quantizing SSL-derived features, then training a separate diffusion model[^Ho2020Denoising], [^Song2020Score-Based], [^Lipman2022Flow] to reconstruct speech conditioned on these tokens.

While this design enables relatively low frame rates and supports a single-layer token representation, it comes with several limitations:
**1) Two-stage training:** the pipeline introduces greater architectural complexity and reduced training efficiency compared to end-to-end approaches;
**2) External dependency:** it relies on pre-trained SSL or supervised models for semantic feature extraction; and
**3) Struggle with extreme compression:** most systems fail to achieve ultra-low token rates (*e.g.*, fewer than 20 tokens per second), which are critical for modeling efficiency and scalability.

To address the limitations of current speech tokenizers, we propose the ***T**ext-**a**ware **Di**ffusion Transformer Speech **Codec*** (*\myname{*}), a novel model that achieves an exceptionally low frame rate of **6.25 Hz** using a **single codebook**, corresponding to a bitrate of **0.0875 kbps** for 24 kHz speech.

Despite this ultra-low rate, \myname{} delivers high-fidelity speech reconstruction and robust performance on downstream speech language modeling tasks.

Specifically: **1)** \myname{} unifies quantization and reconstruction within an **end-to-end diffusion autoencoder**, removing the need for separate semantic distillation or complex adversarial objectives by relying solely on diffusion loss;
**2)** it enhances reconstruction quality and compression efficiency by incorporating **text and prompt guidance** into the diffusion decoder.

Our design is motivated by the increasing availability of transcriptions from automatic speech recognition (ASR) systems[^Radford2023Robust], [^Gao2023Funasr], and the widespread use of paired speech-text data in generative applications.

In zero-shot TTS scenarios, for instance, the target text is inherently available; in end-to-end spoken language systems, speech and text tokens are typically generated jointly[^Zeng2024GLM-4-Voice], [^Li2025Baichuan-Audio], [^Huang2025Step-Audio], [^Ding2025Kimi-Audio], [^Fang2024Llama-Omni], [^Wang2024Freeze-Omni], [^Gao2025Lucy].

Our experiments show that \myname{} achieves performance comparable to or better than existing speech tokenizers in both reconstruction and downstream speech generation, while maintaining a significantly smaller gap between reconstruction and generation.

In addition, it adopts a much simpler pipeline and operates with much fewer tokens.

We evaluate zero-shot TTS using \myname{} under both autoregressive and masked language modeling settings, achieving strong results in intelligibility, speaker similarity, speech quality, and overall training and inference efficiency.

A comparison with other tokenizers is presented in Figure~[fig:comparsion](#fig:comparsion) and Table~[tab:tokenizer-rec-res](#tab:tokenizer-rec-res).

The contributions of our work are summarized as follows:
\vspace{-2mm}

[itemsep=0ex,leftmargin=3ex]

-  We propose *\myname{*}, a novel speech tokenizer with a token rate of 6.25 Hz and a bitrate of 0.0875 kbps, based on a diffusion autoencoder that jointly performs quantization and reconstruction without adversarial training, external pretrained models for semantic distillation, or multi-stage training.

This design enables efficient optimization and simplifies the speech tokenization pipeline.

-  We introduce text-aware and prompt-guided decoding into the diffusion process to facilitate extreme compression.

By leveraging paired speech-text data, this approach enhances reconstruction quality and enables high intelligibility, speaker similarity, and speech quality under ultra-low token rates.

-  We build zero-shot TTS models using our tokenizer under both autoregressive and masked language modeling settings, achieving WERs of 2.28 and 1.19 on *SeedTTS test-en* and *test-zh*, respectively.

Our models demonstrate notable improvements on challenging benchmarks such as articulatory, code-switching, and cross-lingual test sets, and support real-time inference with RTFs ranging from 0.12 to 0.29 across different model sizes.
\vspace{-2mm}

![](image/tadicodec_3d.pdf)

<a id="fig:comparsion">**Comparison between \myname{** and other speech tokenizers.}

We use a three-dimensional coordinate system to display the performance across three dimensions: the x-axis represents WER, the y-axis represents UTMOS, and the z-axis represents SIM.

The size of the markers is proportional to the kbps value.</a>
