# TaDiCodec: Text-Aware Diffusion Speech Tokenizer for Speech Language Modeling

<details>
<summary>基本信息</summary>

- 标题: "TaDiCodec: Text-Aware Diffusion Speech Tokenizer for Speech Language Modeling."
- 作者:
  - 01 Yuancheng Wang
  - 02 Dekun Chen
  - 03 Xueyao Zhang
  - 04 Junan Zhang
  - 05 Jiaqi Li
  - 06 Zhizheng Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2508.16790v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2508.16790v1](_PDF/2025.08.22_2508.16790v1_TaDiCodec__Text-Aware_Diffusion_Speech_Tokenizer_for_Speech_Language_Modeling.pdf)
  - [Publication] #TODO

</details>

## Abstract

Speech tokenizers serve as foundational components for speech language models, yet current designs exhibit several limitations, including: 
1) dependence on multi-layer residual vector quantization structures or high frame rates,
2) reliance on auxiliary pre-trained models for semantic distillation, and
3) requirements for complex two-stage training processes.
In this work, we introduce the ***T**ext-**a**ware **Di**ffusion Transformer Speech **Codec*** (*\myname{*}), a novel approach designed to overcome these challenges.
\myname{} employs end-to-end optimization for quantization and reconstruction through a diffusion autoencoder, while integrating text guidance into the diffusion decoder to enhance reconstruction quality and achieve optimal compression.
\myname{} achieves an extremely low frame rate of **6.25 Hz** and a corresponding bitrate of **0.0875 kbps** with a **single-layer codebook** for 24 kHz speech, 
while maintaining superior performance on critical speech generation evaluation metrics such as Word Error Rate (WER), speaker similarity (SIM), and speech quality (UTMOS).
Notably, \myname{} employs a single-stage, end-to-end training paradigm, and obviating the need for auxiliary pre-trained models.
We also validate the compatibility of \myname{} in language model based zero-shot text-to-speech with both autoregressive modeling and masked generative modeling, demonstrating its effectiveness and efficiency for speech language modeling, as well as a significantly small *reconstruction-generation gap*.
We will open source our code and model checkpoints.
Audio samples are are available at \url{https:/tadicodec.github.io/}.
We release code and model checkpoints at \url{https:/github.com/HeCheng0625/Diffusion-Speech-Tokenizer}.

## 1·Introduction

Recent advances have been made in both large language model (LLM)-based text-to-speech (TTS) systems[^Wang2023Neural], [^Anastassiou2024Seed-TTS], [^Du2024CosyVoice], [^Guo2024FireRedTTS], [^Ye2025Llasa], [^Wang2025Spark-TTS], [^Wang2024Maskgct], [^Kharitonov2023Speak,], [^Peng2024VoiceCraft] and spoken language models[^D{\'e}fossez2024Moshi], [^Zeng2024Scaling], [^Zeng2024GLM-4-Voice], [^Li2025Baichuan-Audio], [^Huang2025Step-Audio], [^Fang2024Llama-Omni], [^Wang2024Freeze-Omni], [^Ding2025Kimi-Audio], [^Xu2025Qwen2.].

At the core of these systems lies the speech tokenizer, which converts continuous speech signals into discrete token sequences, thereby enabling the application of textual LLM paradigms to speech modeling. 
Beyond this, speech tokenizers play a fundamental role in bridging the text and speech modalities, forming the basis for cross-modal learning, alignment, and generation.

However, most existing speech tokenizers are suboptimal for speech language modeling.

Prior works (*e.g.*, EnCodec[^D{\'e}fossez2022High], SoundStream[^Zeghidour2021Soundstream], DAC[^Kumar2024High-Fidelity]) 
primarily target speech signal compression and transmission, 
relying on multi-layer residual vector quantization (RVQ) and operating at high frame rates and bitrates. 
Such configurations make modeling with language models challenging and inefficient.

More recently, several studies[^Xin2024Bigcodec], [^Ji2024Wavtokenizer], [^Wang2025Spark-TTS], [^Ye2025Llasa], [^Parker2024Scaling] have explored techniques for single-layer speech tokenizers.

However, these approaches still fall short in reconstruction quality compared to RVQ-based tokenizers and often maintain high token rates (typically exceeding 50 tokens per second).

Moreover, they usually depend on complex loss designs and adversarial training.

Additionally, many of these models primarily optimize for acoustic-level reconstruction, resulting in discrete representations that lack semantic richness, making them suboptimal for language model modeling and causing *reconstruction-generation gap*.

Recent studies[^Du2024CosyVoice], [^Anastassiou2024Seed-TTS], [^Ye2025Codec], [^D{\'e}fossez2024Moshi], [^Li2025DualCodec], [^Guo2024FireRedTTS], [^Zeng2024Scaling] emphasize that effective speech tokens for language modeling should exhibit low frame rates and semantic richness, 
which criteria that directly shape the design of modern speech tokenizers. 
To achieve this, several works[^Zhang2023Speechtokenizer], [^D{\'e}fossez2024Moshi], [^Li2025DualCodec], [^Ye2025Codec] decompose speech into semantic and acoustic tokens by distilling features from speech self-supervised learning (SSL) models[^Chen2022Wavlm], [^Baevski2020Wav2vec], [^Hsu2021Hubert], [^Chiu2022Self-Supervised]. 
In this framework, semantic tokens exhibit improved alignment with textual representations, thereby facilitating more effective language modeling.

However, preserving reconstruction quality often requires RVQ, along with intricate loss functions, adversarial objectives, and the integration of external SSL models.

An alternative line of work, including systems such as CosyVoice[^Du2024CosyVoice], SeedTTS[^Anastassiou2024Seed-TTS], FireRedTTS[^Guo2024FireRedTTS], and Vevo[^Zhang2025Vevo], adopts a two-stage design: first quantizing SSL-derived features, then training a separate diffusion model[^Ho2020Denoising], [^Song2020Score-Based], [^Lipman2022Flow] to reconstruct speech conditioned on these tokens.

While this design enables relatively low frame rates and supports a single-layer token representation, it comes with several limitations:
**1) Two-stage training:** the pipeline introduces greater architectural complexity and reduced training efficiency compared to end-to-end approaches;
**2) External dependency:** it relies on pre-trained SSL or supervised models for semantic feature extraction; and
**3) Struggle with extreme compression:** most systems fail to achieve ultra-low token rates (*e.g.*, fewer than 20 tokens per second), which are critical for modeling efficiency and scalability.

To address the limitations of current speech tokenizers, we propose the ***T**ext-**a**ware **Di**ffusion Transformer Speech **Codec*** (*\myname{*}), a novel model that achieves an exceptionally low frame rate of **6.25 Hz** using a **single codebook**, corresponding to a bitrate of **0.0875 kbps** for 24 kHz speech.

Despite this ultra-low rate, \myname{} delivers high-fidelity speech reconstruction and robust performance on downstream speech language modeling tasks.

Specifically: **1)** \myname{} unifies quantization and reconstruction within an **end-to-end diffusion autoencoder**, removing the need for separate semantic distillation or complex adversarial objectives by relying solely on diffusion loss;
**2)** it enhances reconstruction quality and compression efficiency by incorporating **text and prompt guidance** into the diffusion decoder.

Our design is motivated by the increasing availability of transcriptions from automatic speech recognition (ASR) systems[^Radford2023Robust], [^Gao2023Funasr], and the widespread use of paired speech-text data in generative applications.

In zero-shot TTS scenarios, for instance, the target text is inherently available; in end-to-end spoken language systems, speech and text tokens are typically generated jointly[^Zeng2024GLM-4-Voice], [^Li2025Baichuan-Audio], [^Huang2025Step-Audio], [^Ding2025Kimi-Audio], [^Fang2024Llama-Omni], [^Wang2024Freeze-Omni], [^Gao2025Lucy].

Our experiments show that \myname{} achieves performance comparable to or better than existing speech tokenizers in both reconstruction and downstream speech generation, while maintaining a significantly smaller gap between reconstruction and generation.

In addition, it adopts a much simpler pipeline and operates with much fewer tokens.

We evaluate zero-shot TTS using \myname{} under both autoregressive and masked language modeling settings, achieving strong results in intelligibility, speaker similarity, speech quality, and overall training and inference efficiency.

A comparison with other tokenizers is presented in Figure~[fig:comparsion](#fig:comparsion) and Table~[tab:tokenizer-rec-res](#tab:tokenizer-rec-res).

The contributions of our work are summarized as follows:
\vspace{-2mm}

[itemsep=0ex,leftmargin=3ex]

-  We propose *\myname{*}, a novel speech tokenizer with a token rate of 6.25 Hz and a bitrate of 0.0875 kbps, based on a diffusion autoencoder that jointly performs quantization and reconstruction without adversarial training, external pretrained models for semantic distillation, or multi-stage training.

This design enables efficient optimization and simplifies the speech tokenization pipeline.

-  We introduce text-aware and prompt-guided decoding into the diffusion process to facilitate extreme compression.

By leveraging paired speech-text data, this approach enhances reconstruction quality and enables high intelligibility, speaker similarity, and speech quality under ultra-low token rates.

-  We build zero-shot TTS models using our tokenizer under both autoregressive and masked language modeling settings, achieving WERs of 2.28 and 1.19 on *SeedTTS test-en* and *test-zh*, respectively.

Our models demonstrate notable improvements on challenging benchmarks such as articulatory, code-switching, and cross-lingual test sets, and support real-time inference with RTFs ranging from 0.12 to 0.29 across different model sizes.
\vspace{-2mm}

![](image/tadicodec_3d.pdf)

<a id="fig:comparsion">**Comparison between \myname{** and other speech tokenizers.}

We use a three-dimensional coordinate system to display the performance across three dimensions: the x-axis represents WER, the y-axis represents UTMOS, and the z-axis represents SIM.

The size of the markers is proportional to the kbps value.</a>

## 2·Related Work

**Discrete Speech Tokenizer**\quad  
Discrete speech tokenizers convert continuous speech into discrete tokens, enabling modern zero-shot TTS and speech language modeling.

Early tokenizers[^Zeghidour2021Soundstream], [^D{\'e}fossez2022High], [^Kumar2024High-Fidelity] focused on audio compression, relying on residual vector quantization (RVQ)[^Zeghidour2021Soundstream], [^Lee2022Autoregressive] and operating at high frame rates and bitrates, settings ill-suited for language modeling.  
Recent work has shifted toward designing tokenizers tailored for language modeling, emphasizing low frame rates[^D{\'e}fossez2024Moshi], [^Li2025DualCodec], semantic-rich representations[^Zhang2023Speechtokenizer], [^D{\'e}fossez2024Moshi], [^Li2025DualCodec], [^Wang2025Spark-TTS], [^Ye2025Codec], [^Ye2025Llasa], [^Liu2024Semanticodec], [^Guo2024FireRedTTS], [^Du2024Cosyvoice], [^Zhang2025Vevo], and single-layer codebooks[^Parker2024Scaling], [^Xin2024Bigcodec], [^Ji2024Wavtokenizer].  
Diffusion-based methods[^Ho2020Denoising], [^Song2020Score-Based] have gained popularity for their performance at low token rates and scalability.

However, they typically follow a two-stage pipeline: extracting tokens via self-supervised speech representations[^Chung2021W2v-Bert], [^Baevski2020Wav2vec], [^Chen2022Wavlm], [^Radford2023Robust], [^Hsu2021Hubert], [^Chiu2022Self-Supervised], then reconstructing waveforms through diffusion.  
For example, [^Welker2025FlowDec], [^Yang2024Generative] apply diffusion to improve de-tokenization quality, but still operate at relatively high token rates.

Achieving ultra-low bitrates (*e.g.*, below 0.2 kbps or 20 tokens/s) with a compact, generative-friendly framework remains a major challenge.

**Zero-shot TTS**\quad  
Modern zero-shot TTS systems typically operate on discrete speech tokens using either autoregressive (AR) language modeling[^Wang2023Neural], [^Anastassiou2024Seed-TTS], [^Du2024CosyVoice], [^Du2024Cosyvoice], [^Guo2024FireRedTTS], [^Ye2025Llasa], [^Wang2025Spark-TTS], [^Kharitonov2023Speak,], [^Peng2024VoiceCraft] or masked generative (language) modeling (MGM)[^Ju2024NaturalSpeech], [^Borsos2023Soundstorm], [^Wang2024Maskgct], [^Yang2025Pseudo-Autoregressive].

Some models[^Wang2024Maskgct], [^Anastassiou2024Seed-TTS], [^Du2024Cosyvoice], [^Du2024CosyVoice], [^Guo2024FireRedTTS] adopt an "AR + diffusion'' framework, where a diffusion decoder enhances waveform quality based on predicted tokens.  
Zero-shot TTS is also foundational in recent end-to-end spoken language models.

For example, Qwen2.5-Omni[^Xu2025Qwen2.] uses a "talker'' module to generate speech tokens from the text output of a "thinker.'' Similar architectures[^Wang2024Freeze-Omni], [^Fang2024Llama-Omni] decode speech directly from text, while others[^Huang2025Step-Audio], [^Ding2025Kimi-Audio], [^Zeng2024Scaling], [^Zeng2024GLM-4-Voice] leverage TTS models to synthesize large-scale speech corpora for training dialogue agents.

## 3·Method

### \myname{

}

**Speech Tokenization with Diffusion Transformer Autoencoder**\quad
Some speech tokenizers adopt raw waveform signals as modeling targets. 
However, raw waveforms often contain a considerable amount of redundant information. 
In this work, we instead adopt the mel-spectrogram as both the input and reconstruction target for the tokenizer, given its compactness and ease of inversion to waveform using vocoder models[^Lee2022Bigvgan], [^Siuzdak2023Vocos].

Formally, we denote the input mel-spectrogram as $\boldsymbol{x} \in \mathbb{R}^{T \times d}$, where $T$ denotes the number of frames, corresponding to the number of waveform frames divided by the hop size. 
The tokenizer's encoder $\mathcal{E}$ transforms $\boldsymbol{x}$ into a sequence of latent embeddings, *i.e.*, $\mathcal{E}(\boldsymbol{x})$.

These embeddings are then quantized by the vector quantization (VQ) module $\mathcal{Q}$ into a discrete token sequence $\boldsymbol{q} = \mathcal{Q}(\mathcal{E}(\boldsymbol{x})) \in \mathbb{Z}^{T_q \times 1}$, where $T_q$ is the length of the token sequence, typically equal to $T$ divided by a predefined down-sampling factor.

Each token $q_i$ (for $i \in [0, T_q)$) corresponds to an index in a codebook.

The decoder $\mathcal{D}$ subsequently reconstructs the mel-spectrogram as $\hat{\boldsymbol{x}} = \mathcal{D}(\boldsymbol{q})$.

Previous speech tokenizers primarily adopted generative adversarial networks (GANs)[^Goodfellow2020Generative] for training the system, typically operating on short speech segments (*e.g.*, 1–3 seconds) and employing CNNs as the backbone. 
However, GANs often suffer from issues related to training stability and efficiency, 
and the reliance on CNN-based architectures and short-segment training further constrains the model's ability to capture long-range dependencies, leading to a focus on only local acoustic patterns.

To overcome these limitations, we use a fully Transformer-based[^Vaswani2017Attention] architecture for both the encoder and decoder, and adopt a diffusion loss for reconstruction training, enabling more stable optimization and improved modeling capabilities.

Specifically, we adopt a flow matching-based decoder[^Lipman2022Flow], [^Liu2022Flow].

During training, we sample Gaussian noise $\boldsymbol{\epsilon}$ and generate a noisy target $\boldsymbol{x}_t$ via a linear interpolation: $\boldsymbol{x}_t = t\boldsymbol{x} + (1 - t)\boldsymbol{\epsilon}$, where $t \in [0, 1]$ is a randomly sampled noise level.

The model is then trained to predict the velocity field $\boldsymbol{v}$, defined as the derivative of $\boldsymbol{x}_t$ with respect to $t$, *i.e.*, $\boldsymbol{v} = \frac{d\boldsymbol{x}_t}{dt} = \boldsymbol{x} - \boldsymbol{\epsilon}$.

We provide more details about flow matching in Appendix~[appendix:fm](#appendix:fm).

\begin{wrapfigure}{r}{0.45\textwidth}
\centering
\includegraphics[width=0.9\linewidth]{image/tadicodec.pdf}
\caption{**Training speech tokenizer with diffusion autoencoder.**

We optimize tokenization and reconstruction end-to-end with diffusion loss.

The input $\boldsymbol{x}$ is passed through the encoder and quantizer to get $\mathcal{Q}(\mathcal{E}(\boldsymbol{x}))$, which is then conditioned and input into the DiT decoder to predict the velocity $\boldsymbol{v}$ corresponding to the noisy $\boldsymbol{x}_{t}$.}
\label{fig:diff_loss}
\vspace{-5mm}
\end{wrapfigure}

**Binary Spherical Quantization**\quad
For quantization, we use Binary Spherical Quantization (BSQ)[^Zhao2024Image], which does not rely on an explicit learnable codebook.

We first apply downsampling to the encoder output $\mathcal{E}(\boldsymbol{x})$, followed by a linear projection to obtain a low-dimensional latent sequence:
$
\boldsymbol{h} = \text{Linear}(\text{Downsample}(\mathcal{E}(\boldsymbol{x}))) \in \mathbb{R}^{T_q \times L},
$
where $T_q$ is the number of quantized frames and $L$ is the latent dimension.

Each vector $\boldsymbol{h}_t \in \mathbb{R}^L$ of $\boldsymbol{h}$ is then projected onto the unit sphere:
$
\boldsymbol{u}_t = \frac{\boldsymbol{h}_t}{\|\boldsymbol{h}_t\|}.
$
Binary quantization is applied independently on each dimension:
$
\hat{\boldsymbol{u}}_t = \frac{1}{\sqrt{L}} \, \text{sign}(\boldsymbol{u}_t),
$
where $\text{sign}(x)$ is the element-wise sign function.

To enable gradient flow through the quantization step, we adopt a Straight-Through Estimator (STE):
$
\text{sign}_{\text{STE}}(x) = \text{sg}(\text{sign}(x) - x) + x,
$
where $\text{sg}(\cdot)$ denotes the stop-gradient operation.

The quantized latent sequence $\hat{\boldsymbol{u}} \in \mathbb{R}^{T_q \times L}$ is then mapped back to the $d$-dimensional space and upsampled to the original temporal resolution:
$
\text{Upsample}(\text{Linear}(\hat{\boldsymbol{u}})) \in \mathbb{R}^{T \times d}.
$
Each quantized vector $\boldsymbol{h}_t$ corresponds to a discrete token index computed by:

$$
k_t = \sum_{i=1}^L 1_{[\boldsymbol{h}_{t,i} > 0]} \cdot 2^{i-1},
$$

where $1_{[\cdot]}$ is the indicator function.

As noted in[^Zhao2024Image], BSQ can be optimized without the need for a commitment loss[^Van2017Neural], since its quantization error is theoretically bounded.

This property enables end-to-end training of the system using only the diffusion loss.

See Appendix~[appendix:bsq](#appendix:bsq) for further details.

**Text-aware De-Tokenization**\quad
Most existing speech tokenizers rely solely on speech features for reconstruction. *However, in the context of speech language modeling, the corresponding text associated with the speech is often readily available.*

For example, in TTS, the target text is always known, and in most end-to-end spoken dialogue systems, text and speech tokens are generated jointly[^D{\'e}fossez2024Moshi], [^Zeng2024Scaling], [^Zeng2024GLM-4-Voice], [^Li2025Baichuan-Audio], [^Huang2025Step-Audio], [^Fang2024Llama-Omni], [^Wang2024Freeze-Omni], [^Ding2025Kimi-Audio], [^Xu2025Qwen2.].

Motivated by this observation, we propose a **text-aware de-tokenization** strategy, which conditions the diffusion decoder on the corresponding text sequence $\boldsymbol{x}_{text}$.

To further improve reconstruction quality under the extremely low compression rate setting, we introduce a **prompt mechanism** into \myname{}, similar to prior works[^Le2024Voicebox], [^Chen2024F5-TTS], [^Eskimez2024E2], [^Wang2024Maskgct].

This mechanism enables the model *to better reconstruct speech when a prompt is provided, making it particularly suitable for speech generation scenarios such as zero-shot TTS and the decoding stage of spoken language models.*

Specifically, during training, we randomly sample a prefix $\boldsymbol{x}_{prompt}$ from the input mel-spectrogram by drawing a segment length \( l \sim \text{Uniform}(0, 0.25L) \), where \( L \) denotes the total number of frames in the mel-spectrogram.

The prefix is preserved without any added noise, while the loss is computed solely on the noisy portion of the sequence.

Table~[tab:ablation](#tab:ablation) shows this prompt mechanism yields substantial improvements in reconstruction performance.

We also experiment with removing text conditioning from the decoder and observe significant performance degradation under extremely low token rate and bitrate settings. *e.g.*, at a frame rate of 12.5 Hz, the WER exceeds 10.

Notably, Unlike prior works[^Du2024CosyVoice], [^Guo2024FireRedTTS], [^Zhang2025Vevo], [^Anastassiou2024Seed-TTS], [^Liu2024Semanticodec], [^Wang2024Maskgct], [^Wang2025Metis], [^Zeng2024Scaling] that adopt a two-stage pipeline: first training a VQ model and then a separate diffusion model for de-tokenization, our tokenizer **jointly learns feature quantization and reconstruction in an end-to-end manner**.

The overall training objective of \myname{} can be formulated as:

$$
\mathcal{L}_{\text{diff}} = \mathbb{E}_{(\boldsymbol{x}, \boldsymbol{x}_{text}), \boldsymbol{\epsilon}, t} \left[ \| (\boldsymbol{x} - \boldsymbol{\epsilon}) - \mathcal{D}_\phi(\mathcal{Q}(\mathcal{E}_\theta(\boldsymbol{x})), \boldsymbol{x}_t, t, \boldsymbol{x}_{text}) \| \right],
$$

where \(\mathcal{E}_\theta\) and \(\mathcal{D}_\phi\) are the encoder and decoder parameterized by \(\theta\) and \(\phi\).

We ignore the prompt for simplification.

We also find that continuing to train the decoder while freezing the encoder and VQ module can further improve performance.

### Speech Language Modeling with \myname{

}

Existing speech tokenizers often neglect their effectiveness in downstream speech language modeling tasks and suffer from a pronounced *reconstruction–generation gap*.

In this work, we apply our tokenizer to large-scale multilingual zero-shot TTS, adopting an *"AR + Diffusion''* paradigm: an autoregressive model first predicts speech tokens $\boldsymbol{q}$ from text $\boldsymbol{x}_{text}$, which are then passed, along with the text, to \myname{}'s diffusion decoder to generate speech.

The AR model, parameterized by $\psi$, is optimized to minimize the negative log-likelihood of the target token sequence conditioned on the input text and previously predicted tokens:

$$
\mathcal{L}_{\text{AR}} = - \mathbb{E}_{(\boldsymbol{q}, \boldsymbol{x}_{text})} \sum_{i=1}^{T_q} \log p(\boldsymbol{q}_i \mid \boldsymbol{q}_{<i}, \boldsymbol{x}_{text}; \psi),
$$

where $\boldsymbol{q}_i$ is the $i$-th token of $\boldsymbol{q}$.

We also apply the non-autoregressive Masked Generative Modeling (MGM)[^Chang2022Maskgit], [^Wang2024Maskgct] for modeling speech tokens.

See more details about MGM in the Appendix~[appendix:mgm](#appendix:mgm).

## 4·Experiments

We first describe the implementation details and datasets (Section~[sec:exp-set](#sec:exp-set)).

We then present the speech reconstruction results of \myname{} in Section~[sec:speech_rec](#sec:speech_rec), including the main results (Section~[sec:tokenizer_main_res](#sec:tokenizer_main_res), Table~[tab:tokenizer-rec-res](#tab:tokenizer-rec-res)), multilingual performance (Table~[tab:rec-multilingual](#tab:rec-multilingual)), subjective evaluation results (Table~[tab:cmos](#tab:cmos)), and ablation studies on tokenizer design (Section~[sec:tokenizer_ablation](#sec:tokenizer_ablation), Table~[tab:ablation](#tab:ablation)).

Section~[sec:zs-tts](#sec:zs-tts) reports the zero-shot TTS results of models built upon \myname{} (Table~[tab:zs-tts-res](#tab:zs-tts-res)), along with results on model size scaling and training and inference efficiency (Table~[tab:tts-scaling-rtf](#tab:tts-scaling-rtf)), and an analysis of the reconstruction–generation gap (Figure~[fig:rec-gen-gap](#fig:rec-gen-gap)).

### Experimental Settings

\label{sec:exp-set}

**Datasets**\quad
We use the Emilia[^He2024Emilia] dataset to train all of our models.

Emilia is a multilingual and diverse in-the-wild speech dataset designed for large-scale speech generation.

It contains 46.8K hours of English, 49.9K hours of Chinese, 1.6K hours of German, 1.4K hours of French, 1.7K hours of Japanese, and 0.2K hours of Korean.

**Implementation Details**\quad
We build \myname{} using standard Llama-style Transformer blocks[^Touvron2023Llama], with bidirectional attention instead of causal attention.

The base configuration employs an 8-layer encoder and a 16-layer decoder, each with hidden size 1024, intermediate size 4096, and 16 attention heads.

We further explore decoder variants; see Section~[sec:tokenizer_ablation](#sec:tokenizer_ablation) and Table~[tab:ablation](#tab:ablation) for details.

We adopt RoPE positional embedding[^Su2024Roformer] and RMSNorm[^Zhang2019Root].

For the text-aware diffusion decoder, RMSNorm is modified to Adaptive RMSNorm to condition on the diffusion step embedding.

Text tokens are adapted from a pretrained LLM vocabulary[^Abdin2024Phi-3], [^Yang2024Qwen2.], and concatenated with speech features along the time axis before being input to the decoder.

For vector quantization, we use BSQ[^Zhao2024Image] with a latent size of 14, yielding a codebook size of $2^{14} = 16384$.

All models are trained on 8 80GB NVIDIA A100 GPUs using dynamic batching with 200 seconds of speech per batch.

We train the tokenizer for 800K steps using AdamW[^Loshchilov2017Decoupled] with a learning rate of $7.5\times10^{-5}$ and 32K warmup steps.

TTS models are trained for 300K steps with a learning rate of $3\times10^{-4}$ unless otherwise specified.

AR models extend the vocabulary of pretrained textual LLMs[^Du2024CosyVoice], [^Ye2025Llasa] and are trained with 0.2B, 0.5B, 3.0B, and 4.0B parameters; see Section~[sec:zs-tts](#sec:zs-tts) for analysis.

For MGM models, we follow the setup of[^Wang2024Maskgct].

**Evaluation**\quad
We evaluate our approach from two main perspectives: speech reconstruction using the proposed tokenizer (Section~[sec:speech_rec](#sec:speech_rec)) and zero-shot TTS performance (Section~[sec:zs-tts](#sec:zs-tts)).

We assess intelligibility (WER), speaker similarity (SIM), and speech quality (UTMOS).

Speaker similarity is computed as the cosine similarity between \texttt{WavLM-TDNN} embeddings of the prompt and generated speech[^Chen2022Wavlm]. 
WER is measured using \texttt{whisper-large-v3}[^Radford2023Robust] for non-Chinese languages and \texttt{paraformer-zh}[^Gao2023Funasr] for Chinese, following prior work[^Wang2025Metis], [^Wang2024Maskgct], [^Anastassiou2024Seed-TTS], [^Du2024CosyVoice]. 
Speech quality is evaluated using the official UTMOS checkpoint.

In addition to objective metrics, we conduct subjective evaluation via Comparative Mean Opinion Score (CMOS).

We do not report signal-level metrics (*e.g.*, PESQ, STOI), as our focus is on generation-oriented performance, in line with[^Wang2025Metis], [^Zhang2025AnyEnhance].

Further Evaluation details are provided in Appendix~[appendix:eval](#appendix:eval).

### Speech Reconstruction

\label{sec:speech_rec}

#### Main Results

\label{sec:tokenizer_main_res}

We report our main results on *SeedTTS test-en*[^Anastassiou2024Seed-TTS] in Table~[tab:tokenizer-rec-res](#tab:tokenizer-rec-res).

We also evaluate our methods on multilingual test sets in Table~[tab:rec-multilingual](#tab:rec-multilingual).

Subjective evaluation results are show in Table~[tab:cmos](#tab:cmos).

**Baselines**\quad We compare with a wide range of baselines in settings where the token rate is less than 150: **1)** single stage with multi-layer codebook and adversarial training: EnCodec[^D{\'e}fossez2022High], DAC[^Kumar2024High-Fidelity], SpeechTokenizer[^Zhang2023Speechtokenizer], Mimi[^D{\'e}fossez2024Moshi], DualCodec[^Li2025DualCodec]; **2)** single stage with single-layer codebook and adversarial training: DAC (with single VQ), BiCodec[^Wang2025Spark-TTS], X-codec 2[^Ye2025Llasa], WavTokenizer[^Ji2024Wavtokenizer], BigCodec[^Xin2024Bigcodec], TAAE[^Parker2024Scaling]; **3)** two stage with diffusion decoder: SemantiCodec[^Liu2024Semanticodec], Vevo Tokenizer[^Zhang2025Vevo], FireRedTTS Tokenizer[^Guo2024FireRedTTS], CosyVoice[^Du2024CosyVoice] \& CosyVoice 2 Tokenizer[^Du2024Cosyvoice], Ints Tokenizer[^Zhang2025Advancing].

We provide more detailed description of these baselines in Appendix~[appendix:baseline_codec](#appendix:baseline_codec).

\input{main_res_tab.tex}

**Results Analysis**\quad
**1) Compression:**
\myname{} demonstrates a significantly higher compression rate compared to all baselines.

It operates at a token rate of 6.25 Hz with a single-layer codebook, resulting in a bitrate of 0.0875 kbps.

Among the baselines, the closest in compression rate to \myname{} is the Ints Tokenizer, which has double the token rate and bitrate of \myname{}.

However, it performs worse in terms of WER (7.14 vs. 2.73) and UTMOS (3.37 vs. 3.73) and requires two-stage training and semantic distillation.

All other baselines have a token rate greater than 25 and a bitrate of at least 0.3 kbps.

Compared to other single-stage and distillation-free models, BigCodec has a higher WER (3.25 vs. 2.73) and lower SIM (0.61 vs. 0.69) than \myname{}, with a bitrate of 1.04 kbps.

Models with lower bitrates, such as TAAE, still have bitrates four times higher than ours and perform significantly worse in WER and SIM.

Other single-layer codebook tokenizers like BiCodec, X-codec 2, and WavTokenizer have bitrates 7.4, 9.1, and 10.3 times higher, respectively.
**2) Reconstruction Quality:**

In terms of WER, \myname{} achieves a score of 3.02 without  decoder continued-training and 2.73 with fine-tuning, ranking just behind DualCodec and X-codec 2, which have scores of 2.57 and 2.63, respectively, but with bitrates 10.6 and 9.1 times higher.

Table~[tab:ablation](#tab:ablation) shows that our setting with a bitrate of 0.175 kbps achieves the best WER.

In terms of SIM, TaDiCodec with  decoder continued-training achieves the best SIM of 0.69, while even without  decoder continued-training, it reaches 0.67, surpassing all baselines except for the CosyVoice 2 tokenizer.

In terms of UTMOS, our model achieves scores of 3.68 and 3.73 (with and without  decoder continued-training), ranking just behind DualCodec and TAAE, which have scores of 3.78 and 3.87.

However, these models operate at much higher bitrates of 0.925 kbps and 0.4 kbps and demonstrate poorer performance in SIM.

**Results for Multilingual**\quad As shown in Table~[tab:rec-multilingual](#tab:rec-multilingual), \myname{} achieves the best WER on English, Chinese, German, and Korean, with especially low WER on Chinese.

It also outperforms all baselines in speaker similarity across all evaluated languages.

<a id="tab:rec-multilingual">**Results of multilingual speech reconstruction.**

In addition to English, we evaluate on five other languages: Chinese (zh), French (fr), German (de), Japanese (ja), and Korean (ko).</a>

**Subject Evaluation Result**\quad
As shown in Table~[tab:cmos](#tab:cmos), our proposed system achieves the highest CMOS score among evaluated baselines.

More details about subjective evaluation are shown in Appendix~[appendix:sub_eval](#appendix:sub_eval).

#### Ablation Study

\label{sec:tokenizer_ablation}

In this section, we explore several designs for \myname{}.

For the ablation study, we report the results on *SeedTTS test-en* and *SeedTTS test-zh*.
**1) Vector Quantization Scheme:**

Replacing BSQ with a standard VQ tokenizer (implemented following [^Yu2021Vector-Quantized], [^Kumar2024High-Fidelity] with an explicit codebook of the same size as BSQ) leads to consistent degradation across all evaluation metrics.

This indicates that BSQ more effectively preserves both speech quality and intelligibility.
**2) Tokenizer Size Scaling:**

Reducing the decoder size to 160M results in substantial performance drops, particularly in English WER. 
In contrast, increasing the decoder size results in marginal improvements.

These results also imply the existence of a scaling law for \myname{}, warranting further investigation in future work.
**3) Prompt Mechanism:**

The introduction of the prompt mechanism substantially improves all three evaluation metrics.

A plausible explanation is that the prompt serves as a global conditioning signal (*e.g.*, speaker identity), thereby reducing the quantizer's burden to encode such global information.
**4) Inference Time Scaling:**

Increasing the number of inference steps to 50 yields marginal improvements, while reducing it to 10 leads to slight degradation.

However, further reduction to 5 steps results in a noticeable drop in performance.

Considering the trade-off between efficiency and quality, using 10 to 32 steps appears to be a reasonable operating range.

We aim to achieve comparable performance with fewer inference steps (*e.g.*, 1-2 steps) by leveraging techniques such as [^Song2023Consistency], [^Zhou2024Score], [^Frans2024One].
**5) Decoder Continued-training:**

We explore freezing the encoder and the VQ module while only continued-training the decoder for an additional 400K steps, focusing solely on reconstruction.

This approach yields further improvements, with WER dropping from 3.02 to 2.73 for English and from 1.11 to 0.94 for Chinese.

SIM also improves for both languages.
\textbf{6) Diffusion vs.

GAN:}

We also replace the diffusion loss with PatchGAN[^Demir2018Patch-Based], but observe a noticeable performance drop in both intelligibility and speech quality.

<a id="tab:cmos">**Subjective CMOS scores.**

We randomly choose 40 samples from a in-the-wild data source.

Comparisons between different models can also be found in demo page.</a>

### Zero-shot TTS

\label{sec:zs-tts}

In this section, we present the zero-shot TTS results using \myname{} as the prediction target.

We evaluate two different language modeling approaches: autoregressive (AR) and masked generative modeling (MGM) and we denote our models as "\myname{}-AR'' and "\myname{}-MGM'' respectively.

The results are reported on eight test sets, including *SeedTTS test-en* and *SeedTTS test-zh*, referred to as *Regular en* and *Regular zh*, which are widely adopted benchmarks for TTS evaluation[^Wang2024Maskgct], [^Anastassiou2024Seed-TTS], [^Du2024CosyVoice], [^Du2024Cosyvoice], [^Chen2024F5-TTS], [^Ye2025Llasa], [^Wang2025Spark-TTS].

In addition, we report performance on more challenging test sets, proposed in[^Zhang2025Advancing], covering articulatory scenarios (such as repeated words and tongue twisters), code-switching, and cross-lingual settings.

We provide more details about the evaluation datasets in Appendix~[appendix:test_sets](#appendix:test_sets).

**Baselines**\quad
We compare with a wide range of open-source and state-of-the-art baselines including: **1)**

AR-based Systems: ARS[^Wang2024Maskgct], CosyVoice 2[^Du2024Cosyvoice], FireRedTTS[^Guo2024FireRedTTS], Ints[^Zhang2025Advancing], SparkTTS[^Wang2025Spark-TTS], Llasa[^Ye2025Llasa]; **2)**

NAR-based systems: MaskGCT[^Wang2024Maskgct] and F5-TTS[^Chen2024F5-TTS].

We provide more detailed description of these baselines in Appendix~[appendix:baseline_tts](#appendix:baseline_tts).

**Main Results**\quad
We report the main results of our models and baselines on eight test sets in Table~[tab:zs-tts-res](#tab:zs-tts-res).

Our models exhibit significant improvements in intelligibility while maintaining speaker similarity comparable to state-of-the-art zero-shot TTS systems.

In terms of WER, **\myname{**-AR achieves the best performance on the *Regular en* and *Regular zh* test sets, reaching 2.28 and 1.19 respectively}, and outperforming all baselines.

On more challenging test sets, \myname{}-AR demonstrates even more pronounced advantages, for example, reducing WER from 15.03 to 9.16 on *Code-switching en*, and from 4.88 to 2.91 on *Cross-lingual en2zh*.

Notably, these improvements are achieved without any task-specific optimization or reinforcement learning fine-tuning[^Rafailov2023Direct], [^Ouyang2022Training] on WER, as done in work such as[^Du2024Cosyvoice].

For \myname{}-MGM, it consistently outperforms or matches the performance of state-of-the-art NAR zero-shot TTS systems across all test sets.

Even with only 10 inference steps, which is significantly more efficient, it achieves a WER of 1.69 on *Regular zh*, compared to 2.28 from MaskGCT.

On more challenging test sets, such as *Cross-lingual en2zh*, it reaches 4.66 (vs. 13.78 from F5-TTS), and on *Code-switching en*, it achieves 14.94 (vs. 35.35 from F5-TTS).

In terms of SIM, both \myname{}-AR and \myname{}-MGM show clear advantages over recent systems such as FireRedTTS, SparkTTS, and Llasa.

Their SIM scores are slightly lower than those of MaskGCT and CosyVoice 2, which operate at higher frame rates of 50 Hz and 25 Hz, respectively.

<a id="tab:zs-tts-res">**The zero-shot TTS results.**

Beyond regular cases, we also evaluate on challenging scenarios, including articulatory, code-switching, and cross-lingual settings.</a>

**Model Size Scaling, Training and Inference Efficiency**\quad We demonstrate that our 6.25 Hz tokenization is not only effective but also significantly **more efficient for both training and generation**.

We further explore how scaling the model size affects both performance and efficiency.

Results are shown in table~[tab:tts-scaling-rtf](#tab:tts-scaling-rtf).

As described in the implementation details, we train all our TTS models for 300K steps.

We find that the models achieve the optimal evaluation results at around 200K steps.

All models can be trained in approximately one day under our setup, which uses 8 NVIDIA A100 GPUs with flash attention and bf16 precision.

For inference efficiency, we measure using Real-Time Factor (RTF).

We use a 5-second speech as a prompt to generate approximately 10 seconds of speech, sampling 5 times and taking the average.

The experiments show that even with 4.0B parameters, our AR model can achieve an RTF of 0.29 without any deployment acceleration.

With vLLM[^Kwon2023Efficient], the 4.0B AR model can achieve an RTF of 0.13.

Additionally, the 0.6B \myname{}-MGM model achieves an RTF of 0.12.

We also observe a reasonable improvement in performance with increasing model parameters, especially on challenging test sets (*Articulatory*, *Code-switching*, and *Cross-lingual*).

Notably, our 0.5B model already matches or surpasses many state-of-the-art systems with an RTF of 0.22.

<a id="tab:tts-scaling-rtf">\textbf{Results and RTF analysis for TTS model size scaling.</a>

![](image/wer_gap.pdf)

<a id="fig:rec-gen-gap">**Performance gap between reconstruction and generation.**

Each system includes both English and Chinese variants.

Bars represent WER and SIM for reconstruction and generation.</a>

**Reconstruction and Generation Gap**\quad
In Figure~[fig:rec-gen-gap](#fig:rec-gen-gap), we present the performance gap between reconstruction and generation across multiple systems.

Our proposed system, \myname{}, demonstrates a notably small performance gap: -16.5\% for English WER (generation better than reconstruction), -5.8\% for English SIM, +26.5\% for Chinese WER, and -0.0\% for Chinese SIM.

These results indicate that \myname{} is highly generation-friendly—preserving most of the reconstruction quality during generation.

In contrast, existing systems such as Mimi exhibit a much larger degradation (*e.g.*, -104.5\% en WER gap and -265.9\% zh WER gap), suggesting that they are less effective in transferring reconstruction capabilities to generation.

This highlights the advantage of our design in ensuring consistency between reconstructed and generated outputs.

## 5·Conclusion

In this work, we introduce \myname{}, a novel speech tokenizer that injects textual information into the decoder and incorporates a prompt mechanism within an end-to-end diffusion autoencoder training framework. \myname{} achieves an extremely low frame rate of 6.25 Hz and a corresponding bitrate of 0.0875 kbps, using a single-layer codebook for 24 kHz speech.

Beyond reconstruction, we apply \myname{} to zero-shot TTS using both AR and MGM, demonstrating its effectiveness, efficiency, and suitability for generation.

These results highlight \myname{} as a viable and innovative solution for speech language modeling.

\clearpage

\bibliography{ref}
\bibliographystyle{unsrtnat}

\newpage

\appendix

## 6·Implementation Details

\label{appendix:imp_detail}

### Model Architecture

\label{appendix:model_arch}

All our models follow the standard Transformer[^Vaswani2017Attention], [^Touvron2023Llama] architecture, employ RoPE positional encoding[^Su2024Roformer] and the SiLU[^Elfwing2018Sigmoid-Weighted] activation function.

The encoder and decoder of the tokenizer and MGM models use bidirectional attention, while the AR models adopt causal attention.

The \myname{}-AR-0.5B and \myname{}-AR-3B models are initialized from the textual LLMs \texttt{Qwen2.5-0.5B-Instruct} and \texttt{Qwen2.5-3B-Instruct} [^Yang2024Qwen2.], respectively, while \myname{}-AR-4B is initialized from \texttt{Phi-3.5-mini-instruct} [^Abdin2024Phi-3].

<a id="tab:model-config">\textbf{Model configurations.</a>

## 7·Flow Matching

\label{appendix:fm}

We provide additional details of the flow matching framework used to train the diffusion decoder in \myname{}.

Flow matching[^Lipman2022Flow] defines a continuous transformation from a prior distribution (*e.g.*, Gaussian noise) to a target data distribution (*e.g.*, mel-spectrograms) by learning a time-dependent velocity field along an interpolated trajectory $\boldsymbol{x}_t$.

While multiple interpolation strategies can be used to construct $\boldsymbol{x}_t$, we adopt the *optimal transport path* formulation[^Lipman2022Flow], [^Liu2022Flow], instantiated in this work as simple linear interpolation.

Specifically, given a clean mel-spectrogram $\boldsymbol{x} \in \mathbb{R}^{T \times d}$ and a noise sample $\boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$, we construct an intermediate sample as:

$$
\boldsymbol{x}_t = t \boldsymbol{x} + (1 - t) \boldsymbol{\epsilon}, \quad t \sim \text{Uniform}(0, 1),
$$

where $t$ is sampled uniformly from $[0, 1]$, and $\boldsymbol{x}_t$ denotes the noisy input at time $t$.

The corresponding ground-truth velocity is the temporal derivative of $\boldsymbol{x}_t$:

$$
\boldsymbol{v} = \frac{d \boldsymbol{x}_t}{dt} = \boldsymbol{x} - \boldsymbol{\epsilon}.
$$

The diffusion decoder $\mathcal{D}_\phi$ is trained to predict $\boldsymbol{v}$, conditioned on the token sequence $\boldsymbol{q} = \mathcal{Q}(\mathcal{E}_\theta(\boldsymbol{x}))$ and the associated text $\boldsymbol{x}_{text}$, using the following objective:

$$
\mathcal{L}_{\text{diff}} = \mathbb{E}_{(\boldsymbol{x}, \boldsymbol{x}_{text}), \boldsymbol{\epsilon}, t} \left[ \left\| (\boldsymbol{x} - \boldsymbol{\epsilon}) - \mathcal{D}_\phi(\boldsymbol{q}, \boldsymbol{x}_t, t, \boldsymbol{x}_{text}) \right\| \right].
$$

\paragraph{Inference}

At inference time, we start with a noise sample $\boldsymbol{x}_0 = \boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$ and solve the ordinary differential equation:

$$
\frac{d\boldsymbol{x}_t}{dt} = \mathcal{D}_\phi(\boldsymbol{q}, \boldsymbol{x}_t, t, \boldsymbol{x}_{text})
$$

from $t = 0$ to $t = 1$ using a simple Euler ODE solver over a discretized set of $N$ time steps.

Flow matching provides a stable and interpretable training signal by directly supervising the instantaneous direction in which a noisy sample $\boldsymbol{x}_t$ should evolve to match the clean target $\boldsymbol{x}$.

In our setting, it enables effective training of the speech tokenizer under low bitrate constraints.

## 8·Binary Spherical Quantization

\label{appendix:bsq}

Binary Spherical Quantization (BSQ)[^Zhao2024Image] optimizes over an implicit codebook $\mathcal{C}_{\mathrm{BSQ}} = \left\{-\frac{1}{\sqrt{L}}, \frac{1}{\sqrt{L}}\right\}^L$, which corresponds to the $L$-dimensional hypercube projected onto the unit sphere.

Each corner $\boldsymbol{c}_k \in \mathcal{C}_{\mathrm{BSQ}}$ represents a unique discrete token $k \in \{0, \dots, 2^L - 1\}$.

Given an encoder output $\mathcal{E}(\boldsymbol{x})$, we first obtain a low-dimensional latent sequence $\boldsymbol{h} \in \mathbb{R}^{T_q \times L}$ after linear projection.

BSQ then projects each vector $\boldsymbol{h}_t$ in $\boldsymbol{h}$ onto the unit sphere:

$$
\boldsymbol{u}_t = \frac{\boldsymbol{h}_t}{\|\boldsymbol{h}_t\|},
$$

and performs binary quantization independently on each dimension:

$$
\hat{\boldsymbol{u}}_t = \frac{1}{\sqrt{L}} \, \text{sign}(\boldsymbol{u}_t),
$$

where $\text{sign}(x)$ is the element-wise sign function, with $\text{sign}(0)$ defined as $1$ to ensure codewords lie on the unit sphere.

To enable gradient-based training, BSQ uses the Straight-Through Estimator (STE) for backpropagation:

$$
\text{sign}_{\text{STE}}(x) = \text{sg}(\text{sign}(x) - x) + x,
$$

where $\text{sg}(\cdot)$ denotes the stop-gradient operation.

For each vector $\boldsymbol{h}_t$, the corresponding discrete token index is computed as:

$$
k_t = \sum_{i=1}^L 1_{[\boldsymbol{h}_{t,i} > 0]} \cdot 2^{i-1},
$$

where $1_{[\cdot]}$ is the indicator function.

This efficient implicit code assignment scheme allows fast token computation and decoding via bitwise operations.

BSQ offers several appealing properties: it avoids the need for an explicit learnable codebook; its quantization error is bounded, allowing the entire system to be trained without a commitment loss[^Van2017Neural].

In this work, we use $L = 14$, resulting in a codebook size of $2^{14} = 16384$.

## 9·Masked Generative Models

\label{appendix:mgm}

In this section, we provide a brief introduction to masked generative models (MGMs)[^Chang2022Maskgit], [^Yu2023Language], [^Wang2024Maskgct].

Let $\boldsymbol{x} = [y_1, y_2, \ldots, y_n]$ denote a discrete sequence of length $n$.

At each time step $t$, we define the masked input as $\boldsymbol{x}_t = \boldsymbol{x} \odot \boldsymbol{m}_t$, where $\boldsymbol{m}_t = [m_{t,1}, m_{t,2}, \ldots, m_{t,n}]$ is a binary mask.

Specifically, $x_i$ is replaced with a special \texttt{[MASK]} token if $m_{t,i} = 1$, and remains unchanged if $m_{t,i} = 0$.

Each mask element $m_{t,i}$ is independently sampled from a Bernoulli distribution with parameter $\gamma(t)$, where $\gamma(t) \in (0, 1]$ is a masking schedule function (e.g., $\gamma(t) = \sin\left(\frac{\pi t}{2T}\right)$ for $t \in (0, T]$).

The fully unmasked input is denoted by $\boldsymbol{x}_0 = \boldsymbol{x}$.

MGMs are trained to reconstruct the original sequence from partially observed inputs, conditioned on an optional context $\boldsymbol{c}$ (*e.g.*, in this paper, text $x_{text}$ is condition), by modeling the conditional distribution $p_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_t, \boldsymbol{c})$.

The model parameters $\theta$ are optimized by minimizing the expected marginal cross-entropy over the masked tokens:

$$
\mathcal{L}_{\text{mask}} = - \mathbb{E}_{\boldsymbol{x}, t, \boldsymbol{m}_t} \sum_{i=1}^{n} m_{t,i} \cdot \log p_{\theta}(y_i \mid \boldsymbol{x}_t, \boldsymbol{c}).
$$

At inference time, MGMs generate tokens in parallel via iterative decoding.

The process begins with a fully masked sequence $\boldsymbol{x}_T$.

Assuming a total of $S$ decoding steps, at each step $j \in \{1, \ldots, S\}$, a prediction $\boldsymbol{\hat{x}}_0$ is sampled from $p_{\theta}(\boldsymbol{x}_0 \mid \boldsymbol{x}_{T - (j-1)\cdot \frac{T}{S}}, \boldsymbol{c})$.

Then, $\lfloor n \cdot \gamma(T - j \cdot \frac{T}{S}) \rfloor$ tokens are selected based on confidence scores to be remasked, resulting in a new masked sequence $\boldsymbol{x}_{T - j\cdot \frac{T}{S}}$.

The confidence score for $\hat{y}_i$ in $\boldsymbol{\hat{x}}_0$ is given by $p_{\theta}(y_i \mid \boldsymbol{x}_{T - (j-1)\cdot \frac{T}{S}}, \boldsymbol{c})$ if the position $i$ was masked; otherwise, its score is set to $1$, indicating that unmasked tokens will not be remasked.

The $\lfloor n \cdot \gamma(T - j \cdot \frac{T}{S}) \rfloor$ tokens with the lowest confidence scores are selected for masking.

Note that the method for computing confidence scores is not unique.

For example, [^Lezama2022Improved] propose *Token-Critic*, a separate critic model trained to estimate token-wise confidence, thereby guiding the sampling process.

In addition, [^Lezama2022Improved], [^Xie2024Show-O] suggest that masked generative modeling can be interpreted as a simplified form of discrete diffusion.

In this work, we develop MGM models for *text-to-token*.

Given the low token rate of 6.25 Hz, the task is relatively easy to model, and 10 to 25 inference steps are sufficient to achieve good results.

## 10·Baselines

\label{appendix:baseline}

### Speech Tokenizer

\label{appendix:baseline_codec}

**EnCodec**[^D{\'e}fossez2022High]\quad
A Residual Vector Quantization (RVQ)-based neural audio codec operating at a frame rate of 75 Hz.

We use two codebooks for inference, achieving a bitrate of 1.5 kbps.

We use the official checkpoint\footnote{\url{https://huggingface.co/facebook/encodec_24khz}}.

**DAC**[^Kumar2024High-Fidelity]\quad
An improved VQGAN-based[^Lezama2022Improved] codec that projects latent features onto a low-dimensional space (*e.g.*, 8 dimensions) prior to quantization.

We reproduce two variants: one utilizing three codebooks at a 25 Hz frame rate, and the other a single codebook at a 75 Hz frame rate.

Both configurations operate at a token rate of 75 Hz and achieve a bitrate of 0.75 kbps.

**SpeechTokenizer**[^Zhang2023Speechtokenizer]\quad
Enhances first-layer speech tokens via semantic distillation using features from HuBERT[^Hsu2021Hubert].

This tokenizer operates at 50 Hz and we use two codebooks for inference.

We use the official checkpoint\footnote{\url{https://github.com/ZhangXInFD/SpeechTokenizer}}.

**Mimi**[^D{\'e}fossez2024Moshi]\quad
Follows the design of SpeechTokenizer but utilizes WavLM[^Chen2022Wavlm] for semantic distillation.

The tokenizer employs eight codebooks, each of size 2,048, at a 12.5 Hz frame rate, resulting in a bitrate of 1.1 kbps.

We use the official checkpoint\footnote{\url{https://huggingface.co/kyutai/mimi}}.

**DualCodec**[^Li2025DualCodec]\quad
A state-of-the-art, low-frame-rate, semantically-enhanced neural audio codec designed for speech generation.

DualCodec directly encodes SSL representations[^Chung2021W2v-Bert] into first-layer codec tokens.

It adopt a configuration with a 12.5 Hz token rate and a 8-layer codebook hierarchy.

The first codebook contains 16,384 entries, while the remaining five each contain 4,096 entries, yielding a bitrate of 1.225 kbps.

We use the official checkpoint\footnote{\url{https://pypi.org/project/dualcodec/0.1.2/}}.

**BiCodec**[^Wang2025Spark-TTS]\quad
A semantically-enhanced tokenizer with a single-layer codebook.

It discretizes audio into semantic tokens based on features from wav2vec 2.0[^Baevski2020Wav2vec].

It operates at a token rate of 50 Hz with a codebook size of 8,192, achieving a bitrate of 0.65 kbps.

We use the official checkpoint\footnote{\url{https://github.com/SparkAudio/Spark-TTS}}.

**X-codec 2**[^Ye2025Llasa]\quad
Employs a dual-encoder design: a semantic encoder based on Wav2Vec2-BERT[^Barrault2023Seamless] and an acoustic encoder for low-level acoustic features.

Their outputs are concatenated prior to quantization.

It operates at a token rate of 50 Hz with a codebook size of 65,536, yielding a bitrate of 0.8 kbps.

We use the official checkpoint\footnote{\url{https://huggingface.co/HKUSTAudio/xcodec2}}.

**WavTokenizer**[^Ji2024Wavtokenizer]\quad
A single-codebook tokenizer trained on 800K hours of mixed-domain audio.

It operates at a 75 Hz token rate with a codebook size of 4,096, resulting in a bitrate of 0.9 kbps.

We use the official checkpoint\footnote{\url{https://huggingface.co/novateur/WavTokenizer-large-speech-75token}}.

**BigCodec**[^Xin2024Bigcodec]\quad
A single-codebook tokenizer with scaled model size.

It integrates sequential modules into convolutional architectures and applies low-dimensional quantization to enhance code utilization.

It operates at an 80 Hz token rate with a codebook size of 8,192, yielding a bitrate of 1.04 kbps.

We use the official checkpoint\footnote{\url{https://huggingface.co/Alethia/BigCodec}}.

**TAAE**[^Parker2024Scaling]\quad
A transformer-based tokenizer using Finite Scalar Quantization (FSQ)[^Mentzer2023Finite] for speech tokenization.

It operates at a 25 Hz token rate with a codebook size of 46,656, resulting in a bitrate of 0.4 kbps.

We use the official implementation\footnote{\url{https://github.com/Stability-AI/stable-codec}}.

**SemantiCodec**[^Liu2024Semanticodec]\quad
Combines a semantic encoder (AudioMAE[^Huang2022Masked] with k-means clustering) and an acoustic encoder, featuring a diffusion decoder for reconstruction.

It operates at a 50 Hz token rate, with codebook sizes of 16,384 (semantic) and 2,048 (acoustic), achieving a bitrate of 0.675 kbps.

We use the official implementation\footnote{\url{https://github.com/haoheliu/SemantiCodec-inference}}.

**Vevo Tokenizer**[^Zhang2025Vevo]\quad
A two-stage tokenizer utilizing features from HuBERT[^Hsu2021Hubert], followed by VQ and a diffusion decoder.

It employs a single codebook of size 8,192 at a 50 Hz token rate, resulting in a bitrate of 0.65 kbps.

We use the official checkpoint\footnote{\url{https://github.com/open-mmlab/Amphion/tree/main/models/vc/vevo}}.

**FireRedTTS Tokenizer**[^Guo2024FireRedTTS]\quad
A single-codebook tokenizer trained in two stages.

Transforms speech into semantic embeddings via features from HuBERT[^Hsu2021Hubert], followed by a ResNet-based encoder and quantization.

It uses a 40 ms frame shift and a codebook size of 16,384.

A global embedding is also incorporated, and decoding is performed using flow matching.

Its implementation is available\footnote{\url{https://github.com/FireRedTeam/FireRedTTS}}.

**CosyVoice Tokenizer**[^Du2024CosyVoice]\quad
A single-codebook tokenizer trained in two stages.

The encoder is initialized from an ASR model[^Radford2023Robust] and subsequently trained with a supervised loss.

A flow matching model is used to predict mel-spectrograms.

It operates at a 25 Hz token rate and 0.3 kbps bitrate.

Its code is available\footnote{\url{https://github.com/FunAudioLLM/CosyVoice}}.

**CosyVoice 2 Tokenizer**[^Du2024Cosyvoice]\quad
An improved version of CosyVoice that replaces VQ with FSQ.

It operates at a 25 Hz token rate and 0.325 kbps bitrate.

Its official implementation is available\footnote{\url{https://github.com/FunAudioLLM/CosyVoice}}.

**Ints Tokenizer**[^Zhang2025Advancing]\quad
Combines the DualCodec[^Li2025DualCodec] semantic encoder with a flow matching decoder, similar to the CosyVoice variants.

It uses a single codebook with 16,384 entries at a 12.5 Hz token rate, resulting in a bitrate of 0.175 kbps.

The resulting TTS model, Ints, demonstrates state-of-the-art intelligibility[^Zhang2025Advancing].

### Zero-shot TTS

\label{appendix:baseline_tts}

**F5-TTS**[^Chen2024F5-TTS]\quad An open-source flow matching-based TTS systems.

It follows E2 TTS[^Eskimez2024E2] and uses a flow matching transformer[^Lipman2022Flow], [^Le2024Voicebox] to convert the text to acoustic features directly[^Chen2024F5-TTS].

**MaskGCT**[^Wang2024Maskgct]\quad An open-source large-scale MGM-based TTS system that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction.

We use the official code and checkpoint\footnote{\url{https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct}} which is trained on Emilia[^He2024Emilia].

**ARS**[^Wang2024Maskgct]\quad Introduced as an AR baseline by[^Wang2024Maskgct]. and referred to as "AR + SoundStorm'' in the original paper[^Wang2024Maskgct].

It adopts a cascaded architecture, including the AR *text-to-token* and the NAR MGM *codec-to-waveform*[^Borsos2023Soundstorm].

**CosyVoice 2**[^Du2024Cosyvoice]\quad
An open-source, large-scale zero-shot TTS system built upon an AR model initialized from \texttt{Qwen2.5-0.5B-Instruct}, which predicts speech codes extracted by the CosyVoice 2 tokenizer.

**FireRedTTS**[^Guo2024FireRedTTS]\quad An open-source, large-scale AR-based zero-shot TTS system, which predicts speech codes extracted by the FireRedTTS tokenizer.

**Ints**[^Zhang2025Advancing]\quad An open-source, large-scale zero-shot TTS system built upon an AR model initialized from \texttt{Phi-3.5-mini-instruct}, which predicts 12.5 Hz speech codes extracted by the Ints tokenizer.

**SparkTTS**[^Wang2025Spark-TTS]\quad An open-source, large-scale zero-shot TTS system built upon an AR model initialized from \texttt{Qwen2.5-0.5B-Instruct}, which predicts speech codes extracted by the BiCodec[^Wang2025Spark-TTS].

**Llasa**[^Ye2025Llasa]\quad An open-source, large-scale zero-shot TTS system built upon an AR model initialized from \texttt{Llama3.2-1B}[^Grattafiori2024Llama], which predicts speech codes extracted by the X-codec 2[^Ye2025Llasa].
