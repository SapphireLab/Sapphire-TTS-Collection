# TaDiCodec

<details>
<summary>基本信息</summary>

- 标题: "TaDiCodec: Text-Aware Diffusion Speech Tokenizer for Speech Language Modeling."
- 作者:
  - 01 Yuancheng Wang
  - 02 Dekun Chen
  - 03 Xueyao Zhang
  - 04 Junan Zhang
  - 05 Jiaqi Li
  - 06 Zhizheng Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2508.16790v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2508.16790v1](_PDF/2025.08.22_2508.16790v1_TaDiCodec__Text-Aware_Diffusion_Speech_Tokenizer_for_Speech_Language_Modeling.pdf)
  - [Publication] #TODO

</details>

## 摘要

<!--
Speech tokenizers serve as foundational components for speech language models, yet current designs exhibit several limitations, including:
1) dependence on multi-layer residual vector quantization structures or high frame rates,
2) reliance on auxiliary pre-trained models for semantic distillation, and
3) requirements for complex two-stage training processes.
In this work, we introduce the ***Text-aware Diffusion Transformer Speech Codec*** (***TaDiCodec***), a novel approach designed to overcome these challenges.
***TaDiCodec*** employs end-to-end optimization for quantization and reconstruction through a diffusion autoencoder, while integrating text guidance into the diffusion decoder to enhance reconstruction quality and achieve optimal compression.
***TaDiCodec*** achieves an extremely low frame rate of **6.25 Hz** and a corresponding bitrate of **0.0875 kbps** with a **single-layer codebook** for 24 kHz speech, while maintaining superior performance on critical speech generation evaluation metrics such as Word Error Rate (WER), speaker similarity (SIM), and speech quality (UTMOS).
Notably, ***TaDiCodec*** employs a single-stage, end-to-end training paradigm, and obviating the need for auxiliary pre-trained models.
We also validate the compatibility of ***TaDiCodec*** in language model based zero-shot text-to-speech with both autoregressive modeling and masked generative modeling, demonstrating its effectiveness and efficiency for speech language modeling, as well as a significantly small *reconstruction-generation gap*.
We will open source our code and model checkpoints.
Audio samples are are available at [Github.IO](https:/tadicodec.github.io/).
We release code and model checkpoints at [Github](https:/github.com/HeCheng0625/Diffusion-Speech-Tokenizer).
-->
语音 Tokenizer 作为语音语言模型的基础组件, 但当前的设计存在诸多限制, 包括:
1. 依赖于多层残差向量量化结构或高帧率;
2. 依赖于辅助的预训练模型进行语义蒸馏;
3. 要求复杂的两阶段训练过程.

本工作提出了一种克服上述挑战的新方法 ***文本感知的扩散 Transformer 语音编解码器 (Text-Aware Diffusion Transformer Speech Codec, TaDiCodec)***.
***TaDiCodec*** 通过扩散自编码器实现了量化和重构的端到端优化, 在扩散解码器中集成了文本引导以增强重构质量并实现最佳压缩.
***TaDiCodec*** 使用单层码本, 对于 24kHz 语音取得了相当低的帧率 **6.25 Hz** 和相应的比特率 **0.0875 kbps**, 同时在关键的语音生成评估指标 (如词错误率 WER, 说话人相似性 SIM, 语音质量 UTMOS) 上保持了优越的性能.
值得注意的是, ***TaDiCodec*** 使用单阶段, 端到端的训练模式, 并消除了对辅助的预训练模型的需求.
我们也验证了 ***TaDiCodec*** 在基于语言模型的零样本文本到语音的兼容性, 包括自回归建模和掩码生成建模, 证明其在语音语言建模的有效性和效率, 以及较小的重构生成差距.
我们将开源我们的代码和模型检查点.
音频示例可在 [Github.IO](https:/tadicodec.github.io/) 获得.
我们将代码和模型检查点发布在 [Github](https:/github.com/HeCheng0625/Diffusion-Speech-Tokenizer).

## 1·引言

<--
Recent advances have been made in both large language model (LLM)-based text-to-speech (TTS) systems (**VALL-E**[^Wang2023Neural], **Seed-TTS**[^Anastassiou2024Seed-TTS], **CosyVoice**[^Du2024CosyVoice], **FireRedTTS**[^Guo2024FireRedTTS], **LLaSA**[^Ye2025Llasa], **Spark-TTS**[^Wang2025Spark-TTS], **MaskGCT**[^Wang2024Maskgct], **SPEAR-TTS**[^Kharitonov2023Speak,], **VoiceCraft**[^Peng2024VoiceCraft]) and spoken language models (**Moshi**[^D{\'e}fossez2024Moshi], **Zeng2024Scaling**[^Zeng2024Scaling], **GLM-4-Voice**[^Zeng2024GLM-4-Voice], **Baichuan-Audio**[^Li2025Baichuan-Audio], **Step-Audio**[^Huang2025Step-Audio], **LLaMA-Omni**[^Fang2024Llama-Omni], **Freeze-Omni**[^Wang2024Freeze-Omni], **Kimi-Audio**[^Ding2025Kimi-Audio], **Qwen2.5-Omni**[^Xu2025Qwen2.5-Omni]).
At the core of these systems lies the speech tokenizer, which converts continuous speech signals into discrete token sequences, thereby enabling the application of textual LLM paradigms to speech modeling.
Beyond this, speech tokenizers play a fundamental role in bridging the text and speech modalities, forming the basis for cross-modal learning, alignment, and generation.
-->



<--
However, most existing speech tokenizers are suboptimal for speech language modeling.
Prior works (e.g., **EnCodec**[^D{\'e}fossez2022High], **SoundStream**[^Zeghidour2021Soundstream], **DAC**[^Kumar2024High-Fidelity])
primarily target speech signal compression and transmission,
relying on multi-layer residual vector quantization (RVQ) and operating at high frame rates and bitrates.
Such configurations make modeling with language models challenging and inefficient.
More recently, several studies (**BigCodec**[^Xin2024Bigcodec], **WavTokenizer**[^Ji2024Wavtokenizer], **Spark-TTS**[^Wang2025Spark-TTS], **LLaSA**[^Ye2025Llasa], **TAAE/Stable-Codec**[^Parker2024Scaling] have explored techniques for single-layer speech tokenizers.
However, these approaches still fall short in reconstruction quality compared to RVQ-based tokenizers and often maintain high token rates (typically exceeding 50 tokens per second).
Moreover, they usually depend on complex loss designs and adversarial training.
Additionally, many of these models primarily optimize for acoustic-level reconstruction, resulting in discrete representations that lack semantic richness, making them suboptimal for language model modeling and causing *reconstruction-generation gap*.
-->

<--
Recent studies (**CosyVoice**[^Du2024CosyVoice], **Seed-TTS**[^Anastassiou2024Seed-TTS], **X-Codec**[^Ye2025Codec], **Moshi**[^D{\'e}fossez2024Moshi], **DualCodec**[^Li2025DualCodec], **FireRedTTS**[^Guo2024FireRedTTS], **Zeng2024Scaling**[^Zeng2024Scaling]) emphasize that effective speech tokens for language modeling should exhibit low frame rates and semantic richness,
which criteria that directly shape the design of modern speech tokenizers.
To achieve this, several works (**SpeechTokenizer**[^Zhang2023Speechtokenizer], **Moshi**[^D{\'e}fossez2024Moshi], **DualCodec**[^Li2025DualCodec], **X-Codec**[^Ye2025Codec]) decompose speech into semantic and acoustic tokens by distilling features from speech self-supervised learning (SSL) models (**WavLM**[^Chen2022Wavlm], **Wav2Vec2.0**[^Baevski2020Wav2vec], **HuBERT**[^Hsu2021Hubert], **BEST-RQ**[^Chiu2022Self-Supervised]).
In this framework, semantic tokens exhibit improved alignment with textual representations, thereby facilitating more effective language modeling.
However, preserving reconstruction quality often requires RVQ, along with intricate loss functions, adversarial objectives, and the integration of external SSL models.
An alternative line of work, including systems such as **CosyVoice**[^Du2024CosyVoice], **Seed-TTS**[^Anastassiou2024Seed-TTS], **FireRedTTS**[^Guo2024FireRedTTS], and **Vevo**[^Zhang2025Vevo], adopts a two-stage design: first quantizing SSL-derived features, then training a separate diffusion model (**DDPM**[^Ho2020Denoising], **Song2020Score**[^Song2020Score-Based], **Flow Matching**[^Lipman2022Flow]) to reconstruct speech conditioned on these tokens.
While this design enables relatively low frame rates and supports a single-layer token representation, it comes with several limitations:
**1. Two-stage training:** the pipeline introduces greater architectural complexity and reduced training efficiency compared to end-to-end approaches;
**2. External dependency:** it relies on pre-trained SSL or supervised models for semantic feature extraction; and
**3. Struggle with extreme compression:** most systems fail to achieve ultra-low token rates (e.g., fewer than 20 tokens per second), which are critical for modeling efficiency and scalability.
-->

<--
To address the limitations of current speech tokenizers, we propose the ***T**ext-**a**ware **Di**ffusion Transformer Speech **Codec*** (***TaDiCodec***), a novel model that achieves an exceptionally low frame rate of **6.25 Hz** using a **single codebook**, corresponding to a bitrate of **0.0875 kbps** for 24 kHz speech.
Despite this ultra-low rate, ***TaDiCodec*** delivers high-fidelity speech reconstruction and robust performance on downstream speech language modeling tasks.
Specifically: **1)** ***TaDiCodec*** unifies quantization and reconstruction within an **end-to-end diffusion autoencoder**, removing the need for separate semantic distillation or complex adversarial objectives by relying solely on diffusion loss;
**2)** it enhances reconstruction quality and compression efficiency by incorporating **text and prompt guidance** into the diffusion decoder.
Our design is motivated by the increasing availability of transcriptions from automatic speech recognition (ASR) systems (**Whisper**[^Radford2023Robust], **FunASR**[^Gao2023Funasr]), and the widespread use of paired speech-text data in generative applications.
In zero-shot TTS scenarios, for instance, the target text is inherently available; in end-to-end spoken language systems, speech and text tokens are typically generated jointly (**GLM-4-Voice**[^Zeng2024GLM-4-Voice], **Baichuan-Audio**[^Li2025Baichuan-Audio], **Step-Audio**[^Huang2025Step-Audio], **Kimi-Audio**[^Ding2025Kimi-Audio], **LLaMA-Omni**[^Fang2024Llama-Omni], **Freeze-Omni**[^Wang2024Freeze-Omni], **LUCY**[^Gao2025Lucy]).
-->

<--
Our experiments show that ***TaDiCodec*** achieves performance comparable to or better than existing speech tokenizers in both reconstruction and downstream speech generation, while maintaining a significantly smaller gap between reconstruction and generation.
In addition, it adopts a much simpler pipeline and operates with much fewer tokens.
We evaluate zero-shot TTS using ***TaDiCodec*** under both autoregressive and masked language modeling settings, achieving strong results in intelligibility, speaker similarity, speech quality, and overall training and inference efficiency.
A comparison with other tokenizers is presented in [Fig.01](#fig:comparsion) and [Table.01](#tab:tokenizer-rec-res).
-->

<--
The contributions of our work are summarized as follows:
-  We propose ***TaDiCodec***, a novel speech tokenizer with a token rate of 6.25 Hz and a bitrate of 0.0875 kbps, based on a diffusion autoencoder that jointly performs quantization and reconstruction without adversarial training, external pretrained models for semantic distillation, or multi-stage training.
This design enables efficient optimization and simplifies the speech tokenization pipeline.
-  We introduce text-aware and prompt-guided decoding into the diffusion process to facilitate extreme compression.
By leveraging paired speech-text data, this approach enhances reconstruction quality and enables high intelligibility, speaker similarity, and speech quality under ultra-low token rates.
-  We build zero-shot TTS models using our tokenizer under both autoregressive and masked language modeling settings, achieving WERs of 2.28 and 1.19 on *SeedTTS test-en* and *test-zh*, respectively.
Our models demonstrate notable improvements on challenging benchmarks such as articulatory, code-switching, and cross-lingual test sets, and support real-time inference with RTFs ranging from 0.12 to 0.29 across different model sizes.
-->

![](image/tadicodec_3d.pdf)

<a id="fig:comparsion">
**Comparison between ***TaDiCodec*** and other speech tokenizers.
We use a three-dimensional coordinate system to display the performance across three dimensions: the x-axis represents WER, the y-axis represents UTMOS, and the z-axis represents SIM.
The size of the markers is proportional to the kbps value.
</a>

## 2·Related Work

**Discrete Speech Tokenizer**
Discrete speech tokenizers convert continuous speech into discrete tokens, enabling modern zero-shot TTS and speech language modeling.
Early tokenizers (**SoundStream**[^Zeghidour2021Soundstream], **EnCodec**[^D{\'e}fossez2022High], **DAC**[^Kumar2024High-Fidelity]) focused on audio compression, relying on residual vector quantization (RVQ) (**SoundStream**[^Zeghidour2021Soundstream], **RQ-VAE**[^Lee2022Autoregressive]) and operating at high frame rates and bitrates, settings ill-suited for language modeling.
Recent work has shifted toward designing tokenizers tailored for language modeling, emphasizing low frame rates (**Moshi**[^D{\'e}fossez2024Moshi], **DualCodec**[^Li2025DualCodec]), semantic-rich representations (**SpeechTokenizer**[^Zhang2023Speechtokenizer], **Moshi**[^D{\'e}fossez2024Moshi], **DualCodec**([^Li2025DualCodec], **Spark-TTS**[^Wang2025Spark-TTS], **X-Codec**[^Ye2025Codec], **LLaSA**[^Ye2025Llasa], **SemantiCodec**[^Liu2024Semanticodec], **FireRedTTS**[^Guo2024FireRedTTS], **CosyVoice2**[^Du2024Cosyvoice], **Vevo**[^Zhang2025Vevo]), and single-layer codebooks (**TAAE/Stable-Codec**[^Parker2024Scaling], **BigCodec**[^Xin2024Bigcodec], **WavTokenizer**[^Ji2024Wavtokenizer]).
Diffusion-based methods (**DDPM**[^Ho2020Denoising], **Song2020Score**[^Song2020Score-Based]) have gained popularity for their performance at low token rates and scalability.
However, they typically follow a two-stage pipeline: extracting tokens via self-supervised speech representations (**W2V-BERT**[^Chung2021W2v-Bert], **Wav2Vec2.0**[^Baevski2020Wav2vec], **WavLM**[^Chen2022Wavlm], **Whisper**[^Radford2023Robust], **HuBERT**[^Hsu2021Hubert], **BEST-RQ**[^Chiu2022Self-Supervised]), then reconstructing waveforms through diffusion.
For example, **FlowDec**[^Welker2025FlowDec], **LaDiffCodec**[^Yang2024Generative] apply diffusion to improve de-tokenization quality, but still operate at relatively high token rates.
Achieving ultra-low bitrates (e.g., below 0.2 kbps or 20 tokens/s) with a compact, generative-friendly framework remains a major challenge.

**Zero-shot TTS**
Modern zero-shot TTS systems typically operate on discrete speech tokens using either autoregressive (AR) language modeling (**VALL-E**[^Wang2023Neural], **Seed-TTS**[^Anastassiou2024Seed-TTS], **CosyVoice**[^Du2024CosyVoice], **CosyVoice2**[^Du2024Cosyvoice], **FireRedTTS**[^Guo2024FireRedTTS], **LLaSA**[^Ye2025Llasa], **Spark-TTS**[^Wang2025Spark-TTS], **SPEAR-TTS**[^Kharitonov2023Speak,], **VoiceCraft**[^Peng2024VoiceCraft]) or masked generative (language) modeling (MGM) (**NaturalSpeech3**[^Ju2024NaturalSpeech], **SoundStorm**[^Borsos2023Soundstorm], **MaskGCT**[^Wang2024Maskgct], **PALLE**[^Yang2025Pseudo-Autoregressive]).
Some models (**MaskGCT**[^Wang2024Maskgct], **Seed-TTS**[^Anastassiou2024Seed-TTS], **CosyVoice2**[^Du2024Cosyvoice], **CosyVoice**[^Du2024CosyVoice], **FireRedTTS**[^Guo2024FireRedTTS]) adopt an "AR + diffusion" framework, where a diffusion decoder enhances waveform quality based on predicted tokens.
Zero-shot TTS is also foundational in recent end-to-end spoken language models.
For example, **Qwen2.5-Omni**[^Xu2025Qwen2.5-Omni] uses a "talker" module to generate speech tokens from the text output of a "thinker".
Similar architectures (**Freeze-Omni**[^Wang2024Freeze-Omni], **LLaMA-Omni**[^Fang2024Llama-Omni]) decode speech directly from text, while others (**Step-Audio**[^Huang2025Step-Audio], **Kimi-Audio**[^Ding2025Kimi-Audio], **Zeng2024Scaling**[^Zeng2024Scaling], **GLM-4-Voice**[^Zeng2024GLM-4-Voice]) leverage TTS models to synthesize large-scale speech corpora for training dialogue agents.

## 3·Method

### TaDiCodec

**Speech Tokenization with Diffusion Transformer Autoencoder**
Some speech tokenizers adopt raw waveform signals as modeling targets.
However, raw waveforms often contain a considerable amount of redundant information.
In this work, we instead adopt the mel-spectrogram as both the input and reconstruction target for the tokenizer, given its compactness and ease of inversion to waveform using vocoder models (**BigVGAN**[^Lee2022Bigvgan], **Vocos**[^Siuzdak2023Vocos]).
Formally, we denote the input mel-spectrogram as $\boldsymbol{x} \in \mathbb{R}^{T \times d}$, where $T$ denotes the number of frames, corresponding to the number of waveform frames divided by the hop size.
The tokenizer's encoder $\mathcal{E}$ transforms $\boldsymbol{x}$ into a sequence of latent embeddings, *i.e.*, $\mathcal{E}(\boldsymbol{x})$.
These embeddings are then quantized by the vector quantization (VQ) module $\mathcal{Q}$ into a discrete token sequence $\boldsymbol{q} = \mathcal{Q}(\mathcal{E}(\boldsymbol{x})) \in \mathbb{Z}^{T_q \times 1}$, where $T_q$ is the length of the token sequence, typically equal to $T$ divided by a predefined down-sampling factor.
Each token $q_i$ (for $i \in [0, T_q)$) corresponds to an index in a codebook.
The decoder $\mathcal{D}$ subsequently reconstructs the mel-spectrogram as $\hat{\boldsymbol{x}} = \mathcal{D}(\boldsymbol{q})$.
Previous speech tokenizers primarily adopted **generative adversarial networks (GANs)**[^Goodfellow2020Generative] for training the system, typically operating on short speech segments (e.g., 1–3 seconds) and employing CNNs as the backbone.
However, GANs often suffer from issues related to training stability and efficiency,
and the reliance on CNN-based architectures and short-segment training further constrains the model's ability to capture long-range dependencies, leading to a focus on only local acoustic patterns.
To overcome these limitations, we use a fully **Transformer**-based[^Vaswani2017Attention] architecture for both the encoder and decoder, and adopt a diffusion loss for reconstruction training, enabling more stable optimization and improved modeling capabilities.
Specifically, we adopt a flow matching-based decoder (**Flow Matching**[^Lipman2022Flow], **Rectified Flow**[^Liu2022Flow]).
During training, we sample Gaussian noise $\boldsymbol{\epsilon}$ and generate a noisy target $\boldsymbol{x}_t$ via a linear interpolation: $\boldsymbol{x}_t = t\boldsymbol{x} + (1 - t)\boldsymbol{\epsilon}$, where $t \in [0, 1]$ is a randomly sampled noise level.
The model is then trained to predict the velocity field $\boldsymbol{v}$, defined as the derivative of $\boldsymbol{x}_t$ with respect to $t$, *i.e.*, $\boldsymbol{v} = \frac{d\boldsymbol{x}_t}{dt} = \boldsymbol{x} - \boldsymbol{\epsilon}$.
We provide more details about flow matching in Appendix~[appendix:fm](#appendix:fm).

Fig.02: **Training speech tokenizer with diffusion autoencoder.**
We optimize tokenization and reconstruction end-to-end with diffusion loss.
The input $\boldsymbol{x}$ is passed through the encoder and quantizer to get $\mathcal{Q}(\mathcal{E}(\boldsymbol{x}))$, which is then conditioned and input into the DiT decoder to predict the velocity $\boldsymbol{v}$ corresponding to the noisy $\boldsymbol{x}_{t}$.

**Binary Spherical Quantization**
For quantization, we use **Binary Spherical Quantization (BSQ)**[^Zhao2024Image], which does not rely on an explicit learnable codebook.
We first apply downsampling to the encoder output $\mathcal{E}(\boldsymbol{x})$, followed by a linear projection to obtain a low-dimensional latent sequence:
$$
\boldsymbol{h} = \text{Linear}(\text{Downsample}(\mathcal{E}(\boldsymbol{x}))) \in \mathbb{R}^{T_q \times L},
$$
where $T_q$ is the number of quantized frames and $L$ is the latent dimension.

Each vector $\boldsymbol{h}_t \in \mathbb{R}^L$ of $\boldsymbol{h}$ is then projected onto the unit sphere:
$$
\boldsymbol{u}_t = \frac{\boldsymbol{h}_t}{\|\boldsymbol{h}_t\|}.
$$
Binary quantization is applied independently on each dimension:
$$
\hat{\boldsymbol{u}}_t = \frac{1}{\sqrt{L}} \, \text{sign}(\boldsymbol{u}_t),
$$
where $\text{sign}(x)$ is the element-wise sign function.

To enable gradient flow through the quantization step, we adopt a Straight-Through Estimator (STE):
$$
\text{sign}_{\text{STE}}(x) = \text{sg}(\text{sign}(x) - x) + x,
$$
where $\text{sg}(\cdot)$ denotes the stop-gradient operation.

The quantized latent sequence $\hat{\boldsymbol{u}} \in \mathbb{R}^{T_q \times L}$ is then mapped back to the $d$-dimensional space and upsampled to the original temporal resolution:
$$
\text{Upsample}(\text{Linear}(\hat{\boldsymbol{u}})) \in \mathbb{R}^{T \times d}.
$$
Each quantized vector $\boldsymbol{h}_t$ corresponds to a discrete token index computed by:
$$
k_t = \sum_{i=1}^L 1_{[\boldsymbol{h}_{t,i} > 0]} \cdot 2^{i-1},
$$

where $1_{[\cdot]}$ is the indicator function.

As noted in **BSQ**[^Zhao2024Image], BSQ can be optimized without the need for a commitment loss (**VQ-VAE**[^Van2017Neural]), since its quantization error is theoretically bounded.
This property enables end-to-end training of the system using only the diffusion loss.
See Appendix~[appendix:bsq](#appendix:bsq) for further details.

**Text-aware De-Tokenization**
Most existing speech tokenizers rely solely on speech features for reconstruction. *However, in the context of speech language modeling, the corresponding text associated with the speech is often readily available.*
For example, in TTS, the target text is always known, and in most end-to-end spoken dialogue systems, text and speech tokens are generated jointly (**Moshi**[^D{\'e}fossez2024Moshi], **Zeng2024Scaling**[^Zeng2024Scaling], **GLM-4-Voice**[^Zeng2024GLM-4-Voice], **Baichuan-Audio**[^Li2025Baichuan-Audio], **Step-Audio**[^Huang2025Step-Audio], **LLaMA-Omni**[^Fang2024Llama-Omni], **Freeze-Omni**[^Wang2024Freeze-Omni], **Kimi-Audio**[^Ding2025Kimi-Audio], **Qwen2.5-Omni**[^Xu2025Qwen2.5-Omni]).
Motivated by this observation, we propose a **text-aware de-tokenization** strategy, which conditions the diffusion decoder on the corresponding text sequence $\boldsymbol{x}_{text}$.
To further improve reconstruction quality under the extremely low compression rate setting, we introduce a **prompt mechanism** into ***TaDiCodec***, similar to prior works (**VoiceBox**[^Le2024Voicebox], **F5-TTS**[^Chen2024F5-TTS], **E2-TTS**[^Eskimez2024E2], **MaskGCT**[^Wang2024Maskgct]).
This mechanism enables the model *to better reconstruct speech when a prompt is provided, making it particularly suitable for speech generation scenarios such as zero-shot TTS and the decoding stage of spoken language models.*
Specifically, during training, we randomly sample a prefix $\boldsymbol{x}_{prompt}$ from the input mel-spectrogram by drawing a segment length \( l \sim \text{Uniform}(0, 0.25L) \), where \( L \) denotes the total number of frames in the mel-spectrogram.
The prefix is preserved without any added noise, while the loss is computed solely on the noisy portion of the sequence.
Table~[tab:ablation](#tab:ablation) shows this prompt mechanism yields substantial improvements in reconstruction performance.
We also experiment with removing text conditioning from the decoder and observe significant performance degradation under extremely low token rate and bitrate settings. e.g., at a frame rate of 12.5 Hz, the WER exceeds 10.
Notably, Unlike prior works (**CosyVoice**[^Du2024CosyVoice], **FireRedTTS**[^Guo2024FireRedTTS], **Vevo**[^Zhang2025Vevo], **Seed-TTS**[^Anastassiou2024Seed-TTS], **SemantiCodec**[^Liu2024Semanticodec], **MaskGCT**[^Wang2024Maskgct], **Metis**[^Wang2025Metis], **Zeng2024Scaling**[^Zeng2024Scaling]) that adopt a two-stage pipeline: first training a VQ model and then a separate diffusion model for de-tokenization, our tokenizer **jointly learns feature quantization and reconstruction in an end-to-end manner**.
The overall training objective of ***TaDiCodec*** can be formulated as:
$$
\mathcal{L}_{\text{diff}} = \mathbb{E}_{(\boldsymbol{x}, \boldsymbol{x}_{text}), \boldsymbol{\epsilon}, t} \left[ \| (\boldsymbol{x} - \boldsymbol{\epsilon}) - \mathcal{D}_\phi(\mathcal{Q}(\mathcal{E}_\theta(\boldsymbol{x})), \boldsymbol{x}_t, t, \boldsymbol{x}_{text}) \| \right],
$$

where \(\mathcal{E}_\theta\) and \(\mathcal{D}_\phi\) are the encoder and decoder parameterized by \(\theta\) and \(\phi\).

We ignore the prompt for simplification.
We also find that continuing to train the decoder while freezing the encoder and VQ module can further improve performance.

### Speech Language Modeling with TaDiCodec

Existing speech tokenizers often neglect their effectiveness in downstream speech language modeling tasks and suffer from a pronounced *reconstruction–generation gap*.
In this work, we apply our tokenizer to large-scale multilingual zero-shot TTS, adopting an *"AR + Diffusion"* paradigm: an autoregressive model first predicts speech tokens $\boldsymbol{q}$ from text $\boldsymbol{x}_{text}$, which are then passed, along with the text, to ***TaDiCodec***'s diffusion decoder to generate speech.
The AR model, parameterized by $\psi$, is optimized to minimize the negative log-likelihood of the target token sequence conditioned on the input text and previously predicted tokens:
$$
\mathcal{L}_{\text{AR}} = - \mathbb{E}_{(\boldsymbol{q}, \boldsymbol{x}_{text})} \sum_{i=1}^{T_q} \log p(\boldsymbol{q}_i \mid \boldsymbol{q}_{<i}, \boldsymbol{x}_{text}; \psi),
$$

where $\boldsymbol{q}_i$ is the $i$-th token of $\boldsymbol{q}$.

We also apply the non-autoregressive Masked Generative Modeling (MGM) (**MaskGIT**[^Chang2022Maskgit], **MaskGCT**[^Wang2024Maskgct]) for modeling speech tokens.
See more details about MGM in the Appendix~[appendix:mgm](#appendix:mgm).

## 4·Experiments

We first describe the implementation details and datasets (Section~[sec:exp-set](#sec:exp-set)).
We then present the speech reconstruction results of ***TaDiCodec*** in Section~[sec:speech_rec](#sec:speech_rec), including the main results (Section~[sec:tokenizer_main_res](#sec:tokenizer_main_res), Table~[tab:tokenizer-rec-res](#tab:tokenizer-rec-res)), multilingual performance (Table~[tab:rec-multilingual](#tab:rec-multilingual)), subjective evaluation results (Table~[tab:cmos](#tab:cmos)), and ablation studies on tokenizer design (Section~[sec:tokenizer_ablation](#sec:tokenizer_ablation), Table~[tab:ablation](#tab:ablation)).
Section~[sec:zs-tts](#sec:zs-tts) reports the zero-shot TTS results of models built upon ***TaDiCodec*** (Table~[tab:zs-tts-res](#tab:zs-tts-res)), along with results on model size scaling and training and inference efficiency (Table~[tab:tts-scaling-rtf](#tab:tts-scaling-rtf)), and an analysis of the reconstruction–generation gap (Figure~[fig:rec-gen-gap](#fig:rec-gen-gap)).

### Experimental Settings

**Datasets**
We use the **Emilia**[^He2024Emilia] dataset to train all of our models.
Emilia is a multilingual and diverse in-the-wild speech dataset designed for large-scale speech generation.
It contains 46.8K hours of English, 49.9K hours of Chinese, 1.6K hours of German, 1.4K hours of French, 1.7K hours of Japanese, and 0.2K hours of Korean.

**Implementation Details**
We build ***TaDiCodec*** using standard Llama-style Transformer blocks (**LLaMA2**[^Touvron2023Llama]), with bidirectional attention instead of causal attention.
The base configuration employs an 8-layer encoder and a 16-layer decoder, each with hidden size 1024, intermediate size 4096, and 16 attention heads.
We further explore decoder variants; see Section~[sec:tokenizer_ablation](#sec:tokenizer_ablation) and Table~[tab:ablation](#tab:ablation) for details.
We adopt RoPE positional embedding (**RoFormer**[^Su2024Roformer]) and **RMSNorm**[^Zhang2019Root].
For the text-aware diffusion decoder, RMSNorm is modified to Adaptive RMSNorm to condition on the diffusion step embedding.
Text tokens are adapted from a pretrained LLM vocabulary (**Phi-3**[^Abdin2024Phi-3], **Qwen2.5**[^Yang2024Qwen2.5]), and concatenated with speech features along the time axis before being input to the decoder.
For vector quantization, we use **BSQ**[^Zhao2024Image] with a latent size of 14, yielding a codebook size of $2^{14} = 16384$.
All models are trained on 8 80GB NVIDIA A100 GPUs using dynamic batching with 200 seconds of speech per batch.
We train the tokenizer for 800K steps using **AdamW**[^Loshchilov2017Decoupled] with a learning rate of $7.5\times10^{-5}$ and 32K warmup steps.
TTS models are trained for 300K steps with a learning rate of $3\times10^{-4}$ unless otherwise specified.
AR models extend the vocabulary of pretrained textual LLMs (**CosyVoice**[^Du2024CosyVoice], **LLaSA**[^Ye2025Llasa]) and are trained with 0.2B, 0.5B, 3.0B, and 4.0B parameters; see Section~[sec:zs-tts](#sec:zs-tts) for analysis.
For MGM models, we follow the setup of **MaskGCT**[^Wang2024Maskgct].

**Evaluation**
We evaluate our approach from two main perspectives: speech reconstruction using the proposed tokenizer (Section~[sec:speech_rec](#sec:speech_rec)) and zero-shot TTS performance (Section~[sec:zs-tts](#sec:zs-tts)).
We assess intelligibility (WER), speaker similarity (SIM), and speech quality (UTMOS).
Speaker similarity is computed as the cosine similarity between **WavLM-TDNN** embeddings of the prompt and generated speech (**WavLM**[^Chen2022Wavlm]).
WER is measured using **Whisper-Large-V3**[^Radford2023Robust] for non-Chinese languages and **Paraformer-ZH (FunASR)**[^Gao2023Funasr] for Chinese, following prior work (**Metis**[^Wang2025Metis], **MaskGCT**[^Wang2024Maskgct], **Seed-TTS**[^Anastassiou2024Seed-TTS], **CosyVoice**[^Du2024CosyVoice]).
Speech quality is evaluated using the official UTMOS checkpoint.
In addition to objective metrics, we conduct subjective evaluation via Comparative Mean Opinion Score (CMOS).
We do not report signal-level metrics (e.g., PESQ, STOI), as our focus is on generation-oriented performance, in line with **Metis**[^Wang2025Metis], **AnyEnhance**[^Zhang2025AnyEnhance].
Further Evaluation details are provided in Appendix~[appendix:eval](#appendix:eval).

### Speech Reconstruction

#### Main Results

We report our main results on **SeedTTS test-en**[^Anastassiou2024Seed-TTS] in Table~[tab:tokenizer-rec-res](#tab:tokenizer-rec-res).
We also evaluate our methods on multilingual test sets in Table~[tab:rec-multilingual](#tab:rec-multilingual).
Subjective evaluation results are show in Table~[tab:cmos](#tab:cmos).

**Baselines**
We compare with a wide range of baselines in settings where the token rate is less than 150:
**1)** single stage with multi-layer codebook and adversarial training: **EnCodec**[^D{\'e}fossez2022High], **DAC**[^Kumar2024High-Fidelity], **SpeechTokenizer**[^Zhang2023Speechtokenizer], **Mimi**[^D{\'e}fossez2024Moshi], **DualCodec**[^Li2025DualCodec];
**2)** single stage with single-layer codebook and adversarial training: DAC (with single VQ), **BiCodec (Spark-TTS)**[^Wang2025Spark-TTS], **X-codec 2 (LLaSA)**[^Ye2025Llasa], **WavTokenizer**[^Ji2024Wavtokenizer], **BigCodec**[^Xin2024Bigcodec], **TAAE/Stable-Codec**[^Parker2024Scaling];
**3)** two stage with diffusion decoder: **SemantiCodec**[^Liu2024Semanticodec], **Vevo** Tokenizer[^Zhang2025Vevo], **FireRedTTS** Tokenizer[^Guo2024FireRedTTS], **CosyVoice**[^Du2024CosyVoice] \& **CosyVoice2** Tokenizer[^Du2024Cosyvoice], **Ints** Tokenizer[^Zhang2025Advancing].

We provide more detailed description of these baselines in Appendix~[appendix:baseline_codec](#appendix:baseline_codec).

**Results Analysis**

**1) Compression:**
***TaDiCodec*** demonstrates a significantly higher compression rate compared to all baselines.
It operates at a token rate of 6.25 Hz with a single-layer codebook, resulting in a bitrate of 0.0875 kbps.
Among the baselines, the closest in compression rate to ***TaDiCodec*** is the Ints Tokenizer, which has double the token rate and bitrate of ***TaDiCodec***.
However, it performs worse in terms of WER (7.14 vs. 2.73) and UTMOS (3.37 vs. 3.73) and requires two-stage training and semantic distillation.
All other baselines have a token rate greater than 25 and a bitrate of at least 0.3 kbps.
Compared to other single-stage and distillation-free models, BigCodec has a higher WER (3.25 vs. 2.73) and lower SIM (0.61 vs. 0.69) than ***TaDiCodec***, with a bitrate of 1.04 kbps.
Models with lower bitrates, such as TAAE, still have bitrates four times higher than ours and perform significantly worse in WER and SIM.
Other single-layer codebook tokenizers like BiCodec, X-codec 2, and WavTokenizer have bitrates 7.4, 9.1, and 10.3 times higher, respectively.

**2) Reconstruction Quality:**
In terms of WER, ***TaDiCodec*** achieves a score of 3.02 without  decoder continued-training and 2.73 with fine-tuning, ranking just behind DualCodec and X-codec 2, which have scores of 2.57 and 2.63, respectively, but with bitrates 10.6 and 9.1 times higher.
Table~[tab:ablation](#tab:ablation) shows that our setting with a bitrate of 0.175 kbps achieves the best WER.
In terms of SIM, TaDiCodec with  decoder continued-training achieves the best SIM of 0.69, while even without  decoder continued-training, it reaches 0.67, surpassing all baselines except for the CosyVoice 2 tokenizer.
In terms of UTMOS, our model achieves scores of 3.68 and 3.73 (with and without  decoder continued-training), ranking just behind DualCodec and TAAE, which have scores of 3.78 and 3.87.
However, these models operate at much higher bitrates of 0.925 kbps and 0.4 kbps and demonstrate poorer performance in SIM.

**Results for Multilingual**
As shown in Table~[tab:rec-multilingual](#tab:rec-multilingual), ***TaDiCodec*** achieves the best WER on English, Chinese, German, and Korean, with especially low WER on Chinese.
It also outperforms all baselines in speaker similarity across all evaluated languages.

<a id="tab:rec-multilingual">
**Results of multilingual speech reconstruction.**
In addition to English, we evaluate on five other languages: Chinese (zh), French (fr), German (de), Japanese (ja), and Korean (ko).
</a>

**Subject Evaluation Result**
As shown in Table~[tab:cmos](#tab:cmos), our proposed system achieves the highest CMOS score among evaluated baselines.
More details about subjective evaluation are shown in Appendix~[appendix:sub_eval](#appendix:sub_eval).

#### Ablation Study

In this section, we explore several designs for ***TaDiCodec***.
For the ablation study, we report the results on *SeedTTS test-en* and *SeedTTS test-zh*.
**1) Vector Quantization Scheme:**
Replacing BSQ with a standard VQ tokenizer (implemented following **VIM**[^Yu2021Vector-Quantized], **DAC**[^Kumar2024High-Fidelity] with an explicit codebook of the same size as BSQ) leads to consistent degradation across all evaluation metrics.
This indicates that BSQ more effectively preserves both speech quality and intelligibility.
**2) Tokenizer Size Scaling:**
Reducing the decoder size to 160M results in substantial performance drops, particularly in English WER.
In contrast, increasing the decoder size results in marginal improvements.
These results also imply the existence of a scaling law for ***TaDiCodec***, warranting further investigation in future work.
**3) Prompt Mechanism:**
The introduction of the prompt mechanism substantially improves all three evaluation metrics.
A plausible explanation is that the prompt serves as a global conditioning signal (e.g., speaker identity), thereby reducing the quantizer's burden to encode such global information.
**4) Inference Time Scaling:**
Increasing the number of inference steps to 50 yields marginal improvements, while reducing it to 10 leads to slight degradation.
However, further reduction to 5 steps results in a noticeable drop in performance.
Considering the trade-off between efficiency and quality, using 10 to 32 steps appears to be a reasonable operating range.
We aim to achieve comparable performance with fewer inference steps (e.g., 1-2 steps) by leveraging techniques such as **Consistency Model**[^Song2023Consistency], [^Zhou2024Score], [^Frans2024One].
**5) Decoder Continued-training:**
We explore freezing the encoder and the VQ module while only continued-training the decoder for an additional 400K steps, focusing solely on reconstruction.
This approach yields further improvements, with WER dropping from 3.02 to 2.73 for English and from 1.11 to 0.94 for Chinese.
SIM also improves for both languages.
**6) Diffusion vs. GAN:**
We also replace the diffusion loss with **PatchGAN**[^Demir2018Patch-Based], but observe a noticeable performance drop in both intelligibility and speech quality.

<a id="tab:cmos">
**Subjective CMOS scores.**
We randomly choose 40 samples from a in-the-wild data source.
Comparisons between different models can also be found in demo page.
</a>

### Zero-shot TTS

In this section, we present the zero-shot TTS results using ***TaDiCodec*** as the prediction target.
We evaluate two different language modeling approaches: autoregressive (AR) and masked generative modeling (MGM) and we denote our models as "***TaDiCodec***-AR" and "***TaDiCodec***-MGM" respectively.
The results are reported on eight test sets, including *SeedTTS test-en* and *SeedTTS test-zh*, referred to as *Regular en* and *Regular zh*, which are widely adopted benchmarks for TTS evaluation **MaskGCT**[^Wang2024Maskgct], **Seed-TTS**[^Anastassiou2024Seed-TTS], **CosyVoice**[^Du2024CosyVoice], **CosyVoice2**[^Du2024Cosyvoice], **F5-TTS**[^Chen2024F5-TTS], **LLaSA**[^Ye2025Llasa], **Spark-TTS**[^Wang2025Spark-TTS].
In addition, we report performance on more challenging test sets, proposed in **Ints**[^Zhang2025Advancing], covering articulatory scenarios (such as repeated words and tongue twisters), code-switching, and cross-lingual settings.
We provide more details about the evaluation datasets in Appendix~[appendix:test_sets](#appendix:test_sets).

**Baselines**
We compare with a wide range of open-source and state-of-the-art baselines including:
**1)** AR-based Systems: **ARS (in MaskGCT)**[^Wang2024Maskgct], **CosyVoice2**[^Du2024Cosyvoice], **FireRedTTS**[^Guo2024FireRedTTS], **Ints**[^Zhang2025Advancing], **Spark-TTS**[^Wang2025Spark-TTS], **LLaSA**[^Ye2025Llasa];
**2)** NAR-based systems: **MaskGCT**[^Wang2024Maskgct] and **F5-TTS**[^Chen2024F5-TTS].
We provide more detailed description of these baselines in Appendix~[appendix:baseline_tts](#appendix:baseline_tts).

**Main Results**
We report the main results of our models and baselines on eight test sets in Table~[tab:zs-tts-res](#tab:zs-tts-res).
Our models exhibit significant improvements in intelligibility while maintaining speaker similarity comparable to state-of-the-art zero-shot TTS systems.
In terms of WER, **\myname{**-AR achieves the best performance on the *Regular en* and *Regular zh* test sets, reaching 2.28 and 1.19 respectively}, and outperforming all baselines.
On more challenging test sets, ***TaDiCodec***-AR demonstrates even more pronounced advantages, for example, reducing WER from 15.03 to 9.16 on *Code-switching en*, and from 4.88 to 2.91 on *Cross-lingual en2zh*.
Notably, these improvements are achieved without any task-specific optimization or reinforcement learning fine-tuning[^Rafailov2023Direct], [^Ouyang2022Training] on WER, as done in work such as **CosyVoice2**[^Du2024Cosyvoice].
For ***TaDiCodec***-MGM, it consistently outperforms or matches the performance of state-of-the-art NAR zero-shot TTS systems across all test sets.
Even with only 10 inference steps, which is significantly more efficient, it achieves a WER of 1.69 on *Regular zh*, compared to 2.28 from MaskGCT.
On more challenging test sets, such as *Cross-lingual en2zh*, it reaches 4.66 (vs. 13.78 from F5-TTS), and on *Code-switching en*, it achieves 14.94 (vs. 35.35 from F5-TTS).
In terms of SIM, both ***TaDiCodec***-AR and ***TaDiCodec***-MGM show clear advantages over recent systems such as **FireRedTTS**, SparkTTS, and Llasa.
Their SIM scores are slightly lower than those of MaskGCT and CosyVoice 2, which operate at higher frame rates of 50 Hz and 25 Hz, respectively.

<a id="tab:zs-tts-res">
**The zero-shot TTS results.**
Beyond regular cases, we also evaluate on challenging scenarios, including articulatory, code-switching, and cross-lingual settings.
</a>

**Model Size Scaling, Training and Inference Efficiency**
We demonstrate that our 6.25 Hz tokenization is not only effective but also significantly **more efficient for both training and generation**.
We further explore how scaling the model size affects both performance and efficiency.
Results are shown in table~[tab:tts-scaling-rtf](#tab:tts-scaling-rtf).
As described in the implementation details, we train all our TTS models for 300K steps.
We find that the models achieve the optimal evaluation results at around 200K steps.
All models can be trained in approximately one day under our setup, which uses 8 NVIDIA A100 GPUs with flash attention and bf16 precision.
For inference efficiency, we measure using Real-Time Factor (RTF).
We use a 5-second speech as a prompt to generate approximately 10 seconds of speech, sampling 5 times and taking the average.
The experiments show that even with 4.0B parameters, our AR model can achieve an RTF of 0.29 without any deployment acceleration.
With vLLM[^Kwon2023Efficient], the 4.0B AR model can achieve an RTF of 0.13.
Additionally, the 0.6B ***TaDiCodec***-MGM model achieves an RTF of 0.12.
We also observe a reasonable improvement in performance with increasing model parameters, especially on challenging test sets (*Articulatory*, *Code-switching*, and *Cross-lingual*).
Notably, our 0.5B model already matches or surpasses many state-of-the-art systems with an RTF of 0.22.

<a id="tab:tts-scaling-rtf">Results and RTF analysis for TTS model size scaling.</a>

![](image/wer_gap.pdf)

<a id="fig:rec-gen-gap">
**Performance gap between reconstruction and generation.**
Each system includes both English and Chinese variants.
Bars represent WER and SIM for reconstruction and generation.
</a>

**Reconstruction and Generation Gap**
In Figure~[fig:rec-gen-gap](#fig:rec-gen-gap), we present the performance gap between reconstruction and generation across multiple systems.
Our proposed system, ***TaDiCodec***, demonstrates a notably small performance gap: -16.5\% for English WER (generation better than reconstruction), -5.8\% for English SIM, +26.5\% for Chinese WER, and -0.0\% for Chinese SIM.
These results indicate that ***TaDiCodec*** is highly generation-friendly—preserving most of the reconstruction quality during generation.
In contrast, existing systems such as Mimi exhibit a much larger degradation (e.g., -104.5\% en WER gap and -265.9\% zh WER gap), suggesting that they are less effective in transferring reconstruction capabilities to generation.
This highlights the advantage of our design in ensuring consistency between reconstructed and generated outputs.

## 5·Conclusion

In this work, we introduce ***TaDiCodec***, a novel speech tokenizer that injects textual information into the decoder and incorporates a prompt mechanism within an end-to-end diffusion autoencoder training framework. ***TaDiCodec*** achieves an extremely low frame rate of 6.25 Hz and a corresponding bitrate of 0.0875 kbps, using a single-layer codebook for 24 kHz speech.
Beyond reconstruction, we apply ***TaDiCodec*** to zero-shot TTS using both AR and MGM, demonstrating its effectiveness, efficiency, and suitability for generation.
These results highlight ***TaDiCodec*** as a viable and innovative solution for speech language modeling.

## A·Implementation Details

### Model Architecture

All our models follow the standard **Transformer**[^Vaswani2017Attention], **LLaMA2**[^Touvron2023Llama] architecture, employ RoPE positional encoding (**RoFormer**[^Su2024Roformer]) and the **SiLU**[^Elfwing2018Sigmoid-Weighted] activation function.
The encoder and decoder of the tokenizer and MGM models use bidirectional attention, while the AR models adopt causal attention.

The ***TaDiCodec***-AR-0.5B and ***TaDiCodec***-AR-3B models are initialized from the textual LLMs **Qwen2.5-0.5B-Instruct** and **Qwen2.5-3B-Instruct**[^Yang2024Qwen2.5], respectively, while ***TaDiCodec***-AR-4B is initialized from **Phi-3.5-mini-instruct**[^Abdin2024Phi-3].

<a id="tab:model-config">Model configurations.</a>

## B·Flow Matching

We provide additional details of the flow matching framework used to train the diffusion decoder in ***TaDiCodec***.
**Flow-Matching**[^Lipman2022Flow] defines a continuous transformation from a prior distribution (e.g., Gaussian noise) to a target data distribution (e.g., mel-spectrograms) by learning a time-dependent velocity field along an interpolated trajectory $\boldsymbol{x}_t$.

While multiple interpolation strategies can be used to construct $\boldsymbol{x}_t$, we adopt the *optimal transport path* formulation (**Flow-Matching**[^Lipman2022Flow], **Rectified Flow**[^Liu2022Flow]), instantiated in this work as simple linear interpolation.
Specifically, given a clean mel-spectrogram $\boldsymbol{x} \in \mathbb{R}^{T \times d}$ and a noise sample $\boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$, we construct an intermediate sample as:
$$
\boldsymbol{x}_t = t \boldsymbol{x} + (1 - t) \boldsymbol{\epsilon}, \quad t \sim \text{Uniform}(0, 1),
$$

where $t$ is sampled uniformly from $[0, 1]$, and $\boldsymbol{x}_t$ denotes the noisy input at time $t$.

The corresponding ground-truth velocity is the temporal derivative of $\boldsymbol{x}_t$:
$$
\boldsymbol{v} = \frac{d \boldsymbol{x}_t}{dt} = \boldsymbol{x} - \boldsymbol{\epsilon}.
$$

The diffusion decoder $\mathcal{D}_\phi$ is trained to predict $\boldsymbol{v}$, conditioned on the token sequence $\boldsymbol{q} = \mathcal{Q}(\mathcal{E}_\theta(\boldsymbol{x}))$ and the associated text $\boldsymbol{x}_{text}$, using the following objective:
$$
\mathcal{L}_{\text{diff}} = \mathbb{E}_{(\boldsymbol{x}, \boldsymbol{x}_{text}), \boldsymbol{\epsilon}, t} \left[ \left\| (\boldsymbol{x} - \boldsymbol{\epsilon}) - \mathcal{D}_\phi(\boldsymbol{q}, \boldsymbol{x}_t, t, \boldsymbol{x}_{text}) \right\| \right].
$$

**Inference**

At inference time, we start with a noise sample $\boldsymbol{x}_0 = \boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$ and solve the ordinary differential equation:
$$
\frac{d\boldsymbol{x}_t}{dt} = \mathcal{D}_\phi(\boldsymbol{q}, \boldsymbol{x}_t, t, \boldsymbol{x}_{text})
$$

from $t = 0$ to $t = 1$ using a simple Euler ODE solver over a discretized set of $N$ time steps.

Flow matching provides a stable and interpretable training signal by directly supervising the instantaneous direction in which a noisy sample $\boldsymbol{x}_t$ should evolve to match the clean target $\boldsymbol{x}$.
In our setting, it enables effective training of the speech tokenizer under low bitrate constraints.

## C·Binary Spherical Quantization

**Binary Spherical Quantization (BSQ)**[^Zhao2024Image] optimizes over an implicit codebook $\mathcal{C}_{\mathrm{BSQ}} = \left\{-\frac{1}{\sqrt{L}}, \frac{1}{\sqrt{L}}\right\}^L$, which corresponds to the $L$-dimensional hypercube projected onto the unit sphere.
Each corner $\boldsymbol{c}_k \in \mathcal{C}_{\mathrm{BSQ}}$ represents a unique discrete token $k \in \{0, \dots, 2^L - 1\}$.
Given an encoder output $\mathcal{E}(\boldsymbol{x})$, we first obtain a low-dimensional latent sequence $\boldsymbol{h} \in \mathbb{R}^{T_q \times L}$ after linear projection.
BSQ then projects each vector $\boldsymbol{h}_t$ in $\boldsymbol{h}$ onto the unit sphere:
$$
\boldsymbol{u}_t = \frac{\boldsymbol{h}_t}{\|\boldsymbol{h}_t\|},
$$

and performs binary quantization independently on each dimension:
$$
\hat{\boldsymbol{u}}_t = \frac{1}{\sqrt{L}} \, \text{sign}(\boldsymbol{u}_t),
$$

where $\text{sign}(x)$ is the element-wise sign function, with $\text{sign}(0)$ defined as $1$ to ensure codewords lie on the unit sphere.

To enable gradient-based training, BSQ uses the Straight-Through Estimator (STE) for backpropagation:
$$
\text{sign}_{\text{STE}}(x) = \text{sg}(\text{sign}(x) - x) + x,
$$

where $\text{sg}(\cdot)$ denotes the stop-gradient operation.

For each vector $\boldsymbol{h}_t$, the corresponding discrete token index is computed as:
$$
k_t = \sum_{i=1}^L 1_{[\boldsymbol{h}_{t,i} > 0]} \cdot 2^{i-1},
$$

where $1_{[\cdot]}$ is the indicator function.

This efficient implicit code assignment scheme allows fast token computation and decoding via bitwise operations.

BSQ offers several appealing properties: it avoids the need for an explicit learnable codebook; its quantization error is bounded, allowing the entire system to be trained without a commitment loss (**VQ-VAE**[^Van2017Neural]).
In this work, we use $L = 14$, resulting in a codebook size of $2^{14} = 16384$.

## D·Masked Generative Models

In this section, we provide a brief introduction to masked generative models (MGMs) (**MaskGIT**[^Chang2022Maskgit], **MAGVITv2**[^Yu2023Language], **MaskGCT**[^Wang2024Maskgct]).
Let $\boldsymbol{x} = [y_1, y_2, \ldots, y_n]$ denote a discrete sequence of length $n$.
At each time step $t$, we define the masked input as $\boldsymbol{x}_t = \boldsymbol{x} \odot \boldsymbol{m}_t$, where $\boldsymbol{m}_t = [m_{t,1}, m_{t,2}, \ldots, m_{t,n}]$ is a binary mask.
Specifically, $x_i$ is replaced with a special \texttt{[MASK]} token if $m_{t,i} = 1$, and remains unchanged if $m_{t,i} = 0$.
Each mask element $m_{t,i}$ is independently sampled from a Bernoulli distribution with parameter $\gamma(t)$, where $\gamma(t) \in (0, 1]$ is a masking schedule function (e.g., $\gamma(t) = \sin\left(\frac{\pi t}{2T}\right)$ for $t \in (0, T]$).
The fully unmasked input is denoted by $\boldsymbol{x}_0 = \boldsymbol{x}$.

MGMs are trained to reconstruct the original sequence from partially observed inputs, conditioned on an optional context $\boldsymbol{c}$ (e.g., in this paper, text $x_{text}$ is condition), by modeling the conditional distribution $p_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_t, \boldsymbol{c})$.
The model parameters $\theta$ are optimized by minimizing the expected marginal cross-entropy over the masked tokens:
$$
\mathcal{L}_{\text{mask}} = - \mathbb{E}_{\boldsymbol{x}, t, \boldsymbol{m}_t} \sum_{i=1}^{n} m_{t,i} \cdot \log p_{\theta}(y_i \mid \boldsymbol{x}_t, \boldsymbol{c}).
$$

At inference time, MGMs generate tokens in parallel via iterative decoding.
The process begins with a fully masked sequence $\boldsymbol{x}_T$.
Assuming a total of $S$ decoding steps, at each step $j \in \{1, \ldots, S\}$, a prediction $\boldsymbol{\hat{x}}_0$ is sampled from $p_{\theta}(\boldsymbol{x}_0 \mid \boldsymbol{x}_{T - (j-1)\cdot \frac{T}{S}}, \boldsymbol{c})$.
Then, $\lfloor n \cdot \gamma(T - j \cdot \frac{T}{S}) \rfloor$ tokens are selected based on confidence scores to be remasked, resulting in a new masked sequence $\boldsymbol{x}_{T - j\cdot \frac{T}{S}}$.

The confidence score for $\hat{y}_i$ in $\boldsymbol{\hat{x}}_0$ is given by $p_{\theta}(y_i \mid \boldsymbol{x}_{T - (j-1)\cdot \frac{T}{S}}, \boldsymbol{c})$ if the position $i$ was masked; otherwise, its score is set to $1$, indicating that unmasked tokens will not be remasked.
The $\lfloor n \cdot \gamma(T - j \cdot \frac{T}{S}) \rfloor$ tokens with the lowest confidence scores are selected for masking.

Note that the method for computing confidence scores is not unique.
For example, **Token-Critic**[^Lezama2022Improved] propose *Token-Critic*, a separate critic model trained to estimate token-wise confidence, thereby guiding the sampling process.
In addition, **Token-Critic**[^Lezama2022Improved], **Show-o**[^Xie2024Show-O] suggest that masked generative modeling can be interpreted as a simplified form of discrete diffusion.

In this work, we develop MGM models for *text-to-token*.
Given the low token rate of 6.25 Hz, the task is relatively easy to model, and 10 to 25 inference steps are sufficient to achieve good results.

## E·Baselines

### Speech Tokenizer

**EnCodec**[^D{\'e}fossez2022High]
A Residual Vector Quantization (RVQ)-based neural audio codec operating at a frame rate of 75 Hz.
We use two codebooks for inference, achieving a bitrate of 1.5 kbps.
We use the [Official Checkpoint](https://huggingface.co/facebook/encodec_24khz).

**DAC**[^Kumar2024High-Fidelity]
An improved VQGAN-based (**Token-Critic**[^Lezama2022Improved]) codec that projects latent features onto a low-dimensional space (e.g., 8 dimensions) prior to quantization.
We reproduce two variants: one utilizing three codebooks at a 25 Hz frame rate, and the other a single codebook at a 75 Hz frame rate.
Both configurations operate at a token rate of 75 Hz and achieve a bitrate of 0.75 kbps.

**SpeechTokenizer**[^Zhang2023Speechtokenizer]
Enhances first-layer speech tokens via semantic distillation using features from **HuBERT**[^Hsu2021Hubert].
This tokenizer operates at 50 Hz and we use two codebooks for inference.
We use the [Official Checkpoint](https://github.com/ZhangXInFD/SpeechTokenizer).

**Mimi**[^D{\'e}fossez2024Moshi]
Follows the design of SpeechTokenizer but utilizes **WavLM**[^Chen2022Wavlm] for semantic distillation.
The tokenizer employs eight codebooks, each of size 2,048, at a 12.5 Hz frame rate, resulting in a bitrate of 1.1 kbps.
We use the [Official Checkpoint](https://huggingface.co/kyutai/mimi).

**DualCodec**[^Li2025DualCodec]
A state-of-the-art, low-frame-rate, semantically-enhanced neural audio codec designed for speech generation.
DualCodec directly encodes SSL representations (**W2V-BERT**[^Chung2021W2v-Bert]) into first-layer codec tokens.
It adopt a configuration with a 12.5 Hz token rate and a 8-layer codebook hierarchy.
The first codebook contains 16,384 entries, while the remaining five each contain 4,096 entries, yielding a bitrate of 1.225 kbps.
We use the [Official Checkpoint](https://pypi.org/project/dualcodec/0.1.2/).

**BiCodec (Spark-TTS)**[^Wang2025Spark-TTS]
A semantically-enhanced tokenizer with a single-layer codebook.
It discretizes audio into semantic tokens based on features from **Wav2Vec2.0**[^Baevski2020Wav2vec].
It operates at a token rate of 50 Hz with a codebook size of 8,192, achieving a bitrate of 0.65 kbps.
We use the [Official Checkpoint](https://github.com/SparkAudio/Spark-TTS).

**X-codec 2**[^Ye2025Llasa]
Employs a dual-encoder design: a semantic encoder based on Wav2Vec2-BERT[^Barrault2023Seamless] and an acoustic encoder for low-level acoustic features.
Their outputs are concatenated prior to quantization.
It operates at a token rate of 50 Hz with a codebook size of 65,536, yielding a bitrate of 0.8 kbps.
We use the [Official Checkpoint](https://huggingface.co/HKUSTAudio/xcodec2).

**WavTokenizer**[^Ji2024Wavtokenizer]
A single-codebook tokenizer trained on 800K hours of mixed-domain audio.
It operates at a 75 Hz token rate with a codebook size of 4,096, resulting in a bitrate of 0.9 kbps.
We use the [Official Checkpoint](https://huggingface.co/novateur/WavTokenizer-large-speech-75token).

**BigCodec**[^Xin2024Bigcodec]
A single-codebook tokenizer with scaled model size.
It integrates sequential modules into convolutional architectures and applies low-dimensional quantization to enhance code utilization.
It operates at an 80 Hz token rate with a codebook size of 8,192, yielding a bitrate of 1.04 kbps.
We use the [Official Checkpoint](https://huggingface.co/Alethia/BigCodec).

**TAAE**[^Parker2024Scaling]
A transformer-based tokenizer using Finite Scalar Quantization (FSQ)[^Mentzer2023Finite] for speech tokenization.
It operates at a 25 Hz token rate with a codebook size of 46,656, resulting in a bitrate of 0.4 kbps.
We use the [Official Implementation](https://github.com/Stability-AI/stable-codec).

**SemantiCodec**[^Liu2024Semanticodec]
Combines a semantic encoder (AudioMAE[^Huang2022Masked] with k-means clustering) and an acoustic encoder, featuring a diffusion decoder for reconstruction.
It operates at a 50 Hz token rate, with codebook sizes of 16,384 (semantic) and 2,048 (acoustic), achieving a bitrate of 0.675 kbps.
We use the [Official Implementation](https://github.com/haoheliu/SemantiCodec-inference).

**Vevo Tokenizer**[^Zhang2025Vevo]
A two-stage tokenizer utilizing features from **HuBERT**[^Hsu2021Hubert], followed by VQ and a diffusion decoder.
It employs a single codebook of size 8,192 at a 50 Hz token rate, resulting in a bitrate of 0.65 kbps.
We use the [Official Checkpoint](https://github.com/open-mmlab/Amphion/tree/main/models/vc/vevo).

**FireRedTTS Tokenizer**[^Guo2024FireRedTTS]
A single-codebook tokenizer trained in two stages.
Transforms speech into semantic embeddings via features from **HuBERT**[^Hsu2021Hubert], followed by a ResNet-based encoder and quantization.
It uses a 40 ms frame shift and a codebook size of 16,384.
A global embedding is also incorporated, and decoding is performed using flow matching.
Its implementation is available[URL](https://github.com/FireRedTeam/FireRedTTS).

**CosyVoice Tokenizer**[^Du2024CosyVoice]
A single-codebook tokenizer trained in two stages.
The encoder is initialized from an ASR model (**Whisper**[^Radford2023Robust]) and subsequently trained with a supervised loss.
A flow matching model is used to predict mel-spectrograms.
It operates at a 25 Hz token rate and 0.3 kbps bitrate.
Its code is available[URL](https://github.com/FunAudioLLM/CosyVoice).

**CosyVoice 2 Tokenizer**[^Du2024Cosyvoice]
An improved version of CosyVoice that replaces VQ with FSQ.
It operates at a 25 Hz token rate and 0.325 kbps bitrate.
Its official implementation is available[URL](https://github.com/FunAudioLLM/CosyVoice).

**Ints Tokenizer**[^Zhang2025Advancing]
Combines the **DualCodec**[^Li2025DualCodec] semantic encoder with a flow matching decoder, similar to the CosyVoice variants.
It uses a single codebook with 16,384 entries at a 12.5 Hz token rate, resulting in a bitrate of 0.175 kbps.
The resulting TTS model, **Ints**, demonstrates state-of-the-art intelligibility [^Zhang2025Advancing].

### Zero-shot TTS

**F5-TTS**[^Chen2024F5-TTS]
An open-source flow matching-based TTS systems.
It follows **E2-TTS**[^Eskimez2024E2] and uses a flow matching transformer (**Flow-Matching**[^Lipman2022Flow], **VoiceBox**[^Le2024Voicebox]) to convert the text to acoustic features directly (**F5-TTS**[^Chen2024F5-TTS]).

**MaskGCT**[^Wang2024Maskgct]
An open-source large-scale MGM-based TTS system that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction.
We use the [official code and checkpoint [URL]](https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct) which is trained on **Emilia**[^He2024Emilia].

**ARS**[^Wang2024Maskgct]
Introduced as an AR baseline by **MaskGCT**[^Wang2024Maskgct]. and referred to as "AR + SoundStorm" in the original paper **MaskGCT**[^Wang2024Maskgct].
It adopts a cascaded architecture, including the AR *text-to-token* and the NAR MGM *codec-to-waveform* (**SoundStorm**[^Borsos2023Soundstorm]).

**CosyVoice 2**[^Du2024Cosyvoice]
An open-source, large-scale zero-shot TTS system built upon an AR model initialized from \texttt{Qwen2.5-0.5B-Instruct}, which predicts speech codes extracted by the CosyVoice 2 tokenizer.

**FireRedTTS**[^Guo2024FireRedTTS]
An open-source, large-scale AR-based zero-shot TTS system, which predicts speech codes extracted by the FireRedTTS tokenizer.

**Ints**[^Zhang2025Advancing]
An open-source, large-scale zero-shot TTS system built upon an AR model initialized from \texttt{Phi-3.5-mini-instruct}, which predicts 12.5 Hz speech codes extracted by the Ints tokenizer.

**Spark-TTS**[^Wang2025Spark-TTS]
An open-source, large-scale zero-shot TTS system built upon an AR model initialized from \texttt{Qwen2.5-0.5B-Instruct}, which predicts speech codes extracted by the **BiCodec (Spark-TTS)**[^Wang2025Spark-TTS].

**LLaSA**[^Ye2025Llasa]
An open-source, large-scale zero-shot TTS system built upon an AR model initialized from \texttt{Llama3.2-1B}[^Grattafiori2024Llama], which predicts speech codes extracted by the **X-codec 2 (LLaSA)**[^Ye2025Llasa].

## F·Evaluation

### Test Sets

***SeedTTS test-en***
We adopt a test set introduced in **Seed-TTS**[^Anastassiou2024Seed-TTS], consisting of 1,000 samples drawn from English public corpora, including the Common Voice dataset[^Ardila2019Common].
We refer to this set as "*Regular en*" and use it for zero-shot TTS evaluation (Table~[tab:zs-tts-res](#tab:zs-tts-res) and Table~[tab:tts-scaling-rtf](#tab:tts-scaling-rtf)).
Additionally, it is used for evaluating the performance of our tokenizer.

***SeedTTS test-zh***
We adopt a test set introduced in Seed-TTS, comprising 2,000 samples drawn from Chinese public corpora, including the DiDiSpeech dataset[^Guo2021Didispeech].
We denote it as "*Regular zh*" for zero-shot TTS evaluation.

***Articulatory en, Articulatory zh***
These sets are introduced in **Ints**[^Zhang2025Advancing] and contain tongue twisters and repeated texts.
For Chinese, the *SeedTTS test-hard* set is used directly.
For English, reference speech prompts are taken from *SeedTTS test-en*, while the corresponding articulatory texts are constructed using Deepseek-V3[^Liu2024Deepseek-V3] to match the style of *SeedTTS test-hard*.
Each set contains 400 samples.

An example:
```
***Prompt text:***
Salmon is one of the most popular fish and very delicious, though usually not sustainable.
***Target text:***
A big black bug bit a big black bear, but the big black bear bled black blood from the bite.
```

***Code-switching en, Code-switching zh***
These sets are introduced in **Ints**[^Zhang2025Advancing], consist of target texts that mix English and Chinese.
Based on *SeedTTS test-en* and *test-zh*, the reference speech prompts are kept unchanged, while Deepseek-V3 is employed to convert the texts into a code-switching format.
Each set contains 500 samples.

An example:
```
***Prompt text:***
创下奥运史上拒绝奥运圣火入境的首例。
***Target text:***
在他~execution~之后~Ogilvie~的~followers~被~rounded up~并~put in jail.
```

***Cross-lingual zh2en, Cross-lingual en2zh***
These sets are introduced in **Ints**[^Zhang2025Advancing], two types of cross-lingual samples are constructed: *zh2en* and *en2zh*, each comprising 500 samples.
The *zh2en* set pairs Chinese reference speech from *SeedTTS test-zh* with English target text from *SeedTTS test-en*, while the *en2zh* set follows the reverse configuration.
Each set contains 500 samples.

An example:
```
***Prompt text:***
调整海外购买住宅征收额外印花税率，从百分之三调整到百分之七而言。
***Target text:***
The recluse from Lithuania and his compatriot were making up stories about mermaids and fays.
```

***Multilingual test sets***
We additionally construct four multilingual test sets to evaluate tokenizer reconstruction in non-English languages, including French (fr), German (de), Japanese (ja), and Korean (ko).
For each language, we randomly sample 300 utterances from Common Voice[^Ardila2019Common].

### Objective Evaluation

**Frame Rate, Token Rate, Bitrate**
Frame rate means the speech is compressed into how many frames per second (measured in Hz), while each frame may contain multiple tokens; token rate refers to how many discrete tokens are produced per second; bitrate indicates the total amount of information retained, computed as token rate multiplied by the number of bits per token (measured in kbps), and reflects the overall compression level of the tokenizer.

For example, suppose a speech tokenizer operates at a frame rate of 25 Hz, meaning the input audio is compressed into 25 frames per second.
If each frame contains 2 codebook tokens (*i.e.*, 2 layers of quantization), and each codebook has a size of 2048 (requiring 11 bits per token since $2^{11} = 2048$), then:

-  **Token Rate** = 25 frames/sec × 2 tokens/frame = 50 tokens/sec
-  **Bitrate** = 50 tokens/sec × 11 bits/token = 550 bps = 0.55 kbps

This means the speech is represented with a bitrate of 0.55 kbps, indicating a high compression level while retaining discrete structure for downstream modeling.

**WER**
Word Error Rate (WER) is employed to assess the intelligibility of reconstructed or generated speech.
We adopt two automatic speech recognition (ASR) models for WER computation: [**Whisper-Large-V3**](https://huggingface.co/openai/whisper-large-v3)[^Radford2023Robust] and [Paraformer-ZH (FunASR)](https://huggingface.co/funasr/paraformer-zh)[^Gao2023Funasr].
The former is used for non-Chinese utterances, while the latter is applied to Chinese speech, following established practices in recent studies (**Metis**[^Wang2025Metis], **MaskGCT**[^Wang2024Maskgct], **Seed-TTS**[^Anastassiou2024Seed-TTS], **CosyVoice**[^Du2024CosyVoice]).

**SIM**
Speaker similarity (SIM) is computed as the cosine similarity between speaker embeddings extracted from the prompt and the generated utterance.
We use the [WavLM-TDNN model](https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification)(**WavLM**[^Chen2022Wavlm]) for speaker embedding extraction, following **VALL-E**[^Wang2023Neural], **NaturalSpeech3**[^Ju2024NaturalSpeech], **VoiceBox**[^Le2024Voicebox], **MaskGCT**[^Wang2024Maskgct], **Seed-TTS**[^Anastassiou2024Seed-TTS].

**UTMOS**
Speech naturalness and perceptual quality are evaluated using UTMOS[^Saeki2022Utmos], a Mean Opinion Score (MOS) prediction system.
UTMOS combines ensemble learning of strong and weak learners: the strong learners are fine-tuned self-supervised learning (SSL) models with architectural enhancements, while the weak learners apply lightweight regression on SSL features.
We use the [official UTMOS checkpoint](https://huggingface.co/spaces/sarulab-speech/UTMOS-demo).

### Subject Evaluation

We conduct a subjective evaluation of speech tokenizers in terms of audio quality using the Comparative Mean Opinion Score (CMOS):
-  **System Interface**: Users listen to two speech samples, A and B, to compare their speech quality.
-  **Instruction**:  Participants are asked, "Which speech has better audio quality?".
-  **Evaluation Criteria**: Five response options: A +2 (Sample A has much better audio quality), A +1 (Sample A has slightly better audio quality), Tie (Both have equal audio quality), B +1 (Sample B has slightly better audio quality), and B +2 (Sample B has much better audio quality).

Figure~[fig:sub_eval](#fig:sub_eval) shows a shotscreen of the evaluation system.

![](image/sub_eval.png)

<a id="fig:sub_eval">Shotscreen of the subjective evaluation system.</a>

We randomly select 40 samples from an in-the-wild dataset.
Each of the six systems: Ours, X-codec 2, DualCodec, WavTokenizer, Mimi, and Ground Truth, generates all 40 samples.
For evaluation, each baseline system is compared against ours, resulting in a total of $40 \times 5 = 200$ sample pairs.
Each pair is evaluated by two human listeners.

## G·Limitations and Future Work

***TaDiCodec*** achieves an extremely low frame rate of 6.25 Hz and a corresponding bitrate of 0.0875 kbps using a single-layer codebook for 24 kHz speech compression, while demonstrating strong performance in both reconstruction and text-to-speech tasks in terms of intelligibility, speaker similarity, and speech quality.
There remains room for improvement and several promising directions for future work:
1) ***TaDiCodec*** employs a diffusion autoencoder for tokenization and de-tokenization, which involves multiple steps during inference.
Compared to GAN-based tokenizers, this results in higher decoding latency.
Future work may explore distillation or more powerful generative models to enable single-step inference while maintaining performance.
2) While ***TaDiCodec*** has shown its effectiveness for speech language modeling through zero-shot TTS, it is worth further evaluating its applicability in speech understanding and dialogue systems.
3) ***TaDiCodec*** currently requires text input for the decoder.
It would be valuable to explore unified models that can transcribe, tokenize, and reconstruct speech simultaneously, enabling one model for joint understanding, compression, and reconstruction.

## H·Broader Impacts

Our model enables high-quality speech modeling, which can benefit applications such as personalized speech interfaces, speech restoration, and accessibility tools.
However, it also poses risks of misuse, including voice spoofing and unauthorized impersonation.
These risks are particularly concerning in scenarios involving biometric authentication or deceptive media.
To prevent misuse,  we advocate for the development of reliable deepfake detection tools, watermarking methods for synthetic speech, and reporting mechanisms to flag suspected abuse.

## References

[^Wang2023Neural]: Neural Codec Language Models Are Zero-Shot Text to Speech Synthesizers. arXiv:2301.02111.
[^Anastassiou2024Seed-TTS]: Seed-Tts: A Family of High-Quality Versatile Speech Generation Models. arXiv:2406.02430.
[^Du2024CosyVoice]: CosyVoice: A Scalable Multilingual Zero-Shot Text-to-Speech Synthesizer Based on Supervised Semantic Tokens. arXiv:2407.05407.
[^Guo2024FireRedTTS]: FireRedTTS: A Foundation Text-to-Speech Framework for Industry-Level Generative Speech Applications. arXiv:2409.03283.
[^Ye2025Llasa]: Llasa: Scaling Train-Time and Inference-Time Compute for Llama-Based Speech Synthesis. arXiv:2502.04128.
[^Wang2025Spark-TTS]: Spark-Tts: An Efficient LLM-Based Text-to-Speech Model With Single-Stream Decoupled Speech Tokens. arXiv:2503.01710.
[^Wang2024Maskgct]: Maskgct: Zero-Shot Text-to-Speech With Masked Generative Codec Transformer. arXiv:2409.00750.
[^Kharitonov2023Speak,]: Speak, Read and Prompt: High-Fidelity Text-to-Speech With Minimal Supervision. Transactions of the Association for Computational Linguistics 2023.
[^Peng2024VoiceCraft]: VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild. arXiv:2403.16973.
[^D{\'e}fossez2024Moshi]: Moshi: A Speech-Text Foundation Model for Real-Time Dialogue. arXiv:2410.00037.
[^Zeng2024Scaling]: Scaling Speech-Text Pre-Training With Synthetic Interleaved Data.
[^Zeng2024GLM-4-Voice]: GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken Chatbot. arXiv:2412.02612.
[^Li2025Baichuan-Audio]: Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction. arXiv:2502.17239.
[^Huang2025Step-Audio]: Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction. arXiv:2502.11946.
[^Fang2024Llama-Omni]: Llama-Omni: Seamless Speech Interaction With Large Language Models. arXiv:2409.06666.
[^Wang2024Freeze-Omni]: Freeze-Omni: A Smart and Low Latency Speech-to-Speech Dialogue Model With Frozen LLM. arXiv:2411.00774.
[^Ding2025Kimi-Audio]: Kimi-Audio Technical Report. arXiv:2504.18425.
[^Xu2025Qwen2.5-Omni]: Qwen2. 5-Omni Technical Report. arXiv:2503.20215.
[^D{\'e}fossez2022High]: High Fidelity Neural Audio Compression. arXiv:2210.13438.
[^Zeghidour2021Soundstream]: Soundstream: An End-to-End Neural Audio Codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing 2021.
[^Kumar2024High-Fidelity]: High-Fidelity Audio Compression With Improved Rvqgan. Advances in Neural Information Processing Systems 2024.
[^Xin2024Bigcodec]: Bigcodec: Pushing the Limits of Low-Bitrate Neural Speech Codec. arXiv:2409.05377.
[^Ji2024Wavtokenizer]: Wavtokenizer: An Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling. arXiv:2408.16532.
[^Parker2024Scaling]: Scaling Transformers for Low-Bitrate High-Quality Speech Coding. arXiv:2411.19842.
[^Ye2025Codec]: Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model. Proceedings of the AAAI Conference on Artificial Intelligence 2025.
[^Li2025DualCodec]: DualCodec: A Low-Frame-Rate, Semantically-Enhanced Neural Audio Codec for Speech Generation. Proceedings of Interspeech 2025 2025.
[^Zhang2023Speechtokenizer]: Speechtokenizer: Unified Speech Tokenizer for Speech Large Language Models. arXiv:2308.16692.
[^Chen2022Wavlm]: Wavlm: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing. IEEE Journal of Selected Topics in Signal Processing 2022.
[^Baevski2020Wav2vec]: Wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations. Advances in Neural Information Processing Systems 2020.
[^Hsu2021Hubert]: Hubert: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units. IEEE/ACM Transactions on Audio, Speech, and Language Processing 2021.
[^Chiu2022Self-Supervised]: Self-Supervised Learning With Random-Projection Quantizer for Speech Recognition. International Conference on Machine Learning 2022.
[^Zhang2025Vevo]: Vevo: Controllable Zero-Shot Voice Imitation With Self-Supervised Disentanglement. arXiv:2502.07243.
[^Ho2020Denoising]: Denoising Diffusion Probabilistic Models. Advances in Neural Information Processing Systems 2020.
[^Song2020Score-Based]: Score-Based Generative Modeling Through Stochastic Differential Equations. arXiv:2011.13456.
[^Lipman2022Flow]: Flow Matching for Generative Modeling. arXiv:2210.02747.
[^Radford2023Robust]: Robust Speech Recognition via Large-Scale Weak Supervision. International Conference on Machine Learning 2023.
[^Gao2023Funasr]: Funasr: A Fundamental End-to-End Speech Recognition Toolkit. arXiv:2305.11013.
[^Gao2025Lucy]: LUCY: Linguistic Understanding and Control Yielding Early Stage of Her. arXiv:2501.16327.
[^Lee2022Autoregressive]: Autoregressive Image Generation Using Residual Quantization. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2022.
[^Liu2024Semanticodec]: Semanticodec: An Ultra Low Bitrate Semantic Audio Codec for General Sound. IEEE Journal of Selected Topics in Signal Processing 2024.
[^Du2024Cosyvoice]: Cosyvoice 2: Scalable Streaming Speech Synthesis With Large Language Models. arXiv:2412.10117.
[^Chung2021W2v-Bert]: W2v-Bert: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training. 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) 2021.
[^Welker2025FlowDec]: FlowDec: A Flow-Based Full-Band General Audio Codec With High Perceptual Quality. arXiv:2503.01485.
[^Yang2024Generative]: Generative De-Quantization for Neural Speech Codec via Latent Diffusion. ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2024.
[^Ju2024NaturalSpeech]: NaturalSpeech 3: Zero-Shot Speech Synthesis With Factorized Codec and Diffusion Models. arXiv:2403.03100.
[^Borsos2023Soundstorm]: Soundstorm: Efficient Parallel Audio Generation. arXiv:2305.09636.
[^Yang2025Pseudo-Autoregressive]: Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis. arXiv:2504.10352.
[^Lee2022Bigvgan]: Bigvgan: A Universal Neural Vocoder With Large-Scale Training. arXiv:2206.04658.
[^Siuzdak2023Vocos]: Vocos: Closing the Gap Between Time-Domain and Fourier-Based Neural Vocoders for High-Quality Audio Synthesis. arXiv:2306.00814.
[^Goodfellow2020Generative]: Generative Adversarial Networks. Communications of the ACM 2020.
[^Vaswani2017Attention]: Attention Is All You Need. Advances in Neural Information Processing Systems 2017.
[^Liu2022Flow]: Flow Straight and Fast: Learning to Generate and Transfer Data With Rectified Flow. arXiv:2209.03003.
[^Zhao2024Image]: Image and Video Tokenization With Binary Spherical Quantization. arXiv:2406.07548.
[^Van2017Neural]: Neural Discrete Representation Learning. Advances in Neural Information Processing Systems 2017.
[^Le2024Voicebox]: Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale. Advances in Neural Information Processing Systems 2024.
[^Chen2024F5-TTS]: F5-Tts: A Fairytaler That Fakes Fluent and Faithful Speech With Flow Matching. arXiv:2410.06885.
[^Eskimez2024E2]: E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS. arXiv:2406.18009.
[^Wang2025Metis]: Metis: A Foundation Speech Generation Model With Masked Generative Pre-Training. arXiv:2502.03128.
[^Chang2022Maskgit]: Maskgit: Masked Generative Image Transformer. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2022.
[^He2024Emilia]: Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for Large-Scale Speech Generation. arXiv:2407.05361.
[^Touvron2023Llama]: Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288.
[^Su2024Roformer]: Roformer: Enhanced Transformer With Rotary Position Embedding. Neurocomputing 2024.
[^Zhang2019Root]: Root Mean Square Layer Normalization. Advances in Neural Information Processing Systems 2019.
[^Abdin2024Phi-3]: Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone. arXiv:2404.14219.
[^Yang2024Qwen2.5]: Qwen2. 5 Technical Report. arXiv:2412.15115.
[^Loshchilov2017Decoupled]: Decoupled Weight Decay Regularization. arXiv:1711.05101.
[^Zhang2025AnyEnhance]: AnyEnhance: A Unified Generative Model With Prompt-Guidance and Self-Critic for Voice Enhancement. arXiv:2501.15417.
[^Zhang2025Advancing]: Advancing Zero-Shot Text-to-Speech Intelligibility Across Diverse Domains via Preference Alignment. arXiv:2505.04113.
[^Yu2021Vector-Quantized]: Vector-Quantized Image Modeling With Improved Vqgan. arXiv:2110.04627.
[^Song2023Consistency]: Consistency Models.
[^Zhou2024Score]: Score Identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation. Forty-First International Conference on Machine Learning 2024.
[^Frans2024One]: One Step Diffusion via Shortcut Models. arXiv:2410.12557.
[^Demir2018Patch-Based]: Patch-Based Image Inpainting With Generative Adversarial Networks. arXiv:1803.07422.
[^Rafailov2023Direct]: Direct Preference Optimization: Your Language Model Is Secretly a Reward Model. Advances in Neural Information Processing Systems 2023.
[^Ouyang2022Training]: Training Language Models to Follow Instructions With Human Feedback. Advances in Neural Information Processing Systems 2022.
[^Kwon2023Efficient]: Efficient Memory Management for Large Language Model Serving With Pagedattention. Proceedings of the 29th Symposium on Operating Systems Principles 2023.
[^Elfwing2018Sigmoid-Weighted]: Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning. Neural Networks 2018.
[^Yu2023Language]: Language Model Beats Diffusion--Tokenizer Is Key to Visual Generation. arXiv:2310.05737.
[^Lezama2022Improved]: Improved Masked Image Generation With Token-Critic. European Conference on Computer Vision 2022.
[^Xie2024Show-O]: Show-O: One Single Transformer to Unify Multimodal Understanding and Generation. arXiv:2408.12528.
[^Barrault2023Seamless]: Seamless: Multilingual Expressive and Streaming Speech Translation. arXiv:2312.05187.
[^Mentzer2023Finite]: Finite Scalar Quantization: Vq-Vae Made Simple. arXiv:2309.15505.
[^Huang2022Masked]: Masked Autoencoders That Listen. Advances in Neural Information Processing Systems 2022.
[^Grattafiori2024Llama]: The Llama 3 Herd of Models. arXiv:2407.21783.
[^Ardila2019Common]: Common Voice: A Massively-Multilingual Speech Corpus. arXiv:1912.06670.
[^Guo2021Didispeech]: Didispeech: A Large Scale Mandarin Speech Corpus. ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2021.
[^Liu2024Deepseek-V3]: Deepseek-V3 Technical Report. arXiv:2412.19437.
[^Saeki2022Utmos]: Utmos: Utokyo-Sarulab System for Voicemos Challenge 2022. arXiv:2204.02152.