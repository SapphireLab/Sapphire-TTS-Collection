# FlexiCodec: A Dynamic Neural Audio Codec for Low Frame Rates

<details>
<summary>基本信息</summary>

- 标题: "FlexiCodec: A Dynamic Neural Audio Codec for Low Frame Rates."
- 作者:
  - 01 Jiaqi Li
  - 02 Yao Qian
  - 03 Yuxuan Hu
  - 04 Leying Zhang
  - 05 Xiaofei Wang
  - 06 Heng Lu
  - 07 Manthan Thakker
  - 08 Jinyu Li
  - 09 Shang Zhao
  - 10 Zhizheng Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2510.00981v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2510.00981v1](PDF/2025.10.01_2510.00981v1_FlexiCodec__A_Dynamic_Neural_Audio_Codec_for_Low_Frame_Rates.pdf)
  - [Publication] #TODO

</details>

## Abstract

Neural audio codecs are foundational to speech language models.
It is expected to have a low frame rate and decoupled semantic and acoustic information.
A lower frame rate codec can reduce the computational cost of speech language models by shortening the sequence length.
Recent studies have developed 12.5Hz low-frame-rate audio codecs, but even lower frame rate codecs remain underexplored.
We find that a major challenge for very low frame rate tokens is missing semantic information.
This paper introduces **FlexiCodec** to address this limitation.
FlexiCodec improves semantic preservation with a **dynamic frame rate** approach and introduces a novel architecture featuring an **ASR feature-assisted dual stream** encoding and Transformer bottlenecks.
With dynamic frame rates, it uses less frames at information-sparse regions through adaptively merging semantically similar frames.
A dynamic frame rate also allows FlexiCodec to support inference-time **controllable frame rates** between 3Hz and 12.5Hz.
Experiments on **6.25Hz, 8.3Hz and 12.5Hz** average frame rates confirm that FlexiCodec excels over baseline systems in semantic information preservation and delivers a high audio reconstruction quality.
We also validate the effectiveness of FlexiCodec in language model-based TTS.
Demos are available at: \url{https://flexicodec.github.io}.

## 1·Introduction

The neural audio codec technique, originally designed for waveform compression[^Zeghidour2021Soundstream], [^D{\'e}fossez2022High], is now widely used in various tasks including Language Model (LM)-based text-to-speech (TTS)[^Wang2023Neural], [^Li2024Investigating], [^Wang2024Maskgct], [^Du2024CosyVoice], [^Wang2025Spark-TTS], [^Guo2024FireRedTTS] and multimodal LLMs[^D{\'e}fossez2024Moshi], [^Zeng2024GLM-4-Voice], [^Ding2025Kimi-Audio].

By compressing raw speech into compact discrete tokens, neural audio codecs enable the application of auto-regressive LLM paradigms to the speech domain.

The standard neural audio codec follows an encoder-quantizer-decoder architecture.

The encoder downsamples the audio waveform into a sequence of fixed-rate continuous latent vectors.

These vectors are then quantized into discrete indices using Residual Vector Quantization (RVQ), a multi-stage process where each quantizer encodes the residual error of the previous one.

In downstream tasks like TTS, the first RVQ layer’s tokens (RVQ-1) are often used to drive an autoregressive (AR) language model or an LLM, while the remaining layers (RVQ-rest) can be predicted by a non-autoregressive (NAR) model to add acoustic detail.

However, a fundamental challenge arises from the high temporal resolution of these codecs.

State-of-the-art models like EnCodec[^D{\'e}fossez2022High], DAC[^Kumar2024High-Fidelity], and SpeechTokenizer[^Zhang2023Speechtokenizer] operate at frame rates exceeding 50Hz, representing one second of audio with over 50 tokens.

This high density, compared to the $\sim$4.5Hz rate of typical text representations[^Wang2024Why], creates two major problems for AR models: (1) a significant computational burden due to the quadratic complexity of attention, and (2) a severe frame rate mismatch between text and audio modalities that may degrade LLM performance[^Wang2024Why].

<a id="tab:model_comparison_intro">Comparison of different audio tokenization methods and their properties.</a>

To mitigate this, recent work has focused on low-frame-rate codecs.

Methods like Mimi[^D{\'e}fossez2024Moshi] and DualCodec[^Li2025DualCodec] successfully reduced the frame rate to 12.5Hz by decoupling speech into two streams: a semantic stream (RVQ-1) derived from self-supervised learning (SSL) models[^Chen2022Wavlm], [^Barrault2023Seamless], and an acoustic stream (RVQ-rest) for residual details.

In this strategy, the low-rate RVQ-1 tokens encode the core semantic information, which is sufficient for many downstream AR models.

While previous works have proposed 12.5Hz solutions, a significant gap remains compared to the $\sim$4.5Hz frame rate of text.

Furthermore, research into neural audio codecs operating below 12.5Hz is limited.

Our initial experiments revealed that pushing to even lower frame rates leads to significant information loss, particularly the omission of semantic/phonetic content.

Driven by this observation, we have the following motivations for designing our **low frame rate** codec:
(1) **Dynamic Frame Rate**: A fixed low rate inevitably discards transient phonetic details, whereas a dynamic rate may adapt to the phonetic complexity to encode more details.
(2) **Richer Semantics**:
SSL features, trained for reconstruction, can be redundant.

Features from an ASR model, trained for text prediction, may offer a more concentrated source of semantic information.
(3) **Controllable Frame Rate**: Existing codecs typically operate at one or more fixed frame rates.

A continuously controllable rate would allow users to dynamically trade off performance and efficiency for downstream tasks.

In this work, we introduce FlexiCodec, a novel low-frame-rate codec built on three principles: dynamic frame rate, ASR-guided semantics, and frame rate controllability.

Instead of a fixed frame rate, FlexiCodec dynamically allocates temporal resolution, using more frames for complex phonetic segments and fewer for sparse regions like long vowels, syllables and silence.

This is achieved through a novel ASR-feature-assisted dual-stream architecture that adaptively merges semantically similar frames.

A key benefit of this dynamic approach is that a single model can support a continuous range of frame rates **(3-12.5Hz)** at inference, enabling flexible trade-offs for applications like adaptive signal transmission or variable-complexity TTS on edge devices.

Our contributions are:

-  We propose FlexiCodec\footnote{Reproduced models will be available at \url{https://github.com/amphionspace/flexicodec}}.

To our best knowledge, it is the **one of first neural audio codecs under 10Hz frame rate**, and the **first work to explore dynamic frame rate on low-frame-rate neural audio codecs**.

We develop a novel ASR feature-assisted dual stream codec architecture with Transformer bottlenecks.

-  We show that FlexiCodec outperforms open-source baselines in semantic intelligibility and acoustic quality.

Experiments confirm that our dynamic frame rate strategy improves semantic information preservation and allows for controllable frame rate as low as 3Hz.

Other design choices including utilizing an ASR encoder, transformer bottlenecks, and FSQ quantization, also contributes to our codec's performance.

-  We demonstrate FlexiCodec’s utility in a flexible TTS system.

The model yields competitive results at multiple frame rates and is substantially faster than existing methods.

## 2·Related Works

### Low-Frame-Rate Neural Audio Codecs

Neural audio codecs convert continuous speech into discrete tokens.

SoundStream[^Zeghidour2021Soundstream], Encodec[^D{\'e}fossez2022High], and DAC[^Kumar2024High-Fidelity] focused on audio compression, relying on residual vector quantization (RVQ)[^Zeghidour2021Soundstream] and operating at high bitrates ($\ge$4kbps) and high frame rates ($\ge$50Hz).

WavTokenizer[^Ji2024Wavtokenizer],
TS3-Codec[^Wu2024Ts3-Codec],
SemantiCodec[^Liu2024Semanticodec],
and StableCodec[^Parker2024Scaling] used a single VQ or FSQ[^Mentzer2023Finite] codebook.

They delivered good audio quality at low bitrates (around 1kbps), but operate at a high frame rates ($\ge$40Hz).

Some recent works develop low-frame-rate codecs.

A lower frame rate limits the information amount that can be carried by each RVQ layer tokens.

Thus,
some works[^D{\'e}fossez2024Moshi], [^Li2025DualCodec]
decompose the speech tokens into semantic (RVQ-1) and acoustic (RVQ-rest) tokens.

Mimi codec[^D{\'e}fossez2024Moshi] was based on Encodec with a higher downsampling rate in its convolutional encoder.

It employed semantic distillation[^Zhang2023Speechtokenizer], a technique that distills RVQ-1 embeddings from SSL features.

DualCodec[^Li2025DualCodec] proposed a dual-stream architecture where a semantic stream directly encodes SSL features into RVQ-1 tokens, and an acoustic stream encodes  RVQ-rest tokens.

ALMTokenizer[^Yang2025ALMTokenizer] proposed a query-based compression strategy using a set of learnable query tokens, and designed an auxiliary MAE loss inspired by SSL models.

Concurrent work XY-Tokenizer[^Gong2025XY-Tokenizer] is a 12.5Hz codec encoded with a concatenative dual-stream architecture consisting of Whisper ASR feature and waveform feature.

Recently, TaDiCodec[^Wang2025TaDiCodec] and TASTE[^Tseng2025Taste]  proposed $\le$10Hz speech tokens,
but operating like text-to-speech (TTS) systems,
they require the text transcription to assist audio synthesis.

Accurate text transcription can be unavailable in some audio coding scenarios.

By comparison, our work is more similar to a conventional codec and does not require transcriptions.

### Dynamic-Rate Compression of Images and Audios

The concept of dynamically adjusting token rates based on content complexity is an emerging trend across image and audio modalities.

In the image domain,
[^Bolya2022Token] first proposed Token Merging (ToMe), a technique to gradually merge most similar tokens in Vision Transformers (ViTs) to accelerate inference.

It used the cosine similarity between the self attention keys to guide the merging process.

DynTok[^Zhang2025DynTok] proposed an improved similarity calculation strategy based on CLIP[^Radford2021Learning] semantic representation.

In the audio domain, one research trend has been the syllable-level semantic unit discoveries which are inherently dynamic-rate.

SD-HuBERT[^Cho2024Sd-Hubert] finetuned HuBERT[^Hsu2021Hubert] with sentence-level self-distillation, and showed that its features distinguish syllable boundaries.

SylBoost[^Baade2024Syllablelm] can discover discrete syllabic units using a min-cut algorithm on the feature self-similarity matrix, followed by a k-means clustering.

It experimented on 8.33Hz and 6.25Hz units.

However, compared to neural audio codec tokens, syllable units only encode coarse semantic information, and
necessitate external speech synthesis models to produce audio.

Dynamic-rate neural audio codec is an emerging research area.
[^Dieleman2021Variable-Rate] proposed an audio VQ-VAE with run-length encoding, operating at an average frame rate of 75Hz.

SNAC[^Siuzdak2024Snac] created a multi-resolution codec stream at 12, 23, and 47Hz for each RVQ layer.

CodecSlime[^Wang2025CodecSlime] proposed a two-stage process to firstly train an 80Hz fixed-frame-rate codec, followed by merging similar features into 40Hz average-frame-rate tokens.

Similarly, TFC[^Zhang2025Unlocking] and VARSTok[^Zheng2025Say] introduced dynamic-frame-rate algorithms on 75Hz RVQ or single-codebook tokens.

Compared with previous works, ours explores more challenging low frame rates ($\leq$10Hz).

Compared with CodecSlime, TFC and VARSTok, our work uses a pre-trained audio semantic feature extractor to guide the merging process, simplifying the needs for multi-stage training or dynamic programming-based merging algorithms.

### Text-to-Speech Synthesis

Modern Text-to-Speech (TTS) has increasingly shifted from statistical parametric methods to systems based on neural audio codecs.

VALL-E[^Wang2023Neural] established an "AR+NAR'' framework: an AR language model generates codec RVQ-1 tokens
conditioned on text; an NAR
language model predicts remaining RVQ layer tokens in parallel.

The AR stage provides essential information for the NAR stage, and the AR stage is usually the most time-consuming part.

Subsequent works have evolved this paradigm, showing the benefits of semantic-rich RVQ-1 tokens in the AR stage[^Borsos2023Audiolm], [^Du2024CosyVoice], [^Zhang2025Vevo], and replacements of the NAR discrete token prediction with diffusion-based models[^Betker2023Better], [^Du2024CosyVoice].

The efficiency of a TTS system directly correlates to codec frame rate.

A lower frame rate codec can significantly boost training and inference speed.

A few works[^Du2024Cosyvoice], [^Deng2025Indextts] have adopted 25Hz frame rate tokens in the AR stage, others mostly rely on $\ge$50Hz tokens[^Guo2024FireRedTTS], [^Wang2025Spark-TTS].

For the NAR stage, these works mostly keep a frame rate $\ge$50Hz.

![](visualizations/flexicodec_arch3.pdf)

<a id="fig:architecture">\text{Overview of FlexiCodec.}

The model encodes speech through two streams.
% an ASR Encoder extracts high-level semantic features; a Codec Encoder captures low-level acoustic details.
The Frame Merging Modules dynamically reduce the 12.5Hz features into lower frame rates, and the Frame Unmerging Module restores a 12.5Hz fixed frame rate.

The model is trained end to end.
% frame rate based on semantic similarity computed from the ASR features.

The resulting dynamic-rate ASR features are quantized into semantic (RVQ-1) tokens through FSQ, while the residual acoustic features are quantized using RVQ.

For reconstruction, the Frame Unmerging Module restores the fixed 12.5Hz frame rate before the Codec Decoder synthesizes the final waveform.</a>

![](visualizations/mergingunmerging3.pdf)

<a id="fig:mergingunmerging">Detailed views of the Frame Merging Module and the Frame Unmerging Module.
% **(a) The Frame Merging Module** converts a fixed-rate sequence into a dynamic-rate one.

It first computes the cosine similarity between adjacent frames of the ASR features.

If the similarity exceeds a threshold $\tau$, the frames are marked for merging.
% The merged tokens are initially represented by averaged features and are subsequently refined by a Transformer model.
% % A Transformer with local attention processes an interleaved sequence of original and averaged feature`s to refine the representations before the final merged frames are retrieved.
% Each output frame is tagged with its original length.
% **(b) The Frame Unmerging Module** reverses this process.

It takes the dynamic-rate sequence and repeats each frame according to its stored length, restoring a 12.5Hz fixed-rate sequence.

A subsequent Transformer smooths the transitions between the repeated frames to produce the final unmerged feature sequence for the decoder.</a>
