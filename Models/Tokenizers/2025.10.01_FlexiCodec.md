# FlexiCodec: A Dynamic Neural Audio Codec for Low Frame Rates

<details>
<summary>基本信息</summary>

- 标题: "FlexiCodec: A Dynamic Neural Audio Codec for Low Frame Rates."
- 作者:
  - 01 Jiaqi Li
  - 02 Yao Qian
  - 03 Yuxuan Hu
  - 04 Leying Zhang
  - 05 Xiaofei Wang
  - 06 Heng Lu
  - 07 Manthan Thakker
  - 08 Jinyu Li
  - 09 Shang Zhao
  - 10 Zhizheng Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2510.00981v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2510.00981v1](PDF/2025.10.01_2510.00981v1_FlexiCodec__A_Dynamic_Neural_Audio_Codec_for_Low_Frame_Rates.pdf)
  - [Publication] #TODO

</details>

## Abstract

Neural audio codecs are foundational to speech language models.
It is expected to have a low frame rate and decoupled semantic and acoustic information.
A lower frame rate codec can reduce the computational cost of speech language models by shortening the sequence length.
Recent studies have developed 12.5Hz low-frame-rate audio codecs, but even lower frame rate codecs remain underexplored.
We find that a major challenge for very low frame rate tokens is missing semantic information.
This paper introduces **FlexiCodec** to address this limitation.
FlexiCodec improves semantic preservation with a **dynamic frame rate** approach and introduces a novel architecture featuring an **ASR feature-assisted dual stream** encoding and Transformer bottlenecks.
With dynamic frame rates, it uses less frames at information-sparse regions through adaptively merging semantically similar frames.
A dynamic frame rate also allows FlexiCodec to support inference-time **controllable frame rates** between 3Hz and 12.5Hz.
Experiments on **6.25Hz, 8.3Hz and 12.5Hz** average frame rates confirm that FlexiCodec excels over baseline systems in semantic information preservation and delivers a high audio reconstruction quality.
We also validate the effectiveness of FlexiCodec in language model-based TTS.
Demos are available at: \url{https://flexicodec.github.io}.

## 1·Introduction

The neural audio codec technique, originally designed for waveform compression[^Zeghidour2021Soundstream], [^D{\'e}fossez2022High], is now widely used in various tasks including Language Model (LM)-based text-to-speech (TTS)[^Wang2023Neural], [^Li2024Investigating], [^Wang2024Maskgct], [^Du2024CosyVoice], [^Wang2025Spark-TTS], [^Guo2024FireRedTTS] and multimodal LLMs[^D{\'e}fossez2024Moshi], [^Zeng2024GLM-4-Voice], [^Ding2025Kimi-Audio].

By compressing raw speech into compact discrete tokens, neural audio codecs enable the application of auto-regressive LLM paradigms to the speech domain.

The standard neural audio codec follows an encoder-quantizer-decoder architecture.

The encoder downsamples the audio waveform into a sequence of fixed-rate continuous latent vectors.

These vectors are then quantized into discrete indices using Residual Vector Quantization (RVQ), a multi-stage process where each quantizer encodes the residual error of the previous one.

In downstream tasks like TTS, the first RVQ layer’s tokens (RVQ-1) are often used to drive an autoregressive (AR) language model or an LLM, while the remaining layers (RVQ-rest) can be predicted by a non-autoregressive (NAR) model to add acoustic detail.

However, a fundamental challenge arises from the high temporal resolution of these codecs.

State-of-the-art models like EnCodec[^D{\'e}fossez2022High], DAC[^Kumar2024High-Fidelity], and SpeechTokenizer[^Zhang2023Speechtokenizer] operate at frame rates exceeding 50Hz, representing one second of audio with over 50 tokens.

This high density, compared to the $\sim$4.5Hz rate of typical text representations[^Wang2024Why], creates two major problems for AR models: (1) a significant computational burden due to the quadratic complexity of attention, and (2) a severe frame rate mismatch between text and audio modalities that may degrade LLM performance[^Wang2024Why].

<a id="tab:model_comparison_intro">Comparison of different audio tokenization methods and their properties.</a>

To mitigate this, recent work has focused on low-frame-rate codecs.

Methods like Mimi[^D{\'e}fossez2024Moshi] and DualCodec[^Li2025DualCodec] successfully reduced the frame rate to 12.5Hz by decoupling speech into two streams: a semantic stream (RVQ-1) derived from self-supervised learning (SSL) models[^Chen2022Wavlm], [^Barrault2023Seamless], and an acoustic stream (RVQ-rest) for residual details.

In this strategy, the low-rate RVQ-1 tokens encode the core semantic information, which is sufficient for many downstream AR models.

While previous works have proposed 12.5Hz solutions, a significant gap remains compared to the $\sim$4.5Hz frame rate of text.

Furthermore, research into neural audio codecs operating below 12.5Hz is limited.

Our initial experiments revealed that pushing to even lower frame rates leads to significant information loss, particularly the omission of semantic/phonetic content.

Driven by this observation, we have the following motivations for designing our **low frame rate** codec:
(1) **Dynamic Frame Rate**: A fixed low rate inevitably discards transient phonetic details, whereas a dynamic rate may adapt to the phonetic complexity to encode more details.
(2) **Richer Semantics**:
SSL features, trained for reconstruction, can be redundant.

Features from an ASR model, trained for text prediction, may offer a more concentrated source of semantic information.
(3) **Controllable Frame Rate**: Existing codecs typically operate at one or more fixed frame rates.

A continuously controllable rate would allow users to dynamically trade off performance and efficiency for downstream tasks.

In this work, we introduce FlexiCodec, a novel low-frame-rate codec built on three principles: dynamic frame rate, ASR-guided semantics, and frame rate controllability.

Instead of a fixed frame rate, FlexiCodec dynamically allocates temporal resolution, using more frames for complex phonetic segments and fewer for sparse regions like long vowels, syllables and silence.

This is achieved through a novel ASR-feature-assisted dual-stream architecture that adaptively merges semantically similar frames.

A key benefit of this dynamic approach is that a single model can support a continuous range of frame rates **(3-12.5Hz)** at inference, enabling flexible trade-offs for applications like adaptive signal transmission or variable-complexity TTS on edge devices.

Our contributions are:

-  We propose FlexiCodec\footnote{Reproduced models will be available at \url{https://github.com/amphionspace/flexicodec}}.

To our best knowledge, it is the **one of first neural audio codecs under 10Hz frame rate**, and the **first work to explore dynamic frame rate on low-frame-rate neural audio codecs**.

We develop a novel ASR feature-assisted dual stream codec architecture with Transformer bottlenecks.

-  We show that FlexiCodec outperforms open-source baselines in semantic intelligibility and acoustic quality.

Experiments confirm that our dynamic frame rate strategy improves semantic information preservation and allows for controllable frame rate as low as 3Hz.

Other design choices including utilizing an ASR encoder, transformer bottlenecks, and FSQ quantization, also contributes to our codec's performance.

-  We demonstrate FlexiCodec’s utility in a flexible TTS system.

The model yields competitive results at multiple frame rates and is substantially faster than existing methods.
