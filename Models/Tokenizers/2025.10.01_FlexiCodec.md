# FlexiCodec: A Dynamic Neural Audio Codec for Low Frame Rates

<details>
<summary>基本信息</summary>

- 标题: "FlexiCodec: A Dynamic Neural Audio Codec for Low Frame Rates."
- 作者:
  - 01 Jiaqi Li
  - 02 Yao Qian
  - 03 Yuxuan Hu
  - 04 Leying Zhang
  - 05 Xiaofei Wang
  - 06 Heng Lu
  - 07 Manthan Thakker
  - 08 Jinyu Li
  - 09 Shang Zhao
  - 10 Zhizheng Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2510.00981v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2510.00981v1](PDF/2025.10.01_2510.00981v1_FlexiCodec__A_Dynamic_Neural_Audio_Codec_for_Low_Frame_Rates.pdf)
  - [Publication] #TODO

</details>

## Abstract

Neural audio codecs are foundational to speech language models.
It is expected to have a low frame rate and decoupled semantic and acoustic information.
A lower frame rate codec can reduce the computational cost of speech language models by shortening the sequence length.
Recent studies have developed 12.5Hz low-frame-rate audio codecs, but even lower frame rate codecs remain underexplored.
We find that a major challenge for very low frame rate tokens is missing semantic information.
This paper introduces **FlexiCodec** to address this limitation.
FlexiCodec improves semantic preservation with a **dynamic frame rate** approach and introduces a novel architecture featuring an **ASR feature-assisted dual stream** encoding and Transformer bottlenecks.
With dynamic frame rates, it uses less frames at information-sparse regions through adaptively merging semantically similar frames.
A dynamic frame rate also allows FlexiCodec to support inference-time **controllable frame rates** between 3Hz and 12.5Hz.
Experiments on **6.25Hz, 8.3Hz and 12.5Hz** average frame rates confirm that FlexiCodec excels over baseline systems in semantic information preservation and delivers a high audio reconstruction quality.
We also validate the effectiveness of FlexiCodec in language model-based TTS.
Demos are available at: \url{https://flexicodec.github.io}.

## 1·Introduction

The neural audio codec technique, originally designed for waveform compression[^Zeghidour2021Soundstream], [^D{\'e}fossez2022High], is now widely used in various tasks including Language Model (LM)-based text-to-speech (TTS)[^Wang2023Neural], [^Li2024Investigating], [^Wang2024Maskgct], [^Du2024CosyVoice], [^Wang2025Spark-TTS], [^Guo2024FireRedTTS] and multimodal LLMs[^D{\'e}fossez2024Moshi], [^Zeng2024GLM-4-Voice], [^Ding2025Kimi-Audio].

By compressing raw speech into compact discrete tokens, neural audio codecs enable the application of auto-regressive LLM paradigms to the speech domain.

The standard neural audio codec follows an encoder-quantizer-decoder architecture.

The encoder downsamples the audio waveform into a sequence of fixed-rate continuous latent vectors.

These vectors are then quantized into discrete indices using Residual Vector Quantization (RVQ), a multi-stage process where each quantizer encodes the residual error of the previous one.

In downstream tasks like TTS, the first RVQ layer’s tokens (RVQ-1) are often used to drive an autoregressive (AR) language model or an LLM, while the remaining layers (RVQ-rest) can be predicted by a non-autoregressive (NAR) model to add acoustic detail.

However, a fundamental challenge arises from the high temporal resolution of these codecs.

State-of-the-art models like EnCodec[^D{\'e}fossez2022High], DAC[^Kumar2024High-Fidelity], and SpeechTokenizer[^Zhang2023Speechtokenizer] operate at frame rates exceeding 50Hz, representing one second of audio with over 50 tokens.

This high density, compared to the $\sim$4.5Hz rate of typical text representations[^Wang2024Why], creates two major problems for AR models: (1) a significant computational burden due to the quadratic complexity of attention, and (2) a severe frame rate mismatch between text and audio modalities that may degrade LLM performance[^Wang2024Why].

<a id="tab:model_comparison_intro">Comparison of different audio tokenization methods and their properties.</a>

To mitigate this, recent work has focused on low-frame-rate codecs.

Methods like Mimi[^D{\'e}fossez2024Moshi] and DualCodec[^Li2025DualCodec] successfully reduced the frame rate to 12.5Hz by decoupling speech into two streams: a semantic stream (RVQ-1) derived from self-supervised learning (SSL) models[^Chen2022Wavlm], [^Barrault2023Seamless], and an acoustic stream (RVQ-rest) for residual details.

In this strategy, the low-rate RVQ-1 tokens encode the core semantic information, which is sufficient for many downstream AR models.

While previous works have proposed 12.5Hz solutions, a significant gap remains compared to the $\sim$4.5Hz frame rate of text.

Furthermore, research into neural audio codecs operating below 12.5Hz is limited.

Our initial experiments revealed that pushing to even lower frame rates leads to significant information loss, particularly the omission of semantic/phonetic content.

Driven by this observation, we have the following motivations for designing our **low frame rate** codec:
(1) **Dynamic Frame Rate**: A fixed low rate inevitably discards transient phonetic details, whereas a dynamic rate may adapt to the phonetic complexity to encode more details.
(2) **Richer Semantics**:
SSL features, trained for reconstruction, can be redundant.

Features from an ASR model, trained for text prediction, may offer a more concentrated source of semantic information.
(3) **Controllable Frame Rate**: Existing codecs typically operate at one or more fixed frame rates.

A continuously controllable rate would allow users to dynamically trade off performance and efficiency for downstream tasks.

In this work, we introduce FlexiCodec, a novel low-frame-rate codec built on three principles: dynamic frame rate, ASR-guided semantics, and frame rate controllability.

Instead of a fixed frame rate, FlexiCodec dynamically allocates temporal resolution, using more frames for complex phonetic segments and fewer for sparse regions like long vowels, syllables and silence.

This is achieved through a novel ASR-feature-assisted dual-stream architecture that adaptively merges semantically similar frames.

A key benefit of this dynamic approach is that a single model can support a continuous range of frame rates **(3-12.5Hz)** at inference, enabling flexible trade-offs for applications like adaptive signal transmission or variable-complexity TTS on edge devices.

Our contributions are:

-  We propose FlexiCodec\footnote{Reproduced models will be available at \url{https://github.com/amphionspace/flexicodec}}.

To our best knowledge, it is the **one of first neural audio codecs under 10Hz frame rate**, and the **first work to explore dynamic frame rate on low-frame-rate neural audio codecs**.

We develop a novel ASR feature-assisted dual stream codec architecture with Transformer bottlenecks.

-  We show that FlexiCodec outperforms open-source baselines in semantic intelligibility and acoustic quality.

Experiments confirm that our dynamic frame rate strategy improves semantic information preservation and allows for controllable frame rate as low as 3Hz.

Other design choices including utilizing an ASR encoder, transformer bottlenecks, and FSQ quantization, also contributes to our codec's performance.

-  We demonstrate FlexiCodec’s utility in a flexible TTS system.

The model yields competitive results at multiple frame rates and is substantially faster than existing methods.

## 2·Related Works

### Low-Frame-Rate Neural Audio Codecs

Neural audio codecs convert continuous speech into discrete tokens.

SoundStream[^Zeghidour2021Soundstream], Encodec[^D{\'e}fossez2022High], and DAC[^Kumar2024High-Fidelity] focused on audio compression, relying on residual vector quantization (RVQ)[^Zeghidour2021Soundstream] and operating at high bitrates ($\ge$4kbps) and high frame rates ($\ge$50Hz).

WavTokenizer[^Ji2024Wavtokenizer],
TS3-Codec[^Wu2024Ts3-Codec],
SemantiCodec[^Liu2024Semanticodec],
and StableCodec[^Parker2024Scaling] used a single VQ or FSQ[^Mentzer2023Finite] codebook.

They delivered good audio quality at low bitrates (around 1kbps), but operate at a high frame rates ($\ge$40Hz).

Some recent works develop low-frame-rate codecs.

A lower frame rate limits the information amount that can be carried by each RVQ layer tokens.

Thus,
some works[^D{\'e}fossez2024Moshi], [^Li2025DualCodec]
decompose the speech tokens into semantic (RVQ-1) and acoustic (RVQ-rest) tokens.

Mimi codec[^D{\'e}fossez2024Moshi] was based on Encodec with a higher downsampling rate in its convolutional encoder.

It employed semantic distillation[^Zhang2023Speechtokenizer], a technique that distills RVQ-1 embeddings from SSL features.

DualCodec[^Li2025DualCodec] proposed a dual-stream architecture where a semantic stream directly encodes SSL features into RVQ-1 tokens, and an acoustic stream encodes  RVQ-rest tokens.

ALMTokenizer[^Yang2025ALMTokenizer] proposed a query-based compression strategy using a set of learnable query tokens, and designed an auxiliary MAE loss inspired by SSL models.

Concurrent work XY-Tokenizer[^Gong2025XY-Tokenizer] is a 12.5Hz codec encoded with a concatenative dual-stream architecture consisting of Whisper ASR feature and waveform feature.

Recently, TaDiCodec[^Wang2025TaDiCodec] and TASTE[^Tseng2025Taste]  proposed $\le$10Hz speech tokens,
but operating like text-to-speech (TTS) systems,
they require the text transcription to assist audio synthesis.

Accurate text transcription can be unavailable in some audio coding scenarios.

By comparison, our work is more similar to a conventional codec and does not require transcriptions.

### Dynamic-Rate Compression of Images and Audios

The concept of dynamically adjusting token rates based on content complexity is an emerging trend across image and audio modalities.

In the image domain,
[^Bolya2022Token] first proposed Token Merging (ToMe), a technique to gradually merge most similar tokens in Vision Transformers (ViTs) to accelerate inference.

It used the cosine similarity between the self attention keys to guide the merging process.

DynTok[^Zhang2025DynTok] proposed an improved similarity calculation strategy based on CLIP[^Radford2021Learning] semantic representation.

In the audio domain, one research trend has been the syllable-level semantic unit discoveries which are inherently dynamic-rate.

SD-HuBERT[^Cho2024Sd-Hubert] finetuned HuBERT[^Hsu2021Hubert] with sentence-level self-distillation, and showed that its features distinguish syllable boundaries.

SylBoost[^Baade2024Syllablelm] can discover discrete syllabic units using a min-cut algorithm on the feature self-similarity matrix, followed by a k-means clustering.

It experimented on 8.33Hz and 6.25Hz units.

However, compared to neural audio codec tokens, syllable units only encode coarse semantic information, and
necessitate external speech synthesis models to produce audio.

Dynamic-rate neural audio codec is an emerging research area.
[^Dieleman2021Variable-Rate] proposed an audio VQ-VAE with run-length encoding, operating at an average frame rate of 75Hz.

SNAC[^Siuzdak2024Snac] created a multi-resolution codec stream at 12, 23, and 47Hz for each RVQ layer.

CodecSlime[^Wang2025CodecSlime] proposed a two-stage process to firstly train an 80Hz fixed-frame-rate codec, followed by merging similar features into 40Hz average-frame-rate tokens.

Similarly, TFC[^Zhang2025Unlocking] and VARSTok[^Zheng2025Say] introduced dynamic-frame-rate algorithms on 75Hz RVQ or single-codebook tokens.

Compared with previous works, ours explores more challenging low frame rates ($\leq$10Hz).

Compared with CodecSlime, TFC and VARSTok, our work uses a pre-trained audio semantic feature extractor to guide the merging process, simplifying the needs for multi-stage training or dynamic programming-based merging algorithms.

### Text-to-Speech Synthesis

Modern Text-to-Speech (TTS) has increasingly shifted from statistical parametric methods to systems based on neural audio codecs.

VALL-E[^Wang2023Neural] established an "AR+NAR'' framework: an AR language model generates codec RVQ-1 tokens
conditioned on text; an NAR
language model predicts remaining RVQ layer tokens in parallel.

The AR stage provides essential information for the NAR stage, and the AR stage is usually the most time-consuming part.

Subsequent works have evolved this paradigm, showing the benefits of semantic-rich RVQ-1 tokens in the AR stage[^Borsos2023Audiolm], [^Du2024CosyVoice], [^Zhang2025Vevo], and replacements of the NAR discrete token prediction with diffusion-based models[^Betker2023Better], [^Du2024CosyVoice].

The efficiency of a TTS system directly correlates to codec frame rate.

A lower frame rate codec can significantly boost training and inference speed.

A few works[^Du2024Cosyvoice], [^Deng2025Indextts] have adopted 25Hz frame rate tokens in the AR stage, others mostly rely on $\ge$50Hz tokens[^Guo2024FireRedTTS], [^Wang2025Spark-TTS].

For the NAR stage, these works mostly keep a frame rate $\ge$50Hz.

![](visualizations/flexicodec_arch3.pdf)

<a id="fig:architecture">\text{Overview of FlexiCodec.}

The model encodes speech through two streams.
% an ASR Encoder extracts high-level semantic features; a Codec Encoder captures low-level acoustic details.
The Frame Merging Modules dynamically reduce the 12.5Hz features into lower frame rates, and the Frame Unmerging Module restores a 12.5Hz fixed frame rate.

The model is trained end to end.
% frame rate based on semantic similarity computed from the ASR features.

The resulting dynamic-rate ASR features are quantized into semantic (RVQ-1) tokens through FSQ, while the residual acoustic features are quantized using RVQ.

For reconstruction, the Frame Unmerging Module restores the fixed 12.5Hz frame rate before the Codec Decoder synthesizes the final waveform.</a>

![](visualizations/mergingunmerging3.pdf)

<a id="fig:mergingunmerging">Detailed views of the Frame Merging Module and the Frame Unmerging Module.
% **(a) The Frame Merging Module** converts a fixed-rate sequence into a dynamic-rate one.

It first computes the cosine similarity between adjacent frames of the ASR features.

If the similarity exceeds a threshold $\tau$, the frames are marked for merging.
% The merged tokens are initially represented by averaged features and are subsequently refined by a Transformer model.
% % A Transformer with local attention processes an interleaved sequence of original and averaged feature`s to refine the representations before the final merged frames are retrieved.
% Each output frame is tagged with its original length.
% **(b) The Frame Unmerging Module** reverses this process.

It takes the dynamic-rate sequence and repeats each frame according to its stored length, restoring a 12.5Hz fixed-rate sequence.

A subsequent Transformer smooths the transitions between the repeated frames to produce the final unmerged feature sequence for the decoder.</a>

## 3·FlexiCodec: A Dynamic Low-Frame-Rate Neural Audio Codec

The core of our work is **FlexiCodec**, a neural audio codec designed to operate at very low average frame rates while preserving crucial semantic information.

Unlike traditional codecs that use a fixed frame rate, FlexiCodec employs a **dynamic frame rate** mechanism that allocates more temporal resolution to information-dense regions of speech and less to information-sparse segments like silence or long vowels.

This is achieved through an architecture that leverages **pre-trained ASR features to encode semantic-rich RVQ-1 tokens**, and to **guide the adaptive frame merging** process.

The overall architecture, depicted in Figure~[fig:architecture](#fig:architecture), follows a dual-stream encoding, dynamic frame merging, quantization, and frame unmerging decoding pipeline.

**Dual-Stream Feature Extraction**\quad
FlexiCodec\ begins by processing a 16kHz speech waveform through two parallel encoders to decouple semantic and acoustic information:
(1) An ASR Encoder, leveraging the pre-trained ASR model, extracts a sequence of semantic features $e_{s}\in R^{T\times d}$, where $T$ and $d$ denote the number of frames and the vector dimension.

And
(2) a convolutional codec encoder, downsampling the waveform to produce a sequence of waveform features $e_{a}\in R^{T\times d}$, also at a 12.5Hz frame rate.

The codec encoder consists of 5 CNN blocks with strides [4,4,5,8,2] to gradually downsample the audio, giving $16000\text{Hz} \div (4 \times 4 \times 5 \times 8 \times 2) = 12.5\text{Hz}$.

Each CNN block contains a strided 1D convolution followed by a ResNet[^He2016Deep].

For the specific ASR model choice, we use pretrained SenseVoice-Small[^An2024Funaudiollm], a 230M-parameter, encoder-only Transformer model trained on 300k hours of data
with CTC loss.

We use its last layer (excluding its CTC logits prediction layers) hidden state  as the semantic feature.

The model outputs 16.67Hz features; we downsample it to 12.5Hz using linear interpolation to align the frame rate with the acoustic stream.

The ASR model is frozen during codec training.

**Dynamic Frame Merging**\quad
This module compresses the number of frames in a sequence, resulting in a sequence where some segments have lower than 12.5Hz frame rate.

Our goal is to compress the frames with a minimal loss of semantic information
by preferentially merging frames that are semantically similar (redundant).

Inspired by DynTok[^Zhang2025DynTok] that used pre-trained image features for token compression, we reuse the extracted ASR feature to guide our merging process, shown in Figure [fig:mergingunmerging](#fig:mergingunmerging)(a).

Specifically, let the 12.5Hz ASR feature vectors be $\{e_s[1], e_s[2], \dots, e_s[T]\}$.

We first compute the cosine similarity between adjacent frames:
$
s_t = \cos\bigl(e_s[t],\,e_s[t+1]\bigr)\quad\text{for }t=1,\dots,T-1.
$
We then scan from left to right to form maximal contiguous segments \([i,\,i{+}1,\dots, j]\) whose adjacent similarities exceed a threshold \(\tau\):
$
\min_{t=i}^{j-1} s_t \;\ge\;\tau.
$
All frames in such a segment are merged into a single frame by averaging, applied to both the semantic and acoustic streams:
\vspace{-2mm}
$$\tilde e_s[k] \;=\; \frac{1}{\ell_k}\sum_{t=i}^{j} e_s[t],\quad \tilde e_a[k] \;=\; \frac{1}{\ell_k}\sum_{t=i}^{j} e_a[t],
\quad \ell_k \;=\; j - i + 1.$$
\vspace{-5mm}

where \(\tilde e_s[k]\) is the $k$-th entry in the compressed frame sequence in the semantic stream, and \(\tilde e_a[k]\) is the sequence in the acoustic stream.

We also record
the length of each merged frame \(\ell_k\)
as attributes, which are required to reconstruct the original fix-frame-rate sequence.

We find that directly using the average-reduced sequence
can degrade the naturalness of the reconstructed audio, particularly causing unnatural transitions between merged frames.

To address this, we adopt a transformer with local windowed attention that processes an interleaved sequence of original and averaged frames (Fig.[fig:mergingunmerging](#fig:mergingunmerging)(a))[^Yang2025ALMTokenizer], [^Li2023Blip-2].

This allows the merged tokens to query their adjacent context and produce refined, context-aware representations.

A local attention is favored over global attention because it allows generalization to longer audios despite trained on fixed-length audio segments.

**Frame Rate Flexibility**\quad
FlexiCodec\ trains with a flexible merging threshold $\tau$, sampled across a range.

This enables the model to support controllable frame rates at inference by adjusting $\tau$.

At $\tau=1.0$ (no frame merging), FlexiCodec functions as a 12.5Hz fixed-frame-rate codec.

At $\tau < 1.0$ (with frame merging), the average frame rate of the new sequence is lower than 12.5Hz, and is calculated as:
$\frac{\text{Total number of frames after merging}}{\text{Audio duration in seconds}}$.

Setting a lower $\tau$ decreases the average frame rate.

**Semantic (RVQ-1) Quantization**\quad
The dynamic-rate ASR features are quantized using a Finite Scalar Quantizer (FSQ)[^Mentzer2023Finite] to produce discrete semantic tokens (we denote as RVQ-1 tokens).

FSQ projects the input representations $e_s$ into a $D$-dimensional low-rank space, where the value of each dimension is quantized using rounding operation ROUND into $L$ levels.

The quantized low-rank vector $\bar{e}$ is subsequently projected back to the original dimension $\bar{e}$:
$$
\bar{e_s} = \mathrm{ROUND}\bigl(\mathrm{Proj}_{\text{down}}(e_s)\bigr),
\quad
\hat{e_s} = \mathrm{Proj}_{\text{up}}(\bar{e_s}).
$$
The semantic token index $q_s$ is obtained by:
$q_s = \sum_{j=0}^{D-1}\bar{e_s}L^j$.

Additionally, we wrap the FSQ block with small ConvNeXt[^Liu2022Convnet] blocks to increase its representation ability, and apply an L2 loss $L_\text{feat}$ to align the semantic token embeddings with the unquantized semantic features.

**Acoustic (RVQ-rest) Quantization**\quad
To encode the detailed acoustic information that is not captured in the semantic tokens, following[^Li2025DualCodec],
we compute a residual by subtracting the dynamic-rate ASR feature from the dynamic-rate waveform feature.

This residual is then quantized using an $(N-1)$-layer Residual Vector Quantization (RVQ)[^Zeghidour2021Soundstream]\footnote{We have not applied FSQ for acoustic quantization because FSQ
is a single-layer quantization,
and we have not discovered a multi-layer FSQ practice in literature.} module to produce acoustic tokens $q_{2:N}\in Z^{(N-1)\times\hat{T}}$, where $\hat{T}$ denotes the sequence length after frame merging.

We employ quantizer dropout[^Zeghidour2021Soundstream] during training.

That is, only the RVQ-1 to RVQ-$n$
layers are subsequently decoded, where $n \in [1, N]$ is randomly chosen.

When $n=1$, only the semantic encoding stream is used.

**Frame Unmerging and Reconstruction**\quad
To reconstruct the audio, the decoder path should reverse the dynamic compression\footnote{This is because the NAR codec decoder can only receive fixed-frame-rate sequence.

It is also possible to generate audios directly from dynamic-rate tokens using an AR model, and we leave this as future work.}.

The embedding features from the first $n$ chosen RVQ tokens are added to form a decoding feature representation.

This dynamic-rate sequence is then passed to the Frame Unmerging Module, detailed in Figure [fig:mergingunmerging](#fig:mergingunmerging)(b).

It uses the frame length attributes to expand the sequence back to 12.5Hz.

It is followed by another Transformer with local attention to smooth the transitions and refine the feature representations.

The resulting feature sequence is then fed into a convolutional codec decoder which mirrors the codec encoder, synthesizing the output waveform.

FlexiCodec is trained end-to-end with a composite loss function:

$$
\mathcal{L} = \mathcal{L}_{\text{recon}} + \lambda_{\text{GAN}}\mathcal{L}_{\text{GAN}} + \lambda_{\text{RVQ}}\mathcal{L}_{\text{RVQ}} + \lambda_{\text{feat}}\mathcal{L}_{\text{feat}},
$$

where $\mathcal{L}_{\text{recon}}$ is a multi-scale L1 mel spectrogram reconstruction loss following[^Kumar2024High-Fidelity], $\mathcal{L}_{\text{GAN}}$ contains adversarial and feature matching losses for Multi-Period Discriminator (MPD)[^Kong2020Hifi-Gan] and Multi-Resolution Spectrogram Discriminator (MRSD)[^Kumar2024High-Fidelity], $\mathcal{L}_{\text{RVQ}}$ involves a L1 codebook update loss
and a commitment loss for RVQ, whereas the FSQ module does not require a training loss.
$\mathcal{L}_\text{feat}$ is the L2 feature alignment loss between the RVQ-1 semantic token embeddings and the unquantized semantic features.

## 4·Experiments

### Experimental Setup

\paragraph{Codec Training Configuration}

We use the 16kHz, 54k-hour Librilight-Large[^Kahn2020Libri-Light] dataset for training.

Each model is trained for 800k steps on 8 Nvidia V100 32GB GPUs.

We use a batch size of 5 samples per GPU; each sample is a 5 second audio segment.

We use AdamW[^Loshchilov2017Decoupled] optimizer with $\text{lr=}1\times10^{-4}$, $\text{betas=}(0.8, 0.99)$; exponential learning rate with gamma=$0.999998$.

In each step, the merging threshold $\tau$ is randomly chosen from $0.7\le\tau\le1.0$.

We set the maximum frame length $\ell_k$ to $8$ so that each $\ell_k$ would take at most $log_28=3$ bits of storage.

Each token in the local attention transformer can attend to $\ell_k=8$ tokens left and right.

The FSQ module has $D=5$ dimensions each quantized to $L=8$ levels, resulting in $8^5=32768$ codebook entries.

The RVQ-rest quantization has 24 RVQ layers, 4096 codebook entries per layer, and 512-dimensional codebook entries.

FlexiCodec has 216M trainable parameters, in which the two frame merging modules each is 20M, and the frame unmerging module is 100M.

\paragraph{Codec Evaluation Metrics}

We evaluate on the 4 to 10 second subset of LibriSpeech-test-clean[^Panayotov2015Librispeech], comprising 1,088 audios.

We evaluate one FlexiCodec on three frame rates: 12.5Hz (80ms hop size), 8.3Hz (120ms hop size), and 6.25Hz (160ms hop size).

Since FlexiCodec has dynamic, controllable frame rate,
we configure its $\tau=1.0, 0.91, 0.867$ for 12.5Hz, 8.3Hz, and 6.25Hz average frame rate, respectively.

These $\tau$ settings have been determined based on trial runs on our test set.

Our evaluations consist of the following semantic and acoustic testings:

$\bullet$ **Semantic testing:**

To evaluate the preservation of semantic content, we transcribe the codec reconstructed audio using a Hubert-Large-LS960-ft[^Hsu2021Hubert]

ASR model and compute the word error rate (WER) against ground truth transcriptions.

The testings include using RVQ-1 alone, and RVQ-1:8 (1 to 8 layers) tokens.

Specifically, RVQ-1 tokens' semantic preservation relates to downstream model performance, where AR LMs only access RVQ-1 tokens[^Li2025DualCodec], [^Zhang2023Speechtokenizer].

Evaluating RVQ-1:8 is a common choice of RVQ-based codecs, implying the acoustic compression quality[^D{\'e}fossez2024Moshi], [^Zhang2023Speechtokenizer].

$\bullet$ **Acoustic testing:**

We evaluate the reconstructed audio from RVQ1:8 tokens against the original audio.

Metrics include the Perceptual Evaluation of Speech Quality (PESQ, narrow band) [^Rix2001Perceptual],
Mel Cepstral Distortion (MCD) [^Kubichek1993Mel-Cepstral],
speaker similarity (SIM, the cosine similarity between speaker embeddings extracted from a WavLM-large-based[^Chen2022Wavlm] speaker verification model),
and a speech perceptual quality score from a neural Mean Opinion Score (MOS) predictor UTMOS[^Saeki2022Utmos].

$\bullet$ **Additional semantic testing with ASR probing:**\quad
Additionally, to evaluate the semantic alignment between FlexiCodec semantic tokens and text, we employ an \text{ASR probing task}[^Gong2025XY-Tokenizer], [^Zhang2023Speechtokenizer], adapted from XARES[^Zhang2025X-Ares] benchmark.

We train a downstream Qwen2.5-0.5B-based[^Yang2025Qwen2.5]

ASR model which is tasked with predicting the text transcription given FlexiCodec's RVQ-1 token embeddings.

During
training, only the parameters of an MLP adapter
are updated.

Models are trained on LibriSpeech train-clean-100 using cross entropy loss.

We evaluate the ASR
WER on LibriSpeech test-clean.

The task's upper bound result is obtained by training with the unquantized SenseVoice-Small encoder feature.

Due to the prolonged evaluation time of this task, we only conduct this task as an extended semantic testing in Section~[sec:ablation_dynamic_frame_rate](#sec:ablation_dynamic_frame_rate).

![](visualizations/frame_rate_comparison_plots1.pdf)

<a id="fig:very_low_frame_rate">Evaluation results on three very low frame rates.
Each baseline system has been retrained for each target frame rate using the same recipe as FlexiCodec.</a>

### Examining the Impact of Very Low Frame Rates

\label{sec:exp_very_low_frame_rates}

We first investigate the performance of representative audio codecs at very low frame rates.

Since open-source baselines operating below 12.5Hz are unavailable, we created three new baseline versions by retraining DAC[^Kumar2024High-Fidelity] and DualCodec[^Li2025DualCodec] to operate at 12.5Hz, 8.3Hz, and 6.25Hz, respectively.

To adapt these systems for lower frame rates,
we increased their encoder downsampling rates and enlarged their codebook sizes to be consistent with FlexiCodec.

Detailed configurations are provided in Appendix~[sec:appendix_more_information_retrained_baselines](#sec:appendix_more_information_retrained_baselines).

Our findings are as follows.

$\bullet$ **RVQ-1 tokens' semantic information preservation is challenging for very low frame rate codecs.\quad**

As shown in Figure~[fig:very_low_frame_rate](#fig:very_low_frame_rate)(a),
we see that the Word Error Rate (WER), evaluated on audios reconstructed from RVQ-1 tokens, increases substantially for DAC and DualCodec when the frame rate drops from 12.5Hz to 6.25Hz.

At 12.5Hz, DualCodec achieves a 5.93\% WER, close to the ground truth (GT) 2.1\%.

However, as its frame rate drops to 6.25Hz, its gap with GT significantly increases to 31.5\% vs. 2.1\%, indicating its **lower resolution tokens fail to capture the whole phonetic details.**

Note that DualCodec architecture already improves WER over DAC thanks to its semantic augmentation using SSL features.

Figure~[fig:very_low_frame_rate](#fig:very_low_frame_rate)(b) confirms this trend, though the performance gap is smaller when using more RVQ layers.

$\bullet$ **FlexiCodec excels at semantic preservation, especially at the lowest frame rates.**

In contrast, FlexiCodec maintains a low WER across all rates.

At the most challenging 6.25Hz average frame rate, FlexiCodec achieves 4.15\% WER which is close to ground truth (2.1\%) and outperforms the best baseline (31.5\%).

To explain FlexiCodec's superior semantic information retention, our further experiments suggest it is contributed by a combination of our proposed design choices, including the ASR-assisted encoding architecture, dynamic frame rate merging, FSQ, and Transformer modules.

These modules are ablated in Appendix~[sec:appendix_ablation_study](#sec:appendix_ablation_study).

We have found that a simple switching from DualCodec's SSL into FlexiCodec ASR feature is very useful and it achieves an RVQ1 WER of 6.0\% at 6.25Hz.

Further improvements are particularly driven by the dynamic frame rate strategy, which we will detail in Section~[sec:ablation_dynamic_frame_rate](#sec:ablation_dynamic_frame_rate).

$\bullet$ **Acoustic quality metrics show more moderate differences across systems and frame rates.**

As shown in Figure~[fig:very_low_frame_rate](#fig:very_low_frame_rate)(c-e), the PESQ, MCD, SIM, and UTMOS metrics show that at each frame rate, FlexiCodec slightly outperforms DualCodec, followed by a larger gap with DAC.

We observe that
the gap between models does not substantially increase when changing to lower frame rates, but has been seen in the semantic testing.

We think that the acoustic fidelity is more constrained by bitrate, and because the bitrates of the three systems are at the same level, the difference is not pronounced.

### Analysis and Ablation of Dynamic Frame Rate

\label{sec:ablation_dynamic_frame_rate}

![](visualizations/phoneme_d2codec_correlation.pdf)

<a id="fig:corr">Correlation between Flexi-Codec frame rate and phoneme rate at a fixed frame merging threshold $\tau$.
Each data point is an audio in TIMIT dataset, representing the audio's average phoneme rate vs. average FlexiCodec frame rate.
% It shows that FlexiCodec uses more frames for faster speaking utterance, vice versa.
% Linear regression shows a pearson correlation $r=0.77$.</a>

To understand the mechanism and performance of dynamic frame rate, we conduct a series of analyses and ablations shown in Figure~[fig:corr](#fig:corr) and Table~[tab:tau](#tab:tau)-[tab:frame_rate_ablations](#tab:frame_rate_ablations).

Our findings are as follows.

$\bullet$ **Dynamic frame merging effectively adapts to the underlying phonetic complexity of speech.**

Figure~[fig:corr](#fig:corr) shows a strong positive correlation (Pearson $r=0.775$) between the utterance-level phoneme rate and FlexiCodec's frame rate on the TIMIT[^Garofolo1993Timit] subset.

This demonstrates that FlexiCodec dynamically adjusts its token frame allocation in proportion to the phonetic density of the audio, assigning more frames to segments with faster speech and fewer frames to slower or silence regions.

Such adaptivity enables efficient semantic compression by aligning token rate with semantic information density.

The fitted line also shows a linear coefficient very close to $0.5$, indicating that each merged frame approximately encodes two phonemes.

We visualize several cases by aligning phonemes labels with FlexiCodec frames, which is shown in Appendix~[appendix_sec:visualizaton](#appendix_sec:visualizaton).

It confirms that typical merged tokens include syllables/short words, long vowels, and silence.

$\bullet$ **Dynamic frame rate enables controllable frame rate as low as 3Hz.**\quad
As shown in Table~[tab:tau](#tab:tau), the merging threshold $\tau$ controls frame rate and semantic preservation trade-off.

Notably, by controlling $\tau$ values at inference time, we can obtain different output sequence lengths,
which is a unique feature compared to conventional fixed-frame-rate codecs.

$\bullet$ **Dynamic frame rate improves semantic information preservation.**\quad
Tables~[tab:semantic_test](#tab:semantic_test) and [tab:frame_rate_ablations](#tab:frame_rate_ablations) compare FlexiCodec and its variants that are retrained with fixed frame rate (FFR).

To obtain the FFR variants, we modify their codec encoder strides to output static 8.3Hz or 6.25Hz, and maintain the total parameter count (detailed in Appendix~[sec:appendix_more_information_retrained_baselines](#sec:appendix_more_information_retrained_baselines)).

Table~[tab:semantic_test](#tab:semantic_test) shows that the FFR variants have consistently worse WERs than FlexiCodec.

The gap is larger at the lower frame rate.

Specifically, the 6.25Hz FFR variant increases RVQ-1 WER by a relative 26\%, the ASR probing WER by 21\%, and the RVQ1:8 WER by 8\%.

These results highlight that dynamic frame rate improves semantic information preservation by a mechanism of phonetic complexity-adaptive frame rate allocation.

We also note that dynamic frame rate may
perform even better on real-world data, which tend to have longer silence regions that are highly compressible.

We leave this investigation as future work.

In terms of acoustic metrics, Table~[tab:frame_rate_ablations](#tab:frame_rate_ablations) shows that PESQ and UTMOS are the same between FlexiCodec and its FFR variant, while a moderate degradation in MCD and SIM is observed in FFR.

This indicates that dynamic frame rate mainly boosts semantic preservation rather than low-level acoustic quality.

One possible reason is that the acoustic information density in a speech can be misaligned with the semantic information density.

For example, semantically unimportant segments may still contain acoustic details like noise, sound and music.

### Comparison with Open-Source Codecs at Various Bitrates

In this experiment, we compare FlexiCodec with open-source neural audio codecs spanning various bitrate
and frame rates.

Information about the baseline codecs are provided in Appendix~[sec:appendix_more_information_baseline_codec](#sec:appendix_more_information_baseline_codec).

We categorize them into 3 bitrate classes, and
FlexiCodec at 12.5Hz, 8.3Hz, and 6.25Hz average frame rates fall into each category, enabling bitrate-consistent comparisons.

Table~[tab:tokenizer-rec-res](#tab:tokenizer-rec-res) presents the results.

<a id="tab:tokenizer-rec-res">\text{Comparison between FlexiCodec and other open-source neural audio codecs.}
\vspace{-4mm}</a>

$\bullet$ **FlexiCodec has state-of-the-art audio quality at various bitrate levels.**\quad
Examining the acoustic test scores, at $>$1kbps, FlexiCodec at 12.5Hz (1.2kbps)  achieves higher acoustic scores than its 12.5Hz counterparts Mimi, XYTokenizer and DualCodec.

It only trails behind DAC which uses a higher bitate (6kbps).

At 0.8kbps and 0.6kbps,
FlexiCodec also demonstrates superior acoustic quality scores than baselines.

These results demonstrate that
FlexiCodec has a high bitrate efficiency.

$\bullet$ **FlexiCodec is competitive to higher frame rate systems at semantic information preservation.**\quad
Across the WER semantic test metrics at both RVQ1 and RVQ1:8 quantization levels,
FlexiCodec consistently achieves competitive or better scores compared to other systems operating at higher frame rates.

Notably,
FlexiCodec at 6.25Hz attains an RVQ1 WER of 4.15,
outperforming larger frame rate models such as SpeechTokenizer-50Hz, Encodec-75Hz, WavTokenizer-75Hz, etc.

\vspace{-1mm}

## 5·More Experiments

$\bullet$ **Downstream TTS experiments.**\quad
In Appendix~[sec:appendix_tts](#sec:appendix_tts),
we detail our low-frame-rate TTS system with FlexiCodec (FlexiCodec-TTS).

Our conclusions are (1) FlexiCodec-TTS achieves competitive performance with significant speedups over baselines, and
(2) a higher NAR frame rate is important for high audio quality, but using lower frame rates for AR does not necessarily degrade performance.

$\bullet$ **Downstream audio understanding experiments.**\quad
In Appendix~[sec:appendix_audio_understanding](#sec:appendix_audio_understanding), we show that FlexiCodec semantic token embeddings surpass other codecs in downstream audio understanding tasks, highlighting the potential to use FlexiCodec in unified multimodal understanding and generation frameworks.

$\bullet$ **Ablation study on other components of FlexiCodec.**\quad
In Appendix~[sec:appendix_ablation_study](#sec:appendix_ablation_study), we confirm our other design choices like utilizing an ASR feature, transformer-based frame merging and unmerging modules, and FSQ quantization are beneficial to our codec's semantic and acoustic performance.

\vspace{-1mm}
