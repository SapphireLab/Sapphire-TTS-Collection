# FlexiCodec: A Dynamic Neural Audio Codec for Low Frame Rates

<details>
<summary>基本信息</summary>

- 标题: "FlexiCodec: A Dynamic Neural Audio Codec for Low Frame Rates."
- 作者:
  - 01 Jiaqi Li
  - 02 Yao Qian
  - 03 Yuxuan Hu
  - 04 Leying Zhang
  - 05 Xiaofei Wang
  - 06 Heng Lu
  - 07 Manthan Thakker
  - 08 Jinyu Li
  - 09 Shang Zhao
  - 10 Zhizheng Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2510.00981v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2510.00981v1](PDF/2025.10.01_2510.00981v1_FlexiCodec__A_Dynamic_Neural_Audio_Codec_for_Low_Frame_Rates.pdf)
  - [Publication] #TODO

</details>

## Abstract

Neural audio codecs are foundational to speech language models.
It is expected to have a low frame rate and decoupled semantic and acoustic information.
A lower frame rate codec can reduce the computational cost of speech language models by shortening the sequence length.
Recent studies have developed 12.5Hz low-frame-rate audio codecs, but even lower frame rate codecs remain underexplored.
We find that a major challenge for very low frame rate tokens is missing semantic information.
This paper introduces **FlexiCodec** to address this limitation.
FlexiCodec improves semantic preservation with a **dynamic frame rate** approach and introduces a novel architecture featuring an **ASR feature-assisted dual stream** encoding and Transformer bottlenecks.
With dynamic frame rates, it uses less frames at information-sparse regions through adaptively merging semantically similar frames.
A dynamic frame rate also allows FlexiCodec to support inference-time **controllable frame rates** between 3Hz and 12.5Hz.
Experiments on **6.25Hz, 8.3Hz and 12.5Hz** average frame rates confirm that FlexiCodec excels over baseline systems in semantic information preservation and delivers a high audio reconstruction quality.
We also validate the effectiveness of FlexiCodec in language model-based TTS.
Demos are available at: \url{https://flexicodec.github.io}.

## 1·Introduction

The neural audio codec technique, originally designed for waveform compression[^Zeghidour2021Soundstream], [^D{\'e}fossez2022High], is now widely used in various tasks including Language Model (LM)-based text-to-speech (TTS)[^Wang2023Neural], [^Li2024Investigating], [^Wang2024Maskgct], [^Du2024CosyVoice], [^Wang2025Spark-TTS], [^Guo2024FireRedTTS] and multimodal LLMs[^D{\'e}fossez2024Moshi], [^Zeng2024GLM-4-Voice], [^Ding2025Kimi-Audio].

By compressing raw speech into compact discrete tokens, neural audio codecs enable the application of auto-regressive LLM paradigms to the speech domain.

The standard neural audio codec follows an encoder-quantizer-decoder architecture.

The encoder downsamples the audio waveform into a sequence of fixed-rate continuous latent vectors.

These vectors are then quantized into discrete indices using Residual Vector Quantization (RVQ), a multi-stage process where each quantizer encodes the residual error of the previous one.

In downstream tasks like TTS, the first RVQ layer’s tokens (RVQ-1) are often used to drive an autoregressive (AR) language model or an LLM, while the remaining layers (RVQ-rest) can be predicted by a non-autoregressive (NAR) model to add acoustic detail.

However, a fundamental challenge arises from the high temporal resolution of these codecs.

State-of-the-art models like EnCodec[^D{\'e}fossez2022High], DAC[^Kumar2024High-Fidelity], and SpeechTokenizer[^Zhang2023Speechtokenizer] operate at frame rates exceeding 50Hz, representing one second of audio with over 50 tokens.

This high density, compared to the $\sim$4.5Hz rate of typical text representations[^Wang2024Why], creates two major problems for AR models: (1) a significant computational burden due to the quadratic complexity of attention, and (2) a severe frame rate mismatch between text and audio modalities that may degrade LLM performance[^Wang2024Why].

<a id="tab:model_comparison_intro">Comparison of different audio tokenization methods and their properties.</a>

To mitigate this, recent work has focused on low-frame-rate codecs.

Methods like Mimi[^D{\'e}fossez2024Moshi] and DualCodec[^Li2025DualCodec] successfully reduced the frame rate to 12.5Hz by decoupling speech into two streams: a semantic stream (RVQ-1) derived from self-supervised learning (SSL) models[^Chen2022Wavlm], [^Barrault2023Seamless], and an acoustic stream (RVQ-rest) for residual details.

In this strategy, the low-rate RVQ-1 tokens encode the core semantic information, which is sufficient for many downstream AR models.

While previous works have proposed 12.5Hz solutions, a significant gap remains compared to the $\sim$4.5Hz frame rate of text.

Furthermore, research into neural audio codecs operating below 12.5Hz is limited.

Our initial experiments revealed that pushing to even lower frame rates leads to significant information loss, particularly the omission of semantic/phonetic content.

Driven by this observation, we have the following motivations for designing our **low frame rate** codec:
(1) **Dynamic Frame Rate**: A fixed low rate inevitably discards transient phonetic details, whereas a dynamic rate may adapt to the phonetic complexity to encode more details.
(2) **Richer Semantics**:
SSL features, trained for reconstruction, can be redundant.

Features from an ASR model, trained for text prediction, may offer a more concentrated source of semantic information.
(3) **Controllable Frame Rate**: Existing codecs typically operate at one or more fixed frame rates.

A continuously controllable rate would allow users to dynamically trade off performance and efficiency for downstream tasks.

In this work, we introduce FlexiCodec, a novel low-frame-rate codec built on three principles: dynamic frame rate, ASR-guided semantics, and frame rate controllability.

Instead of a fixed frame rate, FlexiCodec dynamically allocates temporal resolution, using more frames for complex phonetic segments and fewer for sparse regions like long vowels, syllables and silence.

This is achieved through a novel ASR-feature-assisted dual-stream architecture that adaptively merges semantically similar frames.

A key benefit of this dynamic approach is that a single model can support a continuous range of frame rates **(3-12.5Hz)** at inference, enabling flexible trade-offs for applications like adaptive signal transmission or variable-complexity TTS on edge devices.

Our contributions are:

-  We propose FlexiCodec\footnote{Reproduced models will be available at \url{https://github.com/amphionspace/flexicodec}}.

To our best knowledge, it is the **one of first neural audio codecs under 10Hz frame rate**, and the **first work to explore dynamic frame rate on low-frame-rate neural audio codecs**.

We develop a novel ASR feature-assisted dual stream codec architecture with Transformer bottlenecks.

-  We show that FlexiCodec outperforms open-source baselines in semantic intelligibility and acoustic quality.

Experiments confirm that our dynamic frame rate strategy improves semantic information preservation and allows for controllable frame rate as low as 3Hz.

Other design choices including utilizing an ASR encoder, transformer bottlenecks, and FSQ quantization, also contributes to our codec's performance.

-  We demonstrate FlexiCodec’s utility in a flexible TTS system.

The model yields competitive results at multiple frame rates and is substantially faster than existing methods.

## 2·Related Works

### Low-Frame-Rate Neural Audio Codecs

Neural audio codecs convert continuous speech into discrete tokens.

SoundStream[^Zeghidour2021Soundstream], Encodec[^D{\'e}fossez2022High], and DAC[^Kumar2024High-Fidelity] focused on audio compression, relying on residual vector quantization (RVQ)[^Zeghidour2021Soundstream] and operating at high bitrates ($\ge$4kbps) and high frame rates ($\ge$50Hz).

WavTokenizer[^Ji2024Wavtokenizer],
TS3-Codec[^Wu2024Ts3-Codec],
SemantiCodec[^Liu2024Semanticodec],
and StableCodec[^Parker2024Scaling] used a single VQ or FSQ[^Mentzer2023Finite] codebook.

They delivered good audio quality at low bitrates (around 1kbps), but operate at a high frame rates ($\ge$40Hz).

Some recent works develop low-frame-rate codecs.

A lower frame rate limits the information amount that can be carried by each RVQ layer tokens.

Thus,
some works[^D{\'e}fossez2024Moshi], [^Li2025DualCodec]
decompose the speech tokens into semantic (RVQ-1) and acoustic (RVQ-rest) tokens.

Mimi codec[^D{\'e}fossez2024Moshi] was based on Encodec with a higher downsampling rate in its convolutional encoder.

It employed semantic distillation[^Zhang2023Speechtokenizer], a technique that distills RVQ-1 embeddings from SSL features.

DualCodec[^Li2025DualCodec] proposed a dual-stream architecture where a semantic stream directly encodes SSL features into RVQ-1 tokens, and an acoustic stream encodes  RVQ-rest tokens.

ALMTokenizer[^Yang2025ALMTokenizer] proposed a query-based compression strategy using a set of learnable query tokens, and designed an auxiliary MAE loss inspired by SSL models.

Concurrent work XY-Tokenizer[^Gong2025XY-Tokenizer] is a 12.5Hz codec encoded with a concatenative dual-stream architecture consisting of Whisper ASR feature and waveform feature.

Recently, TaDiCodec[^Wang2025TaDiCodec] and TASTE[^Tseng2025Taste]  proposed $\le$10Hz speech tokens,
but operating like text-to-speech (TTS) systems,
they require the text transcription to assist audio synthesis.

Accurate text transcription can be unavailable in some audio coding scenarios.

By comparison, our work is more similar to a conventional codec and does not require transcriptions.

### Dynamic-Rate Compression of Images and Audios

The concept of dynamically adjusting token rates based on content complexity is an emerging trend across image and audio modalities.

In the image domain,
[^Bolya2022Token] first proposed Token Merging (ToMe), a technique to gradually merge most similar tokens in Vision Transformers (ViTs) to accelerate inference.

It used the cosine similarity between the self attention keys to guide the merging process.

DynTok[^Zhang2025DynTok] proposed an improved similarity calculation strategy based on CLIP[^Radford2021Learning] semantic representation.

In the audio domain, one research trend has been the syllable-level semantic unit discoveries which are inherently dynamic-rate.

SD-HuBERT[^Cho2024Sd-Hubert] finetuned HuBERT[^Hsu2021Hubert] with sentence-level self-distillation, and showed that its features distinguish syllable boundaries.

SylBoost[^Baade2024Syllablelm] can discover discrete syllabic units using a min-cut algorithm on the feature self-similarity matrix, followed by a k-means clustering.

It experimented on 8.33Hz and 6.25Hz units.

However, compared to neural audio codec tokens, syllable units only encode coarse semantic information, and
necessitate external speech synthesis models to produce audio.

Dynamic-rate neural audio codec is an emerging research area.
[^Dieleman2021Variable-Rate] proposed an audio VQ-VAE with run-length encoding, operating at an average frame rate of 75Hz.

SNAC[^Siuzdak2024Snac] created a multi-resolution codec stream at 12, 23, and 47Hz for each RVQ layer.

CodecSlime[^Wang2025CodecSlime] proposed a two-stage process to firstly train an 80Hz fixed-frame-rate codec, followed by merging similar features into 40Hz average-frame-rate tokens.

Similarly, TFC[^Zhang2025Unlocking] and VARSTok[^Zheng2025Say] introduced dynamic-frame-rate algorithms on 75Hz RVQ or single-codebook tokens.

Compared with previous works, ours explores more challenging low frame rates ($\leq$10Hz).

Compared with CodecSlime, TFC and VARSTok, our work uses a pre-trained audio semantic feature extractor to guide the merging process, simplifying the needs for multi-stage training or dynamic programming-based merging algorithms.

### Text-to-Speech Synthesis

Modern Text-to-Speech (TTS) has increasingly shifted from statistical parametric methods to systems based on neural audio codecs.

VALL-E[^Wang2023Neural] established an "AR+NAR'' framework: an AR language model generates codec RVQ-1 tokens
conditioned on text; an NAR
language model predicts remaining RVQ layer tokens in parallel.

The AR stage provides essential information for the NAR stage, and the AR stage is usually the most time-consuming part.

Subsequent works have evolved this paradigm, showing the benefits of semantic-rich RVQ-1 tokens in the AR stage[^Borsos2023Audiolm], [^Du2024CosyVoice], [^Zhang2025Vevo], and replacements of the NAR discrete token prediction with diffusion-based models[^Betker2023Better], [^Du2024CosyVoice].

The efficiency of a TTS system directly correlates to codec frame rate.

A lower frame rate codec can significantly boost training and inference speed.

A few works[^Du2024Cosyvoice], [^Deng2025Indextts] have adopted 25Hz frame rate tokens in the AR stage, others mostly rely on $\ge$50Hz tokens[^Guo2024FireRedTTS], [^Wang2025Spark-TTS].

For the NAR stage, these works mostly keep a frame rate $\ge$50Hz.

![](visualizations/flexicodec_arch3.pdf)

<a id="fig:architecture">\text{Overview of FlexiCodec.}

The model encodes speech through two streams.
% an ASR Encoder extracts high-level semantic features; a Codec Encoder captures low-level acoustic details.
The Frame Merging Modules dynamically reduce the 12.5Hz features into lower frame rates, and the Frame Unmerging Module restores a 12.5Hz fixed frame rate.

The model is trained end to end.
% frame rate based on semantic similarity computed from the ASR features.

The resulting dynamic-rate ASR features are quantized into semantic (RVQ-1) tokens through FSQ, while the residual acoustic features are quantized using RVQ.

For reconstruction, the Frame Unmerging Module restores the fixed 12.5Hz frame rate before the Codec Decoder synthesizes the final waveform.</a>

![](visualizations/mergingunmerging3.pdf)

<a id="fig:mergingunmerging">Detailed views of the Frame Merging Module and the Frame Unmerging Module.
% **(a) The Frame Merging Module** converts a fixed-rate sequence into a dynamic-rate one.

It first computes the cosine similarity between adjacent frames of the ASR features.

If the similarity exceeds a threshold $\tau$, the frames are marked for merging.
% The merged tokens are initially represented by averaged features and are subsequently refined by a Transformer model.
% % A Transformer with local attention processes an interleaved sequence of original and averaged feature`s to refine the representations before the final merged frames are retrieved.
% Each output frame is tagged with its original length.
% **(b) The Frame Unmerging Module** reverses this process.

It takes the dynamic-rate sequence and repeats each frame according to its stored length, restoring a 12.5Hz fixed-rate sequence.

A subsequent Transformer smooths the transitions between the repeated frames to produce the final unmerged feature sequence for the decoder.</a>

## 3·FlexiCodec: A Dynamic Low-Frame-Rate Neural Audio Codec

The core of our work is **FlexiCodec**, a neural audio codec designed to operate at very low average frame rates while preserving crucial semantic information.

Unlike traditional codecs that use a fixed frame rate, FlexiCodec employs a **dynamic frame rate** mechanism that allocates more temporal resolution to information-dense regions of speech and less to information-sparse segments like silence or long vowels.

This is achieved through an architecture that leverages **pre-trained ASR features to encode semantic-rich RVQ-1 tokens**, and to **guide the adaptive frame merging** process.

The overall architecture, depicted in Figure~[fig:architecture](#fig:architecture), follows a dual-stream encoding, dynamic frame merging, quantization, and frame unmerging decoding pipeline.

**Dual-Stream Feature Extraction**\quad
FlexiCodec\ begins by processing a 16kHz speech waveform through two parallel encoders to decouple semantic and acoustic information:
(1) An ASR Encoder, leveraging the pre-trained ASR model, extracts a sequence of semantic features $e_{s}\in R^{T\times d}$, where $T$ and $d$ denote the number of frames and the vector dimension.

And
(2) a convolutional codec encoder, downsampling the waveform to produce a sequence of waveform features $e_{a}\in R^{T\times d}$, also at a 12.5Hz frame rate.

The codec encoder consists of 5 CNN blocks with strides [4,4,5,8,2] to gradually downsample the audio, giving $16000\text{Hz} \div (4 \times 4 \times 5 \times 8 \times 2) = 12.5\text{Hz}$.

Each CNN block contains a strided 1D convolution followed by a ResNet[^He2016Deep].

For the specific ASR model choice, we use pretrained SenseVoice-Small[^An2024Funaudiollm], a 230M-parameter, encoder-only Transformer model trained on 300k hours of data
with CTC loss.

We use its last layer (excluding its CTC logits prediction layers) hidden state  as the semantic feature.

The model outputs 16.67Hz features; we downsample it to 12.5Hz using linear interpolation to align the frame rate with the acoustic stream.

The ASR model is frozen during codec training.

**Dynamic Frame Merging**\quad
This module compresses the number of frames in a sequence, resulting in a sequence where some segments have lower than 12.5Hz frame rate.

Our goal is to compress the frames with a minimal loss of semantic information
by preferentially merging frames that are semantically similar (redundant).

Inspired by DynTok[^Zhang2025DynTok] that used pre-trained image features for token compression, we reuse the extracted ASR feature to guide our merging process, shown in Figure [fig:mergingunmerging](#fig:mergingunmerging)(a).

Specifically, let the 12.5Hz ASR feature vectors be $\{e_s[1], e_s[2], \dots, e_s[T]\}$.

We first compute the cosine similarity between adjacent frames:
$
s_t = \cos\bigl(e_s[t],\,e_s[t+1]\bigr)\quad\text{for }t=1,\dots,T-1.
$
We then scan from left to right to form maximal contiguous segments \([i,\,i{+}1,\dots, j]\) whose adjacent similarities exceed a threshold \(\tau\):
$
\min_{t=i}^{j-1} s_t \;\ge\;\tau.
$
All frames in such a segment are merged into a single frame by averaging, applied to both the semantic and acoustic streams:
\vspace{-2mm}
$$\tilde e_s[k] \;=\; \frac{1}{\ell_k}\sum_{t=i}^{j} e_s[t],\quad \tilde e_a[k] \;=\; \frac{1}{\ell_k}\sum_{t=i}^{j} e_a[t],
\quad \ell_k \;=\; j - i + 1.$$
\vspace{-5mm}

where \(\tilde e_s[k]\) is the $k$-th entry in the compressed frame sequence in the semantic stream, and \(\tilde e_a[k]\) is the sequence in the acoustic stream.

We also record
the length of each merged frame \(\ell_k\)
as attributes, which are required to reconstruct the original fix-frame-rate sequence.

We find that directly using the average-reduced sequence
can degrade the naturalness of the reconstructed audio, particularly causing unnatural transitions between merged frames.

To address this, we adopt a transformer with local windowed attention that processes an interleaved sequence of original and averaged frames (Fig.[fig:mergingunmerging](#fig:mergingunmerging)(a))[^Yang2025ALMTokenizer], [^Li2023Blip-2].

This allows the merged tokens to query their adjacent context and produce refined, context-aware representations.

A local attention is favored over global attention because it allows generalization to longer audios despite trained on fixed-length audio segments.

**Frame Rate Flexibility**\quad
FlexiCodec\ trains with a flexible merging threshold $\tau$, sampled across a range.

This enables the model to support controllable frame rates at inference by adjusting $\tau$.

At $\tau=1.0$ (no frame merging), FlexiCodec functions as a 12.5Hz fixed-frame-rate codec.

At $\tau < 1.0$ (with frame merging), the average frame rate of the new sequence is lower than 12.5Hz, and is calculated as:
$\frac{\text{Total number of frames after merging}}{\text{Audio duration in seconds}}$.

Setting a lower $\tau$ decreases the average frame rate.

**Semantic (RVQ-1) Quantization**\quad
The dynamic-rate ASR features are quantized using a Finite Scalar Quantizer (FSQ)[^Mentzer2023Finite] to produce discrete semantic tokens (we denote as RVQ-1 tokens).

FSQ projects the input representations $e_s$ into a $D$-dimensional low-rank space, where the value of each dimension is quantized using rounding operation ROUND into $L$ levels.

The quantized low-rank vector $\bar{e}$ is subsequently projected back to the original dimension $\bar{e}$:
$$
\bar{e_s} = \mathrm{ROUND}\bigl(\mathrm{Proj}_{\text{down}}(e_s)\bigr),
\quad
\hat{e_s} = \mathrm{Proj}_{\text{up}}(\bar{e_s}).
$$
The semantic token index $q_s$ is obtained by:
$q_s = \sum_{j=0}^{D-1}\bar{e_s}L^j$.

Additionally, we wrap the FSQ block with small ConvNeXt[^Liu2022Convnet] blocks to increase its representation ability, and apply an L2 loss $L_\text{feat}$ to align the semantic token embeddings with the unquantized semantic features.

**Acoustic (RVQ-rest) Quantization**\quad
To encode the detailed acoustic information that is not captured in the semantic tokens, following[^Li2025DualCodec],
we compute a residual by subtracting the dynamic-rate ASR feature from the dynamic-rate waveform feature.

This residual is then quantized using an $(N-1)$-layer Residual Vector Quantization (RVQ)[^Zeghidour2021Soundstream]\footnote{We have not applied FSQ for acoustic quantization because FSQ
is a single-layer quantization,
and we have not discovered a multi-layer FSQ practice in literature.} module to produce acoustic tokens $q_{2:N}\in Z^{(N-1)\times\hat{T}}$, where $\hat{T}$ denotes the sequence length after frame merging.

We employ quantizer dropout[^Zeghidour2021Soundstream] during training.

That is, only the RVQ-1 to RVQ-$n$
layers are subsequently decoded, where $n \in [1, N]$ is randomly chosen.

When $n=1$, only the semantic encoding stream is used.

**Frame Unmerging and Reconstruction**\quad
To reconstruct the audio, the decoder path should reverse the dynamic compression\footnote{This is because the NAR codec decoder can only receive fixed-frame-rate sequence.

It is also possible to generate audios directly from dynamic-rate tokens using an AR model, and we leave this as future work.}.

The embedding features from the first $n$ chosen RVQ tokens are added to form a decoding feature representation.

This dynamic-rate sequence is then passed to the Frame Unmerging Module, detailed in Figure [fig:mergingunmerging](#fig:mergingunmerging)(b).

It uses the frame length attributes to expand the sequence back to 12.5Hz.

It is followed by another Transformer with local attention to smooth the transitions and refine the feature representations.

The resulting feature sequence is then fed into a convolutional codec decoder which mirrors the codec encoder, synthesizing the output waveform.

FlexiCodec is trained end-to-end with a composite loss function:

$$
\mathcal{L} = \mathcal{L}_{\text{recon}} + \lambda_{\text{GAN}}\mathcal{L}_{\text{GAN}} + \lambda_{\text{RVQ}}\mathcal{L}_{\text{RVQ}} + \lambda_{\text{feat}}\mathcal{L}_{\text{feat}},
$$

where $\mathcal{L}_{\text{recon}}$ is a multi-scale L1 mel spectrogram reconstruction loss following[^Kumar2024High-Fidelity], $\mathcal{L}_{\text{GAN}}$ contains adversarial and feature matching losses for Multi-Period Discriminator (MPD)[^Kong2020Hifi-Gan] and Multi-Resolution Spectrogram Discriminator (MRSD)[^Kumar2024High-Fidelity], $\mathcal{L}_{\text{RVQ}}$ involves a L1 codebook update loss
and a commitment loss for RVQ, whereas the FSQ module does not require a training loss.
$\mathcal{L}_\text{feat}$ is the L2 feature alignment loss between the RVQ-1 semantic token embeddings and the unquantized semantic features.

## 4·Experiments

### Experimental Setup

\paragraph{Codec Training Configuration}

We use the 16kHz, 54k-hour Librilight-Large[^Kahn2020Libri-Light] dataset for training.

Each model is trained for 800k steps on 8 Nvidia V100 32GB GPUs.

We use a batch size of 5 samples per GPU; each sample is a 5 second audio segment.

We use AdamW[^Loshchilov2017Decoupled] optimizer with $\text{lr=}1\times10^{-4}$, $\text{betas=}(0.8, 0.99)$; exponential learning rate with gamma=$0.999998$.

In each step, the merging threshold $\tau$ is randomly chosen from $0.7\le\tau\le1.0$.

We set the maximum frame length $\ell_k$ to $8$ so that each $\ell_k$ would take at most $log_28=3$ bits of storage.

Each token in the local attention transformer can attend to $\ell_k=8$ tokens left and right.

The FSQ module has $D=5$ dimensions each quantized to $L=8$ levels, resulting in $8^5=32768$ codebook entries.

The RVQ-rest quantization has 24 RVQ layers, 4096 codebook entries per layer, and 512-dimensional codebook entries.

FlexiCodec has 216M trainable parameters, in which the two frame merging modules each is 20M, and the frame unmerging module is 100M.

\paragraph{Codec Evaluation Metrics}

We evaluate on the 4 to 10 second subset of LibriSpeech-test-clean[^Panayotov2015Librispeech], comprising 1,088 audios.

We evaluate one FlexiCodec on three frame rates: 12.5Hz (80ms hop size), 8.3Hz (120ms hop size), and 6.25Hz (160ms hop size).

Since FlexiCodec has dynamic, controllable frame rate,
we configure its $\tau=1.0, 0.91, 0.867$ for 12.5Hz, 8.3Hz, and 6.25Hz average frame rate, respectively.

These $\tau$ settings have been determined based on trial runs on our test set.

Our evaluations consist of the following semantic and acoustic testings:

$\bullet$ **Semantic testing:**

To evaluate the preservation of semantic content, we transcribe the codec reconstructed audio using a Hubert-Large-LS960-ft[^Hsu2021Hubert]

ASR model and compute the word error rate (WER) against ground truth transcriptions.

The testings include using RVQ-1 alone, and RVQ-1:8 (1 to 8 layers) tokens.

Specifically, RVQ-1 tokens' semantic preservation relates to downstream model performance, where AR LMs only access RVQ-1 tokens[^Li2025DualCodec], [^Zhang2023Speechtokenizer].

Evaluating RVQ-1:8 is a common choice of RVQ-based codecs, implying the acoustic compression quality[^D{\'e}fossez2024Moshi], [^Zhang2023Speechtokenizer].

$\bullet$ **Acoustic testing:**

We evaluate the reconstructed audio from RVQ1:8 tokens against the original audio.

Metrics include the Perceptual Evaluation of Speech Quality (PESQ, narrow band) [^Rix2001Perceptual],
Mel Cepstral Distortion (MCD) [^Kubichek1993Mel-Cepstral],
speaker similarity (SIM, the cosine similarity between speaker embeddings extracted from a WavLM-large-based[^Chen2022Wavlm] speaker verification model),
and a speech perceptual quality score from a neural Mean Opinion Score (MOS) predictor UTMOS[^Saeki2022Utmos].

$\bullet$ **Additional semantic testing with ASR probing:**\quad
Additionally, to evaluate the semantic alignment between FlexiCodec semantic tokens and text, we employ an \text{ASR probing task}[^Gong2025XY-Tokenizer], [^Zhang2023Speechtokenizer], adapted from XARES[^Zhang2025X-Ares] benchmark.

We train a downstream Qwen2.5-0.5B-based[^Yang2025Qwen2.5]

ASR model which is tasked with predicting the text transcription given FlexiCodec's RVQ-1 token embeddings.

During
training, only the parameters of an MLP adapter
are updated.

Models are trained on LibriSpeech train-clean-100 using cross entropy loss.

We evaluate the ASR
WER on LibriSpeech test-clean.

The task's upper bound result is obtained by training with the unquantized SenseVoice-Small encoder feature.

Due to the prolonged evaluation time of this task, we only conduct this task as an extended semantic testing in Section~[sec:ablation_dynamic_frame_rate](#sec:ablation_dynamic_frame_rate).

![](visualizations/frame_rate_comparison_plots1.pdf)

<a id="fig:very_low_frame_rate">Evaluation results on three very low frame rates.
Each baseline system has been retrained for each target frame rate using the same recipe as FlexiCodec.</a>

### Examining the Impact of Very Low Frame Rates

\label{sec:exp_very_low_frame_rates}

We first investigate the performance of representative audio codecs at very low frame rates.

Since open-source baselines operating below 12.5Hz are unavailable, we created three new baseline versions by retraining DAC[^Kumar2024High-Fidelity] and DualCodec[^Li2025DualCodec] to operate at 12.5Hz, 8.3Hz, and 6.25Hz, respectively.

To adapt these systems for lower frame rates,
we increased their encoder downsampling rates and enlarged their codebook sizes to be consistent with FlexiCodec.

Detailed configurations are provided in Appendix~[sec:appendix_more_information_retrained_baselines](#sec:appendix_more_information_retrained_baselines).

Our findings are as follows.

$\bullet$ **RVQ-1 tokens' semantic information preservation is challenging for very low frame rate codecs.\quad**

As shown in Figure~[fig:very_low_frame_rate](#fig:very_low_frame_rate)(a),
we see that the Word Error Rate (WER), evaluated on audios reconstructed from RVQ-1 tokens, increases substantially for DAC and DualCodec when the frame rate drops from 12.5Hz to 6.25Hz.

At 12.5Hz, DualCodec achieves a 5.93\% WER, close to the ground truth (GT) 2.1\%.

However, as its frame rate drops to 6.25Hz, its gap with GT significantly increases to 31.5\% vs. 2.1\%, indicating its **lower resolution tokens fail to capture the whole phonetic details.**

Note that DualCodec architecture already improves WER over DAC thanks to its semantic augmentation using SSL features.

Figure~[fig:very_low_frame_rate](#fig:very_low_frame_rate)(b) confirms this trend, though the performance gap is smaller when using more RVQ layers.

$\bullet$ **FlexiCodec excels at semantic preservation, especially at the lowest frame rates.**

In contrast, FlexiCodec maintains a low WER across all rates.

At the most challenging 6.25Hz average frame rate, FlexiCodec achieves 4.15\% WER which is close to ground truth (2.1\%) and outperforms the best baseline (31.5\%).

To explain FlexiCodec's superior semantic information retention, our further experiments suggest it is contributed by a combination of our proposed design choices, including the ASR-assisted encoding architecture, dynamic frame rate merging, FSQ, and Transformer modules.

These modules are ablated in Appendix~[sec:appendix_ablation_study](#sec:appendix_ablation_study).

We have found that a simple switching from DualCodec's SSL into FlexiCodec ASR feature is very useful and it achieves an RVQ1 WER of 6.0\% at 6.25Hz.

Further improvements are particularly driven by the dynamic frame rate strategy, which we will detail in Section~[sec:ablation_dynamic_frame_rate](#sec:ablation_dynamic_frame_rate).

$\bullet$ **Acoustic quality metrics show more moderate differences across systems and frame rates.**

As shown in Figure~[fig:very_low_frame_rate](#fig:very_low_frame_rate)(c-e), the PESQ, MCD, SIM, and UTMOS metrics show that at each frame rate, FlexiCodec slightly outperforms DualCodec, followed by a larger gap with DAC.

We observe that
the gap between models does not substantially increase when changing to lower frame rates, but has been seen in the semantic testing.

We think that the acoustic fidelity is more constrained by bitrate, and because the bitrates of the three systems are at the same level, the difference is not pronounced.

### Analysis and Ablation of Dynamic Frame Rate

\label{sec:ablation_dynamic_frame_rate}

![](visualizations/phoneme_d2codec_correlation.pdf)

<a id="fig:corr">Correlation between Flexi-Codec frame rate and phoneme rate at a fixed frame merging threshold $\tau$.
Each data point is an audio in TIMIT dataset, representing the audio's average phoneme rate vs. average FlexiCodec frame rate.
% It shows that FlexiCodec uses more frames for faster speaking utterance, vice versa.
% Linear regression shows a pearson correlation $r=0.77$.</a>

To understand the mechanism and performance of dynamic frame rate, we conduct a series of analyses and ablations shown in Figure~[fig:corr](#fig:corr) and Table~[tab:tau](#tab:tau)-[tab:frame_rate_ablations](#tab:frame_rate_ablations).

Our findings are as follows.

$\bullet$ **Dynamic frame merging effectively adapts to the underlying phonetic complexity of speech.**

Figure~[fig:corr](#fig:corr) shows a strong positive correlation (Pearson $r=0.775$) between the utterance-level phoneme rate and FlexiCodec's frame rate on the TIMIT[^Garofolo1993Timit] subset.

This demonstrates that FlexiCodec dynamically adjusts its token frame allocation in proportion to the phonetic density of the audio, assigning more frames to segments with faster speech and fewer frames to slower or silence regions.

Such adaptivity enables efficient semantic compression by aligning token rate with semantic information density.

The fitted line also shows a linear coefficient very close to $0.5$, indicating that each merged frame approximately encodes two phonemes.

We visualize several cases by aligning phonemes labels with FlexiCodec frames, which is shown in Appendix~[appendix_sec:visualizaton](#appendix_sec:visualizaton).

It confirms that typical merged tokens include syllables/short words, long vowels, and silence.

$\bullet$ **Dynamic frame rate enables controllable frame rate as low as 3Hz.**\quad
As shown in Table~[tab:tau](#tab:tau), the merging threshold $\tau$ controls frame rate and semantic preservation trade-off.

Notably, by controlling $\tau$ values at inference time, we can obtain different output sequence lengths,
which is a unique feature compared to conventional fixed-frame-rate codecs.

$\bullet$ **Dynamic frame rate improves semantic information preservation.**\quad
Tables~[tab:semantic_test](#tab:semantic_test) and [tab:frame_rate_ablations](#tab:frame_rate_ablations) compare FlexiCodec and its variants that are retrained with fixed frame rate (FFR).

To obtain the FFR variants, we modify their codec encoder strides to output static 8.3Hz or 6.25Hz, and maintain the total parameter count (detailed in Appendix~[sec:appendix_more_information_retrained_baselines](#sec:appendix_more_information_retrained_baselines)).

Table~[tab:semantic_test](#tab:semantic_test) shows that the FFR variants have consistently worse WERs than FlexiCodec.

The gap is larger at the lower frame rate.

Specifically, the 6.25Hz FFR variant increases RVQ-1 WER by a relative 26\%, the ASR probing WER by 21\%, and the RVQ1:8 WER by 8\%.

These results highlight that dynamic frame rate improves semantic information preservation by a mechanism of phonetic complexity-adaptive frame rate allocation.

We also note that dynamic frame rate may
perform even better on real-world data, which tend to have longer silence regions that are highly compressible.

We leave this investigation as future work.

In terms of acoustic metrics, Table~[tab:frame_rate_ablations](#tab:frame_rate_ablations) shows that PESQ and UTMOS are the same between FlexiCodec and its FFR variant, while a moderate degradation in MCD and SIM is observed in FFR.

This indicates that dynamic frame rate mainly boosts semantic preservation rather than low-level acoustic quality.

One possible reason is that the acoustic information density in a speech can be misaligned with the semantic information density.

For example, semantically unimportant segments may still contain acoustic details like noise, sound and music.

### Comparison with Open-Source Codecs at Various Bitrates

In this experiment, we compare FlexiCodec with open-source neural audio codecs spanning various bitrate
and frame rates.

Information about the baseline codecs are provided in Appendix~[sec:appendix_more_information_baseline_codec](#sec:appendix_more_information_baseline_codec).

We categorize them into 3 bitrate classes, and
FlexiCodec at 12.5Hz, 8.3Hz, and 6.25Hz average frame rates fall into each category, enabling bitrate-consistent comparisons.

Table~[tab:tokenizer-rec-res](#tab:tokenizer-rec-res) presents the results.

<a id="tab:tokenizer-rec-res">\text{Comparison between FlexiCodec and other open-source neural audio codecs.}
\vspace{-4mm}</a>

$\bullet$ **FlexiCodec has state-of-the-art audio quality at various bitrate levels.**\quad
Examining the acoustic test scores, at $>$1kbps, FlexiCodec at 12.5Hz (1.2kbps)  achieves higher acoustic scores than its 12.5Hz counterparts Mimi, XYTokenizer and DualCodec.

It only trails behind DAC which uses a higher bitate (6kbps).

At 0.8kbps and 0.6kbps,
FlexiCodec also demonstrates superior acoustic quality scores than baselines.

These results demonstrate that
FlexiCodec has a high bitrate efficiency.

$\bullet$ **FlexiCodec is competitive to higher frame rate systems at semantic information preservation.**\quad
Across the WER semantic test metrics at both RVQ1 and RVQ1:8 quantization levels,
FlexiCodec consistently achieves competitive or better scores compared to other systems operating at higher frame rates.

Notably,
FlexiCodec at 6.25Hz attains an RVQ1 WER of 4.15,
outperforming larger frame rate models such as SpeechTokenizer-50Hz, Encodec-75Hz, WavTokenizer-75Hz, etc.

\vspace{-1mm}

## 5·More Experiments

$\bullet$ **Downstream TTS experiments.**\quad
In Appendix~[sec:appendix_tts](#sec:appendix_tts),
we detail our low-frame-rate TTS system with FlexiCodec (FlexiCodec-TTS).

Our conclusions are (1) FlexiCodec-TTS achieves competitive performance with significant speedups over baselines, and
(2) a higher NAR frame rate is important for high audio quality, but using lower frame rates for AR does not necessarily degrade performance.

$\bullet$ **Downstream audio understanding experiments.**\quad
In Appendix~[sec:appendix_audio_understanding](#sec:appendix_audio_understanding), we show that FlexiCodec semantic token embeddings surpass other codecs in downstream audio understanding tasks, highlighting the potential to use FlexiCodec in unified multimodal understanding and generation frameworks.

$\bullet$ **Ablation study on other components of FlexiCodec.**\quad
In Appendix~[sec:appendix_ablation_study](#sec:appendix_ablation_study), we confirm our other design choices like utilizing an ASR feature, transformer-based frame merging and unmerging modules, and FSQ quantization are beneficial to our codec's semantic and acoustic performance.

\vspace{-1mm}

## 6·Conclusion

In this work, we introduced FlexiCodec, a novel neural audio codec designed for very low frame rate operation.

By incorporating a dynamic frame rate, an ASR-assisted dual-stream architecture, and Transformer-based frame merging/unmerging modules, FlexiCodec effectively preserves semantic information at low rates.

Experiments confirm its strong performance in low frame rate and low bitrate speech coding, controllable frame rate, and effectiveness in a downstream TTS system.

Limitaion and future work are discussed in Appendix~[sec:appendix_limitation](#sec:appendix_limitation).

## 7·Reproducibility Statement

To ensure the reproducibility of our work, we provide comprehensive details on methodology, training configurations, and evaluation setup throughout the paper and appendices.

All datasets used, including Librilight-Large, LibriSpeech, and TIMIT, are publicly available.

The source code for FlexiCodec, along with scripts for data processing, training, and evaluation, is provided as supplementary material. **We will release our code under an open-source license after the review period.**

## 8·Appendix

\appendix

![](visualizations/binder_merge_pattern1.pdf)

<a id="fig:appendix_merge_patern_visualization">Visualizations of FlexiCodec dynamic-rate tokens aligned with TIMIT dataset phonemes.

We visualize  four random utterances of different speakers speaking the same transcript "She had your dark suit in greasy wash water all year." The inferred content of each merged token is labeled in green font.</a>

## 9·Visualization of Frame Merging Patterns

\label{appendix_sec:visualizaton}

Figure~[fig:appendix_merge_patern_visualization](#fig:appendix_merge_patern_visualization) presents visualizations of FlexiCodec tokens aligned with the TIMIT[^Garofolo1993Timit] dataset phonemes.

We show four randomly selected utterances from different speakers, each speaking the same content.

For each utterance, the inferred content of merged tokens is labeled in green above the token sequence.

Our discussions are as follows.

$\bullet$ **Typical merged frames correspond to phonemically coherent units.**\quad
Frames forming syllables, short or common words (e.g., “su,” “your,” “all”), long vowels (e.g., “/aa/” in "dark”, "water”), and silence intervals ([SIL]) are frequently merged to form single tokens spanning multiple original frame indices.

$\bullet$ **Merging patterns exhibit strong consistency across different speakers for the same utterance.**\quad
Although the four speakers differ in voice quality and prosody, the dynamic tokenization captures similar merging structures.

For example,
phonetic patterns of "she'', "had'', "su'', "grea'', "year'' and the trailing silence are common to all four utterances.

$\bullet$ **The deterministic nature of merging allows interpretability and reproducibility.**\quad
Since merging boundaries are derived from pretrained ASR features without additional trainable parameters, the token boundaries are replicable and interpretable.

In this work, we reused the same ASR feature for both encoding semantic tokens and to derive the merging boundaries.

We leave the exploration of more features as future works.

## 10·Downstream Experiment: TTS

\label{sec:appendix_tts}

![](appendix_visualizations/tts_arch.pdf)

<a id="fig:tts_architecture">Overview of FlexiCodec-TTS.
(a) The AR LM has two lightweight prediction heads to predict FlexiCodec RVQ-1 token and corresponding frame lengths.
(b) The NAR Stage predicts continuous features conditioned on stage 1 outputs.

We experiment with 2 types of continuous features, \textcircled{1} 50Hz Mel Spectrogram, and \textcircled{2} 12.5Hz FlexiCodec feature.</a>

### TTS Model Architecture

We integrate FlexiCodec\ within an AR+NAR(Flow Matching) TTS framework.

Our model architecture is
inspired by CosyVoice[^Du2024CosyVoice].

Modifications have been made in the AR and NAR stages mainly because FlexiCodec produces dynamic frame rate tokens.

\paragraph{AR Stage}

As shown in Figure~[fig:tts_architecture](#fig:tts_architecture)(a), this is a Transformer AR model that predicts FlexiCodec RVQ1 tokens.

Different from previous systems, our model has two token prediction heads instead of one.

This is because each FlexiCodec\ token includes an explicit "frame length" attribute.

Therefore, we treat **duration modeling as an auxiliary classification task** by adding a second prediction head after the last hidden layer.

Each prediction head is a lightweight linear projector.

The LM input sequence is:
\[
\bigl[\circled{S}, \{{\mathbf{y}}_u\}_{u \in [1:U]}, \circled{M}, \{\text{Embed}({q}^{(s)}_t)+\text{Embed}(\mathbf{\ell}_{t-1})\}_{t \in [1:\hat{T}]}, \circled{E}\bigr]
\]
where $\circled{S}$, $\circled{M}$ and $\circled{E}$ denote the start, mid, and end separators. $\{{\mathbf{y}}_u\}_{u \in [1:U]}$ denotes the embedded phoneme sequence, $q_t^{(s)}$ is the  $t$-th FlexiCodec RVQ1 token, and $\ell_{t-1}$ is the frame length of the $t-1$-th RVQ1 token.

For training, we employ next-token prediction with two parallel classification heads attached to the LM's last hidden state, to predict both $q_t^{(s)}$ and $\ell_{t-1}$.

The composite loss $\mathcal{L}$ is calculated as $\mathcal{L} = \mathcal{L}_{\text{sem}} + \lambda\mathcal{L}_{\text{dur}}$, where:
\[
\mathcal{L}_{\text{sem}} = -\sum_{t=1}^{\hat T} \log P\bigl(q^{(s)}_t \mid p_{1:L}, q^{(s)}_{1:t-1}\bigr),
\quad
\mathcal{L}_{\text{dur}} = -\sum_{t=1}^{\hat T} \log P\bigl(\ell_t - 1 \mid p_{1:L}, q^{(s)}_{1:t-1}\bigr).
\]
During inference, the model is prefixed with the phoneme sequence and a prompt speech's RVQ1 (tokens and frame lengths).

The remaining tokens and frame lengths are sampled autoregressively.

\paragraph{Frame Rate Flexibility in AR stage}

To enhance flexibility, the AR model is trained on sequences with either 12.5Hz, 8.3Hz, or 6.25Hz average frame rate, by setting various $\tau$ values when obtaining the RVQ1 tokens.

To distinguish these sequences, a dedicated control token is prepended to the first audio token.

During inference, the user can select from one of the three control tokens to specify an output frame rate.

\paragraph{NAR Stage}

In this stage, we aim to transform the predicted RVQ-1 sequence into audios with acoustic details.

While previous works like VALL-E[^Wang2023Neural] and SoundStorm[^Borsos2023Soundstorm] have used NAR models to predict RVQ codec tokens,
we have opted for predicting continuous features because low frame rate discrete tokens' acoustic quality upper bound is not as good as high frame rate (previously shown in Table~[tab:tokenizer-rec-res](#tab:tokenizer-rec-res)).

Following other works[^Le2024Voicebox], [^Du2024CosyVoice], [^Zhang2025Vevo],
we employ
a conditional flow matching[^Lipman2022Flow].

This allows us to experiment with two acoustic features with different frame rates, (1) the 50Hz Mel spectrogram (100 Mel bands following[^Zhang2025Vevo]), and (2) the 12.5Hz FlexiCodec feature (the feature labeled in dark green in Figure~[fig:architecture](#fig:architecture)).

For (2), it is equivalent to predicting $n-1=23$ layers of RVQ-rest tokens simultaneously.

As shown in Figure~[fig:tts_architecture](#fig:tts_architecture), the model is a flow matching transformer.

During training, given a speech waveform $\mathbf{u}$
and its acoustic feature $\mathbf{y}_1$, we randomly select a prefix of $\mathbf{y}_1$, denoted as $\mathbf{y}_1^{ctx}$, and aim to reconstruct the other part (denoted as $\mathbf{y}_1^{mis}$) conditioned on $\mathbf{y}_1^{ctx}$ and the FlexiCodec RVQ-1 tokens.

To obtain the conditioning FlexiCodec RVQ-1 tokens,
we first expand the dynamic-frame-rate RVQ-1 sequence $q_{1:\hat{T}}^{(s)}$ into 12.5Hz fixed frame rate sequence $q_{1:T}^{(s)}$ using frame lengths $\ell_{1:\hat{T}}$, and then linearly interpolat it to match the frame rate of $\mathbf{y}_1$.

The rest of the formulation follows [^Zhang2025Vevo], [^Du2024CosyVoice], using a 300M Diffusion Transformer to approximate an optimal transport path between gaussian noise $\mathbf{y}_0$ and spectrogram $\mathbf{y}_1$.

Classifier-free guidance (CFG) is applied by randomly dropping conditions during training and combining unconditional and conditional predictions during inference.

During inference, we use 15 timesteps, and a CFG guidance strength of 1.5.

To decode mel spectrogram, we use a pretrained Vocos[^Siuzdak2023Vocos] vocoder from the Amphion[^Li2025Overview], [^Zhang2023Amphion] toolkit.

\paragraph{Frame Rate Flexibility in NAR stage}

The NAR model supports decoding from either 12.5Hz, 8.3Hz, or 6.25Hz FlexiCodec RVQ-1 sequence.

Its conditioning FlexiCodec RVQ-1 sequences are expanded from either 12.5Hz, 8.3Hz, or 6.25Hz average frame rate, by randomly choosing from corresponding $\tau$ values during training.

We do not add additional tokens to distinguish these sequences because we find it yields the same results.

### TTS Experiments

\paragraph{TTS Training Configuration}

We use the 16kHz, 50k-hour Libriheavy[^Kang2024Libriheavy] dataset for training.

Each model is trained for 600k steps on 8 Nvidia V100 32GB GPUs.

The AR model uses AdamW[^Loshchilov2017Decoupled] optimizer with $\text{lr=}1\times10^{-4}$, $\text{betas=}(0.5, 0.999)$;
batch size is 96 seconds per GPU; gradient accumulation step is 3.

The NAR models uses $\text{lr=}7.5\times10^{-5}$, $\text{betas=}(0.5, 0.999)$;
batch size is 24 seconds per GPU; gradient accumulation step is 3.

The AR model has 250M parameters; the NAR model has 300M parameters.

\paragraph{TTS Evaluation Metrics}

We use the E2TTS[^Eskimez2024E2] test suite\footnote{\url{https://github.com/microsoft/e2tts-test-suite}} to test our models.

Its test data consists of 1,132 (reference, test) pairs
from Librispeech-PC-test-clean[^Meister2023Librispeech-Pc].

We evaluate both objective and subjective metrics.

The objective metrics include word error rate (WER) and
speaker similarity (SIM-o).

We report the real-time factor (RTF) of each system tested on an RTX5000 GPU.

RTF is calculated as the ratio of the total processing time to the total duration of generated audios.

The WER metric uses a Hubert-Large-LS960-ft ASR system[^Hsu2021Hubert],
and the SIM-o calculates the cosine similarity of speech samples
of each test utterance against reference clips using  a WavLM-large-based speaker verification model.

For subjective evaluations, we evaluate Naturalness Mean Opinion Score (NMOS) and Quality Mean Opinion Score (QMOS).

For each MOS, six professional linguistic experts evaluate 10 audios per system.

The human evaluation instructions are attached in Appendix~[sec:appendix_human](#sec:appendix_human).

Baseline systems include E2TTS[^Eskimez2024E2], VALL-E[^Wang2023Neural], CosyVoice[^Du2024CosyVoice], and SparkTTS[^Wang2025Spark-TTS], FireRedTTS[^Guo2024FireRedTTS].

<a id="tab:appendix_tts_results">TTS evaluation results.

Systems with * are borrowed from official results.

Each FlexiCodec-TTS is configured to work on each frame rate option without retraining.

The actual average frame rate of FlexiCodec-TTS is calculated on all the generated audios during testing.</a>

\paragraph{TTS Evaluation Results}

We present the evaluation results in Table~[tab:appendix_tts_results](#tab:appendix_tts_results).

Our analysis is as follows:

$\bullet$ **FlexiCodec-TTS achieves competitive performance with significant speedups.**\quad
The FlexiCodec-TTS model with a 50Hz NAR decoder demonstrates highly competitive performance across both objective and subjective metrics.

At its 6.25Hz AR setting, it achieves a WER of 3.2\%, Naturalness MOS (NMOS) of 3.32 and a Quality MOS (QMOS) of 3.40, rivaling or exceeding strong baselines like CosyVoice (WER 3.2\%, NMOS 3.17, QMOS 3.32) and SparkTTS (WER 6.0\%, NMOS 3.15, QMOS 3.47).

Crucially, this high perceptual quality is achieved with a significant reduction in computational cost.

The 6.25Hz AR configuration has an RTF of just 0.07 for the autoregressive stage, representing a 7.3x speedup over CosyVoice’s AR model.

Adding the inference time of 50Hz NAR, the total speedup is also substantial, at 3.4x.

FlexiCodec-TTS using 12.5Hz NAR shows degraded performance, but still outperforms baselines like VALL-E and FireRedTTS, while achieving higher speedups.

This balance of speed and quality makes FlexiCodec-TTS highly suitable for low-latency, resource-constrained applications.

**$\bullet$ A higher NAR frame rate is important for high audio quality, but using lower frame rates for AR does not necessarily degrade performance.**\quad
A comparison between our two FlexiCodec-TTS configurations reveals the distinct roles of the AR and NAR stages.

Using a 50Hz NAR decoder consistently yields superior results over a 12.5Hz NAR across all metrics, including WER, SIM-o, NMOS, and QMOS.

This confirms that a higher temporal resolution in the acoustic decoding stage is vital for reconstructing fine-grained details and preserving naturalness.

Interestingly, for the AR stage, lower frame rates do not always degrade performance.

With 50Hz NAR, the 8.3Hz and 6.25Hz AR configurations achieve comparable or even slightly better WER (2.5 vs. 2.5 vs. 3.2), SIM-o (0.64 vs. 0.65 vs. 0.65), and MOS scores compared to 12.5Hz AR.

To explain this phenomenon, we suggest that (1) lower frame rate tokens might have better semantic and acoustic information disentanglement, and
(2) a shorter sequence may alleviate the challenge of modeling long-range dependencies, allowing the Transformer to learn more effective attention patterns.

## 11·Downstream Experiment: Audio Understanding

\label{sec:appendix_audio_understanding}

<a id="tab:audio_understanding">Evaluation on various audio understanding tasks.</a>

To evaluate the quality of the semantic tokens produced by FlexiCodec, and to validate whether FlexiCodec is suitable for building an audio language model, we test its RVQ-1 token embeddings on a diverse suite of 9 audio understanding tasks from the XARES benchmark[^Zhang2025X-Ares].

We compare against two types of representations: (1) continuous features from strong speech SSL or ASR models (Whisper[^Radford2023Robust], Dasheng[^Dinkel2024Scaling], and SenseVoice-Small[^An2024Funaudiollm]), and (2) RVQ-1 tokens from other audio codecs.

For each task, we train a simple downstream model consisting of a linear classifier on top of the frozen embeddings.

The results are presented in Table~[tab:audio_understanding](#tab:audio_understanding).

The results show that continuous features from large speech models generally outperform codec tokens, which is expected given their continuous nature.

Among the codec models, FlexiCodec demonstrates superior performance across most tasks.

Notably, on keyword spotting, FlexiCodec at 12.5Hz and 8.3Hz achieves a score of 0.99, matching the performance of the best continuous feature model, SenseVoice-Small, and significantly outperforming other codecs.

This highlights the rich semantic information captured in FlexiCodec's tokens.

Furthermore, FlexiCodec excels in emotion recognition, non-speech sound detection, and language identification, consistently surpassing other codec models.

Even at a very low average frame rate of 6.25Hz, FlexiCodec maintains strong performance, demonstrating its efficiency and effectiveness in preserving crucial semantic content.

These results validate that FlexiCodec's tokens are not only suitable for high-quality audio reconstruction but also serve as a powerful and compact representation for general audio understanding.

## 12·Ablation Study

\label{sec:appendix_ablation_study}

<a id="tab:extended_ablation">\text{Extended ablation study of FlexiCodec.}

We underline the results that are significantly degraded from the baseline.</a>

Table~[tab:extended_ablation](#tab:extended_ablation) summarizes our further ablation study of FlexiCodec, examining the impact of removing or modifying key components including the Frame Merging and Unmerging modules, quantization schemes, and semantic feature inputs.

Each ablated model is retrained following the recipe of FlexiCodec and are tested on 6.25Hz average frame rates.

Key observations are summarized below.

$\bullet$ **Transformers in Frame Merging and Unmerging Modules are critical for maintaining high acoustic quality.**\quad
Removing the Transformer refinement from either the frame merging or unmerging module leads to noticeable decreases in acoustic quality metrics (PESQ, UTMOS) and speaker similarity (SIM), as evidenced by the underlined values in rows B1 and C1.

However, the impact on the semantic test scores, especially RVQ-1 WER, is very small.

These results highlight that the Transformer's local attention especially refines acoustic features after compression and expansion, **reducing artifacts and unnatural transitions caused by naive averaging and frame repetition.**

$\bullet$ **Using an SSL feature does not yield a good performance in FlexiCodec, compared to ASR features.**

As shown in row E1, adopting the w2v-bert-2[^Barrault2023Seamless]

SSL feature (which worked in DualCodec) in our system shows degraded semantic and acoustic results, except that its SIM score improves.

We have examined the frame similarity pattern of this feature, and found that its adjacent frames tend to have low similarities, as it required to set a very low threshold $\tau\approx 0.5$ to reduce to a 6.25Hz average frame rate.

We hypothesize that the SSL feature contains leaked acoustic information like timbre, so the frame merging module may not effectively merge semantically consistent tokens together.

As a comparison, we also tried using Whisper-medium ASR (row E2) and layer-averaged Sensevoice-Small ASR (row E3).

Both perform better than SSL feature, suggesting that the semantic-rich ASR features are more suitable for dynamic-frame-rate codecs like FlexiCodec.

## 13·More Information of Baseline Codecs

\label{sec:appendix_more_information_baseline_codec}

### Retrained baselines

\label{sec:appendix_more_information_retrained_baselines}
$\bullet$ **DAC:**

It is a high-fidelity neural audio codec with open-source recipes.

It proposed two key improvements over Encodec.

First, it
addressed the codebook collapse problem by reducing the dimension of the latent vector to a small value for quantization.

Second, It replaced the ReLU activation function with a periodic activation function, offering benefits for reconstructing periodic signals such as speech and music.

It is trained on more than 10k hours of data including speech, audio and music.

$\bullet$ **DualCodec:**

It is a  low-frame-rate neural audio codec specifically designed to enhance speech generation tasks like TTS.

It introduced a dual-stream encoding architecture that processes semantic and acoustic information in parallel.

One stream uses a pre-trained SSL model w2v-bert-2[^Barrault2023Seamless] to extract rich semantic features.

The second stream encodes acoustic information directly from the waveform.

This dual approach ensures that the primary codec tokens are semantically meaningful and allows low frame rates (12.5Hz or 25Hz).

Its waveform encoding stream utilizes the architecture of DAC codec.

To retrain these two systems, we use their public repositories, and modify their codec encoder strides to achieve lower frame rates compared to their original versions.

The configurations are shown in Table~[tab:baseline_framerates](#tab:baseline_framerates).

Their decoders mirror their encoders.

We have also modified their codebook sizes to align with FlexiCodec.

Specifically, we modify to 32768 RVQ-1 codebook size for DualCodec instead of 16384, and use 4096 codebook size for DAC and DualCodec remaining RVQ layers.

To retrain fixed-frame-rate FlexiCodec baselines, we also follow Table~[tab:baseline_framerates](#tab:baseline_framerates) to modify their strides to achieve 8.3Hz and 6.25Hz frame rate, respectively.

<a id="tab:baseline_framerates">The CNN encoder strides settings corresponding to each frame rate configuration on retrained codec systems.</a>

### Other baselines

$\bullet$ **Encodec**[^D{\'e}fossez2022High]: A high-fidelity neural audio codec.

It introduced LSTM bottleneck layers and multi-scale STFT discriminator on top of SoundStream[^Zeghidour2021Soundstream].

$\bullet$ **SpeechTokenizer**[^Zhang2023Speechtokenizer]: A neural audio codec designed for speech language models.

It introduced  semantic distillation, where the first layer of its RVQ is trained to match semantic-rich representations from a pre-trained SSL model HuBERT.

$\bullet$ **Mimi**[^D{\'e}fossez2024Moshi]: A neural audio codec designed for real-time applications.

It compresses 24kHz speech into a 12.5 Hz token representation with a bitrate of 1.1 kbps.

It features a split RVQ system, where one quantizer captures semantic information and the remaining layers encode acoustic details.

It integrates Transformer layers in its bottleneck and distilling semantic knowledge from an SSL model.

$\bullet$ **WavTokenizer**[^Ji2024Wavtokenizer]: A codec with single-layer neural quantizer operating with a large FSQ codebook, optimized for universal speech processing and compression.

It is noted for its efficiency and ease of integration into generative frameworks like GPT-4o.

$\bullet$ **TS3Codec**[^Wu2024Ts3-Codec]: A transformer-based neural audio codec with FSQ bottleneck, designed for minimal bitrate speech encoding.

We acquire its inference audios from its audios.

Its two model offerings, X2 and X4, have been described in its paper.

$\bullet$ **SNAC**[^Siuzdak2024Snac]:
A codec that extends the standard Residual Vector Quantization (RVQ) framework by implementing quantizers operating at different temporal resolutions [^Siuzdak2024Snac].

Its coarse tokens are sampled at lower frequencies.

Each RVQ layer operates at 12Hz, 23Hz, 47Hz, respetively.

$\bullet$ **XYTokenizer**[^Gong2025XY-Tokenizer]:
A dual-channel 12.5Hz neural speech codec with parallel semantic and acoustic processing streams, operating at low bitrates.

The semantic encoder is initialized with frozen Whisper-small weights and includes adapter modules, while the acoustic encoder is trainable from scratch.

$\bullet$ **TaDiCodec**[^Wang2025TaDiCodec]:
A speech tokenizer with 6.25Hz tokens and text-assisted decoding, designed for text-to-speech synthesis.

TaDiCodec has a transformer-based encoder and a diffusion-based decoder.

Its diffusion decoder utilizes text transcriptions to enhance semantic; reference speech to enhance timbre.

During inference, we use the ground truth audio as reference speech, and use HuBERT-Large-LS960-ft[^Hsu2021Hubert] to obtain the text.

We calculate its bitrate as speech token bitrate + text token theoretical bitrate.

We infer each baseline system from its official checkpoint.

Each input audio has been resampled to match the codec's input frame rate.

We also note that Mimi, XY-Tokenizer and SNAC do not support decoding solely from RVQ-1 tokens, so their RVQ-1 semantic tests are left blank in Table~[tab:tokenizer-rec-res](#tab:tokenizer-rec-res).

## 14·More Information of FlexiCodec

\paragraph{Codec Encoder and Decoder}

The acoustic codec encoder in FlexiCodec follows DAC, which is also used by DualCodec.

It has a sequential series of 1D convolutional layers.

It begins with an initial convolution with a kernel size of 7, followed by a set of residual convolutional blocks.

Each block contains two dilated convolutions and a skip-connection, followed by a strided down-sampling layer.

As the signal is down-sampled, the number of channels is doubled.

A final 1D convolution sets the dimensionality of the output features.

The decoder mirrors this architecture, using transposed convolutions in place of strided convolutions to upsample the signal, and reconstructs the final audio waveform.

FlexiCodec has a codec encoder of 15M, and a codec decoder of 35M.

\paragraph{Transformer Configurations}

FlexiCodec has a 20M parameter transformer in each frame merging module, each has 6 layers, 512 intermediate dimensions, 2048 FFN dimension, and 8 attention heads.

The frame unmerging transformer has 100M parameters, with 32 layers, 2048 FFN dimension, and 8 attention heads.

The transformers have bidirectional attentions with rotary postional embeddings.

We also find that delaying the parameter updates of these transformers can help training stability, especially the unstable RVQ losses, so we have bypassed them (setting them as identity functions) in the initial 10\% training steps.

\paragraph{Loss Weights}

We use a weight of 15.0 for the multi-scale spectrogram loss, 2.0 for the GAN feature matching loss, 1.0 for the adversarial loss, 15.0 for the semantic feature alignment loss $L_\text{feat}$, 1.0 and 0.25 for the RVQ codebook and commitment losses, respectively.

The parameter breakdown of FlexiCodec is shown in Table~[tab:flexicodec_params](#tab:flexicodec_params).

Additionally, the discriminators have 54M parameters in total, but are discarded during inference.

<a id="tab:flexicodec_params">Number of parameters breakdown in FlexiCodec</a>
