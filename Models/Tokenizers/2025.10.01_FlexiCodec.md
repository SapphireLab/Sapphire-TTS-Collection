# FlexiCodec: A Dynamic Neural Audio Codec for Low Frame Rates

<details>
<summary>基本信息</summary>

- 标题: "FlexiCodec: A Dynamic Neural Audio Codec for Low Frame Rates."
- 作者:
  - 01 Jiaqi Li
  - 02 Yao Qian
  - 03 Yuxuan Hu
  - 04 Leying Zhang
  - 05 Xiaofei Wang
  - 06 Heng Lu
  - 07 Manthan Thakker
  - 08 Jinyu Li
  - 09 Shang Zhao
  - 10 Zhizheng Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2510.00981v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2510.00981v1](PDF/2025.10.01_2510.00981v1_FlexiCodec__A_Dynamic_Neural_Audio_Codec_for_Low_Frame_Rates.pdf)
  - [Publication] #TODO

</details>

## Abstract

Neural audio codecs are foundational to speech language models.
It is expected to have a low frame rate and decoupled semantic and acoustic information.
A lower frame rate codec can reduce the computational cost of speech language models by shortening the sequence length.
Recent studies have developed 12.5Hz low-frame-rate audio codecs, but even lower frame rate codecs remain underexplored.
We find that a major challenge for very low frame rate tokens is missing semantic information.
This paper introduces **FlexiCodec** to address this limitation.
FlexiCodec improves semantic preservation with a **dynamic frame rate** approach and introduces a novel architecture featuring an **ASR feature-assisted dual stream** encoding and Transformer bottlenecks.
With dynamic frame rates, it uses less frames at information-sparse regions through adaptively merging semantically similar frames.
A dynamic frame rate also allows FlexiCodec to support inference-time **controllable frame rates** between 3Hz and 12.5Hz.
Experiments on **6.25Hz, 8.3Hz and 12.5Hz** average frame rates confirm that FlexiCodec excels over baseline systems in semantic information preservation and delivers a high audio reconstruction quality.
We also validate the effectiveness of FlexiCodec in language model-based TTS.
Demos are available at: \url{https://flexicodec.github.io}.

## 1·Introduction

The neural audio codec technique, originally designed for waveform compression[^Zeghidour2021Soundstream], [^D{\'e}fossez2022High], is now widely used in various tasks including Language Model (LM)-based text-to-speech (TTS)[^Wang2023Neural], [^Li2024Investigating], [^Wang2024Maskgct], [^Du2024CosyVoice], [^Wang2025Spark-TTS], [^Guo2024FireRedTTS] and multimodal LLMs[^D{\'e}fossez2024Moshi], [^Zeng2024GLM-4-Voice], [^Ding2025Kimi-Audio].

By compressing raw speech into compact discrete tokens, neural audio codecs enable the application of auto-regressive LLM paradigms to the speech domain.

The standard neural audio codec follows an encoder-quantizer-decoder architecture.

The encoder downsamples the audio waveform into a sequence of fixed-rate continuous latent vectors.

These vectors are then quantized into discrete indices using Residual Vector Quantization (RVQ), a multi-stage process where each quantizer encodes the residual error of the previous one.

In downstream tasks like TTS, the first RVQ layer’s tokens (RVQ-1) are often used to drive an autoregressive (AR) language model or an LLM, while the remaining layers (RVQ-rest) can be predicted by a non-autoregressive (NAR) model to add acoustic detail.

However, a fundamental challenge arises from the high temporal resolution of these codecs.

State-of-the-art models like EnCodec[^D{\'e}fossez2022High], DAC[^Kumar2024High-Fidelity], and SpeechTokenizer[^Zhang2023Speechtokenizer] operate at frame rates exceeding 50Hz, representing one second of audio with over 50 tokens.

This high density, compared to the $\sim$4.5Hz rate of typical text representations[^Wang2024Why], creates two major problems for AR models: (1) a significant computational burden due to the quadratic complexity of attention, and (2) a severe frame rate mismatch between text and audio modalities that may degrade LLM performance[^Wang2024Why].

<a id="tab:model_comparison_intro">Comparison of different audio tokenization methods and their properties.</a>

To mitigate this, recent work has focused on low-frame-rate codecs.

Methods like Mimi[^D{\'e}fossez2024Moshi] and DualCodec[^Li2025DualCodec] successfully reduced the frame rate to 12.5Hz by decoupling speech into two streams: a semantic stream (RVQ-1) derived from self-supervised learning (SSL) models[^Chen2022Wavlm], [^Barrault2023Seamless], and an acoustic stream (RVQ-rest) for residual details.

In this strategy, the low-rate RVQ-1 tokens encode the core semantic information, which is sufficient for many downstream AR models.

While previous works have proposed 12.5Hz solutions, a significant gap remains compared to the $\sim$4.5Hz frame rate of text.

Furthermore, research into neural audio codecs operating below 12.5Hz is limited.

Our initial experiments revealed that pushing to even lower frame rates leads to significant information loss, particularly the omission of semantic/phonetic content.

Driven by this observation, we have the following motivations for designing our **low frame rate** codec:
(1) **Dynamic Frame Rate**: A fixed low rate inevitably discards transient phonetic details, whereas a dynamic rate may adapt to the phonetic complexity to encode more details.
(2) **Richer Semantics**:
SSL features, trained for reconstruction, can be redundant.

Features from an ASR model, trained for text prediction, may offer a more concentrated source of semantic information.
(3) **Controllable Frame Rate**: Existing codecs typically operate at one or more fixed frame rates.

A continuously controllable rate would allow users to dynamically trade off performance and efficiency for downstream tasks.

In this work, we introduce FlexiCodec, a novel low-frame-rate codec built on three principles: dynamic frame rate, ASR-guided semantics, and frame rate controllability.

Instead of a fixed frame rate, FlexiCodec dynamically allocates temporal resolution, using more frames for complex phonetic segments and fewer for sparse regions like long vowels, syllables and silence.

This is achieved through a novel ASR-feature-assisted dual-stream architecture that adaptively merges semantically similar frames.

A key benefit of this dynamic approach is that a single model can support a continuous range of frame rates **(3-12.5Hz)** at inference, enabling flexible trade-offs for applications like adaptive signal transmission or variable-complexity TTS on edge devices.

Our contributions are:

-  We propose FlexiCodec\footnote{Reproduced models will be available at \url{https://github.com/amphionspace/flexicodec}}.

To our best knowledge, it is the **one of first neural audio codecs under 10Hz frame rate**, and the **first work to explore dynamic frame rate on low-frame-rate neural audio codecs**.

We develop a novel ASR feature-assisted dual stream codec architecture with Transformer bottlenecks.

-  We show that FlexiCodec outperforms open-source baselines in semantic intelligibility and acoustic quality.

Experiments confirm that our dynamic frame rate strategy improves semantic information preservation and allows for controllable frame rate as low as 3Hz.

Other design choices including utilizing an ASR encoder, transformer bottlenecks, and FSQ quantization, also contributes to our codec's performance.

-  We demonstrate FlexiCodec’s utility in a flexible TTS system.

The model yields competitive results at multiple frame rates and is substantially faster than existing methods.

## 2·Related Works

### Low-Frame-Rate Neural Audio Codecs

Neural audio codecs convert continuous speech into discrete tokens.

SoundStream[^Zeghidour2021Soundstream], Encodec[^D{\'e}fossez2022High], and DAC[^Kumar2024High-Fidelity] focused on audio compression, relying on residual vector quantization (RVQ)[^Zeghidour2021Soundstream] and operating at high bitrates ($\ge$4kbps) and high frame rates ($\ge$50Hz).

WavTokenizer[^Ji2024Wavtokenizer],
TS3-Codec[^Wu2024Ts3-Codec],
SemantiCodec[^Liu2024Semanticodec],
and StableCodec[^Parker2024Scaling] used a single VQ or FSQ[^Mentzer2023Finite] codebook.

They delivered good audio quality at low bitrates (around 1kbps), but operate at a high frame rates ($\ge$40Hz).

Some recent works develop low-frame-rate codecs.

A lower frame rate limits the information amount that can be carried by each RVQ layer tokens.

Thus,
some works[^D{\'e}fossez2024Moshi], [^Li2025DualCodec]
decompose the speech tokens into semantic (RVQ-1) and acoustic (RVQ-rest) tokens.

Mimi codec[^D{\'e}fossez2024Moshi] was based on Encodec with a higher downsampling rate in its convolutional encoder.

It employed semantic distillation[^Zhang2023Speechtokenizer], a technique that distills RVQ-1 embeddings from SSL features.

DualCodec[^Li2025DualCodec] proposed a dual-stream architecture where a semantic stream directly encodes SSL features into RVQ-1 tokens, and an acoustic stream encodes  RVQ-rest tokens.

ALMTokenizer[^Yang2025ALMTokenizer] proposed a query-based compression strategy using a set of learnable query tokens, and designed an auxiliary MAE loss inspired by SSL models.

Concurrent work XY-Tokenizer[^Gong2025XY-Tokenizer] is a 12.5Hz codec encoded with a concatenative dual-stream architecture consisting of Whisper ASR feature and waveform feature.

Recently, TaDiCodec[^Wang2025TaDiCodec] and TASTE[^Tseng2025Taste]  proposed $\le$10Hz speech tokens,
but operating like text-to-speech (TTS) systems,
they require the text transcription to assist audio synthesis.

Accurate text transcription can be unavailable in some audio coding scenarios.

By comparison, our work is more similar to a conventional codec and does not require transcriptions.

### Dynamic-Rate Compression of Images and Audios

The concept of dynamically adjusting token rates based on content complexity is an emerging trend across image and audio modalities.

In the image domain,
[^Bolya2022Token] first proposed Token Merging (ToMe), a technique to gradually merge most similar tokens in Vision Transformers (ViTs) to accelerate inference.

It used the cosine similarity between the self attention keys to guide the merging process.

DynTok[^Zhang2025DynTok] proposed an improved similarity calculation strategy based on CLIP[^Radford2021Learning] semantic representation.

In the audio domain, one research trend has been the syllable-level semantic unit discoveries which are inherently dynamic-rate.

SD-HuBERT[^Cho2024Sd-Hubert] finetuned HuBERT[^Hsu2021Hubert] with sentence-level self-distillation, and showed that its features distinguish syllable boundaries.

SylBoost[^Baade2024Syllablelm] can discover discrete syllabic units using a min-cut algorithm on the feature self-similarity matrix, followed by a k-means clustering.

It experimented on 8.33Hz and 6.25Hz units.

However, compared to neural audio codec tokens, syllable units only encode coarse semantic information, and
necessitate external speech synthesis models to produce audio.

Dynamic-rate neural audio codec is an emerging research area.
[^Dieleman2021Variable-Rate] proposed an audio VQ-VAE with run-length encoding, operating at an average frame rate of 75Hz.

SNAC[^Siuzdak2024Snac] created a multi-resolution codec stream at 12, 23, and 47Hz for each RVQ layer.

CodecSlime[^Wang2025CodecSlime] proposed a two-stage process to firstly train an 80Hz fixed-frame-rate codec, followed by merging similar features into 40Hz average-frame-rate tokens.

Similarly, TFC[^Zhang2025Unlocking] and VARSTok[^Zheng2025Say] introduced dynamic-frame-rate algorithms on 75Hz RVQ or single-codebook tokens.

Compared with previous works, ours explores more challenging low frame rates ($\leq$10Hz).

Compared with CodecSlime, TFC and VARSTok, our work uses a pre-trained audio semantic feature extractor to guide the merging process, simplifying the needs for multi-stage training or dynamic programming-based merging algorithms.

### Text-to-Speech Synthesis

Modern Text-to-Speech (TTS) has increasingly shifted from statistical parametric methods to systems based on neural audio codecs.

VALL-E[^Wang2023Neural] established an "AR+NAR'' framework: an AR language model generates codec RVQ-1 tokens
conditioned on text; an NAR
language model predicts remaining RVQ layer tokens in parallel.

The AR stage provides essential information for the NAR stage, and the AR stage is usually the most time-consuming part.

Subsequent works have evolved this paradigm, showing the benefits of semantic-rich RVQ-1 tokens in the AR stage[^Borsos2023Audiolm], [^Du2024CosyVoice], [^Zhang2025Vevo], and replacements of the NAR discrete token prediction with diffusion-based models[^Betker2023Better], [^Du2024CosyVoice].

The efficiency of a TTS system directly correlates to codec frame rate.

A lower frame rate codec can significantly boost training and inference speed.

A few works[^Du2024Cosyvoice], [^Deng2025Indextts] have adopted 25Hz frame rate tokens in the AR stage, others mostly rely on $\ge$50Hz tokens[^Guo2024FireRedTTS], [^Wang2025Spark-TTS].

For the NAR stage, these works mostly keep a frame rate $\ge$50Hz.

![](visualizations/flexicodec_arch3.pdf)

<a id="fig:architecture">\text{Overview of FlexiCodec.}

The model encodes speech through two streams.
% an ASR Encoder extracts high-level semantic features; a Codec Encoder captures low-level acoustic details.
The Frame Merging Modules dynamically reduce the 12.5Hz features into lower frame rates, and the Frame Unmerging Module restores a 12.5Hz fixed frame rate.

The model is trained end to end.
% frame rate based on semantic similarity computed from the ASR features.

The resulting dynamic-rate ASR features are quantized into semantic (RVQ-1) tokens through FSQ, while the residual acoustic features are quantized using RVQ.

For reconstruction, the Frame Unmerging Module restores the fixed 12.5Hz frame rate before the Codec Decoder synthesizes the final waveform.</a>

![](visualizations/mergingunmerging3.pdf)

<a id="fig:mergingunmerging">Detailed views of the Frame Merging Module and the Frame Unmerging Module.
% **(a) The Frame Merging Module** converts a fixed-rate sequence into a dynamic-rate one.

It first computes the cosine similarity between adjacent frames of the ASR features.

If the similarity exceeds a threshold $\tau$, the frames are marked for merging.
% The merged tokens are initially represented by averaged features and are subsequently refined by a Transformer model.
% % A Transformer with local attention processes an interleaved sequence of original and averaged feature`s to refine the representations before the final merged frames are retrieved.
% Each output frame is tagged with its original length.
% **(b) The Frame Unmerging Module** reverses this process.

It takes the dynamic-rate sequence and repeats each frame according to its stored length, restoring a 12.5Hz fixed-rate sequence.

A subsequent Transformer smooths the transitions between the repeated frames to produce the final unmerged feature sequence for the decoder.</a>

## 3·FlexiCodec: A Dynamic Low-Frame-Rate Neural Audio Codec

The core of our work is **FlexiCodec**, a neural audio codec designed to operate at very low average frame rates while preserving crucial semantic information.

Unlike traditional codecs that use a fixed frame rate, FlexiCodec employs a **dynamic frame rate** mechanism that allocates more temporal resolution to information-dense regions of speech and less to information-sparse segments like silence or long vowels.

This is achieved through an architecture that leverages **pre-trained ASR features to encode semantic-rich RVQ-1 tokens**, and to **guide the adaptive frame merging** process.

The overall architecture, depicted in Figure~[fig:architecture](#fig:architecture), follows a dual-stream encoding, dynamic frame merging, quantization, and frame unmerging decoding pipeline.

**Dual-Stream Feature Extraction**\quad
FlexiCodec\ begins by processing a 16kHz speech waveform through two parallel encoders to decouple semantic and acoustic information:
(1) An ASR Encoder, leveraging the pre-trained ASR model, extracts a sequence of semantic features $e_{s}\in R^{T\times d}$, where $T$ and $d$ denote the number of frames and the vector dimension.

And
(2) a convolutional codec encoder, downsampling the waveform to produce a sequence of waveform features $e_{a}\in R^{T\times d}$, also at a 12.5Hz frame rate.

The codec encoder consists of 5 CNN blocks with strides [4,4,5,8,2] to gradually downsample the audio, giving $16000\text{Hz} \div (4 \times 4 \times 5 \times 8 \times 2) = 12.5\text{Hz}$.

Each CNN block contains a strided 1D convolution followed by a ResNet[^He2016Deep].

For the specific ASR model choice, we use pretrained SenseVoice-Small[^An2024Funaudiollm], a 230M-parameter, encoder-only Transformer model trained on 300k hours of data
with CTC loss.

We use its last layer (excluding its CTC logits prediction layers) hidden state  as the semantic feature.

The model outputs 16.67Hz features; we downsample it to 12.5Hz using linear interpolation to align the frame rate with the acoustic stream.

The ASR model is frozen during codec training.

**Dynamic Frame Merging**\quad
This module compresses the number of frames in a sequence, resulting in a sequence where some segments have lower than 12.5Hz frame rate.

Our goal is to compress the frames with a minimal loss of semantic information
by preferentially merging frames that are semantically similar (redundant).

Inspired by DynTok[^Zhang2025DynTok] that used pre-trained image features for token compression, we reuse the extracted ASR feature to guide our merging process, shown in Figure [fig:mergingunmerging](#fig:mergingunmerging)(a).

Specifically, let the 12.5Hz ASR feature vectors be $\{e_s[1], e_s[2], \dots, e_s[T]\}$.

We first compute the cosine similarity between adjacent frames:
$
s_t = \cos\bigl(e_s[t],\,e_s[t+1]\bigr)\quad\text{for }t=1,\dots,T-1.
$
We then scan from left to right to form maximal contiguous segments \([i,\,i{+}1,\dots, j]\) whose adjacent similarities exceed a threshold \(\tau\):
$
\min_{t=i}^{j-1} s_t \;\ge\;\tau.
$
All frames in such a segment are merged into a single frame by averaging, applied to both the semantic and acoustic streams:
\vspace{-2mm}
$$\tilde e_s[k] \;=\; \frac{1}{\ell_k}\sum_{t=i}^{j} e_s[t],\quad \tilde e_a[k] \;=\; \frac{1}{\ell_k}\sum_{t=i}^{j} e_a[t],
\quad \ell_k \;=\; j - i + 1.$$
\vspace{-5mm}

where \(\tilde e_s[k]\) is the $k$-th entry in the compressed frame sequence in the semantic stream, and \(\tilde e_a[k]\) is the sequence in the acoustic stream.

We also record
the length of each merged frame \(\ell_k\)
as attributes, which are required to reconstruct the original fix-frame-rate sequence.

We find that directly using the average-reduced sequence
can degrade the naturalness of the reconstructed audio, particularly causing unnatural transitions between merged frames.

To address this, we adopt a transformer with local windowed attention that processes an interleaved sequence of original and averaged frames (Fig.[fig:mergingunmerging](#fig:mergingunmerging)(a))[^Yang2025ALMTokenizer], [^Li2023Blip-2].

This allows the merged tokens to query their adjacent context and produce refined, context-aware representations.

A local attention is favored over global attention because it allows generalization to longer audios despite trained on fixed-length audio segments.

**Frame Rate Flexibility**\quad
FlexiCodec\ trains with a flexible merging threshold $\tau$, sampled across a range.

This enables the model to support controllable frame rates at inference by adjusting $\tau$.

At $\tau=1.0$ (no frame merging), FlexiCodec functions as a 12.5Hz fixed-frame-rate codec.

At $\tau < 1.0$ (with frame merging), the average frame rate of the new sequence is lower than 12.5Hz, and is calculated as:
$\frac{\text{Total number of frames after merging}}{\text{Audio duration in seconds}}$.

Setting a lower $\tau$ decreases the average frame rate.

**Semantic (RVQ-1) Quantization**\quad
The dynamic-rate ASR features are quantized using a Finite Scalar Quantizer (FSQ)[^Mentzer2023Finite] to produce discrete semantic tokens (we denote as RVQ-1 tokens).

FSQ projects the input representations $e_s$ into a $D$-dimensional low-rank space, where the value of each dimension is quantized using rounding operation ROUND into $L$ levels.

The quantized low-rank vector $\bar{e}$ is subsequently projected back to the original dimension $\bar{e}$:
$$
\bar{e_s} = \mathrm{ROUND}\bigl(\mathrm{Proj}_{\text{down}}(e_s)\bigr),
\quad
\hat{e_s} = \mathrm{Proj}_{\text{up}}(\bar{e_s}).
$$
The semantic token index $q_s$ is obtained by:
$q_s = \sum_{j=0}^{D-1}\bar{e_s}L^j$.

Additionally, we wrap the FSQ block with small ConvNeXt[^Liu2022Convnet] blocks to increase its representation ability, and apply an L2 loss $L_\text{feat}$ to align the semantic token embeddings with the unquantized semantic features.

**Acoustic (RVQ-rest) Quantization**\quad
To encode the detailed acoustic information that is not captured in the semantic tokens, following[^Li2025DualCodec],
we compute a residual by subtracting the dynamic-rate ASR feature from the dynamic-rate waveform feature.

This residual is then quantized using an $(N-1)$-layer Residual Vector Quantization (RVQ)[^Zeghidour2021Soundstream]\footnote{We have not applied FSQ for acoustic quantization because FSQ
is a single-layer quantization,
and we have not discovered a multi-layer FSQ practice in literature.} module to produce acoustic tokens $q_{2:N}\in Z^{(N-1)\times\hat{T}}$, where $\hat{T}$ denotes the sequence length after frame merging.

We employ quantizer dropout[^Zeghidour2021Soundstream] during training.

That is, only the RVQ-1 to RVQ-$n$
layers are subsequently decoded, where $n \in [1, N]$ is randomly chosen.

When $n=1$, only the semantic encoding stream is used.

**Frame Unmerging and Reconstruction**\quad
To reconstruct the audio, the decoder path should reverse the dynamic compression\footnote{This is because the NAR codec decoder can only receive fixed-frame-rate sequence.

It is also possible to generate audios directly from dynamic-rate tokens using an AR model, and we leave this as future work.}.

The embedding features from the first $n$ chosen RVQ tokens are added to form a decoding feature representation.

This dynamic-rate sequence is then passed to the Frame Unmerging Module, detailed in Figure [fig:mergingunmerging](#fig:mergingunmerging)(b).

It uses the frame length attributes to expand the sequence back to 12.5Hz.

It is followed by another Transformer with local attention to smooth the transitions and refine the feature representations.

The resulting feature sequence is then fed into a convolutional codec decoder which mirrors the codec encoder, synthesizing the output waveform.

FlexiCodec is trained end-to-end with a composite loss function:

$$
\mathcal{L} = \mathcal{L}_{\text{recon}} + \lambda_{\text{GAN}}\mathcal{L}_{\text{GAN}} + \lambda_{\text{RVQ}}\mathcal{L}_{\text{RVQ}} + \lambda_{\text{feat}}\mathcal{L}_{\text{feat}},
$$

where $\mathcal{L}_{\text{recon}}$ is a multi-scale L1 mel spectrogram reconstruction loss following[^Kumar2024High-Fidelity], $\mathcal{L}_{\text{GAN}}$ contains adversarial and feature matching losses for Multi-Period Discriminator (MPD)[^Kong2020Hifi-Gan] and Multi-Resolution Spectrogram Discriminator (MRSD)[^Kumar2024High-Fidelity], $\mathcal{L}_{\text{RVQ}}$ involves a L1 codebook update loss
and a commitment loss for RVQ, whereas the FSQ module does not require a training loss.
$\mathcal{L}_\text{feat}$ is the L2 feature alignment loss between the RVQ-1 semantic token embeddings and the unquantized semantic features.
