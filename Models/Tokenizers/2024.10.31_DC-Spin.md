# DC-Spin/SpinHuBERT

<details>
<summary>基本信息</summary>

- 标题: "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models"
- 作者:
  - 01 Heng-Jui Chang (MIT CSAIL, Internship at Meta) - hengjui@mit.edu
  - 02 Hongyu Gong (Meta AI)
  - 03 Changhan Wang (Meta AI)
  - 04 James Glass (MIT CSAIL)
  - 05 Yu-An Chun (Meta AI) - andyyuan@meta.com
- 链接:
  - [ArXiv](https://arxiv.org/abs/2410.24177)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2410.24177v1__DC-Spin__A_Speaker-invariant_Speech_Tokenizer_for_Spoken_Language_Models.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models.
SLMs process text and speech, enabling simultaneous speech understanding and generation.
This paper presents ***Double-Codebook Speaker-invariant Clustering (DC-Spin)***, which aims to improve speech tokenization by bridging audio signals and SLM tokens.
***DC-Spin*** extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis.
We propose a chunk-wise approach to enable streamable ***DC-Spin*** without retraining and degradation.
Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.

</details>
<br>

随着基于文本的仅解码器语言模型的进步, 口语语言模型 (Spoken Language Models, SLMs) 也越来越受到关注.
SLMs 能够处理文本和语音, 实现同时的语音理解和生成.

本文介绍了 ***双码本说话人不变聚类 (Double-Codebook Speaker-invariant Clustering, DC-Spin)***, 旨在通过连接语音信号和 SLM Token 来改进语音分词化.
***DC-Spin*** 提取了具有丰富语音信息且对输入变化不敏感的说话人不变 Token, 从而增强零样本 SLM 任务和语音重合成.

我们提出了一种分块方法, 使得 ***DC-Spin*** 在不重新训练和性能降低的情况下实现流式运行.

对比之前的分词方法 (自监督和神经音频编码器), 模型可扩展性和下游任务代理表明了那些容易被 N 元语言模型建模或与音素对齐的 Token 提供了强大的性能, 为设计 SLMs 的语音分词器提供了见解.

## 1.Introduction: 引言

Spoken language models (SLMs) and related applications have gained more interest with the advancements of large language models (LLM) and audio tokenization techniques ([Survey by Wu et al. (2024)](../../Surveys/S20240220.md)).
These speech LMs resemble causal LMs in natural language processing, but SLMs take speech and, optionally, text as input and generate speech or text.
Hence, these LMs can perform tasks like speech continuation ([GSLM (2021)](../SpeechLM/PureSpeechLM/2021.02.01_GSLM.md)), automatic speech recognition (ASR) ([AudioPaLM (2023)](../SpeechLM/ST2ST/2023.06.22_AudioPaLM.mdaLM.md); [VoxtLM (2023)](../SpeechLM/ST2ST/2023.09.14_VoxtLM.md)), text-to-speech synthesis (TTS) ([VALL-E (2023)](../SpeechLM/ST2S/2023.01.05_VALL-E.md-E.md)), and the more complicated spoken language understanding (SLU) problems ([LTU-AS (2023)](../SpeechLM/Interaction/2023.09.25_LTU-AS.md); [Qwen-Audio (2023)](../SpeechLM/ST2T/2023.11.14_Qwen-Audio.md; [SpiRit-LM (2024)](../SpeechLM/2024.02.08_SpiRit-LM.md)).
SLM has two main research directions: (1) LM architecture and training and (2) speech tokenization techniques, the latter of which is the focus of this paper.

Since directly taking raw audio waveform as input to an SLM is infeasible, tokenizing speech into text-like discrete units has become an essential component of recent SLMs.
We define four key qualifications for a good speech tokenizer inspired by prior studies.
First, the tokens should contain strong phonetic or semantic information so that the SLM can use the content of speech to perform ASR and SLU ([GSLM (2021)](../SpeechLM/PureSpeechLM/2021.02.01_GSLM.md)).
Second, the tokens should retain acoustic details for being resynthesized into speech for generative tasks like TTS and speech-to-speech translation ([Lee et al. (2021)](../S2ST/2021.07.12_Direct_Speech-to-Speech_Translation_with_Discrete_Units.md); [SpeechTokenizer (2023)](2023.08.31_SpeechTokenizer.md); [VALL-E (2023)](../SpeechLM/ST2S/2023.01.05_VALL-E.md-E.md)).
Third, the tokenizer should be robust to perturbations like additive noise, reverberation, and speaker change because the perturbations are irrelevant to how an SLM understands human speech and language ([Gat et al. (2022)](../_Full/2022.09.30_Augmentation_Invariant_Discrete_Representation_for_Generative_Spoken_Language_Modeling.md); [NAST (2024)](2024.06.16_NAST.md)).
Fourth, the tokenizer should be lightweight and fast, supporting real-time interaction between users and SLMs.
Hence, this paper tries to answer the following question:
***how to build and evaluate a good speech tokenizer for spoken language models that satisfies these key qualifications?***

We simplify the setup of this paper by training a unit-based speech LM (uLM) ([GSLM (2021)](../SpeechLM/PureSpeechLM/2021.02.01_GSLM.md)) and a Hifi-GAN unit-to-speech synthesizer ([HiFi-GAN (2020)](../Vocoder/2020.10.12_HiFi-GAN.md); [Polyak et al. (2021)](2021.04.01_Speech_Resynthesis_from_Discrete_Disentangled_Self-Supervised_Representations.md)).
This setup is commonly used in SLM studies and applications ([VoxtLM (2023)](../SpeechLM/ST2ST/2023.09.14_VoxtLM.md); [NAST (2024)](2024.06.16_NAST.md); [TWIST (2024)](../SpeechLM/PureSpeechLM/2023.05.22_TWIST.md)), which is an ideal proxy for more advanced SLMs.
uLMs are decoder-only Transformer LMs ([Transformer (2017)](../_Transformer/2017.06.12_Transformer.md)) and trained with the next-token prediction objective on speech tokens.
uLMs can perform zero-shot tasks by estimating the probability of utterances, including detecting real spoken words and determining correct syntactic structures ([ZeroSpeech (2021)](../../Evaluations/2020.11.23_ZeroSpeech.md)), and can be fine-tuned for ASR.
Moreover, we train Hifi-GANs to convert tokens to audio and quantify the intelligibility of the resynthesized speech to simulate speech generation with SLMs.
With uLM and resynthesis, we can examine speech tokenizers on the first two required qualities.
Next, we follow [Gat et al. (2022)](../_Full/2022.09.30_Augmentation_Invariant_Discrete_Representation_for_Generative_Spoken_Language_Modeling.md) to quantify the robustness by comparing the extracted tokens between clean and perturbed speech.
Finally, we measure the inference speed of offline and streaming tokenization.

After defining the goals and evaluation pipelines, we propose ***Double-Codebook Spin (DC-Spin)*** by extending speaker-invariant clustering (Spin) with an auxiliary codebook to extract better speech units, where Spin is a self-supervised fine-tuning method for capturing phonetic units via online clustering and speaker-invariant swapped prediction ([Spin (2023)](2023.05.18_Spin.md)).
To further boost robustness and token quality, we propose pre-training the Hidden-unit BERT (HuBERT) self-supervised speech encoder with Spin codeword units as a better initialization for ***DC-Spin*** ([HuBERT (2021)](2021.06.14_HuBERT.md)), denoted as ***SpinHuBERT***.
The contributions of this paper are listed as follows:

1) The proposed speech tokenizer produces high-quality speaker-invariant speech tokens, achieving state-of-the-art spoken language modeling and speech resynthesis compared to open-source tokenizers on multiple benchmarks with limited resources.
2) We propose a simple chunk-wise method to repurpose offline speech tokenizers into streaming mode with a negligible performance drop.
3) We analyze multiple proxy tasks to understand the relation between speech tokenizer and SLM performance.
   We find that phoneme and character-normalized mutual information and the proposed n-gram predictability are good proxies for downstream tasks.

## 2.Related Works: 相关工作

**Spoken Language Models (SLM)**

SLMs or speech language models usually refer to decoder-only LMs that input or output speech and sometimes text.
The two main approaches to integrating speech into LMs are adaptor and token-based.
(Terms "token" and "unit" are used interchangeably in this paper, indicating discrete speech units.)
Because of the recent advancements in LLMs, researchers connect speech encoders and text-based LMs through adaptors ([Qwen-Audio (2023)](../SpeechLM/ST2T/2023.11.14_Qwen-Audio.md; [SLAM-ASR (2024)](../ASR/2024.02.13_SLAM-ASR.md); [SALMONN (2023)](../SpokenDialogue/2023.10.20_SALMONN.md)), allowing speech understanding and ASR but requiring a more sophisticated design for speech generation ([LLaMA3 (2024)](../TextLM/2024.07.31_LLaMA3.md)).
In contrast, a more common approach, which is the main focus of this paper, is to tokenize speech to serve as both input and output of SLMs ([GSLM (2021)](../SpeechLM/PureSpeechLM/2021.02.01_GSLM.md); [TWIST (2024)](../SpeechLM/PureSpeechLM/2023.05.22_TWIST.md); [VoxtLM (2023)](../SpeechLM/ST2ST/2023.09.14_VoxtLM.md)).
Under this setup, SLMs treat audio waveforms as text-like tokens, allowing SLMs to process speech and text jointly ([SpiRit-LM (2024)](../SpeechLM/2024.02.08_SpiRit-LM.md)) and to generate speech by synthesizing tokens into audio ([Polyak et al. (2021)](2021.04.01_Speech_Resynthesis_from_Discrete_Disentangled_Self-Supervised_Representations.md)).

In [GSLM (2021)](../SpeechLM/PureSpeechLM/2021.02.01_GSLM.md), SLMs are trained with unlabeled speech tokens to discover the spoken content, which can be evaluated with zero-shot tasks in ([ZeroSpeech (2021)](../../Evaluations/2020.11.23_ZeroSpeech.md)).
This concept allows an SLM to be fine-tuned with paired speech-text data for ASR, TTS, and SLU ([VoxtLM (2023)](../SpeechLM/ST2ST/2023.09.14_VoxtLM.md)).
Advanced techniques like interleaving speech and text tokens ([SpiRit-LM (2024)](../SpeechLM/2024.02.08_SpiRit-LM.md)), initializing with text-based LMs ([TWIST (2024)](../SpeechLM/PureSpeechLM/2023.05.22_TWIST.md)), and integrating multiple token types ([AudioLM (2022)](../SpeechLM/ST2S/2022.09.07_AudioLM.mdLM.md)) are developed to improve performance.
Because SLMs simultaneously understand and generate speech, and the speech tokens are the only media between the models and audio signals, speech tokenizer design has become a crucial part of SLM research.

**Self-supervised Learning (SSL)**

SSL is introduced to leverage large unlabeled audio datasets to pre-train speech encoders, mitigating the need for extensive human labeling ([Survey by Mohamed et al. (2022)](../../Surveys/S20220521.md)).
SSL models are trained to predict pseudo labels given a partial speech utterance.
Pseudo targets could be Mel spectrograms ([APC (2019)](2019.04.05_APC.md); [TERA (2020)](2020.07.12_TERA.md)), vector-quantized features ([Wav2Vec2.0 (2020)](2020.06.20_Wav2Vec2.0.md); [HuBERT (2021)](../SpeechRepresentation/2021.06.14_HuBERT.mdpeechRepresentation/2022.02.03_Best-RQ.md)), or an exponential average of the model itself ([Data2Vec (2022)](2022.02.07_Data2Vec.md); [DinoSR (2023)](2022.02.07_Data2Vec.md))
Pre-trained SSL models offer good initialization for speech processing tasks ([SUPERB (2024)](../../Evaluations/2024.04.15_SUPERB.md)).
Moreover, evidence has shown that speech SSL models excel at extracting phonetic representations ([Pasad et al. (2021)](2021.07.10_Layer-wise_Analysis_of_a_Self-Supervised_Speech_Representation_Model.md); [Spin (2023)](2023.05.18_Spin.md); [Choi et al. (2024)](../_Full/2024.06.12_Self-Supervised_Speech_Representations_are_More_Phonetic_than_Semantic.md)), so quantizing SSL hidden layer embeddings with K-means clustering is widely adopted to tokenize speech ([GSLM (2021)](../SpeechLM/PureSpeechLM/2021.02.01_GSLM.md); [TWIST (2024)](../SpeechLM/PureSpeechLM/2023.05.22_TWIST.md); [VoxtLM (2023)](../SpeechLM/ST2ST/2023.09.14_VoxtLM.md)).
[Gat et al. (2022)](../_Full/2022.09.30_Augmentation_Invariant_Discrete_Representation_for_Generative_Spoken_Language_Modeling.md) and [NAST (2024)](2024.06.16_NAST.md) further fine-tune SSL encoders for robust speech tokenizers.

**Neural Audio Codec**

Neural network-based codecs compress audio into compact units and reconstruct high-fidelity signals from the units ([SoundStream (2021)](../SpeechCodec/2021.07.07_SoundStream.md); [EnCodec (2022)](../SpeechCodec/2022.10.24_EnCodec.md); [AudioDec (2023)](../SpeechCodec/2023.05.26_AudioDec.md)).
These models resemble autoencoders and comprise an encoder, a quantization module, and a decoder.
A commonly used technique for the quantization module is residual vector quantization (RVQ) ([SoundStream (2021)](../SpeechCodec/2021.07.07_SoundStream.md)).
RVQ has multiple codebooks, each quantizing the residual features computed from the previous codebook, making the first few codebooks preserve more critical information for reconstructing audio waveforms.
[SpeechTokenizer (2023)](2023.08.31_SpeechTokenizer.md) proposes SpeechTokenizer by enforcing the first codebook to capture phonetic units by distilling knowledge from a pre-trained SSL teacher, but the teacher bounds the performance.
One of the benefits of neural codecs is that the model itself has an audio resynthesis module, i.e., the decoder.
Still, SSL-based tokenizers can resynthesize speech with a separate vocoder.

Besides the open-source tokenizers, closed models like USM ([AudioPaLM (2023)](../SpeechLM/ST2ST/2023.06.22_AudioPaLM.mdaLM.md)) are claimed to be powerful for SLMs, but these tokenizers are difficult to reproduce or compare because the details remain unrevealed.
In contrast, this paper aims to offer insights into designing tokenizers and shares all details for future studies.
Additionally, some works categorize speech tokens into semantic and acoustic tokens for understanding and generative tasks, respectively ([SpeechTokenizer (2023)](2023.08.31_SpeechTokenizer.md); [AudioLM (2022)](../SpeechLM/ST2S/2022.09.07_AudioLM.mdLM.md)).
However, we will demonstrate that a single type of speech token is sufficient to perform well on both tasks.

## 3.Methodology: 方法

### 3.1.Background

**Speaker-invariant Clustering (Spin)**

Spin is a self-supervised fine-tuning approach inspired by [SwAV (2020)](../CV/2020.06.17_SwAV.md) and captures speaker-invariant content in speech signals through online clustering and swapped prediction ([Spin (2023)](2023.05.18_Spin.md)).
During training, each utterance is perturbed to sound like a different speaker but with the same content by randomly scaling the F0 and formant frequencies.
Both utterances are fed to a pre-trained SSL encoder, and the frame-level output of each utterance is transformed into a sequence of probability distributions with a learnable codebook.
The distributions are smoothed to enforce full codebook usage and serve as the learning target.
Finally, the model performs swapped prediction by minimizing the cross-entropy loss between the original codeword distribution and the smoothed targets from the perturbed output and vice versa.

Spin efficiently improves SSL encoders in content-related problems like ASR and phoneme recognition (PR).
[Robust-Spin (R-Spin) (2023)](2023.11.15_R-Spin.md) extends Spin for robust speech recognition but requires more complicated training stages and implementation.
Although [R-Spin (2023)](2023.11.15_R-Spin.md) have shown that discrete units produced by Spin codebooks are closely aligned with phonemes and characters, the applications of these tokens remain undiscovered.

**Hidden-unit BERT (HuBERT)**

HuBERT is an SSL pre-training method for speech representation learning ([HuBERT (2021)](2021.06.14_HuBERT.md)).
Like [BERT (2018)](../TextLM/2018.10.11_BERT.md) in NLP, HuBERT is pre-trained with a mask prediction objective for multiple iterations with pseudo labels derived by K-means clustered continuous audio representations.
First, the labels are K-means cluster IDs of Mel-frequency cepstral coefficients (MFCCs).
Then, the second iteration model predicts K-means clusters from the first model's hidden embeddings.
Besides serving as pre-training labels, K-means units are useful in SLM and speech-to-speech translation ([GSLM (2021)](../SpeechLM/PureSpeechLM/2021.02.01_GSLM.md); [Lee et al. (2021)](../S2ST/2021.07.12_Direct_Speech-to-Speech_Translation_with_Discrete_Units.md)).
This paper adopts HuBERT as the initialization of the proposed speech tokenizers (Section~\ref{subsec:method-spin}) and further improves HuBERT by introducing better learning targets (Section~\ref{subsec:method-hubert}).

### 3.2.Spin as Speech Tokenizer

This section proposes tokenizing speech with Spin codebook along with methods to improve the quality of Spin discrete units.
Because Spin codebooks capture phonetic information and have a unique speaker-invariant property, the tokens extracted from Spin satisfy the first qualification in Section~\ref{sec:intro}.
These properties are especially useful for speech generation because the vocoder can condition on different speakers, allowing more flexible speech synthesis.
Compared with K-means, Spin's codebook is optimized with gradient descent, proven highly scalable ([Efficient Backprop](../_Basis/1998_Efficient_Backprop.md)).
In contrast, K-means clustering requires extracting and storing hidden features, leading to high memory consumption and special implementation when scaling ([mHuBERT (2024)](2024.06.10_mHuBERT-147.md)).
Furthermore, K-means tokens contain speaker and unrelated information, leading to suboptimal SLM performance ([Yeh et al. (2024)](../_Full/2024.09.09_Estimating_the_Completeness_of_Discrete_Speech_Units.md)).
Motivated by the above reasons, this paper explores the possibilities of tokenizing speech with Spin for SLMs.

First, we fine-tune HuBERT Base with different Spin codebook sizes and use the codeword IDs as discrete units to perform zero-shot spoken LM tasks, where the experimental setup can be found in Section~\ref{subsec:exp-setup}.
As shown in Figure~\ref{fig:method-codebook-slm}, ideal codebook sizes are between 200 and 500.
Note that the codebook size should be large enough for speech resynthesis since low bitrate degrades resynthesis quality (Appendix~\ref{sec:app-resynth}).
Moreover, [Spin (2023)](2023.05.18_Spin.md) found larger Spin codebooks capture better phonetic representations.
The contradictory properties motivate us to develop methods to obtain a small but high-quality codebook.

***Double-Codebook Spin (DC-Spin)***

***DC-Spin*** extends Spin to two learnable codebooks optimized with the same objective.
The first codebook (primary) extracts discrete units for downstream applications.
The second codebook (auxiliary) is a large codebook that enhances the encoder's capability to capture fine-grained phonetic units.
Because both codebooks share the same encoder, the auxiliary codebook is expected to indirectly help the primary codebook encode high-quality units.

**Supervised Fine-tuning (SFT)**

Inspired by the speech encoders in multimodal LLMs ([AudioPaLM (2023)](../SpeechLM/ST2ST/2023.06.22_AudioPaLM.mdaLM.md); [Gemini (2023)](../TextLM/2023.12.19_Gemini.md); [LLaMA3 (2024)](../TextLM/2024.07.31_LLaMA3.md)), we include supervised fine-tuning to boost the token quality.
Specifically, we consider [CTC-based (2006)](../ASR/CTC.md) ASR and PR as the supervised tasks because (1) the data for these objectives are relatively easy to collect compared to frame-wise labels and (2) both tasks force the model to neglect redundant information and extract the content in speech.
CTC fine-tuning can be applied before or during ***DC-Spin*** fine-tuning, but we found the former leads to better results (Appendix~\ref{subsec:app-dcspin-sup}).

### 3.3.HuBERT Pre-Training with Better Targets

Spin can be applied to any pre-trained speech encoder, but the fine-tuned performance depends on the encoder's quality.
In Table~\ref{tab:ssl-pt-compare}, HuBERT and data2vec are superior to other methods, even though all models are fine-tuned with the same ***DC-Spin*** objective.
HuBERT is slightly inferior to data2vec on two tasks, but data2vec is more unstable because the learning target is an exponential moving average of itself ([Data2Vec (2022)](2022.02.07_Data2Vec.md)).
In contrast, HuBERT has fixed learning targets, which can be replaced with better pseudo labels.
The above findings have led us to propose ***SpinHuBERT*** by training HuBERT models with labels Spin units to better initialize ***DC-Spin***.
Because of the speaker-invariant nature of Spin, [Spin (2023)](2023.05.18_Spin.md) and [R-Spin (2023)](2023.11.15_R-Spin.md) have shown that discrete units derived from Spin codebooks are closer to phonetic units than HuBERT K-means units.
Following this observation, ***SpinHuBERT*** is expected to extract better phonetic representations.

Summarizing the proposed ***DC-Spin*** and ***SpinHuBERT***, the training pipeline is shown in Figure~\ref{fig:framwork}.
In stage (I), we pre-train a ***SpinHuBERT*** encoder with pseudo labels generated with Spin.
The optional stage (II) fine-tunes the encoder with CTC-based ASR or PR.
Stage (III) fine-tunes the encoder with the proposed ***DC-Spin*** objective to obtain the discrete speech tokens for downstream applications.

## 4.Experiments: 实验

### 4.1.Setup

**Baseline Tokenizers**

We adopt EnCodec 24kHz ([EnCodec (2022)](../SpeechCodec/2022.10.24_EnCodec.md)) and SpeechTokenizer ([SpeechTokenizer (2023)](2023.08.31_SpeechTokenizer.md),  [Github](https://github.com/ZhangXInFD/SpeechTokenizer)) as the neural audio codec baselines.
For SSL-based methods, we consider K-means clustering, augmentation invariant discrete representation ([Gat et al. (2022)](../_Full/2022.09.30_Augmentation_Invariant_Discrete_Representation_for_Generative_Spoken_Language_Modeling.md)), and [Noise Aware Speech Tokenization (NAST)](2024.06.16_NAST.md) ([Github](https://github.com/ShovalMessica/NAST)), where the second and third methods are designed specifically for SLM by training with perturbation-invariant objectives.
An SSL-based tokenizer using K-means clustering with $K$ units is denoted as "K-means\textsubscript{$K$}."

**Self-supervised Pre-training**

The HuBERT models are trained for 400k steps with 124k hours of unlabeled English speech.
Following the Large and X-Large models in [HuBERT (2021)](2021.06.14_HuBERT.md), our 3rd-iteration HuBERT (it3) learns to predict 500-unit K-means clusters of the 9th layer of HuBERT Base.
***SpinHuBERT*** learns from a Spin model with a codebook size of 4096.
Unless specified otherwise, SSL models operate at a 50Hz framerate.
Details can be found in Appendix~\ref{subsec:app-impl-pt}.

**Supervised Fine-tuning**

Under the SFT setup, we fine-tune pre-trained SSL models with ASR and PR before applying ***DC-Spin*** using two labeled datasets: LibriSpeech and English Labeled 3k.
The latter extends LibriSpeech with an additional 2k hours of speech.
The fine-tuned encoders are denoted by appending "ASR\textsubscript{$n$k}" and "PR\textsubscript{$n$k}" to the encoder's name, where $n =$ 1 or 3, indicating the two dataset sizes.
See Appendix~\ref{subsec:app-impl-asr-pr} for more information.

**Spin \& ***DC-Spin*** Fine-tuning**

We follow [Spin (2023)](2023.05.18_Spin.md) and reimplement Spin in [fairseq (2019)](../Toolkits/2019.04.01_FAIRSeq.md).
We fine-tune SSL models with unlabeled data from LibriSpeech on a single NVIDIA 32GB V100 GPU (see Appendix~\ref{subsec:app-impl-spin}).
"Spin\textsubscript{$K$}" denotes Spin with a codebook size of $K$.
***DC-Spin*** with primary and auxiliary codebook sizes of $K_1$ and $K_2$ is denoted as "DC-Spin\textsubscript{$K_1$,$K_2$}."

**Spoken Language Models**

We adopt unit-based LM as the SLM for a fair comparison with prior works ([GSLM (2021)](../SpeechLM/PureSpeechLM/2021.02.01_GSLM.md)).
Each SLM is a 150M-parameter Transformer decoder ([Transformer (2017)](../_Transformer/2017.06.12_Transformer.md)) that performs next-token prediction on discrete speech units.
The training data are obtained by extracting units from the 6k hours clean subset of [Libri-Light (2019)](../../Datasets/2019.12.17_Libri-Light.md).
After training, SLM estimates the log probability of speech utterances normalized by length for zero-shot SLM tasks.
Furthermore, we fine-tune SLMs with the same training objective but with labeled data from LibriSpeech to perform ASR.
See Appendix~\ref{subsec:app-impl-slm} for more details.

**Speech Resynthesis**

We use the [Expresso dataset (2023)](../../Datasets/2023.08.10_Expresso.md) to train and evaluate unit-to-speech Hifi-GAN vocoders ([HiFi-GAN (2020)](../Vocoder/2020.10.12_HiFi-GAN.md); [Polyak et al. (2021)](2021.04.01_Speech_Resynthesis_from_Discrete_Disentangled_Self-Supervised_Representations.md)).
The input includes a sequence of tokenized speech units, a speaker ID, and a style ID.
After training, we resynthesize all utterances in the dev and test sets with the original speaker and style IDs.

In our experiments, the speech tokens are deduplicated for SLM, i.e., merging repeated consecutive tokens, so the SLM outputs are also deduplicated, requiring the vocoder to include a duration prediction module in real-world applications.
E.g., a token sequence \texttt{45 103 103 34 5 5 5} after deduplication would be \texttt{45 103 34 5}.
However, to avoid further uncertainties, we simplify the vocoder setup to take speech token sequences with the correct length as input ([Chang et al. (2024)](../_Full/2024.06.11_The_Interspeech_2024_Challenge_on_Speech_Processing_Using_Discrete_Units.md)).
We keep the SLM and vocoder simple to reduce the effects of downstream model design and amplify the impact of tokenizers since this paper aims to understand how to design speech tokenizers and how they affect SLM performance.
The applications can be extended by introducing more advanced modeling strategies, but we leave this part for future studies.

## 5.Results: 结果

### Zero-shot Spoken Language Modeling

This section discusses the impact of tokenizers on SLM by adopting the following tasks.
- **tSC**
  We use the "Topic" Spoken StoryCloze to evaluate an SLM's ability to capture continuation coherence and fine-grained textual nuances ([TWIST (2024)](../SpeechLM/PureSpeechLM/2023.05.22_TWIST.md)).
  Each sample comprises two similar spoken stories with different endings.
  The SLM must find the utterance with a consistent ending.
- **sWUGGY** ([Github](https://github.com/zerospeech/benchmarks))
  We adopt the sWUGGY spot-the-word task from [ZeroSpeech (2021)](../../Evaluations/2020.11.23_ZeroSpeech.md).
  Each sample has two spoken words with similar pronunciations, with one of the words absent from the English vocabulary.
  The "all" subset combines the "in-vocab" subset and out-of-vocabulary words that do not appear in the LibriSpeech training set.
- **sBLIMP**
  The sBLIMP acceptability metric is also adopted from ZeroSpeech.
  Each sample comprises two similar utterances, but one is ungrammatical.
  The above tasks require an SLM to compute a pseudo probability for each audio recording in a sample and compare the probabilities to determine which is more likely to be the correct answer.
  The results are reported in accuracy.

Table~\ref{tab:gslm-base} shows the results of unsupervised speech tokenization techniques based on HuBERT Base and LibriSpeech for a fair comparison.
***DC-Spin*** demonstrates superior performance compared with previous methods.
We observe consistent improvement of ***DC-Spin*** over Spin across different unit sizes, but the gap is narrowed when the codebook size is 500.
Among all tasks, ***DC-Spin*** improves sWUGGY most significantly because this problem is closely related to how well speech tokens represent pronunciation, which is directly related to phonetic information.
The results strongly indicate the effectiveness of ***DC-Spin***.

To compare the proposed methods with state-of-the-art SLMs, we report results with unconstrained resources in Table~\ref{tab:slm-unconstrained}.
The proposed ***SpinHuBERT*** with ***DC-Spin*** offers the best performance on sWUGGY and sBLIMP, even using a relatively small SLM and training data size.
For tSC, ***DC-Spin*** performs similarly with 1.3B-parameter [TWIST (2024)](../SpeechLM/PureSpeechLM/2023.05.22_TWIST.md), but the gap increases between ***DC-Spin*** and larger SLMs, showing that this task might correlate more with LM scaling, especially when comparing to the cascaded topline.
Furthermore, ***DC-Spin*** is improved using either ASR or PR SFT with similar performance gains, indicating that either task is suitable for assisting ***DC-Spin***.
As for the baselines, the Whisper Small encoder (87M parameters) with K-means offers low accuracy even though the encoder was trained with 680k hours of speech.
EnCodec tokens result in the worst performance because no explicit constraints are imposed on the encoder or quantizer to extract phonetic or semantic representations.
SpeechTokenizer performs similarly to HuBERT with K-means, corroborating the hypothesis mentioned in Section~\ref{sec:related} that the HuBERT teacher bounds this model.
Hence, building speech tokenizers from speech SSL models offers better representations for SLM.
Overall, the results suggest that speech tokenizers greatly impact SLMs, and the proposed ***SpinHuBERT*** and ***DC-Spin*** achieve state-of-the-art SLMs on several tasks with limited resources.

### Speech Resynthesis

This section focuses on speech generation with SLMs by resynthesizing speech from discrete units and evaluating with the following metrics.

- **ASR-WER**
  This metric uses an ASR model to transcribe the resynthesized speech and computes the word error rate (WER) to quantify the intelligibility of the audio.
  [Weight File](https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_vox_960h_pl.pt)
- **UTMOS**
  Following prior works ([DASB (2024)](../../Evaluations/2024.06.20_DASB.md); [Chang et al. (2024)](../_Full/2024.06.11_The_Interspeech_2024_Challenge_on_Speech_Processing_Using_Discrete_Units.md)), we adopt UTMOS, a neural network-based mean opinion score (MOS) prediction, to assess the quality of the resynthesized speech because this metric highly correlates with human-rated MOS ([UTMOS (2022)](../../Evaluations/2022.04.05_UTMOS.md)).
  Although other metrics exist to evaluate vocoders, we focus on whether the speech tokens preserve sufficient information to synthesize intelligible and human-like speech using the same vocoder.

As shown in Table~\ref{tab:resynthesis-main}, HuBERT with ***DC-Spin*** reduces more than 10\% relative WER compared with K-means, but the K-means and ***DC-Spin*** are similar in ***SpinHuBERT***, showing that training HuBERT with Spin units helps representations for resynthesis.
SFT with 1k hours of data has little impact on the resynthesis results, although SFT has removed some acoustic details.
Moreover, SFT with more data (1k vs. 3k hours) lowers ASR-WER, which might be caused by increased robustness.
Compared with codec-based approaches, ***DC-Spin*** tokens can be synthesized to produce high-intelligibility and quality speech at a relatively low bitrate because the acoustic details are encoded across several RVQ codebooks in codecs.
We notice that UTMOS among SSL-based methods are similar, possibly indicating that the resynthesis quality is less relevant to the tokens than the vocoder.
To summarize, this section demonstrates the effectiveness of SSL-based tokenizers on speech resynthesis, corroborating with the findings in [MMM (2024)](2024.06.14_MMM.md).

### Robustness

This section focuses on the robustness of speech tokenizers via **unit edit distance (UED)** ([Gat et al. (2022)](../_Full/2022.09.30_Augmentation_Invariant_Discrete_Representation_for_Generative_Spoken_Language_Modeling.md), [Github](https://github.com/ShovalMessica/NAST/tree/main/augmentations)).
This metric computes the unit error rate of speech tokens between clean and distorted audio inputs, so lower values imply superior robustness.

In Table~\ref{tab:ued}, Spin and ***DC-Spin*** surpass [Gat et al. (2022)](../_Full/2022.09.30_Augmentation_Invariant_Discrete_Representation_for_Generative_Spoken_Language_Modeling.md) under most distortions even though this baseline tokenizer is explicitly trained with a denoising objective while our methods only have a speaker-invariant constraint.
One surprising finding is that Spin and ***DC-Spin*** are less robust on pitch shift than other distortions, probably because the distortion always shifts the pitch by a major third, making the speakers with higher pitches sound unreal.
In contrast, the speaker perturbation approach in Spin training keeps the speech more natural ([NANSY (2021)](../TTS2_Acoustic/2021.10.27_NANSY.md)).
Moreover, the overall best-proposed ***SpinHuBERT-PR***\textsubscript{3k}~+~***DC-Spin*** tokenizer (the last row) reduces the UED values further.
Overall, the proposed tokenizers demonstrate robustness even in unseen scenarios.

### Inference Efficiency

This section inspects the inference efficiency, the last qualification for good speech tokenizers.
The following metrics are averaged over three runs on LibriSpeech dev-clean and dev-other using a single V100 GPU.
- **Latency** is the average time required to tokenize an utterance.
- **Real Time Factor (RTF)** is the ratio between latency and utterance duration, so a lower value implies faster inference.

**Offline Inference**

We first compute the offline inference efficiency by tokenizing entire utterances.
As shown in Table~\ref{tab:offline-inference}, audio codecs are significantly slower than SSL models with a similar size because the RNNs in the former cannot be parallelized in contrast to the self-attention in the latter.
Next, NAST models are slow because the architecture is a Conformer encoder stacked on top of a HuBERT model ([Conformer (2020)](../ASR/2020.05.16_Conformer.md)).
NAST\textsubscript{50} and NAST\textsubscript{100} have the same model architecture and size.
HuBERT Base with ***DC-Spin*** and K-means have the same inference speed since the encoder and the quantization operation are similar.
In addition, large HuBERT models (300M+ parameters) are slow, which is less ideal for real-time speech tokenization.

**Chunk-wise Streaming**

To optimize user experience with SLMs, we repurpose speech tokenizers by chunk-wise token extraction to simulate streaming tokenization.
Initially, a tokenizer extracts the first chunk of speech with a duration of $T_{\text{chunk}}$ seconds.
And each time, the chunk expands by $T_{\text{shift}}$ seconds to tokenize the incoming audio.
Hence, the context is constantly expanding to improve tokenization accuracy.
As shown in Table~\ref{tab:streaming-inference}, the proposed ***DC-Spin*** has less performance degradation than K-means and maintains downstream performance like tSC.
The results demonstrate the feasibility of repurposing to streaming mode without re-training.
Combining the offline and streaming experiments, ***DC-Spin*** satisfies the fourth qualification of being a good speech tokenizer.
Appendix~\ref{sec:app-streaming} has a more detailed explanation of the chunk-wise approach and additional results.

### Effects of SSL Pre-Training

To understand the effects of SSL pre-training on tokenizing speech, we train HuBERT models with different sizes and objectives, quantize hidden representations with K-means for speech tokenization, and report the results in Table~\ref{tab:hubert-compare}.
SLM ASR is the result of pre-trained SLM fine-tuned with ASR transcription (see Appendix~\ref{subsec:app-impl-slm}).

First, HuBERT second iteration (it2) models perform similarly on several SLM tasks, but HuBERT Large exhibits significantly worse accuracy on sWUGGY, the cause of which remains unknown even though we trained the SLM twice to verify.
The results suggest scaling model size helps SLM-ASR and resynthesis but is not always helpful and also decreases inference efficiency (Table~\ref{tab:offline-inference}).
Second, we pre-train HuBERT it3 models with different framerates and sizes.
Compared to different framerates, 25Hz offers the best overall SLM results, but resynthesis intelligibility is degraded because the lowered framerate increases reconstruction difficulty.
Like HuBERT it2, we found improvement for all metrics when scaling the model size (6 vs. 12 vs. 18 layers).
Third, ***SpinHuBERT*** surpasses HuBERT it3 on all tasks, indicating that enhancing pseudo labels for pre-training has a greater impact on performance than scaling the model.
***SpinHuBERT*** even narrows the performance gap between 50 and 25Hz models.
Comparing K-means with ***DC-Spin*** (gray fonts), the performance gain from applying ***DC-Spin*** is more significant than all other effects.
Thus, results suggest we should focus more on tokenization techniques than scaling SSL encoders.

### Finding Proxy Tasks for Spoken Language Modeling

This section inspects the correlation between tasks to find proper proxies for the actual SLM tasks.
See Appendix~\ref{subsec:app-impl-proxy} for more details.\\
- **Bitrate**: We compute the bitrate of deduplicated tokens by considering the distribution of tokens via entropy.
- **N-gram Predictability**: We propose training a 4-gram LM with deduplicated tokens on LibriSpeech and reporting the average perplexity.
This metric measures the difficulty of modeling speech tokens.\\
- **Phonetic ABX**: ABX error rate quantifies how well a tokenizer can distinguish phonemes ([ABX (2016)](../../Evaluations/ABX.md); [ZeroSpeech (2021)](../../Evaluations/2020.11.23_ZeroSpeech.md)).
- **Phone Normalized Mutual Information (PNMI)**: Proposed by [HuBERT (2021)](2021.06.14_HuBERT.md), PNMI computes the mutual information between the speech tokens and phoneme alignments.
Thereby, higher values imply better alignment with the underlying phoneme distribution.\\
- **Character Normalized Mutual Information (CNMI)**: Similar to PNMI, CNMI compares tokens with character alignments ([R-Spin (2023)](2023.11.15_R-Spin.md)).
We use UnitY2 to compute alignments ([Seamless (2023)](../SpeechLM/2023.12.08_Seamless.md), [Github](https://github.com/facebookresearch/seamless_communication/blob/main/docs/m4t/unity2_aligner_README.md)).

Using 33 tokenizers with 50Hz framerate and 500 units, we compute the Pearson correlation coefficients between proxy and downstream metrics in Figure~\ref{fig:exp-task-corr-part}.
We make the values negative before calculating the coefficients for lower-better metrics (bitrate, 4-gram, ABX, ASR, and resynthesis).
According to Figure~\ref{fig:exp-task-corr-part}, bitrate positively correlates with tSC and sBLIMP, implying short and compact tokens are more suitable for capturing the long context of speech.
Next, low 4-gram perplexity correlates with SLM tasks, so repeating patterns in tokens improves SLM.
The high correlation between PNMI, ABX, and sWUGGY verifies that sWUGGY relies on well-aligned phonetic units (Section~\ref{subsec:exp-slm}).
Similarly, CNMI quantifies the textual alignment quality, making this task more related to sBLIMP and ASR.
Nevertheless, the ABX error rate negatively correlates with tSC and sBLIMP, implying this metric might fail to serve as a proxy.
Furthermore, speech resynthesis highly correlates with phoneme alignment metrics (ABX and PNMI), suggesting this task relies on the phonetic representations captured by the tokens for synthesizing intelligible speech signals.
Overall, n-gram predictability, PNMI, and CNMI are ideal proxies for developing speech tokenizers.
More results can be found in Appendix~\ref{sec:app-task-corr}.

## 6.Conclusions: 结论

This paper studies building and evaluating effective and robust speech tokenizers for spoken language modeling and speech resynthesis.
We propose ***SpinHuBERT*** and ***DC-Spin***, which demonstrate strong capabilities on several tasks compared with open-source speech tokenizers.
Our methods satisfy the four qualifications for an ideal tokenizer: captures phonetic information, preserves acoustic details for resynthesis, is robust to perturbations, and fast inference.
Furthermore, we found n-gram predictability, PNMI, and CNMI metrics highly correlate with downstream performance, making these tasks ideal proxies.
The findings and proxy tasks offer guidelines for future tokenizer and spoken language model development.

### Limitations and Future Works

This paper focuses on the effectiveness of speech tokenizers, so the evaluation tasks are on a smaller scale.
Although the proposed tokenizers achieve state-of-the-art zero-shot metrics with small SLMs, it is worth investigating their gains on multimodal LLMs.
Our models are trained and evaluated on English speech, so extending to multilingual and general audio is left for future studies.
TTS and speech-to-speech translation are also potential applications.

### Reproducibility Statement

The experiments of this paper utilize publicly available datasets and code for better reproducibility.
First, we use public datasets for model training and evaluation as described in Section~\ref{subsec:exp-setup} and Appendix~\ref{sec:app-impl}.
Second, the baseline speech tokenizers and SSL speech encoders are open models that can be accessed easily, as listed in Appendix~\ref{subsec:app-impl-baseline}.
Third, the training code of Spin and ***DC-Spin*** is first adopted from the official code in [Spin (2023)](2023.05.18_Spin.md) and reimplemented in the open-source [fairseq (2019)](../Toolkits/2019.04.01_FAIRSeq.md) library.
We also demonstrate that our implementation matches the original performance in Appendix~\ref{subsec:app-impl-spin}.
Fourth, we follow the original implementation for the evaluation tasks to ensure a fair comparison with prior works.
For reference, we provide the source of the code and data we use in footnotes throughout the paper.
