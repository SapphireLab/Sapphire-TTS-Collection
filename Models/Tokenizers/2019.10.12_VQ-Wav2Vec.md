# VQ-Wav2Vec

<details>
<summary>基本信息</summary>

- 标题: "vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations"
- 作者:
  - 01 Alexei Baevski,
  - 02 Steffen Schneider,
  - 03 Michael Auli
- 链接:
  - [ArXiv](https://arxiv.org/abs/1910.05453)
  - [Publication](https://openreview.net/forum?id=rylwJxrYDS)
  - [Github](https://github.com/pytorch/fairseq)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/1910.05453v3__VQ-Wav2Vec__Self-Supervised_Learning_of_Discrete_Speech_Representations.pdf)
  - [Publication](_PDF/1910.05453p0__VQ-Wav2Vec__ICLR2020Poster.pdf)

</details>

## Abstract: 摘要

We propose ***vq-wav2vec*** to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task.
The algorithm uses either a gumbel softmax or online k-means clustering to quantize the dense representations.
Discretization enables the direct application of algorithms from the NLP community which require discrete inputs.
Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论