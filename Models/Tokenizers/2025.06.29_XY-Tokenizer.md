# XY-Tokenizer

<details>
<summary>基本信息</summary>

- 标题: "XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs"
- 作者:
  - 01 Yitian Gong
  - 02 Luozhijie Jin
  - 03 Ruifan Deng,
  - 04 Dong Zhang,
  - 05 Xin Zhang,
  - 06 Qinyuan Cheng,
  - 07 Zhaoye Fei,
  - 08 Shimin Li,
  - 09 Xipeng Qiu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2506.23325)
  - [Publication]()
  - [Github](https://github.com/gyt1145028706/XY-Tokenizer)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2025.07.09_2506.23325v2__XY-Tokenizer__Mitigating_the_Semantic-Acoustic_Conflict_in_Low-Bitrate_Speech_Codecs.pdf)
  - [Publication] #TODO

</details>

## 摘要

<!--
Speech codecs serve as bridges between speech signals and large language models.
An ideal codec for speech language models should not only preserve acoustic information but also capture rich semantic information.
However, existing speech codecs struggle to balance high-quality audio reconstruction with ease of modeling by language models.
In this study, we analyze the limitations of previous codecs in balancing semantic richness and acoustic fidelity.
We propose ***XY-Tokenizer***, a novel codec that mitigates the conflict between semantic and acoustic capabilities through multi-stage, multi-task learning.
Experimental results demonstrate that ***XY-Tokenizer*** achieves performance in both semantic and acoustic tasks comparable to that of state-of-the-art codecs operating at similar bitrates, even though those existing codecs typically excel in only one aspect.
Specifically, ***XY-Tokenizer*** achieves strong text alignment, surpassing distillation-based semantic modeling methods such as **SpeechTokenizer** and **Mimi**, while maintaining a speaker similarity score of 0.83 between reconstructed and original audio.
The reconstruction performance of ***XY-Tokenizer*** is comparable to that of **BigCodec**, the current state-of-the-art among acoustic-only codecs, which achieves a speaker similarity score of 0.84 at a similar bitrate.
Code and models are available at [this https URL](https://github.com/gyt1145028706/XY-Tokenizer).
-->

语音编解码器作为语音信号和大语言模型之间的桥梁.
语音语言模型的理想编解码器不仅应该保留声学信息, 还应该捕获丰富的语义信息.
然而, 现有的语音编解码器难以平衡高质量音频重构和语言模型建模便利性.

在本研究中, 我们分析了以前的编解码器在平衡语义丰富性和声学保真度方面的限制.
我们提出了 ***XY-Tokenizer***, 一种新型的编解码器, 通过多阶段多任务学习来缓解语义和声学能力之间的冲突.

实验结果表明 ***XY-Tokenizer*** 在语义和声学任务上都达到了与当前最先进的声学编解码器相当的性能, 即使这些现有的编解码器在同等比特率下通常只能达到单个方面.
具体而言, ***XY-Tokenizer*** 实现了强大的文本对齐, 超越基于蒸馏的语义建模方法, 比如 **SpeechTokenizer** 和 **Mimi**, 同时保持了重构音频和原始音频之间的说话人相似度得分为 0.83.
***XY-Tokenizer*** 的重构性能与 **BigCodec**, 目前仅能进行声学编码的编解码器, 达到了类似的说话人相似度得分, 即 0.84.
代码和模型可在[此处](https://github.com/gyt1145028706/XY-Tokenizer)获得.

## 1·引言

In recent years, large language models (LLMs) (**GPT-4**[^Achiam2023GPT-4], **Qwen2.5**[^Yang2024Qwen2.5]) have achieved significant advancements in natural language processing, showcasing remarkable capabilities in understanding and generating text for fluent and natural conversations.
Consequently, speech large language models (Speech LLMs) have garnered increasing attention(**SpeechGPT**[^Zhang2023SpeechGPT], **Qwen2**[^Chu2024Qwen2], **Moshi**[^Defossez2024Moshi]).
A critical component of Speech LLMs is the speech codec, which transforms continuous speech signals into discrete tokens, aligning with the token-based approach of LLMs (**SoundStream**[^Zeghidour2021SoundStream], **EnCodec**[^Defossez2022EnCodec], **DAC**[^Kumar2023DAC], **SpeechTokenizer**[^Zhang2024SpeechTokenizer], **Moshi**[^Defossez2024Moshi]).
Acoustic codecs (**EnCodec**[^Defossez2022EnCodec], **DAC**[^Kumar2023DAC], **BigCodec**[^Xin2024BigCodec]) trained through residual vector quantization GAN (RVQ-GAN) capture the details of the audio waveform and allow for high-quality synthesis (**VALL-E**[^Wang2023VALL-E]).
Self-supervised learning (SSL) models trained with mask language modeling (MLM) (**BERT**[^Devlin2019BERT]) capture contextual dependencies in speech, making them widely used in Speech LLMs(**HuBERT**[^Hsu2021HuBERT], **W2V-BERT**[^Chung2021W2V-BERT], **WavLM**[^Chen2022WavLM]).
Additionally, automatic speech recognition (ASR) models trained on large-scale supervised datasets(**Whisper**[^Radford2022Whisper]) align well with the text modality, and their discrete representations are often utilized as inputs for Speech LLMs (**GLM-4-Voice**[^Zeng2024GLM-4-Voice], **Kimi-Audio**[^Ding2025Kimi-Audio]).

Semantic tokens, typically derived from discretized self-supervised learning (SSL) models, are considered to exhibit high alignment with text while leading to poor reconstruction.
In contrast, acoustic tokens  often derived from speech codecs trained through residual vector quantization GAN (RVQ-GAN), are recognized for capturing the details of the audio waveform, enabling high-quality synthesis, but they do not demonstrate strong alignment with text (**AudioLM**[^Borsos2023AudioLM]).
An ideal speech codec should effectively model both semantic and acoustic information.
**SpeechTokenizer**[^Zhang2024SpeechTokenizer] employs semantic distillation, utilizing the output of the first layer of residual vector quantization (RVQ) to distill representations from a teacher SSL model.
Similarly, the Mimi codec in **Moshi**[^Defossez2024Moshi] adopts a split residual vector quantization architecture, distilling one channel's output with a pretrained SSL model.
**X-Codec**[^Ye2024X-Codec] introduces an **X-shaped** structure, ensuring that tokens at each layer are semantically rich.
However, a key challenge in modeling both semantic and acoustic information lies in the inherent conflict between these tasks, particularly at low bitrates, where achieving high performance in both remains difficult.

In this work, we propose ***XY-Tokenizer***, the first speech codec to successfully model both semantic and acoustic information effectively at low bitrates.
Our codec employs a dual-tower architecture that mitigates the conflict between semantic and acoustic tasks by minimizing shared parameters in a multi-task learning framework.
We introduce a multi-stage, multi-task training paradigm: the first stage aligns the codec with text using an LLM-based ASR approach and employs a reconstruction loss on the original speech signal to ensure coarse-grained audio reconstruction, utilizing a 2-channel encoder-decoder structure, forming an **X-shaped** architecture.
The second stage incorporates a discriminator to model fine-grained audio features using a **generative adversarial network (GAN)**[^Goodfellow2014GAN], where the decoder discards the text-alignment module, resulting in a **Y-shaped** architecture.

Our contributions can be summarized as follows:
- We propose ***XY-Tokenizer***, a speech codec with a 16kHz sampling rate and a 1kbps bitrate.
It employs a dual-tower architecture to model semantic and acoustic information simultaneously through multi-task learning, aligns with text using an LLM-based automatic speech recognition approach, and ensures high-quality speech reconstruction via a codec decoder.
- We introduce a multi-stage, multi-task training paradigm for modeling semantic and acoustic information concurrently.
The first stage aligns the codec with text and models coarse-grained audio features, while the second stage incorporates a discriminator to model fine-grained audio features using a generative adversarial network.
- We analyze the limitations of current speech codecs, particularly the inherent conflict between semantic and acoustic objectives, and propose solutions such as leveraging pretrained automatic speech recognition models and minimizing shared parameters to mitigate these conflicts.
- ***XY-Tokenizer*** achieves performance at 1kbps comparable to the 4kbps codec that simultaneously models semantic and acoustic information (e.g., **SpeechTokenizer**) in both semantic and acoustic dimensions.
At similar low bitrates, it excels in both tasks and matches the performance of state-of-the-art codecs specialized in a single aspect, such as **BigCodec**, which models only acoustic quality without explicitly modeling semantic information.
We conducted extensive ablation studies to validate the effectiveness of our approach, and we will open-source our repository and pretrained models.

## 2·背景

### 2.1·初步实验

Self-supervised learning (SSL) models for speech, such as those trained with masked language modeling(**HuBERT**[^Hsu2021HuBERT], **WavLM**[^Chen2022WavLM]), effectively capture high-level speech features and are widely used in speech large language models (Speech LLMs) (**SpeechGPT**[^Zhang2023SpeechGPT], **IntrinsicVoice**[^Zhang2024IntrinsicVoice]).
Similarly, automatic speech recognition (ASR) models, trained on large-scale supervised datasets of paired speech and transcripts, achieve **strong alignment between speech and text modalities**(**Whisper**[^Radford2022Whisper], Tang2022Unified[^Tang2022Unified], **SpeechT5**[^Ao2022SpeechT5]).
However, training a speech codec from scratch to align with the text modality is data-intensive.
To address this, our proposed **XY-Tokenizer** leverages pretrained ASR or SSL models for the encoder to reduce training complexity.
Although these ASR and SSL models exhibit strong alignment with text, their ability to retain paralinguistic information remains unexplored.
To identify the most suitable pretrained model for our codec, we conduct a preliminary experiment to evaluate their performance in preserving acoustic information.

For this experiment, we selected three pre-trained models: **Whisper**[^Radford2022Whisper], an ASR model, as well as **HuBERT**[^Hsu2021HuBERT] and **WavLM**[^Chen2022WavLM], which are self-supervised learning (SSL) models.
We trained an auto-encoder, distinct from a codec in that it lacks a quantizer, to assess the reconstruction capabilities of these pre-trained models.
Specifically, we used the pre-trained Whisper, HuBERT, and WavLM models as **fixed encoders**, each paired with a decoder of identical parameter size to ensure a fair comparison.
The experimental setup and details are provided in Appendix.A.

> **Model Architecture**
>
> For the preliminary experiments to select the encoder for the XY-Tokenizer, we utilized three pre-trained models: [Whisper-Small](https://huggingface.co/openai/whisper-small), [HuBERT-Large-ll6k](https://huggingface.co/facebook/hubert-large-ll60k), and [WavLM-Large](https://huggingface.co/microsoft/wavlm-large).
These encoders were kept frozen during training to evaluate their reconstruction capabilities.
A decoder with approximately $250\text{M}$ parameters was used, without a quantizer.

> **Dataset and Training**
>
> The auto-encoders were trained on the full Emilia dataset.
Training was performed on a single NVIDIA H100 GPU with a batch size of 8.
The training process consisted of 200,000 steps.
The maximum learning rate was set to $1 \times 10^{-4}$, and DeepSpeed Zero-2 was used for optimization.

As shown in Table~\ref{table:preliminary_experiment_results}, Whisper achieves a superior reconstruction performance, effectively preserving paralinguistic information, such as speaker timbre and acoustic details.
In contrast, HuBERT and WavLM exhibit limitations in retaining certain aspects of speaker timbre and fine-grained acoustic details.
Furthermore, Whisper's pretraining on ASR tasks aligns closely with the LLM-based tasks employed in our codec, facilitating better text-speech alignment.
Based on these findings, we selected Whisper to initialize the encoder of our proposed XY-Tokenizer and further fine-tuned it for our codec training pipeline.

### 2.2·相关工作

#### 语音语言模型

Recent research on speech large language models has attracted considerable interest (**Survey20230824**[^Latif2023Survey20230824], **Survey20240220**[^Wu2024Survey20240220], **WavChat/Survey20241115**[^Ji2024Survey20241115]).
**AudioLM**[^Borsos2023AudioLM] achieves high-quality audio generation with coherent long-term structure through coarse-to-fine token modeling.
**SpeechGPT**[^Zhang2023SpeechGPT], the first end-to-end speech large language model, features strong instruction-following capabilities and effective spoken dialogue interaction, employing a three-stage training methodology to facilitate cross-modal transfer and efficient training.
**SpeechGPT-Gen**[^Zhang2024SpeechGPT-Gen] proposes Chain-of-Information Generation, a modeling approach that disentangles semantic and perceptual aspects for large-scale speech generation.
**IntrinsicVoice**[^Zhang2024IntrinsicVoice] implements GroupFormer to diminish the modality gap between text and speech, thereby enabling the transfer of capabilities from pre-trained large language models to the speech domain, facilitating low-latency and high-quality speech interaction in multi-turn dialogue contexts.
**Moshi**[^Defossez2024Moshi] employs a multi-stream architecture that concurrently processes audio streams from both the user and the system (Moshi itself), supporting dynamic conversations with overlaps and interruptions, thereby achieving full-duplex dialogue.

#### 语音编解码器

Speech codecs play a vital role in speech large language models by converting continuous speech signals into discrete tokens, enabling LLMs to process speech as a form of "foreign language." Neural network-based speech codecs predominantly utilize the RVQGAN paradigm, which can compress audio signals into low-bitrate representations through end-to-end training (**SoundStream**[^Zeghidour2021SoundStream], **EnCodec**[^Defossez2022EnCodec], **DAC**[^Kumar2023DAC]), making them ideal for real-time communication applications.
**BigCodec**[^Xin2024BigCodec] achieves excellent reconstruction quality even at low bitrates by scaling the encoder and decoder parameters.
To align speech codec tokens with large text models, recent efforts have explored modeling both semantic and acoustic features simultaneously (**SpeechTokenizer**[^Zhang2024SpeechTokenizer], **Mimi**[^Defossez2024Moshi], **X-Codec**[^Ye2024X-Codec]).
**SpeechTokenizer**[^Zhang2024SpeechTokenizer] enhances the RVQGAN paradigm with semantic distillation to guide the first layer of RVQ to align with a teacher SSL model (**HuBERT**[^Hsu2021HuBERT]).
**X-Codec**[^Ye2024X-Codec] proposes an X-shaped structure where each layer of RVQ contains both semantic and acoustic information.
**Baichuan Audio Tokenizer**[^Li2025Baichuan-Audio] first obtains a coarse Mel-spectrogram through multi-task learning and text alignment, then generates an enhanced Mel-spectrogram via conditional **flow matching**[^Lipman2022FM], which is finally converted into waveforms using a pretrained vocoder (**HiFi-GAN**[^Kong2020HiFi-GAN]).

## 3·方法

### 3.1·XY-Tokenizer

#### 动机

An ideal speech codec should effectively balance two goals: high-fidelity audio reconstruction and strong semantic alignment with text (**SpeechTokenizer**[^Zhang2024SpeechTokenizer], **UniAudio1.5**[^Yang2024UniAudio1.5]).
However, these two objectives often conflict, as optimizing for one can degrade the other (**Mimi**[^Defossez2024Moshi]).
Our empirical analysis, as shown in Table~\ref{table:empirical_analysis_of_shared_params}, shows that decreasing the number of shared parameters between semantic and acoustic modeling pathways effectively mitigates the trade-off between high-fidelity audio reconstruction and strong semantic alignment.
Moreover, semantic modeling can be effectively approached through automatic speech recognition (ASR) tasks, while acoustic modeling aligns closely with reconstruction through a codec decoder.
To this end, we propose a dual-channel codec architecture that jointly models semantic and acoustic information in a multi-task setup, combining ASR and audio reconstruction, with shared parameters limited to the residual vector quantization (RVQ) module and its adjacent components.

#### 适配器

To enhance the flexibility of embeddings, we incorporate lightweight **Transformer**-based[^Vaswani2017Transformer] adapter modules at multiple components of the XY-Tokenizer.
Each adapter consists of a 4-layer Transformer with a hidden dimension of 768, a feed-forward network (FFN) dimension of 3072, and 12 attention heads.
Adapters are placed after the semantic encoder, before and after the quantizer, and before the LLM-based semantic decoder.

#### 编码器

The encoder comprises two parallel branches: a **semantic channel** and an **acoustic channel**, both processing mel-spectrogram inputs at 100 Hz.
Each channel is initialized with a **Whisper encoder**[^Radford2022Whisper], with the semantic encoder's parameters fixed and the acoustic encoder's parameters trainable.
The semantic channel extracts linguistic features, while the acoustic channel captures paralinguistic information.
The outputs of both channels are concatenated and further processed to produce the final encoder output.
Additional details of the encoder architecture are provided in Appendix.B.

> The input waveform is resampled to 16 kHz, and an 80-channel Mel spectrogram is computed using a 25 ms window length and a 10 ms hop length to serve as the input to the encoder.
>
> The semantic encoder adopts the whisper-small encoder configuration and processes the Mel spectrogram through the following modules:
(1) a 1D convolutional layer with a kernel size of 3 and a stride of 1, projecting the 80-dimensional input to a hidden dimension of 768;
(2) a **GELU** activation function[^Hendrycks2016GELU];
(3) a second 1D convolutional layer with a kernel size of 3 and a stride of 2, reducing the sequence length by a factor of 2;
(4) another GELU activation function;
(5) sinusoidal positional embeddings;
(6) a transformer with 12 layers, 12 attention heads, a dimension of 768, and a feed-forward network dimension of 3072.
The semantic encoder's output is then passed to (7) an adapter module.
The semantic encoder's parameters are fixed during training.
>
> The acoustic encoder follows a similar architecture to the semantic encoder but is trainable and excludes the adapter module.
The outputs of the semantic and acoustic encoders are concatenated along the feature dimension.

#### 量化器

We employ a residual vector quantization (RVQ) module (**SoundStream**[^Zeghidour2021SoundStream]) with 8 layers operating on the encoder output at a temporal resolution of 12.5 Hz.
Each layer uses a codebook of size 1024, resulting in a total bitrate of 1 kbps.
The quantizer is integrated with adapter and convolution modules, with details provided in Appendix.B.

> We employ a residual vector quantizer (RVQ) with 8 layers and a codebook size of 1024 per layer.
The codebook is updated using an exponential moving average (EMA) with a weight decay of 0.99.
To prevent codebook collapse, unused codebook entries are randomly replaced with input vectors from the current batch after several training steps.
The codebook is initialized using $k$-means clustering with 10 iterations.
A 4$\times$ downsampling convolutional layer is applied before the quantizer, reducing the encoder's 50 Hz embeddings to 12.5 Hz, resulting in a bitrate of 1 kbps for our proposed XY-Tokenizer.
Adapter modules are placed before the downsampling convolution and after the quantizer.

#### 解码器

The decoder consists of two parallel branches: a **semantic channel** and an **acoustic channel**, both processing the quantized encoder output.
The semantic channel, a decoder-only large language model, generates text transcriptions, while the acoustic channel reconstructs the waveform.
For details of decoder, see Appendix.B.

> The decoder processes quantized features through two distinct pathways: the semantic decoder for text prediction and the acoustic decoder for audio reconstruction.

> The semantic decoder takes the output of the quantizer as input, passes it through an adapter, and uses the resulting features as conditioning input for a decoder-only large language model (LLM).
> The LLM, based on **Qwen2.5-0.5B**[^Yang2024Qwen2.5], has a hidden dimension of 896, an intermediate layer size of 4864, and 24 layers, generating the final predicted text corresponding to the input speech.

> The acoustic decoder takes the output of the quantizer as input, applies a 4x upsampling convolution to reach 50 Hz, and follows a structure symmetric to the acoustic encoder to achieve 100 Hz.
> Finally, a 30-layer **Vocos** model[^Siuzdak2023Vocos] with a hop size of 160 reconstructs the 16 kHz audio waveform.

**判别器**

To ensure high perceptual quality, we employ three discriminator models: **multi-period discriminator (MPD)**[^Kong2020HiFi-GAN], **multi-scale discriminator (MSD)**[^Kumar2019MelGAN], and **multi-scale short-time fourier transform discriminator (MS-STFTD)**[^Defossez2022EnCodec].
The parameters of our discriminator models are consistent with those used in **SpeechTokenizer** [^Zhang2024SpeechTokenizer].

### 3.2·两阶段训练策略

To streamline the training process and enhance efficiency, we propose a two-stage training strategy, consisting of a pre-training stage and a post-training stage.
In the pre-training stage, we employ multi-task learning to simultaneously model semantic features and coarse acoustic features.
In the post-training stage, we focus on modeling fine-grained acoustic features.
This section elaborates on these stages.

#### 预训练阶段

In the pre-training stage, we focus on two tasks: audio reconstruction and automatic speech recognition (ASR).
All model parameters are trainable, except for the weights of the semantic encoder, initialized from **Whisper encoder**[^Radford2022Whisper], and the large language model (LLM) which is initialized from **Qwen2.5**[^Yang2024Qwen2.5].
To align with text generation, we use the cross-entropy loss for the LLM, defined as:
\[
\mathcal{L}_{{asr}} = -\sum_{t=1}^N \log p(\mathbf{y}_t \mid \mathbf{y}_{<t}, \mathbf{f}; \theta_{{LLM}})
\]

where \(\mathbf{y}_t\) is the predicted text token at time step \( t \), \(\mathbf{y}_{<t}\) denotes the sequence of preceding tokens, \( \mathbf{f} \) represents the audio features input to the LLM, \( N \) is the total number of predicted text tokens, and \( \theta_{{LLM}} \) denotes the parameters of the LLM.

For modeling acoustic features, we employ a multi-scale mel-spectrogram reconstruction loss:
\[
\mathcal{L}_{{recon}} = \sum_{i \in e} \left\| S_i(\mathbf{x}) - S_i(\mathbf{\hat{x}}) \right\|_1
\]

where \( S_i \) is the mel-spectrogram at scale \( i \), computed using a normalized short-time fourier transform (STFT) with a window size of \( 2^i \) and a hop length of \( 2^{i} / 4 \).
The set of scales is defined as \( e = \{5, \dots, 11\} \).
Here, \( \mathbf{x} \) is the ground-truth audio waveform, and \( \mathbf{\hat{x}} \) is the predicted waveform from the acoustic decoder.
No waveform-based reconstruction loss is used.

Additionally, we incorporate a commitment loss to ensure effective quantization:
\[
\mathcal{L}_{{commit}} = \sum_{i=1}^{N_q} \left\| \mathbf{z}_i - sg(q_i(\mathbf{z}_i)) \right\|_1
\]

where \( \mathbf{z}_i \) is the input to the \( i \)-th layer of the quantizer, \( q_i(\mathbf{z}_i) \) is its quantized output, \( N_q \) is the number of quantized vectors, and \( sg \) denotes the stop-gradient operation, which prevents gradients from propagating to the quantizer's codebook.

The total loss for the pre-training stage is a weighted combination of individual losses:
\[
\mathcal{L}_{{pretraining}} = \lambda_{{asr}} \mathcal{L}_{{asr}} + \lambda_{{recon}} \mathcal{L}_{{recon}} + \lambda_{{commit}} \mathcal{L}_{{commit}}
\]

where \( \lambda_{{asr}}, \lambda_{{recon}}, \lambda_{{commit}} \) are hyperparameters that balance the weights of each loss term.

#### 后训练阶段

After the pre-training stage, we obtain an encoder capable of producing rich semantic features.
However, the codec's output may contain artifacts, which significantly degrade perceptual quality and listening experience.
To address this, the post-training stage focuses on modeling fine-grained audio details.
The approach is detailed below.

We adopt a generative adversarial network (GAN) framework for post-training.
For the generator, which corresponds to the codec, we fix the encoder and quantizer, discard the semantic decoder used in the pre-training stage, and keep all other parameters consistent with the pre-training stage, with these parameters remaining trainable.
For the discriminator, we employ **multi-period discriminator (MPD)**[^Kong2020HiFi-GAN], **multi-scale discriminator (MSD)**[^Kumar2019MelGAN], and **multi-scale short-time fourier transform discriminator (MS-STFTD)**[^Defossez2022EnCodec] to model higher-level features and improve the perceptual quality of the generated audio.

The discriminator loss follows the **least squares GAN (LSGAN)** formulation[^Mao2017LSGAN], given by:
\[
\mathcal{L}_D(\mathbf{x}, \hat{\mathbf{x}}) = \frac{1}{K} \sum_{k=1}^{K}  (1 - D_k(\mathbf{x}))^2 + D_k^2(\hat{\mathbf{x}})
\]

where \( D_k \) represents the \( k \)-th discriminator (from MPD, MSD, or MS-STFTD), \( K \) is the total number of discriminators, \( \mathbf{x} \) is the ground-truth audio, and \( \hat{\mathbf{x}} \) is the predicted audio.

For the generator loss, we use the same multi-scale mel-spectrogram reconstruction loss as in the pre-training stage, denoted \( \mathcal{L}_{{recon}} \).
Additionally, we include a feature matching loss:
$$
\mathcal{L}_{{feat}}(\mathbf{x}, \hat{\mathbf{x}}) = \frac{1}{KL} \sum_{k=1}^{K} \sum_{l=1}^{L} \frac{ \left\| D_k^l(\mathbf{x}) - D_k^l(\hat{\mathbf{x}}) \right\|_1}{mean(\left\| D_k^l( \mathbf{x}) \right\|_1)}
$$

where \( D_k^l \) denotes the feature representation from the \( l \)-th layer of the \( k \)-th discriminator, \( L \) is the number of layers per discriminator, and the mean is computed over all dimensions of \( D_k^l(\mathbf{x}) \).
We also incorporate an adversarial loss:
\[
\mathcal{L}_{{adv}}(\hat{\mathbf{x}}) = \frac{1}{K} \sum_{k=1}^{K} (1 - D_k(\hat{\mathbf{x}}))^2
\]

The total generator loss is a weighted combination of these terms:
\[
\mathcal{L}_G(\mathbf{x}, \hat{\mathbf{x}}) = \lambda_{{recon}} \mathcal{L}_{{recon}} + \lambda_{{feat}} \mathcal{L}_{feat} + \lambda_{{adv}} \mathcal{L}_{{adv}}
\]

where \( \lambda_{{recon}}, \lambda_{{feat}}, \lambda_{{adv}} \) are hyperparameters that balance the contributions of each loss term.

## 4·实验

### 4.1·设置

#### 数据集与训练细节

We trained XY-Tokenizer using the full **Emilia**[^He2024Emilia] dataset, comprising approximately 101k hours of audio data, equivalent to about 37 million (audio, transcription) pairs.
All audio data was resampled to 16 kHz.
In the **pre-training stage**, audio clips longer than 30 seconds were truncated to the first 30 seconds, while clips shorter than 30 seconds were padded to 30 seconds, with loss computed only on the non-padded portions.
We utilized 32 NVIDIA H100 GPUs, each with a batch size of 4, a maximum learning rate of \( 1 \times 10^{-4} \), and trained for 500,000 steps using **DeepSpeed Zero2**[^Rajbhandari2020ZeRO].
We used the **AdamW optimizer**[^Loshchilov2017AdamW] with weight decay of 0.01.
In the **post-training stage**, we randomly sampled 5-second segments from each audio clip for training, using a single NVIDIA H100 GPU with a batch size of 16.
The generator was trained with a maximum learning rate of \( 1 \times 10^{-5} \), and the discriminator with a maximum learning rate of \( 1 \times 10^{-4} \), for 250,000 steps.
For both the **pre-training stage** and **post-training stage**, we set \(\lambda_{{recon}} = 15\).
In the pre-training stage, we set \(\lambda_{{asr}} = 20\) and \(\lambda_{{commit}} = 1\).
In the post-training stage, we set \(\lambda_{{feat}} = 1\) and \(\lambda_{{adv}} = 1\).

#### 基线模型

We use **SpeechTokenizer**[^Zhang2024SpeechTokenizer], **Mimi**[^Defossez2024Moshi], **XCodec2.0**[^Ye2025LLaSA], and **Baichuan Audio Tokenizer**[^Li2025Baichuan-Audio] as our baseline codecs, which simultaneously model semantic and acoustic information.
Details of these models are provided in Appendix.C.
Additionally, we include **BigCodec**[^Xin2024BigCodec], **Descript Audio Codec**[^Kumar2023DAC], which exclusively model acoustic information.

> In this section, we provide detailed descriptions of the baseline models used in our experiments.
For **Mimi**, we adopt the official RVQ-8 and RVQ-32 versions, referred to as Mimi-8 and Mimi-32, respectively.
For **XCodec 2.0**, **Baichuan Audio Tokenizer**, and **BigCodec**, we use the official checkpoints provided by their respective authors.

> For **Descript Audio Codec (DAC)**, we utilize the official implementation with a sampling rate of 24 kHz and residual vector quantization (RVQ) levels of 2 and 8, denoted as DAC-2 and DAC-8.

> For **SpeechTokenizer**, we employ the [official `speechtokenizer_hubert_avg` version](https://huggingface.co/fnlp/SpeechTokenizer/tree/main/speechtokenizer_hubert_avg).
Additionally, we train three variants of SpeechTokenizer using the official codebase, modifying only the RVQ layers and the distillation weight (`distill_loss_lambda`).
Specifically, we reduce the RVQ layers from 8 to 3 and train three versions:
(1) RVQ-3 with `distill_loss_lambda` = 24 ($5\times$ smaller than the official setting), denoted as SpeechTokenizer-x1,
(2) RVQ-3 with `distill_loss_lambda` = 120 (matching the official setting), denoted as SpeechTokenizer-x2, and
(3) RVQ-3 with `distill_loss_lambda` = 600 ($5\times$ larger than the official setting), denoted as SpeechTokenizer-x3.

### 4.2·指标

#### 重构评估

To evaluate the preservation of acoustic information, we employ several metrics.
Speaker similarity (SIM) is calculated as the cosine similarity between speaker embeddings extracted from original and reconstructed audio using a [pre-trained speaker verification model](https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification).
We also use **short-time objective intelligibility (STOI)**[^Taal2010STOI] to measure speech intelligibility and **perceptual evaluation of speech quality (PESQ)**[^Rix2001PESQ] to assess audio quality.
All evaluations were conducted on the LibriSpeech test-clean subset[^Panayotov2015LibriSpeech].

#### 语义评估

To evaluate the semantic alignment between the codec and text, we employ an **automatic speech recognition (ASR) probing task**, adapted from the **SUPERB framework**[^Yang2021SUPERB], to assess the semantic quality of tokenized representations.
We trained a downstream ASR model using **quantized embeddings**, with the pretrained codec fixed.
These quantized embeddings are upsampled to a minimum frame rate of 50 Hz via replication before being fed into a downstream model (see Appendix.D for details).

> To enable effective alignment in the automatic speech recognition (ASR) probing task, particularly for low-bitrate codecs, **we upsample the embeddings for models with a frame rate below 50 Hz to a minimum of 50 Hz using replication**.
This upsampling is necessary because an insufficient input sequence length (\( T \)) relative to the target sequence length (\( U \)) can prevent the connectionist temporal classification (CTC) loss from effectively aligning the input sequence (quantized features) with the target sequence (transcription characters).
Specifically, CTC requires \( T \geq U \) to accommodate at least one time step per target label, and in the worst case, \( T \geq 2U + 1 \) to account for potential blank labels between each target label and at the sequence boundaries.
Upsampling ensures that \( T \) is sufficiently large, particularly for low-frame-rate codes, to satisfy these constraints and enable effective alignment.

The downstream model comprises a two-layer bidirectional **LSTM**[^Hochreiter1997LSTM] optimized with **CTC**[^Graves2006CTC] loss for character-level prediction.
All models are trained on the LibriSpeech train-clean-100 subset and evaluated on the **LibriSpeech**[^Panayotov2015LibriSpeech] dev-clean subset, using word error rate (WER) as the metric for semantic performance, where a lower WER indicates better semantic alignment.
The ASR probing task experiments are conducted with a batch size of 4, a maximum learning rate of $1\times10^{-4}$, and training for 400,000 steps.

## 5·结果

### 5.1·评估结果

As shown in Table~\ref{table:comparsions_between_ours_and_baselines}, XY-Tokenizer achieves SOTA-comparable performance at similar bitrates, simultaneously excelling in both speech reconstruction and semantic preservation tasks.

**Speech Reconstruction**

XY-Tokenizer achieves higher SIM scores than Mimi-8, SpeechTokenizer-RVQ-3 (including SpeechTokenizer-x1, SpeechTokenizer-x2, SpeechTokenizer-x3), and Baichuan Audio Tokenizer, with performance comparable to BigCodec, XCodec2.0, and SpeechTokenizer.
These results provide preliminary evidence of the model's effectiveness in reconstructing speech.
However, XY-Tokenizer's reconstruction quality is slightly inferior to high-bitrate codecs like DAC-8, likely due to greater information loss in low-bitrate codecs.
We believe that low-bitrate codecs have significant potential for further enhancement in reconstruction performance.

**ASR Probing Results**

XY-Tokenizer exhibits text alignment performance comparable to Baichuan Audio Tokenizer.
XY-Tokenizer's word error rate (WER) on the ASR probing task is substantially lower than that of codecs employing representation distillation, such as SpeechTokenizer, MimiCodec, and XCodec 2.0.
This performance may be attributed to language-based ASR tasks facilitating stronger text alignment compared to semantic distillation approaches, while also creating less conflict with reconstruction objectives.
Conversely, codecs like DAC and BigCodec, which lack supervised text alignment during pretraining, exhibit higher WER in ASR probing tasks, likely due to a significant disparity between their compressed representations and textual content.

**Semantic-Acoustic Comprehensive Analysis**

Considering both speech reconstruction and text alignment capabilities, our proposed XY-Tokenizer achieves excellent results in both aspects.
While SpeechTokenizer performs well in both reconstruction and semantic tasks at higher bitrates, we observe that at lower bitrates, stronger distillation supervision leads to poorer reconstruction quality.
This indicates that representation distillation methods can cause significant conflicts between semantic and acoustic learning objectives during codec training.
Mimi-8 and XCodec2.0 demonstrate better reconstruction metrics at comparable bitrates than SpeechTokenizer, but perform less effectively on semantic tasks, which we attribute to the inherent trade-off between representation distillation and audio reconstruction objectives.
Our proposed XY-Tokenizer achieves favorable results in both semantic and acoustic dimensions, suggesting that it effectively mitigates the conflict between these competing tasks to a noticeable extent.
For ablation studies on conflict resolution approaches, please refer to Section~\ref{ablation_study}.

### 5.2·消融实验

To evaluate the effectiveness of our proposed XY-Tokenizer in simultaneously modeling semantic and acoustic features while mitigating conflicts between these tasks, we conducted a series of ablation experiments.
Unless otherwise specified, all ablation experiments were performed during the pre-training phase without a post-training stage, utilizing the same dataset and preprocessing methods as described in Section~\ref{exps:datasets_and_training_details}.
We employed a global batch size of 128, a maximum learning rate of $1 \times 10^{-4}$, and trained for 200,000 steps, with all loss weights consistent with those outlined in Section~\ref{exps:datasets_and_training_details}.

**Shared Parameters Cause Conflicts**

In XY-Tokenizer, parameters between semantic and acoustic tasks are shared only in the residual vector quantization (RVQ) module and its adjacent components.
To assess the effectiveness of minimizing shared parameters in reducing semantic-acoustic conflicts, we trained three models: (1) the proposed XY-Tokenizer, (2) XY-Tokenizer without a semantic encoder, where the encoder is a single-channel, trainable acoustic encoder, and (3) XY-Tokenizer without a semantic encoder, with the acoustic encoder initialized using Whisper-medium weights, maintaining a parameter count comparable to (1).
In models (2) and (3), the semantic and acoustic tasks share both the encoder and RVQ modules.
As shown in Table~\ref{table:ablation_of_shared_params}, all three models perform well on the ASR probing task, demonstrating that leveraging an LLM-based ASR approach enables the codec to effectively align with textual semantics.
However, models (2) and (3) exhibit inferior performance on reconstruction metrics compared to (1), despite model (3) having a similar parameter count to (1).
This suggests that sharing model parameters across semantic and acoustic tasks is a primary cause of task conflict.
Thus, reducing shared parameters is an effective strategy for simultaneously achieving high-quality audio reconstruction and improved text alignment.

**Fixing LLM for Enhanced Training Stability**

In the pre-training stage of our XY-Tokenizer, the pretrained LLM is kept fixed.
To explore whether allowing the LLM to be trainable could yield better results, we trained two models: (1) the proposed XY-Tokenizer with a fixed LLM, and (2) XY-Tokenizer with a trainable LLM.
Table~\ref{table:ablation_of_llm_trainable_or_not} indicates that both models achieve comparable audio reconstruction quality.
On the Word Error Rate (WER) of text decoded by the LLM, model (2) outperforms model (1) with a lower WER.
However, on the ASR probing task, model (2) performs worse than model (1).
Moreover, as training progresses, the performance of model(2) on the ASR probing task deteriorates, while model (1) remains stable.
We hypothesize that allowing the LLM to be trainable increases the flexibility of the semantic decoder, causing the encoder's text-alignment capability to gradually shift toward the semantic decoder.
This shift likely explains the discrepancy between the ASR probing task and LLM decoder WER performance in models (1) and (2).
We conclude that fixing the LLM is a more appropriate approach, offering stability and effectively balancing semantic and acoustic modeling.

### 5.3·附加实验

#### Training from Whisper for Reduced Training Complexity

To investigate the role of pre-trained whisper weights in mitigating semantic-acoustic conflicts, we trained two models: (1) a model with an acoustic encoder initialized with whisper-small weights, without a semantic encoder, and (2) the same model architecture as (1), but without loading whisper-small weights.
As shown in Table~\ref{table:ablation_of_loading_whisper_or_not}, model (1) outperforms model (2) in the ASR probing task.
This result highlights that whisper, pre-trained on extensive supervised data, is effective in alleviating semantic-acoustic conflicts in speech codec.

#### Importance of LLM-Based ASR Task in Fine-Tuning Whisper

In the pre-training stage, we adopt a multi-task learning approach to jointly model semantic and acoustic information.
Specifically, we leverage a language model-based automatic speech recognition (ASR) task to align the codec's quantized representations with text, while the codec's decoder preserves paralinguistic information.
The encoder is initialized with pretrained Whisper weights and fine-tuned during training.
Given that Whisper is pretrained on ASR tasks and demonstrates strong text alignment, we investigate whether the LLM-based ASR task further enhances the encoder's alignment with text.
To this end, we conduct the following ablation study.

We train two models:
(1) A modified version of our proposed XY-Tokenizer, where the encoder includes only the acoustic channel and omits the semantic channel, with all other components unchanged;
(2) A variant of (1) that excludes the LLM-based ASR supervision, relying solely on reconstruction loss and commitment loss.
As shown in Table~\ref{table:ablation_of_adding_asr_task_or_not}, both models exhibit strong audio reconstruction capabilities.
However, the word error rate (WER) on the ASR probing task reveals that model (2) performs significantly worse in text alignment compared to model.
These results underscore the critical role of the LLM-based ASR task in enhancing text alignment, enabling XY-Tokenizer to optimize both text alignment and audio reconstruction effectively when combined with the reconstruction task.

## 6·结论

In this study, we propose XY-Tokenizer, a novel codec that mitigates the conflict between understanding and generation capabilities through multi-task learning, enhances reconstruction quality, and improves text alignment at low bitrates.
Experimental results demonstrate that XY-Tokenizer excels in both speech reconstruction and understanding tasks.
We conducted ablation studies to optimize the modeling of semantic and acoustic information.
Please refer to the appendix for additional experimental details.

### 局限性

In this paper, we propose XY-Tokenizer, a codec designed to model both semantic and acoustic information while effectively mitigating conflicts between these tasks.
Experimental results demonstrate that XY-Tokenizer achieves strong performance in audio reconstruction quality and text alignment.
However, several limitations remain.
First, achieving lower bitrates without compromising performance remains a challenge.
Additionally, the scaling law for training speech codecs, particularly with respect to parameter count and dataset size, requires further investigation to optimize training efficiency and generalization.

## 参考文献

[^Achiam2023GPT-4]: [GPT-4](../TextLM/2023.03.15_GPT-4.md)
[^Yang2024Qwen2.5]: [Qwen2.5](../TextLM/2024.12.19_Qwen2.5.md)
[^Zhang2023SpeechGPT]: [SpeechGPT](../SpeechLM/Interaction/2023.05.18_SpeechGPT.md)
[^Chu2024Qwen2]: [Qwen2](../TextLM/2024.07.15_Qwen2.md)
[^Defossez2024Moshi]: [Moshi](../SpeechLM/Interaction/2024.09.17_Moshi.md)
[^Zeghidour2021SoundStream]: [SoundStream](2021.07.07_SoundStream.md)
[^Defossez2022EnCodec]: [EnCodec](2022.10.24_EnCodec.md)
[^Kumar2023DAC]: [DAC](2023.06.11_Descript-Audio-Codec.md)
[^Zhang2024SpeechTokenizer]: [SpeechTokenizer](../SpeechCodec/2023.08.31_SpeechTokenizer.md)
[^Xin2024BigCodec]: [BigCodec](../SpeechCodec/2024.09.09_BigCodec.md)
[^Wang2023VALL-E]: [VALL-E](../SpeechLM/ST2S/2023.01.05_VALL-E.md)
[^Devlin2019BERT]: [BERT](../TextLM/2018.10.11_BERT.md)
[^Hsu2021HuBERT]: [HuBERT](../SpeechRepresentation/2021.06.14_HuBERT.md)
[^Chung2021W2V-BERT]: [W2V-BERT](../SpeechRepresentation/2021.08.07_W2V-BERT.md)
[^Chen2022WavLM]: [WavLM](../SpeechRepresentation/2021.10.26_WavLM.md)
[^Radford2022Whisper]: [Whisper](../ASR/2022.12.06_Whisper.md)
[^Zeng2024GLM-4-Voice]: [GLM-4-Voice](../SpeechLM/Interaction/2024.12.03_GLM-4-Voice.md)
[^Ding2025Kimi-Audio]: [Kimi-Audio](../SpeechLM/2025.04.25_Kimi-Audio.md)
[^Borsos2023AudioLM]: [AudioLM](../SpeechLM/PureSpeechLM/2022.09.07_AudioLM.md)
[^Ye2024X-Codec]: [X-Codec](../SpeechCodec/2024.08.30_X-Codec.md)
[^Goodfellow2014GAN]: [GAN](../_Basis/2014.06.10_GAN.md)
[^Zhang2024IntrinsicVoice]: [IntrinsicVoice](../SpeechLM/Interaction/2024.10.09_IntrinsicVoice.md)
[^Tang2022Unified]: [Unified Speech-Text Pre-Training for Speech Translation and Recognition](../_Full/2022.04.11_Unified_Speech-Text_Pre-Training_for_Speech_Translation_and_Recognition.md)
[^Ao2022SpeechT5]: [SpeechT5](../SpeechLM/ST2ST/2021.10.14_SpeechT5.md)
[^Latif2023Survey20230824]: [Sparks of Large Audio Models: A Survey and Outlook](../../Surveys/S20230824.md)
[^Wu2024Survey20240220]: [Towards Audio Language Modeling - An Overview](../../Surveys/S20240220.md)
[^Ji2024Survey20241115]: [WavChat](../../Surveys/S20241115.md)
[^Zhang2024SpeechGPT-Gen]: [SpeechGPT-Gen](../SpeechLM/Interaction/2024.01.24_SpeechGPT-Gen.md)
[^Li2025Baichuan-Audio]: [Baichuan-Audio/Ocean-Omni/Baichuan-Omni](../SpeechLM/Interaction/2024.10.11_Ocean-Omni.md)
[^Lipman2022FM]: [Flow Matching](../FlowMatching/_2022.10.06_Flow_Matching.md)
[^Kong2020HiFi-GAN]: [HiFi-GAN](../Vocoder/2020.10.12_HiFi-GAN.md)
[^Yang2024UniAudio1.5]: [UniAudio1.5](../SpeechLM/2024.06.14_UniAudio1.5.md)
[^Vaswani2017Transformer]: [Transformer](../_Transformer/2017.06.12_Transformer.md)
[^Hendrycks2016GELU]: [GELU](../../Modules/Activation/GELU.md)
[^Siuzdak2023Vocos]: [Vocos](../Vocoder/2023.03.01_Vocos.md)
[^Kumar2019MelGAN]: [MelGAN](../Vocoder/2019.10.08_MelGAN.md)
[^Mao2017LSGAN]: [LSGAN](../_Basis/2016.11.13_LSGAN.md)
[^He2024Emilia]: [Emilia](../../Datasets/2024.07.07_Emilia.md)
[^Rajbhandari2020ZeRO]: [ZeRO](../_Basis/2019.10.04_ZeRO.md)
[^Loshchilov2017AdamW]: [AdamW](../../Modules/Optimization/2017.11.14_AdamW.md)
[^Ye2025LLaSA]: [LLaSA/X-Codec2.0](../SpeechLM/2025.02.06_LLaSA.md)
[^Taal2010STOI]: [STOI](../../Evaluations/2010.03.14_STOI.md)
[^Rix2001PESQ]: [PESQ](../../Evaluations/PESQ.md)
[^Panayotov2015LibriSpeech]: [LibriSpeech](../../Datasets/2015.04.19_LibriSpeech.md)
[^Yang2021SUPERB]: [SUPERB](../../Evaluations/2021.05.03_SUPERB.md)
[^Hochreiter1997LSTM]: [LSTM](../_Basis/2014.02.05_LSTM.md)
[^Graves2006CTC]: [CTC](../ASR/CTC.md)
