# XY-Tokenizer

<details>
<summary>基本信息</summary>

- 标题: "XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs"
- 作者:
  - 01 Yitian Gong
  - 02 Luozhijie Jin
  - 03 Ruifan Deng,
  - 04 Dong Zhang,
  - 05 Xin Zhang,
  - 06 Qinyuan Cheng,
  - 07 Zhaoye Fei,
  - 08 Shimin Li,
  - 09 Xipeng Qiu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2506.23325)
  - [Publication]()
  - [Github](https://github.com/gyt1145028706/XY-Tokenizer)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2025.07.09_2506.23325v2__XY-Tokenizer__Mitigating_the_Semantic-Acoustic_Conflict_in_Low-Bitrate_Speech_Codecs.pdf)
  - [Publication] #TODO

</details>

## 摘要

<!--
Speech codecs serve as bridges between speech signals and large language models.
An ideal codec for speech language models should not only preserve acoustic information but also capture rich semantic information.
However, existing speech codecs struggle to balance high-quality audio reconstruction with ease of modeling by language models.
In this study, we analyze the limitations of previous codecs in balancing semantic richness and acoustic fidelity.
We propose ***XY-Tokenizer***, a novel codec that mitigates the conflict between semantic and acoustic capabilities through multi-stage, multi-task learning.
Experimental results demonstrate that ***XY-Tokenizer*** achieves performance in both semantic and acoustic tasks comparable to that of state-of-the-art codecs operating at similar bitrates, even though those existing codecs typically excel in only one aspect.
Specifically, ***XY-Tokenizer*** achieves strong text alignment, surpassing distillation-based semantic modeling methods such as **SpeechTokenizer** and **Mimi**, while maintaining a speaker similarity score of 0.83 between reconstructed and original audio.
The reconstruction performance of ***XY-Tokenizer*** is comparable to that of **BigCodec**, the current state-of-the-art among acoustic-only codecs, which achieves a speaker similarity score of 0.84 at a similar bitrate.
Code and models are available at [this https URL](https://github.com/gyt1145028706/XY-Tokenizer).
-->

语音编解码器作为语音信号和大语言模型之间的桥梁.
语音语言模型的理想编解码器不仅应该保留声学信息, 还应该捕获丰富的语义信息.
然而, 现有的语音编解码器难以平衡高质量音频重构和语言模型建模便利性.

在本研究中, 我们分析了以前的编解码器在平衡语义丰富性和声学保真度方面的限制.
我们提出了 ***XY-Tokenizer***, 一种新型的编解码器, 通过多阶段多任务学习来缓解语义和声学能力之间的冲突.

实验结果表明 ***XY-Tokenizer*** 在语义和声学任务上都达到了与当前最先进的声学编解码器相当的性能, 即使这些现有的编解码器在同等比特率下通常只能达到单个方面.
具体而言, ***XY-Tokenizer*** 实现了强大的文本对齐, 超越基于蒸馏的语义建模方法, 比如 **SpeechTokenizer** 和 **Mimi**, 同时保持了重构音频和原始音频之间的说话人相似度得分为 0.83.
***XY-Tokenizer*** 的重构性能与 **BigCodec**, 目前仅能进行声学编码的编解码器, 达到了类似的说话人相似度得分, 即 0.84.
代码和模型可在[此处](https://github.com/gyt1145028706/XY-Tokenizer)获得.

## 1·引言

In recent years, large language models (LLMs)~\citep{achiam2023gpt, yang2024qwen2} have achieved significant advancements in natural language processing, showcasing remarkable capabilities in understanding and generating text for fluent and natural conversations.
Consequently, speech large language models (Speech LLMs) have garnered increasing attention~\citep{zhang2023speechgpt,chu2024qwen2,defossez2024moshi}.
A critical component of Speech LLMs is the speech codec, which transforms continuous speech signals into discrete tokens, aligning with the token-based approach of LLMs~\citep{zeghidour2021soundstream, defossez2022high, kumar2023highfidelityaudiocompressionimproved, zhang2024speechtokenizerunifiedspeechtokenizer, defossez2024moshi}.
Acoustic codecs~\citep{defossez2022high,kumar2023highfidelityaudiocompressionimproved,xin2024bigcodec} trained through residual vector quantization GAN (RVQ-GAN) capture the details of the audio waveform and allow for high-quality synthesis~\citep{wang2023neural}.
Self-supervised learning (SSL) models trained with mask language modeling (MLM)~\citep{devlin2019bert} capture contextual dependencies in speech, making them widely used in Speech LLMs~\citep{hsu2021hubertselfsupervisedspeechrepresentation,chung2021w2v,Chen_2022}.
Additionally, automatic speech recognition (ASR) models trained on large-scale supervised datasets~\citep{radford2022robustspeechrecognitionlargescale} align well with the text modality, and their discrete representations are often utilized as inputs for Speech LLMs~\citep{ zeng2024glm, ding2025kimi}.

Semantic tokens, typically derived from discretized self-supervised learning (SSL) models, are considered to exhibit high alignment with text while leading to poor reconstruction.
In contrast, acoustic tokens  often derived from speech codecs trained through residual vector quantization GAN (RVQ-GAN), are recognized for capturing the details of the audio waveform, enabling high-quality synthesis, but they do not demonstrate strong alignment with text~\citep{borsos2023audiolm}.
An ideal speech codec should effectively model both semantic and acoustic information.
SpeechTokenizer~\citep{zhang2024speechtokenizerunifiedspeechtokenizer} employs semantic distillation, utilizing the output of the first layer of residual vector quantization (RVQ) to distill representations from a teacher SSL model.
Similarly, the Mimi codec in Moshi~\citep{defossez2024moshi} adopts a split residual vector quantization architecture, distilling one channel's output with a pretrained SSL model.
XCodec introduces an **X-shaped** structure, ensuring that tokens at each layer are semantically rich~\citep{ye2024codecdoesmatterexploring}.
However, a key challenge in modeling both semantic and acoustic information lies in the inherent conflict between these tasks, particularly at low bitrates, where achieving high performance in both remains difficult.

In this work, we propose ***XY-Tokenizer***, the first speech codec to successfully model both semantic and acoustic information effectively at low bitrates.
Our codec employs a dual-tower architecture that mitigates the conflict between semantic and acoustic tasks by minimizing shared parameters in a multi-task learning framework.
We introduce a multi-stage, multi-task training paradigm: the first stage aligns the codec with text using an LLM-based ASR approach and employs a reconstruction loss on the original speech signal to ensure coarse-grained audio reconstruction, utilizing a 2-channel encoder-decoder structure, forming an **X-shaped** architecture.
The second stage incorporates a discriminator to model fine-grained audio features using a generative adversarial network (GAN)~\citep{goodfellow2014generative}, where the decoder discards the text-alignment module, resulting in a **Y-shaped** architecture.

Our contributions can be summarized as follows:
- We propose ***XY-Tokenizer***, a speech codec with a 16kHz sampling rate and a 1kbps bitrate.
It employs a dual-tower architecture to model semantic and acoustic information simultaneously through multi-task learning, aligns with text using an LLM-based automatic speech recognition approach, and ensures high-quality speech reconstruction via a codec decoder.
- We introduce a multi-stage, multi-task training paradigm for modeling semantic and acoustic information concurrently.
The first stage aligns the codec with text and models coarse-grained audio features, while the second stage incorporates a discriminator to model fine-grained audio features using a generative adversarial network.
- We analyze the limitations of current speech codecs, particularly the inherent conflict between semantic and acoustic objectives, and propose solutions such as leveraging pretrained automatic speech recognition models and minimizing shared parameters to mitigate these conflicts.
- ***XY-Tokenizer*** achieves performance at 1kbps comparable to the 4kbps codec that simultaneously models semantic and acoustic information (e.g., **SpeechTokenizer**) in both semantic and acoustic dimensions.
At similar low bitrates, it excels in both tasks and matches the performance of state-of-the-art codecs specialized in a single aspect, such as **BigCodec**, which models only acoustic quality without explicitly modeling semantic information.
We conducted extensive ablation studies to validate the effectiveness of our approach, and we will open-source our repository and pretrained models.

## 2·背景

Self-supervised learning (SSL) models for speech, such as those trained with masked language modeling~\citep{hsu2021hubertselfsupervisedspeechrepresentation,Chen_2022}, effectively capture high-level speech features and are widely used in speech large language models (Speech LLMs)~\citep{zhang2023speechgpt, zhang2024intrinsicvoice}.
Similarly, automatic speech recognition (ASR) models, trained on large-scale supervised datasets of paired speech and transcripts, achieve \textbf{strong alignment between speech and text modalities}~\citep{radford2022robustspeechrecognitionlargescale,tang2022unifiedspeechtextpretrainingspeech,ao2022speecht5unifiedmodalencoderdecoderpretraining}.
However, training a speech codec from scratch to align with the text modality is data-intensive.
To address this, our proposed \textbf{XY-Tokenizer} leverages pretrained ASR or SSL models for the encoder to reduce training complexity.
Although these ASR and SSL models exhibit strong alignment with text, their ability to retain paralinguistic information remains unexplored.
To identify the most suitable pretrained model for our codec, we conduct a preliminary experiment to evaluate their performance in preserving acoustic information.

For this experiment, we selected three pre-trained models: Whisper~\citep{radford2022robustspeechrecognitionlargescale}, an ASR model, as well as HuBERT~\citep{hsu2021hubertselfsupervisedspeechrepresentation} and WavLM~\citep{Chen_2022}, which are self-supervised learning (SSL) models.
We trained an auto-encoder, distinct from a codec in that it lacks a quantizer, to assess the reconstruction capabilities of these pre-trained models.
Specifically, we used the pre-trained Whisper, HuBERT, and WavLM models as \textbf{fixed encoders}, each paired with a decoder of identical parameter size to ensure a fair comparison.
The experimental setup and details are provided in Appendix~\ref{appendix:encoder_chosen_exp_settings}.
As shown in Table~\ref{table:preliminary_experiment_results}, Whisper achieves a superior reconstruction performance, effectively preserving paralinguistic information, such as speaker timbre and acoustic details.
In contrast, HuBERT and WavLM exhibit limitations in retaining certain aspects of speaker timbre and fine-grained acoustic details.
Furthermore, Whisper's pretraining on ASR tasks aligns closely with the LLM-based tasks employed in our codec, facilitating better text-speech alignment.
Based on these findings, we selected Whisper to initialize the encoder of our proposed XY-Tokenizer and further fine-tuned it for our codec training pipeline.

## 3·方法

### 3.1·XY-Tokenizer

#### 动机

An ideal speech codec should effectively balance two goals: high-fidelity audio reconstruction and strong semantic alignment with text~\citep{zhang2024speechtokenizerunifiedspeechtokenizer, yang2024uniaudio15largelanguage}.
However, these two objectives often conflict, as optimizing for one can degrade the other~\citep{defossez2024moshi}.
Our empirical analysis, as shown in Table~\ref{table:empirical_analysis_of_shared_params}, shows that decreasing the number of shared parameters between semantic and acoustic modeling pathways effectively mitigates the trade-off between high-fidelity audio reconstruction and strong semantic alignment.
Moreover, semantic modeling can be effectively approached through automatic speech recognition (ASR) tasks, while acoustic modeling aligns closely with reconstruction through a codec decoder.
To this end, we propose a dual-channel codec architecture that jointly models semantic and acoustic information in a multi-task setup, combining ASR and audio reconstruction, with shared parameters limited to the residual vector quantization (RVQ) module and its adjacent components.

#### 编码器

The encoder comprises two parallel branches: a \textbf{semantic channel} and an \textbf{acoustic channel}, both processing mel-spectrogram inputs at 100 Hz.
Each channel is initialized with a whisper encoder~\citep{radford2022robustspeechrecognitionlargescale}, with the semantic encoder's parameters fixed and the acoustic encoder's parameters trainable.
The semantic channel extracts linguistic features, while the acoustic channel captures paralinguistic information.
The outputs of both channels are concatenated and further processed to produce the final encoder output.
Additional details of the encoder architecture are provided in Appendix~\ref{appendix:model_details}.

#### 量化器

We employ a residual vector quantization (RVQ) module~\citep{zeghidour2021soundstream} with 8 layers operating on the encoder output at a temporal resolution of 12.5 Hz.
Each layer uses a codebook of size 1024, resulting in a total bitrate of 1 kbps.
The quantizer is integrated with adapter and convolution modules, with details provided in Appendix~\ref{appendix:model_details}.

#### 解码器

The decoder consists of two parallel branches: a \textbf{semantic channel} and an \textbf{acoustic channel}, both processing the quantized encoder output.
The semantic channel, a decoder-only large language model, generates text transcriptions, while the acoustic channel reconstructs the waveform.
For details of decoder, see Appendix~\ref{appendix:model_details}.

### 3.2·两阶段训练策略

To streamline the training process and enhance efficiency, we propose a two-stage training strategy, consisting of a pre-training stage and a post-training stage.
In the pre-training stage, we employ multi-task learning to simultaneously model semantic features and coarse acoustic features.
In the post-training stage, we focus on modeling fine-grained acoustic features.
This section elaborates on these stages.

#### 预训练阶段


In the pre-training stage, we focus on two tasks: audio reconstruction and automatic speech recognition (ASR).
All model parameters are trainable, except for the weights of the semantic encoder, initialized from whisper encoder~\citep{radford2022robustspeechrecognitionlargescale}, and the large language model (LLM) which is initialized from Qwen2.5~\citep{yang2024qwen2}.
To align with text generation, we use the cross-entropy loss for the LLM, defined as:
\[
\mathcal{L}_{{asr}} = -\sum_{t=1}^N \log p(\mathbf{y}_t \mid \mathbf{y}_{<t}, \mathbf{f}; \theta_{{LLM}})
\]
where \(\mathbf{y}_t\) is the predicted text token at time step \( t \), \(\mathbf{y}_{<t}\) denotes the sequence of preceding tokens, \( \mathbf{f} \) represents the audio features input to the LLM, \( N \) is the total number of predicted text tokens, and \( \theta_{{LLM}} \) denotes the parameters of the LLM.

For modeling acoustic features, we employ a multi-scale mel-spectrogram reconstruction loss:
\[
\mathcal{L}_{{recon}} = \sum_{i \in e} \left\| S_i(\mathbf{x}) - S_i(\mathbf{\hat{x}}) \right\|_1
\]
where \( S_i \) is the mel-spectrogram at scale \( i \), computed using a normalized short-time fourier transform (STFT) with a window size of \( 2^i \) and a hop length of \( 2^{i} / 4 \).
The set of scales is defined as \( e = \{5, \dots, 11\} \).
Here, \( \mathbf{x} \) is the ground-truth audio waveform, and \( \mathbf{\hat{x}} \) is the predicted waveform from the acoustic decoder.
No waveform-based reconstruction loss is used.

Additionally, we incorporate a commitment loss to ensure effective quantization:
\[
\mathcal{L}_{{commit}} = \sum_{i=1}^{N_q} \left\| \mathbf{z}_i - sg(q_i(\mathbf{z}_i)) \right\|_1
\]
where \( \mathbf{z}_i \) is the input to the \( i \)-th layer of the quantizer, \( q_i(\mathbf{z}_i) \) is its quantized output, \( N_q \) is the number of quantized vectors, and \( sg \) denotes the stop-gradient operation, which prevents gradients from propagating to the quantizer's codebook.

The total loss for the pre-training stage is a weighted combination of individual losses:
\[
\mathcal{L}_{{pretraining}} = \lambda_{{asr}} \mathcal{L}_{{asr}} + \lambda_{{recon}} \mathcal{L}_{{recon}} + \lambda_{{commit}} \mathcal{L}_{{commit}}
\]
where \( \lambda_{{asr}}, \lambda_{{recon}}, \lambda_{{commit}} \) are hyperparameters that balance the weights of each loss term.

#### 后训练阶段

After the pre-training stage, we obtain an encoder capable of producing rich semantic features.
However, the codec's output may contain artifacts, which significantly degrade perceptual quality and listening experience.
To address this, the post-training stage focuses on modeling fine-grained audio details.
The approach is detailed below.

We adopt a generative adversarial network (GAN) framework for post-training.
For the generator, which corresponds to the codec, we fix the encoder and quantizer, discard the semantic decoder used in the pre-training stage, and keep all other parameters consistent with the pre-training stage, with these parameters remaining trainable.
For the discriminator, we employ multi-period discriminator (MPD)~\citep{kong2020hifi}, multi-scale discriminator (MSD)~\citep{kumar2019melgan}, and multi-scale short-time fourier transform discriminator (MS-STFTD)~\citep{defossez2022high} to model higher-level features and improve the perceptual quality of the generated audio.

The discriminator loss follows the least squares GAN (LSGAN) formulation~\citep{mao2017least}, given by:
\[
\mathcal{L}_D(\mathbf{x}, \hat{\mathbf{x}}) = \frac{1}{K} \sum_{k=1}^{K}  (1 - D_k(\mathbf{x}))^2 + D_k^2(\hat{\mathbf{x}})
\]
where \( D_k \) represents the \( k \)-th discriminator (from MPD, MSD, or MS-STFTD), \( K \) is the total number of discriminators, \( \mathbf{x} \) is the ground-truth audio, and \( \hat{\mathbf{x}} \) is the predicted audio.

For the generator loss, we use the same multi-scale mel-spectrogram reconstruction loss as in the pre-training stage, denoted \( \mathcal{L}_{{recon}} \).
Additionally, we include a feature matching loss:
\begin{equation}
\mathcal{L}_{{feat}}(\mathbf{x}, \hat{\mathbf{x}}) = \frac{1}{KL} \sum_{k=1}^{K} \sum_{l=1}^{L} \frac{ \left\| D_k^l(\mathbf{x}) - D_k^l(\hat{\mathbf{x}}) \right\|_1}{mean(\left\| D_k^l( \mathbf{x}) \right\|_1)}
\label{eq:feat}
\end{equation}
where \( D_k^l \) denotes the feature representation from the \( l \)-th layer of the \( k \)-th discriminator, \( L \) is the number of layers per discriminator, and the mean is computed over all dimensions of \( D_k^l(\mathbf{x}) \).
We also incorporate an adversarial loss:
\[
\mathcal{L}_{{adv}}(\hat{\mathbf{x}}) = \frac{1}{K} \sum_{k=1}^{K} (1 - D_k(\hat{\mathbf{x}}))^2
\]

The total generator loss is a weighted combination of these terms:
\[
\mathcal{L}_G(\mathbf{x}, \hat{\mathbf{x}}) = \lambda_{{recon}} \mathcal{L}_{{recon}} + \lambda_{{feat}} \mathcal{L}_{feat} + \lambda_{{adv}} \mathcal{L}_{{adv}}
\]
where \( \lambda_{{recon}}, \lambda_{{feat}}, \lambda_{{adv}} \) are hyperparameters that balance the contributions of each loss term.

## 4·实验

## 5·结果

## 6·结论

## 参考文献