# HuBERT

<details>
<summary>基本信息</summary>

- 标题: "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units"
- 作者:
  - 01 Wei-Ning Hsu
  - 02 Benjamin Bolte
  - 03 Yao-Hung Hubert Tsai
  - 04 Kushal Lakhotia
  - 05 Ruslan Salakhutdinov
  - 06 Abdelrahman Mohamed
- 链接:
  - [ArXiv](https://arxiv.org/abs/2106.07447)
  - [Publication](https://doi.org/10.1109/TASLP.2021.3122291)
  - [Github](https://github.com/facebookresearch/fairseq/tree/main/examples/hubert)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2106.07447v1__HuBERT__Self-Supervised_Speech_Representation_Learning_by_Masked_Prediction_of_Hidden_Units.pdf)
  - [Publication](_PDF/2106.07447p0__HuBERT__TASLP2021.pdf)

</details>

## Abstract: 摘要

<table><tr><td width="50%">

Self-supervised approaches for speech representation learning are challenged by three unique problems:
(1) there are multiple sound units in each input utterance,
(2) there is no lexicon of input sound units during the pre-training phase,
(3) sound units have variable lengths with no explicit segmentation.

To deal with these three problems, we propose the ***Hidden-Unit BERT (HuBERT)*** approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss.
A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs.
***HuBERT*** relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels.
Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the ***HuBERT*** model either matches or improves upon the state-of-the-art **Wav2Vec2.0** performance on the **LibriSpeech** (960h) and **Libri-Light** (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets.
Using a 1B parameter model, ***HuBERT*** shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.

OpenSource: https://github.com/pytorch/fairseq/tree/master/examples/hubert

This manuscript is an extended version of [^01].

</td><td>

用于语音表示学习的自监督方法面临三个独特的问题:
1.
每个输入发言中有多个声学单元;
2.
在预训练阶段没有输入的声学单元的词典;
3.
声学单元具有可变长度, 没有明确的分割.

为了处理这三个问题, 本文提出了 ***Hidden-Unit BERT (HuBERT)*** 方法用于自监督语音表示学习, 它利用离线聚类步骤为类 BERT 预测损失提供对齐的目标标签.
这一方法的关键组件是只对掩膜区域应用预测损失, 使得模型能够在连续输入上学习一个声学和语言相结合的模型.

***HuBERT*** 主要依赖于无监督聚类步骤的一致性, 而不是分配的聚类标签的内在质量.
以 100 个聚类教师开始, 使用两次聚类, ***HuBERT*** 模型可以匹配或超过 **Wav2Vec2.0** 在 **LibriSpeech** (960h) 和 **Libri-Light** (60,000h) 基准上分别进行 10min, 1h, 10h, 100h, 和 960h 微调子集的性能.
使用 1B 参数模型, ***HuBERT*** 在更具挑战性的 dev-other 和 test-other 评估子集上分别展示了 19% 和 13% 的相对 WER 降低.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

The north star for many research programs has been learning speech and audio representations through listening and interaction, similar to how babies learn their first language.
High fidelity speech representation includes disentangled aspects of the spoken content along with non-lexical information of how it is delivered, e.g., speaker identity, emotion, hesitation, interruptions.
Furthermore, reaching a complete situational understanding requires modeling structured noise interleaving and overlapping with the speech signal, e.g., laughter, coughing, lip-smacking, background vehicle engine, birds chirping, or food sizzling sounds.

</td><td>

</td></tr>
<tr><td>

The need for such high-fidelity representations drove research in self-supervised learning for speech and audio where the targets driving the learning process of a designed pretext task are drawn from the input signal itself.
Examples of pretext tasks for self-supervised speech representation learning include distinguishing near-by features from temporally distant ones \cite{[^02],[^03],[^04]}, next-step prediction of audio features \cite{[^05]}, masked prediction of audio features given unmasked context \cite{[^06],[^07]}.
Besides, self-supervised learning methods do not rely on any linguistic resources during training, allowing them to learn universal representations since labels, annotations, and text-only material ignores rich information in the input signal.

</td><td>

</td></tr>
<tr><td>

Learning speech representations without reliance on large volumes of labeled data is crucial for industrial applications and products with ever-increasing coverage of new languages and domains.
The time needed to collect large labeled datasets covering each of these scenarios is the real bottleneck in the current fast-moving AI industry, with time-to-market playing a critical role for product success.
Building more inclusive applications covering spoken-only dialects and languages is another significant benefit of reducing dependence on linguistic resources.
Given their non-standard orthographic rules, many of these languages and dialects have very little or no resources at all.

</td><td>

</td></tr>
<tr><td>

Pseudo-labeling (PL), also known as self-training and belongs to the family of semi-supervised learning techniques, has been the dominant approach for utilizing unlabeled speech and audio with successful applications dating back to the mid-1990s \cite{Zavaliagkos_98, ma_bbn_06, kahn2020self, hsu2020semi}.
PL starts with some supervised data to train a "teacher" model in one specific downstream task.
Pseudo-labels are then generated for the unlabeled data using the teacher model.
Next, a student model is trained using the combined supervised and teacher-labeled data either using the standard cross-entropy \cite{kahn2020self} loss or using a contrastive loss \cite{xiao2021contrastive} to account for noise in teacher-generated labels.
The pseudo-labeling process may be repeated multiple times to improve teacher label quality \cite{xu2020iterative} iteratively.

</td><td>

</td></tr>
<tr><td>

Without discounting the immense success of pseudo-labeling techniques, self-supervised representations offer two unique advantages:
(1) Pseudo-label methods force student models to merely mimic a teacher model, which is limited by its supervised data size and the provided annotation quality.
On the other hand, self-supervised pretext tasks force the model to represent the entire input signal by compressing much more bits of information into the learned latent representation.
(2) In pseudo-labeling, the supervised data of the teacher model forces the whole learning to be geared towards a single downstream task.
On the contrary, self-supervised features show better generalization to a multitude of downstream applications.

</td><td>

</td></tr>
<tr><td>

There have been impressive successes for self-supervised learning in Computer Vision (CV) \cite{caron2020Swav, Chen2020SimSiam, grill2020byol} and Natural Language Processing (NLP) \cite{brown2020gpt3, liu2019roberta, lewis2019bart} applications.
Learning representations of discrete input sequences, such as in Natural Language Processing (NLP) applications, uses either masked prediction \cite{devlin2018bert, clark2020electra} or auto-regressive generation \cite{peters2018deep, lewis2019bart} of input sequences with partial obfuscation.
For continuous inputs, such as in Computer Vision (CV) applications, representations are often learned through instance classification, in which each image and its augmentations are treated as a single output class to be pulled together \cite{Chen2020SimSiam, grill2020byol} or contrasted against other negative samples \cite{he2020momentum}.

</td><td>

</td></tr>
<tr><td>

Speech signals differ from text and images in that they are continuous-valued sequences.
Self-supervised learning for the speech recognition domain faces unique challenges from those in CV and NLP.
Firstly, the presence of multiple sounds in each input utterance breaks the instance classification assumption used in many CV pre-training approaches.
Secondly, during pre-training, there is no prior lexicon of discrete sound units available, as in NLP applications in which words or word pieces are used, hindering the use of predictive losses.
Lastly, the boundaries between sound units are not known, which complicates masked prediction pre-training.

</td><td>

</td></tr>
<tr><td>

In this paper, we introduce ***Hidden unit BERT (HuBERT)*** that benefits from an offline clustering step to generate noisy labels for a BERT-like per-training.
Concretely, a BERT model consumes masked continuous speech features to predict pre-determined cluster assignments.
The predictive loss is only applied over the masked regions, forcing the model to learn good high-level representations of unmasked inputs to infer the targets of masked ones correctly.
Intuitively, the ***HuBERT*** model is forced to learn both acoustic and language models from continuous inputs.
First, the model needs to model unmasked inputs into meaningful continuous latent representations, which maps to the classical acoustic modeling problem.
Second, to reduce the prediction error, the model needs to capture the long-range temporal relations between learned representations.
One crucial insight motivating this work is the importance of consistency of the targets, not just their correctness, which enables the model to focus on modeling the sequential structure of input data.
Our approach draws inspiration from the DeepCluster method for self-supervised visual learning \cite{caron2018deep}; however, ***HuBERT*** benefits from the masked prediction loss over speech sequences to represent their sequential structure.

</td><td>

</td></tr>
<tr><td>

When the ***HuBERT*** model is pre-trained on either the standard **LibriSpeech** 960h[^25] or the **Libri-Light** 60k hours[^26], it either matches or improves upon the state-of-the-art **Wav2Vec2.0**[^07] performance on all fine-tuning subsets of 10mins, 1h, 10h, 100h, and 960h.
We present systematic results on three model sizes pre-trained with ***HuBERT***: \textsc{Base} (90M parameters), \textsc{Large} (300M), and \textsc{X-Large} (1B).
The \textsc{X-Large} model shows up to 19\% and 13\% relative WER improvement from \textsc{Large} models on dev-other and test-other evaluation subsets when pre-trained on the **Libri-Light** 60k hours.

</td><td>

</td></tr></table>

## 2·Related Works: 相关工作

<table><tr><td width="50%">

We discuss recent studies on self-supervised speech representation learning by grouping them by training objective.
The earliest line of work learns representations by postulating a generative model for speech with latent variables, which are assumed to capture the relevant phonetic information.
Training of these models amounts to likelihood maximization.
Different latent structures have been applied to encode the prior assumption, such as continuous~\cite{hsu2017learning}, discrete~\cite{chorowski2019unsupervised,van2017neural}, or sequential~\cite{hsu2017unsupervised,ebbers2017hidden,glarner2018full,khurana2019factorial,khurana2020convolutional}.

Prediction-based self-supervised learning has gathered increasing interests recently, where a model is tasked to predict the content of the unseen regions \cite{[^05], chung2020generative, chung2020improved, ling2020deep, wang2020unsupervised, liu2020mockingjay, chi2020audio, ling2020decoar} or to contrast the target unseen frame with randomly sampled ones \cite{oord2018representation, [^04], [^03], [^07]}.
Some models combine both the predictive and the contrastive losses \cite{[^06], baevski2019effectiveness}.
These objectives can usually be interpreted as mutual information maximization~\cite{tsai2020ssl_multi}.
Other objectives do not belong to these categories, for example, \cite{pascual2019learning}.

This work is most related to DiscreteBERT~\cite{baevski2019effectiveness}: both ***HuBERT*** and DiscreteBERT predict discrete targets of masked regions.
However, there are several crucial differences.
First, instead of taking quantized units as input, ***HuBERT*** takes raw waveforms as input to pass as much information as possible to the transformer layers, which was shown to be important in **Wav2Vec2.0**[^07].
Furthermore, in the experiment section, we show that our model, with simple k-means targets, can achieve better performance than DiscreteBERT that uses vq-wav2vec \cite{[^06]} learned units.
Second, we also present many techniques to improve teacher quality instead of using a single fixed teacher as done in DiscreteBERT.

***HuBERT*** is also related to **Wav2Vec2.0**[^07].
However, the latter employs a contrastive loss that requires careful design of where to sample negative frames from, an auxiliary diversity loss to encourage the discrete unit usage, and demands a proper Gumbel-softmax temperature annealing schedule.
In addition, it only explores quantizing the waveform encoder output, which may not be the best feature for quantization due to the limited capacity of the convolutional encoder, as suggested by our ablation studies in Figure~\ref{fig:qual_layer}.
Concretely, our proposed method adopts a more direct predictive loss by separating the acoustic unit discovery step from the masked prediction representation learning phase and achieves the state-of-the-art results that match or outperform **Wav2Vec2.0** on different fine-tuning scales.

Finally, the idea of iterative refinement target labels is similar to iterative pseudo labeling for semi-supervised ASR~\cite{xu2020iterative, likhomanenko2020slimipl}, which leverages an improving student model to generate better pseudo-labels for the next iteration of training.
The ***HuBERT*** approach can be seen as extending this method to the self-supervised setup with a masked prediction loss.

</td></tr></table>

## 3·Methodology: 方法

### Learning the Hidden Units for HuBERT

<table><tr><td width="50%">

An acoustic model trained on text and speech pairs provides pseudo-phonetic labels for each frame via forced alignment in semi-supervised learning.
On the contrary, the self-supervised representation learning setup has access to speech-only data.
Nevertheless, simple discrete latent variable models such as k-means and Gaussian mixture models (GMMs) infer hidden units that exhibit non-trivial correlation with the underlying acoustic units~\cite{lee2012nonparametric} (see also Table~\ref{tab:loss}).
More advanced systems can achieve better acoustic unit discovery performance using better graphical models \cite{ondel2016variational, ebbers2017hidden} or parameterizes the distributions with more powerful neural network models~\cite{hsu2017learning, hsu2017unsupervised, chorowski2019unsupervised, khurana2019factorial, khurana2020convolutional}.

Inspired by this, we propose to use acoustic unit discovery models to provide frame-level targets.
Let $X$ denote a speech utterance $X = [x_1, \cdots, x_T]$ of $T$ frames.
Discovered hidden units are denoted with $h(X) = Z = [z_1, \cdots, z_T]$, where $z_t \in [C]$ is a $C$-class categorical variable and $h$ is a clustering model, e.g.
k-means.

</td></tr></table>

### Representation Learning via Masked Prediction

Let $M \subset [T]$ denote the set of indices to be masked for a length-$T$ sequence $X$, and $\tilde{X} = r(X, M)$ denote a corrupted version of $X$ where $x_t$ is replaced with a mask embedding $\tilde{x}$ if $t \in M$.
A masked prediction model $f$ takes as input $\tilde{X}$ and predicts a distribution over the target indeces at each timestep $p_f(\cdot \mid \tilde{X}, t)$.
There are two decisions to be made for masked prediction: \textit{how to mask} and \textit{where to apply the prediction loss}.

Regarding the first decision, we adopt the same strategies used in SpanBERT~\cite{joshi2020spanbert} and **Wav2Vec2.0**[^07] for mask generation, where $p$\% of the timesteps are randomly selected as start indices, and spans of $l$ steps are masked.
To address the second decision, we denote the cross-entropy loss computed over masked and unmasked timesteps as $L_m$ and $L_u$, respectively.
$L_m$ is defined as:

$$
L_m(f; X, M, Z) = \sum_{t \in M} \log p_f(z_t \mid \tilde{X}, t),
$$

and $L_u$ is of the same form except that it sums over $t \not\in M$.
The final loss is computed as a weighted sum of the two terms: $L = \alpha L_m + (1-\alpha)L_u$.
In the extreme case when $\alpha = 0$, the loss is computed over the unmasked timesteps, which is similar to acoustic modeling in hybrid speech recognition systems \cite{young1996large, abdel2012applying, povey2005discriminative, bourlard2012connectionist}.
In our setup, this limits the learning process to mimicking the clustering model.

In the other extreme with $\alpha=1$, the loss is only computed over the masked timesteps where the model has to predict the targets corresponding to the unseen frames from context, analogous to language modeling.
It forces the model to learn both the acoustic representation of unmasked segments and the long-range temporal structure of the speech data.
We hypothesize that the setup with $\alpha=1$ is more resilient to the quality of cluster targets, which is demonstrated in our experiments (see Table~\ref{tab:loss}).

</td></tr></table>

### Learning with Cluster Ensembles

A simple idea to improve target quality is to utilize multiple clustering models.
While an individual clustering model may perform terribly, cluster ensembles can provide complementary information to facilitate representation learning.
For example, an ensemble of k-means models with different codebook sizes can create targets of different granularity, from manner classes (vowel/consonant) to sub-phone states (senones).
To extend the proposed framework, let $Z^{(k)}$ be the target sequences generated by the $k$-th clustering model.
We can now re-write $L_m$ as:

$$
L_m(f; X, \{ Z^{(k)} \}_k, M) =\sum_{t \in M} \sum_{k} \log p_f^{(k)}(z_t^{(k)} \mid \tilde{X}, t)
$$

and similarly for the unmasked loss $L_u$.
This is analogous to multi-task learning, but with tasks created by unsupervised clustering.

Additionally, ensembling is intriguing because it can be used alongside product quantization (PQ)~\cite{gray1998quantization}, where a feature space is partitioned into multiple subspaces, and each subspace is quantized separately.
PQ allows effective Euclidean distance-based quantization such as k-means for high-dimensional features and heterogeneous features whose scale differs significantly between subspaces.
In this case, the theoretical size of the target space is the product of all codebooks' sizes.

</td></tr></table>

### Iterative Refinement of Cluster Assignments

In addition to using cluster ensembles, another direction for improved representation is \textit{refining} the cluster assignments throughout the learning process.
Since we expect a pre-trained model to provide better representations than the raw acoustic feature such as MFCCs, we can create a new generation of clusters by training a discrete latent model over the learned latent representations.
The learning process then proceeds with the newly discovered units.

</td></tr></table>

### Implementation

Our pre-trained models follows the **Wav2Vec2.0** architecture[^07], with a convolutional waveform encoder, a BERT encoder~\cite{devlin2018bert}, a projection layer and a code embedding layer.
We consider ***HuBERT*** in three different configurations: \textsc{Base}, \textsc{Large}, and \textsc{X-Large}.
The fisrt two follow the architectures of **Wav2Vec2.0** \textsc{Base} and \textsc{Large} closely.
The \textsc{X-Large} architecture expands the model size to about 1 billion parameters, similar to the size of the Conformer XXL model in ~\cite{zhang2020pushing}.
The waveform encoder is identical for all the three configurations, which is composed of seven 512-channel layers with strides `[5,2,2,2,2,2,2]` and kernel widths `[10,3,3,3,3,2,2]`.
The BERT encoder consists of many identical transformer blocks, whose parameters along with the parameter of the subsequent projection layer are specified in Table~\ref{tab:arch}.

The convolutional waveform encoder generates a feature sequence at a 20ms framerate for audio sampled at 16kHz (CNN encoder down-sampling factor is 320x).
The audio encoded features are then randomly masked as described in Section \ref{sec:maskpred}.
The BERT encoder takes as input the masked sequence and outputs a feature sequence $[o_1, \cdots, o_T]$.
The distribution over codewords is parameterized with

$$
    p_f^{(k)}(c \mid \tilde{X}, t) = \frac{\exp(\text{sim}(A^{(k)} o_t, e_c) / \tau)} {\sum_{c'=1}^C \exp(\text{sim}(A^{(k)} o_t, e_{c'}) / \tau)},
$$

where $A$ is the projection matrix, $e_c$ is the embedding for codeword $c$, $\text{sim}(\cdot, \cdot)$ computes the cosine similarity between two vectors, and $\tau$ scales the logit, which is set to 0.1.
When cluster ensembles are used, one projection matrix $A^{(k)}$ is applied for each clustering model $k$.

After ***HuBERT*** pre-training, We use the connectionist temporal classification (CTC)~\cite{graves2006connectionist} loss for ASR fine-tuning of the whole model weights except the convolutional audio encoder, which remains frozen.
The projection layer(s) is removed and replaced with a randomly initialized softmax layer.
The CTC target vocabulary includes 26 English characters, a space token, an apostrophe, and a special CTC blank symbol.

</td></tr></table>

## 4·Experiments: 实验

### Data

For unsupervised pre-training, we use the full 960 hours of **LibriSpeech** audio[^25] or 60,000 hours of **Libri-Light**[^26] audio, both of which are derived from the LibriVox project that contains English recordings of copyright-free audiobooks by volunteers from the Internet.
For supervised fine-tuning, five different partitions are considered: **Libri-Light** 10-minute, 1-hour, 10-hour splits and **LibriSpeech** 100-hour (\texttt{train-clean-100}) and 960-hour (\texttt{train-clean-100}, \texttt{train-clean-360}, \texttt{train-other-500} combined) splits.
The three **Libri-Light** splits are subsets of the the **LibriSpeech** training split, and each of them contain half of the audio from \texttt{train-clean-*} and the other from \texttt{train-other-500}.

</td></tr></table>

### Unsupervised Unit Discovery

To demonstrate the effectiveness of the proposed method on utilizing low-quality cluster assignments, we consider the k-means algorithm~\cite{lloyd1982least} for acoustic unit discovery by default.
It is one of the most naive unit discovery models that can be treated as modeling an isotropic Gaussian with the same scalar variance for each acoustic unit.
To generate labels for the first iteration ***HuBERT*** training over the 960 hour **LibriSpeech** training set, we run k-means clustering with 100 clusters on 39-dimensional MFCC features, which are 13 coefficients with the first and the second-order derivatives.

To generate better targets for the subsequent iterations, we run k-means clustering with 500 clusters on the latent features extracted from the ***HuBERT*** model pre-trained in the previous iteration (not fine-tuned) at some intermediate transformer layer.
Since the feature dimension at the transformer output is much higher than the MFCC features (768-D for ***HuBERT*** \textsc{Base}), we cannot afford to load the entire 960 hour training split to the memory.
So instead, we randomly sample 10\% of the data for fitting the k-means model.

The \texttt{MiniBatchKMeans} algorithm implemented in the \texttt{scikit-learn} \cite{pedregosa2011scikit} package is used for clustering, which fits a mini-batch of samples at a time.\footnote{It still requires loading the entire dataset to the memory first.} We set the mini-batch size to be 10,000 frames.
k-means++~\cite{arthur2006k} with 20 random starts is used for better initialization.

</td></tr></table>

### Pre-Training

We train the \textsc{Base} model for two iterations on the 960 hours of **LibriSpeech** audio on 32 GPUs, with a batch size of at most 87.5 seconds of audio per GPU.
The first iteration is trained for 250k steps, while the second iteration is trained for 400k steps using labels generated by clustering the 6-th transformer layer output of the first iteration model.
Training for 100k steps takes about 9.5 hours.

Next we train ***HuBERT*** \textsc{Large} and \textsc{X-Large} for one iteration on 60,000 hours of **Libri-Light** audio on 128 and 256 GPUs, respectively, for 400k steps.
The batch sizes are reduced to 56.25 and 22.5 seconds of audio per GPU due to memory constraints.
Instead of restarting the iterative process from clustering MFCC features, we extract features from the 9-th transformer layer of the second iteration \textsc{Base} ***HuBERT*** for clustering and use those labels for training these two models.
Hence, these two models can also be seen as the third iteration models.

For all ***HuBERT*** configurations, mask span is set to $l=10$, and $p=8\%$ of the waveform encoder output frames are randomly selected as mask start if not otherwise mentioned.
Adam~\cite{kingma2014adam} optimizer is used with $\beta = (0.9, 0.98)$, and the learning rate ramps up linearly from 0 to the peak learning rate for the first 8\% of the training steps, and then decays linearly back to zero.
The peak learning rates are 5e-4/1.5e-3/3e-3 for \textsc{Base}/\textsc{Large}/\textsc{X-Large} models.

</td></tr></table>

### Supervised Fine-Tuning and Decoding

We fine-tune each model on 8 GPUs on the labeled splits described in Section~\ref{sec:data}.
The batch sizes per GPU are at most 200/80/40 seconds of audio for \textsc{Base}/\textsc{Large}/\textsc{X-Large} models.
During fine-tuning, the convolutional waveform audio encoder parameters are fixed.
Like **Wav2Vec2.0**, we introduce a \textit{freeze-step} hyperparameter to control how many fine-tuning steps the transformer parameters are fixed, and only the new softmax matrix is trained.

We sweep over peak learning rate ([1e-5, 1e-4]), learning rate schedule (percentage of steps for linear ramp-up and decay), number of fine-tuning steps, freeze step, and waveform encoder output masking probability for each model size and fine-tuning split combination using the word error rate (WER) on the \texttt{dev-other} subset as a criterion for model selection.

We use the wav2letter++~\cite{pratap2018wav2letter++} beam search decoder wrapped in Fairseq~\cite{ott2019fairseq} for language model-fused decoding, which optimizes:

$$
    \log p_{CTC}(Y \mid X) + w_1 \log P_{LM}(Y) + w_2 |Y|,
$$

where $Y$ is the predicted text, $|Y|$ is the length of the text, and $w_1$ and $w_2$ denote the language model weight and word score.
The decoding hyperparameters are searched with Ax, a Bayesian optimization toolkit,\footnote{\url{https://github.com/facebook/Ax}}.
In this work, we consider both $n$-gram and transformer language models trained on the official **LibriSpeech** language modeling data.

</td></tr></table>

### Metrics of Target Quality

For analysis, we derive frame-level forced-aligned phonetic transcripts using a hybrid ASR system to measure the correlation between the k-means cluster assignments and the actual phonetic units.

Given aligned frame-level phonetic labels $[y_1, \cdots, y_T]$ and k-means labels $[z_1, \cdots, z_T]$, the joint distribution between the two variables $p_{yz}(i, j)$ can be estimated by counting the occurrences:

$$
    p_{yz}(i, j) = \dfrac{\sum_{t=1}^T [y_t = i \wedge z_t = j] }{T},
$$

where $i$ denotes the $i$-th phoneme class and $j$ denotes the $j$-th k-means label class.
The marginal probabilities are computed as $p_z(j) = \sum_i p_{yz}(i, j)$ and $p_y(j) = \sum_j p_{yz}(i, j)$.

For each phone class $i$, we further compute the most likely target label as:

$$
    z^*(i) = \arg\max_j p_{yz}(i, j).
$$

Likewise, for each k-means class $j$, we compute the most likely phone label as:

$$
    y^*(j) = \arg\max_i p_{yz}(i, j).
$$

Three metrics are considered:

**phone purity (Phn Pur.)**:

$$
\mathbb{E}_{p_z(j)} [ p_{y \mid z}(y^*(j) \mid j) ],
$$

where $p_{y \mid z}(i \mid j) = p_{yz}(i, j) / p_z(j)$ denotes the conditional probability of phone given a k-means label.
This metric measures the average phone purity within one class, which can be interpreted as the frame-level phone accuracy if we transcribe each k-means class with its most likely phone label.
When comparing different sets of target labels with the same number of units, higher purity indicates better quality.
However, this metric is less meaningful when comparing two sets with different numbers of units: in the extreme case where each frame is assigned a unique target label, the phone purity would be 100\%.

**cluster purity (Cls Pur.)**:

$$
\mathbb{E}_{p_y(i)} [ p_{z \mid y}(z^*(i) \mid i) ],
$$

where $p_{z \mid y}(j \mid i) = p_{yz}(i, j) / p_y(i)$ denotes the conditional probability of a k-means label given phone label.
Cluster purity is the counterpart of phone purity, whose value would typically decrease when the number of units increases.
When comparing target labels with the same number of units, higher cluster purity also indicates a better quality, as frames of the same phone are more likely labeled as the same k-means label class.

**phone-normalized mutual information (PNMI)**:

$$
\begin{align}
\dfrac{I(y; z)}{H(y)} &= \dfrac{
    \sum_i \sum_j p_{yz}(i, j) \log \dfrac{p_{yz}(i, j)}{p_{y}(i)p_{z}(j)}
}{
    \sum_i p_{y}(i) \log p_{y}(i)
} \\
&= \dfrac{H(y) - H(y \mid z)} {H(y)} \\
&= 1 - \dfrac{H(y \mid z)} {H(y)}.
\end{align}
$$

PNMI is an information-theoretic metric that measures the percentage of uncertainty about the phone label $y$ eliminated after observing the k-means label $z$.
Higher PNMI also indicates better k-means clustering quality.

</td></tr></table>

## 5·Results: 结果

### Main Results: Low- and High-Resource Setups

Table~\ref{tab:main_lo} presents results for the low-resource setup, where pre-trained models are fine-tuned on 10 minutes, 1 hour, 10 hours, or 100 hours of labeled data.
We include comparison with semi-supervised (iterative pseudo labeling (IPL)~\cite{xu2020iterative}, slimIPL~\cite{likhomanenko2020slimipl}, noisy student~\cite{park2020improved}) and self-supervised approaches (DeCoAR 2.0~\cite{ling2020decoar}, DiscreteBERT~\cite{baevski2019effectiveness}, **Wav2Vec2.0**[^07]) in the literature.
Increasing the amount of unlabeled data and increasing the model size improve performance, demonstrating the scalability of the proposed ***HuBERT*** self-supervised pre-training method.
In the ultra-low resource setup with just 10 minutes of labeled data, the ***HuBERT*** \textsc{Large} model can achieve a WER of 4.7\% on the test-clean set and 7.6\% on the test-other set, which is 0.1\% and 0.6\% WER lower, respectively than the state-of-the-art **Wav2Vec2.0** \textsc{Large} model.
By further scaling up the model size to 1B parameters, the ***HuBERT*** \textsc{X-Large} model can further reduce the WER to 4.6\% and 6.8\% on test-clean and test-other.
The superiority of ***HuBERT*** persists across setups with different amounts of labeled data, with the only exceptions being fine-tuning on 100 hours of labeled data, where ***HuBERT*** \textsc{Large} is 0.1\% WER higher than **Wav2Vec2.0** \textsc{Large} on test-clean, and ***HuBERT*** \textsc{Base} is 0.1\% WER higher than **Wav2Vec2.0** \textsc{Base} on test-other.
In addition, ***HuBERT*** also outperforms DiscreteBERT by a large margin in all setups, while both are trained with a virtually identical objective - masked prediction of discovered units.
The considerable performance gap suggests two things.
First, using waveform as the input to the model is crucial for avoiding loss of information during quantization.
Second, while vq-wav2vec~\cite{[^06]}, the units that DiscreteBERT uses for training, may discover better units than k-means clustering of MFCC features, the proposed iterative refinement benefits from the improving ***HuBERT*** model and learn better units eventually.
We will verify these statements in the ablation study sections.

We report results of fine-tuning ***HuBERT*** models on the full 960 hours of **LibriSpeech** data and compare with the literature in Table~\ref{tab:main_hi}.
Prior studies using additional unpaired speech are classified into:

- self-training: first train an ASR on labeled data to annotate unlabeled speech, and then combine both golden and ASR-annotated text-speech pairs for supervised training.
- pre-training: first use unlabeled speech for pre-training a model, and then fine-tune the model on labeled data with a supervised training objective.
- pre-training + self-training: first pre-train and fine-tune a model, and then use it to annotate unlabeled speech for self-training combined with supervised data.

***HuBERT*** outperforms the state-of-the-art supervised and self-training methods and is on par with the two best pre-training results in the literature; both are based on **Wav2Vec2.0** contrastive learning.
In contrast, it lags behind methods combining pre-training with self-training.
However, as observed in \cite{xu2020self} and \cite{zhang2020pushing}, we expect that ***HuBERT*** can achieve comparable or better performance after combining with self-training, since the pre-trained ***HuBERT*** model is on par or better than the pre-trained model those two methods use for pseudo labeling.

</td></tr></table>

### Analysis: K-Means Stability

To better understand why masked prediction of discovered units is effective, we conduct a series of analyses and ablation studies.
We start with probing the stability of the k-means clustering algorithm concerning different numbers of clusters and different sizes of its training data.
Two features are considered: 39-dimensional MFCC features and 768-dimensional output from the 6-th transformer layer of the first iteration ***HuBERT***-\textsc{Base} model.
These two features are used to produce cluster assignments for the first and the second iteration ***HuBERT*** training, respectively.

For k-means clustering, we consider $K=\{100,500\}$ clusters fitted on \{1, 10, 100\} hours of speech sampled from the **LibriSpeech** training split.
Each combination of the hyperparameters and the features are trained for 10 trials, and the mean and standard deviation of the supervised PNMI metric on the development set (combining dev-clean and dev-other from LibriSpeech) is reported in Table~\ref{tab:stability}.
The results show that the k-means clustering is reasonably stable given the small standard deviations across different hyperparameters and features.
Furthermore, increasing the amount of data used for fitting k-means models improves PNMI in general, but the gain is only as much as 0.012, suggesting the feasibility of using k-means for unit discovery even with limited CPU memory relative to the feature matrix size.
Lastly, the PNMI score is much higher when clustering on ***HuBERT*** features than clustering on MFCC features, and the gap is even larger with 500 clusters, indicating that iterative refinement significantly improves the clustering quality.

</td></tr></table>

### Analysis: Clustering Quality Across Layers and Iterations

We next study how each layer of the ***HuBERT*** model from each iteration performs when used for clustering to generate training targets.
The two \textsc{Base} ***HuBERT*** models from the first two iterations as described in Section~\ref{sec:pretrain} are considered, which are referred to as \textsc{Base}-it1 and \textsc{Base}-it2, respectively.
There are 26 features representing 12 transformer layers plus the input to the first transformer layer (denoted as ``Layer 0'') from the two ***HuBERT*** models.
For each feature, we fit three k-means models ($K=\{100, 500, 1000\}$ clusters) on a 100 hour subset randomly sampled from the **LibriSpeech** training data.
The teacher quality measured in cluster purity, phone purity, and phone normalized mutual information (PNMI) is shown in Figure~\ref{fig:qual_layer}.
As a baseline, MFCC achieves (cluster purity, phone purity, PNMI) = (0.099, 0.335, 0.255) for $K=100$ and (0.031, 0.356, 0.287) for $K=500$.

Both \textsc{Base}-it1 and \textsc{Base}-it2 features result in significantly better clustering quality on all three metrics than MFCC with the same number of clusters.
On the other hand, the best \textsc{Base}-it2 feature is better than the best \textsc{Base}-it1 on phone purity and PNMI, but slightly worse on cluster purity.
Finally, we observe different trends across layers from \textsc{Base}-it1 and \textsc{Base}-it2: while \textsc{Base}-it2 model features generally improve over layers, \textsc{Base}-it1 has the best features in the middle layers around the 6th layer.
Interestingly, the quality of the last few layers degrades dramatically for \textsc{Base}-it1, potentially because it is trained on target assignments of worse quality, and therefore the last few layers learn to mimic their bad label behavior.

</td></tr></table>

### Ablation: The Importance of Predicting Masked Frames

We present a series of ablation studies in the following sections to learn how pre-training objective, cluster quality, and hyperparameters affect the performance.
The models for ablation studies are pre-trained for 100k steps and fine-tuned on the 10-hour **Libri-Light** split using fixed hyperaprameters.
MFCC-based k-means units with C=100 are used if not otherwise mentioned.
We report WERs on the dev-other set decoded with the $n$-gram language model using fixed decoding hyperparameters.

To understand the importance of our proposal to predict the masked frames only, we compare three conditions: 1) predicting masked frames, 2) predicting all frames, and 3) predicting unmasked frames, which can be simulated by setting $\alpha$ to 1.0, 0.5, and 0.0, respectively.
We are comparing three k-means models learned from clustering MFCC teachers with 50, 100, 500 clusters, one learned from clustering ***HuBERT***-\textsc{Base}-it1 6th transformer layer features, and supervised labels obtained from the forced-alignment of character-based HMM models (chenone)~\cite{le2019senones}.

Results shown in Table~\ref{tab:loss} indicate that when learning from bad cluster assignments, computing loss only from the masked regions achieves the best performance, while the inclusion of unmasked loss results in significantly higher WERs.
However, as the clustering quality improves, the model would suffer less when computing losses on the unmasked frames (\textsc{Base}-it1-layer6) or even achieve better performance as the case of chenone.

</td></tr></table>

### Ablation: The Effect of Cluster Ensembles

To understand the effect of combining multiple k-means models for generating targets, we consider two setups.
The first one has k-means models of different numbers of clusters presented in Table~\ref{tab:loss}, denoted with KM-\{50,100,500\}.
The second one has k-means models trained on spliced MFCC features with a window of three; hence, each input feature is represented as a 117-dimensional vector.
In this second case, we apply product quantization on the spliced features, where dimensions are split into the coefficients of the zeroth, first, and second-order derivatives, with each 39-dimensional subspace quantized to a codebook of 100 entries.
We denote these codebooks with Product k-means-\{0,1,2\}-100, respectively.
By comparing the results from Table~\ref{tab:loss} and Table~\ref{tab:ens}, it is clear that using an ensemble leads to better performance than what a single k-means clustering can achieve.

</td></tr></table>

### Ablation: Impact of Hyperparameters

Figure~\ref{fig:prob_bs} and Table~\ref{tab:step} studies how hyperparameters affect ***HuBERT*** pre-training.
It is shown that

- the portion of frames selected as mask start is optimal at $p=$8\%;
- increasing the batch size can significantly improve the performance; % (20\% WER reduction when increasing the batch size by 8 times);
- training for longer consistently helps for both k-means models with C=\{50, 100\}, and the best model achieves a WER of 11.68\%.

These findings are also consistent with those from BERT-like models~\cite{clark2020electra}.
In addition, we include a comparable result from DiscreteBERT~\cite{baevski2019effectiveness} in Table~\ref{tab:step} which applies k-means to quantize the same MFCC features into 13.5k units, used as both the output and the \textit{input} to the BERT model.
Besides using continuous speech input rather than discrete units, We hypothesize that ***HuBERT*** achieves significantly better performance because its fewer k-means clusters of 100 or 500 help capture broad phonetic concepts without delving into inter/intra-speaker variation.

</td></tr></table>

## 6·Conclusions: 结论

<table><tr><td width="50%">

This paper presents ***HuBERT***, a speech representation learning approach that relies on predicting K-means cluster assignments of masked segments of continuous input.
On both the **LibriSpeech** 960 hours and the 60,000 hours **Libri-Light** pre-training setups, ***HuBERT*** matches or outperforms the state-of-the-art systems over all fine-tuning subsets of 10mins, 1h, 10h, 100h, and 960h.
Furthermore, the learned representation quality improves dramatically with iteratively refining K-means cluster assignments using learned latent representations for a previous iteration.
Finally, ***HuBERT*** scales well to a 1B transformer model showing a relative reduction in WER of up to 13% on the test-other subset.
For future work, we plan to improve the ***HuBERT*** training procedure to consist of a single phase.
Furthermore, given the high quality of its representations, we will consider using ***HuBERT*** pre-trained representations for multiple downstream recognition and generation tasks beyond ASR.

</td><td>

本文展示了 ***HuBERT***, 一种依赖于预测掩膜片段的连续输入的 K-means 聚类分配的语音表示学习方法.
在 **LibriSpeech** 960 小时和 **Libri-Light** 60,000 小时的预训练设置上, ***HuBERT*** 超过了所有微调子集的最新系统.
此外, 学习到的表示质量随着迭代式的修正 K-means 聚类分配而显著提高.
最后, ***HuBERT*** 适用于 1B 变压器模型, 显示了 WER 的相对减少在 test-other 子集上最多 13%.
为了未来的工作, 我们计划改进 ***HuBERT*** 训练过程, 使其包含单个阶段.
此外, 由于其高质量的表示, 我们将考虑使用 ***HuBERT*** 预训练表示来进行多任务的语音识别和生成.

</td></tr></table>

## Reference: 参考文献

[^01]: [**HuBERT**: How Much Can a Bad Teacher Benefit ASR Pre-Training?](2021.06.14_HuBERT.md) ICASSP2021.
[^02]:
[^03]:
[^04]:
[^05]:
[^06]:
[^07]:
[^08]:
[^09]:
[^10]:
[^11]:
[^12]:
[^13]:
[^14]:
[^15]:
[^16]:
[^17]:
[^18]:
[^19]:
[^20]:
[^21]:
[^22]:
[^23]:
[^24]:
[^25]: [**LibriSpeech**: An ASR Corpus Based on Public Domain Audio Books.](../../Datasets/2015.04.19_LibriSpeech.md) ICASSP2015.
[^26]:
[^27]:
[^28]:
[^29]:
[^30]:
[^31]:
[^32]:
[^33]:
[^34]:
[^35]:
[^36]:
[^37]:
[^38]:
[^39]:
[^40]:
[^41]:
[^42]:
[^43]:
[^44]:
[^45]:
[^46]:
[^47]:
[^48]:
[^49]:
[^50]:
[^51]:
[^52]:
[^53]:
[^54]:
[^55]:
[^56]:
[^57]:
[^58]:
[^59]:
[^60]:
[^61]:
[^62]:
[^63]:
[^64]:
[^65]: