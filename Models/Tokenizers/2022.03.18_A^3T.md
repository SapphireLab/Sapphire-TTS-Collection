# A^3T

<details>
<summary>基本信息</summary>

- 标题: "A^3T: Alignment-Aware Acoustic and Text Pretraining for Speech Synthesis and Editing"
- 作者:
  - 01 He Bai,
  - 02 Renjie Zheng,
  - 03 Junkun Chen,
  - 04 Xintong Li,
  - 05 Mingbo Ma,
  - 06 Liang Huang
- 链接:
  - [ArXiv](https://arxiv.org/abs/2203.09690)
  - [Publication](https://proceedings.mlr.press/v162/bai22d.html)
  - [Github](https://github.com/richardbaihe/a3t)
  - [Demo](https://educated-toothpaste-462.notion.site/Demo-b0edd300e6004c508744c6259369a468)
- 文件:
  - [ArXiv](_PDF/2203.09690v2__A^3T__Alignment-Aware_Acoustic_and_Text_Pretraining_for_Speech_Synthesis_and_Editing.pdf)
  - [Publication](_PDF/2203.09690p0__A^3T__ICML2022.pdf)

</details>

## Abstract: 摘要

Recently, speech representation learning has improved many speech-related tasks such as speech recognition, speech classification, and speech-to-text translation.
However, all the above tasks are in the direction of speech understanding, but for the inverse direction, speech synthesis, the potential of representation learning is yet to be realized, due to the challenging nature of generating high-quality speech.
To address this problem, we propose our framework, Alignment-Aware Acoustic-Text Pretraining (A3T), which reconstructs masked acoustic signals with text input and acoustic-text alignment during training.
In this way, the pretrained model can generate high quality reconstructed spectrogram, which can be applied to the speech editing and unseen speaker TTS directly.
Experiments show A3T outperforms SOTA models on speech editing, and improves multi-speaker speech synthesis without the external speaker verification model.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论