# SRCodec

<details>
<summary>基本信息</summary>

- 标题: "SRCodec: Split-Residual Vector Quantization for Neural Speech Codec"
- 作者:
  - 01 Youqiang Zheng,
  - 02 Weiping Tu,
  - 03 Li Xiao,
  - 04 Xinmeng Xu
- 链接:
  - [ArXiv]
  - [Publication](https://doi.org/10.1109/ICASSP48485.2024.10445966)
  - [Github]
  - [Demo](https://exercise-book-yq.github.io/SRCodec-demo/)
- 文件:
  - [ArXiv]
  - [Publication](_PDF/2403.00000p0__SRCodec__Split-Residual_Vector_Quantization_for_Neural_Speech_Codec_ICASSP2024.pdf)

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

End-to-end neural speech coding achieves state-of-the-art performance by using residual vector quantization.
However, it is a challenge to quantize the latent variables with as few bits as possible.
In this paper, we propose ***SRCodec***, a neural speech codec that relies on a fully convolutional encoder/decoder network with specifically proposed split-residual vector quantization.
In particular, it divides the latent representation into two parts with the same dimensions.
We utilize two different quantizers to quantize the low-dimensional features and the residual between the low- and high-dimensional features.
Meanwhile, we propose a dual attention module in split-residual vector quantization to improve information sharing along both dimensions.
Both subjective and objective evaluations demonstrate that the effectiveness of our proposed method can achieve a higher quality of reconstructed speech at 0.95 kbps than Lyra-v1 at 3 kbps and Encodec at 3 kbps.

</details>
<br>

端到端神经语音编码通过使用残差向量量化获得最先进的性能.
然而, 使用尽可能少的比特数来量化潜在变量是一项挑战.

本文提出了 ***SRCodec***, 一个依赖于全卷积编码器/解码器网络和特别提出的***划分-残差向量量化 (Split-Residual Vector Quantization)*** 的神经语音编解码器.
特别地, 它将潜在表示划分为维度相同的两部分.
我们使用两个不同的量化器来量化低维特征以及低维和高维特征之间的残差.
同时, 我们提出使用划分-残差向量量化中的双注意力模块来提升维度上的信息共享.

主观评估和客观评估都表明了我们所提方法的有效性, 可以获得比 Lyra-v1 (3 kbps) 和 Encodec (3 kbps) 梗高质量的重构语音, 码率达到 0.95 kbps.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论