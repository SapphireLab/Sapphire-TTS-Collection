# Analysing the Language of Neural Audio Codecs

<details>
<summary>基本信息</summary>

- 标题: "Analysing the Language of Neural Audio Codecs."
- 作者:
  - 01 Joonyong Park
  - 02 Shinnosuke Takamichi
  - 03 David M. Chan
  - 04 Shunsuke Kando
  - 05 Yuki Saito
  - 06 Hiroshi Saruwatari
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.01390v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2509.01390v1](PDF/2025.09.01_2509.01390v1_Analysing_the_Language_of_Neural_Audio_Codecs.pdf)
  - [Publication] #TODO

</details>

## Abstract

This study presents a comparative analysis of the statistical and linguistic properties of neural audio codecs (NACs).
We investigate discrete speech tokens produced by various NAC models, examining their adherence to linguistic statistical laws such as Zipf's law and Heaps' law, as well as their entropy and redundancy.
To assess how these token-level properties relate to semantic and acoustic preservation in synthesized speech, we evaluate intelligibility using error rates of automatic speech recognition, and quality using the UTMOS score.
Our results reveal that NAC tokens, particularly 3-grams, exhibit language-like statistical patterns.
Moreover, these properties, together with measures of information content, are found to correlate with improved performances in speech recognition and resynthesis tasks.
These findings offer insights into the structure of NAC token sequences and inform the design of more effective generative speech models.

## 1·Introduction

Speech discretisation allows models to take advantage of powerful sequence modelling techniques originally developed in natural language processing (NLP).

Among such methods, neural audio codec (NAC) models have recently emerged as highly effective tools for modelling speech.

They are capable of producing fine-grained token sequences capturing acoustic details essential for tasks such as automatic speech recognition (ASR), speech synthesis, and spoken language understanding~\cite{guo2025recent, borsos23}.

While NAC models have primarily been designed for efficient waveform compression and high-fidelity audio reconstruction, they are increasingly being integrated into generative and representation learning frameworks.

Specifically, a speech token sequence derived from a pretrained NAC model serves as intermediate representations for downstream speech processing tasks. 
However, the linguistic and statistical properties of NAC-derived tokens remain underexplored.

Determining whether these tokens exhibit language-like regularities is crucial, as it informs whether NLP-inspired or fundamentally new approaches are needed for generative speech modeling.

To address this open question, we conduct an in-depth analysis of NAC token sequences, comparing their statistical properties with those of natural languages.

We focus on key linguistic regularities—including Zipf's law, Heaps' law, and entropy-based redundancy—to elucidate the extent to which NAC tokens adhere to, or deviate from, language-like statistical behaviours. 
Our analysis shows that these NAC-derived token sequences, particularly at the 3-gram level, closely follow natural language statistical laws, and this linguistic regularity correlates with the improved objective evaluation metrics of resynthesised speech.

![](fig/fig1v1.pdf)

<a id="fig:audio">Analysis of NAC tokens conducted in our paper.

An NAC model segments input audio into short frames and assigns each a symbolic token from a learned codebook, forming "languages'' of audio tokens.</a>

\vspace{-2mm}
