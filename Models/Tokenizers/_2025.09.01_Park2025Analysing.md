# Analysing the Language of Neural Audio Codecs

<details>
<summary>基本信息</summary>

- 标题: "Analysing the Language of Neural Audio Codecs."
- 作者:
  - 01 Joonyong Park
  - 02 Shinnosuke Takamichi
  - 03 David M. Chan
  - 04 Shunsuke Kando
  - 05 Yuki Saito
  - 06 Hiroshi Saruwatari
- 链接:
  - [ArXiv](https://arxiv.org/abs/2509.01390v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2509.01390v1](PDF/2025.09.01_2509.01390v1_Analysing_the_Language_of_Neural_Audio_Codecs.pdf)
  - [Publication] #TODO

</details>

## Abstract

This study presents a comparative analysis of the statistical and linguistic properties of neural audio codecs (NACs).
We investigate discrete speech tokens produced by various NAC models, examining their adherence to linguistic statistical laws such as Zipf's law and Heaps' law, as well as their entropy and redundancy.
To assess how these token-level properties relate to semantic and acoustic preservation in synthesized speech, we evaluate intelligibility using error rates of automatic speech recognition, and quality using the UTMOS score.
Our results reveal that NAC tokens, particularly 3-grams, exhibit language-like statistical patterns.
Moreover, these properties, together with measures of information content, are found to correlate with improved performances in speech recognition and resynthesis tasks.
These findings offer insights into the structure of NAC token sequences and inform the design of more effective generative speech models.

## 1·Introduction

Speech discretisation allows models to take advantage of powerful sequence modelling techniques originally developed in natural language processing (NLP).

Among such methods, neural audio codec (NAC) models have recently emerged as highly effective tools for modelling speech.

They are capable of producing fine-grained token sequences capturing acoustic details essential for tasks such as automatic speech recognition (ASR), speech synthesis, and spoken language understanding~\cite{guo2025recent, borsos23}.

While NAC models have primarily been designed for efficient waveform compression and high-fidelity audio reconstruction, they are increasingly being integrated into generative and representation learning frameworks.

Specifically, a speech token sequence derived from a pretrained NAC model serves as intermediate representations for downstream speech processing tasks. 
However, the linguistic and statistical properties of NAC-derived tokens remain underexplored.

Determining whether these tokens exhibit language-like regularities is crucial, as it informs whether NLP-inspired or fundamentally new approaches are needed for generative speech modeling.

To address this open question, we conduct an in-depth analysis of NAC token sequences, comparing their statistical properties with those of natural languages.

We focus on key linguistic regularities—including Zipf's law, Heaps' law, and entropy-based redundancy—to elucidate the extent to which NAC tokens adhere to, or deviate from, language-like statistical behaviours. 
Our analysis shows that these NAC-derived token sequences, particularly at the 3-gram level, closely follow natural language statistical laws, and this linguistic regularity correlates with the improved objective evaluation metrics of resynthesised speech.

![](fig/fig1v1.pdf)

<a id="fig:audio">Analysis of NAC tokens conducted in our paper.

An NAC model segments input audio into short frames and assigns each a symbolic token from a learned codebook, forming "languages'' of audio tokens.</a>

\vspace{-2mm}

## 2·Related Work

\vspace{-1mm}

### Discrete Tokens from Speech

Initially, self-supervised learning (SSL) models played a pivotal role in advancing speech tokenization.

These models learn to produce discrete tokens directly from raw speech without relying on explicit phoneme labels or text transcriptions, by utilizing the discretisation methods such as $k$-means clustering.

HuBERT~\cite{hsu2021hubert} and wav2vec 2.0~\cite{baevski2020wav2vec2} are well-known examples of these SSL models, whose token sequences have been shown to capture both phonetic and semantic information, thereby enhancing performance across a variety of speech processing tasks~\cite{Mohamed_2022}.

Building upon these foundations, NAC models, such as EnCodec~\cite{defossez2023high} and SoundStream~\cite{Zeghidour2021SoundStreamAE}, have further constructed the process with the explicit goal of speech resynthesis.

NAC models were initially developed to compress audio for efficient data transmission by lowering the bitrate, encoding detailed acoustic information into compact token sequences.

These models are optimized for efficient compression and high-fidelity waveform reconstruction, demonstrating high effectiveness in processing the audio.

This emphasis on preserving fine-grained acoustic detail distinguishes NACs from SSL-based approaches and makes them particularly well-suited for generative speech tasks that require high-quality audio output.

Moreover, NAC models have been successfully applied in a variety of downstream tasks.

For example, in text-to-speech (TTS) synthesis, NAC tokens enable high-quality waveform generation from text-based inputs, contributing to improved audio fidelity and naturalness~\cite{defossez2023high, Zeghidour2021SoundStreamAE}.

NACs have also been integrated into ASR pipelines, where compressed token representations facilitate efficient and accurate transcription while reducing computational costs~\cite{dhawan2024codec}.

Furthermore, NAC models have been explored for speech separation tasks, operating directly in the compressed token space to achieve efficient source separation with lower computational requirements~\cite{yip2024towards}. 
\vspace{-2mm}

### Statistical Laws of Language

In this study, we explore three primary statistical laws, models, and metrics inspired by NLP techniques.

\vspace{0.5em}**Zipf's Law:\:**

In linguistics and information theory, it is well established that the frequency distribution of words or character $n$-grams in natural language tends to follow Zipf's law~\cite{zipf1949human}, characterised by a few high-frequency elements and many low-frequency elements.

For example, the third most frequent word in an English document, "and,'' appears approximately one-third as often as the most frequent word, "the\footnote{https://www.cs.cmu.edu/˜cburch/words/top.html}.'' This relationship is given by the following power-law equation between the frequency rank $r$ of a word and its frequency $f(r)$:
\vspace{-2mm}

$$\begin{aligned}
f(r) = a \cdot r^{-\eta},
\end{aligned}$$

where $a>0$ is a scaling constant and $\eta>0$ represents the sharpness or scaling properties of the distribution.

An ideal Zipfian distribution typically has $\eta \approx 1$.

Such distributions have been often observed not only in natural languages but also in animal communication systems and large language models, making $\eta$ a key indicator of the balance between redundancy and information efficiency~\cite{mandelbrot1953contribution, gelbukh2001zipf}.

\vspace{0.5em}**Heaps' Law:\:**

To capture the properties of a natural language in terms of the uniqueness of vocabulary, analyses based on Heaps' law~\cite{heaps1978information} are also employed.

This law describes a sublinear relationship between the number of words in document \( m \) and the vocabulary size \( V(m) \), expressed as:
\vspace{-1mm}

$$\begin{aligned}

V(m) = K \cdot m^{\beta} \quad (0 < \beta < 1).
\end{aligned}$$

Here, \( K > 0 \) is a scaling factor that indicates the initial rate of vocabulary growth, while \( \beta \) determines the rate at which new vocabulary continues to be introduced as the text grows.

A higher value of \( K \) suggests that the text introduces a relatively large number of unique words early on, whereas a lower value of \( K \) implies a more conservative initial vocabulary expansion.

For values of  \( \beta \) close to 1, the vocabulary grows almost linearly with the document size, implying a rich and diverse vocabulary set.

For lower \( \beta \) values, the vocabulary growth is sublinear, reflecting a language that heavily reuses existing words and introduces new ones less frequently.

\vspace{0.5em}**Entropy and Redundancy:\:**

Entropy, proposed by Shannon~\cite{shannon1951prediction}, is widely used as a measure of the diversity and unpredictability of information within a sequence and serves as a fundamental metric in evaluating linguistic systems and coding efficiency.

The entropy $H$ of a token sequence is defined as:
\vspace{-2mm}

$$\begin{aligned}

H &= -\sum_{i=1}^{V} p_i \log_2 p_i,
\end{aligned}$$

where $V$ is the vocabulary size and $p_i$ is the probability of occurrence of the $i$-th token.

Using entropy-based Huffman coding, it is possible to quantify the bit reduction rate and redundancy of a sequence.

The average code length $L$ achieved by Huffman coding can be evaluated with respect to the entropy as follows:
\vspace{-1mm}

$$\begin{aligned}

H \leq L < H + 1.
\end{aligned}$$

By computing the ratio between the actual average code length $L$ and the entropy $H$, the compression efficiency or redundancy $R$ can be quantified as:
\vspace{-1mm}

$$\begin{aligned}

R = \frac{L - H}{L}.
\end{aligned}$$

A smaller value of $R$ indicates more efficient coding, reflecting lower redundancy and higher compressibility of the token.
\vspace{-2mm}

### Statistical Linguistic Analysis of Speech Tokens

There have been some previous attempts to analyse discrete speech tokens using this type of analysis. 
Takamichi et al.~\cite{takamichi2024learned} analysed whether speech tokens generated by SSL models follow Zipf's law, as observed in natural language text.

Their investigation revealed that these tokens exhibit power-law behaviour, which suggests that speech tokens possess certain linguistic structural properties. 
Moreover, Sicherman et al.~\cite{Sicherman2023AnalysingDS} explored the interpretability and redundancy of SSL-based speech tokens and found a strong correlation between tokens and phonemes.

They introduced a method for identifying redundancies that can degrade the performance of language models, and they proposed techniques to reduce such redundancies, leading to improvements in downstream tasks like speech synthesis and generative spoken language modelling. 
With respect to NAC-based tokens, Liu et al.~\cite{liu2024visual} identified the challenge of inconsistency—different token sequences may be produced from the same speech input due to variations in the tokenization process.

To address this, they introduced robust instruction-tuning strategies to enhance the consistency of tokens across different contexts, thereby improving the reliability of systems. 

\vspace{-2mm}
