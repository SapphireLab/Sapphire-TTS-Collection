# FireRedASR

<details>
<summary>基本信息</summary>

- 标题: "FireRedASR: Open-Source Industrial-Grade Mandarin Speech Recognition Models from Encoder-Decoder to LLM Integration"
- 作者:
  - 01 Kai-Tuo Xu (Xiaohongshu Inc.)
  - 02 Feng-Long Xie (Xiaohongshu Inc.)
  - 03 Xu Tang (Xiaohongshu Inc.)
  - 04 Yao Hu (Xiaohongshu Inc.)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.14350)
  - [Publication]()
  - [Github](https://github.com/FireRedTeam/FireRedASR)
  - [Demo](https://fireredteam.github.io/demos/firered_asr/)
- 文件:
  - [ArXiv](_PDF/2501.14350v1__FireRedASR__Open-Source_Industrial-Grade_Mandarin_Speech_Recognition_Models_from_Encoder-Decoder_to_LLM_Integration.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

We present FireRedASR, a family of large-scale automatic speech recognition (ASR) models for Mandarin, designed to meet diverse requirements in superior performance and optimal efficiency across various applications.
FireRedASR comprises two variants:

**FireRedASR-LLM**: Designed to achieve state-of-the-art (SOTA) performance and to enable seamless end-to-end speech interaction.
It adopts an Encoder-Adapter-LLM framework leveraging large language model (LLM) capabilities.
On public Mandarin benchmarks, FireRedASR-LLM (8.3B parameters) achieves an average Character Error Rate (CER) of 3.05%, surpassing the latest SOTA of 3.33% with an 8.4% relative CER reduction (CERR).
It demonstrates superior generalization capability over industrial-grade baselines, achieving 24%-40% CERR in multi-source Mandarin ASR scenarios such as video, live, and intelligent assistant.

**FireRedASR-AED**: Designed to balance high performance and computational efficiency and to serve as an effective speech representation module in LLM-based speech models.
It utilizes an Attention-based Encoder-Decoder (AED) architecture.
On public Mandarin benchmarks, FireRedASR-AED (1.1B parameters) achieves an average CER of 3.18%, slightly worse than FireRedASR-LLM but still outperforming the latest SOTA model with over 12B parameters.
It offers a more compact size, making it suitable for resource-constrained applications.

Moreover, both models exhibit competitive results on Chinese dialects and English speech benchmarks and excel in singing lyrics recognition.
To advance research in speech processing, we release our models and inference code at [Github](https://github.com/FireRedTeam/FireRedASR).

## 1·Introduction: 引言

Automatic Speech Recognition (ASR) has evolved rapidly in recent years, becoming an essential component in intelligent voice interaction and multimedia content understanding.
Recent advances in ASR have led to several large-scale models, such as Whisper \cite{radford2023robust}, Qwen-Audio \cite{chu2023qwen,chu2024qwen2}, SenseVoice \cite{an2024funaudiollm}, and Seed-ASR \cite{seedasr2024}, showing a paradigm shift from end-to-end models with millions of parameters \cite{li2022recent,prabhavalkar2023end} to larger-scale models \cite{radford2023robust,an2024funaudiollm,zhang2023google,song2024touchasp} and the integration of pre-trained text LLMs \cite{chu2023qwen,chu2024qwen2,seedasr2024,wu2023decoder,rubenstein2023audiopalm,li2023prompting,wang2023slm,pan2023cosmic,yu2024connecting,chen2024salm,lakomkin2024end,geng2024unveiling,ma2024embarrassingly}.

Despite their impressive capabilities and larger model sizes, they face significant limitations in practical applications.
Some models prioritize multilingual and multitask capabilities, resulting in suboptimal performance for specific languages like Mandarin.
Others, despite showing promising results, are limited by their closed-source nature, restricting community-driven improvements and academic research.
The growing demands for modern speech interaction systems, highlighted by GPT-4o \cite{openai2024gpt4o,hurst2024gpt}, underscore the need for open-source, high-performance Mandarin ASR solutions.

To address these limitations, in this technical report, we introduce FireRedASR, a family of large-scale models for Mandarin ASR.
To address varying needs in performance and efficiency across a wide range of application scenarios, FireRedASR consists of two variants: FireRedASR-LLM and FireRedASR-AED.
FireRedASR-LLM utilizes an innovative Encoder-Adapter-LLM framework \cite{seedasr2024,wu2023decoder,geng2024unveiling,ma2024embarrassingly}, comprising 8.3B parameters to push the boundary of recognition accuracy.
This model is particularly well-suited for scenarios where precision is paramount and computational resources are not a primary constraint.
FireRedASR-AED, on the other hand, is designed to balance superior performance and optimal efficiency.
It employs an Attention-based Encoder-Decoder (AED) architecture \cite{bahdanau2016end,chan2016listen} with up to 1.1B parameters.
Beyond its standalone use, FireRedASR-AED also functions as a crucial speech representation component within larger LLM-based speech frameworks.

Key contributions of our work include:

- **High-Accuracy Models with Efficiency**: On public Mandarin benchmarks, FireRedASR-LLM achieves an average Character Error Rate (CER) of 3.05%, surpassing the previous state-of-the-art (Seed-ASR) of 3.33% with an 8.4% relative reduction.
Meanwhile, FireRedASR-AED attains a  CER of 3.18%, outperforming Seed-ASR (over 12B parameters) with significantly fewer parameters.
These results highlight the ability of our models to achieve superior accuracy while maintaining efficiency.
- **Robust Real-World Performance**: In diverse practical scenarios, including short videos, live streaming, auto-captioning, voice input, and intelligent assistants, our models demonstrate exceptional capabilities, achieving 24%-40% relative CER reduction (CERR) compared to popular open-source baseline and leading commercial solutions.
- **Versatile Recognition Capabilities**: Both variants demonstrate remarkable versatility beyond standard Mandarin ASR, showing competitive results on Chinese dialects and English speech benchmarks.
Notably, they achieve 50%-67% CERR in singing lyrics recognition compared to industrial-grade baselines.
- **Comprehensive Open-Source Release**: We contribute to the research community by releasing our model family, including pre-trained weights and efficient inference code.
This open-source release aims to accelerate research progress in speech processing and enable broader applications in modern end-to-end speech interaction systems.

The remainder of this report is organized as follows: Section 2 describes the architectures of FireRedASR-AED and FireRedASR-LLM, along with training data and optimization strategies.
Section 3 presents comprehensive evaluation results across various benchmarks and practical scenarios compared to recently released large-scale ASR models.
Section 4 discusses the key factors contributing to our superior performance.
Section 5 concludes the report.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

In this section, we present the architectural details and methodologies for our two ASR models: FireRedASR-AED and FireRedASR-LLM. FireRedASR-AED follows the conventional Attention-based Encoder-Decoder architecture, whereas FireRedASR-LLM is built on the Encoder-Adapter-LLM architecture that leverages the power of LLM for ASR. Both models share similar input feature processing and acoustic encoding strategies but differ in their approaches to token sequence modeling.

### FireRedASR-AED: Attention-based Encoder-Decoder ASR model

FireRedASR-AED adopts an end-to-end architecture that combines a Conformer-based Encoder (Enc) with a Transformer-based Decoder (Dec)\cite{gulati2020conformer,vaswani2017attention}. This design choice leverages both the ability of Conformer to model local and global dependencies in speech features and the effectiveness of Transformer in sequence transduction. The overall architecture of FireRedASR-AED is illustrated in Figure \ref{img:llm_asr} (bottom right).

**Training Data**: The training corpus consists of approximately 70,000 hours of audio data, predominantly high-quality Mandarin Chinese speech. Unlike weakly-labeled datasets used in Whisper, the majority of our data was manually transcribed by professional annotators, ensuring high transcription accuracy and reliability. The dataset also incorporates approximately 11,000 hours of English speech data to enhance English ASR capabilities.

**Input Features**: The input features are 80-dimensional log Mel filterbank (Fbank) extracted from 25ms windows with 10ms frame shifts, followed by global mean and variance normalization.

**Encoder Structure**: The encoder consists of two main components: a subsampling module and a stack of Conformer blocks. The subsampling module employs two sequential convolutional layers, each with a stride of 2 and a kernel size of 3, followed by ReLU activation functions. This configuration reduces the temporal resolution from 10ms to 40ms per frame, effectively managing computational complexity while preserving essential acoustic information.
The subsampled features are then processed by a stack of Conformer blocks. Each Conformer block consists of four primary components: two Macaron-style feedforward modules positioned at the beginning and end of the block, a multi-head self-attention module incorporating relative positional encoding \cite{dai2019transformer}, and a convolution module equipped with gated linear unit (GLU) and layer normalization. The kernel size for all 1-D depthwise convolution is set to 33. This structure enables effective modeling of both local and global dependencies in the speech signal, while maintaining computational efficiency.

**Decoder Structure**: The decoder follows a standard Transformer architecture with several key design choices. It adopts fixed sinusoidal positional encodings and employs weight tying between input and output token embeddings to reduce model complexity. Each Transformer block consists of three primary components: a multi-head self-attention module, a multi-head cross-attention module, and a position-wise feedforward module, all utilizing pre-norm residual units to enhance training stability and gradient flow.

**Tokenization**: We employ a mixed tokenization strategy: Chinese characters for Chinese text and token-level byte-pair encoding (BPE) \cite{sennrich2015neural} for English text. The total vocabulary size is 7,832, comprising 1,000 English BPE tokens, 6,827 Chinese characters, and 5 special tokens.

We investigated various sizes of FireRedASR-AED, with detailed architectural configurations presented in Table \ref{tab:model_config}, where `\#Params` denotes the number of parameters. Unless otherwise specified, FireRedASR-AED refers to FireRedASR-AED-L.

### FireRedASR-LLM: Encoder-Adapter-LLM-based ASR model

FireRedASR-LLM is also an end-to-end ASR model but designed to integrate robust speech processing capabilities of FireRedASR-AED with the superior language capabilities of LLM. It comprises three core components: a Conformer-based audio Encoder, a lightweight audio-text alignment Adapter and a pre-trained text-based LLM, forming what we term the Encoder-Adapter-LLM architecture. The overall architecture of FireRedASR-LLM is illustrated in Figure \ref{img:llm_asr} (left).

**Input Features and Encoder**: FireRedASR-LLM employs the same training data, input features and processing methods as FireRedASR-AED. The encoder of FireRedASR-LLM is initialized with pre-trained weights from Encoder of FireRedASR-AED. This encoder generates continuous representations that encapsulate both acoustic and semantic characteristics of the input speech.

**Adapter Structure and Functionality**: To seamlessly integrate the audio encoder with the text-based LLM, an adapter network is employed. This adapter transforms the output of encoder into the semantic space of the LLM, enabling the LLM to accurately recognize the corresponding text content from the input speech. The adapter consists of a simple but effective Linear-ReLU-Linear network, which projects the output dimension of encoder to match the input embedding dimension of the LLM.
Even after temporal subsampling from 10ms to 40ms, the output of the encoder remains too lengthy for the LLM to process efficiently. Therefore, we incorporate an additional frame splicing operation at the beginning of the adapter. This operation further reduces the temporal resolution from 40ms to 80ms per frame, thereby decreasing sequence length and improving the computational efficiency for the LLM.

**LLM Initialization and Processing**: The LLM component of FireRedASR-LLM is initialized with pre-trained weights from Qwen2-7B-Instruct \cite{qwen2}, a notable open-source LLM.
During training, the input of FireRedASR-LLM consists of a triplet: (prompt, speech, transcript). The encoder and adapter produces a speech embedding ${E}_{S}$, while the prompt and transcript are tokenized and embedded by the LLM into prompt embedding ${E}_{P}$ and transcript embedding ${E}_{T}$. These embeddings are concatenated as $({E}_{P}, {E}_{S}, {E}_{T})$ and processed by the subsequent layers of LLM.
During inference, the input is reduced to $({E}_{P},{E}_{S})$, enabling the LLM to execute next-token-prediction and generate recognized text from speech.

**Training Strategy**: We employ a carefully designed training strategy that balances adaptation and preservation of pre-trained capabilities: the encoder and adapter are fully trainable, while the majority of LLM parameters remain fixed. We incorporate trainable LLM Low-Rank Adaptation (LoRA) \cite{hu2021lora} to efficiently fine-tune the LLM. This strategy ensures that the encoder and adapter are adequately trained to map speech features into the semantic space of LLM, while preserving its pre-trained capabilities.
The training objective is based on cross-entropy loss, with the loss computed only over the transcript portion of the input, ignoring the prompt and speech embeddings.

We investigated various sizes of FireRedASR-LLM, with detailed architectural configurations presented in Table \ref{tab:model_config}. Unless otherwise specified, FireRedASR-LLM refers to FireRedASR-LLM-L.

## 4·Experiments & 5·Results: 实验 & 结果

## 6·Conclusions: 结论
