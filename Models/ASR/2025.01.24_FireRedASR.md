# FireRedASR

<details>
<summary>基本信息</summary>

- 标题: "FireRedASR: Open-Source Industrial-Grade Mandarin Speech Recognition Models from Encoder-Decoder to LLM Integration"
- 作者:
  - 01 Kai-Tuo Xu (Xiaohongshu Inc.)
  - 02 Feng-Long Xie (Xiaohongshu Inc.)
  - 03 Xu Tang (Xiaohongshu Inc.)
  - 04 Yao Hu (Xiaohongshu Inc.)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.14350)
  - [Publication]()
  - [Github](https://github.com/FireRedTeam/FireRedASR)
  - [Demo](https://fireredteam.github.io/demos/firered_asr/)
- 文件:
  - [ArXiv](_PDF/2501.14350v1__FireRedASR__Open-Source_Industrial-Grade_Mandarin_Speech_Recognition_Models_from_Encoder-Decoder_to_LLM_Integration.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

We present FireRedASR, a family of large-scale automatic speech recognition (ASR) models for Mandarin, designed to meet diverse requirements in superior performance and optimal efficiency across various applications.
FireRedASR comprises two variants:

**FireRedASR-LLM**: Designed to achieve state-of-the-art (SOTA) performance and to enable seamless end-to-end speech interaction.
It adopts an Encoder-Adapter-LLM framework leveraging large language model (LLM) capabilities.
On public Mandarin benchmarks, FireRedASR-LLM (8.3B parameters) achieves an average Character Error Rate (CER) of 3.05%, surpassing the latest SOTA of 3.33% with an 8.4% relative CER reduction (CERR).
It demonstrates superior generalization capability over industrial-grade baselines, achieving 24%-40% CERR in multi-source Mandarin ASR scenarios such as video, live, and intelligent assistant.

**FireRedASR-AED**: Designed to balance high performance and computational efficiency and to serve as an effective speech representation module in LLM-based speech models.
It utilizes an Attention-based Encoder-Decoder (AED) architecture.
On public Mandarin benchmarks, FireRedASR-AED (1.1B parameters) achieves an average CER of 3.18%, slightly worse than FireRedASR-LLM but still outperforming the latest SOTA model with over 12B parameters.
It offers a more compact size, making it suitable for resource-constrained applications.

Moreover, both models exhibit competitive results on Chinese dialects and English speech benchmarks and excel in singing lyrics recognition.
To advance research in speech processing, we release our models and inference code at [Github](https://github.com/FireRedTeam/FireRedASR).

## 1·Introduction: 引言

Automatic Speech Recognition (ASR) has evolved rapidly in recent years, becoming an essential component in intelligent voice interaction and multimedia content understanding.
Recent advances in ASR have led to several large-scale models, such as Whisper \cite{radford2023robust}, Qwen-Audio \cite{chu2023qwen,chu2024qwen2}, SenseVoice \cite{an2024funaudiollm}, and Seed-ASR \cite{seedasr2024}, showing a paradigm shift from end-to-end models with millions of parameters \cite{li2022recent,prabhavalkar2023end} to larger-scale models \cite{radford2023robust,an2024funaudiollm,zhang2023google,song2024touchasp} and the integration of pre-trained text LLMs \cite{chu2023qwen,chu2024qwen2,seedasr2024,wu2023decoder,rubenstein2023audiopalm,li2023prompting,wang2023slm,pan2023cosmic,yu2024connecting,chen2024salm,lakomkin2024end,geng2024unveiling,ma2024embarrassingly}.

Despite their impressive capabilities and larger model sizes, they face significant limitations in practical applications.
Some models prioritize multilingual and multitask capabilities, resulting in suboptimal performance for specific languages like Mandarin.
Others, despite showing promising results, are limited by their closed-source nature, restricting community-driven improvements and academic research.
The growing demands for modern speech interaction systems, highlighted by GPT-4o \cite{openai2024gpt4o,hurst2024gpt}, underscore the need for open-source, high-performance Mandarin ASR solutions.

To address these limitations, in this technical report, we introduce FireRedASR, a family of large-scale models for Mandarin ASR.
To address varying needs in performance and efficiency across a wide range of application scenarios, FireRedASR consists of two variants: FireRedASR-LLM and FireRedASR-AED.
FireRedASR-LLM utilizes an innovative Encoder-Adapter-LLM framework \cite{seedasr2024,wu2023decoder,geng2024unveiling,ma2024embarrassingly}, comprising 8.3B parameters to push the boundary of recognition accuracy.
This model is particularly well-suited for scenarios where precision is paramount and computational resources are not a primary constraint.
FireRedASR-AED, on the other hand, is designed to balance superior performance and optimal efficiency.
It employs an Attention-based Encoder-Decoder (AED) architecture \cite{bahdanau2016end,chan2016listen} with up to 1.1B parameters.
Beyond its standalone use, FireRedASR-AED also functions as a crucial speech representation component within larger LLM-based speech frameworks.

Key contributions of our work include:

- **High-Accuracy Models with Efficiency**: On public Mandarin benchmarks, FireRedASR-LLM achieves an average Character Error Rate (CER) of 3.05%, surpassing the previous state-of-the-art (Seed-ASR) of 3.33% with an 8.4% relative reduction.
Meanwhile, FireRedASR-AED attains a  CER of 3.18%, outperforming Seed-ASR (over 12B parameters) with significantly fewer parameters.
These results highlight the ability of our models to achieve superior accuracy while maintaining efficiency.
- **Robust Real-World Performance**: In diverse practical scenarios, including short videos, live streaming, auto-captioning, voice input, and intelligent assistants, our models demonstrate exceptional capabilities, achieving 24%-40% relative CER reduction (CERR) compared to popular open-source baseline and leading commercial solutions.
- **Versatile Recognition Capabilities**: Both variants demonstrate remarkable versatility beyond standard Mandarin ASR, showing competitive results on Chinese dialects and English speech benchmarks.
Notably, they achieve 50%-67% CERR in singing lyrics recognition compared to industrial-grade baselines.
- **Comprehensive Open-Source Release**: We contribute to the research community by releasing our model family, including pre-trained weights and efficient inference code.
This open-source release aims to accelerate research progress in speech processing and enable broader applications in modern end-to-end speech interaction systems.

The remainder of this report is organized as follows: Section 2 describes the architectures of FireRedASR-AED and FireRedASR-LLM, along with training data and optimization strategies.
Section 3 presents comprehensive evaluation results across various benchmarks and practical scenarios compared to recently released large-scale ASR models.
Section 4 discusses the key factors contributing to our superior performance.
Section 5 concludes the report.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论
