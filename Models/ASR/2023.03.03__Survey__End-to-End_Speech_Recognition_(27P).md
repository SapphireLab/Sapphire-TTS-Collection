# End-to-End Speech Recognition: A Survey

<details>
<summary>基本信息</summary>

- 标题: "End-to-End Speech Recognition: A Survey"
- 作者:
  - 01 Rohit Prabhavalkar
  - 02 Takaaki Hori
  - 03 Tara N.Sainath
  - 04 Ralf Schluter
  - 05 Shinji Watanabe
- 链接:
  - [ArXiv](https://arxiv.org/abs/2303.03329v1)
  - [Publication](https://doi.org/10.1109/TASLP.2023.3328283)
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv](../../Tasks/ASR/PDF/2023.03.03__2303.03329v1__Survey__End-to-End_Speech_Recognition__A_Survey.pdf)
  - [Publication](../../Tasks/ASR/PDF/2023.02.21__2303.03329p0__Survey__End-to-End_Speech_Recognition__A_Survey_TASLP2023.pdf)

</details>

## Abstract: 摘要

<table><tr><td width="50%">

In the last decade of **automatic speech recognition (ASR)** research, the introduction of deep learning has brought considerable reductions in **word error rate** of more than 50% relative, compared to modeling without deep learning.
In the wake of this transition, a number of all-neural ASR architectures have been introduced.
These so-called **end-to-end (E2E)** models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, with lower dependence on ASR domain-specific experience.
The success and enthusiastic adoption of deep learning, accompanied by more generic model architectures has led to E2E models now becoming the prominent ASR approach.
The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relationship to classical **hidden Markov model (HMM)** based ASR architectures.
All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, discussions of performance and deployment opportunities, as well as an outlook into potential future developments.

</td><td>

在过去十年的**自动语音识别 (Automatic Speech Recognition, ASR)** 研究中, 深度学习的引入使得**词错误率 (Word Error Rate, WER)** 相比非深度学习模型相对降低了超过 50%.
随着这一转变, 许多全神经 ASR 架构被提出.
这些所谓的**端到端 (End-to-End, E2E)** 模型提供了高度集成, 完全神经网络化的 ASR 模型, 它高度依赖于通用的机器学习知识, 从数据中更一致地学习, 并且对 ASR 领域特定经验的依赖更低.
深度学习的成功和狂热应用, 以及更通用模型架构的出现, 已经使得 E2E 模型现已成为主流 ASR 方法.
本综述的目标是提供 E2E ASR 模型的分类体系和对应改进, 并讨论它们的性质和经典的基于**隐马尔可夫模型 (Hidden Markov Model, HMM)** 的 ASR 模型架构之间的关系.
本工作涵盖了 E2E ASR 的所有相关方面: 建模, 训练, 解码, 外部语言模型集成, 对性能和部署机遇的讨论, 以及潜在的未来发展方向的展望.

</td></tr></table>

## 1·Introduction: 引言

<table><tr><td width="50%">

The classical statistical architecture decomposes an ASR system into four main components: acoustic feature extraction from speech audio signals, acoustic modeling, language modeling and search based on Bayes decision rule \cite{Bayes:1763,jelinek1997statistical}.
The term "classical" here refers to the former long-term state-of-the-art ASR architecture based on the decomposition into acoustic and language model and with acoustic modeling based on hidden Markov models.
Classical acoustic modeling is based on hidden Markov models (HMM) to account for speaking rate variation.
Within the classical approach, deep learning has been introduced to acoustic and language modeling.
In acoustic modeling, deep learning replaced Gaussian mixture distributions (hybrid HMM \cite{Bourlard+Morgan:1993,Seide+:2011}) or augmented the acoustic feature set (nonlinear disciminant/tandem approach \cite{Fontaine+:1997,Hermansky+:2000}).
In language modeling, deep learning replaced count-based approaches \cite{Nakamura+:1989,Bengio+:2000,Schwenk+Gauvain:2002}.
However, when introducing deep learning, the classical ASR architecture was not yet touched.
Classical state-of-the-art ASR systems today are composed of many separate components and knowledge sources, especially speech signal preprocessing, methods for robustness w.r.t.\ recording conditions, phoneme inventories and pronunciation lexica, phonetic clustering, handling of out-of-vocabulary words, various methods for adaptation/normalization, elaborate training schedules with different objectives and incl.\ sequence discriminative training, etc.
The potential of deep learning on the other hand initiated successful approaches to integrate formerly separate modeling steps, e.g.\ integrating speech signal preprocessing and feature extraction into acoustic modeling \cite{Tuske+:2014,Sainath+:2015}.

</td><td>

</td></tr>
<tr><td>

More consequently, the introduction of deep learning to ASR also initiated research to replace classical ASR architectures based on hidden Markov models (HMM) with more integrated joint neural network model structures \cite{graves2006connectionist,graves2012sequence,chorowski2015attention,chan2016listen}.
These ventures might be seen as trading specific speech processing models for more generic machine learning approaches to sequence-to-sequence processing, maybe in a similar way as statistical approaches to natural language processing used to replace more linguistically oriented models.
For these all-neural approaches recently the term \emph{end-to-end} (\EtoE) \cite{Liang+:2006,Collobert+:2011,graves2012sequence,GravesJaitly14} has been established.
Therefore, first of all an attempt to defining the term \emph{end-to-end} in the context of ASR is due in this survey.
According to the Cambridge Dictionary, the adjective "end-to-end" is defined by: "including all the stages of a process" \cite{CambridgeDictEndToEnd}.
We therefore propose the following definition of end-to-end ASR: an integrated ASR model that enables joint training from scratch; avoids separately obtained knowledge sources; and, provides single-pass recognition consistent with the objective to optimize the task-specific evaluation measure, i.e., usually label (word, character, subword, etc.) error rate.
While this definition suffices for the present discussion, we note that such an idealized definition hides many nuances involved in the term E2E and lacks distinctiveness; we elaborate on some of these nuances in Section II to discuss the various connotations of the term E2E in the context of ASR.

</td><td>

</td></tr>
<tr><td>

What are potential benefits of E2E approaches to ASR? The primary objective when developing an ASR systems is to minimize the expected word error rate; secondary objectives are to reduce time and memory complexity of the resulting decoder, and – assuming a constrained development budget – genericity, and ease of modeling.
First of all, an integrated ASR system, defined in terms of a single neural network structure supports genericity of modeling and may allow for faster development cycles when building ASR systems for new languages or domains.
Similarly, ASR models defined by a single neural network structure may become more ‘lean’ compared to classical modeling, with a simpler decoding process, obviating the need to integrate separate models.
The resulting reduction in memory footprint and power consumption supports embedded ASR applications [21], [22].
Furthermore, end-to-end joint training may help to avoid spurious optima from intermediate training stages.
Avoiding secondary knowledge sources like pronunciation lexica may be helpful for languages/domains where such resources are not easily available.
Also, secondary knowledge sources may themselves be erroneous; avoiding these may improve models trained directly from data, provided that sufficient amounts of task-specific training data are available.

</td><td>

</td></tr>
<tr><td>

With the current surge of interest in E2E ASR models and an increasing diversity of corresponding work, the authors of this review think it is time to provide an overview of this rapidly evolving domain of research.
The goal of this survey is to provide an in-depth overview of the current state of research on E2E ASR systems, covering all relevant aspects of E2E ASR, with a contrastive discussion of the different E2E and classical ASR architectures.

</td><td>

</td></tr>
<tr><td>

This survey of E2E speech recognition is structured as follows.
Section II discusses the nuances in the term E2E as it applies to ASR.
Section III describes the historical evolution of E2E speech recognition, with specific focus on the input-output alignment and an overview of prominent E2E ASR models.
Section IV discusses improvements of the basic E2E models, including E2E model combination, training loss functions, context, encoder/decoder structures and endpointing.
Section V provides an overview of E2E ASR model training.
Decoding algorithms for the different E2E approaches are discussed in Section VI.
Section VII discusses the role and integration of (separate) language models in E2E ASR.
Section VIII reviews experimental comparisons of the different E2E as well as classical ASR approaches.
Section IX provides an overview of applications of E2E ASR.
Section X investigates future directions of E2E research in ASR, before concluding in Section XI.
Finally, we note that this survey paper also includes comparative discussions between novel E2E models and classical HMM-based ASR approaches in terms of various aspects; most sections end with a summarization of the relationship between E2E models and HMM-based ASR approaches in relation to the topics covered within the respective sections.

</td><td>

</td></tr></table>

## 2·Distinctiveness of the Term E2E

<table><tr><td width="50%">

As noted in Section I the term E2E provides an idealized definition of ASR systems, and can benefit from a more detailed discussion based on the following perspectives.

</td><td>

</td></tr>
<tr><td>

(a) **Joint Modeling**: In terms of ASR, the E2E property can be interpreted as considering all components of an ASR system jointly as a single computational graph.
Even more so, the common understanding of E2E in ASR is that of a single joint modeling approach that does not necessarily distinguish separate components, which may also mean dropping the classical separation of ASR into an acoustic model and a language model.
However, in practice E2E ASR systems are often combined with external language models trained on text-only data, which weakens the end-to-end nature of the system to some extent.

</td><td>

</td></tr>
<tr><td>

(b) **Joint Training**: In terms of model training, E2E can be interpreted as estimating all parameters, of all components of a model jointly using a single objective function that is consistent with the task at hand, which in case of ASR means minimizing the expected word error rate.2However, the term lacks distinctiveness here, as classical and/or modular ASR model architectures also support joint training with a single objective.

</td><td>

</td></tr>
<tr><td>

(c) **Training from Scratch**: The E2E property can also be interpreted with respect to the training process itself, by requiring training from scratch, avoiding external knowledge like prior alignments or initial models pre-trained using different criteria or knowledge sources.
However, note that pre-training and fine-tuning strategies are also relevant, if the model has explicit modularity, including self-supervised learning [25] or joint training of front-end and speech recognition models [26].
Especially in case of limited amounts of target task training data, utilizing large pretrained models is important to obtain performant E2E ASR systems.

</td><td>

</td></tr>
<tr><td>

(d) **Avoiding Secondary Knowledge Sources**: For ASR, standard secondary knowledge sources are pronunciation lexica and phoneme sets, as well as phonetic clustering, which in classical state-of-the-art ASR systems usually is based on classification and regression trees (CART) [27].
Secondary knowledge sources and separately trained components may introduce errors, might be inconsistent with the overall training objective and/or may generate additional cost.
Therefore, in an E2E approach, these would be avoided.
Standard joint training of an E2E model requires using a single kind of training data, which in case of ASR would be transcribed speech audio data.
However, in ASR often even larger amounts of text-only data, as well as optional untranscribed speech audio are available.
One of the challenges of E2E modeling therefore is how to take advantage of text-only and audio-only data jointly without introducing secondary (pretrained) models and/or training objectives [28], [29].

</td><td>

</td></tr>
<tr><td>

(e) **Direct Vocabulary Modeling**: Avoiding pronunciation lexica and corresponding subword units leave E2E recognition vocabularies to be derived from whole word or character representations.
Whole word models [30], according to Zipf’s law [31], would require unrealistically high amounts of transcribed training data for large vocabularies, which might not be attainable for many tasks.
On the other hand, methods to generate subword vocabularies based on characters, like the currently popular byte pair encoding (BPE) approach [32], might be seen as secondary approaches outside the E2E objective, even more so if acoustic data is considered for subword derivation [33], [34], [35], [36].

</td><td>

</td></tr>
<tr><td>

(f) **Generic Modeling**: Finally, E2E modeling also requires genericity of the underlying modeling: task-specific constraints are learned completely from data, in contrast to task-specific knowledge which influences the modeling of the system architecture in the first place.
For example, the monotonicity constraint in ASR may be learned completely from data in an end-to-end fashion (e.g., in attention-based approaches [16]), or it may directly be implemented, as in classical HMM structures.
However, model constraints may be considered by way of regularization in E2E ASR model training, and can thus provide an alternative way to introduce task-specific knowledge.

</td><td>

</td></tr>
<tr><td>

(g) **Single-Pass Search**: In terms of the recognition/search problem, the E2E property can be interpreted as integrating all components (models, knowledge sources) of an ASR system before coming to a decision.
This is in line with Bayes’ decision rule, which exactly requires a single global decision integrating all available knowledge sources, which is supported by both classical ASR models as well as E2E models.
On the other hand, multipass search is not only exploited by classical ASR models, but also by E2E ASR models, the most prominent case here being (external) language model rescoring.

</td><td>

</td></tr>
<tr><td>

All in all, we need to conclude that a) “E2E” does not provide a clear distinction between classical and novel, so-called E2E models, and b) the E2E property often is weakened in practice, leaving the term as a more general, idealized perspective on ASR modeling.

</td><td>

</td></tr></table>
