# 标题

<details>
<summary>基本信息</summary>

- 标题: "Everyone-Can-Sing: Zero-Shot Singing Voice Synthesis and Conversion with Speech Reference"
- 作者:
  - 01 Shuqi Dai,
  - 02 Yunyun Wang,
  - 03 Roger B. Dannenberg,
  - 04 Zeyu Jin
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.13870)
  - [Publication]()
  - [Github]()
  - [Demo](http://everyone-can-sing.github.io/)
- 文件:
  - [ArXiv]()
  - [Publication] #TODO

</details>

## Abstract: 摘要

We propose a unified framework for Singing Voice Synthesis (SVS) and Conversion (SVC), addressing the limitations of existing approaches in cross-domain SVS/SVC, poor output musicality, and scarcity of singing data.
Our framework enables control over multiple aspects, including language content based on lyrics, performance attributes based on a musical score, singing style and vocal techniques based on a selector, and voice identity based on a speech sample.
The proposed zero-shot learning paradigm consists of one SVS model and two SVC models, utilizing pre-trained content embeddings and a diffusion-based generator.
The proposed framework is also trained on mixed datasets comprising both singing and speech audio, allowing singing voice cloning based on speech reference.
Experiments show substantial improvements in timbre similarity and musicality over state-of-the-art baselines, providing insights into other low-data music tasks such as instrumental style transfer.
Examples can be found at: [this http URL](http://everyone-can-sing.github.io/).

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论