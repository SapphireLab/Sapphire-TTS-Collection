# Tacotron

<details>
<summary>基本信息</summary>

- 标题: Tacotron: Towards End-to-End Speech Synthesis
- 作者:
  - 01 [Yuxuan Wang](../../Authors/Yuxuan_Wang_(王雨轩).md) 
  - 02 [RJ Skerry-Ryan](../../Authors/RJ_Skerry-Ryan.md) 
  - 03 [Daisy Stanton](../../Authors/Daisy_Stanton.md) 
  - 04 [Yonghui Wu](../../Authors/Yonghui_Wu.md) 
  - 05 [Ron J. Weiss](../../Authors/Ron_J._Weiss.md) 
  - 06 [Navdeep Jaitly](../../Authors/Navdeep_Jaitly.md) 
  - 07 [Zongheng Yang](../../Authors/Zongheng_Yang.md) 
  - 08 [Ying Xiao](../../Authors/Ying_Xiao.md) 
  - 09 [Zhifeng Chen](../../Authors/Zhifeng_Chen.md) 
  - 10 [Samy Bengio](../../Authors/Samy_Bengio.md) 
  - 11 [Quoc Le](../../Authors/Quoc_Le.md) 
  - 12 [Yannis Agiomyrgiannakis](../../Authors/Yannis_Agiomyrgiannakis.md) 
  - 13 [Rob Clark](../../Authors/Rob_Clark.md) 
  - 14 [Rif A. Saurous](../../Authors/Rif_A._Saurous.md) 
- 机构:
  - [Google](../../Institutions/Google.md) 
- 时间:
  - 预印时间: 2017.03.29 ArXiv v1
  - 预印时间: 2017.04.06 ArXiv v2
  - 发表时间: 2017.08.20
  - 更新笔记: 2024.06.16
- 发表:
  - [InterSpeech 2017](../../Publications/InterSpeech.md) 
- 链接:
  - [ArXiv](https://arxiv.org/abs/1703.10135) 
  <!-- - [DOI]()  -->
  <!-- - [Github]() -->
  - [Demo](https://google.github.io/tacotron) 
  - [Scholar](https://scholar.google.com/scholar?cluster=11291739830353377265) 
- 标签:
  - [语音合成](../../Tags/SpeechSynthesis.md) 
  - [Seq2Seq模型](../../Tags/Model_Seq2Seq.md) 
  - [Tacotron模型](../../Tags/Model_Tacotron.md) 
- 页数: 5
- 引用: 26
- 被引: 2037 
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

<details>
<summary>原文</summary>

> A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module.
> Building these components often requires extensive domain expertise and may contain brittle design choices.
> In this paper, we present ***Tacotron***, an end-to-end generative text-to-speech model that synthesizes speech directly from characters.
> Given `<text, audio>` pairs, the model can be trained completely from scratch with random initialization.
> We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task.
> ***Tacotron*** achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness.
> In addition, since ***Tacotron*** generates speech at the frame level, it’s substantially faster than sample-level autoregressive methods. 

</details>
<br>

一个文本到语音合成的系统通常包含多个阶段, 例如文本分析前端, 声学模型和音频合成模块.

构建这些组件通常需要广泛的领域专业知识, 并且可能包含脆弱的设计选择.

在本文中, 我们介绍了 ***Tacotron***, 这是一个端到端的生成式文本到语音模型, 能够直接从字符合成语音.

给定 `<文本, 音频>` 对, 该模型可以从随机初始化开始完全从头训练.

我们提出了几种关键技术, 使得序列到序列框架能够在这个具有挑战性的任务上表现出色.

***Tacotron*** 在美国英语上获得了 3.82 的主观 5 分制平均意见得分, 在自然度方面超过了生产级的参数化系统.

此外, 由于 ***Tacotron*** 在帧级别生成语音, 它比样本级别的自回归方法要快得多.


## 1.Introduction

<details>
<summary>原文</summary>

> Modern text-to-speech (TTS) pipelines are complex (["Text-to-Speech Synthesis (2009) "](../../Books/2009_Text-to-Speech_Synthesis.md)) .
> For example, it is common for statistical parametric TTS to have a text frontend extracting various linguistic features, a duration model, an acoustic feature prediction model and a complex signal-processing-based vocoder (["Statistical Parametric Speech Synthesis (2009) "](), ["Vocaine the Vocoder and Applications in Speech Synthesis (2015) "]()) .
> These components are based on extensive domain expertise and are laborious to design.
> They are also trained independently, so errors from each component may compound.
> The complexity of modern TTS designs thus leads to substantial engineering efforts when building a new system. 

</details>
<br>

现代文本到语音 (TTS) 的流程是复杂的 ([“Text-to-Speech Synthesis (2009) ”]()) .
例如, 统计参数化TTS通常包括一个提取各种语言特征的文本前端, 一个时长模型, 一个声学特征预测模型和一个基于复杂信号处理的声码器 ([“Statistical Parametric Speech Synthesis (2009) ”](), [“Vocaine the Vocoder and Applications in Speech Synthesis (2015) ”]()) .
这些组件基于广泛的领域专业知识, 设计起来费时费力.
它们也是独立训练的, 因此每个组件的错误可能会累积.
因此, 现代TTS设计的复杂性在构建新系统时导致了大量的工程努力.


<details>
<summary>原文</summary>

> There are thus many advantages of an integrated end-to-end TTS system that can be trained on `<text, audio>` pairs with minimal human annotation.
> First, such a system alleviates the need for laborious feature engineering, which may involve heuristics and brittle design choices.
> Second, it more easily allows for rich conditioning on various attributes, such as speaker or language, or high-level features like sentiment.
> This is because conditioning can occur at the very beginning of the model rather than only on certain components.
> Similarly, adaptation to new data might also be easier.
> Finally, a single model is likely to be more robust than a multi-stage model where each component’s errors can compound.
> These advantages imply that an end-to-end model could allow us to train on huge amounts of rich, expressive yet often noisy data found in the real world. 

</details>
<br>

因此, 一个能够基于最小人类标注的`<文本, 音频>`对进行训练的集成端到端TTS系统具有许多优势.
首先, 这样的系统减轻了对费力的特征工程的需求, 这可能涉及启发式方法和脆弱的设计选择.
其次, 它更容易允许对各种属性 (如说话者或语言) 或高级特征 (如情感) 进行丰富的条件化.
这是因为条件化可以在模型的最开始发生, 而不仅仅是在某些组件上.
同样, 适应新数据也可能更容易.
最后, 单个模型可能比多阶段模型更健壮, 其中每个组件的错误可能会累积.
这些优势意味着端到端模型可以让我们在现实世界中找到的大量丰富, 表现力强但往往嘈杂的数据上进行训练.


<details>
<summary>原文</summary>

> TTS is a large-scale inverse problem: a highly compressed source (text) is “decompressed” into audio.
> Since the same text can correspond to different pronunciations or speaking styles, this is a particularly difficult learning task for an end-to-end model: it must cope with large variations at the signal level for a given input.
> Moreover, unlike end-to-end speech recognition [LAS](../../Models/ASR/LAS.md) or machine translation [GNMT](../../Models/NMT/2016.09.26_GNMT.md), TTS outputs are continuous, and output sequences are usually much longer than those of the input.
> These attributes cause prediction errors to accumulate quickly.
> In this paper, we propose ***Tacotron***, an end-to-end generative TTS model based on the [sequence-to-sequence (seq2seq) ](../_Basis/2014.09.10_Seq2Seq.md) with [attention paradigm](../../Models/NMT/2014.09.01_NMT_by_Jointly_Learning_to_Align_&_Translate.md) .
> Our model takes characters as input and outputs raw spectrogram, using several techniques to improve the capability of a vanilla seq2seq model.
> Given `<text, audio>` pairs, ***Tacotron*** can be trained completely from scratch with random initialization.
> It does not require phoneme-level alignment, so it can easily scale to using large amounts of acoustic data with transcripts.
> With a simple waveform synthesis technique, ***Tacotron*** produces a 3.82 mean opinion score (MOS) on an US English eval set, outperforming a production parametric system in terms of naturalness.

</details>
<br>

TTS是一个大规模的逆问题：一个高度压缩的源 (文本) 被“解压缩”成音频.
由于相同的文本可以对应不同的发音或说话风格, 这对端到端模型来说是一个特别困难的学习任务：它必须为给定的输入处理信号级别的大量变化.
此外, 与端到端语音识别 [LAS](../../Models/ASR/LAS.md) 或机器翻译 [GNMT](../../Models/NMT/2016.09.26_GNMT.md) 不同, TTS输出是连续的, 输出序列通常比输入序列长得多.
这些属性导致预测错误迅速累积.
在本文中, 我们提出了***Tacotron***, 一个基于 [序列到序列 (seq2seq) ](../_Basis/2014.09.10_Seq2Seq.md) 和 [注意力范式](../../Models/NMT/2014.09.01_NMT_by_Jointly_Learning_to_Align_&_Translate.md) 的端到端生成TTS模型.
我们的模型以字符作为输入, 输出原始频谱图, 使用几种技术来提高原始 seq2seq 模型的能力.
给定`<文本, 音频>`对, ***Tacotron***可以完全从头开始随机初始化训练.
它不需要音素级别的对齐, 因此可以轻松扩展到使用大量带有转录的声学数据.
通过一个简单的波形合成技术, ***Tacotron***在美国英语评估集上产生了3.82的平均意见得分 (MOS), 在自然度方面超过了生产级的参数化系统.


## 2.Related Works: 相关工作

<details>
<summary>原文</summary>

> [WaveNet](../../Models/TTS3_Vocoder/2016.09.12_WaveNet.md) is a powerful generative model of audio.
> It works well for TTS, but is slow due to its sample-level autoregressive nature.
> It also requires conditioning on linguistic features from an existing TTS frontend, and thus is not end-to-end: it only replaces the vocoder and acoustic model.
> Another recently-developed neural model is [Deep Voice](../../Models/TTS0_System/2017.02.25_DeepVoice.md), which replaces every component in a typical TTS pipeline by a corresponding neural network.
> However, each component is independently trained, and it’s nontrivial to change the system to train in an end-to-end fashion. 

</details>
<br>

 [WaveNet](../../Models/TTS3_Vocoder/2016.09.12_WaveNet.md) 是一个强大的音频生成模型.
它在TTS方面表现良好, 但由于其样本级别的自回归特性而速度较慢.
它还需要从现有的TTS前端获取语言特征的条件化, 因此不是端到端的：它只替换了声码器和声学模型.
另一个最近开发的神经模型是 [Deep Voice](../../Models/TTS0_System/2017.02.25_DeepVoice.md), 它用相应的神经网络替换了典型TTS流程中的每个组件.
然而, 每个组件是独立训练的, 并且以端到端的方式训练系统并不简单.



<details>
<summary>原文</summary>

> To our knowledge, ["First Step Towards End-to-End Parametric TTS Synthesis: Generating Spectral Parameters with Neural Attention (2016) "]() is the earliest work touching end-to-end TTS using seq2seq with attention.
> However, it requires a pre-trained hidden Markov model (HMM) aligner to help the seq2seq model learn the alignment.
> It’s hard to tell how much alignment is learned by the seq2seq per se.
> Second, a few tricks are used to get the model trained, which the authors note hurts prosody.
> Third, it predicts vocoder parameters hence needs a vocoder.
> Furthermore, the model is trained on phoneme inputs and the experimental results seem to be somewhat limited.

</details>
<br>

据我们所知, ["First Step Towards End-to-End Parametric TTS Synthesis: Generating Spectral Parameters with Neural Attention (2016) "]() 是最早使用带有注意力的 seq2seq 接触端到端TTS的工作.
然而, 它需要一个预训练的隐马尔可夫模型 (HMM) 对齐器来帮助 seq2seq 模型学习对齐.
很难说 seq2seq 本身学到了多少对齐.
其次, 使用了一些技巧来训练模型, 作者指出这损害了韵律.
第三, 它预测声码器参数, 因此需要一个声码器.
此外, 该模型是基于音素输入进行训练的, 实验结果似乎有些有限.


<details>
<summary>原文</summary>

> [Char2Wav](../../Models/E2E/2017.02.18_Char2Wav.md) is an independently-developed end-to-end model that can be trained on characters.
> However, [Char2Wav](../../Models/E2E/2017.02.18_Char2Wav.md) still predicts vocoder parameters before using a [SampleRNN](../../Models/TTS3_Vocoder/2016.12.22_SampleRNN.md) neural vocoder, whereas ***Tacotron*** directly predicts raw spectrogram.
> Also, their seq2seq and [SampleRNN](../../Models/TTS3_Vocoder/2016.12.22_SampleRNN.md) models need to be separately pre-trained, but our model can be trained from scratch.
> Finally, we made several key modifications to the vanilla seq2seq paradigm.
> As shown later, a vanilla seq2seq model does not work well for character-level inputs. 

</details>
<br>

 [Char2Wav](../../Models/E2E/2017.02.18_Char2Wav.md) 是一个独立开发的端到端模型, 可以基于字符进行训练.
然而, [Char2Wav](../../Models/E2E/2017.02.18_Char2Wav.md) 仍然在 [SampleRNN](../../Models/TTS3_Vocoder/2016.12.22_SampleRNN.md) 神经声码器之前预测声码器参数, 而***Tacotron***直接预测原始频谱图.
此外, 他们的 seq2seq 和 [SampleRNN](../../Models/TTS3_Vocoder/2016.12.22_SampleRNN.md) 模型需要分别预训练, 但我们的模型可以从头开始训练.
最后, 我们对原始 seq2seq 范式进行了几项关键修改.
如后文所示, 原始 seq2seq 模型在字符级别输入上表现不佳.


## 3.Methodology: 方法

![](Images/2017.03.29_Tacotron_Fig.01.png) 

<details>
<summary>原文</summary>

> The backbone of ***Tacotron*** is a seq2seq model with attention (["Neural Machine Translation by Jointly Learning to Align and Translate (2014) "](../../Models/NMT/2014.09.01_NMT_by_Jointly_Learning_to_Align_&_Translate.md), ["Grammar as A Foreign Language (2015) "]()) .
> Figure.01 depicts the model, which includes an encoder, an attention-based decoder, and a post-processing net.
> At a high-level, our model takes characters as input and produces spectrogram frames, which are then converted to waveforms.
> We describe these components below. 

</details>
<br>

***Tacotron***的主干是一个带有注意力的 seq2seq 模型 (["Neural Machine Translation by Jointly Learning to Align and Translate (2014) "](../../Models/NMT/2014.09.01_NMT_by_Jointly_Learning_to_Align_&_Translate.md), ["Grammar as A Foreign Language (2015) "]()) .
图01展示了该模型, 它包括一个编码器, 一个基于注意力的解码器和一个后处理网络.
从高层次来看, 我们的模型以字符作为输入, 并产生频谱图帧, 然后将其转换为波形.
我们将在下面描述这些组件.


### 3.1.CBHG module: CBHG 模块

![](Images/2017.03.29_Tacotron_Fig.02.png) 

<details>
<summary>原文</summary>

> We first describe a building block dubbed ***CBHG***, illustrated in Figure.02.
> ***CBHG*** consists of a bank of 1-D convolutional filters, followed by [Highway networks](../../Models/_Basis/HighwayNet.md) and a bidirectional [Gated Recurrent Unit (GRU) ](../_Basis/2014.09.03_GRU.md) recurrent neural net (RNN) .
> ***CBHG*** is a powerful module for extracting representations from sequences.
> The input sequence is first convolved with $K$ sets of 1-D convolutional filters, where the $k$-th set contains $C_k$ filters of width $k$ (i.e. $k = 1, 2, \cdots, K$) .
> These filters explicitly model local and contextual information (akin to modeling unigrams, bi-grams, up to $K$-grams) .
> The convolution outputs are stacked together and further max pooled along time to increase local invariances.
> Note that we use a stride of $1$ to preserve the original time resolution.
> We further pass the processed sequence to a few fixed-width 1-D convolutions, whose outputs are added with the original input sequence via residual connections [ResNet](../_Basis/2015.12.10_ResNet.md) .
> [Batch normalization](../../Modules/Normalization/2015.02.11_BatchNorm.md) is used for all convolutional layers.
> The convolution outputs are fed into a multi-layer highway network to extract high-level features.
> Finally, we stack a bidirectional GRU RNN on top to extract sequential features from both forward and backward context.
> ***CBHG*** is inspired from work in machine translation ["Fully Character-Level Neural Machine Translation without Explicit Segmentation (2016) "](../../Models/NMT/2016.10.10_Fully_Character-Level_NMT_without_Explicit_Segmentation.md), where the main differences from ["Fully Character-Level Neural Machine Translation without Explicit Segmentation (2016) "](../../Models/NMT/2016.10.10_Fully_Character-Level_NMT_without_Explicit_Segmentation.md) include using non-causal convolutions, batch normalization, residual connections, and stride=1 max pooling.
> We found that these modifications improved generalization. 

</details>
<br>

我们首先描述了一个名为***CBHG***的构建块, 如图02所示.
***CBHG***由一组1D卷积滤波器, 随后是 [Highway网络](../../Models/_Basis/HighwayNet.md) 和一个双向 [门控循环单元 (GRU) ](../_Basis/2014.09.03_GRU.md) 循环神经网络 (RNN) 组成.
***CBHG***是一个强大的模块, 用于从序列中提取表示.
输入序列首先与$K$组1D卷积滤波器进行卷积, 其中第$k$组包含$C_k$个宽度为$k$的滤波器 (即$k = 1, 2, \cdots, K$) .
这些滤波器明确地模拟了局部和上下文信息 (类似于模拟单字, 双字, 直到$K$字) .
卷积输出沿时间堆叠在一起, 并进一步进行最大池化以增加局部不变性.
请注意, 我们使用步长为$1$以保持原始时间分辨率.
我们进一步将处理后的序列通过几个固定宽度的1D卷积, 其输出通过残差连接 [ResNet](../_Basis/2015.12.10_ResNet.md) 与原始输入序列相加.
所有卷积层都使用了 [批量归一化](../../Modules/Normalization/2015.02.11_BatchNorm.md) .
卷积输出被送入一个多层Highway网络以提取高级特征.
最后, 我们在顶部堆叠一个双向GRU RNN, 以从正向和反向上下文中提取序列特征.
***CBHG***的灵感来自于机器翻译中的工作 ["Fully Character-Level Neural Machine Translation without Explicit Segmentation (2016) "](../../Models/NMT/2016.10.10_Fully_Character-Level_NMT_without_Explicit_Segmentation.md), 与 ["Fully Character-Level Neural Machine Translation without Explicit Segmentation (2016) "](../../Models/NMT/2016.10.10_Fully_Character-Level_NMT_without_Explicit_Segmentation.md) 的主要区别包括使用非因果卷积, 批量归一化, 残差连接和步长=1的最大池化.
我们发现这些修改提高了泛化能力.


### 3.2.Encoder: 编码器

<details>
<summary>原文</summary>

> The goal of the encoder is to extract robust sequential representations of text.
> The input to the encoder is a character sequence, where each character is represented as a one-hot vector and embedded into a continuous vector.
> We then apply a set of non-linear transformations, collectively called a “pre-net”, to each embedding.
> We use a bottleneck layer with dropout as the pre-net in this work, which helps convergence and improves generalization.
> A ***CBHG*** module transforms the pre-net outputs into the final encoder representation used by the attention module.
> We found that this ***CBHG***-based encoder not only reduces overfitting, but also makes fewer mispronunciations than a standard multi-layer RNN encoder (see our linked page of audio samples) . 

</details>
<br>

编码器的目标是提取文本的鲁棒序列表示.
编码器的输入是一个字符序列, 其中每个字符被表示为一个独热向量, 并嵌入到一个连续向量中.
然后我们对每个嵌入应用一组非线性变换, 统称为“预网络” (pre-net) .
在本工作中, 我们使用了一个带有dropout的瓶颈层作为预网络, 这有助于收敛并提高泛化能力.
一个***CBHG***模块将预网络输出转换为最终的编码器表示, 供注意力模块使用.
我们发现, 这种基于***CBHG***的编码器不仅减少了过拟合, 而且比标准的分层RNN编码器产生更少的错误发音 (参见我们链接的音频样本页面) .


### 3.3.Decoder: 解码器

<details>
<summary>原文</summary>

> We use a content-based tanh attention decoder (see e.g. ["Grammar as A Foreign Language (2015) "]()), where a stateful recurrent layer produces the attention query at each decoder time step.
> We concatenate the context vector and the attention RNN cell output to form the input to the decoder RNNs.
> We use a stack of GRUs with vertical residual connections ([GNMT](../../Models/NMT/2016.09.26_GNMT.md)) for the decoder.
> We found the residual connections speed up convergence.
> The decoder target is an important design choice.
> While we could directly predict raw spectrogram, it’s a highly redundant representation for the purpose of learning alignment between speech signal and text (which is really the motivation of using seq2seq for this task) .
> Because of this redundancy, we use a different target for seq2seq decoding and waveform synthesis.
> The seq2seq target can be highly compressed as long as it provides sufficient intelligibility and prosody information for an inversion process, which could be fixed or trained.
> We use 80-band mel-scale spectrogram as the target, though fewer bands or more concise targets such as cepstrum could be used.
> We use a post-processing network (discussed below) to convert from the seq2seq target to waveform.

</details>
<br>

我们使用基于内容的 tanh 注意力解码器 (例如, 参见 ["Grammar as A Foreign Language (2015) "]()), 其中状态化的循环层在每个解码器时间步生成注意力查询.
我们将上下文向量和注意力RNN单元输出连接起来, 形成解码器RNN的输入.
我们使用带有垂直残差连接 ([GNMT](../../Models/NMT/2016.09.26_GNMT.md)) 的GRU堆栈作为解码器.
我们发现残差连接加快了收敛速度.
解码器目标是一个重要的设计选择.
虽然我们可以直接预测原始频谱图, 但对于学习语音信号和文本之间的对齐 (这实际上是使用 seq2seq 进行此任务的动机) 来说, 它是一个高度冗余的表示.
由于这种冗余, 我们为 seq2seq 解码和波形合成使用不同的目标.
只要 seq2seq 目标为反转过程提供了足够的可理解性和韵律信息, 它就可以高度压缩, 这个反转过程可以是固定的或训练的.
我们使用80频带的梅尔尺度频谱图作为目标, 尽管可以使用更少的频带或更简洁的目标, 如倒谱.
我们使用一个后处理网络 (下面讨论) 将 seq2seq 目标转换为波形.


<details>
<summary>原文</summary>

> We use a simple fully-connected output layer to predict the decoder targets.
> An important trick we discovered was predicting multiple, non-overlapping output frames at each decoder step.
> Predicting r frames at once divides the total number of decoder steps by r, which reduces model size, training time, and inference time.
> More importantly, we found this trick to substantially increase convergence speed, as measured by a much faster (and more stable) alignment learned from attention.
> This is likely because neighboring speech frames are correlated and each character usually corresponds to multiple frames.
> Emitting one frame at a time forces the model to attend to the same input token for multiple timesteps; emitting multiple frames allows the attention to move forward early in training.
> A similar trick is also used in ["Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric Speech Synthesizers for Mobile Devices (2016) "]() but mainly to speed up inference.

</details>
<br>

我们使用一个简单的全连接输出层来预测解码器目标.
我们发现的一个重要技巧是在每个解码器步骤预测多个非重叠的输出帧.
一次预测r帧将解码器步骤总数除以r, 这减少了模型大小, 训练时间和推理时间.
更重要的是, 我们发现这个技巧大大加快了收敛速度, 正如通过注意力学习到的更快 (且更稳定) 的对齐所衡量的那样.
这可能是因为相邻的语音帧是相关的, 并且每个字符通常对应多个帧.
一次发出一个帧迫使模型在多个时间步上关注相同的输入令牌; 一次发出多个帧允许注意力在训练早期向前移动.
类似技巧也在 ["Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric Speech Synthesizers for Mobile Devices (2016) "]() 中使用, 但主要是为了加快推理速度.


<details>
<summary>原文</summary>

> The first decoder step is conditioned on an all-zero frame, which represents a `<GO>` frame.
> In inference, at decoder step $t$, the last frame of the $r$ predictions is fed as input to the decoder at step $t + 1$.
> Note that feeding the last prediction is an ad-hoc choice here – we could use all $r$ predictions.
> During training, we always feed every $r$-th ground truth frame to the decoder.
> The input frame is passed to a pre-net as is done in the encoder.
> Since we do not use techniques such as scheduled sampling ["Scheduled Sampling for Sequence Prediction with RNNs (2015) "]() (we found it to hurt audio quality), the dropout in the pre-net is critical for the model to generalize, as it provides a noise source to resolve the multiple modalities in the output distribution. 

</details>
<br>

第一个解码器步骤是基于一个全零帧的条件, 它代表一个`<GO>`帧.
在推理过程中, 在解码器步骤 $t$, 最后预测的 $r$ 帧被作为输入提供给步骤 $t + 1$ 的解码器.
请注意, 在这里使用最后一个预测是一个即兴的选择——我们可以使用所有 $r$ 个预测.
在训练期间, 我们总是将每 $r$ 个真实帧提供给解码器.
输入帧像在编码器中一样通过预网络传递.
由于我们没有使用诸如 ["Scheduled Sampling for Sequence Prediction with RNNs (2015) "]() 这样的技术 (我们发现它会损害音频质量), 预网络中的dropout对于模型泛化至关重要, 因为它提供了一个噪声源来解决输出分布中的多模态问题.


### 3.4.Post-processing Net and Waveform Synthesis: 后处理网络和波形合成

<details>
<summary>原文</summary>

> As mentioned above, the post-processing net’s task is to convert the seq2seq target to a target that can be synthesized into waveforms.
> Since we use [Griffin-Lim](../_Basis/1984.04.00_Griffin-Lim.md) as the synthesizer, the post-processing net learns to predict spectral magnitude sampled on a linear-frequency scale.
> Another motivation of the post-processing net is that it can see the full decoded sequence.
> In contrast to seq2seq, which always runs from left to right, it has both forward and backward information to correct the prediction error for each individual frame.
> In this work, we use a ***CBHG*** module for the post-processing net, though a simpler architecture likely works as well.
> The concept of a postprocessing network is highly general.
> It could be used to predict alternative targets such as vocoder parameters, or as a WaveNetlike neural vocoder ([WaveNet](../../Models/TTS3_Vocoder/2016.09.12_WaveNet.md), [SampleRNN](../../Models/TTS3_Vocoder/2016.12.22_SampleRNN.md), [Deep Voice](../../Models/TTS0_System/2017.02.25_DeepVoice.md)) that synthesizes waveform samples directly.

</details>
<br>

如上所述, 后处理网络的任务是将 seq2seq 目标转换为可以合成波形的目标.
由于我们使用 [Griffin-Lim](../_Basis/1984.04.00_Griffin-Lim.md) 作为合成器, 后处理网络学习预测线性频率尺度上的频谱幅度采样.
后处理网络的另一个动机是它可以查看完整的解码序列.
与始终从左到右运行的 seq2seq 不同, 它既有正向信息也有反向信息来纠正每个单独帧的预测错误.
在本工作中, 我们使用一个 ***CBHG*** 模块作为后处理网络, 尽管一个更简单的架构可能同样有效.
后处理网络的概念非常通用.
它可以用来预测诸如声码器参数之类的替代目标, 或者作为类似WaveNet的神经声码器 ([WaveNet](../../Models/TTS3_Vocoder/2016.09.12_WaveNet.md), [SampleRNN](../../Models/TTS3_Vocoder/2016.12.22_SampleRNN.md), [Deep Voice](../../Models/TTS0_System/2017.02.25_DeepVoice.md)), 直接合成波形样本.


<details>
<summary>原文</summary>

> We use the [Griffin-Lim](../_Basis/1984.04.00_Griffin-Lim.md) algorithm to synthesize waveform from the predicted spectrogram.
> We found that raising the predicted magnitudes by a power of $1.2$ before feeding to [Griffin-Lim](../_Basis/1984.04.00_Griffin-Lim.md) reduces artifacts, likely due to its harmonic enhancement effect.
> We observed that [Griffin-Lim](../_Basis/1984.04.00_Griffin-Lim.md) converges after $50$ iterations (in fact, about $30$ iterations seems to be enough), which is reasonably fast.
> We implemented [Griffin-Lim](../_Basis/1984.04.00_Griffin-Lim.md) in [TensorFlow](../_Basis/2016.03.14_TensorFlow.md) hence it’s also part of the model.
> While GriffinLim is differentiable (it does not have trainable weights), we do not impose any loss on it in this work.
> We emphasize that our choice of [Griffin-Lim](../_Basis/1984.04.00_Griffin-Lim.md) is for simplicity; while it already yields strong results, developing a fast and high-quality trainable spectrogram to waveform inverter is ongoing work. 

</details>
<br>

我们使用 [Griffin-Lim](../_Basis/1984.04.00_Griffin-Lim.md) 算法从预测的频谱图合成波形.
我们发现, 在将预测的幅度输入到 [Griffin-Lim](../_Basis/1984.04.00_Griffin-Lim.md) 之前, 将其提高到$1.2$的幂次可以减少伪影, 这可能是由于其谐波增强效果.
我们观察到 [Griffin-Lim](../_Basis/1984.04.00_Griffin-Lim.md) 在$50$次迭代后收敛 (实际上, 大约$30$次迭代似乎就足够了), 这是相当快的.
我们在 [TensorFlow](../_Basis/2016.03.14_TensorFlow.md) 中实现了 [Griffin-Lim](../_Basis/1984.04.00_Griffin-Lim.md), 因此它也是模型的一部分.
尽管Griffin-Lim是可微分的 (它没有可训练的权重), 但我们在本工作中不对它施加任何损失.
我们强调, 我们选择 [Griffin-Lim](../_Basis/1984.04.00_Griffin-Lim.md) 是为了简单性; 虽然它已经产生了强大的结果, 但开发一种快速且高质量的可训练频谱图到波形反转器是正在进行的工作.


### 3.5.Model Details: 模型细节

<details>
<summary>原文</summary>

> Table.01 lists the hyper-parameters and network architectures.
> We use log magnitude spectrogram with Hann windowing, 50 ms frame length, 12.5 ms frame shift, and 2048-point Fourier transform.
> We also found pre-emphasis (0.97) to be helpful.
> We use 24 kHz sampling rate for all experiments.

</details>
<br>

表01列出了超参数和网络架构.
我们使用带有汉宁窗的log幅度频谱图, 50毫秒帧长, 12.5毫秒帧移, 以及2048点傅里叶变换.
我们还发现预加重 (0.97) 是有帮助的.
我们为所有实验使用24 kHz的采样率.


<details>
<summary>原文</summary>

> We use $r = 2$ (output layer reduction factor) for the MOS results in this paper, though larger r values (e.g. $r = 5$) also work well.
> We use the [Adam](../_Basis/2014.12.22_Adam.md) optimizer with learning rate decay, which starts from $0.001$ and is reduced to $0.0005$, $0.0003$, and $0.0001$ after $500K$, $1M$ and $2M$ global steps, respectively.
> We use a simple $l_1$ loss for both seq2seq decoder (mel-scale spectrogram) and post-processing net (linear-scale spectrogram) .
> The two losses have equal weights.

</details>
<br>

我们在这篇论文中使用$r = 2$ (输出层减少因子) 进行MOS结果, 尽管更大的$r$值 (例如$r = 5$) 也表现良好.
我们使用 [Adam](../_Basis/2014.12.22_Adam.md) 优化器, 带有学习率衰减, 它从$0.001$开始, 并在$500K$, $1M$和$2M$全局步骤后分别降低到$0.0005$, $0.0003$和$0.0001$.
我们为 seq2seq 解码器 (梅尔尺度频谱图) 和后处理网络 (线性尺度频谱图) 使用简单的$l_1$损失.
这两个损失具有相等的权重.


<details>
<summary>原文</summary>

> We train using a batch size of $32$, where all sequences are padded to a max length.
> It’s a common practice to train sequence models with a loss mask, which masks loss on zero-padded frames.
> However, we found that models trained this way don’t know when to stop emitting outputs, causing repeated sounds towards the end.
> One simple trick to get around this problem is to also reconstruct the zero-padded frames. 

</details>
<br>

我们使用批量大小为$32$进行训练, 其中所有序列都被填充到最大长度.
训练序列模型时使用损失掩码是一种常见做法, 它掩盖了零填充帧上的损失.
然而, 我们发现这样训练的模型不知道何时停止发出输出, 导致在结尾处出现重复的声音.
解决这个问题的一个简单技巧是也重建零填充帧.


## 4.Experiments: 实验

<details>
<summary>原文</summary>

> We train ***Tacotron*** on an internal North American English dataset, which contains about 24.6 hours of speech data spoken by a professional female speaker.
> The phrases are text normalized, e.g. “16” is converted to “sixteen”. 

</details>
<br>

我们在一个内部北美英语数据集上训练 ***Tacotron***, 该数据集包含大约24.6小时的由专业女性发言人所说的语音数据.
短语已经文本规范化, 例如“16”被转换为“sixteen”.


### 4.1.Ablation Analysis: 消融分析

<details>
<summary>原文</summary>

> We conduct a few ablation studies to understand the key components in our model.
> As is common for generative models, it’s hard to compare models based on objective metrics, which often do not correlate well with perception ["A Note on the Evaluation of Generative Models (2015) "]() .
> We mainly rely on visual comparisons instead.
> We strongly encourage readers to listen to the provided samples.
> First, we compare with a vanilla seq2seq model.
> Both the encoder and decoder use 2 layers of residual RNNs, where each layer has 256 GRU cells (we tried LSTM and got similar results) .
> No pre-net or post-processing net is used, and the decoder directly predicts linear-scale log magnitude spectrogram.
> We found that scheduled sampling (sampling rate 0.5) is required for this model to learn alignments and generalize.
> We show the learned attention alignment in Figure.03.
> Figure.03(a) reveals that the vanilla seq2seq learns a poor alignment.
> One problem is that attention tends to get stuck for many frames before moving forward, which causes bad speech intelligibility in the synthesized signal.
> The naturalness and overall duration are destroyed as a result.
> In contrast, our model learns a clean and smooth alignment, as shown in Figure.03(c) .
> Second, we compare with a model with the ***CBHG*** encoder replaced by a 2-layer residual GRU encoder.
> The rest of the model, including the encoder pre-net, remain exactly the same.
> Comparing Figure.03(b) and Figure.03(c), we can see that the alignment from the GRU encoder is noisier.
> Listening to synthesized signals, we found that noisy alignment often leads to mispronunciations.
> The ***CBHG*** encoder reduces overfitting and generalizes well to long and complex phrases.
> Figure.04(a) and Figure.04(b) demonstrate the benefit of using the post-processing net.
> We trained a model without the postprocessing net while keeping all the other components untouched (except that the decoder RNN predicts linear-scale spectrogram) .
> With more contextual information, the prediction from the post-processing net contains better resolved harmonics (e.g. higher harmonics between bins $100$ and $400$) and high frequency formant structure, which reduces synthesis artifacts. 

</details>
<br>

我们进行了一些消融研究, 以了解我们模型中的关键组件.
对于生成模型来说, 很难基于客观指标比较模型, 这些指标通常与感知 ["A Note on the Evaluation of Generative Models (2015) "]() 不太相关.
我们主要依赖于视觉比较.
我们强烈建议读者听取提供的样本.
首先, 我们与一个原始的 seq2seq 模型进行比较.
编码器和解码器都使用 2 层残差 RNN, 每层有 256 个 GRU 单元 (我们尝试了 LSTM 并得到了类似的结果) .
没有使用预网络或后处理网络, 解码器直接预测线性尺度的对数幅度频谱图.
我们发现, 为了学习对齐并泛化, 这个模型需要使用计划采样 (采样率为 0.5) .
我们在图 03 中展示了学习的注意力对齐.
图 03(a) 揭示了原始 seq2seq 学习了一个差的对齐.
一个问题是在向前移动之前, 注意力倾向于在许多帧上停滞, 这导致合成信号中的语音可理解性差.
结果是自然度和整体持续时间被破坏.
相比之下, 我们的模型学习了一个干净和光滑的对齐, 如图 03(c) 所示.
其次, 我们将带有 ***CBHG*** 编码器的模型与一个替换为2层残差GRU编码器的模型进行比较.
模型的其余部分, 包括编码器预网络, 保持完全相同.
比较图 03(b) 和图 03(c), 我们可以看到来自GRU编码器的对齐更嘈杂.
听取合成信号, 我们发现嘈杂的对齐经常导致发音错误.
***CBHG*** 编码器减少了过拟合并很好地泛化到长而复杂的短语.
图 04(a) 和图 04(b) 展示了使用后处理网络的好处.
我们训练了一个没有后处理网络的模型, 同时保持所有其他组件不变 (除了解码器RNN预测线性尺度频谱图) .
有了更多的上下文信息, 来自后处理网络的预测包含了更好地解析的谐波 (例如, 在箱 $100$ 和 $400$ 之间的更高谐波) 和高频共振峰结构, 这减少了合成伪影.


### 4.2.Mean Opinion Score Tests: 平均意见得分测试

<details>
<summary>原文</summary>

> We conduct mean opinion score tests, where the subjects were asked to rate the naturalness of the stimuli in a $5$-point Likert scale score.
> The MOS tests were crowd-sourced from native speakers. 100 unseen phrases were used for the tests and each phrase received $8$ ratings.
> When computing MOS, we only include ratings where headphones were used.
> We compare our model with a parametric (based on LSTM ["Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric Speech Synthesizers for Mobile Devices (2016) "]()) and a concatenative system ["Recent Advances in Google Real-Time HMM-Driven Unit Selection Synthesizer (2016) "](), both of which are in production.
> As shown in Table.02, ***Tacotron*** achieves an MOS of $3.82$, which outperforms the parametric system.
> Given the strong baselines and the artifacts introduced by the [Griffin-Lim](../_Basis/1984.04.00_Griffin-Lim.md) synthesis, this represents a very promising result. 

</details>
<br>

我们进行了平均意见得分 (MOS) 测试, 其中受试者被要求在 5 点 Likert 量表上评价刺激的自然度.
MOS测试是通过众包从母语者那里进行的.
我们使用了 100 个未见过的短语进行测试, 每个短语收到了 8 个评分.
在计算 MOS 时, 我们只包括使用耳机的评分.
我们将我们的模型与基于 LSTM 的参数化系统 (["Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric Speech Synthesizers for Mobile Devices (2016) "]()) 和一个拼接系统 (["Recent Advances in Google Real-Time HMM-Driven Unit Selection Synthesizer (2016) "]()) 进行了比较, 这两个系统都在生产中.
如表02所示, ***Tacotron*** 实现了3.82的MOS, 这超过了参数化系统.
考虑到强大的基线和由 [Griffin-Lim](../_Basis/1984.04.00_Griffin-Lim.md) 合成引入的伪影, 这是一个非常有希望的结果.


## 5.Conclusions: 结论

<details>
<summary>原文</summary>

> We have proposed ***Tacotron***, an integrated end-to-end generative TTS model that takes a character sequence as input and outputs the corresponding spectrogram.
> With a very simple waveform synthesis module, it achieves a $3.82$ MOS score on US English, outperforming a production parametric system in terms of naturalness.
> ***Tacotron*** is frame-based, so the inference is substantially faster than sample-level autoregressive methods.
> Unlike previous work, ***Tacotron*** does not need hand-engineered linguistic features or complex components such as an HMM aligner.
> It can be trained from scratch with random initialization.
> We perform simple text normalization, though recent advancements in learned text normalization ["RNN Approaches to Text Normalization: A Challenge (2016) "]() may render this unnecessary in the future.

</details>
<br>

我们提出了 ***Tacotron***, 一个集成的端到端生成式TTS模型, 它以字符序列作为输入并输出相应的频谱图.
通过一个非常简单的波形合成模块, 它在美式英语上实现了3.82的MOS得分, 在自然度方面超过了生产中的参数化系统.
***Tacotron*** 是基于帧的, 因此推理速度比样本级自回归方法快得多.
与以往的工作不同, ***Tacotron*** 不需要手工设计的语言特征或复杂的组件, 如HMM对齐器.
它可以从头开始训练, 随机初始化.
我们进行了简单的文本规范化, 尽管最近在学习的文本规范化方面的进展 (["RNN Approaches to Text Normalization: A Challenge (2016) "]()) 可能会在未来使这变得不必要.


<details>
<summary>原文</summary>

> We have yet to investigate many aspects of our model; many early design decisions have gone unchanged.
> Our output layer, attention module, loss function, and [Griffin-Lim](../_Basis/1984.04.00_Griffin-Lim.md) -based waveform synthesizer are all ripe for improvement.
> For example, it’s well known that [Griffin-Lim](../_Basis/1984.04.00_Griffin-Lim.md) outputs may have audible artifacts.
> We are currently working on fast and high-quality neural-network-based spectrogram inversion. 

</details>
<br>

我们尚未调查我们模型的许多方面; 许多早期设计决策一直未变.
我们的输出层, 注意力模块, 损失函数和基于 [Griffin-Lim](../_Basis/1984.04.00_Griffin-Lim.md) 的波形合成器都有待改进.
例如, 众所周知, [Griffin-Lim](../_Basis/1984.04.00_Griffin-Lim.md) 输出可能会有可听见的伪影.
我们目前正在研究快速且高质量的基于神经网络的频谱图反转.
