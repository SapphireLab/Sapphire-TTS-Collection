# Tacotron2

<details>
<summary>基本信息</summary>

- 标题: "Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions"
- 作者:
  - 01 Jonathan Shen,
  - 02 Ruoming Pang,
  - 03 Ron J. Weiss,
  - 04 Mike Schuster,
  - 05 Navdeep Jaitly,
  - 06 Zongheng Yang,
  - 07 Zhifeng Chen,
  - 08 Yu Zhang,
  - 09 Yuxuan Wang,
  - 10 RJ Skerry-Ryan,
  - 11 Rif A. Saurous,
  - 12 Yannis Agiomyrgiannakis,
  - 13 Yonghui Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/1712.05884)
  - [Publication](https://doi.org/10.1109/ICASSP.2018.8461368)
  - [Github]
    - 2018.05.04 [NVIDIA/tacotron2](https://github.com/NVIDIA/tacotron2)
    - [coqui-ai/TTS](https://github.com/coqui-ai/TTS/blob/dev/TTS/tts/models/tacotron2.py)
  - [Demo](https://google.github.io/tacotron/publications/tacotron2)
- 文件:
  - [ArXiv](_PDF/1712.05884v2__Tacotron2__Natural_TTS_Synthesis_by_Conditioning_WaveNet_on_Mel_Spectrogram_Predictions.pdf)
  - [Publication](_PDF/1712.05884p0__Tacotron2__ICASSP2018.pdf)

</details>

## Abstract: 摘要

<table>
<tr>
<td width="50%">

This paper describes ***Tacotron2***, a neural network architecture for speech synthesis directly from text.
The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified **WaveNet** model acting as a vocoder to synthesize time-domain waveforms from those spectrograms.
Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech.
To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to **WaveNet** instead of linguistic, duration, and F0 features.
We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the **WaveNet** architecture.

</td>
<td>

本文描述了 ***Tacotron2***, 一个神经网络架构用于直接从文本进行语音合成.
该系统由循环序列到序列特征预测网络 (将字符嵌入映射到梅尔尺度频谱图) 后接修改版的 **WaveNet** 模型 (作为声码器用于从这些频谱图合成时域波形) 组成.
我们的模型达到了 4.53 的平均意见得分, 而专业录制的语音的平均意见得分为 4.58.
为了验证我们的设计选择, 我们展示了我们系统中关键组件的消融实验, 并评估使用梅尔频谱而不是语言时长和 F0 特征作为 **WaveNet** 的条件输入的影响.
我们进一步展示了使用这一紧凑声学中间表示可以显著减小 **WaveNet** 架构的大小.

</td>
</tr>
</table>

## 1·Introduction: 引言

<table>
<tr>
<td width="50%">

Generating natural speech from text (text-to-speech synthesis, TTS) remains a challenging task despite decades of investigation [^01].
Over time, different techniques have dominated the field.
**Concatenative Synthesis** with unit selection, the process of stitching small units of pre-recorded waveforms together [^02], [^03] was the state-of-the-art for many years.
**Statistical Parametric Speech Synthesis** [^04], [^05], [^06], [^07], which directly generates smooth trajectories of speech features to be synthesized by a vocoder, followed, solving many of the issues that concatenative synthesis had with boundary artifacts.
However, the audio produced by these systems often sounds muffled and unnatural compared to human speech.

[^01]: [Book: Text-to-Speech Synthesis.](../../_Books/2009_Text-to-Speech_Synthesis.md) Cambridge University Press 2009.
[^02]: Unit Selection in a Concatenative Speech Synthesis System Using a Large Speech Database. ICASSP 1996.
[^03]: Automatically Clustering Similar Units for Unit Selection in Speech Synthesis. Eurospeech 1997.
[^04]: Speech Parameter Generation Algorithms for HMM-Based Speech Synthesis. ICASSP 2000.
[^05]: Statistical Parametric Speech Synthesis. Speech Communication 2009.
[^06]: Statistical Parametric Speech Synthesis Using Deep Neural Networks. ICASSP 2013.
[^07]: Speech Synthesis Based on Hidden Markov Models. IEEE 2013.

</td>
<td>

</td>
</tr>
<tr>
<td>

**WaveNet**[^08], a generative model of time domain waveforms, produces audio quality that begins to rival that of real human speech and is already used in some complete TTS systems (**DeepVoice**[^09], **DeepVoice2**[^10], **DeepVoice3**[^11]).
The inputs to **WaveNet** (linguistic features, predicted log fundamental frequency (F0), and phoneme durations), however, require significant domain expertise to produce, involving elaborate text-analysis systems as well as a robust lexicon (pronunciation guide).

[^08]: [**WaveNet**: A Generative Model for Raw Audio.](../Vocoder/2016.09.12_WaveNet.md) ArXiv 2016.
[^09]: [**Deep Voice**: Real-time Neural Text-to-Speech.](../TTS0_System/2017.02.25_DeepVoice.md) ArXiv 2017.
[^10]: [**Deep Voice 2**: Multi-Speaker Neural Text-to-Speech.](../TTS0_System/2017.05.24_DeepVoice2.md) ArXiv 2017.
[^11]: [**Deep Voice 3**: 2000-Speaker Neural Text-to-Speech.](../Acoustic/2017.10.20_DeepVoice3.md) ArXiv 2017.

</td>
<td>

</td>
</tr>
<tr>
<td>

**Tacotron**[^12], a **Sequence-to-Sequence** architecture[^13] for producing magnitude spectrograms from a sequence of characters, simplifies the traditional speech synthesis pipeline by replacing the production of these linguistic and acoustic features with a single neural network trained from data alone.
To vocode the resulting magnitude spectrograms, **Tacotron** uses the **Griffin-Lim** algorithm[^14] for phase estimation, followed by an inverse short-time Fourier transform.
As the authors note, this was simply a placeholder for future neural vocoder approaches, as Griffin-Lim produces characteristic artifacts and lower audio quality than approaches like **WaveNet**.

[^12]: [**Tacotron**: Towards End-to-End Speech Synthesis.](../Acoustic/2017.03.29_Tacotron.md) InterSpeech 2017.
[^13]: [Sequence-to-Sequence Learning with Neural Networks.](../_Basis/2014.09.10_Seq2Seq.md) 2014.
[^14]: [**Griffin-Lim**: Signal Estimation from Modified Short-Time Fourier Transform](../Vocoder/1984.04.00_Griffin-Lim.md)

</td>
</tr>
<tr>
<td>

In this paper, we describe a unified, entirely neural approach to speech synthesis that combines the best of the previous approaches: a sequence-to-sequence **Tacotron**-style[^12] model that generates mel spectrograms, followed by a modified **WaveNet** vocoder (**DeepVoice**[^10], **Speaker-Dependent WaveNet**[^15]).
Trained directly on normalized character sequences and corresponding speech waveforms, our model learns to synthesize natural sounding speech that is difficult to distinguish from real human speech.

[^15]: Speaker-Dependent WaveNet Vocoder. InterSpeech 2017.

</td>
<td>

</td>
</tr>
<tr>
<td>

**DeepVoice3**[^11] describes a similar approach.
However, unlike our system, its naturalness has not been shown to rival that of human speech.
**Char2Wav**[^16] describes yet another similar approach to end-to-end TTS using a neural vocoder.
However, they use different intermediate representations (traditional vocoder features) and their model architecture differs significantly.

[^16]: [**Char2Wav**: End-to-End Speech Synthesis.](../E2E/2017.02.18_Char2Wav.md) ICLR 2017.

</td>
<td>

</td>
</tr>
</table>

## 2·Related Works: 相关工作

None

## 3·Methodology: 方法

![](Images/2017.12.16_Tacotron2_Fig.01.png)
<a id="Figure.01">Figure.01. Block Diagram of Tacotron2 System Architecture.</a>

<table>
<tr>
<td width="50%">

Our proposed system consists of two components, shown in [Figure.01](#Figure.01):
(1) a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence,
(2) a modified version of **WaveNet** which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames.

</td>
<td>

我们所提出的系统由两个组件组成, 如[图 01](#Figure.01) 所示.
1. 循环序列到序列特征预测网络, 结合注意力机制, 用于从输入字符序列预测梅尔频谱图帧序列.
2. 修改版的 **WaveNet**, 以预测的梅尔频谱图帧为条件生成时域波形样本.

</td>
</tr>
</table>

### 3.1.Intermediate Feature Representation: 中间特征表示

<table>
<tr>
<td width="50%">

In this work we choose a low-level acoustic representation: mel-frequency spectrograms, to bridge the two components.
Using a representation that is easily computed from time-domain waveforms allows us to train the two components separately.
This representation is also smoother than waveform samples and is easier to train using a squared error loss because it is invariant to phase within each frame.

</td>
<td>

在本项工作中, 我们选择低级别的声学表示: 梅尔频率谱图 (Mel-Frequency Spectrograms) 作为两个组件的桥梁.
使用可由时域波形简单计算得到的表示允许我们可以分别训练两个组件.
这种表示比波形样本更平滑, 并且更容易使用均方误差损失进行训练, 因为它在每帧内都不受相位的影响.

</td>
</tr>
<tr>
<td>

A mel-frequency spectrogram is related to the linear-frequency spectrogram, i.e., the short-time Fourier transform (STFT) magnitude.
It is obtained by applying a nonlinear transform to the frequency axis of the STFT, inspired by measured responses from the human auditory system, and summarizes the frequency content with fewer dimensions.
Using such an auditory frequency scale has the effect of emphasizing details in lower frequencies, which are critical to speech intelligibility, while de-emphasizing high frequency details, which are dominated by fricatives and other noise bursts and generally do not need to be modeled with high fidelity.
Because of these properties, features derived from the mel scale have been used as an underlying representation for speech recognition for many decades [^17].

[^17]: Comparison of Parametric Representations for Monosyllabic Word Recognition in Continuously Spoken Sentences. TASLP 1980.

</td>
<td>

梅尔频率谱图和线性频率谱图相关, 即短时傅里叶变换 (STFT) 幅度.
它是通过对 STFT 的频率轴应用非线性变换得到的, 受到人类听觉系统的度量响应启发, 并使用更少的维度对频率内容进行总结.
使用这种听觉频率尺度有着强调对语音可理解性重要的低频细节的效果, 同时抑制高频细节, 主要由辅音和其他噪声波形所主导, 并不需要用高精度模型来建模.
由于这些特性, 基于梅尔尺度导出的特征已作为语音识别的基础表示用了几十年 [^17].

</td>
</tr>
<tr>
<td>

While linear spectrograms discard phase information (and are therefore lossy), algorithms such as **Griffin-Lim**[^14] are capable of estimating this discarded information, which enables time-domain conversion via the inverse short-time Fourier transform.
Mel spectrograms discard even more information, presenting a challenging inverse problem.
However, in comparison to the linguistic and acoustic features used in **WaveNet**, the mel spectrogram is a simpler, lower-level acoustic representation of audio signals.
It should therefore be straightforward for a similar **WaveNet** model conditioned on mel spectrograms to generate audio, essentially as a neural vocoder.
Indeed, we will show that it is possible to generate high quality audio from mel spectrograms using a modified **WaveNet** architecture.

</td>
<td>

虽然线性谱丢失了相位信息 (有损的), 如 **Griffin-Lim** 等算法能够估计被丢弃的信息, 这使得通过逆短时傅里叶变换 (ISTFT) 进行时域转换成为可能.
梅尔频谱丢失了更多信息, 使得逆变换问题变得棘手.
然而, 和 **WaveNet** 中使用的语言和声学特征相比, 梅尔频谱图是一种更简单的, 低级别的音频信号表示.
因此, 以梅尔频谱为条件的相似 **WaveNet** 模型应该可以直接生成音频, 以作为神经声码器.
事实上, 我们将展示使用修改版 **WaveNet** 架构可以从梅尔频谱图生成高质量音频.

</td>
</tr>
</table>

### 3.2.Spectrogram Prediction Network: 频谱预测网络

<table>
<tr>
<td width="50%">

As in **Tacotron**, mel spectrograms are computed through a short-time Fourier transform (STFT) using a 50 ms frame size, 12.5 ms frame hop, and a Hann window function.
We experimented with a 5 ms frame hop to match the frequency of the conditioning inputs in the original **WaveNet**, but the corresponding increase in temporal resolution resulted in significantly more pronunciation issues.

</td>
<td>

和 **Tacotron** 一样, 梅尔频谱通过短时傅里叶变换计算, 使用 50 毫秒帧大小, 12.5 毫秒帧移, Hann 窗函数.
我们对 5 毫秒帧移进行实验, 以匹配原始 **WaveNet** 中的条件输入的频率, 但相应增加的时域分辨率会导致显著更多的发音问题.

</td>
</tr>
<tr>
<td>

We transform the STFT magnitude to the mel scale using an 80 channel mel filterbank spanning 125 Hz to 7.6 kHz, followed by log dynamic range compression.
Prior to log compression, the filterbank output magnitudes are clipped to a minimum value of 0.01 in order to limit dynamic range in the logarithmic domain.

</td>
<td>

我们将 STFT 幅度转换到梅尔尺度, 使用 80 通道梅尔滤波器, 范围从 125 Hz 到 7.6 kHz, 后跟对数动态范围压缩.
在对数压缩之前, 滤波器输出幅度进行裁剪, 最小值为 0.01, 以限制对数域中的动态范围.

</td>
</tr>
<tr>
<td>

The network is composed of an encoder and a decoder with attention.
The encoder converts a character sequence into a hidden feature representation which the decoder consumes to predict a spectrogram.
Input characters are represented using a learned 512-dimensional character embedding, which are passed through a stack of 3 convolutional layers each containing 512 filters with shape5 × 1, i.e., where each filter spans 5 characters, followed by **Batch Normalization**[^18] and ReLU activations.
As in **Tacotron**, these convolutional layers model longer-term context (e.g.,N-grams) in the input character sequence.
The output of the final convolutional layer is passed into a single bi-directional[^19] **LSTM**[^20] layer containing 512 units (256 in each direction) to generate the encoded features.

[^18]: [**Batch Normalization**: Accelerating Deep Network Training by Reducing Internal Covariate Shift.](../../Modules/Normalization/2015.02.11_BatchNorm.md) ICML 2015.
[^19]: Bidirectional Recurrent Neural Networks. TSP 1997.
[^20]: [**LSTM**: Long Short-Term Memory.](../_Basis/2014.02.05_LSTM.md) Neural Computation. 1997.

</td>
<td>

网络由一个编码器和解码器组成, 结合了注意力机制.
编码器将字符序列转换到隐藏特征表示, 解码器用于预测频谱图.
输入字符使用学习到的 512 维字符嵌入进行表示, 然后通过堆叠的三个卷积层, 每个卷积层包含 512 个 5 × 1 过滤器, 即每个过滤器跨越 5 个字符, 后跟 **Batch Normalization**[^18] 和 ReLU 激活函数.
和 **Tacotron** 一样, 这些卷积层建模输入字符序列中的更长的上下文 (例如 N-grams).
最后卷积层的输出被传入单个双向[^19] LSTM [^20] 层, 包含 512 个单元 (双向各 256 个), 用于生成编码特征.

</td>
</tr>
<tr>
<td>

The encoder output is consumed by an attention network which summarizes the full encoded sequence as a fixed-length context vector for each decoder output step.
We use the location-sensitive attention from [^21], which extends the additive attention mechanism [^22] to use cumulative attention weights from previous decoder time steps as an additional feature.
This encourages the model to move forward consistently through the input, mitigating potential failure modes where some subsequences are repeated or ignored by the decoder.
Attention probabilities are computed after projecting inputs and location features to 128-dimensional hidden representations.
Location features are computed using 32 1-D convolution filters of length 31.

[^21]: Attention-Based Models for Speech Recognition. NIPS 2015.
[^22]: [Neural Machine Translation by Jointly Learning to Align and Translate.](../NMT/2014.09.01_NMT_by_Jointly_Learning_to_Align_&_Translate.md) ICLR 2015.

</td>
<td>

编码器输出通过注意力网络, 总结整个编码后序列为一个固定长度的上下文向量, 用于每个解码器输出步骤.
我们使用对位置敏感的注意力机制[^21], 将加性注意力机制[^22]扩展到使用从之前的解码器时间步累加注意力权重作为附加特征.
这鼓励模型通过和输入一致地向前移动, 减轻潜在的失败模式, 其中一些子序列被解码器忽略或重复.
注意力概率在将输入和位置特征映射到 128 位隐藏表示后进行计算.
位置特征使用 32 个长为 31 的一维卷积滤波器进行计算.

</td>
</tr>
<tr>
<td>

The decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time.
The prediction from the previous time step is first passed through a small pre-net containing 2 fully connected layers of 256 hidden ReLU units.
We found that the pre-net acting as an information bottleneck was essential for learning attention.
The pre-net output and attention context vector are concatenated and passed through a stack of 2 uni-directional LSTM layers with 1024 units.
The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame.
Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction.
Each post-net layer is comprised of 512 filters with shape5 × 1with batch normalization, followed by tanh activations on all but the final layer.

</td>
<td>

解码器是自回归循环神经网络, 从编码后的输入序列一次一帧地预测梅尔频谱.
从前一个时间步地预测首先通过小的 Pre-Net, 包含两个全连接层, 256 个隐藏 ReLU 单元.
我们发现 Pre-Net 作为信息瓶颈对于学习注意力至关重要.
Pre-Net 输出和注意力上下文向量被拼接并通过两个单向 LSTM 层, 1024 个单元.
LSTM 的输出和注意力上下文向量的拼接通过线性变换层映射以预测目标频谱帧.
最后, 预测得到的梅尔频谱传递给 5 层卷积 Post-Net, 用于预测残差以添加给预测以提升整体重构性能.
每个 Post-Net 层由 512 个滤波器组成, 形状为 5 × 1, 后跟批归一化, 和 Tanh 激活函数, 但不包括最后一层.

</td>
</tr>
<tr>
<td>

We minimize the summed mean squared error (MSE) from before and after the post-net to aid convergence.
We also experimented with a log-likelihood loss by modeling the output distribution with a Mixture Density Network [^23], [^24] to avoid assuming a constant variance over time, but found that these were more difficult to train and they did not lead to better sounding samples.

[^23]: Mixture Density Networks. 1994.
[^24]: PhD Thesis: On Supervised Learning from Sequential Data with Applications for Speech Recognition. 1999.

</td>
<td>

我们 Post-Net 之前和之后的最小化求和均方误差以辅助收敛.
我们也通过使用 Mixture Density Network 模型[^23], [^24] 建模输出分布用对数似然损失进行实验, 以避免时间上的恒定方差, 但发现训练起来更困难, 并且没有得到更自然的音频样本.

</td>
</tr>
<tr>
<td>

In parallel to spectrogram frame prediction, the concatenation of decoder LSTM output and the attention context is projected down to a scalar and passed through a sigmoid activation to predict the probability that the output sequence has completed.
This “stop token” prediction is used during inference to allow the model to dynamically determine when to terminate generation instead of always generating for a fixed duration.
Specifically, generation completes at the first frame for which this probability exceeds a threshold of 0.5.

</td>
<td>

平行于频谱图帧预测, 解码器 LSTM 的输出和注意力上下文的拼接映射到一个标量并通过 Sigmoid 激活函数以预测输出序列是否完成的概率.
这一 "停止 Token" 的预测在推理时使用以允许模型动态决定何时终止生成而不是总是生成固定时长.
具体来说, 生成过程在概率超过 0.5 的第一个帧完成.

</td>
</tr>
<tr>
<td>

The convolutional layers in the network are regularized using **Dropout**[^25] with probability 0.5, and LSTM layers are regularized using **Zoneout**[^26] with probability 0.1.
In order to introduce output variation at inference time, dropout with probability 0.5 is applied only to layers in the pre-net of the autoregressive decoder.

[^25]: [**Dropout**: A Simple Way to Prevent Neural Networks from Overfitting.](../_Basis/Dropout.md) JMLR 2014.
[^26]: [**Zoneout**: Regularizing RNNs by Randomly Preserving Hidden Activations.](../_Basis/Zoneout.md) ICLR 2017.

</td>
<td>

网络中的卷积层使用 **Dropout**[^25] 进行正则化, 概率为 0.5.
LSTM 层使用 **Zoneout**[^26] 进行正则化, 概率为 0.1.
为了在推理时引入输出变化, 只在自回归解码器的 Pre-Net 层上使用 Dropout, 概率为 0.5.

</td>
</tr>
<tr>
<td>

In contrast to the original **Tacotron**, our model uses simpler building blocks, using vanilla LSTM and convolutional layers in the encoder and decoder instead of “CBHG” stacks and GRU recurrent layers.
We do not use a “reduction factor”, i.e., each decoder step corresponds to a single spectrogram frame.

</td>
<td>

和原始 **Tacotron** 相比, 我们的模型使用更简单的构造块, 使用原始 LSTM 和卷积层代替 "CBHG" 模块堆叠和 GRU 循环层.
我们不使用 "降采样因子", 即每个解码器步对应一个频谱帧.

</td>
</tr>
</table>

### 3.3.WaveNet Vocoder: WaveNet 声码器

<table>
<tr>
<td width="50%">

We use a modified version of the **WaveNet**[^08] architecture to invert the mel spectrogram feature representation into time-domain waveform samples.
As in the original architecture, there are 30 dilated convolution layers, grouped into 3 dilation cycles, i.e., the dilation rate of layer k (k = 0 . . . 29) is2k (mod 10).
To work with the 12.5 ms frame hop of the spectrogram frames, only 2 upsampling layers are used in the conditioning stack instead of 3 layers.

</td>
<td>

我们使用 **WaveNet**[^08] 架构的修改版, 将梅尔频谱特征表示逆变换为时域波形样本.
和原始架构一样, 共有 30 层膨胀卷积层, 分组为 3 个膨胀周期, 即第 k 层的膨胀率为 $2^{k (\text{mod} 10)}$.
为了适配 12.5 毫秒的频谱帧移, 条件模块堆叠中只使用 2 个上采样层, 而不是 3 层.

</td>
</tr>
<tr>
<td>

Instead of predicting discretized buckets with a softmax layer, we follow **PixelCNN++**[^27] and **Parallel WaveNet**[^28] and use a 10-component mixture of logistic distributions (MoL) to generate 16-bit samples at 24 kHz.
To compute the logistic mixture distribution, the **WaveNet** stack output is passed through a ReLU activation followed by a linear projection to predict parameters (mean, log scale, mixture weight) for each mixture component.
The loss is computed as the negative log-likelihood of the ground truth sample.

[^27]: **PixelCNN++**: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications. ICLR 2017.
[^28]: [**Parallel WaveNet**: Fast High-Fidelity Speech Synthesis.](../Vocoder/2017.11.28_Parallel_WaveNet.md) ArXiv 2017..

</td>
<td>

和使用 Softmax 层预测离散分桶不同, 我们遵循 **PixelCNN++**[^27] 和 **Parallel WaveNet**[^28], 使用 10 组 Logistic 分布 (Mixture of Logistic Distributions, MoL) 生成 16 位, 采样率为 24 kHz 的音频, **WaveNet** 堆叠输出经过 ReLU 激活, 然后线性投影预测每个混合成分的参数 (均值, 对数尺度, 混合权重).
损失以真实样本的负对数似然计算.

</td>
</tr>
</table>

## 4·Experiments: 实验

### 4.1.Training Setup: 训练设置

[^29]: [**Adam**: A Method for Stochastic Optimization.](../../Modules/Optimization/2014.12.22_Adam.md) ICLR 2015.

### 4.2.Evaluation: 评价

[^30]: Recent Advances in Google Real-Time HMM-Driven Unit Selection Synthesize. InterSpeech 2016.
[^31]: Fast, Compact, and High-Quality LSTM-RNN Based Statistical Parametric Speech Synthesizers for Mobile Devices. InterSpeech 2016.

### 4.3.Ablation Studies: 消融研究

## 5·Results: 结果

## 6·Conclusions: 结论

<table>
<tr>
<td width="50%">

This paper describes ***Tacotron2***, a fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predicts mel spectrograms with a modified **WaveNet** vocoder.
The resulting system synthesizes speech with **Tacotron**-level prosody and **WaveNet**-level audio quality.
This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech.

</td>
<td>

</td>
</tr>
</table>