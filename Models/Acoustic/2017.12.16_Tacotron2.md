# Tacotron2

<details>
<summary>基本信息</summary>

- 标题: "Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions"
- 作者:
  - 01 Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A. Saurous, Yannis Agiomyrgiannakis, Yonghui Wu
- 链接:
  - [ArXiv](https://arxiv.org/abs/1712.05884)
  - [Publication](https://doi.org/10.1109/ICASSP.2018.8461368)
  - [Github]
    - 2018.05.04 [NVIDIA/tacotron2](https://github.com/NVIDIA/tacotron2)
    - [coqui-ai/TTS](https://github.com/coqui-ai/TTS/blob/dev/TTS/tts/models/tacotron2.py)
  - [Demo](https://google.github.io/tacotron/publications/tacotron2)
- 文件:
  - [ArXiv](_PDF/1712.05884v2__Tacotron2__Natural_TTS_Synthesis_by_Conditioning_WaveNet_on_Mel_Spectrogram_Predictions.pdf)
  - [Publication](_PDF/1712.05884p0__Tacotron2__ICASSP2018.pdf)

</details>

## Abstract: 摘要

This paper describes Tacotron2, a neural network architecture for speech synthesis directly from text.
The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms.
Our model achieves a mean opinion score (MOS) of4.53comparable to a MOS of4.58for professionally recorded speech.
To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, andF0features.
We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.

## 1·Introduction: 引言

Generating natural speech from text (text-to-speech synthesis, TTS) remains a challenging task despite decades of investigation ([2009_Text-to-Speech_Synthesis](../../_Books/2009_Text-to-Speech_Synthesis.md)).
Over time, different techniques have dominated the field.
Concatenative synthesis with unit selection, the process of stitching small units of pre-recorded waveforms together [2], [3] was the state-of-the-art for many years.
Statistical parametric speech synthesis [4], [5], [6], [7], which directly generates smooth trajectories of speech features to be synthesized by a vocoder, followed, solving many of the issues that concatenative synthesis had with boundary artifacts.
However, the audio produced by these systems often sounds muffled and unnatural compared to human speech.

[2016.09.12_WaveNet](../Vocoder/2016.09.12_WaveNet.md), a generative model of time domain waveforms, produces audio quality that begins to rival that of real human speech and is already used in some complete TTS systems [2017.02.25_DeepVoice](../TTS0_System/2017.02.25_DeepVoice.md), [2017.05.24_DeepVoice2](../TTS0_System/2017.05.24_DeepVoice2.md), [2017.10.20_DeepVoice3](../TTS2_Acoustic/2017.10.20_DeepVoice3.md).
The inputs to WaveNet (linguistic features, predicted log fundamental frequency (F0), and phoneme durations), however, require significant domain expertise to produce, involving elaborate text-analysis systems as well as a robust lexicon (pronunciation guide).

[2017.03.29_Tacotron](2017.03.29_Tacotron.md), a [2014.09.10_Seq2Seq](../_Basis/2014.09.10_Seq2Seq.md) for producing magnitude spectrograms from a sequence of characters, simplifies the traditional speech synthesis pipeline by replacing the production of these linguistic and acoustic features with a single neural network trained from data alone.
To vocode the resulting magnitude spectrograms, Tacotron uses the [1984.04.00_Griffin-Lim](../Vocoder/1984.04.00_Griffin-Lim.md) algorithm for phase estimation, followed by an inverse short-time Fourier transform.
As the authors note, this was simply a placeholder for future neural vocoder approaches, as Griffin-Lim produces characteristic artifacts and lower audio quality than approaches like WaveNet.

In this paper, we describe a unified, entirely neural approach to speech synthesis that combines the best of the previous approaches: a sequence-to-sequence [2017.03.29_Tacotron](2017.03.29_Tacotron.md)-style model that generates mel spectrograms, followed by a modified WaveNet vocoder [2017.05.24_DeepVoice2](../TTS0_System/2017.05.24_DeepVoice2.md), [15].
Trained directly on normalized character sequences and corresponding speech waveforms, our model learns to synthesize natural sounding speech that is difficult to distinguish from real human speech.

[2017.10.20_DeepVoice3](../TTS2_Acoustic/2017.10.20_DeepVoice3.md) describes a similar approach.
However, unlike our system, its naturalness has not been shown to rival that of human speech.
[2017.02.18_Char2Wav](../E2E/2017.02.18_Char2Wav.md) describes yet another similar approach to end-to-end TTS using a neural vocoder.
However, they use different intermediate representations (traditional vocoder features) and their model architecture differs significantly.

## 2·Related Works: 相关工作

None

## 3·Methodology: 方法

![](../../2017.12.16.Tacotron2.Fig.01.png)

Our proposed system consists of two components, shown in Figure.01: (1) a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence, and (2) a modified version of WaveNet which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames.

### 3.1.Intermediate Feature Representation: 中间特征表示

In this work we choose a low-level acoustic representation: mel-frequency spectrograms, to bridge the two components.
Using a representation that is easily computed from time-domain waveforms allows us to train the two components separately.
This representation is also smoother than waveform samples and is easier to train using a squared error loss because it is invariant to phase within each frame.

A mel-frequency spectrogram is related to the linear-frequency spectrogram, i.e., the short-time Fourier transform (STFT) magnitude.
It is obtained by applying a nonlinear transform to the frequency axis of the STFT, inspired by measured responses from the human auditory system, and summarizes the frequency content with fewer dimensions.
Using such an auditory frequency scale has the effect of emphasizing details in lower frequencies, which are critical to speech intelligibility, while de-emphasizing high frequency details, which are dominated by fricatives and other noise bursts and generally do not need to be modeled with high fidelity.
Because of these properties, features derived from the mel scale have been used as an underlying representation for speech recognition for many decades [17].

While linear spectrograms discard phase information (and are therefore lossy), algorithms such as [1984.04.00_Griffin-Lim](../Vocoder/1984.04.00_Griffin-Lim.md) are capable of estimating this discarded information, which enables time-domain conversion via the inverse short-time Fourier transform.
Mel spectrograms discard even more information, presenting a challenging inverse problem.
However, in comparison to the linguistic and acoustic features used in WaveNet, the mel spectrogram is a simpler, lower-level acoustic representation of audio signals.
It should therefore be straightforward for a similar WaveNet model conditioned on mel spectrograms to generate audio, essentially as a neural vocoder.
Indeed, we will show that it is possible to generate high quality audio from mel spectrograms using a modified WaveNet architecture.

### 3.2.Spectrogram Prediction Network: 频谱预测网络

As in Tacotron, mel spectrograms are computed through a short-time Fourier transform (STFT) using a 50 ms frame size, 12.5 ms frame hop, and a Hann window function.
We experimented with a 5 ms frame hop to match the frequency of the conditioning inputs in the original WaveNet, but the corresponding increase in temporal resolution resulted in significantly more pronunciation issues.

We transform the STFT magnitude to the mel scale using an 80 channel mel filterbank spanning 125 Hz to 7.6 kHz, followed by log dynamic range compression.
Prior to log compression, the filterbank output magnitudes are clipped to a minimum value of 0.01 in order to limit dynamic range in the logarithmic domain.

The network is composed of an encoder and a decoder with attention.
The encoder converts a character sequence into a hidden feature representation which the decoder consumes to predict a spectrogram.
Input characters are represented using a learned 512-dimensional character embedding, which are passed through a stack of 3 convolutional layers each containing 512 filters with shape5 × 1, i.e., where each filter spans 5 characters, followed by [2015.02.11_BatchNorm](../../Modules/Normalization/2015.02.11_BatchNorm.md) and ReLU activations.
As in Tacotron, these convolutional layers model longer-term context (e.g.,N-grams) in the input character sequence.
The output of the final convolutional layer is passed into a single bi-directional [19] [2014.02.05_LSTM](../_Basis/2014.02.05_LSTM.md) layer containing 512 units (256 in each direction) to generate the encoded features.

The encoder output is consumed by an attention network which summarizes the full encoded sequence as a fixed-length context vector for each decoder output step.
We use the location-sensitive attention from [21], which extends the additive attention mechanism [22] to use cumulative attention weights from previous decoder time steps as an additional feature.
This encourages the model to move forward consistently through the input, mitigating potential failure modes where some subsequences are repeated or ignored by the decoder.
Attention probabilities are computed after projecting inputs and location features to 128-dimensional hidden representations.
Location features are computed using 32 1-D convolution filters of length 31.

The decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time.
The prediction from the previous time step is first passed through a small pre-net containing 2 fully connected layers of 256 hidden ReLU units.
We found that the pre-net acting as an information bottleneck was essential for learning attention.
The pre-net output and attention context vector are concatenated and passed through a stack of 2 uni-directional LSTM layers with 1024 units.
The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame.
Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction.
Each post-net layer is comprised of 512 filters with shape5 × 1with batch normalization, followed by tanh activations on all but the final layer.

We minimize the summed mean squared error (MSE) from before and after the post-net to aid convergence.
We also experimented with a log-likelihood loss by modeling the output distribution with a Mixture Density Network [23], [24] to avoid assuming a constant variance over time, but found that these were more difficult to train and they did not lead to better sounding samples.

In parallel to spectrogram frame prediction, the concatenation of decoder LSTM output and the attention context is projected down to a scalar and passed through a sigmoid activation to predict the probability that the output sequence has completed.
This “stop token” prediction is used during inference to allow the model to dynamically determine when to terminate generation instead of always generating for a fixed duration.
Specifically, generation completes at the first frame for which this probability exceeds a threshold of 0.5.

The convolutional layers in the network are regularized using [Dropout](../_Basis/Dropout.md) with probability 0.5, and LSTM layers are regularized using zoneout [26] with probability 0.1.
In order to introduce output variation at inference time, dropout with probability 0.5 is applied only to layers in the pre-net of the autoregressive decoder.

In contrast to the original Tacotron, our model uses simpler building blocks, using vanilla LSTM and convolutional layers in the encoder and decoder instead of “CBHG” stacks and GRU recurrent layers.
We do not use a “reduction factor”, i.e., each decoder step corresponds to a single spectrogram frame.

### 3.3.WaveNet Vocoder: WaveNet 声码器

We use a modified version of the [2016.09.12_WaveNet](../Vocoder/2016.09.12_WaveNet.md) architecture to invert the mel spectrogram feature representation into time-domain waveform samples.
As in the original architecture, there are 30 dilated convolution layers, grouped into 3 dilation cycles, i.e., the dilation rate of layer k (k = 0 . . . 29) is2k (mod 10).
To work with the 12.5 ms frame hop of the spectrogram frames, only 2 upsampling layers are used in the conditioning stack instead of 3 layers.

Instead of predicting discretized buckets with a softmax layer, we follow PixelCNN++ [27] and [2017.11.28_Parallel_WaveNet](../Vocoder/2017.11.28_Parallel_WaveNet.md) and use a 10-component mixture of logistic distributions (MoL) to generate 16-bit samples at 24 kHz.
To compute the logistic mixture distribution, the WaveNet stack output is passed through a ReLU activation followed by a linear projection to predict parameters (mean, log scale, mixture weight) for each mixture component.
The loss is computed as the negative log-likelihood of the ground truth sample.

## 4·Experiments: 实验

### 4.1.Training Setup: 训练设置

### 4.2.Evaluation: 评价

### 4.3.Ablation Studies: 消融研究

## 5·Results: 结果

## 6·Conclusions: 结论

This paper describes Tacotron2, a fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predicts mel spectrograms with a modified WaveNet vocoder.
The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality.
This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech.