# FleSpeech

<details>
<summary>基本信息</summary>

- 标题: "FleSpeech: Flexibly Controllable Speech Generation with Various Prompts"
- 作者:
  - 01 Hanzhao Li (NPU@ASLP, lihanzhao.mail@gmail.com)
  - 02 Yuke Li (NPU@ASLP, yukeli6479@gmail.com)
  - 03 Xinsheng Wang (HKUST, w.xinshawn@gmail.com)
  - 04 Jingbin Hu (NPU@ASLP, hujingbin553@gmail.com)
  - 05 Qicong Xie (Tencent AI Lab, jerryqcxie@tencent.com)
  - 06 Shan Yang (Tencent AI Lab, shaanyang@tencent.com)
  - 07 Lei Xie (NPU@ASLP, lxie@nwpu.edu.cn)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.04644)
  - [Publication]()
  - [Github]()
  - [Demo](https://kkksuper.github.io/FleSpeech/)
- 文件:
  - [ArXiv](_PDF/2501.04644v1__FleSpeech__Flexibly_Controllable_Speech_Generation_with_Various_Prompts.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Controllable speech generation methods typically rely on single or fixed prompts, hindering creativity and flexibility.
These limitations make it difficult to meet specific user needs in certain scenarios, such as adjusting the style while preserving a selected speaker's timbre, or choosing a style and generating a voice that matches a character's visual appearance.
To overcome these challenges, we propose ***FleSpeech***, a novel multi-stage speech generation framework that allows for more flexible manipulation of speech attributes by integrating various forms of control.
***FleSpeech*** employs a multimodal prompt encoder that processes and unifies different text, audio, and visual prompts into a cohesive representation.
This approach enhances the adaptability of speech synthesis and supports creative and precise control over the generated speech.
Additionally, we develop a data collection pipeline for multimodal datasets to facilitate further research and applications in this field.
Comprehensive subjective and objective experiments demonstrate the effectiveness of ***FleSpeech***.
Audio samples are available at [this https URL](https://kkksuper.github.io/FleSpeech/).

</details>
<br>

可控语音生成方法通常依赖于单个或固定提示, 限制了创造性和灵活性.
这些限制使得难以满足某些场景下的特定用户需求, 例如调整风格同时保留选定说话人的音色, 或选择风格并生成与角色视觉外观相匹配的声音.

为了克服这些挑战, 我们提出了一种新式多阶段语音生成框架 ***FleSpeech***, 通过集成多种形式的控制来实现对语音属性的更灵活的控制.
***FleSpeech*** 采用多模态提示编码器, 将不同形式的文本音频和视觉提示统一为一个连贯的表示.
这种方法增强了语音合成的适应性, 并支持对生成语音的创造性和精确控制.

此外, 我们开发了多模态数据集的数据收集流程以促进该领域的进一步的研究和应用.
全面的主观和客观实验表明了 ***FleSpeech*** 的有效性.
音频示例可在[此链接](https://kkksuper.github.io/FleSpeech/)获得.

## 1·Introduction: 引言

Speech synthesis plays a pivotal role in content creation and human-computer interaction.
With the advancement of powerful generative models, such as large language models~\cite{Valle, TorToiSe, BASETTS, SEEDTTS, ClaM-TTS} and diffusion models~\cite{audiobox, E2TTS, F5TTS}, speech synthesis has experienced rapid progress in recent years~\cite{ControllableSurvey}.
Beyond a focus on realism, there is a growing emphasis on \textit{flexible and controllable} speech synthesis~\cite{MM-TTS}, such as the ability to manipulate the style of generated speech based on textual descriptions~\cite{PromptStyle, SALLE, PromptTTS2, UniStyle}.

Despite the variety of available speech generation control methods, each approach has its inherent limitations.
For instance, while speech synthesis based on natural language descriptions offers flexibility, language often struggles to precisely capture all desired attributes, particularly when it comes to describing a speaker's timbre, as textual representations are inherently limited.
In contrast, the reference audio-based method can clearly define all attributes but relies on existing audio, which lacks creativity and flexibility.
These constraints make it difficult to address specific user needs in certain scenarios, such as adjusting style while preserving a selected speaker timbre or choosing a style and generating a voice that aligns with a character's visual appearance.

To overcome these constraints and move beyond controllable speech synthesis techniques based on a single or a few control methods, we propose a more flexible controllable speech generation method, \textit{FleSpeech}, which supports multiple forms of control and allows for the combination of different control strategies, thereby meeting the flexible control requirements across various scenarios as illustrated in Fig.~\ref{fig:intro}.
To this end, we first introduce a multi-stage speech generation framework, with each stage modeling the style and timbre of speech.
With this framework, we can provide different prompts at different stages, enabling flexibly controllable speech generation.
Second, we propose a multimodal prompt encoder to embed multimodal prompts into a unified representation.
Finally, considering the scarcity of multimodal data, we built a data collection pipeline to facilitate research in this area.
We will release this data collection pipeline upon the acceptance of this paper.

In summary, the main contributions of this work are as follows:

- We propose FleSpeech, a multi-stage speech generation framework that supports multiple prompt inputs to flexibly control different properties of speech.
Experiments across different tasks demonstrate both the objective and subjective superiority of this method.
- We propose a unified multimodal prompt encoder, which allows us to input any combination of text, audio, and visual modal prompts and operate them in a unified embedding space.
- We built a pipeline to facilitate data collection for subsequent multimodal speech generation work.

## 2·Related Works: 相关工作

### Controllable Speech Synthesis: 可控语音合成

The employment of category labels, such as speaker identity~\cite{DBLP:conf/interspeech/ChenTRXSZQ20, DBLP:conf/nips/GibianskyADMPPR17} and emotion~\cite{DBLP:journals/corr/abs-1711-05447, DBLP:journals/speech/Lorenzo-TruebaH18}, serves as a prevalent technique for controlling specific speech attributes.
To address the limited control capabilities of labels, Skerry-Ryan et al.~\cite{skerry2018towards} introduced a style transfer method based on reference acoustic representation.
Subsequently, this reference audio-based approach has gained substantial popularity, particularly in the context of emotion transfer~\cite{li2022cross, lei2022msemotts} and zero-shot TTS~\cite{Valle, ClaM-TTS, du2024cosyvoice}.

To achieve more flexible control, InstructTTS~\cite{yang2024instructtts} and PromptTTS~\cite{guo2023prompttts} are pioneering text description-based speech synthesis, employing natural language to specify the attributes to be controlled.
Subsequent efforts~\cite{lyth2024natural, yamauchi2024stylecap, ji2024textrolspeech, PromptTTS2, jin2024speechcraft}  are focused on exploring the use of automated methods to capture more diverse natural language descriptions,  thereby enabling control over an expanded range of attributes.

Additionally, a speaker's facial image can also serve as a form of control information for speech synthesis~\cite{Face2Speech, lee2023imaginary, wang2022anyonenet, FVTTS}.
Specifically, AnyoneNet~\cite{wang2022anyonenet} employs face embeddings, projecting them into the same embedding space as reference audio embeddings.
This approach aims to generate voices that align with the character's visual appearance, thus facilitating the production of speaker videos that incorporate speech, derived from a single facial image and accompanying text.

Most recently, research has begun to explore control methods beyond single-modality-based methods.
MM-TTS~\cite{MM-TTS} pioneers a unified framework that accommodates multimodal prompts from text, audio, or facial modalities.
Further advancing this field, StyleFusion TTS~\cite{StyleFusion} introduces a multi-prompt framework that leverages both style descriptions and an audio prompt to simultaneously control audio style and timbre.
Unlike StyleFusion TTS, which necessitates simultaneous input of both prompts during inference, our proposed FleSpeech accommodates inputs from any number of arbitrary modalities.
This flexibility significantly enhances the adaptability and controllability of speech synthesis.

### Speech Attribute Editing: 语音属性编辑

Editing speech attributes typically involves modifications to timbre or speaking styles.
The former, known as Voice Conversion (VC), specifically aims to transform the timbre to match that of another target speaker while retaining the linguistic information.
A typical method employs pre-trained models to extract speaker timbre representations and speech content features, which are then merged to reconstruct the converted speech~\cite{AutoVC, VQMIVC, Expressive-VC}.
However, this approach often struggles to generalize to unseen speakers due to model capacity constraints when handling large-scale speech data.
To address this challenge, language model-based voice conversion methods have begun to emerge~\cite{wang2024streamvoice, wang2024streamvoice+}.

Instead of changing timbre, style editing focuses on modifying the speech style while preserving linguistic content and timbre.
VoxEditor~\cite{VoxEditor} introduces a voice attribute editing model that facilitates the modification of speech style attributes using a given source audio and textual description.
Similarly, AudioBox~\cite{audiobox} presents a flow-matching-based framework that enables the restyling of any audio sample through text descriptions.
Extending beyond just editing timbre or style, our proposed FleSpeech allows for the simultaneous editing of both speaker timbre and style.

## 3·Methodology: 方法

### Overview: 概览

FleSpeech is designed to flexibly control the synthesis of speech either through any single-form prompt or a combination of different prompt formats.
For instance, it can control style using a text description while managing timbre with reference audio.
To facilitate this, as illustrated in Fig.~\ref{fig:modela}, FleSpeech comprises a language model module for semantic token prediction and a Flow Matching-based module for acoustic feature prediction.
To handle different forms of prompts, a multimodal prompt encoder (MPE) is proposed.
Specifically, MPE is designed to handle prompts in any format, i.e., text, audio, or image, to obtain a unified representation.
This unified representation serves as a condition in either the language model or the flow matching module, facilitating targeted control.

Here, we first introduce the language model and flow matching, both of which play crucial roles in speech generation and are classified as components of the multimodal prompt-based speech generator.
Subsequently, we describe MPE, which is used to control the generator.

### Multimodal Prompt-based Speech Generator

**Language Model for Semantic Generation**

Inspired by the outstanding performance of language models in speech synthesis tasks~\cite{Valle}, we tokenize speech into semantic tokens and then employ a decoder-only transformer-based language model to predict these tokens.
Specifically, the input text is first converted into a phoneme sequence.
The language model then takes this phoneme embedding sequence, concatenated with the global condition embedding obtained via MPE, to predict semantic tokens in an autoregressive manner.
Details about the model parameters are provided in Appendix ~\ref{append:model_config}.

As for speech tokenization, inspired by Vec-Tok~\cite{VecTok}, our tokenizer employs WavLM~\cite{WavLM}, pre-trained on 94k hour dataset\footnote{https://huggingface.co/microsoft/wavlm-large}, to extract speech features.
We then use the K-means clustering method to discretize these features into 300 tokens, primarily associated with linguistic information.

**Flow Matching for Acoustic Feature Generation**

The absence of acoustic details in semantic tokens results in a gap with the corresponding audio.
To bridge this gap, a diffusion transformer based flow-matching-based module, similar to Stable Diffusion 3~\cite{stablediffusion3}, is used to generate acoustic features from semantic tokens, supplemented by the conditional embedding created by the MPE.
Details about this module can be found in Appendix ~\ref{append:model_config}.

Compared to pre-designed acoustic features such as the Mel-spectrum, Glow-WaveGAN~\cite{GlowWaveGAN} demonstrates that the acoustic latent representation learned by a variational autoencoder performs better in acoustic feature prediction and vocoder-based speech synthesis processes.
Therefore, instead of using the Mel-spectrum as the acoustic feature to be predicted by the flow matching module as in CosyVoice~\cite{du2024cosyvoice}, we adopt WaveGAN implemented in Glow-WaveGAN to extract the latent representation as the acoustic feature via the encoder.
The decoder is then used as a vocoder to generate the final audio.

### Multimodal Prompt Encoder

The objective of the MPE is to obtain a unified condition embedding based on prompts from multiple modalities.
Given that the reference audio contains the most comprehensive information and is always available during the speech generation training process, the core idea behind MPE is to map the representations of textual and visual prompts to the space of reference audio embeddings.
To achieve this, following the approach of IP-Adapter~\cite{IPAdapter}, a query-based encoder structure is employed, which uses some learnable query tokens to extract speech-related information from the representations of different prompts.
Additionally, due to the many-to-one relationship between reference audio and other prompt modalities, such as multiple voices that correspond to the textual style description "a male speaking loudly and very fast", a diffusion-based method is adopted to model this diversity.
Specifically, as shown in the Fig.~\ref{fig:modelb}, the embeddings from different prompt modalities are input into the query-based encoder separately.
These embeddings are then concatenated with the noisy audio embedding $x_t$ and fed into the diffusion process.
The diffusion model subsequently predicts the ground truth audio embedding $x_0$ through denoising.

**The reference audio prompt embedding**, serving as the anchor for prompt embeddings from different modalities, captures all time-invariant information, such as style and timbre.
Consequently, the embedding created by the reference audio encoder can be directly used as the conditional embedding in speech generation.
Similar to Meta-StyleSpeech~\cite{min2021meta}, the reference audio encoder consists of six attention blocks, and the output of the last block is average-pooled to obtain a global audio embedding.

**The textual prompt embedding** can be derived from either the description of the speaking style or facial visual information.
In this case, the description text is embedded using a pre-trained BERT~\cite{BERT}\footnote{https://huggingface.co/google-bert/bert-large-uncased}, which is to capture the semantic information of the descriptions.

**The visual prompt embedding**, specifically referring to the embedding of face information, is inspired by ID-Animator~\cite{IDAnimator} and aims to capture both static and dynamic information naturally present in face videos.
Static information encompasses the facial features of the speaker in a specific frame, such as gender, age, hair colour, and body type, and is closely related to the acoustic features of the speaker.
In contrast, dynamic information reflects the speaker's state and behaviour, such as laughing or chatting.
This dynamic information complements the static facial features and helps capture nuances that go beyond the capabilities of static images.

MPE is designed to accept inputs from any modality during both training and inference.
Embeddings from non-input modalities are masked prior to the diffusion process.
Furthermore, given that different speech attributes are modelled at various stages, the parameters of MPE corresponding to token prediction and acoustic feature generation are not shared.

### Training Strategy

To address the scarcity of multimodal data, we propose a three-stage training strategy.
We use two types of data: 50,000 hours of large-scale low-expressivity speech data from LibriHeavy and 616 hours of high-expressivity speech data collected from the open-source dataset.

In the first stage, the model is trained on a combination of two datasets to achieve basic speech synthesis capabilities with the large-scale corpus ensuring stability.
In the second stage, the model is fine-tuned on high-expressive data to achieve domain alignment.
In the third stage, we freeze the generation model backbone and start training the multimodal encoder to enable the model to support modal inputs other than speech prompts.
Notably, during this stage,  the multimodal prompt encoder is updated with the generation loss in addition to the diffusion loss.

## Multimodal Dataset: 多模态数据集

Due to the scarcity of multimodal controllable speech synthesis data, we propose a method for constructing such a database.
Compared to existing data, the collected data is not only larger in scale but also includes facial modality with richer facial annotation information.
Details about the collected data and comparisons with other multimodal speech synthesis datasets can be found in Appendix~\ref{append:dataset}.

**The collection of the talking head video dataset** is based on the CelebV-HQ~\cite{CelebVHQ}, GRID~\cite{GRID}, LRS2~\cite{LRS2}, and MEAD~\cite{MEAD} datasets, which primarily feature talking faces with one person speaking most of the time.
After web crawling, the videos are segmented according to the timestamps provided in the dataset.
To ensure speech quality, we first apply the S-DCCRN~\cite{lv2022s} model to denoise the crawled videos, retaining only those with a Signal-to-Noise Ratio (SNR) test score greater than 0.6 and a DNSMOS~\cite{reddy2022dnsmos} greater than 2.6.
Finally, we use Whisper~\cite{radford2023robust}~\footnote{https://huggingface.co/openai/whisper-large-v3} to get the speech transcription and filter out sentences with fewer than three words.
Additionally, the face descriptions are also created, and the details are introduced in section~\ref{sc:face prompt}.

**The collection of the speech dataset** is based on a large-scale, high-quality TTS dataset, TextrolSpeech~\cite{SALLE}, which concludes emotional content and attribute labels such as gender and emotion.
Based on this, we re-caption the speaking style according to the distribution of our entire dataset.
This re-caption method is detailed in section~\ref{sc: text prompt}

### Face Description

Following ID-Animator~\cite{IDAnimator}, we use both static and dynamic face descriptions.
First, we crop all face videos based on timestamp and face range coordinates, selecting a random frame as the face image prompt.
This image is processed ShareGPT4V~\cite{chen2025sharegpt4v}~\footnote{https://huggingface.co/Lin-Chen/ShareGPT4V-7B} to generate a static description focused on facial attributes (e.g., gender, age, fatness).
To capture the speaker timbre, influenced by facial expressions, we extract video clips and use Video-LLava~\cite{lin2023video} to generate dynamic descriptions focused on facial changes and movements during speech.
Finally, we combine both descriptions using a large language model (LLM)~\footnote{We use ChatGPT (gpt-3.5-turbo) as the LLM.} to ensure cohesive and high-quality outputs with relevant details and human-like expression.

### Speaking Style Description

To obtain text descriptions of speaking style, we extract gender and emotion labels from the TextrolSpeech and MEAD datasets.
For other talking head video datasets, we use a face gender classification model~\cite{serengil2021lightface}~\footnote{https://github.com/serengil/deepface} to extract gender labels.
Acoustic attributes, including pitch, speech rate, and Root Mean Square(RMS) of energy are extracted using the signal processing method.
Silent frames are filtered by checking for zero pitch values.
In addition, we calculate the mean and variance of pitch to measure the pitch and its fluctuation, and the average RMS to measure the volume.

After feature extraction, we analyze their distribution and apply Mean and One Standard Deviation Splitting to divide each attribute into three intervals: "low," "normal," and "high" intervals.
We then use a LLM to generate multiple synonymous words or phrases for each attribute category.
Using different prompts, we combine these into single sentences to create various speech style descriptions with the same method.
This stage enables the simultaneous generation of multiple speech style descriptions with similar meanings.
This method has been shown to provide rich and diverse contextual clues to enhance the effectiveness of zero-shot control.

## 4·Experiments: 实验

### Test Dataset

To comprehensively evaluate the performance and generalization of the proposed model, two groups of datasets are used for testing.
One test set is reserved from the collected multimodal data, which includes 20 voice prompts from TextrolSpeech and 20 facial prompts from the talking head video dataset.
The other test set is an out-of-domain dataset from the HDTF dataset~\cite{hdtf}, consisting of image and audio prompts that undergo the same data processing procedures as the training set.
Additionally, we selected 16 emotional audio and image prompts from the MEAD dataset to evaluate emotion accuracy.
The synthesized transcripts were derived from a random selection of 100 sentences from the multimodal dataset.

### Evaluation Metrics

**Objective metrics** includes Word Error Rate (WER), Speaker Similarity (SPK-Sim), UTMOS~\cite{saeki2022utmos}~\footnote{https://github.com/tarepan/SpeechMOS}, Emotion Accuracy, Gender Accuracy, and other speech attribute accuracy.
Details about these objective metrics can be found in Appendix~\ref{append: metrics-obj}.

**Subjective metrics** include the Mean Opinion Score (MOS) to evaluate speech naturalness (N-MOS) and similarity (Sim-MOS).
Higher N-MOS means better naturalness while higher Sim-MOS indicates better similarity with the specific target.
Details about the subjective metrics can be found in Appendix~\ref{append: metrics-subj}

## 5·Results: 结果

We evaluated FleSpeech using both single-type prompts and various combinations of prompt types.
Additionally, the extended capabilities of FleSpeech, including speech editing and voice conversion, were also assessed.
The introduction to the various comparison methods, including MM-TTS~\cite{MM-TTS}, Salle~\cite{SALLE}, NaturalSpeech2~\cite{NaturalSpeech2}, and PromptTTS2~\cite{PromptTTS2} can be found in the Appendix~\ref{append:baseline}.

### Single-Prompt Controllable TTS

To evaluate  FleSpeech's single-prompt control capabilities, we compared it with other models using text, face image, or audio as the prompt.
We also conducted an ablation study to show the effectiveness of FleSpeech's design.

#### Comparison with Other Methods

**Speech generation with text prompt** was conducted using a set of text prompts with various emotional and prosodic attributes.
As shown in the \textit{Text} section of Table~\ref{tab:singleprompt}, FleSpeech achieved excellent results in terms of different style attributes and emotional accuracy.
Subjective testing results indicate that the speech generated by FleSpeech closely follows the text prompts and exhibits high naturalness.

**Speech generation with audio prompt** is presented in the \textit{Audio} section of Table~\ref{tab:singleprompt}.
Compared to MM-TTS, FleSpeech demonstrates significantly better speaker similarity, primarily due to the model capacity of the large-scale speech synthesis system.
Furthermore, FleSpeech outperforms NaturalSpeech2 in terms of emotion accuracy, gender accuracy, and speaker similarity, highlighting that its multi-stage framework is more effective at capturing various attributes, such as style and tone from the audio prompts.
With the cascading structure of LM and flow matching, FleSpeech has significantly improved naturalness and audio quality.

**Speech generation with face prompt** presented in the \textit{Face} section of Table~\ref{tab:singleprompt} showcases that FleSpeech achieved optimal performance across most metrics except for speaker similarity.
This is primarily due to the absence of an explicit objective relationship between speaker timbre and facial features.
Instead, the matching is more subjective in nature.
Subjective results indicate that the speech generated by FleSpeech has a higher correlation with the facial images, suggesting its ability to capture key information from the face and synthesize matching speaker timbre.

#### Ablation Study

To evaluate the effectiveness of face captions, an ablation study was conducted, which can be found in the \textit{Face} section of Table~\ref{tab:singleprompt}.
We first removed the dynamic attributes of the face description (w/o Face dyn-cap), which resulted in a sharp decline in emotional similarity, indicating a reduced ability of the model to capture emotional information from the face.
Moreover, when we eliminated both the static and dynamic attributes of the face description (w/o Face cap), the model relied solely on Clip representations for speaker-timbre-related information.
The experimental results show a comprehensive decline in terms of all metrics, demonstrating the effectiveness of combining Clip and facial descriptions.
Finally, we replaced Clip with FaceNet (w/ FaceNet emb), a facial recognition model capable of extracting embeddings that represent unique attributes among different individuals for face-driven speech synthesis.
The experimental results indicated that FaceNet’s ability to capture facial information is insufficient for synthesizing speech corresponding to the face prompt.

We further visualized the speaker embedding similarity matrix between different generated sentences.
As shown in Fig.~\ref{fig:spkemb}, compared to the results with w/ FaceNet emb, Clip (i.e., w/o Face cap) exhibits higher speaker consistency, indicating the effectiveness of the Clip encoder in extracting implicit representations.
By gradually adding static or dynamic face captions, the colors outside the diagonal gradually deepen, indicating a stronger binding between facial images and speaker timbre.
FleSpeech demonstrates the highest speaker consistency, highlighting the effectiveness of combining Clip with dynamic and static captions.

#### Overall Analysis

In addition to individual tasks, we conducted an overall analysis of the different experimental results.
The comparative results in various sections of table~\ref{tab:singleprompt} indicate that the audio modality achieves the highest accuracy in terms of emotion and gender, followed by text.
This suggests that audio provides the most fine-grained information, and through the text prompt encoder, the model can effectively extract relevant speech attributes from textual descriptions.
Image prompts, on the other hand, are generally less discernible, leading to a decrease in accuracy.
Moreover, the WER and UTMOS of speech generated from text prompts show a significant decline, which may be attributed to the one-to-many problem, especially in the text modality, where a larger sample space results in poorer stability.
Finally, despite being trained on a small-scale dataset, we observed that MM-TTS using face prompt outperforms the audio prompt in terms of SPK-Simi and UTMOS.
This reflects the generalization advantage of the face prompt, considering the complex acoustic environments present in the audio prompt.

### Multi-Prompt Controllable TTS

To evaluate the unique flexible control capability of  FleSpeech, we assessed its performance using multiple prompts.
Specifically, we provide different prompts at various stages to control different speech aspects.
We examined four combinations of prompts.
To validate the effectiveness of each stage, we included emotional or neutral prompts in the first stage and only neutral prompts in the second stage.
As shown in Table~\ref{tab:multiprompt}, compared to using a single prompt for control, FleSpeech effectively controls style and emotional attributes while reproducing the timbre of the target speaker despite some performance loss.

Additionally, we removed the facial caption(w/o Face cap) in the combination of audio and prompts.
We observed a significant decrease in gender accuracy, which indicates that fine-grained information provided by the audio prompt affects speaker timbre modeling in the second stage.
The experimental results demonstrate that incorporating the face caption can alleviate the impact of audio prompts, leading to higher consistency with the face prompt.

Furthermore, by comparing the results of different tasks, we found that the WER and UTMOS are highest for the model using two audio prompts, while models using text as the first-stage prompt have the lowest values.
This further indicates a negative correlation between the diversity and stability of the speech attribute space.
Moreover, models using text as the first-stage prompt generally achieve higher SPK-Sim compared to those using audio modality.
This suggests that more fine-grained information in the first stage can influence the speaker timbre modeling in the second stage.

### Extensibility

In addition to speech synthesis, we conducted additional experiments on other tasks to evaluate the scalability of FleSpeech.

#### Speaking Style Editing

Speaking style editing refers to modifying speech attributes without altering the content or speaker timbre.
To edit the attribute of a given utterance based on the text description, the transcription of this utterance obtained via Whisper and the text description can be used as the input for the language model.
Then this utterance can work as the audio prompt for the second stage.
We compared our method with Audiobox~\cite{audiobox}, a unified audio generation model based on flow matching that can redesign the provided audio examples using natural language instructions.
As shown in Table~\ref{tab:editing}, FleSpeech achieves satisfactory results.
Regarding emotional expression, FleSpeech scores lower, primarily because Audiobox incorporates non-verbal sounds, such as laughter, which enhance emotional perception.

#### Voice Conversion

FleSpeech allows for the speaker timbre editing by facial caption when given a facial image and its corresponding caption.
For instance, it can explicitly specify attributes such as the speaker's age, race, and fatness, which have been previously proved to be associated with speaker timbre~\cite{ stathopoulos2011changes, souza2018body, yang2022speaker}.
We evaluate the effectiveness of these edits through accuracy testing and subjective preference assessments.
The MOS indicates the degree of match, with higher scores reflecting better alignment.
Preference indicates perceived accuracy, where participants choose which audio, before or after editing, better matches the edited facial caption.
The test results are shown in Table~\ref{tab:vc}, where FleSpeech achieves an editing accuracy exceeding 70\%, demonstrating its capability to effectively edit speaker-timbre-related attributes to match facial features.
The subjective scores further corroborate this conclusion.
Additionally, the accuracy for age is higher than for BMI, suggesting that age is more perceptible in facial images.

## 6·Conclusions: 结论

In this work, we propose a flexible and controllable speech generation framework called FleSpeech.
Specifically, we implement a two-stage speech generation framework composed of a language model and a flow matching module, allowing for flexible control by providing different prompts at various stages.
Additionally, we introduce a multimodal prompt encoder that can accept prompts from different modalities and embed them into a unified style space, enabling more adaptable prompting.
Comprehensive subjective and objective experiments demonstrate the effectiveness of FleSpeech.