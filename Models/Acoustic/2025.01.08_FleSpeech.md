# FleSpeech

<details>
<summary>基本信息</summary>

- 标题: "FleSpeech: Flexibly Controllable Speech Generation with Various Prompts"
- 作者:
  - 01 Hanzhao Li (NPU@ASLP, lihanzhao.mail@gmail.com)
  - 02 Yuke Li (NPU@ASLP, yukeli6479@gmail.com)
  - 03 Xinsheng Wang (HKUST, w.xinshawn@gmail.com)
  - 04 Jingbin Hu (NPU@ASLP, hujingbin553@gmail.com)
  - 05 Qicong Xie (Tencent AI Lab, jerryqcxie@tencent.com)
  - 06 Shan Yang (Tencent AI Lab, shaanyang@tencent.com)
  - 07 Lei Xie (NPU@ASLP, lxie@nwpu.edu.cn)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.04644)
  - [Publication]()
  - [Github]()
  - [Demo](https://kkksuper.github.io/FleSpeech/)
- 文件:
  - [ArXiv](_PDF/2501.04644v1__FleSpeech__Flexibly_Controllable_Speech_Generation_with_Various_Prompts.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Controllable speech generation methods typically rely on single or fixed prompts, hindering creativity and flexibility.
These limitations make it difficult to meet specific user needs in certain scenarios, such as adjusting the style while preserving a selected speaker's timbre, or choosing a style and generating a voice that matches a character's visual appearance.
To overcome these challenges, we propose ***FleSpeech***, a novel multi-stage speech generation framework that allows for more flexible manipulation of speech attributes by integrating various forms of control.
***FleSpeech*** employs a multimodal prompt encoder that processes and unifies different text, audio, and visual prompts into a cohesive representation.
This approach enhances the adaptability of speech synthesis and supports creative and precise control over the generated speech.
Additionally, we develop a data collection pipeline for multimodal datasets to facilitate further research and applications in this field.
Comprehensive subjective and objective experiments demonstrate the effectiveness of ***FleSpeech***.
Audio samples are available at [this https URL](https://kkksuper.github.io/FleSpeech/).

</details>
<br>

可控语音生成方法通常依赖于单个或固定提示, 限制了创造性和灵活性.
这些限制使得难以满足某些场景下的特定用户需求, 例如调整风格同时保留选定说话人的音色, 或选择风格并生成与角色视觉外观相匹配的声音.

为了克服这些挑战, 我们提出了一种新式多阶段语音生成框架 ***FleSpeech***, 通过集成多种形式的控制来实现对语音属性的更灵活的控制.
***FleSpeech*** 采用多模态提示编码器, 将不同形式的文本音频和视觉提示统一为一个连贯的表示.
这种方法增强了语音合成的适应性, 并支持对生成语音的创造性和精确控制.

此外, 我们开发了多模态数据集的数据收集流程以促进该领域的进一步的研究和应用.
全面的主观和客观实验表明了 ***FleSpeech*** 的有效性.
音频示例可在[此链接](https://kkksuper.github.io/FleSpeech/)获得.

## 1·Introduction: 引言

Speech synthesis plays a pivotal role in content creation and human-computer interaction.
With the advancement of powerful generative models, such as large language models~\cite{Valle, TorToiSe, BASETTS, SEEDTTS, ClaM-TTS} and diffusion models~\cite{audiobox, E2TTS, F5TTS}, speech synthesis has experienced rapid progress in recent years~\cite{ControllableSurvey}.
Beyond a focus on realism, there is a growing emphasis on \textit{flexible and controllable} speech synthesis~\cite{MM-TTS}, such as the ability to manipulate the style of generated speech based on textual descriptions~\cite{PromptStyle, SALLE, PromptTTS2, UniStyle}.

Despite the variety of available speech generation control methods, each approach has its inherent limitations.
For instance, while speech synthesis based on natural language descriptions offers flexibility, language often struggles to precisely capture all desired attributes, particularly when it comes to describing a speaker's timbre, as textual representations are inherently limited.
In contrast, the reference audio-based method can clearly define all attributes but relies on existing audio, which lacks creativity and flexibility.
These constraints make it difficult to address specific user needs in certain scenarios, such as adjusting style while preserving a selected speaker timbre or choosing a style and generating a voice that aligns with a character's visual appearance.

To overcome these constraints and move beyond controllable speech synthesis techniques based on a single or a few control methods, we propose a more flexible controllable speech generation method, \textit{FleSpeech}, which supports multiple forms of control and allows for the combination of different control strategies, thereby meeting the flexible control requirements across various scenarios as illustrated in Fig.~\ref{fig:intro}.
To this end, we first introduce a multi-stage speech generation framework, with each stage modeling the style and timbre of speech.
With this framework, we can provide different prompts at different stages, enabling flexibly controllable speech generation.
Second, we propose a multimodal prompt encoder to embed multimodal prompts into a unified representation.
Finally, considering the scarcity of multimodal data, we built a data collection pipeline to facilitate research in this area.
We will release this data collection pipeline upon the acceptance of this paper.

In summary, the main contributions of this work are as follows:

- We propose FleSpeech, a multi-stage speech generation framework that supports multiple prompt inputs to flexibly control different properties of speech.
Experiments across different tasks demonstrate both the objective and subjective superiority of this method.
- We propose a unified multimodal prompt encoder, which allows us to input any combination of text, audio, and visual modal prompts and operate them in a unified embedding space.
- We built a pipeline to facilitate data collection for subsequent multimodal speech generation work.

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论