# FleSpeech

<details>
<summary>基本信息</summary>

- 标题: "FleSpeech: Flexibly Controllable Speech Generation with Various Prompts"
- 作者:
  - 01 Hanzhao Li (NPU@ASLP, lihanzhao.mail@gmail.com)
  - 02 Yuke Li (NPU@ASLP, yukeli6479@gmail.com)
  - 03 Xinsheng Wang (HKUST, w.xinshawn@gmail.com)
  - 04 Jingbin Hu (NPU@ASLP, hujingbin553@gmail.com)
  - 05 Qicong Xie (Tencent AI Lab, jerryqcxie@tencent.com)
  - 06 Shan Yang (Tencent AI Lab, shaanyang@tencent.com)
  - 07 Lei Xie (NPU@ASLP, lxie@nwpu.edu.cn)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.04644)
  - [Publication]()
  - [Github]()
  - [Demo](https://kkksuper.github.io/FleSpeech/)
- 文件:
  - [ArXiv](_PDF/2501.04644v1__FleSpeech__Flexibly_Controllable_Speech_Generation_with_Various_Prompts.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Controllable speech generation methods typically rely on single or fixed prompts, hindering creativity and flexibility.
These limitations make it difficult to meet specific user needs in certain scenarios, such as adjusting the style while preserving a selected speaker's timbre, or choosing a style and generating a voice that matches a character's visual appearance.
To overcome these challenges, we propose ***FleSpeech***, a novel multi-stage speech generation framework that allows for more flexible manipulation of speech attributes by integrating various forms of control.
***FleSpeech*** employs a multimodal prompt encoder that processes and unifies different text, audio, and visual prompts into a cohesive representation.
This approach enhances the adaptability of speech synthesis and supports creative and precise control over the generated speech.
Additionally, we develop a data collection pipeline for multimodal datasets to facilitate further research and applications in this field.
Comprehensive subjective and objective experiments demonstrate the effectiveness of ***FleSpeech***.
Audio samples are available at [this https URL](https://kkksuper.github.io/FleSpeech/).

</details>
<br>

可控语音生成方法通常依赖于单个或固定提示, 限制了创造性和灵活性.
这些限制使得难以满足某些场景下的特定用户需求, 例如调整风格同时保留选定说话人的音色, 或选择风格并生成与角色视觉外观相匹配的声音.

为了克服这些挑战, 我们提出了一种新式多阶段语音生成框架 ***FleSpeech***, 通过集成多种形式的控制来实现对语音属性的更灵活的控制.
***FleSpeech*** 采用多模态提示编码器, 将不同形式的文本音频和视觉提示统一为一个连贯的表示.
这种方法增强了语音合成的适应性, 并支持对生成语音的创造性和精确控制.

此外, 我们开发了多模态数据集的数据收集流程以促进该领域的进一步的研究和应用.
全面的主观和客观实验表明了 ***FleSpeech*** 的有效性.
音频示例可在[此链接](https://kkksuper.github.io/FleSpeech/)获得.

## 1·Introduction: 引言

Speech synthesis plays a pivotal role in content creation and human-computer interaction.
With the advancement of powerful generative models, such as large language models~\cite{Valle, TorToiSe, BASETTS, SEEDTTS, ClaM-TTS} and diffusion models~\cite{audiobox, E2TTS, F5TTS}, speech synthesis has experienced rapid progress in recent years~\cite{ControllableSurvey}.
Beyond a focus on realism, there is a growing emphasis on \textit{flexible and controllable} speech synthesis~\cite{MM-TTS}, such as the ability to manipulate the style of generated speech based on textual descriptions~\cite{PromptStyle, SALLE, PromptTTS2, UniStyle}.

Despite the variety of available speech generation control methods, each approach has its inherent limitations.
For instance, while speech synthesis based on natural language descriptions offers flexibility, language often struggles to precisely capture all desired attributes, particularly when it comes to describing a speaker's timbre, as textual representations are inherently limited.
In contrast, the reference audio-based method can clearly define all attributes but relies on existing audio, which lacks creativity and flexibility.
These constraints make it difficult to address specific user needs in certain scenarios, such as adjusting style while preserving a selected speaker timbre or choosing a style and generating a voice that aligns with a character's visual appearance.

To overcome these constraints and move beyond controllable speech synthesis techniques based on a single or a few control methods, we propose a more flexible controllable speech generation method, \textit{FleSpeech}, which supports multiple forms of control and allows for the combination of different control strategies, thereby meeting the flexible control requirements across various scenarios as illustrated in Fig.~\ref{fig:intro}.
To this end, we first introduce a multi-stage speech generation framework, with each stage modeling the style and timbre of speech.
With this framework, we can provide different prompts at different stages, enabling flexibly controllable speech generation.
Second, we propose a multimodal prompt encoder to embed multimodal prompts into a unified representation.
Finally, considering the scarcity of multimodal data, we built a data collection pipeline to facilitate research in this area.
We will release this data collection pipeline upon the acceptance of this paper.

In summary, the main contributions of this work are as follows:

- We propose FleSpeech, a multi-stage speech generation framework that supports multiple prompt inputs to flexibly control different properties of speech.
Experiments across different tasks demonstrate both the objective and subjective superiority of this method.
- We propose a unified multimodal prompt encoder, which allows us to input any combination of text, audio, and visual modal prompts and operate them in a unified embedding space.
- We built a pipeline to facilitate data collection for subsequent multimodal speech generation work.

## 2·Related Works: 相关工作

### Controllable Speech Synthesis: 可控语音合成

The employment of category labels, such as speaker identity~\cite{DBLP:conf/interspeech/ChenTRXSZQ20, DBLP:conf/nips/GibianskyADMPPR17} and emotion~\cite{DBLP:journals/corr/abs-1711-05447, DBLP:journals/speech/Lorenzo-TruebaH18}, serves as a prevalent technique for controlling specific speech attributes.
To address the limited control capabilities of labels, Skerry-Ryan et al.~\cite{skerry2018towards} introduced a style transfer method based on reference acoustic representation.
Subsequently, this reference audio-based approach has gained substantial popularity, particularly in the context of emotion transfer~\cite{li2022cross, lei2022msemotts} and zero-shot TTS~\cite{Valle, ClaM-TTS, du2024cosyvoice}.

To achieve more flexible control, InstructTTS~\cite{yang2024instructtts} and PromptTTS~\cite{guo2023prompttts} are pioneering text description-based speech synthesis, employing natural language to specify the attributes to be controlled.
Subsequent efforts~\cite{lyth2024natural, yamauchi2024stylecap, ji2024textrolspeech, PromptTTS2, jin2024speechcraft}  are focused on exploring the use of automated methods to capture more diverse natural language descriptions,  thereby enabling control over an expanded range of attributes.

Additionally, a speaker's facial image can also serve as a form of control information for speech synthesis~\cite{Face2Speech, lee2023imaginary, wang2022anyonenet, FVTTS}.
Specifically, AnyoneNet~\cite{wang2022anyonenet} employs face embeddings, projecting them into the same embedding space as reference audio embeddings.
This approach aims to generate voices that align with the character's visual appearance, thus facilitating the production of speaker videos that incorporate speech, derived from a single facial image and accompanying text.

Most recently, research has begun to explore control methods beyond single-modality-based methods.
MM-TTS~\cite{MM-TTS} pioneers a unified framework that accommodates multimodal prompts from text, audio, or facial modalities.
Further advancing this field, StyleFusion TTS~\cite{StyleFusion} introduces a multi-prompt framework that leverages both style descriptions and an audio prompt to simultaneously control audio style and timbre.
Unlike StyleFusion TTS, which necessitates simultaneous input of both prompts during inference, our proposed FleSpeech accommodates inputs from any number of arbitrary modalities.
This flexibility significantly enhances the adaptability and controllability of speech synthesis.

### Speech Attribute Editing: 语音属性编辑

Editing speech attributes typically involves modifications to timbre or speaking styles.
The former, known as Voice Conversion (VC), specifically aims to transform the timbre to match that of another target speaker while retaining the linguistic information.
A typical method employs pre-trained models to extract speaker timbre representations and speech content features, which are then merged to reconstruct the converted speech~\cite{AutoVC, VQMIVC, Expressive-VC}.
However, this approach often struggles to generalize to unseen speakers due to model capacity constraints when handling large-scale speech data.
To address this challenge, language model-based voice conversion methods have begun to emerge~\cite{wang2024streamvoice, wang2024streamvoice+}.

Instead of changing timbre, style editing focuses on modifying the speech style while preserving linguistic content and timbre.
VoxEditor~\cite{VoxEditor} introduces a voice attribute editing model that facilitates the modification of speech style attributes using a given source audio and textual description.
Similarly, AudioBox~\cite{audiobox} presents a flow-matching-based framework that enables the restyling of any audio sample through text descriptions.
Extending beyond just editing timbre or style, our proposed FleSpeech allows for the simultaneous editing of both speaker timbre and style.

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论