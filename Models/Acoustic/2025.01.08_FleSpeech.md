# FleSpeech

<details>
<summary>基本信息</summary>

- 标题: "FleSpeech: Flexibly Controllable Speech Generation with Various Prompts"
- 作者:
  - 01 Hanzhao Li (NPU@ASLP, lihanzhao.mail@gmail.com)
  - 02 Yuke Li (NPU@ASLP, yukeli6479@gmail.com)
  - 03 Xinsheng Wang (HKUST, w.xinshawn@gmail.com)
  - 04 Jingbin Hu (NPU@ASLP, hujingbin553@gmail.com)
  - 05 Qicong Xie (Tencent AI Lab, jerryqcxie@tencent.com)
  - 06 Shan Yang (Tencent AI Lab, shaanyang@tencent.com)
  - 07 Lei Xie (NPU@ASLP, lxie@nwpu.edu.cn)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.04644)
  - [Publication]()
  - [Github]()
  - [Demo](https://kkksuper.github.io/FleSpeech/)
- 文件:
  - [ArXiv](_PDF/2501.04644v1__FleSpeech__Flexibly_Controllable_Speech_Generation_with_Various_Prompts.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Controllable speech generation methods typically rely on single or fixed prompts, hindering creativity and flexibility.
These limitations make it difficult to meet specific user needs in certain scenarios, such as adjusting the style while preserving a selected speaker's timbre, or choosing a style and generating a voice that matches a character's visual appearance.
To overcome these challenges, we propose ***FleSpeech***, a novel multi-stage speech generation framework that allows for more flexible manipulation of speech attributes by integrating various forms of control.
***FleSpeech*** employs a multimodal prompt encoder that processes and unifies different text, audio, and visual prompts into a cohesive representation.
This approach enhances the adaptability of speech synthesis and supports creative and precise control over the generated speech.
Additionally, we develop a data collection pipeline for multimodal datasets to facilitate further research and applications in this field.
Comprehensive subjective and objective experiments demonstrate the effectiveness of ***FleSpeech***.
Audio samples are available at [this https URL](https://kkksuper.github.io/FleSpeech/).

</details>
<br>

可控语音生成方法通常依赖于单个或固定提示, 限制了创造性和灵活性.
这些限制使得难以满足某些场景下的特定用户需求, 例如调整风格同时保留选定说话人的音色, 或选择风格并生成与角色视觉外观相匹配的声音.

为了克服这些挑战, 我们提出了一种新式多阶段语音生成框架 ***FleSpeech***, 通过集成多种形式的控制来实现对语音属性的更灵活的控制.
***FleSpeech*** 采用多模态提示编码器, 将不同形式的文本音频和视觉提示统一为一个连贯的表示.
这种方法增强了语音合成的适应性, 并支持对生成语音的创造性和精确控制.

此外, 我们开发了多模态数据集的数据收集流程以促进该领域的进一步的研究和应用.
全面的主观和客观实验表明了 ***FleSpeech*** 的有效性.
音频示例可在[此链接](https://kkksuper.github.io/FleSpeech/)获得.

## 1·Introduction: 引言

Speech synthesis plays a pivotal role in content creation and human-computer interaction.
With the advancement of powerful generative models, such as large language models~\cite{Valle, TorToiSe, BASETTS, SEEDTTS, ClaM-TTS} and diffusion models~\cite{audiobox, E2TTS, F5TTS}, speech synthesis has experienced rapid progress in recent years~\cite{ControllableSurvey}.
Beyond a focus on realism, there is a growing emphasis on \textit{flexible and controllable} speech synthesis~\cite{MM-TTS}, such as the ability to manipulate the style of generated speech based on textual descriptions~\cite{PromptStyle, SALLE, PromptTTS2, UniStyle}.

Despite the variety of available speech generation control methods, each approach has its inherent limitations.
For instance, while speech synthesis based on natural language descriptions offers flexibility, language often struggles to precisely capture all desired attributes, particularly when it comes to describing a speaker's timbre, as textual representations are inherently limited.
In contrast, the reference audio-based method can clearly define all attributes but relies on existing audio, which lacks creativity and flexibility.
These constraints make it difficult to address specific user needs in certain scenarios, such as adjusting style while preserving a selected speaker timbre or choosing a style and generating a voice that aligns with a character's visual appearance.

To overcome these constraints and move beyond controllable speech synthesis techniques based on a single or a few control methods, we propose a more flexible controllable speech generation method, \textit{FleSpeech}, which supports multiple forms of control and allows for the combination of different control strategies, thereby meeting the flexible control requirements across various scenarios as illustrated in Fig.~\ref{fig:intro}.
To this end, we first introduce a multi-stage speech generation framework, with each stage modeling the style and timbre of speech.
With this framework, we can provide different prompts at different stages, enabling flexibly controllable speech generation.
Second, we propose a multimodal prompt encoder to embed multimodal prompts into a unified representation.
Finally, considering the scarcity of multimodal data, we built a data collection pipeline to facilitate research in this area.
We will release this data collection pipeline upon the acceptance of this paper.

In summary, the main contributions of this work are as follows:

- We propose FleSpeech, a multi-stage speech generation framework that supports multiple prompt inputs to flexibly control different properties of speech.
Experiments across different tasks demonstrate both the objective and subjective superiority of this method.
- We propose a unified multimodal prompt encoder, which allows us to input any combination of text, audio, and visual modal prompts and operate them in a unified embedding space.
- We built a pipeline to facilitate data collection for subsequent multimodal speech generation work.

## 2·Related Works: 相关工作

### Controllable Speech Synthesis: 可控语音合成

The employment of category labels, such as speaker identity~\cite{DBLP:conf/interspeech/ChenTRXSZQ20, DBLP:conf/nips/GibianskyADMPPR17} and emotion~\cite{DBLP:journals/corr/abs-1711-05447, DBLP:journals/speech/Lorenzo-TruebaH18}, serves as a prevalent technique for controlling specific speech attributes.
To address the limited control capabilities of labels, Skerry-Ryan et al.~\cite{skerry2018towards} introduced a style transfer method based on reference acoustic representation.
Subsequently, this reference audio-based approach has gained substantial popularity, particularly in the context of emotion transfer~\cite{li2022cross, lei2022msemotts} and zero-shot TTS~\cite{Valle, ClaM-TTS, du2024cosyvoice}.

To achieve more flexible control, InstructTTS~\cite{yang2024instructtts} and PromptTTS~\cite{guo2023prompttts} are pioneering text description-based speech synthesis, employing natural language to specify the attributes to be controlled.
Subsequent efforts~\cite{lyth2024natural, yamauchi2024stylecap, ji2024textrolspeech, PromptTTS2, jin2024speechcraft}  are focused on exploring the use of automated methods to capture more diverse natural language descriptions,  thereby enabling control over an expanded range of attributes.

Additionally, a speaker's facial image can also serve as a form of control information for speech synthesis~\cite{Face2Speech, lee2023imaginary, wang2022anyonenet, FVTTS}.
Specifically, AnyoneNet~\cite{wang2022anyonenet} employs face embeddings, projecting them into the same embedding space as reference audio embeddings.
This approach aims to generate voices that align with the character's visual appearance, thus facilitating the production of speaker videos that incorporate speech, derived from a single facial image and accompanying text.

Most recently, research has begun to explore control methods beyond single-modality-based methods.
MM-TTS~\cite{MM-TTS} pioneers a unified framework that accommodates multimodal prompts from text, audio, or facial modalities.
Further advancing this field, StyleFusion TTS~\cite{StyleFusion} introduces a multi-prompt framework that leverages both style descriptions and an audio prompt to simultaneously control audio style and timbre.
Unlike StyleFusion TTS, which necessitates simultaneous input of both prompts during inference, our proposed FleSpeech accommodates inputs from any number of arbitrary modalities.
This flexibility significantly enhances the adaptability and controllability of speech synthesis.

### Speech Attribute Editing: 语音属性编辑

Editing speech attributes typically involves modifications to timbre or speaking styles.
The former, known as Voice Conversion (VC), specifically aims to transform the timbre to match that of another target speaker while retaining the linguistic information.
A typical method employs pre-trained models to extract speaker timbre representations and speech content features, which are then merged to reconstruct the converted speech~\cite{AutoVC, VQMIVC, Expressive-VC}.
However, this approach often struggles to generalize to unseen speakers due to model capacity constraints when handling large-scale speech data.
To address this challenge, language model-based voice conversion methods have begun to emerge~\cite{wang2024streamvoice, wang2024streamvoice+}.

Instead of changing timbre, style editing focuses on modifying the speech style while preserving linguistic content and timbre.
VoxEditor~\cite{VoxEditor} introduces a voice attribute editing model that facilitates the modification of speech style attributes using a given source audio and textual description.
Similarly, AudioBox~\cite{audiobox} presents a flow-matching-based framework that enables the restyling of any audio sample through text descriptions.
Extending beyond just editing timbre or style, our proposed FleSpeech allows for the simultaneous editing of both speaker timbre and style.

## 3·Methodology: 方法

### Overview: 概览

FleSpeech is designed to flexibly control the synthesis of speech either through any single-form prompt or a combination of different prompt formats.
For instance, it can control style using a text description while managing timbre with reference audio.
To facilitate this, as illustrated in Fig.~\ref{fig:modela}, FleSpeech comprises a language model module for semantic token prediction and a Flow Matching-based module for acoustic feature prediction.
To handle different forms of prompts, a multimodal prompt encoder (MPE) is proposed.
Specifically, MPE is designed to handle prompts in any format, i.e., text, audio, or image, to obtain a unified representation.
This unified representation serves as a condition in either the language model or the flow matching module, facilitating targeted control.

Here, we first introduce the language model and flow matching, both of which play crucial roles in speech generation and are classified as components of the multimodal prompt-based speech generator.
Subsequently, we describe MPE, which is used to control the generator.

### Multimodal Prompt-based Speech Generator

**Language Model for Semantic Generation**

Inspired by the outstanding performance of language models in speech synthesis tasks~\cite{Valle}, we tokenize speech into semantic tokens and then employ a decoder-only transformer-based language model to predict these tokens.
Specifically, the input text is first converted into a phoneme sequence.
The language model then takes this phoneme embedding sequence, concatenated with the global condition embedding obtained via MPE, to predict semantic tokens in an autoregressive manner.
Details about the model parameters are provided in Appendix ~\ref{append:model_config}.

As for speech tokenization, inspired by Vec-Tok~\cite{VecTok}, our tokenizer employs WavLM~\cite{WavLM}, pre-trained on 94k hour dataset\footnote{https://huggingface.co/microsoft/wavlm-large}, to extract speech features.
We then use the K-means clustering method to discretize these features into 300 tokens, primarily associated with linguistic information.

**Flow Matching for Acoustic Feature Generation**

The absence of acoustic details in semantic tokens results in a gap with the corresponding audio.
To bridge this gap, a diffusion transformer based flow-matching-based module, similar to Stable Diffusion 3~\cite{stablediffusion3}, is used to generate acoustic features from semantic tokens, supplemented by the conditional embedding created by the MPE.
Details about this module can be found in Appendix ~\ref{append:model_config}.

Compared to pre-designed acoustic features such as the Mel-spectrum, Glow-WaveGAN~\cite{GlowWaveGAN} demonstrates that the acoustic latent representation learned by a variational autoencoder performs better in acoustic feature prediction and vocoder-based speech synthesis processes.
Therefore, instead of using the Mel-spectrum as the acoustic feature to be predicted by the flow matching module as in CosyVoice~\cite{du2024cosyvoice}, we adopt WaveGAN implemented in Glow-WaveGAN to extract the latent representation as the acoustic feature via the encoder.
The decoder is then used as a vocoder to generate the final audio.

### Multimodal Prompt Encoder

The objective of the MPE is to obtain a unified condition embedding based on prompts from multiple modalities.
Given that the reference audio contains the most comprehensive information and is always available during the speech generation training process, the core idea behind MPE is to map the representations of textual and visual prompts to the space of reference audio embeddings.
To achieve this, following the approach of IP-Adapter~\cite{IPAdapter}, a query-based encoder structure is employed, which uses some learnable query tokens to extract speech-related information from the representations of different prompts.
Additionally, due to the many-to-one relationship between reference audio and other prompt modalities, such as multiple voices that correspond to the textual style description "a male speaking loudly and very fast", a diffusion-based method is adopted to model this diversity.
Specifically, as shown in the Fig.~\ref{fig:modelb}, the embeddings from different prompt modalities are input into the query-based encoder separately.
These embeddings are then concatenated with the noisy audio embedding $x_t$ and fed into the diffusion process.
The diffusion model subsequently predicts the ground truth audio embedding $x_0$ through denoising.

**The reference audio prompt embedding**, serving as the anchor for prompt embeddings from different modalities, captures all time-invariant information, such as style and timbre.
Consequently, the embedding created by the reference audio encoder can be directly used as the conditional embedding in speech generation.
Similar to Meta-StyleSpeech~\cite{min2021meta}, the reference audio encoder consists of six attention blocks, and the output of the last block is average-pooled to obtain a global audio embedding.

**The textual prompt embedding** can be derived from either the description of the speaking style or facial visual information.
In this case, the description text is embedded using a pre-trained BERT~\cite{BERT}\footnote{https://huggingface.co/google-bert/bert-large-uncased}, which is to capture the semantic information of the descriptions.

**The visual prompt embedding**, specifically referring to the embedding of face information, is inspired by ID-Animator~\cite{IDAnimator} and aims to capture both static and dynamic information naturally present in face videos.
Static information encompasses the facial features of the speaker in a specific frame, such as gender, age, hair colour, and body type, and is closely related to the acoustic features of the speaker.
In contrast, dynamic information reflects the speaker's state and behaviour, such as laughing or chatting.
This dynamic information complements the static facial features and helps capture nuances that go beyond the capabilities of static images.

MPE is designed to accept inputs from any modality during both training and inference.
Embeddings from non-input modalities are masked prior to the diffusion process.
Furthermore, given that different speech attributes are modelled at various stages, the parameters of MPE corresponding to token prediction and acoustic feature generation are not shared.

### Training Strategy

To address the scarcity of multimodal data, we propose a three-stage training strategy.
We use two types of data: 50,000 hours of large-scale low-expressivity speech data from LibriHeavy and 616 hours of high-expressivity speech data collected from the open-source dataset.

In the first stage, the model is trained on a combination of two datasets to achieve basic speech synthesis capabilities with the large-scale corpus ensuring stability.
In the second stage, the model is fine-tuned on high-expressive data to achieve domain alignment.
In the third stage, we freeze the generation model backbone and start training the multimodal encoder to enable the model to support modal inputs other than speech prompts.
Notably, during this stage,  the multimodal prompt encoder is updated with the generation loss in addition to the diffusion loss.

## Multimodal Dataset: 多模态数据集

Due to the scarcity of multimodal controllable speech synthesis data, we propose a method for constructing such a database.
Compared to existing data, the collected data is not only larger in scale but also includes facial modality with richer facial annotation information.
Details about the collected data and comparisons with other multimodal speech synthesis datasets can be found in Appendix~\ref{append:dataset}.

**The collection of the talking head video dataset** is based on the CelebV-HQ~\cite{CelebVHQ}, GRID~\cite{GRID}, LRS2~\cite{LRS2}, and MEAD~\cite{MEAD} datasets, which primarily feature talking faces with one person speaking most of the time.
After web crawling, the videos are segmented according to the timestamps provided in the dataset.
To ensure speech quality, we first apply the S-DCCRN~\cite{lv2022s} model to denoise the crawled videos, retaining only those with a Signal-to-Noise Ratio (SNR) test score greater than 0.6 and a DNSMOS~\cite{reddy2022dnsmos} greater than 2.6.
Finally, we use Whisper~\cite{radford2023robust}~\footnote{https://huggingface.co/openai/whisper-large-v3} to get the speech transcription and filter out sentences with fewer than three words.
Additionally, the face descriptions are also created, and the details are introduced in section~\ref{sc:face prompt}.

**The collection of the speech dataset** is based on a large-scale, high-quality TTS dataset, TextrolSpeech~\cite{SALLE}, which concludes emotional content and attribute labels such as gender and emotion.
Based on this, we re-caption the speaking style according to the distribution of our entire dataset.
This re-caption method is detailed in section~\ref{sc: text prompt}

### Face Description

Following ID-Animator~\cite{IDAnimator}, we use both static and dynamic face descriptions.
First, we crop all face videos based on timestamp and face range coordinates, selecting a random frame as the face image prompt.
This image is processed ShareGPT4V~\cite{chen2025sharegpt4v}~\footnote{https://huggingface.co/Lin-Chen/ShareGPT4V-7B} to generate a static description focused on facial attributes (e.g., gender, age, fatness).
To capture the speaker timbre, influenced by facial expressions, we extract video clips and use Video-LLava~\cite{lin2023video} to generate dynamic descriptions focused on facial changes and movements during speech.
Finally, we combine both descriptions using a large language model (LLM)~\footnote{We use ChatGPT (gpt-3.5-turbo) as the LLM.} to ensure cohesive and high-quality outputs with relevant details and human-like expression.

### Speaking Style Description

To obtain text descriptions of speaking style, we extract gender and emotion labels from the TextrolSpeech and MEAD datasets.
For other talking head video datasets, we use a face gender classification model~\cite{serengil2021lightface}~\footnote{https://github.com/serengil/deepface} to extract gender labels.
Acoustic attributes, including pitch, speech rate, and Root Mean Square(RMS) of energy are extracted using the signal processing method.
Silent frames are filtered by checking for zero pitch values.
In addition, we calculate the mean and variance of pitch to measure the pitch and its fluctuation, and the average RMS to measure the volume.

After feature extraction, we analyze their distribution and apply Mean and One Standard Deviation Splitting to divide each attribute into three intervals: "low," "normal," and "high" intervals.
We then use a LLM to generate multiple synonymous words or phrases for each attribute category.
Using different prompts, we combine these into single sentences to create various speech style descriptions with the same method.
This stage enables the simultaneous generation of multiple speech style descriptions with similar meanings.
This method has been shown to provide rich and diverse contextual clues to enhance the effectiveness of zero-shot control.

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论