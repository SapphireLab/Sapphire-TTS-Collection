# FleSpeech

<details>
<summary>基本信息</summary>

- 标题: "FleSpeech: Flexibly Controllable Speech Generation with Various Prompts"
- 作者:
  - 01 Hanzhao Li (NPU@ASLP, lihanzhao.mail@gmail.com)
  - 02 Yuke Li (NPU@ASLP, yukeli6479@gmail.com)
  - 03 Xinsheng Wang (HKUST, w.xinshawn@gmail.com)
  - 04 Jingbin Hu (NPU@ASLP, hujingbin553@gmail.com)
  - 05 Qicong Xie (Tencent AI Lab, jerryqcxie@tencent.com)
  - 06 Shan Yang (Tencent AI Lab, shaanyang@tencent.com)
  - 07 Lei Xie (NPU@ASLP, lxie@nwpu.edu.cn)
- 链接:
  - [ArXiv](https://arxiv.org/abs/2501.04644)
  - [Publication]()
  - [Github]()
  - [Demo](https://kkksuper.github.io/FleSpeech/)
- 文件:
  - [ArXiv](_PDF/2501.04644v1__FleSpeech__Flexibly_Controllable_Speech_Generation_with_Various_Prompts.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<details>
<summary>展开原文</summary>

Controllable speech generation methods typically rely on single or fixed prompts, hindering creativity and flexibility.
These limitations make it difficult to meet specific user needs in certain scenarios, such as adjusting the style while preserving a selected speaker's timbre, or choosing a style and generating a voice that matches a character's visual appearance.
To overcome these challenges, we propose ***FleSpeech***, a novel multi-stage speech generation framework that allows for more flexible manipulation of speech attributes by integrating various forms of control.
***FleSpeech*** employs a multimodal prompt encoder that processes and unifies different text, audio, and visual prompts into a cohesive representation.
This approach enhances the adaptability of speech synthesis and supports creative and precise control over the generated speech.
Additionally, we develop a data collection pipeline for multimodal datasets to facilitate further research and applications in this field.
Comprehensive subjective and objective experiments demonstrate the effectiveness of ***FleSpeech***.
Audio samples are available at [this https URL](https://kkksuper.github.io/FleSpeech/).

</details>
<br>

可控语音生成方法通常依赖于单个或固定提示, 限制了创造性和灵活性.
这些限制使得难以满足某些场景下的特定用户需求, 例如调整风格同时保留选定说话人的音色, 或选择风格并生成与角色视觉外观相匹配的声音.

为了克服这些挑战, 我们提出了一种新式多阶段语音生成框架 ***FleSpeech***, 通过集成多种形式的控制来实现对语音属性的更灵活的控制.
***FleSpeech*** 采用多模态提示编码器, 将不同形式的文本音频和视觉提示统一为一个连贯的表示.
这种方法增强了语音合成的适应性, 并支持对生成语音的创造性和精确控制.

此外, 我们开发了多模态数据集的数据收集流程以促进该领域的进一步的研究和应用.
全面的主观和客观实验表明了 ***FleSpeech*** 的有效性.
音频示例可在[此链接](https://kkksuper.github.io/FleSpeech/)获得.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论