# MelNet

<details>
<summary>基本信息</summary>

- 标题: "MelNet: A Generative Model for Audio in the Frequency Domain"
- 作者:
  - 01 Sean Vasquez,
  - 02 Mike Lewis
- 链接:
  - [ArXiv](https://arxiv.org/abs/1906.01083)
  - [Publication]
  - [Github]
  - [Demo](https://audio-samples.github.io)
- 文件:
  - [ArXiv](_PDF/1906.01083v1__MelNet__A_Generative_Model_for_Audio_in_the_Frequency_Domain.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

Capturing high-level structure in audio waveforms is challenging because a single second of audio spans tens of thousands of timesteps.
While long-range dependencies are difficult to model directly in the time domain, we show that they can be more tractably modelled in two-dimensional time-frequency representations such as spectrograms.
By leveraging this representational advantage, in conjunction with a highly expressive probabilistic model and a multiscale generation procedure, we design a model capable of generating high-fidelity audio samples which capture structure at timescales that time-domain models have yet to achieve.
We apply our model to a variety of audio generation tasks, including unconditional speech generation, music generation, and text-to-speech synthesis---showing improvements over previous approaches in both density estimates and human judgments.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论