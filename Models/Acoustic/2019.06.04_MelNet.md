# MelNet

<details>
<summary>基本信息</summary>

- 标题: "MelNet: A Generative Model for Audio in the Frequency Domain"
- 作者:
  - 01 Sean Vasquez,
  - 02 Mike Lewis
- 链接:
  - [ArXiv](https://arxiv.org/abs/1906.01083)
  - [Publication]
  - [Github]
  - [Demo](https://audio-samples.github.io)
- 文件:
  - [ArXiv](_PDF/1906.01083v1__MelNet__A_Generative_Model_for_Audio_in_the_Frequency_Domain.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

<table><tr><td width="50%">

Capturing high-level structure in audio waveforms is challenging because a single second of audio spans tens of thousands of timesteps.
While long-range dependencies are difficult to model directly in the time domain, we show that they can be more tractably modelled in two-dimensional time-frequency representations such as spectrograms.
By leveraging this representational advantage, in conjunction with a highly expressive probabilistic model and a multiscale generation procedure, we design a model capable of generating high-fidelity audio samples which capture structure at timescales that time-domain models have yet to achieve.
We apply our model to a variety of audio generation tasks, including unconditional speech generation, music generation, and text-to-speech synthesis---showing improvements over previous approaches in both density estimates and human judgments.

</td><td>

捕获音频波形中的高级结构具有挑战性, 因为音频的一秒钟包含数千个时间步.
虽然在时间域中直接建模长距离依赖困难, 但我们证明可以在二维时间-频率表示 (如频谱图) 中更容易地建模.
通过利用这种表示优势, 结合高度表现力的概率模型和多尺度生成过程, 我们设计了一个模型, 能够生成高保真度的音频样本, 这些样本能够捕获时间域模型至今尚未实现的时间尺度的结构.
我们将我们的模型应用于多种音频生成任务, 包括无条件语音生成, 音乐生成, 以及文本到语音合成, 展示了与之前方法相比的改进, 包括密度估计和人工判断.

</td></tr></table>

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论