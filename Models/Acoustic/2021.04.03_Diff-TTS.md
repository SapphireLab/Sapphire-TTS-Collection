# Diff-TTS

<details>
<summary>基本信息</summary>

- 标题: "Diff-TTS: A Denoising Diffusion Model for Text-to-Speech"
- 作者:
  - 01 Myeonghun Jeong,
  - 02 Hyeongju Kim,
  - 03 Sung Jun Cheon,
  - 04 Byoung Jin Choi,
  - 05 Nam Soo Kim
- 链接:
  - [ArXiv](https://arxiv.org/abs/2104.01409)
  - [Publication](https://doi.org/10.21437/Interspeech.2021-469)
  - [Github]
  - [Demo](https://jmhxxi.github.io/Diff-TTS-demo/index.html)
- 文件:
  - [ArXiv](_PDF/2104.01409v1__Diff-TTS__A_Denoising_Diffusion_Model_for_Text-to-Speech.pdf)
  - [Publication](_PDF/2104.01409p0__Diff-TTS__InterSpeech2021.pdf)

</details>

## Abstract: 摘要

Although neural text-to-speech (TTS) models have attracted a lot of attention and succeeded in generating human-like speech, there is still room for improvements to its naturalness and architectural efficiency.
In this work, we propose a novel non-autoregressive TTS model, namely Diff-TTS, which achieves highly natural and efficient speech synthesis.
Given the text, Diff-TTS exploits a denoising diffusion framework to transform the noise signal into a mel-spectrogram via diffusion time steps.
In order to learn the mel-spectrogram distribution conditioned on the text, we present a likelihood-based optimization method for TTS.
Furthermore, to boost up the inference speed, we leverage the accelerated sampling method that allows Diff-TTS to generate raw waveforms much faster without significantly degrading perceptual quality.
Through experiments, we verified that Diff-TTS generates 28 times faster than the real-time with a single NVIDIA 2080Ti GPU.

## 1·Introduction: 引言

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论