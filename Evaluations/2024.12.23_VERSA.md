# VERSA

<details>
<summary>基本信息</summary>

- 标题: "VERSA: A Versatile Evaluation Toolkit for Speech, Audio, and Music"
- 作者:
  - 01 Jiatong Shi
  - 02 Hye-jin Shim
  - 03 Jinchuan Tian
  - 04 Siddhant Arora
  - 05 Haibin Wu
  - 06 Darius Petermann
  - 07 Jia Qi Yip
  - 08 You Zhang
  - 09 Yuxun Tang
  - 10 Wangyou Zhang
  - 11 Dareen Safar Alharthi
  - 12 Yichen Huang
  - 13 Koichi Saito
  - 14 Jionghao Han
  - 15 Yiwen Zhao
  - 16 Chris Donahue
  - 17 Shinji Watanabe
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.17667)
  - [Publication]()
  - [Github](https://github.com/shinjiwlab/versa)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2412.17667v1__VERSA__A_Versatile_Evaluation_Toolkit_for_Speech_Audio_and_Music.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

In this work, we introduce VERSA, a unified and standardized evaluation toolkit designed for various speech, audio, and music signals.
The toolkit features a Pythonic interface with flexible configuration and dependency control, making it user-friendly and efficient.
With full installation, VERSA offers 63 metrics with 711 metric variations based on different configurations.
These metrics encompass evaluations utilizing diverse external resources, including matching and non-matching reference audio, text transcriptions, and text captions.
As a lightweight yet comprehensive toolkit, VERSA is versatile to support the evaluation of a wide range of downstream scenarios.
To demonstrate its capabilities, this work highlights example use cases for VERSA, including audio coding, speech synthesis, speech enhancement, singing synthesis, and music generation.
The toolkit is available at this https URL.

## 1·Introduction: 引言

With the rapid advancements in artificial intelligence-generated content~(AIGC), deep generative models have demonstrated remarkable capabilities in producing high-quality outputs across various domains, including image, video, and sound generation.
As generative models become increasingly sophisticated, the need for comprehensive AIGC evaluation has grown, aimed at identifying the strengths and weaknesses of the generated outputs.

As an essential part of the language processing community, diverse generative models for speech, music, and general audio have shown significant potential in applications such as conversational interfaces~\cite{mctear2002spoken}, entertainment~\cite{fraser2018spoken, fancourt2019present, dash2024ai}, and task management~\cite{kulkarni2019conversational}.
Due to the perceptual nature of sound-based applications, human subjective assessment is widely regarded as the gold standard for evaluating sound generative models.

The most fundamental and widely used metric for these models is the mean opinion score~(MOS)~\cite{recommendation1994telephone}.
The initial purpose of MOS was to measure the naturalness of generated audio, but it has since evolved into various specific forms, such as evaluating speaker similarity~\cite{toda2016voice}, comparative performance against baseline systems~\cite{harada19995}, emotional similarity~\cite{choi2019multi}, and alignment with prompts~\cite{guo2023prompttts, li24y_interspeech}.
Despite its importance, subjective evaluations relying on human input are challenging to conduct due to their labor-intensive nature.
Furthermore, achieving statistically significant results often requires a substantial number of samples~\cite{rosenberg2017bias}.
Additionally, range-equalizing bias is frequently observed in MOS evaluations due to the psychological grounding of human subjective assessments~\cite{cooper23_interspeech, le2024limits}.
Such biases introduce considerable challenges in achieving comparable results across different evaluation datasets and participants, thereby complicating the process of benchmarking generative models effectively.
Last but not least, certain evaluation variants may require individuals with expert knowledge to conduct the assessment, particularly in the music domain~\cite{ji2020comprehensive}.

An alternative approach is to develop automatic metrics that align with human preferences.
The design of such metrics can vary based on the use of external references and the specific application scenarios, which further complicates their selection.

The basic setup for evaluation involves using only the candidate audio being evaluated~\cite{liang1994output, falk2006single, lo19_interspeech, saeki22c_interspeech}, which has been actively discussed in recent VoiceMOS challenges~\cite{huang22f_interspeech, cooper2023voicemos, huang2024voicemos}.
In addition, a variety of external resources can be optionally utilized for evaluation, as illustrated in Fig.\ref{fig:external-resources}.
These resources may include matching reference audio~\cite{rix2001perceptual}, non-matching reference audio~\cite{NORESQA-Manocha2021,ragano2024nomad}, textual transcription~\cite{hayashi2020espnet}, textual captions~\cite{huang2023make}, or visual cues~\cite{hu2021neural}.\footnote{Note that we specifically exclude pre-trained models from our notion of ``external resources''.}

The focus of automatic metrics can vary significantly depending on the application scenario.
For instance, in voice conversion, discriminative speaker embeddings can be employed to measure speaker similarity between the generated speech and speech from the same speakers~\cite{das2020predictions}.
In cases involving domain-specific content, such as singing, pre-trained models in singing voices may better align with assessments of singing naturalness~\cite{tang2024singmos}.
For a fine-grained, sample-level generation, signal-to-noise ratio-related metrics are more suitable, particularly for tasks such as speech enhancement and separation~\cite{luo2019convtasnet}.
Due to the creative nature of music, distributional metrics are often better suited for evaluating music generation~\cite{kilgour19_interspeech}.
Given the diversity of audio signals, effectively evaluating them requires a broad understanding of general audio events and their contextual relevance~\cite{huang2023make}.

Considering the diversity of metrics and the evolution of generative models for speech, audio, and music, the need for standardized evaluation metrics has become increasingly evident.
Without a unified framework, the diversity in evaluation methodologies often leads to inconsistent results, making it challenging to benchmark models or assess advancements effectively.
Standardization ensures that all systems are evaluated under comparable conditions, fostering fairness, reproducibility, and meaningful insights across studies.
Moreover, centralizing metrics within a single toolkit not only reduces redundancy and inefficiencies but also encourages collaboration by providing researchers with a shared foundation for assessing performance.
This need for consistency and centralization underpins the development of ***VERSA***, a toolkit designed to address these challenges.

Extended from its prior version in \cite{shi2024espnet}, this work introduces a versatile evaluation toolkit for speech, audio, and music: ***VERSA***.
Built on a Pythonic interface, ***VERSA*** integrates 63 metrics and more than 711 variants, offering a wide array of automatic evaluation tools tailored for speech, audio, and music.
By providing diverse metrics, ***VERSA*** aims to serve as a one-stop solution for multi-domain, multi-level, and multi-focus sound evaluation across various downstream applications.
With examples showcasing the use of ***VERSA*** in \textit{ESPnet-Codec} and \textit{ESPnet-SpeechLM}, we anticipate that the ***VERSA*** toolkit will become a key component in advancing sound generation benchmarks, addressing challenges in speech generation, and supporting multi-modal generative frameworks.
The codebase is publicly available at \url{https://github.com/shinjiwlab/versa}.\footnote{A video demonstration is available at \url{https://youtu.be/t7UP1uFvaCM}.}

## 2·VERSA

This section details the design of ***VERSA***, focusing on its general framework, supported metrics, and potential benefits for the community.

### VERSA Framework

As illustrated in Fig.~\ref{fig:versa-dir}, the core library of ***VERSA*** is implemented as a Python package with two straightforward interfaces: \texttt{scorer.py} and \texttt{aggregate\_result.py}.
The \texttt{scorer.py} interface computes the automatic metrics, while \texttt{aggregate\_result.py} consolidates the results into a final report for users.

Once installed via \texttt{pip}, using ***VERSA*** is as simple as the following:

```
python versa/bin/scorer.py \
    --score_config egs/general.yaml \
    --gt <ground truth audio list> \
    --pred <candidate audio list> \
    --output_file test_result
```

where the ground truth audio list is optional and is not required for independent metrics.

**I/O Interface**

***VERSA*** offers three I/O interfaces for handling audio samples: the \texttt{Soundfile} interface, the \texttt{Directory} interface, and the \texttt{Kaldi} interface.
These interfaces support a variety of audio formats (e.g., PCM, FLAC, MP3, Kaldi-ARK) and different file organizations, such as \texttt{wav.scp} files or individual audio files stored within a parent directory.
For each (candidate, reference) audio signal pair, a resampling is conducted for each metric according to the specific sampling rate required by that metric.
\texttt{librosa} is used for resampling.

**Flexible Configuration**

***VERSA*** employs a unified \texttt{YAML}-style configuration file to define which metrics to use and control their detailed setups.
While users can directly explore the library code, we also provide example configuration files for different metrics in the \texttt{egs} directory, as shown in Fig.~\ref{fig:versa-dir}.
For instance, \texttt{egs/general.yaml} provides a configuration template for the default installation metrics.
Additionally, individual \texttt{YAML} configuration files for specific metrics are available under \texttt{egs/separate\_metric}.
Some example configurations are discussed in Appendix~\ref{sec:config}.

**Strict Dependency Control**

Managing dependencies can be challenging when using diverse evaluation metrics.
To address this, ***VERSA*** offers a minimal-dependency installation that supports a core set of metrics by default, while additional installation scripts are provided for metrics with extra requirements.
This approach significantly reduces dependency overhead during ***VERSA*** installation, especially for metrics with heavy dependencies or complex compilation needs.
As shown in Fig.~\ref{fig:versa-dir}, these additional installation scripts are located in the \texttt{tools} directory.

To ensure correct model usage, many official packages released by model providers enforce strict dependency control by specifying exact package versions (e.g., specific versions of PyTorch or NumPy).
While this ensures compatibility, it often introduces unnecessary dependency conflicts with major packages.
To provide a more flexible environment for users, ***VERSA*** bypasses these strict version requirements by adapting the interfaces of such metrics into our own local forks.
These forks are supplemented with additional numerical tests to ensure functionality without adhering to rigid version control.

Moreover, using our own fork allows us to integrate ***VERSA***-specific interfaces into external metrics that may otherwise conflict with the toolkit’s design philosophy.
This flexibility ensures a consistent and seamless interface across the ***VERSA*** library, enhancing usability and maintaining the toolkit's design integrity.

In Appendix~\ref{sec: add-system-design}, we further discuss additional system design concepts on job scheduling, test protocol, resource management, and community-driven contribution guidelines.

### Supported Metrics

***VERSA*** stands out for its extensive range of supported metrics, categorized into four main types:

**Independent metrics**: These metrics do not require any dependent external resources, other than pre-trained models.
Notably, we adopt an extended form of independent metrics in speech and audio assessment, which also considers profiling-related metrics, such as voice activity detection and speaker turn-taking.
***VERSA*** currently supports 22  independent metrics.

**Dependent metrics**: These metrics rely on matching sound references.
In ***VERSA***, 25 dependent metrics are supported.

**Non-matching metrics**: These metrics use non-matching reference data or different modalities.
***VERSA*** currently supports 11 non-matching metrics.

**Distributional metrics**: These metrics conduct distribution comparisons between two datasets, providing a more general view of the generative models' performance.
***VERSA*** currently supports 5 distributional metrics.

As summarized in Table~\ref{tab:versa-metrics}, ***VERSA*** supports a total of 63 metrics, 39 of which are included in the minimal installation.
Of these, 54 metrics are applicable to speech tasks, 22 to general audio tasks, and 22 to music tasks.
Additionally, several metrics feature variations based on different pre-trained models, such as word error rate, speaker similarity, and Fréchet Audio Distance (FAD) scores.
By simply modifying the configuration file,\footnote{An example of a configuration change is provided in Appendix~\ref{ssec:config-change}.} ***VERSA*** can generate up to 711 distinct metric variants, offering flexibility for a wide range of evaluation scenarios.

### Advantages of ***VERSA***

This section highlights the key benefits of ***VERSA***, focusing on its ability to ensure consistency, facilitate comparability, provide comprehensive insights, and enhance efficiency.

**Consistency**: ***VERSA*** ensures uniform evaluation criteria across experiments, addressing a critical need in the field of sound generative models.
By standardizing the implementation of metrics, ***VERSA*** minimizes the variability introduced by subjective judgments or inconsistent evaluation setups (e.g., coding environments).

**Comparability**: One of ***VERSA***'s key advantages is its ability to facilitate benchmarking against existing models and methodologies.
By providing a unified set of metrics, it ensures that evaluations are conducted under fair and objective conditions.
This comparability is important for assessing the relative performance of new approaches, fostering innovation, and enabling the broader research community to progress effectively.

**Comprehensiveness**: ***VERSA*** supports a wide array of evaluation metrics, including dimensions such as perceptual quality, intelligibility, affective information, and creative diversity.
By incorporating these diverse measures, the toolkit provides a holistic view of system performance, especially for researchers to gain deeper insights into both the strengths and weaknesses of each method.

**Efficiency**: With its all-in-one design, ***VERSA*** significantly enhances efficiency by supporting multiple metrics within a single toolkit.
Users no longer need to rely on separate tools or perform manual calculations to assess different aspects of audio performance.
This workflow reduces the time, effort, and potential errors associated with using fragmented evaluation methods, accelerating the overall research and development process.

## 3·Comparison to Other Toolkits

As discussed in Sec.~\ref{sec:intro}, the challenges associated with subjective evaluation have propelled the community toward exploring objective evaluation metrics.
The growing demand for sound evaluation toolkits has led to numerous efforts in domain-specific evaluation for sound generation.

In the speech domain, prior to the deep learning era, the International Telecommunication Union Telecommunication Standardization Sector (ITU-T) played a pivotal role in designing evaluation metrics for speech processing tasks such as speech coding and speech enhancement.
More recently, text-to-speech (TTS) toolkits have begun incorporating speech quality assessment features, exemplified by \textit{ESPnet-TTS}~\cite{hayashi2020espnet, hayashi2021espnet2} and \textit{Amphion}~\cite{zhang2023amphion}.
Additionally, the \textit{Speech Human Evaluation Estimation Toolkit~(SHEET)} framework provides an all-in-one recipe-style toolkit for data preparation, speech quality prediction model training, and evaluation~\cite{huang2024mos}.
In speech enhancement, foundational signal-level metrics were first consolidated in~\cite{fevotte2005bss_eval}, followed by extensions such as \textit{Pysepm}, which supports 10 metrics~\cite{loizou2013se}.
More recent advancements include the addition of 14 speech enhancement metrics in \textit{ClearerVoice}~\cite{clearvoice}.

In the audio domain, \textit{AudioLDM-Eval} focuses on evaluating audio language models with nine types of metrics~\cite{liu2023audioldm, audioldm2-2024taslp}.
Stability AI has introduced three audio metrics~\cite{stable-audio}, while Sony CSL has open-sourced four additional types of audio metrics~\cite{grachten2024measuring}.

In the music domain, \textit{MIR\_EVAL} is a pioneering toolkit that aggregates metrics for music information retrieval tasks~\cite{raffel2014mir_eval}.
More recently, Microsoft released \textit{FADTK}, which emphasizes a comprehensive FAD-embedding space for generative music evaluation~\cite{gui2024adapting}.

A summary of the related toolkits is presented in Table~\ref{tab:comparison}.
Each framework has made significant contributions to the community, many serving as foundational tools for sound generation research.
Building on these previous toolkits, ***VERSA*** distinguishes itself with a general design applicable across multiple domains and its comprehensive inclusion of 63 metrics with 711 variants—capabilities that have not been achieved before.

## 4·Demonstration

We demonstrate several use cases of ***VERSA*** in diverse scenarios, including speech coding, speech synthesis, speech enhancement, singing synthesis, and music generation.

**Speech coding** remains one of the most widely utilized applications within the speech-processing community.
In this demonstration, we leverage ***VERSA*** to evaluate nine publicly available codecs.
More details and corresponding results are discussed in Table~\ref{tab:codec} in Appendix~\ref{ssec:codec}.

**Text-to-speech** aims to convert text into speech signals.
In this demonstration, we use ***VERSA*** to compare nine open-source TTS models.
More details and corresponding are shown in Table~\ref{tab:tts}, Appendix~\ref{ssec:tts}.

**Speech enhancement**: we also demonstrate the ***VERSA*** usage in speech enhancement in three speech enhancement models.
More details and corresponding are shown in Table~\ref{tab:se}, Appendix~\ref{ssec:se}.

**Singing synthesis** is an intersection between speech and music generation, where both speech-oriented and music-oriented evaluation metrics are needed.
***VERSA*** offers a one-stop solution to this problem based on the collection of a variety of metrics in each domain.
In this work, we demonstrate the evaluation of a range of singing voice synthesis models.
More details and corresponding are shown in Table~\ref{tab:svs}, Appendix~\ref{ssec:svs}.

**Music generation**: the evaluation of music generation systems has received increasing attention due to the rapid progress in model development.
To consider both creativity and musical harmony, recent evaluation methods mostly utilize distribution metrics for the evaluation, exhibiting a large difference to other sound generation systems.
In this demonstration, ***VERSA*** aggregates a range of music generation evaluation metrics into a single toolkit.
More details and corresponding are shown in Table~\ref{tab:music}, Appendix~\ref{ssec:music}.

## 5·Conclusion

In this work, we introduced ***VERSA***, a comprehensive and versatile evaluation toolkit for assessing speech, audio, and music signals.
With its flexible Pythonic interface and extensive suite of metrics, ***VERSA*** empowers researchers and developers to conduct rigorous and reproducible evaluations across diverse generative tasks.

Through its integration of more than 63 metrics and 711 variants, ***VERSA*** provides unparalleled support for evaluating speech synthesis, audio coding, music generation, and more.
The toolkit not only simplifies the evaluation process but also addresses challenges such as bias in subjective evaluations and the need for domain-specific expertise.

## 6·Ethics Statement

***VERSA*** is designed to address the challenges of evaluating sound generative models in diverse linguistic, cultural, and acoustic contexts.
Generative sound models often risk perpetuating biases due to their reliance on datasets that may underrepresent certain languages, accents, and cultural expressions.
To mitigate these risks, ***VERSA*** incorporates evaluation metrics and methodologies that accommodate a wide range of audio characteristics, including tonal, phonetic, and rhythmic variations present across global languages and music traditions.

The toolkit encourages the use of regionally diverse datasets and provides flexibility to integrate culturally specific evaluation resources, such as non-standard phonetic systems or traditional musical structures.
By doing so, ***VERSA*** seeks to foster the development of sound generative models that respect and represent the full spectrum of human audio diversity.

Through these efforts, ***VERSA*** empowers researchers and developers to create more inclusive generative models, ensuring that advances in speech, audio, and music technologies benefit communities worldwide, regardless of linguistic or cultural background.

## 7·Broader Impact

### Current Limitations

While ***VERSA*** offers a comprehensive and versatile evaluation framework for speech, audio, and music generation, it is not without its limitations.
Below, we outline some areas where the toolkit could be further improved:

**Dependence on External Resources**: Many of ***VERSA***'s metrics require external resources, such as pre-trained models, reference datasets, or additional Python packages.
The quality and diversity of these resources can impact the accuracy and fairness of evaluations, particularly for underrepresented languages and cultures.
While ***VERSA*** provides flexible configurations to accommodate various scenarios, the availability of such resources remains a bottleneck in some cases.

**Bias in Metric Design**: Despite efforts to include diverse evaluation metrics, some metrics may still reflect biases inherent in the training data or methodologies used to develop them.
For example, evaluation frameworks optimized for widely spoken languages or Western music may not fully capture the nuances of less-studied languages, dialects, or musical traditions.
This bias can lead to less accurate evaluations for certain domains or cultural contexts.

**Subjectivity in Perceptual Metrics**: While ***VERSA*** incorporates automatic metrics designed to align with human subjective assessments, these metrics may not always perfectly reflect human preferences or perceptions.
Human evaluations remain the gold standard for certain aspects of sound quality, naturalness, and emotional expressiveness, which automated metrics cannot fully replicate.

**Evolving Standards in Generative Models**: The rapid advancement of generative audio technologies means that new evaluation needs and metrics may arise that ***VERSA*** does not yet support.
As a result, maintaining the toolkit's relevance and adaptability will require ongoing updates and contributions from the community.

### Future Adaptation

By accommodating a wide range of configurations, external resources, and application scenarios, we expect ***VERSA*** to bridge the gap between human subjective assessments and automatic evaluation metrics, ensuring robust and scalable benchmarking.
The adoption and usage of ***VERSA*** are not restricted by geographic boundaries.
As an open-source toolkit, it is accessible globally to researchers and practitioners, fostering international collaboration in advancing sound generation technologies.
***VERSA*** is designed with adaptability in mind, allowing users to integrate their local resources and datasets to perform culturally and regionally relevant evaluations.

Furthermore, ***VERSA*** facilitates cross-border innovation by providing a standardized framework for evaluating generative audio models.
This standardization reduces duplication of effort and promotes reproducibility, ensuring that advancements in sound generation and evaluation can transcend political and cultural borders.
Our commitment to accessibility and inclusivity reinforces our belief in the universal potential of AI to benefit humanity without being limited by geographic, linguistic, or cultural barriers.

Looking ahead, we envision ***VERSA*** as a key enabler for advancing the field of sound generation.
Its modular design ensures adaptability to future developments, while its open-source availability fosters collaboration and community-driven enhancements.
By setting a new standard for sound evaluation, ***VERSA*** could pave the way for more transparent and effective comparisons of generative models, ultimately accelerating progress in AI-driven audio and music technologies.
