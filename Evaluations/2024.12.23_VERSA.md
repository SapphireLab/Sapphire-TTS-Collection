# VERSA

<details>
<summary>基本信息</summary>

- 标题: "VERSA: A Versatile Evaluation Toolkit for Speech, Audio, and Music"
- 作者:
  - 01 Jiatong Shi
  - 02 Hye-jin Shim
  - 03 Jinchuan Tian
  - 04 Siddhant Arora
  - 05 Haibin Wu
  - 06 Darius Petermann
  - 07 Jia Qi Yip
  - 08 You Zhang
  - 09 Yuxun Tang
  - 10 Wangyou Zhang
  - 11 Dareen Safar Alharthi
  - 12 Yichen Huang
  - 13 Koichi Saito
  - 14 Jionghao Han
  - 15 Yiwen Zhao
  - 16 Chris Donahue
  - 17 Shinji Watanabe
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.17667)
  - [Publication]()
  - [Github](https://github.com/shinjiwlab/versa)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2412.17667v1__VERSA__A_Versatile_Evaluation_Toolkit_for_Speech_Audio_and_Music.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

In this work, we introduce VERSA, a unified and standardized evaluation toolkit designed for various speech, audio, and music signals.
The toolkit features a Pythonic interface with flexible configuration and dependency control, making it user-friendly and efficient.
With full installation, VERSA offers 63 metrics with 711 metric variations based on different configurations.
These metrics encompass evaluations utilizing diverse external resources, including matching and non-matching reference audio, text transcriptions, and text captions.
As a lightweight yet comprehensive toolkit, VERSA is versatile to support the evaluation of a wide range of downstream scenarios.
To demonstrate its capabilities, this work highlights example use cases for VERSA, including audio coding, speech synthesis, speech enhancement, singing synthesis, and music generation.
The toolkit is available at this https URL.

## 1·Introduction: 引言

With the rapid advancements in artificial intelligence-generated content~(AIGC), deep generative models have demonstrated remarkable capabilities in producing high-quality outputs across various domains, including image, video, and sound generation. As generative models become increasingly sophisticated, the need for comprehensive AIGC evaluation has grown, aimed at identifying the strengths and weaknesses of the generated outputs.

As an essential part of the language processing community, diverse generative models for speech, music, and general audio have shown significant potential in applications such as conversational interfaces~\cite{mctear2002spoken}, entertainment~\cite{fraser2018spoken, fancourt2019present, dash2024ai}, and task management~\cite{kulkarni2019conversational}. Due to the perceptual nature of sound-based applications, human subjective assessment is widely regarded as the gold standard for evaluating sound generative models.

The most fundamental and widely used metric for these models is the mean opinion score~(MOS)~\cite{recommendation1994telephone}. The initial purpose of MOS was to measure the naturalness of generated audio, but it has since evolved into various specific forms, such as evaluating speaker similarity~\cite{toda2016voice}, comparative performance against baseline systems~\cite{harada19995}, emotional similarity~\cite{choi2019multi}, and alignment with prompts~\cite{guo2023prompttts, li24y_interspeech}. Despite its importance, subjective evaluations relying on human input are challenging to conduct due to their labor-intensive nature. Furthermore, achieving statistically significant results often requires a substantial number of samples~\cite{rosenberg2017bias}. Additionally, range-equalizing bias is frequently observed in MOS evaluations due to the psychological grounding of human subjective assessments~\cite{cooper23_interspeech, le2024limits}. Such biases introduce considerable challenges in achieving comparable results across different evaluation datasets and participants, thereby complicating the process of benchmarking generative models effectively. Last but not least, certain evaluation variants may require individuals with expert knowledge to conduct the assessment, particularly in the music domain~\cite{ji2020comprehensive}.

An alternative approach is to develop automatic metrics that align with human preferences. The design of such metrics can vary based on the use of external references and the specific application scenarios, which further complicates their selection.

The basic setup for evaluation involves using only the candidate audio being evaluated~\cite{liang1994output, falk2006single, lo19_interspeech, saeki22c_interspeech}, which has been actively discussed in recent VoiceMOS challenges~\cite{huang22f_interspeech, cooper2023voicemos, huang2024voicemos}. In addition, a variety of external resources can be optionally utilized for evaluation, as illustrated in Fig.\ref{fig:external-resources}. These resources may include matching reference audio~\cite{rix2001perceptual}, non-matching reference audio~\cite{NORESQA-Manocha2021,ragano2024nomad}, textual transcription~\cite{hayashi2020espnet}, textual captions~\cite{huang2023make}, or visual cues~\cite{hu2021neural}.\footnote{Note that we specifically exclude pre-trained models from our notion of ``external resources''.}

The focus of automatic metrics can vary significantly depending on the application scenario. For instance, in voice conversion, discriminative speaker embeddings can be employed to measure speaker similarity between the generated speech and speech from the same speakers~\cite{das2020predictions}. In cases involving domain-specific content, such as singing, pre-trained models in singing voices may better align with assessments of singing naturalness~\cite{tang2024singmos}. For a fine-grained, sample-level generation, signal-to-noise ratio-related metrics are more suitable, particularly for tasks such as speech enhancement and separation~\cite{luo2019convtasnet}. Due to the creative nature of music, distributional metrics are often better suited for evaluating music generation~\cite{kilgour19_interspeech}. Given the diversity of audio signals, effectively evaluating them requires a broad understanding of general audio events and their contextual relevance~\cite{huang2023make}.

Considering the diversity of metrics and the evolution of generative models for speech, audio, and music, the need for standardized evaluation metrics has become increasingly evident. Without a unified framework, the diversity in evaluation methodologies often leads to inconsistent results, making it challenging to benchmark models or assess advancements effectively. Standardization ensures that all systems are evaluated under comparable conditions, fostering fairness, reproducibility, and meaningful insights across studies. Moreover, centralizing metrics within a single toolkit not only reduces redundancy and inefficiencies but also encourages collaboration by providing researchers with a shared foundation for assessing performance. This need for consistency and centralization underpins the development of \textit{VERSA}, a toolkit designed to address these challenges.

Extended from its prior version in \cite{shi2024espnet}, this work introduces a versatile evaluation toolkit for speech, audio, and music: \textit{VERSA}. Built on a Pythonic interface, \textit{VERSA} integrates 63 metrics and more than 711 variants, offering a wide array of automatic evaluation tools tailored for speech, audio, and music. By providing diverse metrics, \textit{VERSA} aims to serve as a one-stop solution for multi-domain, multi-level, and multi-focus sound evaluation across various downstream applications. With examples showcasing the use of \textit{VERSA} in \textit{ESPnet-Codec} and \textit{ESPnet-SpeechLM}, we anticipate that the \textit{VERSA} toolkit will become a key component in advancing sound generation benchmarks, addressing challenges in speech generation, and supporting multi-modal generative frameworks. The codebase is publicly available at \url{https://github.com/shinjiwlab/versa}.\footnote{A video demonstration is available at \url{https://youtu.be/t7UP1uFvaCM}.}

## 2·VERSA

This section details the design of \textit{VERSA}, focusing on its general framework, supported metrics, and potential benefits for the community.

### VERSA Framework

As illustrated in Fig.~\ref{fig:versa-dir}, the core library of \textit{VERSA} is implemented as a Python package with two straightforward interfaces: \texttt{scorer.py} and \texttt{aggregate\_result.py}. The \texttt{scorer.py} interface computes the automatic metrics, while \texttt{aggregate\_result.py} consolidates the results into a final report for users.

Once installed via \texttt{pip}, using \textit{VERSA} is as simple as the following:

```
python versa/bin/scorer.py \
    --score_config egs/general.yaml \
    --gt <ground truth audio list> \
    --pred <candidate audio list> \
    --output_file test_result
```

where the ground truth audio list is optional and is not required for independent metrics.

**I/O Interface**

\textit{VERSA} offers three I/O interfaces for handling audio samples: the \texttt{Soundfile} interface, the \texttt{Directory} interface, and the \texttt{Kaldi} interface. These interfaces support a variety of audio formats (e.g., PCM, FLAC, MP3, Kaldi-ARK) and different file organizations, such as \texttt{wav.scp} files or individual audio files stored within a parent directory. For each (candidate, reference) audio signal pair, a resampling is conducted for each metric according to the specific sampling rate required by that metric. \texttt{librosa} is used for resampling.

**Flexible Configuration**

\textit{VERSA} employs a unified \texttt{YAML}-style configuration file to define which metrics to use and control their detailed setups. While users can directly explore the library code, we also provide example configuration files for different metrics in the \texttt{egs} directory, as shown in Fig.~\ref{fig:versa-dir}. For instance, \texttt{egs/general.yaml} provides a configuration template for the default installation metrics. Additionally, individual \texttt{YAML} configuration files for specific metrics are available under \texttt{egs/separate\_metric}. Some example configurations are discussed in Appendix~\ref{sec:config}.

**Strict Dependency Control**

Managing dependencies can be challenging when using diverse evaluation metrics. To address this, \textit{VERSA} offers a minimal-dependency installation that supports a core set of metrics by default, while additional installation scripts are provided for metrics with extra requirements. This approach significantly reduces dependency overhead during \textit{VERSA} installation, especially for metrics with heavy dependencies or complex compilation needs. As shown in Fig.~\ref{fig:versa-dir}, these additional installation scripts are located in the \texttt{tools} directory.

To ensure correct model usage, many official packages released by model providers enforce strict dependency control by specifying exact package versions (e.g., specific versions of PyTorch or NumPy). While this ensures compatibility, it often introduces unnecessary dependency conflicts with major packages. To provide a more flexible environment for users, \textit{VERSA} bypasses these strict version requirements by adapting the interfaces of such metrics into our own local forks. These forks are supplemented with additional numerical tests to ensure functionality without adhering to rigid version control.

Moreover, using our own fork allows us to integrate \textit{VERSA}-specific interfaces into external metrics that may otherwise conflict with the toolkit’s design philosophy. This flexibility ensures a consistent and seamless interface across the \textit{VERSA} library, enhancing usability and maintaining the toolkit's design integrity.

In Appendix~\ref{sec: add-system-design}, we further discuss additional system design concepts on job scheduling, test protocol, resource management, and community-driven contribution guidelines.

### Supported Metrics

\textit{VERSA} stands out for its extensive range of supported metrics, categorized into four main types:

\noindent \textbf{Independent metrics}: These metrics do not require any dependent external resources, other than pre-trained models.
Notably, we adopt an extended form of independent metrics in speech and audio assessment, which also considers profiling-related metrics, such as voice activity detection and speaker turn-taking.
\textit{VERSA} currently supports 22  independent metrics.

\noindent \textbf{Dependent metrics}: These metrics rely on matching sound references. In \textit{VERSA}, 25 dependent metrics are supported.

\noindent\textbf{Non-matching metrics}: These metrics use non-matching reference data or different modalities. \textit{VERSA} currently supports 11 non-matching metrics.

\noindent \textbf{Distributional metrics}: These metrics conduct distribution comparisons between two datasets, providing a more general view of the generative models' performance. \textit{VERSA} currently supports 5 distributional metrics.

As summarized in Table~\ref{tab:versa-metrics}, \textit{VERSA} supports a total of 63 metrics, 39 of which are included in the minimal installation. Of these, 54 metrics are applicable to speech tasks, 22 to general audio tasks, and 22 to music tasks. Additionally, several metrics feature variations based on different pre-trained models, such as word error rate, speaker similarity, and Fréchet Audio Distance (FAD) scores. By simply modifying the configuration file,\footnote{An example of a configuration change is provided in Appendix~\ref{ssec:config-change}.} \textit{VERSA} can generate up to 711 distinct metric variants, offering flexibility for a wide range of evaluation scenarios.

### Advantages of \textit{VERSA}

This section highlights the key benefits of \textit{VERSA}, focusing on its ability to ensure consistency, facilitate comparability, provide comprehensive insights, and enhance efficiency.

\noindent \textbf{Consistency}: \textit{VERSA} ensures uniform evaluation criteria across experiments, addressing a critical need in the field of sound generative models. By standardizing the implementation of metrics, \textit{VERSA} minimizes the variability introduced by subjective judgments or inconsistent evaluation setups (e.g., coding environments).

\noindent \textbf{Comparability}: One of \textit{VERSA}'s key advantages is its ability to facilitate benchmarking against existing models and methodologies. By providing a unified set of metrics, it ensures that evaluations are conducted under fair and objective conditions. This comparability is important for assessing the relative performance of new approaches, fostering innovation, and enabling the broader research community to progress effectively.

\noindent \textbf{Comprehensiveness}: \textit{VERSA} supports a wide array of evaluation metrics, including dimensions such as perceptual quality, intelligibility, affective information, and creative diversity. By incorporating these diverse measures, the toolkit provides a holistic view of system performance, especially for researchers to gain deeper insights into both the strengths and weaknesses of each method.

\noindent \textbf{Efficiency}: With its all-in-one design, \textit{VERSA} significantly enhances efficiency by supporting multiple metrics within a single toolkit. Users no longer need to rely on separate tools or perform manual calculations to assess different aspects of audio performance. This workflow reduces the time, effort, and potential errors associated with using fragmented evaluation methods, accelerating the overall research and development process.
