# VERSA

<details>
<summary>基本信息</summary>

- 标题: "VERSA: A Versatile Evaluation Toolkit for Speech, Audio, and Music"
- 作者:
  - 01 Jiatong Shi
  - 02 Hye-jin Shim
  - 03 Jinchuan Tian
  - 04 Siddhant Arora
  - 05 Haibin Wu
  - 06 Darius Petermann
  - 07 Jia Qi Yip
  - 08 You Zhang
  - 09 Yuxun Tang
  - 10 Wangyou Zhang
  - 11 Dareen Safar Alharthi
  - 12 Yichen Huang
  - 13 Koichi Saito
  - 14 Jionghao Han
  - 15 Yiwen Zhao
  - 16 Chris Donahue
  - 17 Shinji Watanabe
- 链接:
  - [ArXiv](https://arxiv.org/abs/2412.17667)
  - [Publication]()
  - [Github](https://github.com/shinjiwlab/versa)
  - [Demo]()
- 文件:
  - [ArXiv](_PDF/2412.17667v1__VERSA__A_Versatile_Evaluation_Toolkit_for_Speech_Audio_and_Music.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

In this work, we introduce VERSA, a unified and standardized evaluation toolkit designed for various speech, audio, and music signals.
The toolkit features a Pythonic interface with flexible configuration and dependency control, making it user-friendly and efficient.
With full installation, VERSA offers 63 metrics with 711 metric variations based on different configurations.
These metrics encompass evaluations utilizing diverse external resources, including matching and non-matching reference audio, text transcriptions, and text captions.
As a lightweight yet comprehensive toolkit, VERSA is versatile to support the evaluation of a wide range of downstream scenarios.
To demonstrate its capabilities, this work highlights example use cases for VERSA, including audio coding, speech synthesis, speech enhancement, singing synthesis, and music generation.
The toolkit is available at this https URL.

## 1·Introduction: 引言

With the rapid advancements in artificial intelligence-generated content~(AIGC), deep generative models have demonstrated remarkable capabilities in producing high-quality outputs across various domains, including image, video, and sound generation. As generative models become increasingly sophisticated, the need for comprehensive AIGC evaluation has grown, aimed at identifying the strengths and weaknesses of the generated outputs.

As an essential part of the language processing community, diverse generative models for speech, music, and general audio have shown significant potential in applications such as conversational interfaces~\cite{mctear2002spoken}, entertainment~\cite{fraser2018spoken, fancourt2019present, dash2024ai}, and task management~\cite{kulkarni2019conversational}. Due to the perceptual nature of sound-based applications, human subjective assessment is widely regarded as the gold standard for evaluating sound generative models.

The most fundamental and widely used metric for these models is the mean opinion score~(MOS)~\cite{recommendation1994telephone}. The initial purpose of MOS was to measure the naturalness of generated audio, but it has since evolved into various specific forms, such as evaluating speaker similarity~\cite{toda2016voice}, comparative performance against baseline systems~\cite{harada19995}, emotional similarity~\cite{choi2019multi}, and alignment with prompts~\cite{guo2023prompttts, li24y_interspeech}. Despite its importance, subjective evaluations relying on human input are challenging to conduct due to their labor-intensive nature. Furthermore, achieving statistically significant results often requires a substantial number of samples~\cite{rosenberg2017bias}. Additionally, range-equalizing bias is frequently observed in MOS evaluations due to the psychological grounding of human subjective assessments~\cite{cooper23_interspeech, le2024limits}. Such biases introduce considerable challenges in achieving comparable results across different evaluation datasets and participants, thereby complicating the process of benchmarking generative models effectively. Last but not least, certain evaluation variants may require individuals with expert knowledge to conduct the assessment, particularly in the music domain~\cite{ji2020comprehensive}.

An alternative approach is to develop automatic metrics that align with human preferences. The design of such metrics can vary based on the use of external references and the specific application scenarios, which further complicates their selection.

The basic setup for evaluation involves using only the candidate audio being evaluated~\cite{liang1994output, falk2006single, lo19_interspeech, saeki22c_interspeech}, which has been actively discussed in recent VoiceMOS challenges~\cite{huang22f_interspeech, cooper2023voicemos, huang2024voicemos}. In addition, a variety of external resources can be optionally utilized for evaluation, as illustrated in Fig.\ref{fig:external-resources}. These resources may include matching reference audio~\cite{rix2001perceptual}, non-matching reference audio~\cite{NORESQA-Manocha2021,ragano2024nomad}, textual transcription~\cite{hayashi2020espnet}, textual captions~\cite{huang2023make}, or visual cues~\cite{hu2021neural}.\footnote{Note that we specifically exclude pre-trained models from our notion of ``external resources''.}

The focus of automatic metrics can vary significantly depending on the application scenario. For instance, in voice conversion, discriminative speaker embeddings can be employed to measure speaker similarity between the generated speech and speech from the same speakers~\cite{das2020predictions}. In cases involving domain-specific content, such as singing, pre-trained models in singing voices may better align with assessments of singing naturalness~\cite{tang2024singmos}. For a fine-grained, sample-level generation, signal-to-noise ratio-related metrics are more suitable, particularly for tasks such as speech enhancement and separation~\cite{luo2019convtasnet}. Due to the creative nature of music, distributional metrics are often better suited for evaluating music generation~\cite{kilgour19_interspeech}. Given the diversity of audio signals, effectively evaluating them requires a broad understanding of general audio events and their contextual relevance~\cite{huang2023make}.

Considering the diversity of metrics and the evolution of generative models for speech, audio, and music, the need for standardized evaluation metrics has become increasingly evident. Without a unified framework, the diversity in evaluation methodologies often leads to inconsistent results, making it challenging to benchmark models or assess advancements effectively. Standardization ensures that all systems are evaluated under comparable conditions, fostering fairness, reproducibility, and meaningful insights across studies. Moreover, centralizing metrics within a single toolkit not only reduces redundancy and inefficiencies but also encourages collaboration by providing researchers with a shared foundation for assessing performance. This need for consistency and centralization underpins the development of \textit{VERSA}, a toolkit designed to address these challenges.

Extended from its prior version in \cite{shi2024espnet}, this work introduces a versatile evaluation toolkit for speech, audio, and music: \textit{VERSA}. Built on a Pythonic interface, \textit{VERSA} integrates 63 metrics and more than 711 variants, offering a wide array of automatic evaluation tools tailored for speech, audio, and music. By providing diverse metrics, \textit{VERSA} aims to serve as a one-stop solution for multi-domain, multi-level, and multi-focus sound evaluation across various downstream applications. With examples showcasing the use of \textit{VERSA} in \textit{ESPnet-Codec} and \textit{ESPnet-SpeechLM}, we anticipate that the \textit{VERSA} toolkit will become a key component in advancing sound generation benchmarks, addressing challenges in speech generation, and supporting multi-modal generative frameworks. The codebase is publicly available at \url{https://github.com/shinjiwlab/versa}.\footnote{A video demonstration is available at \url{https://youtu.be/t7UP1uFvaCM}.}

## 2·Related Works: 相关工作

## 3·Methodology: 方法

## 4·Experiments: 实验

## 5·Results: 结果

## 6·Conclusions: 结论