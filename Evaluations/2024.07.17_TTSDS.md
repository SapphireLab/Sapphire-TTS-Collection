# TTSDS

<details>
<summary>基本信息</summary>

- 标题: TTSDS: Text-to-Speech Distribution Score
- 作者:
  - 01 [Christoph Minixhofer](../Authors/Christoph_Minixhofer.md)
  - 02 [Ondřej Klejch](../Authors/Ondrej_Klejch.md)
  - 03 [Peter Bell](../Authors/Peter_Bell.md)
- 机构:
  - [The University of Edinburgh](../Institutions/GBR-University_of_Edinburgh.md)
- 时间:
  - 预印时间: 2024.07.17 ArXiv v1
  - 更新笔记: 2024.07.20
- 发表:
  - 期刊/会议 
- 链接:
  - [ArXiv](https://arxiv.org/abs/2407.12707)
  - [DOI]()
  - [Github](https://github.com/ttsds/ttsds)
  - [Demo](https://ttsdsbenchmark.com)
  - [Scholar](https://scholar.google.com/scholar?cluster=)
- 标签:
  - ?
- 页数: 8
- 引用: ?
- 被引: ?
- 数据:
  - ? 
- 对比:
  - ?
- 复现:
  - ?

</details>

## Abstract: 摘要

<details>
<summary>原文</summary>

> Many recently published Text-to-Speech (TTS) systems produce audio close to real speech.
> However, TTS evaluation needs to be revisited to make sense of the results obtained with the new architectures, approaches and datasets.
> We propose evaluating the quality of synthetic speech as a combination of multiple factors such as prosody, speaker identity, and intelligibility.
> Our approach assesses how well synthetic speech mirrors real speech by obtaining correlates of each factor and measuring their distance from both real speech datasets and noise datasets.
> We benchmark 35 TTS systems developed between 2008 and 2024 and show that our score computed as an unweighted average of factors strongly correlates with the human evaluations from each time period.

</details>
<br>

许多近期发布的文本转语音系统能够生成接近真实语音的音频.
然而, 为了理解在新架构, 新方法, 新数据集上获得的结果, 需要重新审视文本转语音的评估.
我们提出将合成语音的质量以多个因素的组合进行评估, 这些因素包括韵律, 说话人身份, 可理解性等.
我们的方法通过获得每个因素的相关性并度量在真实语音数据集和噪音数据集上它们之间的距离, 来评估合成语音在多大程度上反映真实语音.
我们对 2008 ~ 2024 年开发的 35 个文本转语音系统进行了基准测试, 并表明我们对这些因素进行无加权平均后的分数与每个时期的人类评估结果高度相关.

## 1.Introduction: 引言

> There has been a recent surge in synthetic speech generation quality enabled by systems using language modeling architectures generating discrete units (**[EnCodec (2022)](../Models/Speech_Neural_Codec/2022.10.24_EnCodec.md)**) which are then used to reconstruct the waveform (**[Parler-TTS (2024)](../Models/Speech_LLM/2024.02.02_Parler-TTS.md)**).
> However, many of the most recent systems have been released to the community without accompanying papers and/or evaluation.
> This is understandable since the field is moving at a quick pace, and evaluation of synthetic speech is hard.
> The remaining systems are evaluated mostly with Mean Opinion Score (MOS) and its factors such as naturalness, speaker similarity, and sometimes intelligibility.
> Some systems also rely on MOS predicted with trained neural networks (**[VALL-E 2 (2024)](../Models/Speech_LLM/2024.06.08_VALL-E2.md)**; **[XTTS (2024)](../Models/Speech_LLM/2024.06.07_XTTS.md)**).
> Unfortunately, MOS is becoming less useful as real and synthetic speech get closer in quality, and cannot be compared across studies and over time \cite{le2022back}.
>
> Many factors, such as prosody, intelligibility, naturalness, and speaker similarity, contribute to perceived overall speech quality.
> Some works focus on the prosody factor by comparing pitch or duration of real to synthetic speech \cite{budzianowski2024pheme,li2024styletts,minixhofer2023eval}, while others include metrics on Word Error Rate (WER) to account for intelligibility or more general algorithmic measures such as Mel-Cepstral Distoration (MCD) (**[VALL-E 2 (2024)](../Models/Speech_LLM/2024.06.08_VALL-E2.md)**; budzianowski2024pheme).
> To the best of our knowledge, there is only one published effort which evaluated state-of-the-art TTS systems based on LLM architectures \cite{ttsarena}.
> It uses crowdsourced A/B preference tests to produce an Elo rating for each system.
> As these ratings are the first comparative study of the next generation of TTS systems, they are a great resource, however this type of evaluation comes with a set of challenges.
> Since the evaluation is centralized, it is up to the organizers to add newly released systems.
> Furthermore, human rating of the speech could change as time progresses, which has been shown to be the case for MOS scores \cite{le2022back}. 
> Finally, adding more complex domains such as long-form speech synthesis, or testing the contribution of individual factors, requires running the evaluation from scratch.
>
> Some solutions for objective evaluation of generative systems have emerged in other domains: In image generation, Fréchet Inception Distance (FID) \cite{heusel2017fid} has become the de-facto standard, but while there have been attempts to apply these methods to TTS \cite{gritsenko2020fdsd}, they have not taken hold, perhaps due to the large number of required samples~\cite{chen2023pfid}.
> In NLP, the advances of capabilities of Large Language Models (LLMs) have lead to a number of published benchmarks, most ranking the model on a variety of tasks, such as the GLUE \cite{wang2018glue}, SuperGLUE \cite{wang2019superglue} and CoQA \cite{reddy2019coqa} benchmark.
> For speech processing, the SUPERB benchmark also uses this task approach \cite{yang2021superb}.
>
> Unlike math problems, or automatic speech recognition (ASR), speech synthesis does not have one correct solution, but many possible ones, which makes evaluation difficult.
> However, we can use the concept of factors for evaluation.
> As we can test a LLM on mathematical ability and reading comprehension as parts of an overall measure of reasoning ability, we can test a TTS system on factors such as prosody or intelligibility, and we can combine them to reflect the overall performance of the system.
> This approach also gives us more information on preferring one system over another.
> For example, for character voices in a video game, a system with higher scores in prosody might be preferred, while for a language-learning app intelligibility would be the most important factor.
>
> In this paper, we devise a benchmark with the intelligibility, prosody, speakers, environment and general factors.
> We include intelligibility and prosody as they are important measures of synthetic speech quality \cite{campbell2007eval}.
> We include the speaker category to evaluate how closely TTS systems can model realistic speakers \cite{fan2015multi}.
> We also include environment as a factor due to the prevalence of artifacts present in speech synthesis \cite{wagner2019eval}, and the difficulty of some TTS system to generate speech with realistic recording conditions \cite{minixhofer2023eval}.
> The general factor is similar to previous measures of speech distributions as represented by latent representations such as Fréchet DeepSpeech Distance \cite{gritsenko2020fdsd}.
> Finally, we compute the overall TTSDS score as an average of all factors.
>
> We compute the score for each factor by comparing the distributions of both high-dimensional features (e.g., embeddings) and scalar features (e.g., pitch) extracted from the speech.
> This comparison allows us to measure the deviation of synthetic speech from real speech without assuming predefined distributions, thereby avoiding common pitfalls associated with objective speech evaluation measures.
> For instance, intelligibility is often quantified using Word Error Rate (WER) \cite{cooper2024ttseval}, where lower values are typically preferred.
> However, if the target domain naturally exhibits high WER (e.g., children's speech), a TTS system should also reflect this characteristic, producing higher WER accordingly.
> Therefore, we compare the utterance-level distribution of our features (such as WER) rather than their mean.
>
> We evaluate our benchmark by comparing to MOS scores and A/B test results obtained for 35 TTS systems from 2008, 2022 and 2024.
> An average of our factor scores show correlation coefficients ranging from 0.60 to 0.83 for each time period, while the performance of state-of-the-art MOS prediction are less consistent, ranging from 0.05 to 0.85.
> Additionally, we observe a shift in priorities of human evaluators over time, with environment being more important for earlier systems, and prosody being more important for later ones.
> We make our benchmark suite and leaderboard openly available at https://ttsdsbenchmark.com

## 2.Related Works: 相关工作

None

## 3.Methodology: 方法

<details>
<summary>原文</summary>

> The first step of developing any measure is to define the concept being measured.
> Since there is no objective way of directly measuring the naturalness or quality of synthetic speech, we define our measure as "the distance between the distribution of real and synthetic speech".
> The next step is to define relevant factors ("viswanathan2005measuring").
> For speech we define the following five major factors.
>
> A General factor which measures the distribution of the speech without any assumptions.
> We use self-supervised learning (SSL) representations of the speech.
>
> An Environment factor which measures noise or distortion present in the speech.
> We use correlates for signal-to-noise ratio (SNR) and reverberation.
>
> An Intelligibility factor which measures how easy the content of the speech is to recognize.
> We use WER obtained using the transcripts provided to the TTS systems and ASR systems.
>
> A Prosody factor which measures how realistic the prosody of the speech is.
> We use a pitch extractor, a SSL representation of the prosody and a proxy for duration and speaking rate.
> 
> A Speaker factor which measures how close the speakers are to real ones.
> We use representations obtained from speaker verification systems.
>
> Since none of these factors can be measured directly, we rely on several features which correlate to the factors.
> The extensive body of work in speech processing and representation learning provides these features, including representations from self-supervised models, statistical features, and algorithmic features.
> For each feature derived from synthetic data, we define its score as how close it is to the same feature derived from real speech.

</details>
<br>

建立度量标准的第一步是定义所要度量的概念.
因为没有直接度量合成语音的自然度或质量的客观方法, 我们定义我们的度量方式为 "真实语音和合成语音分布之间的距离".
那么下一步是定义相关因素 (参见 "Measuring Speech Quality for Text-To-Speech Systems: Development and Assessment of a Modified Mean Opinion Score (MOS) Scale").
对于语音我们定义如下五种主要因素:
1. 一般因素: 无任何假设下度量语音的分布. 我们使用语音的自监督学习表示.
2. 环境因素: 度量语音中的噪声或失真. 我们使用信噪比 (SNR) 和混响 (Reverberation) 相关.
3. 可理解性因素: 度量语音内容的易读性. 我们使用为 TTS 系统和 ASR 系统提供的转写文本计算词错误率.
4. 韵律因素: 测量韵律的真实性. 我们使用一个音高提取器, 韵律的自监督学习表示和时长和说话速度的代理.
5. 说话人因素: 度量说话人和真实说话人的接近程度. 我们使用说话人验证系统的表示.

由于上述因素都不能够直接度量得到, 我们依赖于与这些因素相关的多个特征.
语音处理和表示学习的大量工作提供了这些特征, 包括来自自监督模型的表示, 统计特征, 算法特征等.
对于从合成数据中得到的每个特征, 我们将其分数定义为该特征和从真实语音得到的相应特征之间的接近程度.

### 3.1.Features extracted from Speech

> Here we identify the specific features (i.e., data points derived from speech) that represent each factor.
> Table~\ref{tab:features} summarizes the models and algorithms that we use to extract these features.
> For each of the aforementioned factors, we use two to three features to achieve a robust benchmark despite the low number of 80-100 samples per system.
> For measuring the General factor score, we use frame-level self-supervised representations extracted from the middle layers of the Hubert base \cite{hsu2021hubert} and wav2vec 2.0 base ~\cite{baevski2020wav2vec2} models.
> For the Environment distribution, we use two one-dimensional correlates of noise present in the signal -- we use VoiceFixer \cite{liu2021voicefixer} to remove noise from the speech, and then measure PESQ between the enhanced sample and the original one; we also use WadaSNR \cite{kim2008wada} to estimate the SNR of each sample.
> For Intelligibility, we calculate WER using the reference transcripts and automatic transcripts generated using wav2vec 2.0~\cite{baevski2020wav2vec2} fine tuned on 960 hours of LibriSpeech \cite{panayotov2015librispeech} and Whisper (small)~\cite{radford2023whisper}. 
> Prosody is quantified using frame-level representations from a self-supervised prosody model \cite{mpm} and frame-level pitch features \cite{morise2016world}. 
> Additionally, we get a proxy for the segmental durations by using Hubert tokens (with 100 clusters) and extracting their lengths (i.e.
> how many of the same token occur in a row).
> Finally, for the Speaker factor, we use d-vectors \cite{wan2018dvec} and the more recent WeSpeaker \cite{wang2023wespeaker} representations.

### 3.2.Speech Distributions

> The distribution of a feature can be computed on any audio dataset, whether it consists of synthetic speech, real speech, or noise. %\ondrej{I think that we should put the text back to introduce notation.
> I would just use \( x_i \sim \hat{P}(X|D)\) instead of \( \hat{P}(M|D) = \{ \mathbf{m}_1, \mathbf{m}_2, \ldots, \mathbf{m}_n \} \).}
> We let \( D \) represent an audio dataset, and \( X \) be the feature derived from the dataset.
> We can sample observed values $x_i$ from the empirical distribution \( \hat{P}(X|D) \) as:

\[
 x_i \sim \hat{P}(X|D)
\]

> where \( \mathbf{x}_i \) can be a scalar for one-dimensional features or a vector for multi-dimensional features.

### 3.3.Computing Distances Between Distributions

> To compare the distributions of features derived from different datasets, we utilize the Wasserstein distance, specifically the 2-Wasserstein distance $W_2$, also known as the Earth Mover's Distance. $W_2$ measures the amount of "work" needed to transfer one probability distribution to another as an optimal transport problem \cite{vaserstein1969wasserstein}.
> This distance measure is widely used in computer vision as the Fréchet Inception Distance (FID)~\cite{heusel2017fid} and in audio processing as the Fréchet Audio Distance~\cite{kilgour2019fad}. %or Fréchet DeepSpeech Distance \cite{gritsenko2020fdsd}.
> Here, we formulate how to compute \( W_2 \) given the empirical distributions of a feature \( X \) computed on datasets \( D_1 \) and \( D_2 \) for both the one-dimensional and multi-dimensional case.
> We denote the corresponding empirical probability distributions \( \hat{P}(X|D_*) \) as \(\hat{P}_* \).

#### One-Dimensional Case

> In the one-dimensional case, \( W_2 \) can be computed as a function of the ordered samples \cite{kolouri2018sliced}: 

$$
W_2( \hat{P}_1, \hat{P}_2 ) =
\sqrt{
\frac{1}{n}
\sum_{i=1}^{n}
\left( x_i - y_i \right)^2
}
$$

> where \( \{ x_1, \ldots, x_n \} \) denote sorted samples of \( \hat{P}(X|D_1) \), and \( \{ y_1, \ldots, y_n \} \) denote sorted samples of \( \hat{P}(X|D_2) \) .

#### Multi-Dimensional Case

> We can compute \( W_2 \) distance for two normally distributed multi-dimensional \( \hat{P_1} \) and \( \hat{P_2} \) using their mean vectors \( \mu_1 \) and \( \mu_2 \) and their covariance matrices \( \Sigma_1 \) and \( \Sigma_2 \) \cite{heusel2017fid}:

$$
W_2( \hat{P}_1, \hat{P}_2 ) =
\sqrt{
\|\mu_1-\mu_2\|^2+D_{B}(\Sigma_1,\Sigma_2)
}
$$

> where \( D_B \) is the unnormalized Bures metric defined as

\[
D_B(\Sigma_1,\Sigma_2)=
\text{trace} \left (
    \Sigma_1 + \Sigma_1
    - 2 ( \Sigma_2^{1/2} \Sigma_1 \Sigma_2^{1/2} )^{1/2} 
\right )
\]

> We can use this approach since latent representations of neural networks are presumably normally distributed~\cite{heusel2017fid}.

### 3.4.Overall Score

> To evaluate how close a synthetic speech dataset \( D_{\text{syn}} \) is to real speech given a particular feature \( X \), we compute its Wasserstein distance \(W_2\) for real reference datasets \( \mathfrak{D}_{\text{real}} \) and distractor (noise) datasets \( \mathfrak{D}_{\text{noise}} \).
> We find the smallest \( W_2 \) among the real and noise datasets respectively as

$$
  \begin{split}
    W_{\text{real}} &= \min_{D_{\text{real}} \in \mathfrak{D}_{\text{real}}} W_2(\hat{P}_{\text{syn}}, \hat{P}_{\text{real}}) \\
    W_{\text{noise}} &= \min_{D_{\text{noise}} \in \mathfrak{D}_{\text{noise}}} W_2(\hat{P}_{\text{syn}}, \hat{P}_{\text{noise}})  
  \end{split}
$$

> We define the overall score (ranging from 0 to 100) as

\[
S=100\times \frac{
W_{\text{real}}
}
{
W_{\text{real}}+W_{\text{noise}}
}
\]

> Any score can be intuitively interpreted -- if \( S \) is greater than 50, then \( D_{\text{syn}} \) is more similar to the closest real speech dataset than to the closest noise dataset for a particular feature.
> An example of this can be seen in Figure~\ref{fig:dist}, in which we show the difference (for a SSL representation feature) between the best-performing and the worst-performing system in the TTS Arena dataset.
> The higher score of system (a) corresponds with a larger overlap with the reference dataset and smaller overlap with the noise dataset than system (b).

## 4.Experiments: 实验

> To validate our benchmark, we compare our factors and TTSDS scores to subjective measures across three different time periods.
> The Blizzard 2008 challenge \cite{king2008blizzard} compared 22 TTS systems across several tasks using MOS.
> We choose the "Voice A" audio-book task with 15 hours of data.
> Later, the "Back to the Future" (BTTF) \cite{le2022back} compared unit selection, hybrid and statistical parametric HMM-based systems from the Blizzard 2013 challenge \cite{king2013blizzard} with deep learning systems inspired by the Blizzard 2021 challenge \cite{ling2021blizzard} based on FastPitch \cite{lancucki2021fastpitch} and Tacotron~\cite{wang2017tacotron}.
> The latest systems, which are most commonly based on discrete speech representations generatively modeled by LLM-like systems (**[VALL-E 2 (2024)](../Models/Speech_LLM/2024.06.08_VALL-E2.md)**), are represented by the TTS Arena leaderboard \cite{ttsarena}, which is a crowdsourced effort to evaluate these systems.
> Only systems released in 2023 and 2024 are featured in this evaluation.
> Since the data is not publicly released, we reproduce datasets for as many of the systems as possible.\footnote{We had to exclude MetaVoice and GPT-SoVITS due to reference audio length requirements; and MeloTTS due to only female voices being available.
> This leaves us 9 out of the 12 publicly available systems.
> As conditioning for the TTS systems, we use speaker reference waveforms from the LibriTTS test set, coupled with unrelated transcripts from the same set to make sure the data could not have been encountered during training.
> 
> Our reference speech datasets are LibriTTS, LibriTTS-R, LJSpeech, VCTK, and the training sets for the Blizzard challenges \cite{le2022back,king2008blizzard}.
> We sample 100 utterances at random from each dataset (if available, from their test sets).
> For distractor/noise datasets, we use the ESC dataset of background noises \cite{piczak2015esc}, as well as the following generated noise -- random uniform, random normal, all zeros and all ones.
>
> We compare our benchmark with two MOS prediction methods.
> The first method is WVMOS ~\cite{andreev2022hifi}, which uses wav2vec 2.0 model~\cite{baevski2020wav2vec2} fine-tuned to predict MOS scores.
> Its system-level correlation coefficients range from 0.68 to 0.96 for different corruptions of speech and their corresponding MOS scores~\cite{andreev2022hifi}.
> The second method is UTMOS ~\cite{saeki2022utmos}, which is an ensemble MOS prediction system that won several categories in the 2022 VoiceMOS challenge \cite{huang2022voicemos}, and has since been used for evaluation of several leading TTS systems (**[XTTS (2024)](../Models/Speech_LLM/2024.06.07_XTTS.md)**; **[NaturalSpeech3 (2024)](../Models/Diffusion/2024.03.05_NaturalSpeech3.md)**).
>
> For all system\(\times\)feature combinations, we compute the score as described in Section~\ref{sec:overall}.
> We average all features for each factor, which gives us the corresponding factor score.
> Averaging all factor scores in turn gives the TTSDS score.

## 5.Results: 结果

> Given the scores of all systems and the subjective measures reported for the given datasets (MOS for Blizzard'08 and BTTF; Elo Rating for TTS Arena), we report their Spearman rank correlation coefficients in Figure~\ref{fig:heatmap}.
> We observe both our factors and baseline MOS prediction systems vary strongly between different the different corpora, but the TTSDS score correlates consistently well with subjective evaluation results.
> 
> The **baseline MOS prediction methods** achieve mixed results for the Blizzard'08 and BTTF data.
> UTMOS and WVMOS respectively achieve a high correlation on one of the two datasets while only yielding low correlation on the other.
> We hypothesize that UTMOS might have included unit-selection systems in its training data, but it have not encountered enough variants of the FastPitch/Tactotron systems present in BTTF.
> The inverse seems to be the case for WVMOS.
> For TTS Arena, both systems do not perform well.
> In summary, these MOS prediction systems sometimes achieve high correlation with ground-truth MOS, but do not seem to generalize.

### 5.1.TTSDS Benchmark

> We now discuss the individual scores of our benchmark -- the development of individual of these scores' correlations with subjective evaluation can be seen in Figure~\ref{fig:overtime}.
>
> The **General score** shows some correlation with human evaluation results, but the correlation is generally low.
> The General score only slightly outperforms MOS prediction for TTS Arena, and shows the lowest correlation of all factors for the Blizzard'08 systems.
> For unit selection voices, this might be explained by the fact they consist of real speech samples, however, speaker verification representations should suffer from the same problem and they do not seem to be affected as much.
>
> The **Environment score** has a low correlation with subjective measures for both Blizzard'08 and TTS Arena, but it is interestingly the most important factor for BTTF.
> Due to BTTF consisting of both deep learning systems from 2021 and systems from 2013, this factor might pick up on artifacts which are only present in the latter.
> 
> Meanwhile for the Blizzard'08 systems, these artifacts might be similar enough between systems listeners didn't prioritize them in evaluation, while for modern systems in TTS Arena, hardly any noise or artifacts are present.
>
> The **Intelligibility score** shows a high correlation for Blizzard'08, but it is the only of our scores showing a negative correlation for BTTF.
> Again, this could be due to the difference between unit selection and neural voices, with the former perhaps having more realistic intelligibility, but worse naturalness as perceived by humans.
>
> The **Prosody score** is consistent between datasets, which might be in part due to the diversity of the underlying features (i.e.
> pitch, SSL prosody representations and segmental durations).
> It also increases over time, with the TTS Arena system scores showing the highest correlation with our prosody score.
> This confirms the intuition that good prosody has always been a factor in subjective  evaluation.
> The increase in prosody score might indicate that human evaluators focus more on the prosody of the speech as other factors such as the intelligibility or noise conditions have vastly improved with modern systems.
>
> The **Speaker score** also shows high correlation for Blizzard'08 and TTS Arena, but fails for BTTF.
> We believe this is due to older unit-selection systems included in BTTF producing a natural speaker embedding for concatenated parts of real speech.
> This effect is pronounced because we only achieve a significant TTSDS score correlation when the Speech factor is excluded for BTTF.
>
> The **TTSDS score** achieves higher correlation than any single factor for all datasets included in our study, despite the low number of 80-100 samples per dataset.
> One of the baseline MOS prediction networks still performed better for the early Blizzard'08 systems, but both MOS prediction networks were significantly outperformed by our benchmark for the more modern systems.
> However, individual factors often show lower correlations with MOS than the baseline systems, highlighting the need for combining several factors.
> We hypothesize that this might be the reason measures similar to the Fréchet Inception Distance \cite{heusel2017fid} for computer vision have not become popular for speech evaluation -- with the low number of samples typically used for TTS evaluation, and the many factors contributing to what "good" speech synthesis is, and single distance measure might not be enough to show correlation with human evaluation.
>
> Table~\ref{tab:leaderboard} shows our benchmark compared to MOS prediction and the subjective human evaluation rating from TTS Arena.
> While UTMOS correctly predicts the best system, the other scores by the MOS prediction systems show little to no correlation with Elo ratings; our prosody factor, speaker factor and overall TTSDS scores correlate well with the Elo ratings.
> However, OpenVoice v2 \cite{qin2023openvoice} is scored highly by TTSDS but achieved low scores in TTS Arena -- this might be due to differences in configuration, as the details for generating the speech used in TTS Arena are not public. 
> To evaluate whether our benchmark could be used for system selection, we perform a Wilcoxon signed-rank test (Figure~\ref{fig:wilcoxon}).
> We observe that while the worst-performing systems can generally be distinguished from the highest-performing ones, there is no statistically significant difference between the better-performing systems. 
> Finding significant differences between TTS systems has been difficult, even with previous subjective evaluation methods \cite{le2022back,king2008blizzard}.
> However, using more speech samples and features for future iterations of TTSDS could mitigate this.

## 6.Conclusions: 结论

> In this work, we proposed a benchmark assessing prosody, speaker identity, intelligibility, environment, and general distribution of synthetic speech.
> Evaluating 35 TTS systems from 2008 to 2024, our benchmark showed strong correlation with human evaluations (0.60 to 0.83).
> This highlights the robustness and adaptability of our approach to evolving evaluation criteria.
> Individual factors alone showed limited correlation, but their combination significantly outperformed traditional MOS prediction systems, especially for modern TTS systems.
> Our results underscore the importance of intelligibility and prosody, and the need for TTS systems to replicate realistic recording conditions and speaker characteristics.
> We revealed limitations in existing MOS prediction systems, emphasizing the need for a nuanced approach to TTS evaluation.
> High correlation with human evaluations suggests our benchmark provides a reliable and comprehensive framework for assessing synthetic speech quality.
